{
    "rule": "{\n    \"Root Cause\": \"Calling torch.view_as_complex with a zero dimensional tensor leads to a segmentation fault.\",\n    \"Argument Type\": \"The buggy argument is a zero dimensional tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/b6b1c01adfdadf93a4a1d30c3661ff177412a876"
},
{
    "rule": "{\n  \"Root Cause\": \"Using integer division to calculate blocks for cuda kernel launch in GET_BLOCKS function.\",\n  \"Argument Type\": \"Integer (int64_t) variable 'N'.\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/c010ef7f0c6d837809a7e973048afac76373e3de"
},
{
    "rule": "{\n    \"Root Cause\": \"Implicit assumption in `at::native::embedding` that `weight` argument would be 1-D or greater, which caused a segmentation fault when given a 0-D tensor, fixed by raising a `RuntimeError` instead.\",\n    \"Argument Type\": \"Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/42b4a7132e7c6f1df963b473d1583e4791fb1808"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to trying to initialize a generator with 'cuda' device on a machine that does not have CUDA-enabled hardware/drivers. This results in a segmentation fault.\",\n    \"Argument Type\": \"Tensor device parameter\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/42281"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to the lack of support for `torch.bool` datatype in the NCCL backend.\",\n    \"Argument Type\": \"torch.bool\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/24137"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to unsupported data type (`torch.bool`) used in tensor broadcasting with NCCL backend.\",\n    \"Argument Type\": \"Boolean (torch.bool) tensor.\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/327"
},
{
    "rule": "{\n   \"Root Cause\":\"Bug due to unsupported data type for NCCL process group. Specifically, the NCCL backend doesn't support 'torch.bool' datatype. Broadcasting a tensor of this type throws error 'RuntimeError: Unsupported data type for NCCL process group'\",\n   \"Argument Type\":\"Boolean\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/327"
},
{
    "rule": "{\n  \"Root Cause\": \"Bug due to passing unsupported data type `torch.bool` in `dist.broadcast()` function.\",\n  \"Argument Type\": \"torch.bool\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/38764"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to a calculation error in torch.remainder when the dividend is very large.\",\n    \"Argument Type\": \"Floating point numbers (torch.float or torch.double)\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/63b1ae69831cd21bc4d6059a5854bc1155a152c9"
},
{
    "rule": "{\n\"Root Cause\": \"Uncaught mismatch between index tensor and source or output tensor sizes in the gather function resulted in nonsense values and segfaults.\",\n\"Argument Type\": \"Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/7aec364bdf9ed7297b77e8445a6a6d4116265dde"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to inability of `torch.zeros()` to create contiguous memory layout CUDA tensor which is required by `numpy()` method of the tensor object. Therefore, `numpy()` method crashes when called.\",\n    \"Argument Type\": \"CUDA Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/33300"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to PyTorch not being able to handle indexing with a single element array\",\n    \"Argument Type\": \"Single element array\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/33001"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to using int32 instead of int64 in pdist kernel to handle batches >= 46342\",\n    \"Argument Type\": \"Tensor with large batch size\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/a64d0ffe81b250889a3e6670daa9c7d07d946e32"
},
{
    "rule": "{\n    \"Root Cause\": \"Incorrect scaling of uint8 NCHW tensor when calling add_images() method on SummaryWriter in tensorboard, resulting in overflow behavior\",\n    \"Argument Type\": \"torch.uint8 Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/ddff014b79303e5239d5cb876ba97143cad6405a"
},
{
    "rule": "{\n    \"Root Cause\": \"Failure to check for infinite and NaN values in the input tensor, as well as invalid min-max values resulting in a runtime error.\",\n    \"Argument Type\": \"input tensor, min and max values\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/136bb07a93b779acbc84ff341bc397551a8cfcc2"
},
{
    "rule": "{\n    \"Root Cause\": \"Int32 overflow due to multiplication with a very large integer variable\",\n    \"Argument Type\": \"int32\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/ec8e75ea92ae2b5ea73b4aeb3ec7cb39e9f95db9"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to indexing a tensor using a single element array resulting in an illegal memory access.\",\n    \"Argument Type\": \"Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/24309"
},
{
    "rule": "{\n    \"Root Cause\": \"Overflow error due to int variable trying to index tensors that are larger than the maximum value representable by a 32-bit int.\",\n    \"Argument Type\": \"int\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/74828be4a7d0d2dba3f0ec3f6e79265cdfae5329"
},
{
    "rule": "{\n    \"Root Cause\": \"Binomial overflow occurs when the value of logits is very large\",\n    \"Argument Type\": \"logits (a torch.tensor containing float values)\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/071971476d7431a24e527bdc181981678055a95d"
},
{
    "rule": "{\n    \"Root Cause\": \"Binomial overflow due to unstable calculation of normalization term.\",\n    \"Argument Type\": \"logits (tensor of float data type)\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/071971476d7431a24e527bdc181981678055a95d"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to incorrect handling of isinf function for integral tensors resulting in RuntimeError: value cannot be converted to type int16_t without overflow: inf\",\n    \"Argument Type\": \"Integral Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/commits/d4712ee218cd6af3c2096ca7a76fef350173b703"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to enforcing the input data to be in decreasing order in order to apply the pack_padded_sequence function correctly. If the input is not sorted, the function tries to access out-of-bounds memory locations that eventually lead to segfaults (Segmentation faults).\",\n    \"Argument Type\": \"The argument causing the bug is the 'enforce_sorted' parameter of the pack_padded_sequence() function, which is a boolean variable that specifies whether the input sequences need to be sorted or not.\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/13324"
},
{
    "rule": "{\n  \"Root Cause\": \"Bug due to a memory access violation when trying to sort sequences that are not in decreasing order. The implementation of pack_padded_sequence assumes that the input sequences are sorted in decreasing order, and if this assumption is violated, the function tries to access memory outside the bounds of the input tensor, causing a segmentation fault.\",\n  \"Argument Type\": \"The input to pack_padded_sequence is a tensor of shape (batch_size, max_sequence_length, feature_dim), where batch_size is the number of sequences in the batch, max_sequence_length is the maximum length of the sequences, and feature_dim is the dimensionality of the features. The buggy argument is the sequence lengths parameter, which is a list of integers of length batch_size that specifies the length of each sequence in the batch.\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/9264"
},
{
    "rule": "{\n  \"Root Cause\": \"Bug due to the implementation of the function `torch.nn.utils.rnn.pack_padded_sequence` which encounters an issue related to memory leak in autograd using ndarray[None] when input is not in decreasing order.\",\n  \"Argument Type\": \"pad_sequence input (list of tensors)\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/3498"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to the fact that `torch.special.round` is declared as an alias of `torch.round` in the `OpInfo`, but they behave differently due to implementation issues.\",\n    \"Argument Type\": \"The buggy argument is a tensor with an unsupported dtype being passed to `torch.special.round` function.\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/86326"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to attempting to initialize MPSNDArray with a descriptor that exceeds the maximum bytes representable by 32-bit integer, which is 2^32, while calling the MPS version of multinomial op on very large arrays with replacement.\",\n    \"Argument Type\": \"Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/86279"
},
{
    "rule": "{\n  \"Root Cause\": \"Bug due to using a pad_sequence with inputs not in decreasing order causing pack_padded_sequence function to force python environment to shut down due to a segmentation fault\",\n  \"Argument Type\": \"List of Tensors\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/86162"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to the incompatible operand types of torch.remainder and torch.fmod. The APIs are primarily designed to work with float tensors, but in this case, we are using integer tensors as inputs. This is leading to inaccurate results.\",\n    \"Argument Type\": \"Integer\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/86074"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to performing in-place operation on the input tensor. When `out` argument is not specified, `torch.mm` creates a new tensor to store the output. However, when `out` is specified as the input tensor, the in-place operation can cause the result to be overwritten before it is fully computed.\",\n    \"Argument Type\": \"Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/85852"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to providing a weight tensor with invalid dimensions to the native_batch_norm function, which is causing a segmentation fault. The weight tensor should have dimensions (num_features), but in the provided example, it has dimensions (14, 9, 12, 0, 6, 0, 15, 0, 0, 0).\",\n    \"Argument Type\": \"Weight Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/85217"
},
{
    "rule": "{\n  \"Root Cause\": \"Bug due to passing an out-of-range value for the dimension argument, causing a segmentation fault.\",\n  \"Argument Type\": \"Integer\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/85216"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to passing very large integer values for the `stride` and `dilation` arguments which exceed the maximum allowable integer value in the implementation of `mkldnn_reorder_conv2d_weight` and `mkldnn_reorder_conv3d_weight` functions.\",\n    \"Argument Type\": \"Integer\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/85214"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to passing empty tensors to `embedding_bag`, `_embedding_bag` and `_embedding_bag_forward_only` functions. Empty tensors of incorrect shape are causing a segmentation fault error.\",\n    \"Argument Type\": \"Empty tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/85213"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to passing 'None', which is an invalid value, as an argument to 'torch.jit.wait' function.\",\n    \"Argument Type\": \"NoneType\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/85072"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to passing invalid input argument to `torch.futures.collect_all` method which is causing a segmentation fault.\",\n    \"Argument Type\": \"Tuple with None value\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/84990"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to incomplete ONNX generated by torch.nn.functional.pad when default padding value is used. The official documentation claims that the default padding value is 0, however, the generated ONNX does not include the default value which creates problems when the model is converted to other formats like OpenVINO, TFlite, etc.\",\n    \"Argument Type\": \"Padding value argument\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/84979"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to a memory access violation, as an invalid input to `torch.futures.collect_all()` is causing a segmentation fault.\", \n    \"Argument Type\": \"Invalid input to `torch.futures.collect_all()`\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/83585"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to incorrect implementation of `torch.pinverse` function which fails to calculate the correct output for a 3x3 tensor.\",\n    \"Argument Type\": \"Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/83494"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to float16 data type not being supported by torch.nn.Conv2d.\",\n    \"Argument Type\": \"float16\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/83328"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to passing negative kernel_size argument in torch.nn.MaxUnpool2d. When the kernel_size is negative, the output tensor shape also becomes negative.\",\n    \"Argument Type\": \"Integer\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/83229"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to the lack of type verification of the 'num_features' parameter in torch.nn.InstanceNorm{1|2|3}d, which allows negative integral / string / list and other types of values to be set as 'num_features'.\",\n    \"Argument Type\": \"The 'num_features' parameter in torch.nn.InstanceNorm{1|2|3}d can be set to negative integral / string / list and other types of values.\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/83221"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to passing an extremely large integer value as the number of layers while initializing torch.nn.GRU, leading to long execution times and eventual program hanging.\",\n    \"Argument Type\": \"Integer\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/83175"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to very large integer value provided as padding argument, which causes the underlying algorithm to run for an extremely long duration.\",\n    \"Argument Type\": \"padding (int)\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/83152"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to a mismatch in gradient computation when the inputs to torch.min and torch.max are the same.\",\n    \"Argument Type\": \"Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/82635"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to the presence of NaN values in the input argument of torch.matrix_exp function\",\n    \"Argument Type\": \"torch.Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/82282"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to calling the 'torch._C._nn.adaptive_avg_pool2d' API with some input and output shapes that are not compatible.\",\n    \"Argument Type\": \"Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/81409"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to a change in the weighting function for the '`torch._weight_norm`' API in torch 1.12, causing it to return wrong results. In detail, the weight normalization procedure changed in torch 1.12 with the introduction of a new parameter 'eps' (epsilon), which affects the computation of norm factor in '`torch.nn.functional.normalize`'. The default value of 'eps' is modified from '1e-12' to '1e-06' causing the differences in the results from the previous version ('torch1.11') in specific cases like '`torch._weight_norm`' with specified dim.\",\n    \"Argument Type\": \"The buggy argument is a torch Tensor object which is used as one of the inputs for '`torch._weight_norm`' API in the specified dimension.\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/81195"
},
{
    "rule": "{\n  \"Root Cause\": \"Bug due to a change in the behavior of torch.nn.functional.linear from torch 1.11 to torch 1.12, where it does not support multi-dimensional bias input. This change is not documented and was not mentioned in any release notes.\",\n  \"Argument Type\": \"Multi-dimensional Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/80946"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to memory overflow\",\n    \"Argument Type\": \"torch.Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/80805"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to numerical instability caused by dividing by zero in the calculation of the renormalization factor when `p` is even and `maxnorm=0`.\",\n    \"Argument Type\": \"torch.Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/80804"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to numerical instability when performing repeated small additions in the torch.baddbmm function on CPU.\",\n    \"Argument Type\": \"torch.FloatTensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/80588"
},
{
    "rule": "{\n  \"Root Cause\": \"Bug due to using log values in KL divergence calculation, which leads to negative values\",\n  \"Argument Type\": \"log values\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/80488"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to passing an empty tensor to `sequences_1` which leads to `NaN` value in `pack_padded_sequence()`\",\n    \"Argument Type\": \"Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/78153"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to passing empty tensor to batch_sizes parameter. The function expects batch_sizes tensor to contain the batch sizes, but in this case, an empty tensor is passed which causes a segmentation fault.\",\n    \"Argument Type\": \"batch_sizes: Empty Tensor(torch.Size([0]))\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/78131"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to providing an empty tensor as the first argument (input tensor) for the function call to `torch._grid_sampler_2d_cpu_fallback()` causing an attempt to read/write to an invalid memory location resulting in a segmentation fault.\",\n    \"Argument Type\": \"Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/78130"
},
{
    "rule": "{\n  \"Root Cause\": \"Bug due to passing empty tensor as an argument to the torch._embedding_bag_forward_only function, which causes segmentation fault.\",\n  \"Argument Type\": \"torch.Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/78129"
},
{
    "rule": "{\n  \"Root Cause\": \"Bug due to passing an empty list as argument intarrayref_5. This causes null pointer dereferencing in the C++ backend.\",\n  \"Argument Type\": \"List\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/78128"
},
{
    "rule": "{\n  \"Root Cause\": \"Bug due to passing an empty list as argument to `torch._C._nn.reflection_pad2d` function. The function implementation expects a non-empty integer array as a second argument. However, it doesn't check for an empty list and offsets it by one, leading to a segmentation fault.\",\n  \"Argument Type\": \"List\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/78127"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to passing an empty array as the size argument intarrayref_2 to the max_unpool3d function in torch library. An empty array doesn't provide the required size information, causing the segmentation fault.\",\n    \"Argument Type\": \"Empty List\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/78126"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to passing invalid interpolation_mode or padding_mode argument to torch.grid_sampler_3d function.\",\n    \"Argument Type\": \"Integer\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/78125"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to providing negative value for one of the arguments (numel, n_bins, ratio, bit_width) in `torch.choose_qparams_optimized` function.\",\n    \"Argument Type\": \"Integer (int)\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/78123"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to providing weights tensor with incorrect shape/size.\",\n    \"Argument Type\": \"Weights tensor (should be 1-dimensional with length equal to the maximum value of the input tensor + 1)\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/78122"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to providing an out of range value for the `out_dim` argument in `torch._remove_batch_dim()` function. Since this value is way beyond the maximum memory limit, it results in a segmentation fault.\",\n    \"Argument Type\": \"Integer\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/77893"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to mismatch in the dtype of input and exponent tensors while computing forward AD in torch.pow function.\",\n    \"Argument Type\": \"torch.Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/77493"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to inconsistency in handling empty tensors of different types, leading to an invalid gradient returned by ScatterAddBackward0 function.\",\n    \"Argument Type\": \"Complex tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/77231"
},
{
    "rule": "{\n\"Root Cause\": \"Bug due to incompatible type conversion while computing gradients for input of `float64` type and `mat` and `vec` of `complex128` type.\",\n\"Argument Type\": \"input\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/76778"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to torch.unique() implementation on GPU with NaN inputs. The implementation of the function does not handle NaN inputs properly on GPU which leads to a segmentation fault in the _remove_batch_dim function.\",\n    \"Argument Type\": \"Float tensor with NaN values\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/76571"
},
{
    "rule": "{\n   \"Root Cause\": \"Bug due to non-ascii characters in the file path causing a segmentation fault in `_remove_batch_dim`.\",\n   \"Argument Type\": \"String\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/75171"
},
{
    "rule": "{\n    \"Root Cause\": \"Bug due to creating tensor from transposed non-contiguous NumPy array. When a tensor is created from a NumPy array, the underlying memory is shared between the two objects. However, when the NumPy array is non-contiguous, the memory pointers are switched, which results in the tensor pointing to the wrong memory locations. In this case, the tensor is created from a transposed NumPy array, which results in a non-contiguous array. Therefore, the tensor points to the wrong memory locations, causing incorrect results when using torch.distributed.gather.\",\n    \"Argument Type\": \"Tensor\"\n}",
    "link": "https://api.github.com/repos/pytorch/pytorch/issues/74809"
},
