[
    {
        "Id": 177,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/b6920405da340bbd3397b80bf16d9c360b0c48d2",
        "Violation": "improper",
        "Bug report": "reorder checks to shave 1 us off no-op dispatch time ",
        "Number of deleted lines": 5,
        "Deleted lines": "    \"`with torch.no_grad():` instead.\";\n\nstatic bool check_has_torch_dispatch(PyObject *obj) {\n  PyTypeObject *tp = Py_TYPE(obj);\n  py::object attr = PyObject_FastGetAttrString(obj, \"__torch_dispatch__\");\n  return (\n    !THPVariable_CheckTypeExact(tp) &&\n    attr.ptr() != nullptr &&\n    attr.ptr() != torch::disabled_torch_dispatch_impl()\n  );\n}\n\nstatic PyObject* device_to_py_class_ [static_cast<size_t>(c10::DeviceType::COMPILE_TIME_MAX_DEVICE_TYPES)];\n"
    },
    {
        "Id": 75,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/157d478a30f27fd9d866c1235841721a559c8d0b",
        "Violation": "improper",
        "Bug report": "Fix omission of shape in size check in index. ",
        "Number of deleted lines": 1,
        "Deleted lines": "                    IndexError\n                )\n                for j in range(index.ndim):\n                    check(\n                        index[j] <= self.shape[k + j],\n                        lambda: f\"The shape of the mask {index.shape} at index {i} \"\n                                f\"does not match the shape of the indexed tensor {self.shape} at index {k + j}\",\n                        IndexError\n                    )\n                    result.append(nonzero.select(1, j))\n            else:"
    },
    {
        "Id": 213,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9f8ad5ff118166537d42f87f1ee254f83ba553f0",
        "Violation": "improper",
        "Bug report": "Fix CUDA version check (format is 1000 * major + 10 * minor). ",
        "Number of deleted lines": 1,
        "Deleted lines": "\nport::StatusOr<std::vector<uint8>> LinkGpuAsm(\n    gpu::GpuContext* context, std::vector<CubinOrPTXImage> images) {\n  const bool linking_supported = [] {\n    if (CUDA_VERSION < 11300) {\n      return true;\n    }\n    auto version_or_status = gpu::Diagnostician::FindKernelDriverVersion();\n    if (!version_or_status.ok()) {\n      LOG(WARNING) << \"Couldn't read CUDA driver version.\";\n      return false;"
    },
    {
        "Id": 21,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/c99277e177cf16736262251c7e92ea5e9ba2c5c2",
        "Violation": "improper",
        "Bug report": " handle the case in acc_ops.sum when dim == 0, differentiating it from the case when dim is None. handle the case in acc_ops.sum when dim == 0, differentiating it from the case when dim is None",
        "Number of deleted lines": 1,
        "Deleted lines": "\n\n@register_acc_op\ndef sum(*, input, dim=None, keepdim=False, dtype=None):\n    if dim:\n        return torch.sum(**locals())\n    else:\n        return input.sum(dtype=dtype)\n\n\n@register_acc_op_mapping(op_and_target=(\"call_function\", torch.sigmoid))"
    },
    {
        "Id": 234,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1908d7ef706f0f3f8c7a300068355bf795fb3d17",
        "Violation": "improper",
        "Bug report": "Fix out-of-bounds StringPiece access in ForwardNUTF8CharPositions().  Even a simple invocation like 'int p = 0; ForwardNUTF8CharPositions(\"a\", 1, &p);' will cause an invalid access to in[1]. Checking for *pos < size before that access fixes this issue.",
        "Number of deleted lines": 1,
        "Deleted lines": "  while (utf8_chars_counted < num_utf8_chars_to_shift && *pos < size) {\n    do {\n      ++*pos;\n    } while (IsTrailByte(in[*pos]) && *pos < size);\n    ++utf8_chars_counted;\n  }\n  return utf8_chars_counted == num_utf8_chars_to_shift;\n}\n"
    },
    {
        "Id": 262,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a0ca4bcb81dfd07fdb1c7872b5852f84cfc1a081",
        "Violation": "improper",
        "Bug report": "Fix separable convolution bias check",
        "Number of deleted lines": 1,
        "Deleted lines": "    if self.data_format == 'channels_first':\n      outputs = array_ops.transpose(outputs, (0, 3, 1, 2))\n\n    if self.bias:\n      outputs = nn.bias_add(\n          outputs,\n          self.bias,\n          data_format=utils.convert_data_format(self.data_format, ndim=4))\n\n    if self.activation is not None:"
    },
    {
        "Id": 4,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/6b4c686b9a33a1503a4a4133f9067dd31e0822f7",
        "Violation": "improper",
        "Bug report": "Forward fix a performance regression caused by #110510. When a model is run once, all those kernel pointers are initialized and removing the if-nullptr check will cause those loadKernel be unnecessarily executed again when we rerun the foward function.",
        "Number of deleted lines": 2,
        "Deleted lines": "    def generate_load_kernel_once(\n        self, name: str, mangled_name: str, cubin_path: str, shared_mem: int\n    ):\n        if V.graph.aot_mode:\n            self.writeline(\n                f\"\"\"kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n            )\n        else:\n            self.writeline(\n                f\"\"\"{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n            )\n\n    def generate_args_decl(self, call_args):\n        dynamic_symbols = V.graph.sizevars.free_symbols()\n        new_args = []\n        for arg in call_args:"
    },
    {
        "Id": 281,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cb164786dc891ea11d3a900e90367c339305dc7b",
        "Violation": "improper",
        "Bug report": " Properly handle the case where SpecializeType() returns an error `Status`. If the error case in `SpecializeType()` is reached, then we would get a crash when trying to access the value of an errorenous `StatusOr` object",
        "Number of deleted lines": 1,
        "Deleted lines": "    const OpDef& op_def, const std::vector<const Tensor*>& input_tensors,\n    const std::vector<ShapeHandle>& input_tensors_as_shapes) {\n  const auto ret = full_type::SpecializeType(attrs_, op_def);\n  DCHECK(ret.status().ok()) << \"while instantiating types: \" << ret.status();\n  ret_types_ = ret.ValueOrDie();\n\n  input_tensors_ = input_tensors;\n  input_tensors_as_shapes_ = input_tensors_as_shapes;\n\n  construction_status_ ="
    },
    {
        "Id": 355,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/653d47379b3a716f82058148c35b2f491bfa2856",
        "Violation": "improper",
        "Bug report": " Speed up 4-bit unpacking loop in reference implementation. The new implementation moves the odd-length check out of the main loop, and unrolls slightly differently.",
        "Number of deleted lines": 6,
        "Deleted lines": "}\n\nvoid UnpackDenseInt4IntoInt8(const int8_t* src_buffer, int num_elements,\n                             int8_t* dst_buffer) {\n  for (int i = 0; i < num_elements; i += 2) {\n    dst_buffer[i] = static_cast<int8_t>(src_buffer[i / 2] << 4) >> 4;\n    if (i + 1 == num_elements) break;\n    dst_buffer[i + 1] = static_cast<int8_t>(src_buffer[i / 2]) >> 4;\n  }\n}\n\n}  // namespace tensor_utils\n}  // namespace tflite\n"
    },
    {
        "Id": 42,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/f02b7a9c36dd6182da694bc47a5c345285dfd951",
        "Violation": "improper",
        "Bug report": "don't error when unused fill value is zero. In the python version of `F.pad`, checking that the fill value was left as default was done by comparing against zero. So if someone does explicitly pass in a zero-value, then this `TORCH_CHECK` was an accidental BC-break.",
        "Number of deleted lines": 4,
        "Deleted lines": "\n  if (mode == at::padding_mode::constant) {\n    return at::constant_pad_nd(self, pad, value.value_or(0.0));\n  }\n  TORCH_CHECK(\n      !value.has_value(), \"Padding mode \\\"\",\n      padding_mode_string(mode),\n      \"\\\" doesn't take in value argument\");\n\n  if (pad.size() == 2 && (input_dim == 2 || input_dim == 3)) {\n    switch (mode) {\n      case at::padding_mode::reflect: return at::reflection_pad1d(self, pad);\n      case at::padding_mode::replicate: return at::replication_pad1d(self, pad);\n      case at::padding_mode::circular: return at::_pad_circular(self, pad);"
    },
    {
        "Id": 282,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c7f79bb75b5b83c3011e164ccd617a6ada910ea4",
        "Violation": "improper",
        "Bug report": "StatSummarizer: Put size check error message outside of if block where it belongs.",
        "Number of deleted lines": 9,
        "Deleted lines": "        return;\n      }\n      const auto& stored = detail->outputs[slot];\n      const auto& current = output.tensor_description();\n      bool do_shapes_match = true;\n      if (stored.shape().dim_size() != current.shape().dim_size()) {\n        do_shapes_match = false;\n      } else {\n        for (int i = 0; i < stored.shape().dim_size(); ++i) {\n          if (stored.shape().dim(i).size() != current.shape().dim(i).size()) {\n            do_shapes_match = false;\n          }\n        }\n\n        if ((stored.dtype() != current.dtype()) || !do_shapes_match) {\n          LOG(WARNING) << \"Output tensor changed between runs for '\"\n                       << ns.node_name();\n        }\n      }\n    }\n  }\n}\n\nnamespace {"
    },
    {
        "Id": 54,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/63cbdc92a750a667ffdcfbdac563d02db6fd9559",
        "Violation": "improper",
        "Bug report": "switching the exact check to isinstance check. Simplifying a type check if an object is a SymIntNode in `is_symint_node`",
        "Number of deleted lines": 2,
        "Deleted lines": "\nnamespace torch {\ninline bool is_symint_node(py::handle obj) {\n  auto static tp_symn = py::type::of<c10::SymIntNodeImpl>();\n  if (obj.get_type().equal(tp_symn)) {\n    TORCH_CHECK(\n        !jit::tracer::isTracing(), \"JIT tracing of SymInts isn't supported!\");\n    return true;\n  }\n  return false;\n}"
    },
    {
        "Id": 126,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/babb28d2a3a755424f72518bc360d9f511a24463",
        "Violation": "improper",
        "Bug report": "Change DHCECK to CAFFE_ENFORCE in softmax_with_loss_op.cc. Summary: Based on discussion on the post in Caffe2 users. Changing DCHECK that works only in debug mode to CAFFE_ENFORCE that throws exception and is a better option. Update: Also correct the check for label_data >= 0, did not check for all elements previously. Moved it to inner loop.",
        "Number of deleted lines": 5,
        "Deleted lines": "  } else {\n    const float* label_data = T.data<float>();\n\n    for (int i = 0; i < N; ++i) {\n      CAFFE_ENFORCE(\n          label_data[i] >= 0,\n          \"Label prob seems incorrect: label prob value must be nonnegative: \",\n          label_data[i]);\n      float l = 0.0;\n      float total_prob = 0.0;\n      float weight = weights ? weights[i] : 1.0;\n      for (int j = 0; j < D; ++j) {\n        l += -log(std::max(Pdata[i * D + j], 1e-20f)) * label_data[i * D + j] *\n            weight;\n        total_prob += label_data[i * D + j];\n      }\n      loss_sum += l;\n      DCHECK(std::abs(total_prob - 1.) < 1e-5f);\n      weight_sum += weight;\n    }\n  }\n\n  avg_loss->Resize(vector<TIndex>());\n  float* avg_loss_data = avg_loss->mutable_data<float>();"
    },
    {
        "Id": 38,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/be253b8ee8a104997773d11ed28928a48193217d",
        "Violation": "improper",
        "Bug report": "The existing check isn't safe for 32-bit `size_t` because the max 64-bit int will overflow.",
        "Number of deleted lines": 1,
        "Deleted lines": "namespace at {\n\nbool geometry_is_contiguous(IntArrayRef sizes, IntArrayRef strides) {\n  assert(sizes.size() < static_cast<std::size_t>(std::numeric_limits<std::int64_t>::max()));\n  auto dim = static_cast<std::int64_t>(sizes.size());\n  int64_t expected_stride = 1;\n  bool contig_if_nonempty = true;\n  for (int64_t i = dim - 1; i >= 0; i--) {\n    if (sizes[i] == 0) {\n      return true;"
    },
    {
        "Id": 352,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5bc536f1afbaff5d3d5a14a9185cd1e3cc31b302",
        "Violation": "improper",
        "Bug report": "[Fix] bug fix during check static shape.",
        "Number of deleted lines": 1,
        "Deleted lines": "  int index = 0;\n  ArrayRef<int64_t> shape;\n  for (Value value : values) {\n    auto shaped_type = value.getType().dyn_cast<ShapedType>();\n    if (!shaped_type && !shaped_type.hasStaticShape()) {\n      return false;\n    }\n    if (index == 0) {\n      shape = shaped_type.getShape();\n    } else {\n      if (shape != shaped_type.getShape()) {"
    },
    {
        "Id": 427,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a4b6b6bcc7a813162bf0378727950e321aca19c",
        "Violation": "improper",
        "Bug report": "Add stricter type checking for tf.math.real (using is_numeric)",
        "Number of deleted lines": 1,
        "Deleted lines": "    input = ops.convert_to_tensor(input, name=\"input\")\n    if input.dtype.is_complex:\n      real_dtype = input.dtype.real_dtype\n      return gen_math_ops.real(input, Tout=real_dtype, name=name)\n    elif tf.debugging.is_numeric_tensor(input):\n      return input\n    else:\n      raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))\n\n\n@tf_export(\"math.imag\", v1=[\"math.imag\", \"imag\"])"
    },
    {
        "Id": 32,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/a9deda5469a6ef73692a9dd796cc4eeba4436d6c",
        "Violation": "improper",
        "Bug report": "The at::native::_validate_sparse_coo_tensor_args only supports checking the indices on CUDA device and CPU device. To extend the function to support more device type.",
        "Number of deleted lines": 1,
        "Deleted lines": "        std::get</* values */ 0>(indices.min(/* dim */ 1, /* keepdim */ false));\n    Tensor max_indices =\n        std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n    Tensor cpu_min_indices, cpu_max_indices;\n    if (indices.is_cuda()) {\n      cpu_min_indices = min_indices.to(at::DeviceType::CPU);\n      cpu_max_indices = max_indices.to(at::DeviceType::CPU);\n    } else {\n      cpu_min_indices = min_indices;\n      cpu_max_indices = max_indices;\n    }"
    },
    {
        "Id": 116,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/ee91c328da5739ce03b3127cd7c542ce505212b8",
        "Violation": "improper",
        "Bug report": "Fix cuda/cpu check on NoneType ",
        "Number of deleted lines": 1,
        "Deleted lines": "            if torch.overrides.has_torch_function(tensor_args):\n                why_not_fast_path = \"some Tensor argument has_torch_function\"\n            elif not all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):\n                why_not_fast_path = \"some Tensor argument is neither CUDA nor CPU\"\n            elif torch.is_grad_enabled() and any([x.requires_grad for x in tensor_args]):\n                why_not_fast_path = (\"grad is enabled and at least one of query or the \"\n                                     \"input/output projection weights or biases requires_grad\")\n            if not why_not_fast_path:\n                merged_mask, mask_type = self.merge_masks(attn_mask, key_padding_mask, query)"
    },
    {
        "Id": 338,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c9b4689bc4d4024aa16b7d6cfc1c65fa1ed8486e",
        "Violation": "improper",
        "Bug report": "Removed no longer supported call to in_eager_execution. Swapped context.in_eager_execution() to the currently supported context.executing_eagerly(). Added negation to eager check. In all likelihood, the negation was always supposed to be there since getting default graph in eager mode does not make sense",
        "Number of deleted lines": 1,
        "Deleted lines": "    options: see ALL_ADVICE example above. Default checks everything.\n  Returns:\n    Returns AdviceProto proto\n  \"\"\"\n  if not graph and context.in_eager_execution():\n    graph = ops.get_default_graph()\n\n  if options == _DEFAULT_ADVISE_OPTIONS:\n    options = ALL_ADVICE.copy()\n"
    },
    {
        "Id": 221,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2f3b69e4976d3b14eaa6ae070eb68f37d1556d98",
        "Violation": "improper",
        "Bug report": "Changed empty check ",
        "Number of deleted lines": 3,
        "Deleted lines": "            \"Object not assigned a value from checkpoint: %s\" % (node,))\n    for checkpointable_object in list_objects(self._root_checkpointable):\n      if (isinstance(checkpointable_object,\n                     data_structures.CheckpointableDataStructure) and\n              len(checkpointable_object.variables) == 0):\n        continue\n      self._checkpoint.all_python_objects.add(checkpointable_object)\n    unused_python_objects = (\n        _ObjectIdentitySet(self._checkpoint.all_python_objects)\n        - _ObjectIdentitySet(self._checkpoint.object_by_proto_id.values()))\n    if unused_python_objects:"
    },
    {
        "Id": 378,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4a1d1c8413a3752af7dc91a7128e202660b0f05c",
        "Violation": "improper",
        "Bug report": " Fix mismatch of shape restriction in DrawBoundingBoxes. In the kernel of DrawBoundingBoxes, the shape of the input images should be 4-D. Though in the shape function, at the end `UnchangedShapeWithRankAtLeast(c, 3)` was used instead (at the beginning of the shape function the validation is `WithRank(c->input(0), 4, &images)` which is correct). This fix address the discrepancy by changing to `UnchangedShape`.",
        "Number of deleted lines": 1,
        "Deleted lines": "      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(boxes, 2), 4, &unused));\n\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 3);\n    });\n\nREGISTER_OP(\"SampleDistortedBoundingBox\")\n    .Input(\"image_size: T\")\n    .Input(\"bounding_boxes: float\")"
    },
    {
        "Id": 256,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c7c4a42c4372ca560ea415fe3a798e18286cedec",
        "Violation": "improper",
        "Bug report": "Fix an error in keras input_layer.Input() dtype type checking. ",
        "Number of deleted lines": 2,
        "Deleted lines": "      if input_tensor is None:\n        dtype = backend.floatx()\n      else:\n        dtype = backend.dtype(input_tensor)\n    elif input_tensor and input_tensor.dtype != dtype:\n      raise ValueError('`input_tensor.dtype` differs from `dtype`.')\n    super(InputLayer, self).__init__(dtype=dtype, name=name)\n    self.built = True\n    self.sparse = sparse\n    self.batch_size = batch_size\n    self.supports_masking = True\n"
    },
    {
        "Id": 351,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9c92b50fc4b95985a0749101976d04896bf19bfe",
        "Violation": "improper",
        "Bug report": " [conv3d_transpose] Fix dim check for bias. Per discussion with @thaink, the previous way to do the dim check for bias is not correct. So we need this change.",
        "Number of deleted lines": 1,
        "Deleted lines": "  const TfLiteTensor* bias = GetInput(context, node, 3);\n  if (bias) {\n    TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input->type);\n    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 4));\n  }\n\n  if (params->dilation_depth_factor > 1 || params->dilation_height_factor > 1 ||\n      params->dilation_width_factor > 1) {\n    kernel_type = kReference;"
    },
    {
        "Id": 263,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1ff493ed1a2059f82f7607a7f0a0aa2ce8d5a542",
        "Violation": "improper",
        "Bug report": "Replace a defensive check with TF_RET_CHECK",
        "Number of deleted lines": 4,
        "Deleted lines": "  DCHECK_NE(out_can_pick_device == nullptr, out_device_picked == nullptr);\n\n  absl::flat_hash_set<absl::string_view> device_names_set;\n  for (absl::string_view device_name : device_names) {\n    if (!device_name.empty()) {\n      device_names_set.insert(device_name);\n    }\n  }\n\n  absl::optional<absl::string_view> maybe_gpu_device;\n  absl::optional<absl::string_view> maybe_cpu_device;\n  absl::optional<absl::string_view> maybe_unknown_device;\n"
    },
    {
        "Id": 423,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b68b869e75916e6de37c2ca23a93643faf333011",
        "Violation": "improper",
        "Bug report": "Fix invalid keras tensor isinstance check",
        "Number of deleted lines": 1,
        "Deleted lines": "      self._batch_input_shape = batch_input_shape\n    else:\n      raise_eager_tensor_error = False\n      if keras_tensor.keras_tensors_enabled():\n        if not isinstance(input_tensor, keras_tensor.keras_tensors_enabled()):\n          raise_eager_tensor_error = True\n      else:\n        if not tf_utils.is_symbolic_tensor(input_tensor):\n          raise_eager_tensor_error = True\n      if raise_eager_tensor_error:\n        raise ValueError('You should not pass an EagerTensor to `Input`. '"
    },
    {
        "Id": 6,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/bede7d999523d02e636a8981c0dff233b67f1a62",
        "Violation": "improper",
        "Bug report": "It does not check if `kind` variable fits in array of pointer called `names`",
        "Number of deleted lines": 1,
        "Deleted lines": "                                \"gs\",\n                                \"ty\",\n                                \"tys\",\n                                \"ival\"};\n  AT_ASSERT(size_t(kind) < sizeof(names) / sizeof(AttributeKind));\n  return names[int(kind)];\n}\n\nstruct AttributeValue {\n  AttributeValue(Symbol name) : name(name) {}\n  using Ptr = std::unique_ptr<AttributeValue>;"
    },
    {
        "Id": 422,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8a2e7deb21f02e4072d6b62cf7f447b9264afe01",
        "Violation": "improper",
        "Bug report": " Adjust checks for type(Tensor) to isinstance or is_eager/is_symbolic_tensor.",
        "Number of deleted lines": 1,
        "Deleted lines": "  Raises:\n    `TypeError` if undefined type in the tensors structure.\n  \"\"\"\n  tensors_type = type(tensors)\n  if tensors_type is ops.Tensor:\n    return apply_fn(tensors)\n  elif isinstance(tensors, variables.Variable):\n    return apply_fn(tensors.value())\n  elif isinstance(tensors, (list, tuple)):\n    tensors = [_recursive_apply(t, apply_fn) for t in tensors]\n    if tensors_type is list:"
    },
    {
        "Id": 142,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/0584fd9339af7c939ab7d955db05743ba58ff86d",
        "Violation": "improper",
        "Bug report": "Only insert observers for fixed qparam ops. Fixed a condition check for fixed qparam ops, previously we were including CopyNodes as well",
        "Number of deleted lines": 1,
        "Deleted lines": "                    return input_arg.name in observed_node_names_set\n                elif isinstance(input_arg, list):\n                    return all(map(is_observed, input_arg))\n\n            if activation_dtype(qconfig) == torch.float16:\n                insert_observer(\n                    node, qconfig.activation(),\n                    model, activation_post_process_map, env, observed_graph,\n                    load_arg, observed_node_names_set)\n            else:"
    },
    {
        "Id": 26,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/0c0c9e743e82b398435ed07719e998aa15ac1ce1",
        "Violation": "improper",
        "Bug report": "Fix dimensions check ",
        "Number of deleted lines": 1,
        "Deleted lines": "    const auto& input = Input(0);\n    const auto* input_data = input.template data<T>();\n    auto* Y = Output(0);\n\n    CHECK_LT(num_reduce_dims_, input.dims().size());\n    const int M = FIRSTDIMS\n        ? input.size_to_dim(num_reduce_dims_)\n        : input.size_to_dim(input.ndim() - num_reduce_dims_);\n    const int N = FIRSTDIMS\n        ? input.size_from_dim(num_reduce_dims_)\n        : input.size_from_dim(input.ndim() - num_reduce_dims_);"
    },
    {
        "Id": 20,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/e9c1ccee2247a7746fde202067a7d47b72809968",
        "Violation": "improper",
        "Bug report": "Bug fix: allow std 0 in the meta definition of normal_. All other `normal` variants allow 0.  Looks like a mistake made while copying the check. ",
        "Number of deleted lines": 1,
        "Deleted lines": "  return at::native::templates::normal_impl_<NormalStub, Generator>(self, mean, std, gen);\n}\n\nTensor& normal_meta_(Tensor& self, double mean, double std, c10::optional<Generator> gen) {\n  TORCH_CHECK(std > 0.0, \"normal_ expects std > 0.0, but found std=\", std);  // TODO: dedupe\n  return self;\n}\n\nTensor& normal_out(const Tensor& mean, double std, c10::optional<Generator> gen, Tensor& output) {\n  return at::native::templates::normal_out_impl<NormalStub, Generator>(output, mean, std, gen);\n}"
    },
    {
        "Id": 252,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/076f909b70b251daea6c443c9b1929b9745aed20",
        "Violation": "improper",
        "Bug report": "fix boolean expression in length check ",
        "Number of deleted lines": 1,
        "Deleted lines": "    for (int64_t len : lengths) {\n      OP_REQUIRES(ctx, len == length,\n                  errors::Unimplemented(\"All lengths have to be the same\"));\n    }\n    OP_REQUIRES(ctx, length,\n                errors::Unimplemented(\"All lengths must be positive\"));\n    OP_REQUIRES(\n        ctx, element_dims[0] % length == 0,\n        errors::Unimplemented(\"Buffer size has to be a multiple of length\"));\n    std::vector<int64_t> new_dims = {element_dims[0] / length, length};\n    for (int i = 1; i < element_dims.size(); i++) {"
    },
    {
        "Id": 76,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/c1384ef99e7a0d8a439df8972532fe4f155a5683",
        "Violation": "improper",
        "Bug report": "Fix NDPooling gradient non-symmetric padding check. ",
        "Number of deleted lines": 6,
        "Deleted lines": "      cudnn_input_dims_ = X.dims();\n      setTensorDescriptor<T>(X.ndim(), order_, N, C, H, W, D, bottom_desc_);\n      setTensorDescriptor<T>(\n          Y.ndim(), order_, N, C, H_out, W_out, D_out, top_desc_);\n      if (pad_t() != pad_l() || pad_l() != pad_r()) {\n        CAFFE_ENFORCE(\n            legacy_pad_ == LegacyPadding::CAFFE_LEGACY_POOLING,\n            \"Cudnn pooling only supports even padding on both sides, with \"\n            \"the only exception of the caffe legacy pooling case where we \"\n            \"try to preserve backward compatibility with Caffe.\");\n      }\n      if (kernel_.size() == 2) {\n        CUDNN_ENFORCE(cudnnSetPooling2dDescriptor(\n            pooling_desc_,\n            mode_,\n            CUDNN_NOT_PROPAGATE_NAN,"
    },
    {
        "Id": 13,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/611080a118fff166c85f3200d860f3b059abac6f",
        "Violation": "improper",
        "Bug report": "uda 11.0.x doesn't support sm86.",
        "Number of deleted lines": 2,
        "Deleted lines": "  list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"8.0\")\n  list(APPEND CUDA_ALL_GPU_ARCHITECTURES \"8.0\")\n\n  if(CUDA_VERSION VERSION_LESS \"11.1\")\n    set(CUDA_LIMIT_GPU_ARCHITECTURE \"8.6\")\n    list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"8.0+PTX\")\n  endif()\nendif()\n\nif(CUDA_VERSION VERSION_GREATER \"11.0\")\n  list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"8.6\" \"8.6+PTX\")\n  list(APPEND CUDA_ALL_GPU_ARCHITECTURES \"8.6\")\n\n  if(CUDA_VERSION VERSION_LESS \"12.0\")\n    set(CUDA_LIMIT_GPU_ARCHITECTURE \"9.0\")\n  endif()\nendif()\n"
    },
    {
        "Id": 399,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/58759659ee547a957c5d36e72f2274ab34fdb6cb",
        "Violation": "improper",
        "Bug report": "Fix OOB check for result_index in header generation",
        "Number of deleted lines": 1,
        "Deleted lines": "  TF_RETURN_IF_ERROR(ValidateConfig(config));\n  TF_RETURN_IF_ERROR(ValidateFeedFetchCppNames(config));\n  const int64 result_index = compile_result.aot->result_buffer_index();\n  const xla::BufferSizes& temp_sizes = compile_result.aot->buffer_sizes();\n  if (result_index < 0 || result_index > temp_sizes.size()) {\n    return errors::InvalidArgument(\"result index: \", result_index,\n                                   \" is outside the range of temp sizes: [0,\",\n                                   temp_sizes.size(), \")\");\n  }\n"
    },
    {
        "Id": 340,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/be5116dd131a92da298dbb68d26e0d47f66f2fe5",
        "Violation": "improper",
        "Bug report": " Correct graph check in broadcast_to gradient. ",
        "Number of deleted lines": 1,
        "Deleted lines": "def _BroadcastToGrad(op, grad):\n  input_value = op.inputs[0]\n  broadcast_shape = op.inputs[1]\n  input_value_shape = array_ops.shape(input_value)\n  if not context.executing_eagerly():\n    broadcast_shape_static = tensor_shape.TensorShape(\n        pywrap_tf_session.TF_TryEvaluateConstant_wrapper(\n            broadcast_shape.graph._c_graph, broadcast_shape._as_tf_output()))  # pylint: disable=protected-access\n    if broadcast_shape_static.is_fully_defined():\n      broadcast_shape = constant_op.constant(\n          broadcast_shape_static.as_list(), dtype=dtypes.int32)"
    },
    {
        "Id": 37,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/b8ab3080b1043a610ba2825a2be406a1833b1d70",
        "Violation": "improper",
        "Bug report": "If kernel sizes were specified via \"kernel_w\" and \"kernel_h\", tensor size inference was incorrect in InferShapesAndTypes(): it was checking for \"helper_w\" instead of \"kernel_w\".",
        "Number of deleted lines": 1,
        "Deleted lines": "\n    if (helper.HasArgument(\"kernel\")) {\n      kernel.resize(2, helper.GetSingleArgument<int>(\"kernel\", 1));\n    } else if (\n        helper.HasArgument(\"kernel_h\") && helper.HasArgument(\"helper_w\")) {\n      kernel.push_back(helper.GetSingleArgument<int>(\"kernel_h\", 1));\n      kernel.push_back(helper.GetSingleArgument<int>(\"kernel_w\", 1));\n    }\n\n    if (helper.HasArgument(\"stride\")) {\n      strides.resize(2, helper.GetSingleArgument<int>(\"stride\", 1));"
    },
    {
        "Id": 402,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d23458fdd2655c83ff9d54725062ded31b644ba4",
        "Violation": "improper",
        "Bug report": " [XLA:CPU] Do not check that the size of the XLA parameter buffer is exactly equal to the size of the underlying given buffer Instead, check that the underlying allocation is \"large enough\". This is also more consistent with XLA:GPU behavior. The mismatch can happen when the input comes from tf.where, which is backed by an allocation larger than is actually required.",
        "Number of deleted lines": 1,
        "Deleted lines": "  if (allocation.is_entry_computation_parameter()) {\n    se::DeviceMemoryBase out = arguments[allocation.parameter_number()]\n                                   .Buffer(allocation.param_shape_index())\n                                   .AsDeviceMemoryBase();\n    CHECK_EQ(allocation.size(), out.size())\n        << \"Size mismatch on param \" << allocation.parameter_number()\n        << \" at shape index \" << allocation.param_shape_index().ToString();\n    VLOG(3) << \"allocation is a parameter\";\n    return MaybeOwningDeviceMemory{out};\n  } else if (allocation.is_constant()) {\n    VLOG(3) << \"allocation is a constant\";"
    },
    {
        "Id": 121,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/91066559a8c8e5978ed4de722317576b222267c5",
        "Violation": "improper",
        "Bug report": "truthy check for empty string in NameScope(). As in name. LATTE translation team moving some code from Python 2 to 3 uncovered a case where comparison between unicode and str types leads NameScope('') to prepend a separator to the beginning of blob names. This fixes it.",
        "Number of deleted lines": 1,
        "Deleted lines": "    global _threadlocal_scope\n    assert isinstance(prefix, basestring), \\\n        \"NameScope takes in a string as its argument.\"\n    old_scope = CurrentNameScope()\n    prefix = prefix + _NAMESCOPE_SEPARATOR if prefix is not '' else ''\n    if reset:\n        _threadlocal_scope.namescope = prefix\n    else:\n        _threadlocal_scope.namescope = _threadlocal_scope.namescope + prefix\n\n    try:"
    },
    {
        "Id": 425,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a0de0ca6a39f3037e1be6ec740829863bcda3e8",
        "Violation": "improper",
        "Bug report": "[XLA:GPU] Fix type check in IsMatrixMultiplication",
        "Number of deleted lines": 1,
        "Deleted lines": "       output_primitive_type == F16 || output_primitive_type == BF16 ||\n       output_primitive_type == F32 || output_primitive_type == F64 ||\n       output_primitive_type == C64 || output_primitive_type == C128) ||\n      (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&\n       lhs_shape.element_type() == S8);\n  bool shapes_are_valid =\n      type_is_allowed &&\n      IsRank2(lhs_shape, dim_numbers.lhs_batch_dimensions_size()) &&\n      IsRank2(rhs_shape, dim_numbers.lhs_batch_dimensions_size()) &&\n      IsRank2(dot.shape(), dim_numbers.lhs_batch_dimensions_size()) &&\n      !ShapeUtil::IsZeroElementArray(lhs_shape) &&"
    },
    {
        "Id": 401,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2ff14318050e26302785a49a1719d29ddcc91b4",
        "Violation": "improper",
        "Bug report": " [XNNPACK] Fix incorrect check in slice node. begin+size == input dimension is valid, e.g. input size is 3, begin is 2, size is 1.",
        "Number of deleted lines": 2,
        "Deleted lines": "                                 \"dimension %d in SLICE node #%d\",\n                                 size[i], output_shape->data[i], i, node_index);\n        return kTfLiteError;\n      }\n      if (begin[i] + size[i] >= input_shape->data[i]) {\n        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n                                 \"begin + size (%\" PRId64 \" + %\" PRId64\n                                 \") must be less input \"\n                                 \"dimension %d in SLICE node #%d\",\n                                 begin[i], size[i], input_shape->data[i],\n                                 node_index);\n        return kTfLiteError;\n      }\n    }"
    },
    {
        "Id": 225,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bd1f1ac1fec05d38f1b8fc98f650c1c55ac06790",
        "Violation": "improper",
        "Bug report": "Fix operator check ",
        "Number of deleted lines": 1,
        "Deleted lines": "    if m is not None and l is not None:\n      return m == l\n\n  if (operator_a.is_square != operator_b.is_square) and (\n      operator_a.is_square is not None and operator_a.is_square is not None):\n    return False\n\n  return None\n\n"
    },
    {
        "Id": 368,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/48393637f8154be16088d84742485a0e153ecbb2",
        "Violation": "improper",
        "Bug report": "Change check to allow tensors with up to 6 dims.",
        "Number of deleted lines": 2,
        "Deleted lines": "    return;\n  }\n  CHECK(dims_array.data_type == ArrayDataType::kInt32) << \"dims must be int32\";\n  CHECK_LE(RequiredBufferSizeForShape(dims_array.shape()), 4)\n      << \"dims vector can be no larger than 4 values\";\n\n  std::vector<int32> const& dims =\n      dims_array.GetBuffer<ArrayDataType::kInt32>().data;\n  *(output_array.mutable_shape()->mutable_dims()) = dims;\n}\n"
    },
    {
        "Id": 322,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1a1a381b5be7701843c3f1e34aa1846ae2a1d0ce",
        "Violation": "improper",
        "Bug report": "Fix a SIGSEGV bug in InferShapeForXlaGatherOp. Since `ComputeOutputComponent` may return nullptr, we need to check for null attributes explicitly to be safe.",
        "Number of deleted lines": 1,
        "Deleted lines": "      matchPattern(op.getSliceSizes(), m_Constant(&attr))) {\n    slice_sizes_attr = attr;\n  } else if (const auto it = results_.find(ValuePort(op.getSliceSizes()));\n             it != results_.end() &&\n             llvm::isa<DenseIntElementsAttr>(it->second)) {\n    slice_sizes_attr = llvm::cast<DenseIntElementsAttr>(it->second);\n  } else {\n    return false;\n  }\n\n  llvm::SmallVector<int64_t> slice_sizes;"
    },
    {
        "Id": 19,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/9234f5026dbaf09a41b82bb6cf5f10ad4eeb03f2",
        "Violation": "improper",
        "Bug report": "at::cuda::CUDAEvent is \"lazy\" and only creates an event when it's first recorded. Until then, at::cuda::CUDAEvent is empty. If we use at::cuda::CUDAEvent::query() this is taken into account (an empty event is always ready), but WorkNCCL extracts the raw cudaEvent_t value from at::cuda::CUDAEvent and calls cudaEventQuery manually and doesn't check this. This could cause a failure. It's unclear if this is ever supposed to happen, but we're seeing that failure, and we want to sort it out in order to see if there's something \"deeper\" going on.",
        "Number of deleted lines": 5,
        "Deleted lines": "\nbool ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const {\n  for (size_t i = 0; i < devices_.size(); ++i) {\n    auto ret = cudaEventQuery((*cudaEvents_)[i]);\n    if (ret != cudaSuccess && ret != cudaErrorNotReady) {\n      AT_CUDA_CHECK(ret);\n    }\n    if (ret == cudaErrorNotReady) {\n      return false;\n    }\n  }\n  return true;\n}\n"
    },
    {
        "Id": 185,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/5023995292f5119c447de15c20a375b7e3aa2d0b",
        "Violation": "improper",
        "Bug report": " fix output size adjustment for onnxifi_op. Summary: this breaks if we cut the net at certain int8 ops boundary.",
        "Number of deleted lines": 1,
        "Deleted lines": "          \" vs \",\n          real_shape.dims(j),\n          \")\");\n      begin_ptr[j] = 0;\n      if (max_shape[j] > real_shape.dims(j)) {\n        end_ptr[j] = real_shape.dims(j);\n        mismatch += j;\n      } else {\n        end_ptr[j] = -1;\n      }\n    }"
    },
    {
        "Id": 28,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/b2d110447190abe5d66b0b59a775cc4881f3e30e",
        "Violation": "improper",
        "Bug report": "Fixed numpy bool check",
        "Number of deleted lines": 1,
        "Deleted lines": "        self.batch_first = batch_first\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n\n        if self._qkv_same_embed_dim is False:\n            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n            self.register_parameter('in_proj_weight', None)\n        else:\n            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))"
    },
    {
        "Id": 199,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4a343043dbc6ce229b4dcf2258f7b6352db32b64",
        "Violation": "improper",
        "Bug report": "Fix hasattr check in saved_model_cli ",
        "Number of deleted lines": 2,
        "Deleted lines": "\ndef main():\n  parser = create_parser()\n  args = parser.parse_args()\n  if not hasattr(args.func):\n    parser.error(\"too few arguments\")\n  args.func(args)\n\n\nif __name__ == '__main__':\n  sys.exit(main())\n"
    },
    {
        "Id": 160,
        "Library": "pytorch",
        "Commit Link": "https://github.com/pytorch/pytorch/commit/bbb5e106ad6228953df6c7f5c8916b26dc0cb457",
        "Violation": "improper",
        "Bug report": "Improve error checking of CUDALoops. Same change as was applied to CPU loops -- separate out checking of the inputs and outputs.",
        "Number of deleted lines": 1,
        "Deleted lines": "  using arg0_t = typename traits::result_type;\n  constexpr int ntensors = traits::arity + 1;\n\n  TORCH_INTERNAL_ASSERT(iter.can_use_32bit_indexing());\n  TORCH_INTERNAL_ASSERT(iter.ntensors() == traits::arity + 1);\n\n  at::detail::Array<char*, ntensors> data;\n  for (int i = 0; i < ntensors; i++) {\n    data[i] = (char*)iter.data_ptr(i);\n  }\n"
    },
    {
        "Id": 415,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/25821f0d91623d654bb1bdd62423e644bae9f7f8",
        "Violation": "improper",
        "Bug report": "TensorFlow: Fix OP_REQUIRES check for depthwise pooling.",
        "Number of deleted lines": 2,
        "Deleted lines": "\n    if (params.depth_window > 1) {\n      OP_REQUIRES(context, params.out_depth % params.depth_window > 0,\n                  errors::Unimplemented(\n                      \"Depthwise max pooling requires \"\n                      \"the depth window to evenly divide the input depth.\"));\n      OP_REQUIRES(\n          context, params.out_depth == params.depth_stride,\n          errors::Unimplemented(\"Depthwise max pooling requires \"\n                                \"the depth window to equal the depth stride.\"));\n\n      DepthwiseMaxPool(context, output, tensor_in, params);\n    } else {\n      SpatialMaxPool(context, output, tensor_in, params, padding_);"
    },
    {
        "Id": 317,
        "Library": "tensorflow",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3a7b36bca7f43ce4f0d0791ce0e0d84ece8683d9",
        "Violation": "improper",
        "Bug report": " [Grappler] Remove DCHECK from a MutableGraphView CanDedupControlWithRegularInput check.",
        "Number of deleted lines": 2,
        "Deleted lines": "bool CanDedupControlWithRegularInput(const MutableGraphView& graph,\n                                     absl::string_view control_node_name) {\n  NodeDef* control_node = graph.GetNode(control_node_name);\n  DCHECK(control_node != nullptr)\n      << \"Didn't find a node for control dependency: \" << control_node_name;\n  return CanDedupControlWithRegularInput(graph, *control_node);\n}\n\nbool HasRegularFaninNode(const MutableGraphView& graph, const NodeDef& node,\n                         absl::string_view fanin_node_name) {\n  const int num_regular_fanins ="
    }
]