[{
    "Id": 162,
    "Library": "tensorflow",
    "Date": "2023/01/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8405f33e590e1639927dcf0658724b03a5ee6ddc",
    "Root Cause": "N.A",
    "Bug report": "Exports solver model for debugging.\n\nPiperOrigin-RevId: 500847965",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include \"file/base/helpers.h\"\n+#ifdef PLATFORM_GOOGLE\n+  // Exports the model for debugging.\n+  bool dump_model = false;\n+  if (dump_model) {\n+    operations_research::MPModelProto model_proto;\n+    solver->ExportModelToProto(&model_proto);\n+    auto write_status = file::SetTextProto(\n+        // Modify this file path if needed.\n+        absl::StrCat(\"/tmp/model_\", solver->NumVariables(), \".proto\"),\n+        model_proto, file::Defaults());\n+    if (!write_status.ok()) {\n+      LOG(ERROR) << write_status.message();\n+    }\n+  }\n+#endif",
    "Label": "clean"
},
{
    "Id": 197,
    "Library": "tensorflow",
    "Date": "2022/09/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/471101427983a1e591e2661a873eb20790b93199",
    "Root Cause": "N.A",
    "Bug report": "Apply clang-tidy fixes for bugprone-argument-comment in hlo_ops.cc (NFC)\n\nPiperOrigin-RevId: 474710269",
    "Number of deleted lines": 4,
    "Deleted lines": "-  if (failed(mlir::hlo::verifyReplicaGroups(*this, /*is_uniform_sized=*/true)))\n-      /*operand_types=*/{operand().getType()},\n-      /*result_types=*/{getType()},\n-      /*scatter_dimension=*/scatter_dimension());",
    "Added lines": "+  if (failed(mlir::hlo::verifyReplicaGroups(*this, /*isUniformSized=*/true)))\n+      /*operandTypes=*/{operand().getType()},\n+      /*resultTypes=*/{getType()},\n+      /*scatterDimension=*/scatter_dimension());",
    "Label": "clean"
},
{
    "Id": 212,
    "Library": "tensorflow",
    "Date": "2022/07/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/67352edf6c6a99c6e3d16bd10a55828428ad8f04",
    "Root Cause": "N.A",
    "Bug report": "Add logging when keeping data on host in SameWorkerRecvDone\n\nSome data is always kept on host (CPU) by SameWorkerRecvDone in core/common_runtime/rendezvous_mgr.cc even if there is \"fake\" placement information due to the src or dst op for the data being on device (GPU, etc.). Add debug logging when this case occurs.\n\nPiperOrigin-RevId: 462469508",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (VLOG_IS_ON(3)) {\n+      bool src_override =\n+          send_args.alloc_attrs.on_host() && !(parsed.src.type == \"CPU\");\n+      bool dst_override =\n+          recv_args.alloc_attrs.on_host() && !(parsed.dst.type == \"CPU\");\n+      if (src_override || dst_override) {\n+        VLOG(3) << \"Shortcut to keep tensor on host (src_override \"\n+                << src_override << \" and dst_override \" << dst_override\n+                << \") tensor dtype:\" << DataTypeString(in.dtype()) << \" \"\n+                << parsed.FullKey();\n+      }\n+    }",
    "Label": "clean"
},
{
    "Id": 303,
    "Library": "tensorflow",
    "Date": "2021/10/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b7c807287d23254e04da48033cabd63e7380628f",
    "Root Cause": "N.A",
    "Bug report": "[XLIR] Dump TFRT MLIR text if debug options say so.\n\nPiperOrigin-RevId: 404645361\nChange-Id: Ic5608d7b2588dec121446b9d205fbe8a940e4636",
    "Number of deleted lines": 3,
    "Deleted lines": "-                                           std::string entry_function_name) {\n-  TF_ASSIGN_OR_RETURN(results->thunks_or_bef,\n-                      LowerToBef(*mlir_module, entry_function.getName().str()));",
    "Added lines": "+                                           std::string entry_function_name,\n+                                           HloModule* hlo_module) {\n+  if (DumpingEnabledForHloModule(*hlo_module)) {\n+    std::string tfrt_mlir;\n+    llvm::raw_string_ostream tfrt_mlir_ostream(tfrt_mlir);\n+    mlir_module.print(tfrt_mlir_ostream);\n+    DumpToFileInDirOrStdout(*hlo_module, \"\", \"tfrt_mlir\", tfrt_mlir);\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(\n+      results->thunks_or_bef,\n+      LowerToBef(*mlir_module, entry_function.getName().str(), hlo_module));",
    "Label": "clean"
},{
    "Id": 781,
    "Library": "pytorch",
    "Date": "2024/04/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/601112fdb4107d0c0c4d9a3766789d5075b9af10",
    "Root Cause": "N.A",
    "Bug report": "[dynamo][log] Print missing skipped frame info on debug (#124078)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124078\nApproved by: https://github.com/yanboliang",
    "Number of deleted lines": 7,
    "Deleted lines": "-                if not is_skipfile or config.verbose:\n-                    log.debug(\n-                        \"skipping: %s (reason: %s, file: %s)\",\n-                        frame.f_code.co_name,\n-                        skip_reason,\n-                        frame.f_code.co_filename,\n-                    )",
    "Added lines": "+                log.debug(\n+                    \"skipping: %s (reason: %s, file: %s)\",\n+                    frame.f_code.co_name,\n+                    skip_reason,\n+                    frame.f_code.co_filename,\n+                )",
    "Label": "clean"
},{
    "Id": 782,
    "Library": "pytorch",
    "Date": "2024/04/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e00282fecfcb53790aebfb24cc48a8703577778e",
    "Root Cause": "N.A",
    "Bug report": "[c10d] make monitorThread sleep when we try to dump (#123788)\n\nSummary:\nWe seperated the FR dump logic from the desync debug logic,\nso we no longer set collectiveDebugInfoMode_ to true when we just need FR\ndump. That's why monitor thread did not sleep and try to kill the\nprocess without waiting for the dump.\n\nThe fix is simple, we should sleep whenever shouldDump_ is true\nTest Plan:\nExisting unit tests\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123788\nApproved by: https://github.com/wconstab",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if ((terminateProcessGroup_.load() || collectiveDebugInfoMode_.load()) &&",
    "Added lines": "+          shouldDump_.store(true);\n+        shouldDump_.store(true);\n+  if ((terminateProcessGroup_.load() || collectiveDebugInfoMode_.load() ||\n+       shouldDump_.load()) &&",
    "Label": "clean"
},{
    "Id": 784,
    "Library": "pytorch",
    "Date": "2024/04/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f688d7a2f7dcb6caf5ded0e1eabedd5a7a6dc43b",
    "Root Cause": "N.A",
    "Bug report": "Only suggest debug envvar when debug is on (#123647)\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123647\nApproved by: https://github.com/Chillee",
    "Number of deleted lines": 1,
    "Deleted lines": "-        else:",
    "Added lines": "+        elif is_debug:",
    "Label": "clean"
},{
    "Id": 792,
    "Library": "pytorch",
    "Date": "2024/03/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/235f24fc66078538249f978c275dc07c9e79d07e",
    "Root Cause": "N.A",
    "Bug report": "[inductor] Add FileLock around V.debug.copy (#122665)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122665\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 5,
    "Deleted lines": "-        if os.path.exists(new_path):\n-            shutil.rmtree(new_path)\n-            shutil.copytree(self._path, new_path)\n-            self._path = new_path\n-            pass",
    "Added lines": "+        from filelock import FileLock\n+\n+            with FileLock(f\"{new_path}.lock\"):\n+                if os.path.exists(new_path):\n+                    shutil.rmtree(new_path)\n+                shutil.copytree(self._path, new_path)",
    "Label": "clean"
},{
    "Id": 889,
    "Library": "pytorch",
    "Date": "2023/07/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ed5ea15714361db57aff7f8b6dbf6f9d6cb3e95c",
    "Root Cause": "N.A",
    "Bug report": "[Easy] remove debug code (#104915)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104915\nApproved by: https://github.com/mlazos",
    "Number of deleted lines": 2,
    "Deleted lines": "-            if not len(node.tensor_weakrefs) == len(node.stack_traces):\n-                breakpoint()",
    "Added lines": "",
    "Label": "clean"
},{
    "Id": 1053,
    "Library": "pytorch",
    "Date": "2022/01/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/479ce1c3a0dad2c3d182fc0b3f26e647b99c1253",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch] Add isUndefined to ExclusivelyOwnedTraits<TensorBase> debug msg (#70638)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70638\n\nWe are seeing these assertions fire infrequently. Add more information to aid in debugging when they fire.\nghstack-source-id: 146819527\n\nTest Plan: CI\n\nReviewed By: bdhirsh\n\nDifferential Revision: D33412651\n\nfbshipit-source-id: 7e35faf9f4eeaa5f2455a4392e00f62fe692811c",
    "Number of deleted lines": 4,
    "Deleted lines": "-        toDestroy->refcount_ == 1 || (toDestroy->refcount_ == 0 && toDestroy == UndefinedTensorImpl::singleton()),\n-        \"ExclusivelyOwned<Tensor> destroyed with refcount \", toDestroy->refcount_, \", expected 1!\");\n-        \"ExclusivelyOwned<Tensor> destroyed with weakcount \", toDestroy->weakcount_, \", expected 1!\");\n-    if (toDestroy != UndefinedTensorImpl::singleton()) {",
    "Added lines": "+    const bool isUndefined = toDestroy == UndefinedTensorImpl::singleton();\n+        toDestroy->refcount_ == 1 || (toDestroy->refcount_ == 0 && isUndefined),\n+        \"ExclusivelyOwned<Tensor> destroyed with isUndefined \", isUndefined, \" and refcount \", toDestroy->refcount_, \", expected 1 or, if isUndefined, 0!\");\n+        \"ExclusivelyOwned<Tensor> destroyed with isUndefined \", isUndefined, \" and weakcount \", toDestroy->weakcount_, \", expected 1 or, if isUndefined, 0!\");\n+    if (!isUndefined) {",
    "Label": "clean"
},{
    "Id": 1258,
    "Library": "pytorch",
    "Date": "2019/11/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8e3486de81d848e5c9a375134b3b14998ac36654",
    "Root Cause": "N.A",
    "Bug report": "No debug symbols in release android buidls (#30123)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30123\n\nIn groovy string `'false'` is resolved as boolean `true`\n\nthats why even as in `gradle.properties`:\n```\nnativeLibsDoNotStrip=false\n```\nbranch `if (nativeLibsDoNotStrip)` always passed\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18606907\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: c10140e775624294c732e78ae3c41e05c7c9ad92",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if (nativeLibsDoNotStrip) {",
    "Added lines": "+        if (nativeLibsDoNotStrip.toBoolean()) {\n+            logger.warn('WARNING: nativeLibsDoNotStrip==true; debug symbols included')",
    "Label": "clean"
},{
    "Id": 1295,
    "Library": "pytorch",
    "Date": "2019/03/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bd1271338ada8eda28a72e028a5521480d118bfb",
    "Root Cause": "N.A",
    "Bug report": "Add python_variable._is_view for debugging. (#18197)\n\nSummary:\nI don't know if we actually want to expose this or not, but it's useful for debugging.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18197\n\nReviewed By: ezyang\n\nDifferential Revision: D14530712\n\nPulled By: gchanan\n\nfbshipit-source-id: 98fdba9cf113738f0db3a198c49365de536b9919",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+static PyObject * THPVariable__is_view(PyObject *self, PyObject* args)\n+{\n+  HANDLE_TH_ERRORS\n+  auto& self_ = reinterpret_cast<THPVariable*>(self)->cdata;\n+  if (self_.is_view()) {\n+    Py_RETURN_TRUE;\n+  } else {\n+    Py_RETURN_FALSE;\n+  }\n+  END_HANDLE_TH_ERRORS\n+}\n+\n+  {\"_is_view\", (PyCFunction)THPVariable__is_view, METH_NOARGS, NULL},",
    "Label": "clean"
},{
    "Id": 1311,
    "Library": "pytorch",
    "Date": "2018/12/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cbd1c519c46186eb7c70590ba02f126297ee251f",
    "Root Cause": "N.A",
    "Bug report": "Replace non-printable-ascii characters in ProtoDebugString (#14918)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/14918\n\nWhen ProtoBuf-Lite is in use, ProtoDebugString just calls SerializeAsString.\nThis produces binary output, which is not a very suitable \"debug\" string.\nSpecifically, we've observed it causing problems when calling code tries to\nadd the debug string to a Java exception message (which requires valid UTF-8).\nNow, we replace all non-ASCII bytes with \"?\".\n\nThis is not a very fast implementation, but generating debug strings shouldn't\nbe a performance-sensitive operation in any application.\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D13385540\n\nfbshipit-source-id: 8868172baf20efaf53fecf7d666a6980f59b64f5",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return proto.SerializeAsString();",
    "Added lines": "+  string serialized = proto.SerializeAsString();\n+  for (char& c : serialized) {\n+    if (c < 0x20 || c >= 0x7f) {\n+      c = '?';\n+    }\n+  }\n+  return serialized;",
    "Label": "clean"
},{
    "Id": 1330,
    "Library": "pytorch",
    "Date": "2018/08/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/16ecd6f99c468027e4a08feb273e4fb31bee50bb",
    "Root Cause": "N.A",
    "Bug report": "Fix Debug Build On Windows (#10359)\n\nSummary:\ncompile files in torch/csrc with /MDd runtime library option for debug build on Windows\nPull Request resolved: https://github.com/pytorch/pytorch/pull/10359\n\nDifferential Revision: D9316946\n\nPulled By: SsnL\n\nfbshipit-source-id: c84bfad81d61cd49f39b7bce7177edd2b1e8bd69",
    "Number of deleted lines": 1,
    "Deleted lines": "-    /MD",
    "Added lines": "+  if (${CMAKE_BUILD_TYPE} MATCHES \"Debug\")\n+    set (MSVC_RUNTIME_LIBRARY_FLAG \"/MDd\")\n+  else()\n+    set (MSVC_RUNTIME_LIBRARY_FLAG \"/MD\")\n+  endif()\n+  \n+    ${MSVC_RUNTIME_LIBRARY_OPTION}",
    "Label": "clean"
},{
    "Id": 41,
    "Library": "tensorflow",
    "Date": "2023/12/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84002d3519423aa5edca41e47761e5702173d7cf",
    "Root Cause": "N.A",
    "Bug report": "PR #7344: [XLA:GPU] Error out for ptxas 12.3.1 version\n\nImported from GitHub PR https://github.com/openxla/xla/pull/7344\n\nptxas 12.3.1 has a bug that we think can affect XLA. Error out for this version.\nCopybara import of the project:\n\n--\nf210fe2f386f870eba4d3ea5c786f897300b4821 by Ayan Moitra <amoitra@nvidia.com>:\n\nptxas 12.3.1 has a bug that we think can affect XLA. Error out for this version\n\n--\n9058e1607953eec3b3e7d5a3431665bedbe53866 by Ayan Moitra <amoitra@nvidia.com>:\n\nfix\n\n--\n7881b7711d9f4f6a1bb1b2c960125bebb634a80c by Ayan Moitra <amoitra@nvidia.com>:\n\nnit fix\n\nMerging this change closes #7344\n\nPiperOrigin-RevId: 588355928",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  auto ptxas_version_tuple = GetAsmCompilerVersion(options.preferred_cuda_dir);\n+  if (ptxas_version_tuple.value() == std::array<int64_t, 3>{12, 3, 1}) {\n+    return tsl::errors::Internal(\n+        absl::StrFormat(\"ptxas 12.3.1 has a bug that we think can affect XLA. \"\n+                        \"Please use a different version.\"));\n+  }",
    "Label": "clean"
},{
    "Id": 236,
    "Library": "pytorch",
    "Date": "2023/07/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0faf8ed49f9da63d1b5339f152586ffd7cde3e83",
    "Root Cause": "N.A",
    "Bug report": "Skip TS backend in FBCODE (#104354)\n\nSummary:\nFixes:\n```\nTraceback (most recent call last):\n  File \"/data/sandcastle/boxes/fbsource/buck-out/v2/gen/fbcode/1a4194a16794cc72/caffe2/test/__torch__/torch#link-tree/torch/testing/_internal/common_device_type.py\", line 543, in setUpClass\n    torch._lazy.ts_backend.init()\n  File \"/data/sandcastle/boxes/fbsource/buck-out/v2/gen/fbcode/1a4194a16794cc72/caffe2/test/__torch__/torch#link-tree/torch/_lazy/ts_backend.py\", line 6, in init\n    torch._C._lazy_ts_backend._init()\nRuntimeError: TorchScript backend not yet supported in FBCODE/OVRSOURCE builds\n```\n\nTest Plan: Sandcastle\n\nDifferential Revision: D47093028\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104354\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-        desired_device_type_test_bases.append(LazyTestBase)",
    "Added lines": "+import sys\n+        if IS_FBCODE:\n+            print(\"TorchScript backend not yet supported in FBCODE/OVRSOURCE builds\", file=sys.stderr)\n+        else:\n+            desired_device_type_test_bases.append(LazyTestBase)",
    "Label": "Clean"
},{
    "Id": 201,
    "Library": "pytorch",
    "Date": "2023/05/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7c2641d5f1081811e664267406df1059687aad7a",
    "Root Cause": "N.A",
    "Bug report": "apply constexpr and if constexpr when possible (#102471)\n\nNow that we have full C++17 support, we can use if constexpr in some identified cases.\n\n<!--\ncopilot:summary\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at df4c16d</samp>\n\nThe pull request improves the performance, readability, and consistency of various function templates in the `ATen` and `torch` modules by using `constexpr` keywords and C++17 features. It also fixes some type conversion and overflow issues for different input and output types. The changes affect the code for distributions, BLAS, batch normalization, embedding bag, random number generation, vectorized operations, cuBLAS, XNNPACK, CUTLASS, and shape inference. The affected files include `DistributionsHelper.h`, `vec256_int.h`, `vec512_int.h`, `BlasKernel.cpp`, `IndexKernel.cpp`, `EmbeddingBag.cpp`, `Normalization.cpp`, `rng_test.h`, `vec_test_all_types.h`, `TransformationHelper.h`, `CUDABlas.cpp`, `DistributionKernels.cpp`, `DistributionTemplates.h`, `RangeFactories.cu`, `RangeFactories.cpp`, `qconv.cpp`, `StructuredSparseLinearCUTLASS.cu`, `vec_test_all_types.cpp`, and `shape_inference.cpp`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/102471\nApproved by: https://github.com/Skylion007, https://github.com/malfet",
    "Number of deleted lines": 4,
    "Deleted lines": "-    if (std::is_same<T, double>::value || std::is_same<T, int64_t>::value) {\n-    if(std::is_same<T, double>::value) {\n-    if (std::is_same<T, double>::value) {\n-    if (std::is_same<T, double>::value) {",
    "Added lines": "+    if constexpr (std::is_same_v<T, double> || std::is_same_v<T, int64_t>) {\n+    if constexpr (std::is_same_v<T, double>) {\n+    if constexpr (std::is_same_v<T, double>) {\n+    if constexpr (std::is_same_v<T, double>) {",
    "Label": "Clean"
},{
    "Id": 167,
    "Library": "pytorch",
    "Date": "2023/04/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/013c7f5ba4f56b2ac5e47e05bb27dada273142ab",
    "Root Cause": "N.A",
    "Bug report": "[inductor] Move `tl.broadcast` call out codegen.common (#98304)\n\nThis makes only a cosmetic change to the generated code, but means\ntriton's broadcasting logic doesn't leak out into the CSE class.\n\nBefore:\n```python\n    tmp5_load = tl.load(in_ptr1 + (0))\n    tmp5 = tl.broadcast_to(tmp5_load, [XBLOCK])\n```\n\nAfter:\n```python\n    tmp5 = tl.load(in_ptr1 + (0))\n    tmp6 = tl.broadcast_to(tmp5, [XBLOCK])\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98304\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 15,
    "Deleted lines": "-        append_broadcast=None,\n-        if append_broadcast:\n-            assert isinstance(append_broadcast, str)\n-            cache_key = expr + append_broadcast\n-                if append_broadcast:\n-                    var_suffix = \"_load\"\n-                else:\n-                    var_suffix = \"\"\n-                buffer.writeline(\n-                    f\"{self.prefix}{var}{var_suffix} = {expr}{self.suffix}\"\n-                )\n-                if append_broadcast:\n-                    buffer.writeline(\n-                        f\"{self.prefix}{var} = tl.broadcast_to({var}{var_suffix}, {append_broadcast})\"\n-                    )",
    "Added lines": "+                buffer.writeline(f\"{self.prefix}{var} = {expr}{self.suffix}\")",
    "Label": "Clean"
},{
    "Id": 100,
    "Library": "pytorch",
    "Date": "2022/12/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e1674d7dc0fe1ac1ab602f3e28f21513f8652803",
    "Root Cause": "N.A",
    "Bug report": "avoid fork in torch/__init__.py for deploy/multipy (#90492)\n\nSummary:\nWe should not fork in deploy when initializing torch.\n\n    Traceback (most recent call last):\n    File \"<string>\", line 38, in <module>\n    File \"<string>\", line 36, in __run\n    File \"/usr/local/fbcode/platform010/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n        return _run_code(code, main_globals, None,\n    File \"/usr/local/fbcode/platform010/lib/python3.8/runpy.py\", line 87, in _run_code\n        exec(code, run_globals)\n    File \"/data/users/zyan/fbsource/buck-out/v2/gen/fbcode/104a4d5c3a690252/multipy/runtime/__test_py__/test_py#link-tree/multipy/runtime/test_py.py\", line 61, in <module>\n        import torch # has to be done serially otherwise things will segfault\n    File \"/data/users/zyan/fbsource/buck-out/v2/gen/fbcode/104a4d5c3a690252/multipy/runtime/__test_py__/test_py#link-tree/torch/__init__.py\", line 158, in <module>\n        platform.system() != 'Windows':\n    File \"/usr/local/fbcode/platform010/lib/python3.8/platform.py\", line 891, in system\n        return uname().system\n    File \"/usr/local/fbcode/platform010/lib/python3.8/platform.py\", line 857, in uname\n        processor = _syscmd_uname('-p', '')\n    File \"/usr/local/fbcode/platform010/lib/python3.8/platform.py\", line 613, in _syscmd_uname\n        output = subprocess.check_output(('uname', option),\n\nTest Plan: override a local script run trigger init and set `subprocess.check_output` to None\n\nReviewed By: yinghai, houseroad\n\nDifferential Revision: D41848592\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90492\nApproved by: https://github.com/PaliC",
    "Number of deleted lines": 3,
    "Deleted lines": "-    if platform.system() == 'Windows' or sys.executable == 'torch_deploy':\n-        platform.system() != 'Windows':\n-    if platform.system() == 'Windows' or sys.executable == 'torch_deploy':",
    "Added lines": "+    if sys.executable == 'torch_deploy' or platform.system() == 'Windows':\n+        (sys.executable == \"torch_deploy\" or platform.system() != 'Windows'):\n+    if sys.executable == 'torch_deploy' or platform.system() == 'Windows':",
    "Label": "Clean"
},{
    "Id": 88,
    "Library": "pytorch",
    "Date": "2022/11/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/af448e84eb2978062dc6ca4d3d538ed46b58f3d6",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in dynamo dashboard summary stats diff (#89226)\n\nFixes issue where a suite may not be present in one of the logs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89226\nApproved by: https://github.com/anijain2305",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                    if suite + \"_prev\" not in row or suite + \"_cur\" not in row:\n+                        continue",
    "Label": "Clean"
},{
    "Id": 36,
    "Library": "pytorch",
    "Date": "2022/08/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c26b53f6a4c05a280aabe525a5c5918e3db3da57",
    "Root Cause": "N.A",
    "Bug report": "[Profiler] Encapsulate callback handle management. (#83892)\n\nRight now the profiler is capible of leaking callback handles if a client does not call `at::removeCallback`. (As well as a double free if two clients handle it.) This modestly improves the situation by pulling removal into a single method and calling that removal code in the dtor unless explicitly opted out. Once we deprecate the legacy profiler we can further simplify by making the ProfilerThreadLocalStateBase own the handle outright.\n\nDifferential Revision: [D38920537](https://our.internmc.facebook.com/intern/diff/D38920537/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/83892\nApproved by: https://github.com/slgong-fb",
    "Number of deleted lines": 3,
    "Deleted lines": "-  if (state_ptr->hasCallbackHandle()) {\n-    at::removeCallback(state_ptr->callbackHandle());\n-  }",
    "Added lines": "+  state_ptr->removeCallback();",
    "Label": "Clean"
},{
    "Id": 33,
    "Library": "pytorch",
    "Date": "2022/08/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7fe19c03e482be1d108fbfca8fb0214e133970ad",
    "Root Cause": "N.A",
    "Bug report": "fix functionalization <> fake tensor mode (#83701)\n\nThe bug is that:\n\n(1) functionalization kernels internally call `at::empty_strided()` to construct meta tensors, and then call the meta tensor op\n(2) This happens with the Python dispatch key already added to the TLS exclude set, so we expect these meta tensors never to enter python\n(3) When calling detach() though, `TensorImpl::shallow_copy_and_detach()` will currently always call into python when a PythonMode is set. Instead, I updated it to check if the Python key is in the TLS exclude set first.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/83701\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (maybe_torch_dispatch_mode_state) {",
    "Added lines": "+  if (maybe_torch_dispatch_mode_state &&\n+      !c10::impl::tls_is_dispatch_key_excluded(DispatchKey::Python)) {",
    "Label": "Clean"
},{
    "Id": 11,
    "Library": "pytorch",
    "Date": "2022/05/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ff9558a2ea626b1526ded6848e06e1e809fae53c",
    "Root Cause": "N.A",
    "Bug report": "[functorch] Update neural_tangent_kernels.ipynb (pytorch/functorch#788)\n\nFix a small bug\r\n```python3\r\n    if compute == 'full':\r\n        return result\r\n    if compute == 'trace':\r\n        return torch.einsum('NMKK->NM')        # should be torch.einsum('NMKK->NM', result)\r\n    if compute == 'diagonal':\r\n        return torch.einsum('NMKK->NMK')        # should be torch.einsum('NMKK->NMK', result)\r\n```",
    "Number of deleted lines": 2,
    "Deleted lines": "-    \"        return torch.einsum('NMKK->NM')\\n\",\n-    \"        return torch.einsum('NMKK->NMK')\"",
    "Added lines": "+    \"        return torch.einsum('NMKK->NM', result)\\n\",\n+    \"        return torch.einsum('NMKK->NMK', result)\"",
    "Label": "Clean"
},{
    "Id": 8,
    "Library": "pytorch",
    "Date": "2022/02/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9b5d63ea76df5b135cfae866a35b34d81ae506ac",
    "Root Cause": "N.A",
    "Bug report": "[functorch] Fix vmap level skipping bug (pytorch/functorch#535)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+  if (!areAnyBatchedAtLevel({grad_output_, input_, weight_}, cur_level)){\n+    c10::impl::ExcludeDispatchKeyGuard guard(kBatchedKey);\n+    return at::convolution_backward(\n+        grad_output_, input_, weight_, bias_sizes_opt, stride, padding,\n+        dilation, transposed, output_padding, groups, output_mask);\n+  }\n+",
    "Label": "Clean"
},{
    "Id": 209,
    "Library": "pytorch",
    "Date": "2023/06/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2e21cb095a04dccb1900367623b4ca59ddcf26a2",
    "Root Cause": "N.A",
    "Bug report": "Remove capture_scalar_outputs sanity check prepping for dynamic by default (#103292)\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103292\nApproved by: https://github.com/voznesenskym",
    "Number of deleted lines": 8,
    "Deleted lines": "-        elif (\n-            name == \"item\"\n-            and config.capture_scalar_outputs\n-            and not config.dynamic_shapes\n-        ):\n-            raise AssertionError(\n-                \"To capture_scalar_outputs, you must also set dynamic_shapes = True\"\n-            )",
    "Added lines": "",
    "Label": "Clean"
},{
    "Id": 118,
    "Library": "pytorch",
    "Date": "2023/01/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc6916b341904cd156335b13d06a576462c452a6",
    "Root Cause": "N.A",
    "Bug report": "optimize gather performance for gnn usage on CPU (#87586)\n\nOn classic pyg user case for message passing, `gather` has `index` tensor in a broadcasted shape, e.g. with shape `5000, 128` and stride `[1, 0]`. That indicated gather is done on each row of the self tensor. The current implementation will try to parallel on the inner dimension which is bad performance for CPU and unable to be vectorized.\n\nThis PR addressed this use case and optimize in a similar manner to index_select, parallel on outer dimension of `index` and do vectorized copy on inner dimension.\n\nPerformance benchmarking on Xeon Icelake single socket on `GCN`: the `gather` reduced from `150.787ms` to `10.926ms`, after this optimization, `gather` will no longer be the major bottleneck for training of GNN models when `EdgeIndex` is in COO format.\n\nfor more details, please refer to https://github.com/pyg-team/pytorch_geometric/issues/4891#issuecomment-1288423705\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87586\nApproved by: https://github.com/rusty1s, https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-  gather_stub(result.device().type(), result, self, dim, index);",
    "Added lines": "+DEFINE_DISPATCH(gather_expanded_index_stub);\n+  if (can_use_expanded_index_path(result, dim, index, self, /*is_scatter_like=*/false)) {\n+    gather_expanded_index_stub(result.device().type(), result, self, index);\n+  } else {\n+    gather_stub(result.device().type(), result, self, dim, index);\n+  }",
    "Label": "Clean"
},{
    "Id": 320,
    "Library": "pytorch",
    "Date": "2023/11/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c1c4882367c592d49e15268a0b99631c207d662e",
    "Root Cause": "N.A",
    "Bug report": "[aps] Sync thrift (#113810)\n\nSummary:\nBased on discussions with Sherlock + Zhengxu in D51118067, updated the internal thrift schema to match the OSS schema.\n\nVerifier failures:\n* Test contains a None as input, resulting in no meta[\"val\"]\n* Test contains torch.autograd.grad_mode.set_grad_enabled as an op, which also results in no meta[\"val\"]\n* torch.autograd.grad_mode.set_grad_enabled is also not a valid op\n* Test adds a \"parameter\" to the state dict but the parameter is not an nn.Parameter, causing an assertion failure\n\nSo to bypass these failures I did the following hacks(?):\n* Before creating the exported program in deserialization, populate nodes w/o meta[\"val\"] with meta[\"val\"] = None\n* Add torch.autograd.grad_mode.set_grad_enabled to the skip opset\n* Duplicated ExportGraphSignature into aot_export.py so that the graph signature checks will be skipped\n\nConfigerator changes in D51343615\n\nTest Plan: CI\n\nReviewed By: zhxchen17\n\nDifferential Revision: D51342921\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113810\nApproved by: https://github.com/zhxchen17",
    "Number of deleted lines": 1,
    "Deleted lines": "-                if op not in _allowed_builtin_ops():",
    "Added lines": "+            # TODO Remove this allowlist.\n+            _allowed_torch_functions = (torch.autograd.grad_mode.set_grad_enabled,)\n+\n+                if op not in _allowed_builtin_ops() and op not in _allowed_torch_functions:\n+                        f\"Valid torch functions: {_allowed_torch_functions}\"",
    "Label": "Clean"
},{
    "Id": 30,
    "Library": "pytorch",
    "Date": "2022/08/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6266003d71e85beabef52da54ccf2ae70c11491d",
    "Root Cause": "N.A",
    "Bug report": "Revert \"Check if IMPORT_DISABLED_TESTS is set (#83436)\"\n\nThis reverts commit 1187dedd336e4f6c0028e0d081b676c2f5796316.\n\nReverted https://github.com/pytorch/pytorch/pull/83436 on behalf of https://github.com/huydhn due to The previous change breaks internal builds D38714900 and other OSS tests. The bug has been fixed by this PR. But we decide that it is safer to revert both, merge them into one PR, then reland the fix",
    "Number of deleted lines": 2,
    "Deleted lines": "-        elif IMPORT_DISABLED_TESTS and os.path.exists(IMPORT_DISABLED_TESTS):\n-            # IMPORT_DISABLED_TESTS can be None here",
    "Added lines": "+        elif os.path.exists(IMPORT_DISABLED_TESTS):",
    "Label": "Clean"
},{
    "Id": 324,
    "Library": "tensorflow",
    "Date": "2021/03/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c1ce3541fcd31141a2372c9ad285e5912781e3d9",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Minor fix for debug mode.\n\nPiperOrigin-RevId: 363807230\nChange-Id: I0262ead723e1add585c8aa91885fd335b61cb3a5",
    "Number of deleted lines": 1,
    "Deleted lines": "-      if DEBUG_MODE or def_function.functions_run_eagerly():",
    "Added lines": "+      if DEBUG_MODE:\n+        if def_function.functions_run_eagerly():\n+          warnings.warn(\n+              \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n+              \"option is set, this option does not apply to tf.data functions. \"\n+              \"To force eager execution of tf.data functions, please use \"\n+              \"`tf.data.experimental.enable.debug_mode()`.\")",
    "Label": "clean"
},{
    "Id": 163,
    "Library": "tensorflow",
    "Date": "2022/09/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/345d064279aeaba924d961322a129555d850bb8b",
    "Root Cause": "N.A",
    "Bug report": "Enable debug info in pretty form for LMHLO so that it is easier to see the corresponding HLO node for each instruction.\n\nPiperOrigin-RevId: 476144750",
    "Number of deleted lines": 1,
    "Deleted lines": "-  op->print(outputFile->os(), mlir::OpPrintingFlags().useLocalScope());",
    "Added lines": "+#include \"mlir/IR/OperationSupport.h\"  // from @llvm-project\n+  mlir::OpPrintingFlags print_flags = mlir::OpPrintingFlags().useLocalScope();\n+  // Enable debug info so that it is easier to see the corresponding HLO node.\n+  if (file_prefix == \"lmhlo\") {\n+    print_flags.enableDebugInfo(/*prettyForm=*/true);\n+  }\n+  op->print(outputFile->os(), print_flags);",
    "Label": "clean"
},
{
    "Id": 99,
    "Library": "tensorflow",
    "Date": "2023/04/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4e35e4de4d75e15cf420f601beb827337d434cf3",
    "Root Cause": "N.A",
    "Bug report": "Add emit-debug info flag to odml_to_stablehlo to preserve MLIR location info\n\nPiperOrigin-RevId: 523773467",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+// NOLINTNEXTLINE\n+opt<bool> debug_info(\n+    \"debug-info\", llvm::cl::desc(\"Inclide MLIR debug location info in output.\"),\n+    llvm::cl::Optional, llvm::cl::init(false));\n+\n+  if (debug_info) {\n+    printing_flags.enableDebugInfo();\n+  }",
    "Label": "clean"
},{
    "Id": 93,
    "Library": "tensorflow",
    "Date": "2023/04/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d769af73c5b555cb0464179b9fd49b2fc6c5b80d",
    "Root Cause": "N.A",
    "Bug report": "Fix bypass_filter in DebugDataDumper\n\nPiperOrigin-RevId: 527607293",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (!ShouldDump(name, group)) return;",
    "Added lines": "+  if (!ShouldDump(name, group) && !bypass_filter) return;",
    "Label": "clean"
},{
    "Id": 79,
    "Library": "tensorflow",
    "Date": "2023/05/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cc199296c36488b363409415d41d053b94d071d6",
    "Root Cause": "N.A",
    "Bug report": "[XLA:CPU Next] Outline elementwise clusters.\n\nWe didn't outline before, because it triggered completely unrelated bug (cl/530892255).\n\nPiperOrigin-RevId: 534815188",
    "Number of deleted lines": 4,
    "Deleted lines": "-constexpr llvm::StringRef kElementwiseLabel = \"__elementwise_label__\";\n-    // TODO(shyshkov): Enable outlining for elementwise clusters.\n-    if (hasLabel(fusionOp, kElementwiseLabel)) return;\n-",
    "Added lines": "",
    "Label": "clean"
},{
    "Id": 292,
    "Library": "tensorflow",
    "Date": "2021/06/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/61b97bb772c96dea93ad1e3a9016093b34cc6c55",
    "Root Cause": "N.A",
    "Bug report": "Print the IR after each pass in CompileGraphSetup pipeline logging.\n\nThe logging is intended to help with debugging, therefore save the entire module\n(instead of single functions) and disable multi-threading if logging is\nactivated which helps with reproducibility.\n\nPiperOrigin-RevId: 378404376\nChange-Id: If70d58c6f4961cd7bb930515f6c3094f7d482ad2",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (VLOG_IS_ON(2)) {\n+    // Print the whole module after each pass which requires disabling\n+    // multi-threading as well.\n+    module_op.getContext()->disableMultithreading();\n+    pm.enableIRPrinting(std::make_unique<tensorflow::BridgeLoggerConfig>(\n+        /*print_module_scope=*/true));\n+  }\n+",
    "Label": "clean"
},{
    "Id": 303,
    "Library": "tensorflow",
    "Date": "2021/04/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b1a3085fda2b731c4e991bfe13c76225be5db9c8",
    "Root Cause": "N.A",
    "Bug report": "Enable warning diagnostics for all log levels.\n\nThis fixes a bug in the previous code where the warning diagnostics is issued\nwhen vlog is disabled but suppressed when vlog is disabled.\n\nThe ideal behavior is to only issue warning diagnostics when vlog is enabled,\nbut we don't have a way to enable vlog in the OSS code that is consistent with\nthe google internal code, causing some test cases to fail.\n\nPiperOrigin-RevId: 371398589\nChange-Id: If725d21bd626743e045c24e011deb9f182a1fad5",
    "Number of deleted lines": 4,
    "Deleted lines": "-  // Non-error diagnostic are ignored when VLOG isn't enabled.\n-  if (diag->getSeverity() != DiagnosticSeverity::Error && VLOG_IS_ON(1))\n-    return success();\n-",
    "Added lines": "",
    "Label": "clean"
},{
    "Id": 318,
    "Library": "tensorflow",
    "Date": "2021/03/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f8aee66b8c227da41273a8d3cef558d9dcf431de",
    "Root Cause": "N.A",
    "Bug report": "Fix typo/logic bug in modular plugins.\n\nWe want to enable plugins only if the environment variable exists!\n\nCC @yongtang, @kvignesh1420, @vnvo2409.\n\nPiperOrigin-RevId: 366056903\nChange-Id: Ia6f0918c97cbd6e064e19a02ce3a7d46f48b2799",
    "Number of deleted lines": 1,
    "Deleted lines": "-      if (!(load_plugin == \"true\" || load_plugin == \"1\")) {",
    "Added lines": "+      if (load_plugin == \"true\" || load_plugin == \"1\") {",
    "Label": "clean"
},
{
    "Id": "ok",
    "Library": "tensorflow",
    "Date": "2023/10/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1bbd10c3bb8de86bfe126ae7ace123f7e85354cf",
    "Root Cause": "N.A",
    "Bug report": "Extend coverage of TF2XLA MLIR generic phase 1 bridge\n\nTF2XLA bridge currently only allows CPU/GPU graphs in parameter server jobs with resource variable arguments to go through the phase 1 bridge. This change removes this restriction to cover more graphs. It would allow feature parity  (outside compilation, distributed strategy, etc) on CPU/GPU/TPU graphs to provide long-term benefits (outside compilation, consistent clustering, debuggability, etc), and unblocks the unification of CPU/GPU and TPU MLIR-based phase 1 pipelines.\n\nPiperOrigin-RevId: 570504216",
    "Number of deleted lines": 46,
    "Deleted lines": "-// Check if the `graph` has parameter serverjobs and resource variable arguments\n-// that are on parameter servers\n-bool HasPsWithResourceVariable(const Graph& graph) {\n-  // Check parameter serverjobs and resource variable arguments that are\n-  // on parameter servers.\n-  const std::string jobType = \"ps\";\n-  const std::string nodeType = \"_Arg\";\n-  const std::string attrKey = \"T\";\n-  for (const Node* node : graph.nodes()) {\n-    if (node->type_string() == nodeType) {\n-      auto device_name = node->assigned_device_name();\n-      DeviceNameUtils::ParsedName device;\n-      if (DeviceNameUtils::ParseFullName(device_name, &device) &&\n-          device.has_job && device.job == jobType) {\n-        for (const auto& attr : node->attrs()) {\n-          auto attr_key = attr.first;\n-          auto attr_value = attr.second;\n-          if (attr_key == attrKey &&\n-              attr_value.value_case() == AttrValue::kType &&\n-              attr_value.type() == DT_RESOURCE) {\n-            return true;\n-            break;\n-          }\n-        }\n-      }\n-    }\n-  }\n-  return false;\n-}\n-\n-// Check if non TPU pipeline should be used\n-bool EnableNonTpuBridge(const Graph& graph) {\n-  // Remark that this is staging change. It will be expanded later for further\n-  // check based on the requirement.\n-  return HasPsWithResourceVariable(graph) && HasQualifiedNonTPUOp(graph);\n-}\n-\n-  if (!run_tpu_bridge && !EnableNonTpuBridge(graph)) {\n-    // Only record CPU/GPU graphs that are qualified but filtered out\n-    if (HasQualifiedNonTPUOp(graph)) {\n-      metrics::UpdateTfMlirBridgeFirstPhaseCounter(\n-          /*device type*/ \"cpu/gpu\",\n-          /*bridge version*/ \"tfxla\",\n-          /*fallback_enabled*/ false,\n-          /*result*/ \"invalid_graph\");\n-    }",
    "Added lines": "+  if (!run_tpu_bridge && !HasQualifiedNonTPUOp(graph)) {",
    "Label": "clean"
},
    {
    "Id": "ok",
    "Library": "tensorflow",
    "Date": "2024/01/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c150bb6edd8d82686efeeff8c1083bcd0a57c96f",
    "Root Cause": "N.A",
    "Bug report": "PR #8051: [ROCm] Fix GraphDebugDotPrint build failure\n\nImported from GitHub PR https://github.com/openxla/xla/pull/8051\n\nCopybara import of the project:\n\n--\n34acc4f468e124c97663057c318f3149f43df4a8 by Dragan Mladjenovic <Dragan.Mladjenovic@amd.com>:\n\n[ROCm] Fix GraphDebugDotPrint build failure\n\nMerging this change closes #8051\n\nPiperOrigin-RevId: 595077142",
    "Number of deleted lines": 5,
    "Deleted lines": "-/* static */ tsl::Status GpuDriver::GraphDebugDotPrint(hipGraph_t graph,\n-                                                       const char* path) {\n-  if (VLOG_IS_ON(100)) {\n-      VLOG(200) << \"HIP graph \" << graph << \" debug file:\\n\" << data;\n-  return ::tsl::OkStatus();",
    "Added lines": "+/* static */ tsl::StatusOr<std::string> GpuDriver::GraphDebugDotPrint(\n+    hipGraph_t graph, const char* path, bool return_printed_graph) {\n+  if (return_printed_graph) {\n+      return data;\n+  return std::string(path);",
    "Label": "clean"
},
    {
        "Id": 32,
        "Library": "tensorflow",
        "Date": "2024/01/11",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6c0ae64819af838154ebbf8816267d28387c0879",
        "Root Cause": "N.A",
        "Bug report": "TFLite Converter users are seeing a lot text printing to the terminal to display the conversion statistics. While this is really useful for debugging, this is not something every customer will want to see in production.\n\nMoving the `createPrintOpStatsPass` pass to run behind a flag that is used for debugging purposes.\n\nPiperOrigin-RevId: 597674724",
        "Number of deleted lines": 8,
        "Deleted lines": "-  pass_manager.clear();\n-  // Print out a detailed report of ops that are not converted to TFL ops.\n-  pass_manager.addPass(mlir::odml::createPrintOpStatsPass(\n-      mlir::odml::GetAcceptedTFLiteDialects()));\n-  if (failed(pass_manager.run(module))) {\n-    return statusHandler.ConsumeStatus();\n-  }\n-",
        "Added lines": "+    pass_manager.clear();\n+    // Print out a detailed report of ops that are not converted to TFL ops.\n+    pass_manager.addPass(mlir::odml::createPrintOpStatsPass(\n+        mlir::odml::GetAcceptedTFLiteDialects()));\n+    if (failed(pass_manager.run(module))) {\n+      return statusHandler.ConsumeStatus();\n+    }\n+",
        "Label": "clean"
    },
    {
        "Id": 17,
        "Library": "tensorflow",
        "Date": "2024/03/22",
        "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f0ae6583f5d39ac2f3d53e1ac22bedd51ccc6d8d",
        "Root Cause": "N.A",
        "Bug report": "#tf-data To help debugging, output the autotuned `max_outstanding_requests` in xprof.\n\nPiperOrigin-RevId: 618198886",
        "Number of deleted lines": 0,
        "Deleted lines": "",
        "Added lines": "+  int64_t autotuned_max_outstanding_requests = model::kAutotune;\n+    autotuned_max_outstanding_requests = max_outstanding_requests_;\n+  if (params_.max_outstanding_requests == model::kAutotune) {\n+    result.push_back(std::make_pair(\n+        \"autotuned_max_outstanding_requests\",\n+        strings::Printf(\"%lld\", static_cast<long long>(\n+                                    autotuned_max_outstanding_requests))));\n+  }",
        "Label": "clean"
    },
    
    {
    "Id": 1,
    "Library": "tensorflow",
    "Date": "2024/05/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e33b6a9fc398eaa316b5f0180c692cfdf5e7489e",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Raise internal error on mismatched backend config descriptors.\n\nThis is more consistent with the original goal of this function. Originally, it crashed if the descriptors mismatched. It was later refactored to avoid the crash and inadvertently changed to simply fall through instead of returning an error. This fallthrough introduced a subtle bug: If the descriptors are mismatching but the stored `proto_` is not empty, the stored `proto_` will be overwritten.\n\nPiperOrigin-RevId: 630368223",
    "Number of deleted lines": 3,
    "Deleted lines": "-    if (proto_ptr->GetDescriptor() == proto->GetDescriptor()) {\n-      proto->CopyFrom(*proto_ptr);\n-      return OkStatus();",
    "Added lines": "+    if (proto_ptr->GetDescriptor() != proto->GetDescriptor()) {\n+      return Internal(\"Mismatched backend config descriptors.\");\n+\n+    proto->CopyFrom(*proto_ptr);\n+    return OkStatus();",
    "Label": "clean"
},
{
    "Id": 2,
    "Library": "tensorflow",
    "Date": "2024/01/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1f976fc26c9e031eaf34b32449d4ae096608f25d",
    "Root Cause": "N.A",
    "Bug report": "Hookup debug data logger to the old bridge XlaCompiler\n\nPiperOrigin-RevId: 602371154",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (VLOG_IS_ON(2)) {",
    "Added lines": "+#include \"tensorflow/core/util/debug_data_dumper.h\"\n+  if (VLOG_IS_ON(2) || DEBUG_DATA_DUMPER()->ShouldDump(name, kDebugGroupMain)) {",
    "Label": "clean"
},
{
    "Id": 7,
    "Library": "tensorflow",
    "Date": "2023/10/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1bbd10c3bb8de86bfe126ae7ace123f7e85354cf",
    "Root Cause": "N.A",
    "Bug report": "Extend coverage of TF2XLA MLIR generic phase 1 bridge\n\nTF2XLA bridge currently only allows CPU/GPU graphs in parameter server jobs with resource variable arguments to go through the phase 1 bridge. This change removes this restriction to cover more graphs. It would allow feature parity  (outside compilation, distributed strategy, etc) on CPU/GPU/TPU graphs to provide long-term benefits (outside compilation, consistent clustering, debuggability, etc), and unblocks the unification of CPU/GPU and TPU MLIR-based phase 1 pipelines.\n\nPiperOrigin-RevId: 570504216",
    "Number of deleted lines": 46,
    "Deleted lines": "-// Check if the `graph` has parameter serverjobs and resource variable arguments\n-// that are on parameter servers\n-bool HasPsWithResourceVariable(const Graph& graph) {\n-  // Check parameter serverjobs and resource variable arguments that are\n-  // on parameter servers.\n-  const std::string jobType = \"ps\";\n-  const std::string nodeType = \"_Arg\";\n-  const std::string attrKey = \"T\";\n-  for (const Node* node : graph.nodes()) {\n-    if (node->type_string() == nodeType) {\n-      auto device_name = node->assigned_device_name();\n-      DeviceNameUtils::ParsedName device;\n-      if (DeviceNameUtils::ParseFullName(device_name, &device) &&\n-          device.has_job && device.job == jobType) {\n-        for (const auto& attr : node->attrs()) {\n-          auto attr_key = attr.first;\n-          auto attr_value = attr.second;\n-          if (attr_key == attrKey &&\n-              attr_value.value_case() == AttrValue::kType &&\n-              attr_value.type() == DT_RESOURCE) {\n-            return true;\n-            break;\n-          }\n-        }\n-      }\n-    }\n-  }\n-  return false;\n-}\n-\n-// Check if non TPU pipeline should be used\n-bool EnableNonTpuBridge(const Graph& graph) {\n-  // Remark that this is staging change. It will be expanded later for further\n-  // check based on the requirement.\n-  return HasPsWithResourceVariable(graph) && HasQualifiedNonTPUOp(graph);\n-}\n-\n-  if (!run_tpu_bridge && !EnableNonTpuBridge(graph)) {\n-    // Only record CPU/GPU graphs that are qualified but filtered out\n-    if (HasQualifiedNonTPUOp(graph)) {\n-      metrics::UpdateTfMlirBridgeFirstPhaseCounter(\n-          /*device type*/ \"cpu/gpu\",\n-          /*bridge version*/ \"tfxla\",\n-          /*fallback_enabled*/ false,\n-          /*result*/ \"invalid_graph\");\n-    }",
    "Added lines": "+  if (!run_tpu_bridge && !HasQualifiedNonTPUOp(graph)) {",
    "Label": "clean"
},
{
    "Id": 8,
    "Library": "tensorflow",
    "Date": "2023/05/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/16e29ce9f7b4ccc9c5f3ebe7ada1954d1b1e306d",
    "Root Cause": "N.A",
    "Bug report": "debug_string_ no longer starts empty. Always set to the new value.\n\nPiperOrigin-RevId: 531366810",
    "Number of deleted lines": 8,
    "Deleted lines": "-    if (debug_string_.empty()) {\n-      debug_string_ = std::move(debug_string);\n-    }\n-  void SetToString(std::string to_string) {\n-    if (to_string_.empty()) {\n-      to_string_ = std::move(to_string);\n-    }\n-  }",
    "Added lines": "+    debug_string_ = std::move(debug_string);\n+  void SetToString(std::string to_string) { to_string_ = std::move(to_string); }",
    "Label": "clean"
},
{
    "Id": 9,
    "Library": "tensorflow",
    "Date": "2023/04/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7880014be998032d349419886a17481fe67bc28e",
    "Root Cause": "N.A",
    "Bug report": "#tf-data-service Add more information to SnapshotReader error message.\n\nThe test job is getting `Failed to get element: Snappy_Uncompress failed.`\nerrors. This will help debug which file causes the error.\n\nPiperOrigin-RevId: 521494593",
    "Number of deleted lines": 3,
    "Deleted lines": "-      Status s = reader_->ReadTensors(out_tensors);\n-      if (errors::IsOutOfRange(s)) {\n-      return s;",
    "Added lines": "+      Status status = reader_->ReadTensors(out_tensors);\n+      if (errors::IsOutOfRange(status)) {\n+      TF_RETURN_WITH_CONTEXT_IF_ERROR(\n+          status,\n+          \" Failed to read tf.data snapshot file: \", dataset()->chunk_file_);\n+      return status;",
    "Label": "clean"
},
{
    "Id": 10,
    "Library": "tensorflow",
    "Date": "2023/02/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a1b9edad9aee7475d176e29d4af73c5e809f8be1",
    "Root Cause": "N.A",
    "Bug report": "Add extra vlogs for coordinated reads to help with debugging.\n\nPiperOrigin-RevId: 511245481",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      VLOG(3) << \"Returning from GetNext with internal error\";\n+    if (result->skip) {\n+      VLOG(3) << \"Skipping result from task \" << result->task_id;\n+    }\n+    VLOG(1) << \"Returning end_of_sequence\";",
    "Label": "clean"
},
{
    "Id": 11,
    "Library": "tensorflow",
    "Date": "2022/09/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/aa6d0555a2f99762809f50e3cb339795b02d653e",
    "Root Cause": "N.A",
    "Bug report": "Deprecate ImplicitBatchModeCompatible dynamic shape strategy\n\nThis strategy is intended for ease of use for people familiar with\nthe implicit batch mode profile, in that it does not require the user\nto specifically call build() before trying to run an inference with\nTFTRT converted graph. Since this is actually a dynamic shape mode,\nand input shapes are required in dynamic shape mode for TensorRT\nprofile generation; this mode makes some educated guesses for\nminimum and maximum shapes for inputs the TensorRT engine.\n\nThis has proven to be buggy for models that include transpose and\nreshape operations, among others.\n\nDue to the above, and since dynamic shape mode requires users to\ncall build() with the correct input shapes to generate TensorRT\nprofiles correctly, this mode is being deprecated.\n\nSigned-off-by: Meenakshi Venkataraman <meenakshiv@nvidia.com>",
    "Number of deleted lines": 2,
    "Deleted lines": "-     * `ImplicitBatchModeCompatible`: create the profiles that will produce the\n-       same GPU engines as the implicit_batch_mode would produce.",
    "Added lines": "+    if strategy == \"ImplicitBatchModeCompatible\":\n+      logging.warn(\"ImplicitBatchModeCompatible strategy is deprecated, and\"\n+          \" using it may result in errors during engine building. Please\"\n+          \" consider using a different profile strategy.\")",
    "Label": "clean"
},
{
    "Id": 12,
    "Library": "tensorflow",
    "Date": "2022/09/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5b6f7d228f4bb57ec1282eb7bc0c8f738233432d",
    "Root Cause": "N.A",
    "Bug report": "[xla:runtime] NFC: Add explicit std::move to work around compiler bugs\n\nPiperOrigin-RevId: 476468768",
    "Number of deleted lines": 3,
    "Deleted lines": "-#include \"tensorflow/compiler/xla/mlir/ir/runtime/rt_ops.h\"\n-  if (auto converted = ConvertCanonicalType(type, *this)) return converted;\n-    if (auto converted = conversion(type)) return converted;",
    "Added lines": "+#include \"tensorflow/compiler/xla/mlir/ir/runtime/rt_dialect.h\"\n+  if (std::unique_ptr<Type> converted = ConvertCanonicalType(type, *this))\n+    return std::move(converted);\n+    if (std::unique_ptr<Type> converted = conversion(type))\n+      return std::move(converted);",
    "Label": "clean"
},
{
    "Id": 13,
    "Library": "tensorflow",
    "Date": "2022/08/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2768a9e9073ac46f2be1779dfeaa635ef5bbe2d3",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Revert work-around for temporary MLIR bug.\n\nThe [fix](https://github.com/llvm/llvm-project/commit/00a52c75655bb352f875729a93c3f2ae990e5b78) has landed and the work-around is no longer needed.\n\nPiperOrigin-RevId: 466018381",
    "Number of deleted lines": 9,
    "Deleted lines": "-#include \"mlir/IR/BuiltinAttributes.h\"\n-    if (symbol == gpuModuleOp.getNameAttr()) {\n-      continue;\n-    }\n-    // gpu.module name changed, update symbol uses in gpu.launch_func.\n-    funcOp->walk([&](gpu::LaunchFuncOp launch) {\n-      launch.kernelAttr(\n-          SymbolRefAttr::get(symbol, launch.kernel().getNestedReferences()));\n-    });",
    "Added lines": "+    if (failed(symbolTable.replaceAllSymbolUses(gpuModuleOp, symbol, funcOp)))\n+      return rewriter.notifyMatchFailure(fusionOp, \"failed to replace symbol\");",
    "Label": "clean"
},
{
    "Id": 14,
    "Library": "tensorflow",
    "Date": "2021/12/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/45d7d254a5736a1702c2378aa24ec7c4e26daaf2",
    "Root Cause": "N.A",
    "Bug report": "Use Protobuf TextFormat instead of DebugString to create a text proto\n\nPiperOrigin-RevId: 414749793\nChange-Id: Ib0e42421c3c904fecd12c759aaec95d731c12c82",
    "Number of deleted lines": 11,
    "Deleted lines": "-#include \"tensorflow/core/platform/protobuf.h\"\n-      std::string attr_second_text;\n-      ::tensorflow::protobuf::TextFormat::Printer printer;\n-      printer.SetSingleLineMode(true);\n-      printer.PrintToString(attr.second, &attr_second_text);\n-      if (!attr_second_text.empty() &&\n-          attr_second_text[attr_second_text.size() - 1] == ' ') {\n-        attr_second_text.resize(attr_second_text.size() - 1);\n-      }\n-      string attr_str =\n-          absl::Substitute(\"('$0', $1)\", attr.first, attr_second_text);",
    "Added lines": "+      string attr_str = absl::Substitute(\"('$0', $1)\", attr.first,\n+                                         attr.second.ShortDebugString());",
    "Label": "clean"
},
{
    "Id": 15,
    "Library": "tensorflow",
    "Date": "2021/10/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6ab00d035f162354cbb946b775b354e02dcd73dc",
    "Root Cause": "N.A",
    "Bug report": "[XLIR] Dump TFRT BEF binary if debug options say so.\n\nPiperOrigin-RevId: 406170157\nChange-Id: Icb30faf0972893372dacf6bd3450e3ba0a777300",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (DumpingEnabledForHloModule(*hlo_module)) {\n+    DumpToFileInDirOrStdout(*hlo_module, \"\", \"tfrt_bef\", bef);\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 27,
    "Library": "pytorch",
    "Date": "2024/05/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e5cc7ada6706f737959d8488d96028b3eb29aeea",
    "Root Cause": "N.A",
    "Bug report": "skip triton template precompilation in 311.0-3.11.7 to workaround 311 cpython bug (#125446)\n\nFix for https://github.com/pytorch/pytorch/issues/125374. We dont have CI for this specific versions, but I verified locally. THere is a cpython bug from 3.11.0->3.11.7 where the ast parsing state is global, and errors with multiple threads. when dust settles a little around the new process based compilation we can look into migrating.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125446\nApproved by: https://github.com/Chillee\nghstack dependencies: #125289",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            # https://github.com/python/cpython/issues/106905\n+            if (\n+                sys.version_info.major == 3\n+                and sys.version_info.minor == 11\n+                and sys.version_info.micro <= 8\n+            ):\n+                return no_op\n+",
    "Label": "clean"
},
{
    "Id": 28,
    "Library": "pytorch",
    "Date": "2024/04/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e00282fecfcb53790aebfb24cc48a8703577778e",
    "Root Cause": "N.A",
    "Bug report": "[c10d] make monitorThread sleep when we try to dump (#123788)\n\nSummary:\nWe seperated the FR dump logic from the desync debug logic,\nso we no longer set collectiveDebugInfoMode_ to true when we just need FR\ndump. That's why monitor thread did not sleep and try to kill the\nprocess without waiting for the dump.\n\nThe fix is simple, we should sleep whenever shouldDump_ is true\nTest Plan:\nExisting unit tests\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123788\nApproved by: https://github.com/wconstab",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if ((terminateProcessGroup_.load() || collectiveDebugInfoMode_.load()) &&",
    "Added lines": "+          shouldDump_.store(true);\n+        shouldDump_.store(true);\n+  if ((terminateProcessGroup_.load() || collectiveDebugInfoMode_.load() ||\n+       shouldDump_.load()) &&",
    "Label": "clean"
},
{
    "Id": 29,
    "Library": "pytorch",
    "Date": "2023/03/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/be49d3b1702c629057560893db20f2a6b6901656",
    "Root Cause": "N.A",
    "Bug report": "[CI] Turn on debug logging for dla102 and gernet_l (#97307)\n\nSummary: Log the generated code for those two flaky tests to see if\nthere is any codegen difference when they fail.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97307\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        if (\n+            args.ci\n+            and args.accuracy\n+            and args.training\n+            and args.only in {\"dla102\", \"gernet_l\"}\n+        ):\n+            # Log generated code for flaky tests, to check if there is any codegen difference\n+            inductor_config.debug = True\n+",
    "Label": "clean"
},
{
    "Id": 30,
    "Library": "pytorch",
    "Date": "2022/10/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6c604c9262307ffcaf1d7dd68bfa5f6b44513d06",
    "Root Cause": "N.A",
    "Bug report": "[CuDNN v8 API][Quantization]fix alignment function in quantized cuDNN V8 path (#86253)\n\nThis bug was in the native cuDNN V8 API integration and was fixed a while ago, but the change was never ported here.\n\nPreviously the returned alignment could be twice the actual alignment of the data if the alignment was smaller than 16.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86253\nApproved by: https://github.com/dzdang",
    "Number of deleted lines": 1,
    "Deleted lines": "-  while (address % alignment == 0 && alignment < 16) alignment *= 2;",
    "Added lines": "+  for (; alignment < 16; alignment *= 2) {\n+    if (address % (alignment * 2)) {\n+      return alignment;\n+    }\n+  }",
    "Label": "clean"
},
{
    "Id": 31,
    "Library": "pytorch",
    "Date": "2022/10/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/adfd8f382331adbf9cbfa14039ef3b61f2f4e10c",
    "Root Cause": "N.A",
    "Bug report": "[FSDP] assert to runtime error (#86336)\n\nPrefer raising an error over `assert` which should mostly to indicate a developer bug, but user can cause this error path.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86336\nApproved by: https://github.com/awgu",
    "Number of deleted lines": 1,
    "Deleted lines": "-        assert self._is_root, \"`no_sync()` on inner FSDP instances is not supported\"",
    "Added lines": "+        if not self._is_root:\n+            raise RuntimeError(\n+                \"`no_sync()` on inner FSDP instances is not supported. Please call `no_sync()` on root FSDP module.\"\n+            )",
    "Label": "clean"
},
{
    "Id": 32,
    "Library": "pytorch",
    "Date": "2022/09/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e09a84a184e1687f4ddc7f3fc875eaaf5b9ec74f",
    "Root Cause": "N.A",
    "Bug report": "Removed debug output that doesn't work with faketensors (#85992)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/85992\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 8,
    "Deleted lines": "-        if config.debug_partitioner:\n-            fw_outs = call_func_with_args(compiled_fw_func, deduped_flat_args)\n-            activation_sizes = 0\n-            for out in fw_outs[_num_outs:]:\n-                if isinstance(out, torch.Tensor):\n-                    activation_sizes += out.storage().nbytes()\n-            print(f\"Real Activations Stored(GB): {activation_sizes/1e9}\")\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 33,
    "Library": "pytorch",
    "Date": "2022/06/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cec251fc4bfa76ad77ad748559d52055cc579f27",
    "Root Cause": "N.A",
    "Bug report": "[lint] Don't invoke lintrunner with --verbose\n\nIt's been running for a while and is stable, so we don't need debugging\nlogging anymore. This should reduce noise for people.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/79216\n\nApproved by: https://github.com/seemethere",
    "Number of deleted lines": 1,
    "Deleted lines": "-          if ! lintrunner --verbose --force-color --all-files --tee-json=lint.json; then",
    "Added lines": "+          if ! lintrunner --force-color --all-files --tee-json=lint.json; then",
    "Label": "clean"
},
{
    "Id": 1176,
    "Library": "pytorch",
    "Date": "2024/02/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f3a2094065c8b4b7bae426e71c923a8a8abb74b5",
    "Root Cause": "Type Checking",
    "Bug report": "[Dynamo][Export] Mitigate legacy issue that aten op as export entrance function (#119528)\n\nThis is going to fix a legacy issue like:\n```\ntorch._dynamo.export(torch.ops.aten.scaled_dot_product_attention, ...)(*inputs,)\n```\nThis is not supported any more, now the top level ```torch.export``` only support ```nn.Module```, but there are still some tests using the internal APIs and caused the ```trace_rules.check``` assertion error. This PR is going to mitigate such cases.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119528\nApproved by: https://github.com/ydwu4",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            and not isinstance(\n+                call_to_inspect, (torch._ops.OpOverloadPacket, torch._ops.OpOverload)\n+            )",
    "Label": "Buggy"
},
{
    "Id": 1177,
    "Library": "pytorch",
    "Date": "2022/03/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/012829eb3657aff2d58cead0bd166089c6e90c7f",
    "Root Cause": "Device Type",
    "Bug report": "[Lazy][JIT] Do not crash when target device is unsupported by fuser (#73820)\n\nSummary:\nThe `canFuseOnDevice` function now crashes when the device is not covered (i.e., CPU, GPU, XPU). However, now we have some devices, such as XLA and Lazy, that could perform fusion by themselves. This checker then prevents these devices from working on the models partially implemented in `jit.script`.\n\nThis PR proposes to remove this checker and simply return false for all uncovered cases. Another alternative is adding the following logic if it is unsafe to simply remove the checker:\n```\nelse if (device-> type() == DeviceType::XLA || device-> type() == DeviceType::Lazy) {\n  return false;\n} else {\n  TORCH_CHECK_NOT_IMPLEMENTED(false, \"Unknown device for tensorexpr fuser\")\n}\n```\n\ncc wconstab\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73820\n\nReviewed By: navahgar\n\nDifferential Revision: D34731314\n\nPulled By: wconstab\n\nfbshipit-source-id: 1c0a90dcd6c67803a27fa2f305d78b2a539f3604\n(cherry picked from commit 94c61d6c0a9c2ef6e0d046f7f06a7158b43d4d61)",
    "Number of deleted lines": 2,
    "Deleted lines": "-    } else {\n-      TORCH_CHECK_NOT_IMPLEMENTED(false, \"Unknown device for tensorexpr fuser\")",
    "Added lines": "+    return false;",
    "Label": "Buggy"
},
{
    "Id": 1178,
    "Library": "pytorch",
    "Date": "2022/01/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/027c0d7f8e37e583c02b372df5331d73793c06b1",
    "Root Cause": "Device Type",
    "Bug report": "fixed compilations on xla tensor print (#71147)\n\nSummary:\nFixes multiple compilation on xla tensor print. Please check the conversation here: https://github.com/pytorch/xla/pull/3253\n\nThis is done to avoid compilations during tensor printing. Torch performs some tensor operations like slicing to make the tensor readable. These operations result in compilations. Hence to avoid the compilations, copying the tensor to cpu before printing.\n\nexample:\n\n```\ndev = xm.xla_device()\ndef test_linear(input_shape=(8, 1024)):\n    import pdb\n    pdb.set_trace()\n    linear = torch.nn.Linear(in_features=1024, out_features=4096, bias=True).to(dev)\n    inp = torch.randn(*input_shape).to(dev)\n    output = linear(inp)\n    xm.mark_step()\n    return output\n```\nReturning from this function would have resulted in 63 compiles, since PDB prints the value of the return output. In this case it is a xla tensor.\n\nNow with the current change, there is no compilation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71147\n\nReviewed By: shunting314\n\nDifferential Revision: D33795177\n\nPulled By: wconstab\n\nfbshipit-source-id: 74b53d9a1cb7ef67f9d8b0a32064f3896be449b5\n(cherry picked from commit a9e0687fc5c9981fb55ea4dc406c283c80fa20c9)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    # Tensor printing performs tensor operations like slice, indexing, etc to make it in a\n+    # representable format. These operations on xla/lazy tensor results in compilations. Hence,\n+    # to avoid compilations, copying the tensor to cpu before printing.\n+    if self.device.type == 'xla' or self.device.type == 'lazy':\n+        self = self.to('cpu')\n+",
    "Label": "Buggy"
},
{
    "Id": 1179,
    "Library": "pytorch",
    "Date": "2023/12/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/02e2158e754bafda46e663052c838aeb6ab6b560",
    "Root Cause": "Boundary Value",
    "Bug report": "Fix for out of bounds read in mobile interpreter INTERFACE_CALL opcode handler (#110301)\n\nSummary:\nThe INTERFACE_CALL opcode for the mobile TorchScript interpreter contained an out of bounds read issue leading to memory corruption.\n\nThis change adds an explicit check that the number of inputs passed to the format method called when handling the INTERFACE_CALL opcode is a valid and within bounds of the stack.\n\nTest Plan: contbuild + OSS signals\n\nDifferential Revision: D49739450\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110301\nApproved by: https://github.com/dbort",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+          if (inst.N == 0 || inst.N > stack.size()) {\n+            TORCH_CHECK(\n+                false,\n+                \"INTERFACE_CALL N=\",\n+                inst.N,\n+                \" not in range [1, \",\n+                stack.size(),\n+                \"]\");\n+          }",
    "Label": "Buggy"
},
{
    "Id": 1180,
    "Library": "pytorch",
    "Date": "2023/10/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/07f0f383fa23e63eca164036ab58ab983e9437eb",
    "Root Cause": "Type Checking",
    "Bug report": "update tensor-like to check instance for torch function impl (#111087)\n\ntensor like should check the instance for a torch function impl, not the type\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111087\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return type(inp) is torch.Tensor or hasattr(type(inp), \"__torch_function__\")",
    "Added lines": "+    return type(inp) is torch.Tensor or hasattr(inp, \"__torch_function__\")",
    "Label": "Buggy"
},
{
    "Id": 1181,
    "Library": "pytorch",
    "Date": "2023/10/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/097defb1608827d82b18b27adeec0a98b72a9281",
    "Root Cause": "Device Availability",
    "Bug report": "[device mesh] only check when world size > num_devices per host (#111091)\n\nas titled\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111091\nApproved by: https://github.com/awgu, https://github.com/wz337\nghstack dependencies: #110898, #110900",
    "Number of deleted lines": 1,
    "Deleted lines": "-            if world_size % num_devices_per_host != 0:",
    "Added lines": "+            if (\n+                world_size > num_devices_per_host\n+                and world_size % num_devices_per_host != 0\n+            ):",
    "Label": "Buggy"
},
{
    "Id": 1182,
    "Library": "pytorch",
    "Date": "2023/03/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1359d16fe8ca0cb7041674c455f2f99a9636fec0",
    "Root Cause": "Execution Mode",
    "Bug report": "[CI] Further tighten the checking of two eager runs (#95902)\n\nSummary: To catch nondeterminism in eager if there is any.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95902\nApproved by: https://github.com/jansel",
    "Number of deleted lines": 2,
    "Deleted lines": "-                fp64_ref=None,  # Two eager runs should be the same without comparing against fp64_output\n-        torch.backends.cudnn.deterministic = True",
    "Added lines": "+            # Two eager runs should have exactly same result\n+                fp64_ref=None,\n+                cos_similarity=False,\n+                tol=0,\n+        torch.use_deterministic_algorithms(True)\n+        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n+        torch.backends.cudnn.deterministic = True\n+        torch.backends.cuda.matmul.allow_tf32 = False",
    "Label": "Buggy"
},
{
    "Id": 1183,
    "Library": "pytorch",
    "Date": "2021/11/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/152f665dee05377f7459d985d60dc1edb782d40e",
    "Root Cause": "Type Checking",
    "Bug report": "Inserted check for PyObject_IsInstance in THPVariableCheck (#67588)\n\nSummary:\nInserted check for the return of PyObject_IsInstance to capture the case in which it raises an exception and return -1. When this happen THPVariable_Check now throws a python_error to signal the exception.\n\nFixes https://github.com/pytorch/pytorch/issues/65084\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67588\n\nReviewed By: mruberry\n\nDifferential Revision: D32064776\n\nPulled By: albanD\n\nfbshipit-source-id: 895c7682e0991ca257e27f9638a7462d83707320",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return THPVariableClass && PyObject_IsInstance(obj, THPVariableClass);",
    "Added lines": "+#include <torch/csrc/Exceptions.h>\n+  if (!THPVariableClass)\n+      return false;\n+\n+  const auto result = PyObject_IsInstance(obj, THPVariableClass);\n+  if (result == -1)\n+      throw python_error();\n+  return result;",
    "Label": "Buggy"
},
{
    "Id": 1184,
    "Library": "pytorch",
    "Date": "2022/06/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/157d478a30f27fd9d866c1235841721a559c8d0b",
    "Root Cause": "Edge Cases",
    "Bug report": "Fix omission of shape in size check in index.\n\nSigned-off-by: Edward Z. Yang <ezyangfb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78897\n\nApproved by: https://github.com/Lezcano, https://github.com/anjali411",
    "Number of deleted lines": 1,
    "Deleted lines": "-                        index[j] <= self.shape[k + j],",
    "Added lines": "+                        index.shape[j] == self.shape[k + j],",
    "Label": "Buggy"
},
{
    "Id": 1185,
    "Library": "pytorch",
    "Date": "2023/09/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/175ccfc4c8443bcc65c87d9c942272d3ebf16b0b",
    "Root Cause": "Null Value",
    "Bug report": "Verify flatbuffer module fields are initialized (#109794)\n\nFixes #109793\n\nAdd validation on flatbuffer module field to prevent segfault\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109794\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 2,
    "Deleted lines": "-  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")\n-      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\")",
    "Added lines": "+      ivalues && module->object_types(),\n+      \"Parsing flatbuffer module: Corrupted ivalues/object_types field\");\n+  TORCH_CHECK(\n+      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\");",
    "Label": "Buggy"
},
{
    "Id": 1186,
    "Library": "pytorch",
    "Date": "2023/12/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1becd2c314f45bded8d3fbec91d785e7190b4afe",
    "Root Cause": "Device Availability",
    "Bug report": "Align checks in `_use_cudnn_ctc_loss` with those in `_cudnn_ctc_loss` (#115617)\n\nThis PR is intended to fix the following problem:\n\nWhen using `CTCLoss`, there is a cudnn path gated by a call to [`_use_cudnn_ctc_loss`](\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/cudnn/LossCTC.cpp#L73-L101) which checks some conditions\n\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/LossCTC.cpp#L486-L496\n\nHowever, there are more checks in `_cudnn_ctc_loss`\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/cudnn/LossCTC.cpp#L122-L130\n\nsome of which are not present in `_use_cudnn_ctc_loss` (e.g. the check that `targets` is on CPU which will cause a RuntimeError after dispatching to `_cudnn_ctc_loss`). Instead, these checks should be in `_use_cudnn_ctc_loss` so that the normal `_ctc_loss` path will be used if the checks are not met)\n\ne.g. Before this PR\n\n```python\n>>> import torch\n>>> ctcloss = torch.nn.CTCLoss()\n>>> log_probs = torch.randn((50, 3, 15), device='cuda').log_softmax(2)\n>>> target = torch.randint(1, 15, (30 + 25 + 20,), dtype = torch.int)\n>>> input_lengths = torch.tensor((50, 50, 50), device='cuda')\n>>> target_lengths = torch.tensor((30, 25, 20), device='cuda')\n>>> ctcloss(log_probs, target, input_lengths, target_lengths)\ntensor(4.1172, device='cuda:0')\n>>> target = target.to('cuda')\n>>> ctcloss(log_probs, target, input_lengths, target_lengths)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/loss.py\", line 1779, in forward\n    return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n  File \"/data/users/mg1998/pytorch/torch/nn/functional.py\", line 2660, in ctc_loss\n    return torch.ctc_loss(\nRuntimeError: Expected tensor to have CPU Backend, but got tensor with CUDA Backend (while checking arguments for cudnn_ctc_loss)\n```\n\nAfter this PR the above snippet runs without error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115617\nApproved by: https://github.com/janeyx99",
    "Number of deleted lines": 1,
    "Deleted lines": "-      (log_probs.device().type() == at::kCUDA);",
    "Added lines": "+      (log_probs.device().type() == at::kCUDA) &&\n+      (targets.device().type() == at::kCPU) &&\n+      (targets.is_contiguous()) &&\n+      (log_probs.dim() == 3);",
    "Label": "Buggy"
},
{
    "Id": 1187,
    "Library": "pytorch",
    "Date": "2022/03/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1c5a8125798392f8d7c57e88735f43a14ae0beca",
    "Root Cause": "Type Checking",
    "Bug report": "Better type checking in disable_torch_function/dispatch\n\nA follow up fix for PR #74509\nOriginal issue: #73933\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/74720\nApproved by: https://github.com/gchanan",
    "Number of deleted lines": 4,
    "Deleted lines": "-  } else if (PyList_CheckExact(args)) {\n-  } else {\n-  } else if (PyList_CheckExact(args)) {\n-  } else {",
    "Added lines": "+  } else if (PyList_Check(args)) {\n+  } else if (PyTuple_Check(args)) {\n+  } else {\n+    throw torch::TypeError(\"expected List or Tuple (got %s)\", Py_TYPE(args)->tp_name);\n+  } else if (PyList_Check(args)) {\n+  } else if (PyTuple_Check(args)) {\n+  } else {\n+    throw torch::TypeError(\"expected List or Tuple (got %s)\", Py_TYPE(args)->tp_name);",
    "Label": "Buggy"
},
{
    "Id": 1188,
    "Library": "pytorch",
    "Date": "2022/06/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1f819ee965894b8332cb364a67c91855c91c9dcc",
    "Root Cause": "Others",
    "Bug report": "Add check for no grad in transformer encoder nestedtensor conversion (#78832)\n\nBefore, we allowed inputs with grad to be converted to NestedTensors. Autograd attempts to find the size of the NestedTensor, but NestedTensor throws an exception for its size function. This causes all calls to nn.TransformerEncoder with grad enabled to fail.\n\nFix: we add a check for no grad in transformer encoder so we do not convert tensor with grad to nestedtensor.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78832\nApproved by: https://github.com/cpuhrsch, https://github.com/jbschlosser",
    "Number of deleted lines": 3,
    "Deleted lines": "-                        if output.is_cuda or 'cpu' in str(output.device):\n-                            convert_to_nested = True\n-                            output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())",
    "Added lines": "+                        if not torch.is_grad_enabled() or all([not x.requires_grad for x in tensor_args]):\n+                            if output.is_cuda or 'cpu' in str(output.device):\n+                                convert_to_nested = True\n+                                output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())",
    "Label": "Buggy"
},
{
    "Id": 1189,
    "Library": "pytorch",
    "Date": "2022/10/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/232fbd90ff6d93362120d955befeeb297179ddad",
    "Root Cause": "Device Type",
    "Bug report": "[TorchDynamo]: fused bias for cpu convolution path (#87050)\n\nFor aten.convolution CPU path, the bias always can be fused, so this PR adds a device check: if inputs' device is CPU, we will fuse it for a good performance.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87050\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "Number of deleted lines": 2,
    "Deleted lines": "-            None,  # bias handled below\n-    if bias is not None:",
    "Added lines": "+    is_cpu = all(\n+        input.get_device().type == \"cpu\"\n+        for input in (x, weight, bias)\n+        if input is not None\n+    )\n+            bias if is_cpu else None,  # For cpu path, bias can always be fused\n+    if not is_cpu and bias is not None:",
    "Label": "Buggy"
},
{
    "Id": 1190,
    "Library": "pytorch",
    "Date": "2022/12/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2597d5d72272d196b4cb5442ffc8cde376d1f785",
    "Root Cause": "Type Checking",
    "Bug report": "TorchDynamo: always convert flexiblelayout to be FixedLayout when given a stride_order (#89904)\n\nFor convolution, we always call **require_stride_order** to convert the input to the target stride order,  if the original input's layout is flexiblelayout, there always have a memory copy because the **is_stride_order_storage_and_layout** only checks the init stride order,  I think for flexiblelayout, means it's layout can be changed, if the user gives a stride order, I think we always need to convert the flexiblelayout to be FixedLayout using given strider order.\n\nGiven a CV user case, the max_pooling's output is used by two convolutions, there has two memory copies:\n\n```\nkernel_cpp_0 = async_compile.cpp('''\n#include \"/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h\"\nextern \"C\" void kernel(const float* __restrict__ in_ptr0,\n                       float* __restrict__ out_ptr0,\n                       float* __restrict__ out_ptr1,\n                       float* __restrict__ out_ptr2)\n{\n    #pragma GCC ivdep\n    for(long i0=0; i0<128; i0+=1)\n    {\n        #pragma GCC ivdep\n        for(long i1=0; i1<3; i1+=1)\n        {\n            #pragma GCC ivdep\n            for(long i2=0; i2<3; i2+=1)\n            {\n                #pragma GCC ivdep\n                for(long i3=0; i3<3; i3+=1)\n                {\n                    {\n                        {\n                            auto tmp0 = in_ptr0[i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp1 = in_ptr0[3 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp3 = in_ptr0[6 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp5 = in_ptr0[21 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp7 = in_ptr0[24 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp9 = in_ptr0[27 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp11 = in_ptr0[42 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp13 = in_ptr0[45 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp15 = in_ptr0[48 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp2 = (tmp0 != tmp0) ? tmp0 : std::max(tmp1, tmp0);\n                            auto tmp4 = (tmp2 != tmp2) ? tmp2 : std::max(tmp3, tmp2);\n                            auto tmp6 = (tmp4 != tmp4) ? tmp4 : std::max(tmp5, tmp4);\n                            auto tmp8 = (tmp6 != tmp6) ? tmp6 : std::max(tmp7, tmp6);\n                            auto tmp10 = (tmp8 != tmp8) ? tmp8 : std::max(tmp9, tmp8);\n                            auto tmp12 = (tmp10 != tmp10) ? tmp10 : std::max(tmp11, tmp10);\n                            auto tmp14 = (tmp12 != tmp12) ? tmp12 : std::max(tmp13, tmp12);\n                            auto tmp16 = (tmp14 != tmp14) ? tmp14 : std::max(tmp15, tmp14);\n                            out_ptr0[i3 + (3*i2) + (9*i1) + (27*i0)] = tmp16;\n                        }\n                    }\n                }\n            }\n        }\n    }\n    #pragma GCC ivdep\n    for(long i0=0; i0<128; i0+=1)\n    {\n        #pragma GCC ivdep\n        for(long i1=0; i1<3; i1+=1)\n        {\n            #pragma GCC ivdep\n            for(long i2=0; i2<9; i2+=1)\n            {\n                {\n                    {\n                        auto tmp0 = out_ptr0[i1 + (3*i2) + (27*i0)];\n                        out_ptr1[i1 + (3*i2) + (27*i0)] = tmp0;\n                        out_ptr2[i1 + (3*i2) + (27*i0)] = tmp0;\n                    }\n                }\n            }\n        }\n    }\n}\n''')\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1 = args\n    args.clear()\n    buf0 = empty_strided((128, 3, 3, 3), (27, 1, 9, 3), device='cpu', dtype=torch.float32)\n    buf2 = empty_strided((128, 3, 3, 3), (27, 1, 9, 3), device='cpu', dtype=torch.float32)\n    buf4 = empty_strided((128, 3, 3, 3), (27, 1, 9, 3), device='cpu', dtype=torch.float32)\n    kernel_cpp_0(c_void_p(arg4_1.data_ptr()), c_void_p(buf0.data_ptr()), c_void_p(buf2.data_ptr()), c_void_p(buf4.data_ptr()))\n    del arg4_1\n    del buf0\n    buf3 = torch.ops.mkldnn._convolution_pointwise(buf2, arg0_1, arg1_1, (0, 0), (1, 1), (1, 1), 1, 'none', [], '')\n    assert_size_stride(buf3, (128, 3, 3, 3), (27, 1, 9, 3))\n    del arg0_1\n    del arg1_1\n    del buf2\n    buf5 = torch.ops.mkldnn._convolution_pointwise(buf4, arg2_1, arg3_1, (0, 0), (1, 1), (1, 1), 1, 'none', [], '')\n    assert_size_stride(buf5, (128, 3, 3, 3), (27, 1, 9, 3))\n    del arg2_1\n    del arg3_1\n    return (buf3, buf5, )\n```\n\nAfter this PR, the generated  code will remove the redundant memory copy:\n\n```\nkernel_cpp_0 = async_compile.cpp('''\n#include \"/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h\"\nextern \"C\" void kernel(const float* __restrict__ in_ptr0,\n                       float* __restrict__ out_ptr0)\n{\n    #pragma GCC ivdep\n    for(long i0=0; i0<128; i0+=1)\n    {\n        #pragma GCC ivdep\n        for(long i1=0; i1<3; i1+=1)\n        {\n            #pragma GCC ivdep\n            for(long i2=0; i2<3; i2+=1)\n            {\n                #pragma GCC ivdep\n                for(long i3=0; i3<3; i3+=1)\n                {\n                    {\n                        {\n                            auto tmp0 = in_ptr0[i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp1 = in_ptr0[3 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp3 = in_ptr0[6 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp5 = in_ptr0[21 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp7 = in_ptr0[24 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp9 = in_ptr0[27 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp11 = in_ptr0[42 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp13 = in_ptr0[45 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp15 = in_ptr0[48 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp2 = (tmp0 != tmp0) ? tmp0 : std::max(tmp1, tmp0);\n                            auto tmp4 = (tmp2 != tmp2) ? tmp2 : std::max(tmp3, tmp2);\n                            auto tmp6 = (tmp4 != tmp4) ? tmp4 : std::max(tmp5, tmp4);\n                            auto tmp8 = (tmp6 != tmp6) ? tmp6 : std::max(tmp7, tmp6);\n                            auto tmp10 = (tmp8 != tmp8) ? tmp8 : std::max(tmp9, tmp8);\n                            auto tmp12 = (tmp10 != tmp10) ? tmp10 : std::max(tmp11, tmp10);\n                            auto tmp14 = (tmp12 != tmp12) ? tmp12 : std::max(tmp13, tmp12);\n                            auto tmp16 = (tmp14 != tmp14) ? tmp14 : std::max(tmp15, tmp14);\n                            out_ptr0[i3 + (3*i2) + (9*i1) + (27*i0)] = tmp16;\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n''')\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1 = args\n    args.clear()\n    buf0 = empty_strided((128, 3, 3, 3), (27, 1, 9, 3), device='cpu', dtype=torch.float32)\n    kernel_cpp_0(c_void_p(arg4_1.data_ptr()), c_void_p(buf0.data_ptr()))\n    del arg4_1\n    buf2 = torch.ops.mkldnn._convolution_pointwise(buf0, arg0_1, arg1_1, (0, 0), (1, 1), (1, 1), 1, 'none', [], '')\n    assert_size_stride(buf2, (128, 3, 3, 3), (27, 1, 9, 3))\n    del arg0_1\n    del arg1_1\n    buf3 = torch.ops.mkldnn._convolution_pointwise(buf0, arg2_1, arg3_1, (0, 0), (1, 1), (1, 1), 1, 'none', [], '')\n    assert_size_stride(buf3, (128, 3, 3, 3), (27, 1, 9, 3))\n    del arg2_1\n    del arg3_1\n    return (buf2, buf3, )\n\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89904\nApproved by: https://github.com/jansel",
    "Number of deleted lines": 3,
    "Deleted lines": "-            if isinstance(\n-                x.get_layout(), FlexibleLayout\n-            ) and is_stride_order_storage_and_layout(x, order):",
    "Added lines": "+            if isinstance(x.get_layout(), FlexibleLayout):",
    "Label": "Buggy"
},
{
    "Id": 1191,
    "Library": "pytorch",
    "Date": "2021/12/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2c9dd886afc656a8bfe5d8bbcb601ee5877cee21",
    "Root Cause": "Edge Cases",
    "Bug report": "Modify torch.movedim to handle scalar as no-op (#69537)\n\nSummary:\n`torch.movedim` directly handle the case of a scalar tensor (0-dim) in input as a no-op by returning a view of the input tensor (after all the usual checks for the other parameters)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69537\n\nTest Plan:\nThis code now works fine and res1 is a view of tensor\n```\nimport torch\n\ntensor = torch.rand(torch.Size([]))\nres1 = torch.movedim(tensor, 0, 0)\n```\n\nFixes https://github.com/pytorch/pytorch/issues/69432\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33020014\n\nPulled By: albanD\n\nfbshipit-source-id: b3b2d380d70158bd3b3d6b40c073377104e09007",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // handle the case of scalar tensor as a no-op\n+  if (self_dim == 0)\n+    return self.alias();\n+",
    "Label": "Buggy"
},
{
    "Id": 1192,
    "Library": "pytorch",
    "Date": "2023/06/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2dafa70d61a1a5af849ab79c7aed4c84686337a0",
    "Root Cause": "Type Checking",
    "Bug report": "Add a little more error checking to minifier (#103057)\n\nPrompted by https://github.com/pytorch/pytorch/issues/101408\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103057\nApproved by: https://github.com/bdhirsh",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    def __post_init__(self):\n+        ph_nodes = get_placeholders(self.graph)\n+        assert len(ph_nodes) == len(self.inps)\n+\n+    assert isinstance(inps, (tuple, list))\n+",
    "Label": "Buggy"
},
{
    "Id": 1193,
    "Library": "pytorch",
    "Date": "2022/04/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/37dea0454dd310cfe443859f717862657df6b753",
    "Root Cause": "Computation Graph",
    "Bug report": "[quant] add checking number of args when checking observer in same graph (#75460)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75460\n\nadd checking for number of args checking observer in same graph\n\nTest Plan:\npython3 test/test_quantization.py TestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D35479504\n\nfbshipit-source-id: d7dc38a27fdf8e0b236b6976d484b0701c61184c\n(cherry picked from commit 45542f796f5e6f6259f3ec647dbd2a9fa69ababc)",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if isinstance(node.args[0], Node):",
    "Added lines": "+    if len(node.args) > 0 and isinstance(node.args[0], Node):",
    "Label": "Buggy"
},
{
    "Id": 1194,
    "Library": "pytorch",
    "Date": "2023/05/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3ef4d697df5bfdbd27dfc7a79c0679da2b87e3af",
    "Root Cause": "Backend Type",
    "Bug report": "[c10d] default backend need to check for nccl availability (#102470)\n\nAs titled, we can only initialize nccl backend when NCCL is available\nPull Request resolved: https://github.com/pytorch/pytorch/pull/102470\nApproved by: https://github.com/Skylion007, https://github.com/XilunWu",
    "Number of deleted lines": 4,
    "Deleted lines": "-            self.device_backend_map = {\n-                \"cpu\": Backend.GLOO,\n-                \"cuda\": Backend.NCCL,\n-            }",
    "Added lines": "+            self.device_backend_map = {\"cpu\": Backend.GLOO}\n+            if is_nccl_available():\n+                self.device_backend_map[\"cuda\"] = Backend.NCCL",
    "Label": "Buggy"
},
{
    "Id": 1195,
    "Library": "pytorch",
    "Date": "2023/01/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/40d6f2a02027023216607adb892d3b9c7493904c",
    "Root Cause": "Type Checking",
    "Bug report": "Update sdp_utils to check gradmode and subclassed tensors (#92323)\n\n# Summary\nFix up the grad check test to check for subclassed tensors and gradmode\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92323\nApproved by: https://github.com/soulitzer",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (params.query.requires_grad() || params.key.requires_grad() || params.value.requires_grad()) {",
    "Added lines": "+#include <ATen/TensorSubclassLikeUtils.h>\n+  bool any_tensors_are_subclass =\n+      at::areAnyTensorSubclassLike({params.query, params.key, params.value});\n+  const bool any_inputs_require_grad = params.query.requires_grad() ||\n+      params.key.requires_grad() || params.value.requires_grad();\n+  const bool gradmode_enabled = at::GradMode::is_enabled();\n+  if ((any_inputs_require_grad && gradmode_enabled) || any_tensors_are_subclass) {",
    "Label": "Buggy"
},
{
    "Id": 1196,
    "Library": "pytorch",
    "Date": "2022/02/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/41ad221751e57c2d2ccc82b431f56d6ed62e1741",
    "Root Cause": "Edge Cases",
    "Bug report": "[PyTorch] MHA: fix contiguity assumption in transform_bias_rescale_qkv (#72465)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72465\n\nThis code path incorrectly assumed input tensors were contiguous. Now we check that.\nghstack-source-id: 149201476\n\nTest Plan: CI\n\nReviewed By: ngimel\n\nDifferential Revision: D34007665\n\nfbshipit-source-id: c43438f2495e32304ea3f7846e01eceb4a9448f7\n(cherry picked from commit 0767b225f23846c1636ac3622f46b0c5ec071d96)",
    "Number of deleted lines": 3,
    "Deleted lines": "-  AT_DISPATCH_FLOATING_TYPES_AND2(\n-        scalar_t* qkv_data = qkv.data_ptr<scalar_t>();\n-        scalar_t* qkv_bias_data = qkv_bias.data_ptr<scalar_t>();",
    "Added lines": "+  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(q_k_v.is_contiguous());\n+  const auto qkv_contig = qkv.expect_contiguous();\n+  const auto qkv_bias_contig = qkv_bias.expect_contiguous();\n+ AT_DISPATCH_FLOATING_TYPES_AND2(\n+        scalar_t* qkv_data = qkv_contig->data_ptr<scalar_t>();\n+        scalar_t* qkv_bias_data = qkv_bias_contig->data_ptr<scalar_t>();\n+  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(q_k_v_s.size() == 3);",
    "Label": "Buggy"
},
{
    "Id": 1197,
    "Library": "pytorch",
    "Date": "2023/03/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/45296f87ec865a7a500a6fd98353035c040d0cb7",
    "Root Cause": "Edge Cases",
    "Bug report": "Fix for verify_dynamo on ROCm (#97013)\n\nPrior to this change ROCm was not exiting check_cuda, causing an exception at packaging.version.parse(torch.version.cuda), this change exits check_cuda if torch.version.cuda is None\n\n```\npython verify_dynamo.py\n\nPython version: 3.9.16\n`torch` version: 2.1.0a0+git2b2f10c\nCUDA version: None\nROCM version: 5.4\n\nAll required checks passed\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97013\nApproved by: https://github.com/jithunnair-amd, https://github.com/malfet, https://github.com/kit1980",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if not torch.cuda.is_available():",
    "Added lines": "+    if not torch.cuda.is_available() or torch.version.hip is not None:",
    "Label": "Buggy"
},
{
    "Id": 1198,
    "Library": "pytorch",
    "Date": "2021/11/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/45d5b3248b430aca70111316accd165954464589",
    "Root Cause": "Edge Cases",
    "Bug report": "Fixed C++ BatchNorm pretty_print() with optional momentum (#67335)\n\nSummary:\nSummary : Inserted a check for the momentum and print  \"None\" in case is not defined. See  https://github.com/pytorch/pytorch/issues/65143\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67335\n\nTest Plan:\nThe code below now prints `torch::nn::BatchNorm2d(128, eps=1e-05, momentum=None, affine=true, track_running_stats=true)` without generating errors.\n```\ntorch::nn::BatchNorm2d m(torch::nn::BatchNormOptions(128).momentum(c10::nullopt));\nstd::cerr << *m << \"\\n\";\n```\nFixes https://github.com/pytorch/pytorch/issues/65143\n\nReviewed By: mruberry\n\nDifferential Revision: D32067820\n\nPulled By: ngimel\n\nfbshipit-source-id: f40f9bbe090aa78e00f6c3a57deae393d946b88d",
    "Number of deleted lines": 1,
    "Deleted lines": "-         << \"momentum=\" << this->options.momentum().value() << \", \"",
    "Added lines": "+         << \"momentum=\";\n+\n+  if (this->options.momentum().has_value()) {\n+      stream << this->options.momentum().value();\n+  } else {\n+      stream << \"None\";\n+  }\n+\n+   stream << \", \"",
    "Label": "Buggy"
},
{
    "Id": 1199,
    "Library": "pytorch",
    "Date": "2023/10/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/468a73f0e3527c52495c864c7d48dc26684f6c0b",
    "Root Cause": "Type Checking",
    "Bug report": "Support Numpy ints in the `torch.nn.functional.interpolate` dtype check (#110778)\n\nIn https://github.com/pytorch/pytorch/pull/99243, a check was added to ensure the `size` only contained integers.\n\nThis PR updates the check to also include numpy integers based on this comment (cc @kit1980): https://github.com/pytorch/pytorch/pull/99243#issuecomment-1646736646. Similar to the other commenter, I also ran into issues where existing software broke due to this after upgrading to PT2.1:\n\n```\n                if not torch.jit.is_scripting():\n                    if not all(_is_integer(x) for x in size):\n>                       raise TypeError(\n                            \"expected size to be one of int or Tuple[int] or Tuple[int, int] or \"\n                            f\"Tuple[int, int, int], but got size with types {[type(x) for x in size]}\"\n                        )\nE                       TypeError: expected size to be one of int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], but got size with types [<class 'numpy.int64'>, <class 'numpy.int64'>]\n\n/conda-env/lib/python3.8/site-packages/torch/nn/functional.py:3924: TypeError\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110778\nApproved by: https://github.com/mikaylagawarecki",
    "Number of deleted lines": 1,
    "Deleted lines": "-    Will return True for int, SymInt and Tensors with integer elements.",
    "Added lines": "+try:\n+    import numpy as np\n+except ModuleNotFoundError:\n+    np = None\n+\n+    Will return True for int, SymInt, Numpy integers and Tensors with integer elements.\n+    if np is not None and isinstance(x, np.integer):\n+        return True",
    "Label": "Buggy"
},
{
    "Id": 1200,
    "Library": "pytorch",
    "Date": "2022/10/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4839f73f329b38819e6f69a8662d61dc36558e52",
    "Root Cause": "Others",
    "Bug report": "Fix incorrect tensor storage check  (#86845)\n\nFix incorrect tensor storage check\n\nThis change contains an incorrect check for storage: https://github.com/pytorch/pytorch/pull/86557\n**self.storage is not None**\nshould have been:\n**not torch._C._has_storage(self)**\n\nThese fixes were run through the DirectML test suite, and confirm the check is now working correctly.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86845\nApproved by: https://github.com/martinb35, https://github.com/bdhirsh",
    "Number of deleted lines": 3,
    "Deleted lines": "-\n-                or (self.storage is None and self.device.type == \"privateuseone\")\n-            self.storage is None and self.device.type == \"privateuseone\"",
    "Added lines": "+                or (\n+                    not torch._C._has_storage(self)\n+                    and self.device.type == \"privateuseone\"\n+                )\n+            not torch._C._has_storage(self) and self.device.type == \"privateuseone\"",
    "Label": "Buggy"
},
{
    "Id": 1201,
    "Library": "pytorch",
    "Date": "2023/07/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/48a49b2683ffa21eb1b472e503c129c043c18f87",
    "Root Cause": "Error Message",
    "Bug report": "use more informative error message for ConstandPad2d/3d (#104762)\n\nFixes #104508\n\nAs discussed in #104508, the current error message for `torch.nn.ConstantPad2d` and `torch.nn.ConstantPad3d` is misleading, this PR fixes the problem.\nThe fixed error message is shown below:\nFor `torch.nn.ConstantPad2d`:\n<img width=\"619\" alt=\"image\" src=\"https://github.com/pytorch/pytorch/assets/6964699/dd15f42a-b6ad-4c6d-aa41-f26d08144189\">\nFor `torch.nn.ConstantPad3d`:\n<img width=\"630\" alt=\"image\" src=\"https://github.com/pytorch/pytorch/assets/6964699/ac99b80f-73c1-4d7f-b9a1-74bf45ee4c21\">\n\ncc:\n@mikaylagawarecki Please help me check this PR, thanks!\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104762\nApproved by: https://github.com/mikaylagawarecki",
    "Number of deleted lines": 1,
    "Deleted lines": "-  TORCH_CHECK(static_cast<int64_t>(pad.size()) <= input_dim * 2, \"Padding length too large\");",
    "Added lines": "+  TORCH_CHECK(static_cast<int64_t>(pad.size()) <= input_dim * 2,\n+              \"Padding length should be less than or equal to two times the input dimension but got padding length \", pad.size(), \" and input of dimension \", input_dim);",
    "Label": "Buggy"
},
{
    "Id": 1202,
    "Library": "pytorch",
    "Date": "2023/12/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/490f2d75700a806bdc6110e881e78493cde163e3",
    "Root Cause": "Device Availability",
    "Bug report": "Skip privateuse1's checkZeroPoints (#114117)\n\nWe want to use ``quantize_per_channel`` to create a quantized tensor, but we found that ``checkZeroPoints`` for ``privateuse1`` backend failed.\n\n``quantize_tensor_per_channel_affine`` will ``checkZeroPoints`` for all backends expect ``CUDA``:\nhttps://github.com/pytorch/pytorch/blob/140c54e6ccc5e97f1b7f1e0fcd3d8c6af7dd2ab2/aten/src/ATen/native/quantized/AffineQuantizer.cpp#L162-L164\n\nHowever, our ``privateuse1`` backend will get a segmentation error if we try to cast our data to int64_t in ``checkZeroPoints``:\nhttps://github.com/pytorch/pytorch/blob/140c54e6ccc5e97f1b7f1e0fcd3d8c6af7dd2ab2/aten/src/ATen/native/quantized/AffineQuantizer.cpp#L82-L88\n\nSo if we can skip ``privateuse1``'s ``checkZeroPoints`` and check this item in the actual device function? What do you think?\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114117\nApproved by: https://github.com/jerryzh168",
    "Number of deleted lines": 4,
    "Deleted lines": "-    if(qtensor.device().type() != c10::DeviceType::CUDA){\n-    }  // for cuda, this check will occur in the actual cuda function\n-    if(qtensor.device().type() != c10::DeviceType::CUDA){\n-    }  // for cuda, this check will occur in the actual cuda function",
    "Added lines": "+    if (qtensor.device().type() != c10::DeviceType::CUDA &&\n+        qtensor.device().type() != c10::DeviceType::PrivateUse1) {\n+    }  // for cuda and privateuse1, this check will occur in the actual device function\n+    if(qtensor.device().type() != c10::DeviceType::CUDA &&\n+       qtensor.device().type() != c10::DeviceType::PrivateUse1){\n+    }  // for cuda and privateuse1, this check will occur in the actual device function",
    "Label": "Buggy"
},
{
    "Id": 1203,
    "Library": "pytorch",
    "Date": "2023/03/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4ab1588d9919bc1a62219a5c2393e0784ddaae70",
    "Root Cause": "Error Message",
    "Bug report": "Enhance error message for dependency check (#96642)\n\nIf python development library is missing when building pytorch from source, cmake will raise the error like:\n```\nCMake Error at cmake/Dependencies.cmake:1079 (if):\n  if given arguments:\n\n    \"VERSION_LESS\" \"3\"\n\n  Unknown arguments specified\n```\n\nit's quite a misleading information that user would consider it's a syntax error or cmake version problem.\n\nThis PR add a check to ensure `PYTHONLIBS_VERSION_STRING` exist before using.\n\nRelated  #87993\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/96642\nApproved by: https://github.com/kit1980",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if(NOT PYTHONLIBS_VERSION_STRING)\n+    message(FATAL_ERROR\n+      \"Python development libraries could not be found.\")\n+  endif()\n+",
    "Label": "Buggy"
},
{
    "Id": 1204,
    "Library": "pytorch",
    "Date": "2023/11/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4d07428edee863e7f5920f0672957a9711a9f0b5",
    "Root Cause": "Boundary Value",
    "Bug report": "Fix for out of bounds read in mobile interpreter FORMAT opcode handler (#110303)\n\nSummary:\nThe FORMAT opcode for the mobile TorchScript interpreter contained an out of bounds read issue leading to memory corruption.\n\nThis change adds an explicit check that the number of inputs passed to the format method called when handling the FORMAT opcode is a valid and within bounds of the stack.\n\nTest Plan: contbuild + OSS signals\n\nDifferential Revision: D49739095\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110303\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (num_inputs == 0 || num_inputs > stack.size()) {\n+    AT_ERROR(\"Invalid number of inputs for format string: \", num_inputs);\n+  }\n+",
    "Label": "Buggy"
},
{
    "Id": 1205,
    "Library": "pytorch",
    "Date": "2023/09/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4ee179c9528c8c6aae17a01f2b0d7e8235219219",
    "Root Cause": "Edge Cases",
    "Bug report": "Fix `ConstantVariable` init method if NumPy is missing (#109388)\n\nBy adding `np is not None` check before `isinstance(value, np.number)`\n\nPartially addresses https://github.com/pytorch/pytorch/issues/109387\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109388\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if isinstance(value, np.number):",
    "Added lines": "+        if np is not None and isinstance(value, np.number):",
    "Label": "Buggy"
},
{
    "Id": 1206,
    "Library": "pytorch",
    "Date": "2023/10/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/52e76a305677dfaf26cd5d59bd1aa239375f833c",
    "Root Cause": "Edge Cases",
    "Bug report": "fix ShardedTensor.gather when shard is empty (#110962)\n\nSummary:\ncurrent ShardedTensor.gather is not working as expectation when the shard is empty on any rank\n\nThe root cause is identified that when a sharded tensor has no placement on a specific rank, the metadata doesn't include that rank's placement which introduces KeyError in :                 ```shard_offset = shard_placement[shard. Metadata][1]```\n\nIt's fixed by adding an empty tensor check.\n\nTest Plan:\nbefore change:\n\nafter change:\n\nDifferential Revision: D50114085\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110962\nApproved by: https://github.com/wz337",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                if src.nelement() == 0 :\n+                    warnings.warn(\"Gathering a tensor with zero elements on rank \" + str(rank))\n+                    return",
    "Label": "Buggy"
},
{
    "Id": 1207,
    "Library": "pytorch",
    "Date": "2023/09/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/55c19a3c6d38a49fe34e008c4c566445c43810f0",
    "Root Cause": "Type Checking",
    "Bug report": "Inductor: Increase multiplier to 3 for Inductor AMP benchmark correctness check (#109097)\n\n**Summary**\nAs reported in https://github.com/pytorch/pytorch/issues/108333, we find some of the models have failed the benchmark's correctness check. However, the end-to-end model's accuracy ([test script](https://gist.github.com/leslie-fang-intel/aac8b3c2b450532fd0517c758bb845e0)) when comparing AMP with FP32 is within a difference of less than 0.1%. Thus, it's possible that the correctness check failures for these models are false alarms. We use multiplier of 3 instead of 2 in this PR to avoid these false alarms. Model end-to-end accuracy test results are:\n\n<html xmlns:v=\"urn:schemas-microsoft-com:vml\"\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\nxmlns:x=\"urn:schemas-microsoft-com:office:excel\"\nxmlns=\"http://www.w3.org/TR/REC-html40\">\n\n<head>\n\n<meta name=ProgId content=Excel.Sheet>\n<meta name=Generator content=\"Microsoft Excel 15\">\n<link id=Main-File rel=Main-File\nhref=\"file:///C:/Users/jiahaofa/AppData/Local/Temp/msohtmlclip1/01/clip.htm\">\n<link rel=File-List\nhref=\"file:///C:/Users/jiahaofa/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml\">\n</head>\n\n<body link=\"#0563C1\" vlink=\"#954F72\">\n\nSPR | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n-- | -- | -- | -- | -- | -- | --\n\u00a0 | FP32 Imperative TOP1 Accuracy | FP32 Imperative TOP5 Accuracy | BF16 AMP Inductor TOP1 Accuracy | BF16 AMP Inductor TOP5 Accuracy | BF16/FP32 Relative Loss TOP1 Accuracy | BF16/FP32 Relative Loss TOP5 Accuracy\ngluon_inception_v3 | 73.262 | 90.774 | 73.256 | 90.802 | -0.01% | 0.03%\nmobilenetv2_100 | 72.89 | 90.996 | 72.826 | 90.946 | -0.09% | -0.05%\nmobilenetv3_large_100 | 75.72 | 92.55 | 75.764 | 92.554 | 0.06% | 0.00%\n\n</body>\n\n</html>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109097\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "Number of deleted lines": 1,
    "Deleted lines": "-                multiplier = 2.0",
    "Added lines": "+\n+                # In the case of using AMP (Automatic Mixed Precision), certain models have\n+                # failed the benchmark's correctness check. However, the end-to-end model's\n+                # accuracy when comparing AMP with FP32 is within a difference of less than 0.1%.\n+                # Thus, it's possible that the correctness check failures for these models are\n+                # false alarms. We use multiplier of 3 instead of 2 to avoid these false alarms.\n+                multiplier = 3.0 if res.dtype == torch.bfloat16 else 2.0",
    "Label": "Buggy"
},
{
    "Id": 1208,
    "Library": "pytorch",
    "Date": "2022/06/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/577e90ae9bf257040acb68da3626d9a64d07bf7a",
    "Root Cause": "Error Message",
    "Bug report": "Improve error message for missing ops (#80005)\n\nThe current error message is ill formed. Example\n\nerror: Following ops cannot be found. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/aten::to.prim_dtype ()\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/80005\nApproved by: https://github.com/cccclai",
    "Number of deleted lines": 2,
    "Deleted lines": "-        \"Following ops cannot be found. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/\",\n-        c10::Join(\", \", unsupported_op_names));",
    "Added lines": "+        \"Following ops cannot be found: [\",\n+        c10::Join(\", \", unsupported_op_names),\n+        \"]. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/c/mobile/\");",
    "Label": "Buggy"
},
{
    "Id": 1209,
    "Library": "pytorch",
    "Date": "2022/02/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5b7c72101ca8e9d4edba1d16b6121ad900ca3936",
    "Root Cause": "Tensor Quantization",
    "Bug report": "[Quant][devs] Removed check for is_quantized in dequantize_cpu_or_cuda (#71958)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71958\n\nThis PR is part of a series of PRs addressing https://github.com/pytorch/pytorch/issues/54150,\nrelated to using dispatcher for calls to quantized backends as opposed to if/else conditionals.\nThis particular PR isn't dispatcher related but does remove the extraneous torch check for a quant tensor\nsince the dispatcher already handles a quantized backend for this particular function\n\nDifferential Revision:\nD33833765\nD33833765\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nPulled By: dzdang\n\nfbshipit-source-id: c3bb531a5c09326bdf724b5185a19ea0a379bba7\n(cherry picked from commit f053b8248f895446f6a9d352de4038df6c6d4b2d)",
    "Number of deleted lines": 1,
    "Deleted lines": "-  TORCH_CHECK(!self.is_quantized());",
    "Added lines": "",
    "Label": "Buggy"
},
{
    "Id": 1210,
    "Library": "pytorch",
    "Date": "2022/08/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/61b9d8fccd3361f21e1f3548c2a9538b62cc7525",
    "Root Cause": "Null Value",
    "Bug report": "[Profiler][Trivial] Add null handling to `AppendOnlyList::copy` memcpy path. (#83963)\n\nIt is apparently undefined behavior to do pointer arithmetic on nullptr. In the case of AppendOnlyList, `next_` will only be null if `end_` is also null and thus the `memcpy` path will only be triggered if `n == 0`. Nonetheless, it is UB to `memcpy(0, 0, 0)`\n\nThe extra null check is in a `C10_LIKELY` block so the extra cost should be negligible, and indeed after dusting off the component microbenchmarks there's no observable difference.\n\nDifferential Revision: [D38969443](https://our.internmc.facebook.com/intern/diff/D38969443/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/83963\nApproved by: https://github.com/slgong-fb",
    "Number of deleted lines": 2,
    "Deleted lines": "-    int n = src.size();\n-    if (C10_LIKELY(next_ + n <= end_)) {",
    "Added lines": "+    size_t n = src.size();\n+    if (C10_LIKELY(next_ && (next_ + n <= end_))) {",
    "Label": "Buggy"
},
{
    "Id": 1211,
    "Library": "pytorch",
    "Date": "2023/09/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/62732bdcdb8b6112e01366d4ad1c2a50e61da1ed",
    "Root Cause": "Computation Graph",
    "Bug report": "[ez][inductor][fx passes] quick fix for invalid nodes (#109234)\n\nSummary: As title.Need to check whether node is valid before fusion\n\nTest Plan: To add test\n\nDifferential Revision: D49241525\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109234\nApproved by: https://github.com/yanboliang",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        and is_node_meta_valid(input)\n+        and is_node_meta_valid(weight)",
    "Label": "Buggy"
},
{
    "Id": 1212,
    "Library": "pytorch",
    "Date": "2022/08/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/63cbdc92a750a667ffdcfbdac563d02db6fd9559",
    "Root Cause": "Type Checking",
    "Bug report": "switching the exact check to isinstance check (#84023)\n\nSimplifying a type check if an object is a SymIntNode in `is_symint_node`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/84023\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 2,
    "Deleted lines": "-  // TODO: switch this to `isinstance`\n-  if (obj.get_type().equal(tp_symn)) {",
    "Added lines": "+  if (py::isinstance(obj, tp_symn)) {",
    "Label": "Buggy"
},
{
    "Id": 1213,
    "Library": "pytorch",
    "Date": "2023/01/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/63e47c68a692c70bc64c49d687f85f7f5cd02ce3",
    "Root Cause": "Edge Cases",
    "Bug report": "[cpp] remove checks from embedding bag impl (#92982)\n\nThese checks incur an H2D sync on every embedding bag forward. Also, the equivalent python code for embedding_bag does not have them. Kill!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92982\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 11,
    "Deleted lines": "-    TORCH_CHECK(\n-        offsets_[0].item<int64_t>() == 0,\n-        \"offsets[0] has to be 0, i.e., the first sequence in the mini-batch has to start from position 0. However, got \",\n-        offsets_[0].item<int64_t>());\n-    TORCH_CHECK(\n-        offsets_[-1].item<int64_t>() <= input_.size(0),\n-        \"offsets[-1] can not be greater than input's length({\",\n-        input_.size(0),\n-        \"}), but got offsets[-1] of {\",\n-        offsets_[-1].item<int64_t>(),\n-        \"}\");",
    "Added lines": "",
    "Label": "Buggy"
},
{
    "Id": 1214,
    "Library": "pytorch",
    "Date": "2022/08/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6592259ea52f45e1fc9a633ccb5b154ba5099334",
    "Root Cause": "Device Availability",
    "Bug report": "[HPU] Enable torch.jit.load for HPU (#81759)\n\nAs per torch.jit.load documentation, all previously saved modules,\nirrespective of their device, are first loaded onto CPU, and then\nare moved to the devices they were saved from. So far, supported\ndevices included CPU and CUDA only. To enable torch.jit.load for\nHPU, additional check for HPU is introduced.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/81759\nApproved by: https://github.com/eellison",
    "Number of deleted lines": 2,
    "Deleted lines": "-      if (device.is_cuda() || device.is_xpu() || device.is_meta()) {\n-            \"supported devices include CPU and CUDA, however got \",",
    "Added lines": "+      if (device.is_cuda() || device.is_xpu() || device.is_meta() ||\n+          device.is_hpu()) {\n+            \"supported devices include CPU, CUDA and HPU, however got \",",
    "Label": "Buggy"
},
{
    "Id": 1215,
    "Library": "pytorch",
    "Date": "2022/01/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/666ff0ae220e1a5c406b0bc5cd43283e1b18b38e",
    "Root Cause": "Edge Cases",
    "Bug report": "Update _create_c10d_store to check port value (#71863)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71863\n\nPort number is int in python, but needs to be uint16_t when called for TCPStore constructor.\n\nRelated to #67172\n\nTest Plan: Imported from OSS\n\nReviewed By: cbalioglu\n\nDifferential Revision: D33793270\n\nPulled By: H-Huang\n\nfbshipit-source-id: 89ab47ec8bd7518f9ecbf7d01871fe059b0e77b1\n(cherry picked from commit 84bff1f5bb11029ff3fcf7a04faa3b9c7b25286a)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    # check if port is uint16_t\n+    if not 0 <= port < 2**16:\n+        raise ValueError(f\"port must have value from 0 to 65535 but was {port}.\")",
    "Label": "Buggy"
},
{
    "Id": 1216,
    "Library": "pytorch",
    "Date": "2023/12/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/66b04e3cb707d30c4698b269c83cb6221848f17a",
    "Root Cause": "Null Value",
    "Bug report": "[nccl flight recorder] nullptr profiling name (#115851)\n\nSometimes profiling name can be a nullptr, which\nthrows on conversion to std::string. This adds a check.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115851\nApproved by: https://github.com/wconstab",
    "Number of deleted lines": 1,
    "Deleted lines": "-        profiling_name,",
    "Added lines": "+        profiling_name == nullptr ? \"\" : profiling_name,",
    "Label": "Buggy"
},
{
    "Id": 1217,
    "Library": "pytorch",
    "Date": "2022/02/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/678c08bb55eef0c2e707a17d0cd6e50f5b9bd427",
    "Root Cause": "Backend Type",
    "Bug report": "[PG Wrapper] Small fix (#72657)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72657\n\n_ProcessGroupWrapper check needs to be gated on Gloo availability,\nthis fails when gloo is not avail_ProcessGroupWrapper check needs to be gated\non Gloo availability, this fails when gloo is not avail.\nghstack-source-id: 148837056\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D34144848\n\nfbshipit-source-id: 42a04918b968247f3259cd2cde5438e1265b04fe\n(cherry picked from commit ba5de989396cb621dffc6ef424938df494eb1be6)",
    "Number of deleted lines": 4,
    "Deleted lines": "-    # It is not expected for PG to be wrapped many times, but support it just\n-    # in case\n-    while isinstance(pg, _ProcessGroupWrapper):\n-        pg = pg.wrapped_pg",
    "Added lines": "+    # Gate PG wrapper check on Gloo availability.\n+    if _GLOO_AVAILABLE:\n+        # It is not expected for PG to be wrapped many times, but support it just\n+        # in case\n+        while isinstance(pg, _ProcessGroupWrapper):\n+            pg = pg.wrapped_pg",
    "Label": "Buggy"
},
{
    "Id": 1218,
    "Library": "pytorch",
    "Date": "2023/10/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6b4c686b9a33a1503a4a4133f9067dd31e0822f7",
    "Root Cause": "Device Version",
    "Bug report": "[aotindutor] Forward fix a performance regression (#110800)\n\nSummary: Forward fix a performance regression caused by https://github.com/pytorch/pytorch/pull/110510. When a model is run once, all those kernel pointers are initialized and removing the if-nullptr check will cause those loadKernel be unnecessarily executed again when we rerun the foward function. Another way to do this is to codegen loadKernel in the initializer, which I may do in a later PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110800\nApproved by: https://github.com/jansel",
    "Number of deleted lines": 2,
    "Deleted lines": "-                f\"\"\"kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n-                f\"\"\"{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"",
    "Added lines": "+            self.writeline(f\"if (kernels.{name} == nullptr) {{\")\n+                f\"\"\"    kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n+            self.writeline(\"}\")\n+            self.writeline(f\"if ({name} == nullptr) {{\")\n+                f\"\"\"    {name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n+            self.writeline(\"}\")",
    "Label": "Buggy"
},
{
    "Id": 1219,
    "Library": "pytorch",
    "Date": "2023/01/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6bf0e3b697ce688bc8325440dea3b51fea571c3d",
    "Root Cause": "Backend Type",
    "Bug report": "[inductor] Check for BackendCompilerFailed on CI (#91634)\n\nSummary: https://github.com/pytorch/pytorch/pull/91283/ skips certain\nrandom triton failure on CI, but we need to check against the\nBackendCompilerFailed exception type.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/91634\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 5,
    "Deleted lines": "-                if self.args.ci and (\n-                    (\n-                        isinstance(e, RuntimeError)\n-                        and \"Internal Triton PTX codegen error\" in str(e)\n-                    or (isinstance(e, KeyError) and \"cubin\" in str(e))",
    "Added lines": "+from torch._dynamo.exc import BackendCompilerFailed\n+                if (\n+                    self.args.ci\n+                    and isinstance(e, BackendCompilerFailed)\n+                    and (\n+                        \"Internal Triton PTX codegen error\" in str(e)\n+                        or \"cubin\" in str(e)",
    "Label": "Buggy"
},
{
    "Id": 1220,
    "Library": "pytorch",
    "Date": "2023/06/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6cc0f1c20c2f87a6c7b0e4abd5419e5007920999",
    "Root Cause": "Null Value",
    "Bug report": "Checking for nullptr in get_model_bytecode_version (#97149)\n\nOne-liner commit to check that the ptr is not null. Just had `test_jit` that had a segfault there.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97149\nApproved by: https://github.com/kit1980",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TORCH_CHECK(data != nullptr, \"Pointer to bytes is null.\");",
    "Label": "Buggy"
},
{
    "Id": 1221,
    "Library": "pytorch",
    "Date": "2022/12/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6f5945e4bb1258d39a2878a08a910fcc8f659d5e",
    "Root Cause": "Device Availability",
    "Bug report": "triton supports devices < 7.0, not 6.0 (#90020)\n\ntriton is still buggy with Pascal devices, so make the error checker reflect that.\n\nAlso, this < 6.0 never worked, as the `has_triton` definition in utils.py was checking >= 7.0.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90020\nApproved by: https://github.com/yanboliang, https://github.com/anijain2305",
    "Number of deleted lines": 2,
    "Deleted lines": "-                if device_props.major < 6:\n-                        f\"Found {device_props.name} which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 6.0, but your device is of CUDA capability {device_props.major}.{device_props.minor}\"  # noqa: B950",
    "Added lines": "+                if device_props.major < 7:\n+                        f\"Found {device_props.name} which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability {device_props.major}.{device_props.minor}\"  # noqa: B950",
    "Label": "Buggy"
},
{
    "Id": 1222,
    "Library": "pytorch",
    "Date": "2022/04/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/75be4f9cdb503d6eff189b2bc5c05d96bff66653",
    "Root Cause": "Others",
    "Bug report": "check tensor has storage before refer to tensor data ptr\n\nIn the exporter dedupe initializers passes, check the tensor has storage before reference to tensor's data_ptr, otherwise it will result in a crash.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76342\nApproved by: https://github.com/BowenBao",
    "Number of deleted lines": 1,
    "Deleted lines": "-      (t1.data_ptr() == t2.data_ptr());",
    "Added lines": "+      (t1.has_storage() && t2.has_storage() && t1.data_ptr() == t2.data_ptr());",
    "Label": "Buggy"
},
{
    "Id": 1223,
    "Library": "pytorch",
    "Date": "2023/04/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7684044b713761abd4f51225dc5d83ce5869562a",
    "Root Cause": "Edge Cases",
    "Bug report": "Add size check before calling .back() in rpc/script_call.cpp (#94297)\n\nHi!\n\nI've been fuzzing different pytorch modules, and found a crash inside one of them.\n\nSpecifically, I'm talking about a module that processes `script_call` rpc requests and a function `ScriptCall::fromIValues(std::vector<at::IValue>& ivalues)`.\n\nRunning this test case causes a crash that occurs when `ivalues.back()` is called [script_call.cpp:90](https://github.com/pytorch/pytorch/blob/abc54f93145830b502400faa92bec86e05422fbd/torch/csrc/distributed/rpc/script_call.cpp#L90). The crash occurs because the vector `ivalues` is empty.\n\nAll tests were performed on this pytorch version: [abc54f93145830b502400faa92bec86e05422fbd](https://github.com/pytorch/pytorch/tree/abc54f93145830b502400faa92bec86e05422fbd)\n\nThe provided patch checks if there are enough elements in the ivalues vector.\n\n### How to reproduce\n\n1. To reproduce the crash, use provided docker: [Dockerfile](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch)\n\n2. Build the container: `docker build -t oss-sydr-fuzz-pytorch-reproduce .`\n\n3. Copy crash file to the current directory:\n\n    - [crash-9f76d4e37a2391136a4ce07d47269db1e063e4b4.zip](https://github.com/pytorch/pytorch/files/10674059/crash-9f76d4e37a2391136a4ce07d47269db1e063e4b4.zip)\n\n4. Run the container: ``docker run --privileged --network host -v `pwd`:/homedir --rm -it oss-sydr-fuzz-pytorch-reproduce /bin/bash``\n\n5. And execute the binary: `/message_deserialize_fuzz /homedir/crash-9f76d4e37a2391136a4ce07d47269db1e063e4b4`\n\nAfter execution completes you will see this stacktrace:\n\n```asan\nAddressSanitizer:DEADLYSIGNAL\n=================================================================\n==57==ERROR: AddressSanitizer: SEGV on unknown address (pc 0x0000008e7b19 bp 0x7ffd2fdded70 sp 0x7ffd2fddec40 T0)\n==57==The signal is caused by a READ memory access.\n==57==Hint: this fault was caused by a dereference of a high value address (see register values below).  Disassemble the provided pc to learn which register was used.\n    #0 0x8e7b19 in c10::IValue::isString() const /pytorch_fuzz/aten/src/ATen/core/ivalue.h:639:27\n    #1 0x8e7b19 in c10::IValue::toStringRef[abi:cxx11]() const /pytorch_fuzz/aten/src/ATen/core/ivalue_inl.h:2179:3\n    #2 0xe04fb58 in torch::distributed::rpc::ScriptCall::fromIValues(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/distributed/rpc/script_call.cpp:90:53\n    #3 0xe0511f0 in torch::distributed::rpc::ScriptCall::fromMessage(torch::distributed::rpc::Message const&) /pytorch_fuzz/torch/csrc/distributed/rpc/script_call.cpp:133:10\n    #4 0xe0ff71e in torch::distributed::rpc::deserializeRequest(torch::distributed::rpc::Message const&) /pytorch_fuzz/torch/csrc/distributed/rpc/utils.cpp:102:14\n    #5 0x602a41 in LLVMFuzzerTestOneInput /message_deserialize_fuzz.cc:192:27\n    #6 0x52ce61 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n    #7 0x516d7c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n    #8 0x51cacb in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n    #9 0x546062 in main /llvm-project/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n    #10 0x7f41e42a8082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082)\n    #11 0x51169d in _start (/message_deserialize_fuzz+0x51169d)\n\nAddressSanitizer can not provide additional info.\nSUMMARY: AddressSanitizer: SEGV /pytorch_fuzz/aten/src/ATen/core/ivalue.h:639:27 in c10::IValue::isString() const\n==57==ABORTING\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94297\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TORCH_INTERNAL_ASSERT(\n+      ivalues.size() > 1,\n+      \"At least 2 IValues are required to build a ScriptCall.\");\n+",
    "Label": "Buggy"
},
{
    "Id": 1224,
    "Library": "pytorch",
    "Date": "2022/09/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7ddf167ba5db277e02f983a6bde2bc3f5fbe1caa",
    "Root Cause": "Edge Cases",
    "Bug report": "Move the asserts in shape functions upsample_nearest_2d op. (#85801)\n\nThe assert check are moved to top and the function now returns out. This is needed by the downstream torch-mlir project to correctly determine the output type.\n\nFixes #ISSUE_NUMBER\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/85801\nApproved by: https://github.com/eellison",
    "Number of deleted lines": 3,
    "Deleted lines": "-        return out\n-        return out\n-    assert 0, \"Either output_size or scale_factors must be presented\"",
    "Added lines": "+\n+    if (scale_factors is None and output_size is None):\n+        assert 0, \"Either output_size or scale_factors must be presented\"\n+\n+\n+    return out",
    "Label": "Buggy"
},
{
    "Id": 1225,
    "Library": "pytorch",
    "Date": "2022/12/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7e9bf2ed860b8b60d252eead4cc457c3fe5f1667",
    "Root Cause": "Computation Graph",
    "Bug report": "When nopython=True, Dynamo can't allow graph breaks. (#90970)\n\nI count the number of sub-graphs (for tiny-GPT2 in huggingface) by\n```\n    class GraphCaptureCompiler:\n        def __init__(self):\n            self.captured_graphs = []\n        def compile(self, gm, example_inputs):\n            self.captured_graphs.append(gm)\n            return gm\n    compiler = GraphCaptureCompiler()\n    torch._dynamo.optimize(compiler, nopython=True)(Wrapper(fn))(*args)\n```\n\nAlthough `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90970\nApproved by: https://github.com/ezyang, https://github.com/jansel",
    "Number of deleted lines": 1,
    "Deleted lines": "-                if self.has_backedge():",
    "Added lines": "+                if self.has_backedge() and self.should_compile_partial_graph():",
    "Label": "Buggy"
},
{
    "Id": 1226,
    "Library": "pytorch",
    "Date": "2023/11/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8198474eb763c8d526ede3418211479c2f4cbd30",
    "Root Cause": "Computation Graph",
    "Bug report": "Fix scope name when parent scope is empty for torch.onnx.export (#112654)\n\nPrevious to this PR, we only checked TorchScript nodes for scope compatibility, skipping their parent's scope reference check.\nThis PR fixes adds a check not only for the node being traversed, but its parents as well\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112654\nApproved by: https://github.com/BowenBao",
    "Number of deleted lines": 1,
    "Deleted lines": "-  while (!parent->isRoot()) {",
    "Added lines": "+  while (isCompatibleScope(parent)) {",
    "Label": "Buggy"
},
{
    "Id": 1227,
    "Library": "pytorch",
    "Date": "2023/06/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8340762211e3b55caa178bac748bd902249f6fc0",
    "Root Cause": "Type Checking",
    "Bug report": "Update lr_scheduler.py to check the type of eta_min (#97003)\n\nAdd float assertion to `eta_min` parameter in `CosineAnnealingWarmRestarts`.\n\nFixes #87757\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97003\nApproved by: https://github.com/janeyx99",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        if not isinstance(eta_min, (float, int)):\n+            raise ValueError(\"Expected float or int eta_min, but got {} of type {}\".format(eta_min, type(eta_min)))",
    "Label": "Buggy"
},
{
    "Id": 1228,
    "Library": "pytorch",
    "Date": "2023/09/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/871b5caae76185cff141c522b3133e7543c8dabf",
    "Root Cause": "Device Availability",
    "Bug report": "Fix hpu deserialization bug (#109499)\n\n# Motivation\nfix hpu deserialization bug. It should check hpu model if and only if location start with hpu. Otherwise, it always raise an AssertError if hpu is not imported. This break the serialization/desirialization functionality abourt other third-party like IPEX.\n\n# Solution\nonly assert hpu model when start with hpu\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109499\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 2,
    "Deleted lines": "-    hpu = getattr(torch, \"hpu\", None)\n-    assert hpu is not None, \"HPU device module is not loaded\"",
    "Added lines": "+        hpu = getattr(torch, \"hpu\", None)\n+        assert hpu is not None, \"HPU device module is not loaded\"",
    "Label": "Buggy"
},
{
    "Id": 1229,
    "Library": "pytorch",
    "Date": "2022/01/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/871e240e6367f94966a3e2f9deefbfa98e314d6d",
    "Root Cause": "Error Message",
    "Bug report": "Improved error message for interpolation (#72066)\n\nSummary:\nDescription:\n- Improved error message for CUDA interpolation with antialiasing\n\njbschlosser could you please check this PR and the wording if the error message is more clear now ? Thank.\nI'm skipping all the tests now and once we are agreed on the wording if any updates are required, I update and restart the tests to ensure nothing is broken.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72066\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D33892729\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 6249c7a1c51aa2e242f4bb8bfbe3f2abab17a8e8\n(cherry picked from commit 44eb5391cf4fed54b379e96dfa9f23ef6ab1ecfa)",
    "Number of deleted lines": 2,
    "Deleted lines": "-            \"Too much shared memory required: \", shmem_size, \" vs \", sharedMemPerBlock);\n-            \"Too much shared memory required: \", shmem_size, \" vs \", sharedMemPerBlock);",
    "Added lines": "+            \"Provided interpolation parameters can not be handled with current algorithm implementation. \",\n+            \"Please reduce the scale factor. Too much shared memory required: \",\n+            shmem_size, \" vs \", sharedMemPerBlock);\n+            \"Provided interpolation parameters can not be handled with current algorithm implementation. \",\n+            \"Please reduce the scale factor. Too much shared memory required: \",\n+            shmem_size, \" vs \", sharedMemPerBlock);",
    "Label": "Buggy"
},
{
    "Id": 1230,
    "Library": "pytorch",
    "Date": "2023/06/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8b37821813b60a3ce2ae92e7a06057183578a450",
    "Root Cause": "Device Type",
    "Bug report": "make balance check in DP only for cuda (#103311)\n\nFixes #103825\n1. if we want to use dp on other device ranther than \"cuda\", this balance  check will raise error, so I make the balance check only effective for `cuda`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103311\nApproved by: https://github.com/kit1980",
    "Number of deleted lines": 1,
    "Deleted lines": "-        _check_balance(self.device_ids)",
    "Added lines": "+        if device_type == \"cuda\":\n+            _check_balance(self.device_ids)",
    "Label": "Buggy"
},
{
    "Id": 1231,
    "Library": "pytorch",
    "Date": "2023/04/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8ee59280d78a4fefc4de0da04b287e067c28de0d",
    "Root Cause": "Edge Cases",
    "Bug report": "Bug - check config for dynamic (#99676)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/99676\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 2,
    "Deleted lines": "-                automatic_dynamic = curr_sizes is None or curr_sizes[i] is None\n-",
    "Added lines": "+                automatic_dynamic = config.automatic_dynamic_shapes and (\n+                    curr_sizes is None or curr_sizes[i] is None\n+                )",
    "Label": "Buggy"
},
{
    "Id": 1232,
    "Library": "pytorch",
    "Date": "2023/11/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/93cea394dee1315c7a85ead7bb7af21363157c4f",
    "Root Cause": "Device Version",
    "Bug report": "CMake: Loosen CUDA consistency check (#113174)\n\nCloses #108931, closes #108932, see also conda-forge/pytorch-cpu-feedstock#203\n\nCurrently we compare `CUDA_INCLUDE_DIRS` and expect exact equality\nwith `CUDAToolkit_INCLUDE_DIR` however this fails in the presense of\nsymbolic links or for split installs where there are multiple include paths.\nGiven that, it makes sense to loosen the requirement to just version\nequality under the assumption that two installs of the same version\nshould still be compatible.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113174\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 4,
    "Deleted lines": "-if(NOT CMAKE_CUDA_COMPILER_VERSION STREQUAL CUDAToolkit_VERSION OR\n-    NOT CUDA_INCLUDE_DIRS STREQUAL CUDAToolkit_INCLUDE_DIR)\n-  message(FATAL_ERROR \"Found two conflicting CUDA installs:\\n\"\n-                      \"V${CUDAToolkit_VERSION} in '${CUDAToolkit_INCLUDE_DIR}'\")",
    "Added lines": "+if(NOT CMAKE_CUDA_COMPILER_VERSION VERSION_EQUAL CUDAToolkit_VERSION)\n+  message(FATAL_ERROR \"Found two conflicting CUDA versions:\\n\"\n+                      \"V${CUDAToolkit_VERSION} in '${CUDAToolkit_INCLUDE_DIRS}'\")",
    "Label": "Buggy"
},
{
    "Id": 1233,
    "Library": "pytorch",
    "Date": "2023/05/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/97245a06e14a5b87a0bca1908d7200603aff2c8c",
    "Root Cause": "Edge Cases",
    "Bug report": "Turn on TORCH_CHECK for NT wrap_buffer (#100596)\n\nTORCH_INTERNAL_ASSERT_DEBUG_ONLY won't be enabled during non-debug builds, but for 1 dimension Tensors the check is cheap enough and not catching this can slow down development a lot.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/100596\nApproved by: https://github.com/drisspg",
    "Number of deleted lines": 5,
    "Deleted lines": "-inline at::Tensor wrap_buffer(\n-    at::Tensor buffer,\n-    at::Tensor nested_sizes) {\n-  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(\n-      buffer.is_contiguous(), \"Given buffer must be contiguous.\");",
    "Added lines": "+inline at::Tensor wrap_buffer(at::Tensor buffer, at::Tensor nested_sizes) {\n+  TORCH_CHECK(\n+      buffer.dim() == 1,\n+      \"Expected given buffer to be 1dim, but got \",\n+      buffer.dim(),\n+      \" instead.\");\n+  TORCH_CHECK(\n+      buffer.is_contiguous(), \"Expected given buffer to be contiguous.\");",
    "Label": "Buggy"
},
{
    "Id": 1234,
    "Library": "pytorch",
    "Date": "2023/05/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/999bae0f54108ffc5b7cf2524a02a83901554b16",
    "Root Cause": "Edge Cases",
    "Bug report": "Add padding check for use_nnpack (#92238)\n\nFixes #90142\nnnp_convolution_output doesn't support the case of input padding > = kernel_size.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92238\nApproved by: https://github.com/jgong5, https://github.com/ganler",
    "Number of deleted lines": 1,
    "Deleted lines": "-           (at::symint::size<T>(weight, 2) < 17) && (at::symint::size<T>(weight, 3) < 17) // NNPACK only supports kernels up to 16x16",
    "Added lines": "+           (at::symint::size<T>(weight, 2) < 17) && (at::symint::size<T>(weight, 3) < 17) && // NNPACK only supports kernels up to 16x16\n+           (padding[0] < at::symint::size<T>(weight, 2)) && (padding[1] < at::symint::size<T>(weight, 3)) // NNPACK only supports padding < kernel_size. See https://github.com/pytorch/pytorch/issues/90142.",
    "Label": "Buggy"
},
{
    "Id": 1235,
    "Library": "pytorch",
    "Date": "2022/05/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9bcb4de16878073896d8743fbd70d5abe28b595a",
    "Root Cause": "Edge Cases",
    "Bug report": "check parameter k and l\n\nFixes #76715\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76719\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 1,
    "Deleted lines": "-  TORCH_CHECK((unsigned)l < dims.size());",
    "Added lines": "+  TORCH_CHECK((unsigned)l < dims.size() && (unsigned)k < dims.size());",
    "Label": "Buggy"
},
{
    "Id": 1236,
    "Library": "pytorch",
    "Date": "2022/12/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9e314bd8224f93b4ba1f9e4c065150e47a2de2cc",
    "Root Cause": "Type Checking",
    "Bug report": "[dtensor] handle the case where output of op is Optional[Tensor] (#90241)\n\nObserved by @aazzolini, some op might have Optional[Tensor] returns\nwhere it return None (i.e. native_layer_norm_backward), it's a mismatch\nbetween C++ aten op signature and python None, but we need to handle it\nin the python side\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90241\nApproved by: https://github.com/aazzolini",
    "Number of deleted lines": 1,
    "Deleted lines": "-OutputSpecType = Optional[Union[DTensorSpec, Sequence[DTensorSpec]]]",
    "Added lines": "+OutputSpecType = Optional[Union[DTensorSpec, Sequence[Optional[DTensorSpec]]]]\n+\n+        # NOTE: local results might return Optional Tensor from ATen op, so we need to\n+        # handle that case and make sure we don't wrap None with DTensor.\n+        # (i.e. native_layer_norm.backward)\n+            if e is not None and s is not None else None",
    "Label": "Buggy"
},
{
    "Id": 1237,
    "Library": "pytorch",
    "Date": "2023/11/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a076a74f1118da171cf70d00d1de4abbe27cf85d",
    "Root Cause": "Device Availability",
    "Bug report": "[Nested Tensor] Add xpu device in assertion for nested tensor creation (#114664)\n\nAdd xpu device checking in nested tensor creation\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114664\nApproved by: https://github.com/jgong5, https://github.com/xunnanxu",
    "Number of deleted lines": 2,
    "Deleted lines": "-      storage_device.is_cpu() || storage_device.is_cuda() || storage_device.is_privateuseone(),\n-      \"NestedTensorImpl storage must be either CUDA, CPU or \", get_privateuse1_backend(), \" but got \",",
    "Added lines": "+      storage_device.is_cpu() || storage_device.is_cuda() || storage_device.is_xpu() || storage_device.is_privateuseone(),\n+      \"NestedTensorImpl storage must be either CUDA, CPU, XPU or \", get_privateuse1_backend(), \" but got \",",
    "Label": "Buggy"
},
{
    "Id": 1238,
    "Library": "pytorch",
    "Date": "2023/04/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a3701b674046bcefb5927a6643364b186f77dbcf",
    "Root Cause": "Device Availability",
    "Bug report": "fix backward bug for custom device (#98586)\n\nFixes #ISSUE_NUMBER\nIn the backward on some device , it may get an error to get device index because of exchange a new thread.\nSo just set_device and check the device index in `setDevice`  func may be better for some many kinds of devices.\nFor CUDA, the device index check is also included in `setDevice`  func.https://github.com/pytorch/pytorch/blob/master/c10/cuda/impl/CUDAGuardImpl.h#:~:text=%7D-,void%20setDevice(Device%20d)%20const%20override%20%7B,%7D,-void%20uncheckedSetDevice(Device\n```\nvoid setDevice(Device d) const override {\n    TORCH_INTERNAL_ASSERT(d.is_cuda());\n    Device current_device = getDevice();\n    if (current_device != d) {\n      C10_CUDA_CHECK(cudaSetDevice(d.index()));\n    }\n  }\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98586\nApproved by: https://github.com/albanD",
    "Number of deleted lines": 2,
    "Deleted lines": "-      if (impl && device < impl->deviceCount() &&\n-          impl->getDevice().index() != device) {",
    "Added lines": "+      if (impl && device < impl->deviceCount()) {",
    "Label": "Buggy"
},
{
    "Id": 1239,
    "Library": "pytorch",
    "Date": "2023/06/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a5ca445f7953711bc90c111c3cad2ec87f02e74a",
    "Root Cause": "Null Value",
    "Bug report": "Check for corrupted ivalues. (#104243)\n\nHi! We've been fuzzing torchvision project with [sydr-fuzz](https://github.com/ispras/oss-sydr-fuzz).\nWe've found a SEGV error at address 0x0 at `vector.h:163` in pytorch third-party project flatbuffers.\n\nThe error occurs because the `ivalues` field of flatbuffer module can be null, so the corresponding check must be inserted.\n\ntorchvision version: 9d0a93eee90bf7c401b74ebf9c8be80346254f15\n\npytorch version: 0f1621df1a0a73956c7ce4e2f72f069e610e0137\n\nOS: Ubuntu 20.04\n\nHow to reproduce\n\n1. Build docker from [here](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/torchvision) and run the container:\n\n        sudo docker build -t oss-sydr-fuzz-torchvision .\n        sudo docker run --privileged --rm -v `pwd`:/fuzz -it oss-sydr-fuzz-torchvision /bin/bash\n\n2. Run the target on this input:\n[malformed-module.txt](https://github.com/pytorch/pytorch/files/11879653/malformed-module.txt)\n\n        /encode_png_fuzz malformed-module.txt\n\n3. You will see the following output:\n\n        AddressSanitizer:DEADLYSIGNAL\n        =================================================================\n        ==1154==ERROR: AddressSanitizer: SEGV on unknown address 0x000000000000 (pc 0x00000d17cc61 bp 0x7ffcbe8637f0 sp 0x7ffcbe863660 T0)\n        ==1154==The signal is caused by a READ memory access.\n        ==1154==Hint: address points to the zero page.\n            #0 0xd17cc61 in flatbuffers::Vector<flatbuffers::Offset<torch::jit::mobile::serialization::IValue> >::size() const /pytorch/third_party/flatbuffers/include/flatbuffers/vector.h:163:48\n            #1 0xd17cc61 in torch::jit::(anonymous namespace)::FlatbufferLoader::parseModule(torch::jit::mobile::serialization::Module*) /pytorch/torch/csrc/jit/mobile/flatbuffer_loader.cpp:293:32\n            #2 0xd17dd23 in torch::jit::parse_and_initialize_mobile_module_for_jit(void*, unsigned long, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, std::vector<c10::IValue, std::allocator<c10::IValue> >&, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >*) /pytorch/torch/csrc/jit/mobile/flatbuffer_loader.cpp:809:29\n            #3 0xdd661b4 in torch::jit::parse_and_initialize_jit_module(std::shared_ptr<char>, unsigned long, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, c10::optional<c10::Device>) /pytorch/torch/csrc/jit/serialization/import.cpp:345:28\n            #4 0xdd6b24a in torch::jit::_load_jit_module_from_bytes(std::shared_ptr<char>, unsigned long, std::shared_ptr<torch::jit::CompilationUnit>, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:547:14\n            #5 0xdd6c6df in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:443:10\n            #6 0xdd6c1c7 in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:421:10\n            #7 0xdd6dce4 in torch::jit::load(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:503:10\n            #8 0xf2d3f75 in torch::serialize::InputArchive::load_from(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>) /pytorch/torch/csrc/api/src/serialize/input-archive.cpp:97:13\n            #9 0x60509c in void torch::load<at::Tensor, char*&>(at::Tensor&, char*&) /pytorch/torch/include/torch/csrc/api/include/torch/serialize.h:107:11\n            #10 0x6036be in LLVMFuzzerTestOneInput /vision/encode_png.cc:38:5\n            #11 0x66b041 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n            #12 0x6544cc in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n            #13 0x65a61b in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n            #14 0x654222 in main /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n            #15 0x7f0c87b9c082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082) (BuildId: 1878e6b475720c7c51969e69ab2d276fae6d1dee)\n            #16 0x542cdd in _start (/encode_png_fuzz+0x542cdd)\n\n        AddressSanitizer can not provide additional info.\n        SUMMARY: AddressSanitizer: SEGV /pytorch/third_party/flatbuffers/include/flatbuffers/vector.h:163:48 in flatbuffers::Vector<flatbuffers::Offset<torch::jit::mobile::serialization::IValue> >::size() const\n        ==1154==ABORTING\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104243\nApproved by: https://github.com/kit1980",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")",
    "Label": "Buggy"
},
{
    "Id": 1240,
    "Library": "pytorch",
    "Date": "2023/06/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a69f427f957a37eee9c1dd5df681f30ab38ed3e4",
    "Root Cause": "Type Checking",
    "Bug report": "aten: Ensure dim is size_t (#104201)\n\nAttempts to fix failures introduced in https://github.com/pytorch/pytorch/pull/103930 (example failures: https://github.com/pytorch/pytorch/actions/runs/5363450214/jobs/9731034104)\n\n<!--\ncopilot:all\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at 67d5076</samp>\n\n### Summary\n\ud83d\udd27\ud83d\udea8\ud83d\udea6\n\n<!--\n1.  \ud83d\udd27 (wrench) - This emoji can be used to indicate a bug fix or a minor improvement to the code quality or performance.\n2.  \ud83d\udea8 (rotating light) - This emoji can be used to indicate a change that affects the error handling or validation logic of the code, or that adds or modifies a test case.\n3.  \ud83d\udea6 (vertical traffic light) - This emoji can be used to indicate a change that affects the control flow or branching logic of the code, or that adds or modifies a condition or assertion.\n-->\nFix a compiler warning in `Expand.cpp` by casting a tensor dimension to `size_t`. This improves the code quality and correctness of the `expand` function for the Vulkan backend.\n\n> _`expand` tensor_\n> _cast `dim()` to `size_t`_\n> _autumn leaves warning_\n\n### Walkthrough\n*  Cast `self.dim()` to `size_t` to avoid signed-unsigned comparison warning in `expand` function ([link](https://github.com/pytorch/pytorch/pull/104201/files?diff=unified&w=0#diff-c175e908cbcb8595b22696e672b526202ed3a4a11341603c1522397e499b5c2bL29-R29))\n\n<details>\n<summary> Fix done using chatgpt </summary>\n\n![Screenshot 2023-06-26 at 11 52 14 AM](https://github.com/pytorch/pytorch/assets/1700823/95c141e5-36b6-4916-85ca-85415bcc507f)\n\n</details>\nSigned-off-by: Eli Uriegas <eliuriegas@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104201\nApproved by: https://github.com/lucylq, https://github.com/huydhn, https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-      self.dim() <= output_size.size(),",
    "Added lines": "+      static_cast<size_t>(self.dim()) <= output_size.size(),",
    "Label": "Buggy"
},
{
    "Id": 1241,
    "Library": "pytorch",
    "Date": "2023/02/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a6a433aecd0da3ac3c8d49cb36091623f1b5ec9e",
    "Root Cause": "Edge Cases",
    "Bug report": "Add stack emptiness checks inside interpreter.cpp (#94298)\n\nHi!\n\nI've been fuzzing different pytorch modules, and found a few crashes inside one of them.\n\nSpecifically, I'm talking about a module for interpreting the JIT code and a function called `InterpreterState::run()`. Running this function with provided crash file results in a crash, which occurs while calling `dim()` on a `stack` with 0 elements ([line-686](https://github.com/pytorch/pytorch/blob/abc54f93145830b502400faa92bec86e05422fbd/torch/csrc/jit/runtime/interpreter.cpp#L686)). The crash itself occurs later, when std::move is called with incorrect value of type `IValue`.\n\nThe second crash is similar and occurs on [line 328](https://github.com/pytorch/pytorch/blob/abc54f93145830b502400faa92bec86e05422fbd/torch/csrc/jit/runtime/interpreter.cpp#LL328C15-L328C48), where `reg(inst.X + i - 1) = pop(stack);` is executed. The error here is the same, `Stack stack` might not contain enough elements.\n\nThe third crash occurs on [line 681](https://github.com/pytorch/pytorch/blob/abc54f93145830b502400faa92bec86e05422fbd/torch/csrc/jit/runtime/interpreter.cpp#L681). The problem here is the same as for previous crashes. There are not enough elements in the stack.\n\nIn addition to these places, there are many others (in the same function) where border checking is also missing. I am not sure what is the best way to fix these problems, however I suggest adding a boundary check inside each of these case statement.\n\nAll tests were performed on this pytorch version: [abc54f93145830b502400faa92bec86e05422fbd](https://github.com/pytorch/pytorch/tree/abc54f93145830b502400faa92bec86e05422fbd)\n\n### How to reproduce\n\n1. To reproduce the crash, use provided docker: [Dockerfile](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch)\n\n2. Build the container: `docker build -t oss-sydr-fuzz-pytorch-reproduce .`\n\n3. Copy these crash files to the current directory:\n\n    - [crash-4f18c5128c9a5a94343fcbbd543d7d6b02964471.zip](https://github.com/pytorch/pytorch/files/10674143/crash-4f18c5128c9a5a94343fcbbd543d7d6b02964471.zip)\n    - [crash-55384dd7c9689ed7b94ac6697cc43db4e0dd905a.zip](https://github.com/pytorch/pytorch/files/10674147/crash-55384dd7c9689ed7b94ac6697cc43db4e0dd905a.zip)\n    - [crash-06b6125d01c5f91fae112a1aa7dcc76d71b66576.zip](https://github.com/pytorch/pytorch/files/10674152/crash-06b6125d01c5f91fae112a1aa7dcc76d71b66576.zip)\n\n4. Run the container: ``docker run --privileged --network host -v `pwd`:/homedir --rm -it oss-sydr-fuzz-pytorch-reproduce /bin/bash``\n\n5. And execute the binary: `/jit_differential_fuzz /homedir/crash-4f18c5128c9a5a94343fcbbd543d7d6b02964471`\n\nAfter execution completes you will see this stacktrace:\n\n```asan\n=36==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x6060001657f8 at pc 0x00000060bc91 bp 0x7fff00b33380 sp 0x7fff00b33378\nREAD of size 4 at 0x6060001657f8 thread T0\n    #0 0x60bc90 in c10::IValue::IValue(c10::IValue&&) /pytorch_fuzz/torch/include/ATen/core/ivalue.h:214:43\n    #1 0xc20e7cd in torch::jit::pop(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/aten/src/ATen/core/stack.h:102:12\n    #2 0xc20e7cd in torch::jit::dim(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/mobile/promoted_prim_ops.cpp:119:20\n    #3 0xc893060 in torch::jit::InterpreterStateImpl::runImpl(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/runtime/interpreter.cpp:686:13\n    #4 0xc85c47b in torch::jit::InterpreterStateImpl::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/runtime/interpreter.cpp:1010:9\n    #5 0x600598 in runGraph(std::shared_ptr<torch::jit::Graph>, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) /jit_differential_fuzz.cc:66:38\n    #6 0x601d99 in LLVMFuzzerTestOneInput /jit_differential_fuzz.cc:107:25\n    #7 0x52ccf1 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n    #8 0x516c0c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n    #9 0x51c95b in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n    #10 0x545ef2 in main /llvm-project/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n    #11 0x7f9ec069a082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082)\n    #12 0x51152d in _start (/jit_differential_fuzz+0x51152d)\n\n0x6060001657f8 is located 8 bytes to the left of 64-byte region [0x606000165800,0x606000165840)\nallocated by thread T0 here:\n    #0 0x5fd42d in operator new(unsigned long) /llvm-project/compiler-rt/lib/asan/asan_new_delete.cpp:95:3\n    #1 0xa16ab5 in void std::vector<c10::IValue, std::allocator<c10::IValue> >::_M_realloc_insert<c10::IValue&>(__gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue, std::allocator<c10::IValue> > >, c10::IValue&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:440:33\n    #2 0xa168f1 in c10::IValue& std::vector<c10::IValue, std::allocator<c10::IValue> >::emplace_back<c10::IValue&>(c10::IValue&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:121:4\n    #3 0xc89b53c in torch::jit::InterpreterStateImpl::runImpl(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/runtime/interpreter.cpp:344:19\n    #4 0xc85c47b in torch::jit::InterpreterStateImpl::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/runtime/interpreter.cpp:1010:9\n    #5 0x600598 in runGraph(std::shared_ptr<torch::jit::Graph>, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) /jit_differential_fuzz.cc:66:38\n    #6 0x601d99 in LLVMFuzzerTestOneInput /jit_differential_fuzz.cc:107:25\n    #7 0x52ccf1 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n    #8 0x516c0c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n    #9 0x51c95b in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n    #10 0x545ef2 in main /llvm-project/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n    #11 0x7f9ec069a082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082)\n\nSUMMARY: AddressSanitizer: heap-buffer-overflow /pytorch_fuzz/torch/include/ATen/core/ivalue.h:214:43 in c10::IValue::IValue(c10::IValue&&)\nShadow bytes around the buggy address:\n  0x0c0c80024aa0: fd fd fd fd fd fd fd fa fa fa fa fa 00 00 00 00\n  0x0c0c80024ab0: 00 00 00 fa fa fa fa fa fd fd fd fd fd fd fd fd\n  0x0c0c80024ac0: fa fa fa fa fd fd fd fd fd fd fd fd fa fa fa fa\n  0x0c0c80024ad0: fd fd fd fd fd fd fd fd fa fa fa fa fd fd fd fd\n  0x0c0c80024ae0: fd fd fd fd fa fa fa fa 00 00 00 00 00 00 00 00\n=>0x0c0c80024af0: fa fa fa fa fd fd fd fd fd fd fd fd fa fa fa[fa]\n  0x0c0c80024b00: 00 00 00 00 00 00 00 00 fa fa fa fa fa fa fa fa\n  0x0c0c80024b10: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0c80024b20: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0c80024b30: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0c80024b40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\nShadow byte legend (one shadow byte represents 8 application bytes):\n  Addressable:           00\n  Partially addressable: 01 02 03 04 05 06 07\n  Heap left redzone:       fa\n  Freed heap region:       fd\n  Stack left redzone:      f1\n  Stack mid redzone:       f2\n  Stack right redzone:     f3\n  Stack after return:      f5\n  Stack use after scope:   f8\n  Global redzone:          f9\n  Global init order:       f6\n  Poisoned by user:        f7\n  Container overflow:      fc\n  Array cookie:            ac\n  Intra object redzone:    bb\n  ASan internal:           fe\n  Left alloca redzone:     ca\n  Right alloca redzone:    cb\n==36==ABORTING\n```\n\n6. Executing the remaining crashes gives similar crash reports\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94298\nApproved by: https://github.com/davidberard98",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(stack.size() >= inst.N);\n+            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!stack.empty());\n+            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!stack.empty());",
    "Label": "Buggy"
},
{
    "Id": 1242,
    "Library": "pytorch",
    "Date": "2023/10/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a8653f35de02c7fb038e3c184dda6e67a12a39e2",
    "Root Cause": "Device Availability",
    "Bug report": "One more small Perf Tweak to fill_ (#110294)\n\n# Summary\nPerf win by check which device tensors are on\n\n## Before this PR:\n``` Shell\nCPU | CPU: 1.3328152848407626\nGPU | GPU: 6.614773320034146\nCPU | GPU: 29.027153505012393\nGPU | CPU: 17.22372299991548\n```\n## After this PR\n``` Shell\nCPU | CPU: 1.4241038949694484\nGPU | GPU: 7.060713530518115\nCPU | GPU: 15.149936103262007\nGPU | CPU: 5.774620908778161\n```\n\n#### Repro Script\n``` Python\n    a = torch.tensor([0.2, 0.5], device=\"cpu\")\n    amax = torch.tensor(0.5, device=\"cpu\")\n    print(f\"CPU | CPU: {benchmark_torch_function_in_microseconds(torch.fill_, a, amax)}\")\n\n    a = torch.tensor([0.2, 0.5], device=\"cuda\")\n    amax = torch.tensor(0.5, device=\"cuda\")\n    print(f\"GPU | GPU: {benchmark_torch_function_in_microseconds(torch.fill_, a, amax)}\")\n\n    a = torch.tensor([0.2, 0.5], device=\"cpu\")\n    amax = torch.tensor(0.5, device=\"cuda\")\n    print(f\"CPU | GPU: {benchmark_torch_function_in_microseconds(torch.fill_, a, amax)}\")\n\n    a = torch.tensor([0.2, 0.5], device=\"cuda\")\n    amax = torch.tensor(0.5, device=\"cpu\")\n    print(f\"GPU | CPU: {benchmark_torch_function_in_microseconds(torch.fill_, a, amax)}\")\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110294\nApproved by: https://github.com/mikaylagawarecki",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (self.device() != value.device()){\n+    return fill_out(self, value.item());\n+  }",
    "Label": "Buggy"
},
{
    "Id": 1243,
    "Library": "pytorch",
    "Date": "2022/04/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a9deda5469a6ef73692a9dd796cc4eeba4436d6c",
    "Root Cause": "Device Type",
    "Bug report": "Fix issue in sparce_coo_tensor only supporting CUDA device.\n\n## Motivation\nThe at::native::_validate_sparse_coo_tensor_args only supports checking the indices on CUDA device and CPU device. To extend the function to support more device type.\n\n## Solution\nCopy  the indices to the CPU to validate the correctness.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76293\nApproved by: https://github.com/mrshenli",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (indices.is_cuda()) {",
    "Added lines": "+    if (!indices.is_cpu()) {",
    "Label": "Buggy"
},
{
    "Id": 1244,
    "Library": "pytorch",
    "Date": "2023/01/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/aab55d6d0d7b958e32cfdbb69794e107cfceb6bc",
    "Root Cause": "Computation Graph",
    "Bug report": "[Quant] Remove all the dequant nodes when the ref module has multi input args (#90157)\n\n**Summary**:\nWhen converting a ref module into a quant module, `_lower_static_weighted_ref_module` pass assumes the `ref_node` only has 1 input node, and only remove the first `dequant` node. We add a check in this PR to ensure this is the case for `_lower_static_weighted_ref_module` pass.\n\n**Test Plan**:\nWe only add a check in this PR, there is no new added test case.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90157\nApproved by: https://github.com/Xia-Weiwen, https://github.com/jgong5, https://github.com/jerryzh168",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        assert(len(ref_node.args) == 1)",
    "Label": "Buggy"
},
{
    "Id": 1245,
    "Library": "pytorch",
    "Date": "2023/04/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ae55619a2bb73ebcdc80b02a6ccd72275a9ce23e",
    "Root Cause": "Type Checking",
    "Bug report": "Add check for same dtype in tensordot implementation (#98938)\n\nFixes #77517\n\nI believe[ the first bullet point in this comment](https://github.com/pytorch/pytorch/issues/77517#issuecomment-1129233539) from the linked issue is no longer a concern, but please let me know if I'm incorrect here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98938\nApproved by: https://github.com/lezcano",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TORCH_CHECK(input1.scalar_type() == input2.scalar_type(), \"both inputs should have same dtype\");",
    "Label": "Buggy"
},
{
    "Id": 1246,
    "Library": "pytorch",
    "Date": "2023/06/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b287cb816c1ac52165920a121c98643c08d31ff7",
    "Root Cause": "Boundary Value",
    "Bug report": "inductor: make the vec_transpose's tiling stride doesn't depend on out_idx and tiling_idex (#103651)\n\nFor TIMM swin_base_patch4_window7_224 dynamic shape path, there has an accuracy issue with horizontal reduction with vec_transpose:\n```\n#pragma omp for\nfor(long i0=static_cast<long>(0L); i0<static_cast<long>(ks0); i0+=static_cast<long>(1L))\n{\n    #pragma GCC ivdep\n    for(long i1=static_cast<long>(0L); i1<static_cast<long>(3136L); i1+=static_cast<long>(16L))\n    {\n        {\n            #pragma omp declare reduction(+:at::vec::Vectorized<float>:omp_out = omp_out + omp_in) initializer(omp_priv={{0}})\n            float tmp_acc0 = 0;\n            auto tmp_acc0_vec = at::vec::Vectorized<float>(tmp_acc0);\n            for(long i2=static_cast<long>(0L); i2<static_cast<long>(128L); i2+=static_cast<long>(16L))\n            {\n                float tmp1[16*16] __attribute__ ((aligned (16)));\n                at::vec::transpose_mxn<float,16,16>(in_ptr1 + static_cast<long>(i2 + (128L*(static_cast<long>((static_cast<long>(i1) % static_cast<long>(56L))) % static_cast<long>(7L))) + (896L*(static_cast<long>(at::native::div_floor_integer(i1, 56L)) % static_cast<long>(7L))) + (6272L*(at::native::div_floor_integer((static_cast<long>(i1) % static_cast<long>(56L)), 7L))) + (50176L*(at::native::div_floor_integer(i1, 392L))) + (401408L*i0)), static_cast<long>(((-50176L)*(at::native::div_floor_integer(i1, 392L))) + ((-6272L)*(at::native::div_floor_integer((static_cast<long>(i1) % static_cast<long>(56L)), 7L))) + ((-896L)*(static_cast<long>(at::native::div_floor_integer(i1, 56L)) % static_cast<long>(7L))) + ((-128L)*(static_cast<long>((static_cast<long>(i1) % static_cast<long>(56L))) % static_cast<long>(7L))) + (128L*(static_cast<long>((static_cast<long>((1L + i1)) % static_cast<long>(56L))) % static_cast<long>(7L))) + (896L*(static_cast<long>(at::native::div_floor_integer((1L + i1), 56L)) % static_cast<long>(7L))) + (6272L*(at::native::div_floor_integer((static_cast<long>((1L + i1)) % static_cast<long>(56L)), 7L))) + (50176L*(at::native::div_floor_integer((1L + i1), 392L)))), tmp1, 16);\n                for (long i2_inner = 0; i2_inner < 16; i2_inner++)\n                {\n                    auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + static_cast<long>(i1 + (3136L*i2) + (3136L*i2_inner) + (401408L*i0)));\n                    auto tmp2 = at::vec::Vectorized<float>::loadu(tmp1 + static_cast<long>(16L*i2_inner));\n                    auto tmp3 = tmp0 + tmp2;\n                    tmp_acc0_vec = tmp_acc0_vec + tmp3;\n                }\n            }\n            tmp_acc0_vec.store(out_ptr0 + static_cast<long>(i1 + (3136L*i0)));\n        }\n    }\n}\n```\n\nThe ```transpose_mxn```'s ```ld_src``` depends on ```i1``` which is not expected. This PR will  add a check to make sure the tiling stride doesn't depend on out_idx(```i2```) and tiling_idex(```i1```)\n\nAfter this PR, the generated code will be like this:\n```\n#pragma omp for\nfor(long i0=static_cast<long>(0L); i0<static_cast<long>(ks0); i0+=static_cast<long>(1L))\n{\n    #pragma GCC ivdep\n    for(long i1=static_cast<long>(0L); i1<static_cast<long>(3136L); i1+=static_cast<long>(16L))\n    {\n        {\n            #pragma omp declare reduction(+:at::vec::Vectorized<float>:omp_out = omp_out + omp_in) initializer(omp_priv={{0}})\n            float tmp_acc0 = 0;\n            auto tmp_acc0_vec = at::vec::Vectorized<float>(tmp_acc0);\n            for(long i2=static_cast<long>(0L); i2<static_cast<long>(128L); i2+=static_cast<long>(16L))\n            {\n                for (long i2_inner = 0; i2_inner < 16; i2_inner++)\n                {\n                    auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + static_cast<long>(i1 + (3136L*i2) + (3136L*i2_inner) + (401408L*i0)));\n                    auto tmp1 = ([&]() { __at_align__ float tmpbuf[16]; for (long i1_inner = 0; i1_inner < 16; i1_inner++) tmpbuf[i1_inner] = in_ptr1[static_cast<long>(i2 + i2_inner + (128L*(static_cast<long>((static_cast<long>((i1 + i1_inner)) % static_cast<long>(56L))) % static_cast<long>(7L))) + (896L*(static_cast<long>(at::native::div_floor_integer((i1 + i1_inner), 56L)) % static_cast<long>(7L))) + (6272L*(at::native::div_floor_integer((static_cast<long>((i1 + i1_inner)) % static_cast<long>(56L)), 7L))) + (50176L*(at::native::div_floor_integer((i1 + i1_inner), 392L))) + (401408L*i0))]; return at::vec::Vectorized<float>::loadu(tmpbuf); })();\n                    auto tmp2 = tmp0 + tmp1;\n                    tmp_acc0_vec = tmp_acc0_vec + tmp2;\n                }\n            }\n            tmp_acc0_vec.store(out_ptr0 + static_cast<long>(i1 + (3136L*i0)));\n        }\n    }\n}\n```\n\nHow to reproduce this issue:\n```\npython -m torch.backends.xeon.run_cpu --node_id 0 benchmarks/dynamo/timm_models.py --accuracy --float32 -dcpu --inference -n5 --inductor --dynamic-shapes --only swin_base_patch4_window7_224\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103651\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "Number of deleted lines": 2,
    "Deleted lines": "-        return stride_at(self.itervars[self.outer_idx], index) == 1 and index.has(\n-            self.itervars[self.tiling_idx]",
    "Added lines": "+        return (\n+            stride_at(self.itervars[self.outer_idx], index) == 1\n+            and index.has(self.itervars[self.tiling_idx])\n+            and not stride_at(self.itervars[self.tiling_idx], index).has(\n+                self.itervars[self.tiling_idx]\n+            )\n+            and not stride_at(self.itervars[self.tiling_idx], index).has(\n+                self.itervars[self.outer_idx]\n+            )",
    "Label": "Buggy"
},
{
    "Id": 1247,
    "Library": "pytorch",
    "Date": "2023/06/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b3ace213f240dc0f0f2a738f825f46e0d0dffca4",
    "Root Cause": "Edge Cases",
    "Bug report": "Heap buffer overflow at `source_range_serialization.cpp:73` (#103969)\n\nHi! We've been fuzzing torchvision project with [sydr-fuzz](https://github.com/ispras/oss-sydr-fuzz).\nWe've found a heap buffer overflow error at `source_range_serialization.cpp:73` in pytorch project.\n\nThe error occurs because there is not check in `deserialize_source` that `text_table_` size can be less than `fnameIndex`. To prevent the error the corresponding check must be located.\n\ntorchvision version: 9d0a93eee90bf7c401b74ebf9c8be80346254f15\npytorch version: 0f1621df1a0a73956c7ce4e2f72f069e610e0137\n\nOS: Ubuntu 20.04\n\nHow to reproduce\n\n1. Build docker from [here](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/torchvision) and run the container:\n\n        sudo docker build -t oss-sydr-fuzz-torchvision .\n        sudo docker run --privileged --rm -v `pwd`:/fuzz -it oss-sydr-fuzz-torchvision /bin/bash\n\n2. Run the target on this input:  [serialization-crash.txt](https://github.com/pytorch/pytorch/files/11819901/serialization-crash.txt)\n\n        /encode_png_fuzz serialization-crash.txt\n\n3. You will see the following output:\n\n        =================================================================\n        ==13==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60200055a630 at pc 0x0000010197b7 bp 0x7ffd4cfb15f0 sp 0x7ffd4cfb15e8\n        READ of size 8 at 0x60200055a630 thread T0\n            #0 0x10197b6 in std::__shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, (__gnu_cxx::_Lock_policy)2>::get() const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1325:16\n            #1 0x10197b6 in std::__shared_ptr_access<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, (__gnu_cxx::_Lock_policy)2, false, false>::_M_get() const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1024:66\n            #2 0x10197b6 in std::__shared_ptr_access<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, (__gnu_cxx::_Lock_policy)2, false, false>::operator*() const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1011:10\n            #3 0xde888c2 in torch::jit::SourceRangeDeserializer::deserialize_source(c10::IValue const&) /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:73:16\n            #4 0xde8802b in torch::jit::SourceRangeDeserializer::deserialize(c10::IValue const&) /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:51:37\n            #5 0xde8e9c7 in torch::jit::ConcreteSourceRangeUnpickler::unpickle() /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:224:39\n            #6 0xde8fb19 in torch::jit::ConcreteSourceRangeUnpickler::findSourceRangeThatGenerated(torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:231:3\n            #7 0x10798e7 in torch::jit::Source::findSourceRangeThatGenerated(torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/frontend/source_range.cpp:144:23\n            #8 0x1079d9a in torch::jit::SourceRange::findSourceRangeThatGenerated() const /pytorch/torch/csrc/jit/frontend/source_range.h:384:26\n            #9 0x1079acd in torch::jit::SourceRange::highlight(std::ostream&) const /pytorch/torch/csrc/jit/frontend/source_range.cpp:149:32\n            #10 0x1026fe2 in torch::jit::Lexer::expected(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::Token const&) /pytorch/torch/csrc/jit/frontend/lexer.h:461:13\n            #11 0x10417d9 in torch::jit::Lexer::expected(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/frontend/lexer.h:465:5\n            #12 0x102e52c in torch::jit::Lexer::expect(int) /pytorch/torch/csrc/jit/frontend/lexer.h:471:7\n            #13 0xcee774c in torch::jit::ParserImpl::parseIdent() /pytorch/torch/csrc/jit/frontend/parser.cpp:52:16\n            #14 0xcef4ea8 in torch::jit::ParserImpl::parseBaseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:195:22\n            #15 0xcef2c1b in torch::jit::ParserImpl::parseExp(int) /pytorch/torch/csrc/jit/frontend/parser.cpp:284:16\n            #16 0xcefac6a in torch::jit::ParserImpl::parseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:262:12\n            #17 0xcefac6a in torch::jit::ParserImpl::parseSubscriptExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:403:15\n            #18 0xceff39f in torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()::operator()() const /pytorch/torch/csrc/jit/frontend/parser.cpp:354:54\n            #19 0xceff39f in torch::jit::Expr std::__invoke_impl<void, torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()&>(std::__invoke_other, torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:60:14\n            #20 0xceea935 in torch::jit::ParserImpl::parseSequence(int, int, int, std::function<void ()> const&) /pytorch/torch/csrc/jit/frontend/parser.cpp:339:7\n            #21 0xceefd69 in torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)()) /pytorch/torch/csrc/jit/frontend/parser.cpp:353:5\n            #22 0xcef895a in torch::jit::ParserImpl::parseSubscript(c10::intrusive_ptr<torch::jit::Tree, c10::detail::intrusive_target_default_null_type<torch::jit::Tree> > const&) /pytorch/torch/csrc/jit/frontend/parser.cpp:430:9\n            #23 0xcef5e5c in torch::jit::ParserImpl::parseBaseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:206:18\n            #24 0xcef2c1b in torch::jit::ParserImpl::parseExp(int) /pytorch/torch/csrc/jit/frontend/parser.cpp:284:16\n            #25 0xceeeb9d in torch::jit::ParserImpl::parseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:262:12\n            #26 0xceeeb9d in torch::jit::ParserImpl::parseExpOrExpTuple() /pytorch/torch/csrc/jit/frontend/parser.cpp:94:19\n            #27 0xcee8a36 in torch::jit::ParserImpl::parseStmt(bool) /pytorch/torch/csrc/jit/frontend/parser.cpp:612:20\n            #28 0xcee7e72 in torch::jit::ParserImpl::parseStatements(bool, bool) /pytorch/torch/csrc/jit/frontend/parser.cpp:697:23\n            #29 0xcee56f5 in torch::jit::ParserImpl::parseClass() /pytorch/torch/csrc/jit/frontend/parser.cpp:747:9\n            #30 0xcee544a in torch::jit::Parser::parseClass() /pytorch/torch/csrc/jit/frontend/parser.cpp:812:17\n            #31 0xdddbea9 in torch::jit::SourceImporterImpl::parseSourceIfNeeded(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:182:42\n            #32 0xdddadbc in torch::jit::SourceImporterImpl::findNamedType(c10::QualifiedName const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:135:3\n            #33 0xdde1d88 in torch::jit::SourceImporterImpl::resolveType(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:261:10\n            #34 0xcf2ba5f in torch::jit::ScriptTypeParser::parseTypeFromExpr(torch::jit::Expr const&) const /pytorch/torch/csrc/jit/frontend/script_type_parser.cpp:238:24\n            #35 0xcf2bec7 in torch::jit::ScriptTypeParser::parseType(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/frontend/script_type_parser.cpp:312:10\n            #36 0xddf4284 in torch::jit::SourceImporter::loadType(c10::QualifiedName const&) const /pytorch/torch/csrc/jit/serialization/import_source.cpp:786:27\n            #37 0xdd739f7 in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0::operator()(c10::QualifiedName const&) const /pytorch/torch/csrc/jit/serialization/import.cpp:146:33\n            #38 0xdd739f7 in c10::StrongTypePtr std::__invoke_impl<c10::StrongTypePtr, torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&>(std::__invoke_other, torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:60:14\n            #39 0xdd73880 in std::enable_if<is_invocable_r_v<c10::StrongTypePtr, torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&>, c10::StrongTypePtr>::type std::__invoke_r<c10::StrongTypePtr, torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&>(torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:113:9\n            #40 0xdd736d6 in std::_Function_handler<c10::StrongTypePtr (c10::QualifiedName const&), torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0>::_M_invoke(std::_Any_data const&, c10::QualifiedName const&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/std_function.h:291:9\n            #41 0xdd76349 in std::function<c10::StrongTypePtr (c10::QualifiedName const&)>::operator()(c10::QualifiedName const&) const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/std_function.h:622:14\n            #42 0xdeb9f48 in torch::jit::Unpickler::readGlobal(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/unpickler.cpp:835:9\n            #43 0xdeb012d in torch::jit::Unpickler::readInstruction() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:511:7\n            #44 0xdeae437 in torch::jit::Unpickler::run() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:251:27\n            #45 0xdeae0d2 in torch::jit::Unpickler::parse_ivalue() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:204:3\n            #46 0xddd6de3 in torch::jit::readArchiveAndTensors(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<std::function<c10::StrongTypePtr (c10::QualifiedName const&)> >, c10::optional<std::function<c10::intrusive_ptr<c10::ivalue::Object, c10::detail::intrusive_target_default_null_type<c10::ivalue::Object> > (c10::StrongTypePtr, c10::IValue)> >, c10::optional<c10::Device>, caffe2::serialize::PyTorchStreamReader&, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&), std::shared_ptr<torch::jit::DeserializationStorageContext>) /pytorch/torch/csrc/jit/serialization/import_read.cpp:53:20\n            #47 0xdd732dd in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/import.cpp:184:10\n            #48 0xdd69885 in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::deserialize(c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:287:19\n            #49 0xdd6c855 in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:438:25\n            #50 0xdd6c1c7 in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:421:10\n            #51 0xdd6dce4 in torch::jit::load(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:503:10\n            #52 0xf2d3f75 in torch::serialize::InputArchive::load_from(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>) /pytorch/torch/csrc/api/src/serialize/input-archive.cpp:97:13\n            #53 0x60509c in void torch::load<at::Tensor, char*&>(at::Tensor&, char*&) /pytorch/torch/include/torch/csrc/api/include/torch/serialize.h:107:11\n            #54 0x6036be in LLVMFuzzerTestOneInput /vision/encode_png.cc:38:5\n            #55 0x66b041 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n            #56 0x6544cc in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n            #57 0x65a61b in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n            #58 0x654222 in main /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n            #59 0x7f3d12cc7082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082) (BuildId: 1878e6b475720c7c51969e69ab2d276fae6d1dee)\n            #60 0x542cdd in _start (/encode_png_fuzz+0x542cdd)\n\n        0x60200055a630 is located 16 bytes to the right of 16-byte region [0x60200055a610,0x60200055a620)\n        allocated by thread T0 here:\n            #0 0x60057d in operator new(unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/asan/asan_new_delete.cpp:95:3\n            #1 0xde9185d in std::_Vector_base<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_M_allocate(unsigned long) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:346:20\n            #2 0xde9185d in void std::vector<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_M_realloc_insert<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >(__gnu_cxx::__normal_iterator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >*, std::vector<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > >, std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >&&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:440:33\n            #3 0xde916a1 in std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >& std::vector<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::emplace_back<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >(std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >&&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:121:4\n            #4 0xde8f445 in torch::jit::SourceRangeDeserializer::SourceRangeDeserializer(c10::IValue) /pytorch/torch/csrc/jit/serialization/source_range_serialization.h:42:19\n            #5 0xde8e141 in torch::jit::ConcreteSourceRangeUnpickler::unpickle() /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:215:28\n            #6 0xde8fb19 in torch::jit::ConcreteSourceRangeUnpickler::findSourceRangeThatGenerated(torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:231:3\n            #7 0x10798e7 in torch::jit::Source::findSourceRangeThatGenerated(torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/frontend/source_range.cpp:144:23\n            #8 0x1079d9a in torch::jit::SourceRange::findSourceRangeThatGenerated() const /pytorch/torch/csrc/jit/frontend/source_range.h:384:26\n            #9 0x1079acd in torch::jit::SourceRange::highlight(std::ostream&) const /pytorch/torch/csrc/jit/frontend/source_range.cpp:149:32\n            #10 0x1026fe2 in torch::jit::Lexer::expected(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::Token const&) /pytorch/torch/csrc/jit/frontend/lexer.h:461:13\n            #11 0x10417d9 in torch::jit::Lexer::expected(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/frontend/lexer.h:465:5\n            #12 0xcee774c in torch::jit::ParserImpl::parseIdent() /pytorch/torch/csrc/jit/frontend/parser.cpp:52:16\n            #13 0xcef4ea8 in torch::jit::ParserImpl::parseBaseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:195:22\n            #14 0xcef2c1b in torch::jit::ParserImpl::parseExp(int) /pytorch/torch/csrc/jit/frontend/parser.cpp:284:16\n            #15 0xcefac6a in torch::jit::ParserImpl::parseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:262:12\n            #16 0xcefac6a in torch::jit::ParserImpl::parseSubscriptExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:403:15\n            #17 0xceff39f in torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()::operator()() const /pytorch/torch/csrc/jit/frontend/parser.cpp:354:54\n            #18 0xceff39f in torch::jit::Expr std::__invoke_impl<void, torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()&>(std::__invoke_other, torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:60:14\n            #19 0xceea935 in torch::jit::ParserImpl::parseSequence(int, int, int, std::function<void ()> const&) /pytorch/torch/csrc/jit/frontend/parser.cpp:339:7\n            #20 0xceefd69 in torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)()) /pytorch/torch/csrc/jit/frontend/parser.cpp:353:5\n            #21 0xcef895a in torch::jit::ParserImpl::parseSubscript(c10::intrusive_ptr<torch::jit::Tree, c10::detail::intrusive_target_default_null_type<torch::jit::Tree> > const&) /pytorch/torch/csrc/jit/frontend/parser.cpp:430:9\n            #22 0xcef5e5c in torch::jit::ParserImpl::parseBaseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:206:18\n            #23 0xcef2c1b in torch::jit::ParserImpl::parseExp(int) /pytorch/torch/csrc/jit/frontend/parser.cpp:284:16\n            #24 0xceeeb9d in torch::jit::ParserImpl::parseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:262:12\n            #25 0xceeeb9d in torch::jit::ParserImpl::parseExpOrExpTuple() /pytorch/torch/csrc/jit/frontend/parser.cpp:94:19\n            #26 0xcee8a36 in torch::jit::ParserImpl::parseStmt(bool) /pytorch/torch/csrc/jit/frontend/parser.cpp:612:20\n            #27 0xcee7e72 in torch::jit::ParserImpl::parseStatements(bool, bool) /pytorch/torch/csrc/jit/frontend/parser.cpp:697:23\n            #28 0xcee56f5 in torch::jit::ParserImpl::parseClass() /pytorch/torch/csrc/jit/frontend/parser.cpp:747:9\n            #29 0xcee544a in torch::jit::Parser::parseClass() /pytorch/torch/csrc/jit/frontend/parser.cpp:812:17\n            #30 0xdddbea9 in torch::jit::SourceImporterImpl::parseSourceIfNeeded(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:182:42\n            #31 0xdddadbc in torch::jit::SourceImporterImpl::findNamedType(c10::QualifiedName const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:135:3\n            #32 0xdde1d88 in torch::jit::SourceImporterImpl::resolveType(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:261:10\n            #33 0xcf2ba5f in torch::jit::ScriptTypeParser::parseTypeFromExpr(torch::jit::Expr const&) const /pytorch/torch/csrc/jit/frontend/script_type_parser.cpp:238:24\n\n        SUMMARY: AddressSanitizer: heap-buffer-overflow /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1325:16 in std::__shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, (__gnu_cxx::_Lock_policy)2>::get() const\n        Shadow bytes around the buggy address:\n          0x0c04800a3470: fa fa 00 00 fa fa 00 00 fa fa fd fa fa fa 00 00\n          0x0c04800a3480: fa fa fd fa fa fa fd fd fa fa fd fd fa fa fd fa\n          0x0c04800a3490: fa fa fd fd fa fa 00 00 fa fa 00 00 fa fa 00 00\n          0x0c04800a34a0: fa fa fd fa fa fa fd fd fa fa fd fa fa fa 00 fa\n          0x0c04800a34b0: fa fa fd fd fa fa fd fd fa fa fd fa fa fa fd fd\n        =>0x0c04800a34c0: fa fa 00 00 fa fa[fa]fa fa fa fa fa fa fa fa fa\n          0x0c04800a34d0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n          0x0c04800a34e0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n          0x0c04800a34f0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n          0x0c04800a3500: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n          0x0c04800a3510: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n        Shadow byte legend (one shadow byte represents 8 application bytes):\n          Addressable:           00\n          Partially addressable: 01 02 03 04 05 06 07\n          Heap left redzone:       fa\n          Freed heap region:       fd\n          Stack left redzone:      f1\n          Stack mid redzone:       f2\n          Stack right redzone:     f3\n          Stack after return:      f5\n          Stack use after scope:   f8\n          Global redzone:          f9\n          Global init order:       f6\n          Poisoned by user:        f7\n          Container overflow:      fc\n          Array cookie:            ac\n          Intra object redzone:    bb\n          ASan internal:           fe\n          Left alloca redzone:     ca\n          Right alloca redzone:    cb\n        ==13==ABORTING\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103969\nApproved by: https://github.com/davidberard98",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    TORCH_CHECK(\n+        (uint64_t)fnameIndex < text_table_.size(),\n+        \"Text table index is out of range\")",
    "Label": "Buggy"
},
{
    "Id": 1248,
    "Library": "pytorch",
    "Date": "2022/05/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b6920405da340bbd3397b80bf16d9c360b0c48d2",
    "Root Cause": "Type Checking",
    "Bug report": "reorder checks to shave 1 us off no-op dispatch time\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78316\n\nApproved by: https://github.com/Chillee, https://github.com/ezyang",
    "Number of deleted lines": 5,
    "Deleted lines": "-  return (\n-    !THPVariable_CheckTypeExact(tp) &&\n-    // TODO: test if Python key is disabled\n-    attr.ptr() != nullptr &&\n-    attr.ptr() != torch::disabled_torch_dispatch_impl()",
    "Added lines": "+  if (THPVariable_CheckTypeExact(tp)) {\n+    return false;\n+  }\n+  return (attr.ptr() != nullptr &&\n+          attr.ptr() != torch::disabled_torch_dispatch_impl()",
    "Label": "Buggy"
},
{
    "Id": 1249,
    "Library": "pytorch",
    "Date": "2022/10/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b90db4a78f8d760377a81a5a64d03ab4b67599de",
    "Root Cause": "Type Checking",
    "Bug report": "[DataPipe] Fix type checking to accept both Iter and Map DataPipe (#87285)\n\nFixes https://github.com/pytorch/data/issues/841\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87285\nApproved by: https://github.com/NivekT",
    "Number of deleted lines": 1,
    "Deleted lines": "-    assert isinstance(datapipe, IterDataPipe)",
    "Added lines": "+    assert isinstance(datapipe, (IterDataPipe, MapDataPipe))",
    "Label": "Buggy"
},
{
    "Id": 1250,
    "Library": "pytorch",
    "Date": "2022/04/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bc371a2cd03ce573f3ad4f7be141364136028905",
    "Root Cause": "Edge Cases",
    "Bug report": "[quant][fx][fix] Add additional checks when tracing back during maybe share output observer function\n\nSummary:\nCurrently in `maybe_make_input_output_share_observers`  we trace back from a node to find the activation_post_process\nof the input node, we have internal use case which would error out during tracing back, this PR is adding a guard\nduring this process to return False early when the node doesn't have any input\n\nTest Plan:\nnot sure when this would happen, verify within the internal test case\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75650\n\nApproved by: https://github.com/vkuzo",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                # failed to trace back since no input arg for the current node\n+                if len(input_arg.args) < 1:\n+                    return False",
    "Label": "Buggy"
},
{
    "Id": 1251,
    "Library": "pytorch",
    "Date": "2023/01/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bdbd3ed312e0fc81e75302239ea78b3445fe95e7",
    "Root Cause": "Computation Graph",
    "Bug report": "When nopython=True, Dynamo can't allow graph breaks. (#90970)\n\nI count the number of sub-graphs (for tiny-GPT2 in huggingface) by\n```\n    class GraphCaptureCompiler:\n        def __init__(self):\n            self.captured_graphs = []\n        def compile(self, gm, example_inputs):\n            self.captured_graphs.append(gm)\n            return gm\n    compiler = GraphCaptureCompiler()\n    torch._dynamo.optimize(compiler, nopython=True)(Wrapper(fn))(*args)\n```\n\nAlthough `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90970\nApproved by: https://github.com/ezyang, https://github.com/jansel, https://github.com/thiagocrepaldi",
    "Number of deleted lines": 1,
    "Deleted lines": "-                if self.has_backedge():",
    "Added lines": "+                if self.has_backedge() and self.should_compile_partial_graph():",
    "Label": "Buggy"
},
{
    "Id": 1252,
    "Library": "pytorch",
    "Date": "2023/05/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bde7b81f34925491fbcbb9e355697eb594e36923",
    "Root Cause": "Null Value",
    "Bug report": "[S337714] Back out \"[PyTorch] Don't do extra numel() check in TensorImpl::data() (#98090)\" (#100893)\n\nSummary:\nOriginal commit changeset: 9875964c3b32\n\nOriginal Phabricator Diff: D44586464\n\nReviewed By: drdarshan\n\nDifferential Revision: D45664329\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/100893\nApproved by: https://github.com/xush6528",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (data == nullptr) {",
    "Added lines": "+    if (is_empty()) {",
    "Label": "Buggy"
},
{
    "Id": 1253,
    "Library": "pytorch",
    "Date": "2022/02/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/be253b8ee8a104997773d11ed28928a48193217d",
    "Root Cause": "Edge Cases",
    "Bug report": "Fix overflow check in `geometry_is_contiguous` (#73162)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73162\n\nThe existing check isn't safe for 32-bit `size_t` because the max\n64-bit int will overflow.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D34524229\n\nPulled By: malfet\n\nfbshipit-source-id: 58b1bb2fa605e9972644723ba8b37643ba024f38\n(cherry picked from commit b15aa0fbd49153bcdf61bc7676449f3ee4a5bb51)",
    "Number of deleted lines": 1,
    "Deleted lines": "-  assert(sizes.size() < static_cast<std::size_t>(std::numeric_limits<std::int64_t>::max()));",
    "Added lines": "+  assert(!overflows<std::int64_t>(sizes.size()));",
    "Label": "Buggy"
},
{
    "Id": 1254,
    "Library": "pytorch",
    "Date": "2023/11/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c1e51fcbfc70c089276530ee64fb626e3f7f4f2b",
    "Root Cause": "Device Type",
    "Bug report": "[ONNX][Bench] Relax tolerance for cuda accuracy check (#114767)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114767\nApproved by: https://github.com/thiagocrepaldi\nghstack dependencies: #112179",
    "Number of deleted lines": 1,
    "Deleted lines": "-            # Workaround for ONNX for non-tensor outputs",
    "Added lines": "+                # Workaround for ONNX for non-tensor outputs\n+                # Relax tolerance for ONNX cuda\n+                if current_device == \"cuda\":\n+                    tolerance = 1e-2\n+",
    "Label": "Buggy"
},
{
    "Id": 1255,
    "Library": "pytorch",
    "Date": "2023/03/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c69b3b8d4f484cf537d98974a3a4143b77edf3c8",
    "Root Cause": "Device Availability",
    "Bug report": "[CUDA12] Autograd engine use current device only (#92354)\n\nThis is a device agnostic version #91191.\nThe reason of existence of this PR is device agnostic policy of autograd engine. Hence, the compile time `USE_CUDA` is not supported, so doing something like:\nhttps://github.com/pytorch/pytorch/blob/fa1ea9f9bcaa77c1370468059be95ad9b421f500/torch/csrc/autograd/engine.cpp#L351-L357\nis not effective.\n\nIn this PR a check upon CUDA devices in device registry is added such that threads set the same CUDA device.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92354\nApproved by: https://github.com/albanD, https://github.com/ngimel",
    "Number of deleted lines": 9,
    "Deleted lines": "-\n-#if defined(USE_CUDA)\n-  if (at::detail::getCUDAHooks().hasPrimaryContext(device)) {\n-    set_device(device);\n-  }\n-#else\n-  set_device(device);\n-#endif\n-      if (impl && device < impl->deviceCount()) {",
    "Added lines": "+  worker_device = device;\n+      set_device(worker_device);\n+\n+      if (impl && device < impl->deviceCount() &&\n+          impl->getDevice().index() != device) {",
    "Label": "Buggy"
},
{
    "Id": 1256,
    "Library": "pytorch",
    "Date": "2022/02/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cafd0f33042f5344a27ccde33b352eab676a0bdd",
    "Root Cause": "Boundary Value",
    "Bug report": "[jit][edge] Fix array index checking in mobile interpreter. (#73241)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73241\n\nStop using non-portable out-of-range indexing in mobile interpreter, also change code types indexing to use vector.at() to catch out-of-range bugs earlier.\n\nTest Plan: buck test mode/dbg mode/no-gpu -c fbcode.platform=platform010 //caffe2/test/cpp/jit:jit -- BackendTest.TestCompiler\n\nReviewed By: dhruvbird, r-barnes\n\nDifferential Revision: D34370237\n\nfbshipit-source-id: 1827f75ed00ecc10bbcece48329b0ac87189b079\n(cherry picked from commit ab943ef414c8d109bd766f672def63be28af2571)",
    "Number of deleted lines": 6,
    "Deleted lines": "-          listConstruct(stack, *code.types_[inst.X], inst.N);\n-          dictConstruct(stack, *code.types_[inst.X], inst.N);\n-          namedTupleConstruct(stack, code.types_[inst.X], inst.N);\n-          auto type = code.types_[inst.X]->expect<c10::ClassType>();\n-          at::ArrayRef<TypePtr> types(\n-              &(code.types_[inst.X]), &(code.types_[inst.X + inst.N]));",
    "Added lines": "+          listConstruct(stack, *code.types_.at(inst.X), inst.N);\n+          dictConstruct(stack, *code.types_.at(inst.X), inst.N);\n+          namedTupleConstruct(stack, code.types_.at(inst.X), inst.N);\n+          auto type = code.types_.at(inst.X)->expect<c10::ClassType>();\n+          at::ArrayRef<TypePtr> types(&code.types_.at(inst.X), inst.N);",
    "Label": "Buggy"
},
{
    "Id": 1257,
    "Library": "pytorch",
    "Date": "2022/06/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cc6a51c9f3ee97a06ff9c0b84477e88e33e31137",
    "Root Cause": "Edge Cases",
    "Bug report": "added shape checking to WeightedRandomSampler (#78585)\n\nFixes #78236\n\nAn erronously shaped weights vector will result in the following output\n\n```\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n~/datarwe/pytorch/torch/utils/data/sampler.py in <module>\n      [274](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=273) WeightedRandomSampler([1,2,3], 10)\n----> [275](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=274) WeightedRandomSampler([[1,2,3], [4,5,6]], 10)\n\n~/datarwe/pytorch/torch/utils/data/sampler.py in __init__(self, weights, num_samples, replacement, generator)\n    [192](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=191)         weights = torch.as_tensor(weights, dtype=torch.double)\n    [193](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=192)         if len(weights.shape) != 1:\n--> [194](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=193)             raise ValueError(\"weights should be a 1d sequence but given \"\n    [195](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=194)                              \"weights have shape {}\".format(tuple(weights.shape)))\n    [196](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=195)\n\nValueError: weights should be a 1d sequence but given weights have shape (2, 3)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78585\nApproved by: https://github.com/NivekT, https://github.com/ejguan",
    "Number of deleted lines": 1,
    "Deleted lines": "-        self.weights = torch.as_tensor(weights, dtype=torch.double)",
    "Added lines": "+\n+        weights_tensor = torch.as_tensor(weights, dtype=torch.double)\n+        if len(weights_tensor.shape) != 1:\n+            raise ValueError(\"weights should be a 1d sequence but given \"\n+                             \"weights have shape {}\".format(tuple(weights_tensor.shape)))\n+\n+        self.weights = weights_tensor",
    "Label": "Buggy"
},
{
    "Id": 1258,
    "Library": "pytorch",
    "Date": "2023/03/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cf732053e4f6b93b0a93006613552cd97f415b80",
    "Root Cause": "Boundary Value",
    "Bug report": "nn.EmbeddingBag bound check (#96022)\n\nSummary: Today if we're accessing out of bound embedding rows, it'll either go through or throw IMA. This is not ideal - adding bound checks. This will probably slow things down - need to benchmark it.\n\nTest Plan:\nTODO: add some tests\n\nTried a simple example and it's showing this:\n```\naten/src/ATen/native/cuda/EmbeddingBag.cu:143: EmbeddingBag_updateOutputKernel_sum_mean: block: [0,0,0], thread: [0,1,0] Assertion `input[emb] < numRows` failed.\n```\n\nDifferential Revision: D43810777\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/96022\nApproved by: https://github.com/cpuhrsch, https://github.com/ngimel",
    "Number of deleted lines": 4,
    "Deleted lines": "-    index_t padding_idx) {\n-    index_t padding_idx) {\n-            padding_idx);\n-            padding_idx);",
    "Added lines": "+    index_t padding_idx, int64_t numRows) {\n+        CUDA_KERNEL_ASSERT(input[emb] < numRows);\n+    index_t padding_idx, int64_t numRows) {\n+        CUDA_KERNEL_ASSERT(input[emb] < numRows);\n+            padding_idx, weight.size(0));\n+            padding_idx, weight.size(0));",
    "Label": "Buggy"
},
{
    "Id": 1259,
    "Library": "pytorch",
    "Date": "2022/02/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d23231fd8cd50e4eb657eb7c3cf102475634f9c6",
    "Root Cause": "Edge Cases",
    "Bug report": "Fix upgrader codegen when constant list is 0 (#72199)\n\nSummary:\nWhen the constant list is empty, previous codegen will generate something like\n```\nstd::vector<c10::IValue>({\n\n}), // constants list,\n```\nHowever it will fail quick-check, because it includes trailing spaces. This pr will generate the following instead.\n```\nstd::vector<c10::IValue>(), // constants list,\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72199\n\nghstack-source-id: 148231023\n\nTest Plan: CI\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D33952046\n\nfbshipit-source-id: 359b8a418928c89bbeb446b44774b312c94f03bc\n(cherry picked from commit 060490f66724e418a43548c2eaffa3244e780557)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+CONSTANTS_LIST_EMPTY = \"\"\"std::vector<c10::IValue>(), // constants list\"\"\"\n+\n+    if len(constants_list_part) == 0:\n+        return CONSTANTS_LIST_EMPTY",
    "Label": "Buggy"
},
{
    "Id": 1260,
    "Library": "pytorch",
    "Date": "2023/09/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d471eaeb1d2fbc7efcde6408d7d1e513b969af25",
    "Root Cause": "Null Value",
    "Bug report": "fix inline_container.cc inplace loading (#108573)\n\nSummary:\nbypass-github-pytorch-ci-checks\nbypass-github-export-checks\nforce-merge-on-github\n\nDifferential Revision: D48971847\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108573\nApproved by: https://github.com/wqfish",
    "Number of deleted lines": 1,
    "Deleted lines": "-",
    "Added lines": "+  std::vector<uint8_t> buffer;\n+  if (buf == nullptr) {\n+    buffer.resize(chunk_size);\n+    buf = buffer.data();\n+  }",
    "Label": "Buggy"
},
{
    "Id": 1261,
    "Library": "pytorch",
    "Date": "2022/11/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d8466964b348b6172317f70b8e52de02402bad54",
    "Root Cause": "Edge Cases",
    "Bug report": "Add range check to multi margin loss target (#89008)\n\nFixes https://github.com/pytorch/pytorch/issues/88724\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89008\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  CUDA_KERNEL_ASSERT(target_k >= 0 && target_k < dim && \"target index is out of bounds\");",
    "Label": "Buggy"
},
{
    "Id": 1262,
    "Library": "pytorch",
    "Date": "2023/07/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/db1ac4e29b0f557711190c8d49d4afb5da1844e8",
    "Root Cause": "Backend Type",
    "Bug report": "fix functional collective's allgather for gloo (#104681)\n\nSummary: We should explicitly check for the gloo backend instead of relying on the shard's device, because user might pass a GPU tensor as input and a process group gloo as the pg, and expect that should work.\n\nDifferential Revision: D47249172\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104681\nApproved by: https://github.com/rohan-varma, https://github.com/fduwjj",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if shard.is_cpu:",
    "Added lines": "+    if dist.get_backend(group) == dist.Backend.GLOO or shard.is_cpu:",
    "Label": "Buggy"
},
{
    "Id": 1263,
    "Library": "pytorch",
    "Date": "2024/01/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc43ad428603539a2051940c09b191825f66203d",
    "Root Cause": "Others",
    "Bug report": "add is_grad_enabled check in runtime_wrapper before running with torch.no_grad (#117089)\n\nWe observed that `with torch.no_grad()` in runtime_wrapper introduced ~10% (0.06ms->0.066ms) inference performance regression on lennard_jones on cpu.\nFor inference tasks in benchmark, grad has been disabled, but in the current runtime_wrapper, no_grad is set again and its time is counted into the running time.\nTherefore, we add `is_grad_enabled` check in runtime_wrapper before running with torch.no_grad. If grad has been disabled, there is no need to set no_grad.\n\nBefore this pr:\n1.043x\ndev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks\ncpu,lennard_jones,1,**1.043427**,**0.068366**,4.756151,0.941846,45.056819,47.838822,9,1,0,0\n\nAfter this pr:\n1.146x\ndev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks\ncpu,lennard_jones,1,**1.146190**,**0.061844**,4.468380,0.936456,44.427264,47.441920,9,1,0,0\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117089\nApproved by: https://github.com/jgong5, https://github.com/bdhirsh",
    "Number of deleted lines": 1,
    "Deleted lines": "-            with torch.no_grad():",
    "Added lines": "+            if torch.is_grad_enabled():\n+                with torch.no_grad():\n+                    all_outs = call_func_at_runtime_with_args(\n+                        compiled_fn,\n+                        args,\n+                        disable_amp=disable_amp,\n+                    )\n+            else:",
    "Label": "Buggy"
},
{
    "Id": 1264,
    "Library": "pytorch",
    "Date": "2023/08/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e31038d574712d383fdc4c2f1bb63fc82f256ed0",
    "Root Cause": "Type Checking",
    "Bug report": "Check results dtype in index_out (#108167)\n\nThis logic exists for index_put and index_add, but for some reason not for `index.out`\nSkip testing, as this function is not technically exposed on the Python level.\n\n<!--\ncopilot:poem\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at c688cfd</samp>\n\n> _`index_out` checks types_\n> _avoiding errors in autumn_\n> _complex tensors work_\n\nFixes https://github.com/pytorch/pytorch/issues/107698\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108167\nApproved by: https://github.com/albanD",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    TORCH_CHECK(self.scalar_type() == result.scalar_type(),\n+                \"index_out: self (\", self.scalar_type(), \") and result (\", result.scalar_type(),\n+                \") must have the same scalar type\");",
    "Label": "Buggy"
},
{
    "Id": 1265,
    "Library": "pytorch",
    "Date": "2022/11/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e856a4d66bead8997a83f8714547c09fcbcdc263",
    "Root Cause": "Device Version",
    "Bug report": "Add an env var to skip cudnn version compatibility check (#89184)\n\nskip the check by setting `PYTORCH_SKIP_CUDNN_COMPATIBILITY_CHECK=1`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89184\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                if os.environ.get('PYTORCH_SKIP_CUDNN_COMPATIBILITY_CHECK', '0') == '1':\n+                    return True",
    "Label": "Buggy"
},
{
    "Id": 1267,
    "Library": "pytorch",
    "Date": "2022/11/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ee91c328da5739ce03b3127cd7c542ce505212b8",
    "Root Cause": "Device Type",
    "Bug report": "Fix cuda/cpu check on NoneType (#88854)\n\nSummary: Fix cuda/cpu check on NoneType\n\nTest Plan: sabdcastle/ github CI/CD\n\nDifferential Revision: D41203955\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/88854\nApproved by: https://github.com/drisspg, https://github.com/ngimel",
    "Number of deleted lines": 1,
    "Deleted lines": "-            elif not all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):",
    "Added lines": "+            elif not all([(x is None or x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):",
    "Label": "Buggy"
},
{
    "Id": 1268,
    "Library": "pytorch",
    "Date": "2022/04/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f02b7a9c36dd6182da694bc47a5c345285dfd951",
    "Root Cause": "Edge Cases",
    "Bug report": "Pad: don't error when unused fill value is zero\n\nFixes pytorch/vision#5873\n\nIn the python version of `F.pad`, checking that the fill value was\nleft as default was done by comparing against zero:\nhttps://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/nn/functional.py#L4366\n\nSo if someone does explicitly pass in a zero-value, then this\n`TORCH_CHECK` was an accidental BC-break.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76307\n\nApproved by: https://github.com/albanD, https://github.com/jbschlosser, https://github.com/datumbox",
    "Number of deleted lines": 4,
    "Deleted lines": "-  TORCH_CHECK(\n-      !value.has_value(), \"Padding mode \\\"\",\n-      padding_mode_string(mode),\n-      \"\\\" doesn't take in value argument\");",
    "Added lines": "+  TORCH_CHECK(!value.has_value() || *value == 0,\n+              \"Padding mode \\\"\", padding_mode_string(mode),\n+              \"\\\" doesn't take in value argument\");",
    "Label": "Buggy"
},
{
    "Id": 1269,
    "Library": "pytorch",
    "Date": "2023/03/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f386312ec936a94bfb1abe44acdd61d498f4272b",
    "Root Cause": "Null Value",
    "Bug report": "[PyTorch] Don't do extra numel() check in TensorImpl::data() (#98090)\n\n`is_empty()` checks `numel() == 0`, but we don't need to access `numel_` at all (or the policy that `numel()` checks) in our happy path -- we just need the data pointer from `storage_`. Let's do the check we need to do using only the data we strictly need, rather than adding instructions loading other pieces of data.\n\nDifferential Revision: [D44586464](https://our.internmc.facebook.com/intern/diff/D44586464/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98090\nApproved by: https://github.com/Skylion007",
    "Number of deleted lines": 4,
    "Deleted lines": "-    if (is_empty()) {\n-    return static_cast<void*>(\n-        static_cast<char*>(storage_.data()) +\n-        data_type_.itemsize() * storage_offset_);",
    "Added lines": "+    char* const data = static_cast<char*>(storage_.data());\n+    if (data == nullptr) {\n+    return static_cast<void*>(data + data_type_.itemsize() * storage_offset_);",
    "Label": "Buggy"
},
{
    "Id": 1270,
    "Library": "pytorch",
    "Date": "2024/01/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f6639359357452de8bfc691430396ded98ea399c",
    "Root Cause": "Boundary Value",
    "Bug report": "[MPS] Fix boundary checks in generateKernelOffsets (#116915)\n\n`TORCH_CHECK(i<UINT32_MAX)` is always false, it should be `TORCH_CHECK(iterShape[i] < UINT32_MAX)`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116915\nApproved by: https://github.com/Skylion007, https://github.com/kulinseth\nghstack dependencies: #116903, #116904",
    "Number of deleted lines": 3,
    "Deleted lines": "-    TORCH_CHECK(i <= UINT32_MAX);\n-    iterShapeData[i] = (uint32_t)(iterShape[i]);\n-      strides[i][offset] = iter.strides(offset)[i];",
    "Added lines": "+  TORCH_CHECK(iter.can_use_32bit_indexing(), \"Can't be indexed using 32-bit iterator\");\n+    iterShapeData[i] = static_cast<uint32_t>(iterShape[i]);\n+      strides[i][offset] = static_cast<uint32_t>(iter.strides(offset)[i]);",
    "Label": "Buggy"
},
{
    "Id": 1271,
    "Library": "pytorch",
    "Date": "2022/03/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f810d96806d0e767aeca9fe9cf50e0bdcaab7d52",
    "Root Cause": "Edge Cases",
    "Bug report": "remove redundant index check for index_select_out_cpu_dim1_ (#74093)\n\nSummary:\nFor  **index_select_out_cpu_dim1_**, there has a redundant idex check, **check_indexarray_range** has checked  **the index>=0 and  index < slect_dim**, we don't need re-check it at copy step.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/74093\n\nReviewed By: ezyang\n\nDifferential Revision: D34823673\n\nPulled By: ngimel\n\nfbshipit-source-id: c723b5bd6254c36588063da0175470268bffca5d\n(cherry picked from commit 1791fa4e488cdf77f0d6850f364a74a7a92fedee)",
    "Number of deleted lines": 7,
    "Deleted lines": "-            if (idx < 0) {\n-              idx = idx + src_indexing_axis_dim;\n-            }\n-            if (idx < 0) {\n-              idx = idx + src_indexing_axis_dim;\n-            }\n-",
    "Added lines": "",
    "Label": "Buggy"
},
{
    "Id": 1272,
    "Library": "pytorch",
    "Date": "2023/05/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/faa7eb81c634492b70fcc0327622bb0aa812cacd",
    "Root Cause": "Error Message",
    "Bug report": "change error_message for XPU Autocast data type check (#102073)\n\nXPU autocast supports bf16 and fp16 data types, we are going to change the error_message for that.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/102073\nApproved by: https://github.com/jgong5",
    "Number of deleted lines": 1,
    "Deleted lines": "-                error_message += 'XPU Autocast only supports dtype of torch.bfloat16 currently.'",
    "Added lines": "+                error_message += 'XPU Autocast only supports dtypes of torch.bfloat16 and torch.float16 currently.'",
    "Label": "Buggy"
},
{
    "Id": 1273,
    "Library": "pytorch",
    "Date": "2023/11/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fb25fd6f865ed0532caf710ca130b6cc23a772a8",
    "Root Cause": "Others",
    "Bug report": "[DTensor] Replaced neg dim normalization with assert in helper (#114141)\n\nThis is a replacement for https://github.com/pytorch/pytorch/pull/113922. I think we can still leave the check for negative shard dimension in `compute_local_shape_and_global_offset` and replace the normalization logic with an assert. This should provide us a stack trace to see which user-facing API did not normalize the dim as expected.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114141\nApproved by: https://github.com/wanchaol\nghstack dependencies: #113919, #113924, #114134, #113925, #113930",
    "Number of deleted lines": 2,
    "Deleted lines": "-                # normalize shard dim to be positive\n-                shard_placement.dim += len(tensor_shape)",
    "Added lines": "+                raise AssertionError(\n+                    \"Shard placements should have negative dims normalized in \"\n+                    f\"the user-facing APIs: {shard_placement}\"\n+                )",
    "Label": "Buggy"
},
{
    "Id": 1274,
    "Library": "pytorch",
    "Date": "2022/05/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fe6aa0844466e5dd2669092eac5edde153108b28",
    "Root Cause": "Type Checking",
    "Bug report": "[PyTorch] IValue(const c10::Scalar&) improvements\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72551\n\nDon't delegate to `operator=` for construction. Catch hypothetical addition of a new Scalar type via debug assertion rather than checking in prod.\n\nDifferential Revision: [D34090560](https://our.internmc.facebook.com/intern/diff/D34090560/)\n\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 5,
    "Deleted lines": "-      *this = s.toDouble();\n-      *this = s.toBool();\n-    } else if (s.isIntegral(false)) {\n-      *this = s.toLong();\n-      TORCH_CHECK(false, \"Unknown type in Scalar\");",
    "Added lines": "+      tag = Tag::Double;\n+      payload.u.as_double = s.toDouble();\n+      tag = Tag::Bool;\n+      payload.u.as_bool = s.toBool();\n+      TORCH_INTERNAL_ASSERT_DEBUG_ONLY(s.isIntegral(false), \"Unknown type in Scalar\");\n+      tag  = Tag::Int;\n+      payload.u.as_int = s.toLong();",
    "Label": "Buggy"
},
{
    "Id": 1275,
    "Library": "tensorflow",
    "Date": "2023/08/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/00517642a356c5e04f009ea61c74638d89746392",
    "Root Cause": "Edge Cases",
    "Bug report": "Return error on invalid input in `tfl.splitv`\n\nPiperOrigin-RevId: 555138718",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      return kTfLiteError;\n+    return kTfLiteError;",
    "Label": "Buggy"
},
{
    "Id": 1276,
    "Library": "tensorflow",
    "Date": "2024/02/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/06b89ed1bdf606adb21d66664ca7ab5eaffdd58f",
    "Root Cause": "Others",
    "Bug report": "BundleReader was not waiting for concurrents reads to complete before\nchecking their result value.\n\nAlso changed the large value reading test to actually exercise the\nmulti-threaded reading path. Previously, the whole multi-threaded path\nwas being skipped because the reads were smaller than kBufferSize.\n\nPiperOrigin-RevId: 604300943",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (entry.size() > kBufferSize) {",
    "Added lines": "+    if (entry.size() > kBufferSize || enable_multi_threading_for_testing_) {\n+        reader_pool = nullptr;  // Wait for reads to finish\n+",
    "Label": "Buggy"
},
{
    "Id": 1277,
    "Library": "tensorflow",
    "Date": "2023/03/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/076f909b70b251daea6c443c9b1929b9745aed20",
    "Root Cause": "Edge Cases",
    "Bug report": "fix boolean expression in length check\n\nPiperOrigin-RevId: 513891216",
    "Number of deleted lines": 1,
    "Deleted lines": "-    OP_REQUIRES(ctx, length,",
    "Added lines": "+    OP_REQUIRES(ctx, length > 0,",
    "Label": "Buggy"
},
{
    "Id": 1278,
    "Library": "tensorflow",
    "Date": "2022/04/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0d5668cbdc6b46d099bd3abd93374c09b2e8121f",
    "Root Cause": "Edge Cases",
    "Bug report": "[XLA:SHAPE_UTIL] Return nullopt instead of a check failure if the input dimensions are not sorted.\n\nPiperOrigin-RevId: 439966260",
    "Number of deleted lines": 1,
    "Deleted lines": "-  CHECK(std::is_sorted(input_dim_indices.begin(), input_dim_indices.end()));",
    "Added lines": "+  if (!std::is_sorted(input_dim_indices.begin(), input_dim_indices.end())) {\n+    return absl::nullopt;\n+  }",
    "Label": "Buggy"
},
{
    "Id": 1279,
    "Library": "tensorflow",
    "Date": "2023/08/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/11030308c5d25df5b36f8a583f1b4607e4ea2b7f",
    "Root Cause": "Edge Cases",
    "Bug report": "Add a check to check if all sharding strategies are dropped due to infinity costs\n\nPiperOrigin-RevId: 560125787",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    size_t num_skipped_due_to_infinity_costs = 0;\n+        num_skipped_due_to_infinity_costs++;\n+    CHECK_LT(num_skipped_due_to_infinity_costs, strategies->leaf_vector.size())\n+        << \"All strategies removed due to infinite resharding costs\";",
    "Label": "Buggy"
},
{
    "Id": 1280,
    "Library": "tensorflow",
    "Date": "2021/11/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1595906c2192b7f402f746652042a592ad290378",
    "Root Cause": "Edge Cases",
    "Bug report": "Prevent `CHECK`-fail DOS in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.\n\nPiperOrigin-RevId: 410952517\nChange-Id: Ica269fc5695d2a467c2e3c7b0681d717d152e2a3",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(feature_indices_t->shape()),\n+                errors::InvalidArgument(\n+                    \"feature_indices must be a matrix, received shape \",\n+                    feature_indices_t->shape().DebugString()));",
    "Label": "Buggy"
},
{
    "Id": 1281,
    "Library": "tensorflow",
    "Date": "2023/04/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1610da3f992487bd9a8181d1e83cae99fe1e34d9",
    "Root Cause": "Edge Cases",
    "Bug report": "add more sanity check on AvgPoolGrad op",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+      OP_REQUIRES(\n+          context, orig_input_dims_mkl_order[0] == diff_dst_dims[0],\n+          errors::InvalidArgument(\n+              \"Expected first dimension of orig_input and diff_dst to match, \"\n+              \"got \",\n+              orig_input_dims_mkl_order[0], \" and \", diff_dst_dims[0]));\n+",
    "Label": "Buggy"
},
{
    "Id": 1282,
    "Library": "tensorflow",
    "Date": "2021/11/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1610f391833738972b538e4ee97f90dbd30fc745",
    "Root Cause": "Edge Cases",
    "Bug report": "Replace `DCHECK` with actual validation in `AddRangeStats`\n\nPiperOrigin-RevId: 411138402\nChange-Id: I123eafe70e292f286ba384d4ed73222edf61a6ea",
    "Number of deleted lines": 2,
    "Deleted lines": "-  DCHECK_LE(start_instance, end_instance);\n-    DCHECK_LT(start_feature_dim, end_feature_dim);",
    "Added lines": "+  OP_REQUIRES(context, start_instance <= end_instance,\n+              errors::InvalidArgument(\n+                  \"start_instance = \", start_instance,\n+                  \" which is not at most end_instance=\", end_instance));\n+    OP_REQUIRES(context, start_feature_dim < end_feature_dim,\n+                errors::InvalidArgument(\n+                    \"start_feature_dim = \", start_feature_dim,\n+                    \" which is not at most end_feature_dim=\", end_feature_dim));",
    "Label": "Buggy"
},
{
    "Id": 1283,
    "Library": "tensorflow",
    "Date": "2023/10/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1707ed9b9b0cc5cb02df22a06718c9c738825d39",
    "Root Cause": "Others",
    "Bug report": "Add a check to make sure that the allocation before an Evict() is not a prefetch.\n\nPiperOrigin-RevId: 577049438",
    "Number of deleted lines": 3,
    "Deleted lines": "-  // TODO(b/306478911): prev_allocation can never be a prefetch, or we would be\n-  // using an incorrect start time (we would need to wait until the copies\n-  // finish)",
    "Added lines": "+  // We do not ever expect an Evict() to be immediately proceeded by a prefetch.\n+  // If that case ever occurs, the eviction_exclusive_start_time below will be\n+  // calculated incorrectly, as it will need to come after the prefetch finishes\n+  // coping data.\n+  CHECK(!prev_allocation->is_copy_like_allocation())\n+      << \"Evict has been given copy-like previous allocation.\\nEvict \"\n+         \"candidate:\\n\"\n+      << request.allocation_value->ToString() << \"\\nPrevious allocation:\\n\"\n+      << prev_allocation->ToString();",
    "Label": "Buggy"
},
{
    "Id": 1284,
    "Library": "tensorflow",
    "Date": "2023/05/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/178d62a63ea043a4b9969b4cd6f8983eb8eae523",
    "Root Cause": "Error Message",
    "Bug report": "Update check failure to logging a warning for repeated computation placer registration.\n\nThis is to bypass a duplicated registration issue seen in open-source build during TF/PJRT integration.\n\nPiperOrigin-RevId: 532886771",
    "Number of deleted lines": 1,
    "Deleted lines": "-  CHECK(computation_placers->find(platform_id) == computation_placers->end());",
    "Added lines": "+  if (computation_placers->find(platform_id) != computation_placers->end()) {\n+    // TODO(b/282059652): Consider logging the platform name using\n+    // MultiPlatformManager::PlatformWithId(). No doing that for now to avoid\n+    // introducing unwanted dependency.\n+    LOG(WARNING) << \"computation placer already registered. Please check \"\n+                    \"linkage and avoid linking the same target more than once.\";\n+  }",
    "Label": "Buggy"
},
{
    "Id": 1285,
    "Library": "tensorflow",
    "Date": "2022/03/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/18dd91ccd4b1817cd5c34e40f76823a162bea029",
    "Root Cause": "Error Message",
    "Bug report": "[XLA] Report that real -> complex bitcast_convert is not allowed\n\nThe check as exists is bidirectional: it prevents conversions from complex to real and real to complex alike, but the reported error message was unidirectional.\n\nPiperOrigin-RevId: 432075224",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return InvalidArgument(\"Conversion from complex to real type %s => %s.\",",
    "Added lines": "+    return InvalidArgument(\"Conversion between complex and real type %s => %s.\",",
    "Label": "Buggy"
},
{
    "Id": 1286,
    "Library": "tensorflow",
    "Date": "2022/11/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1908d7ef706f0f3f8c7a300068355bf795fb3d17",
    "Root Cause": "Boundary Value",
    "Bug report": "Fix out-of-bounds StringPiece access in ForwardNUTF8CharPositions()\n\nEven a simple invocation like 'int p = 0; ForwardNUTF8CharPositions(\"a\", 1, &p);' will cause an invalid access to in[1]. Checking for *pos < size before that access fixes this issue.\n\nLuckily the invalid access has only ever happened when the *pos < size part of the condition is false and thus the outcome of the IsTrailByte check is irrelevant. Thus this probably hasn't had any observable impact except when extra guards against invalid memory accesses are enabled.\n\nPiperOrigin-RevId: 488292704",
    "Number of deleted lines": 1,
    "Deleted lines": "-    } while (IsTrailByte(in[*pos]) && *pos < size);",
    "Added lines": "+    } while (*pos < size && IsTrailByte(in[*pos]));",
    "Label": "Buggy"
},
{
    "Id": 1287,
    "Library": "tensorflow",
    "Date": "2022/03/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/199f1ff12a28d571100b323ec54a5eee47078d8b",
    "Root Cause": "Edge Cases",
    "Bug report": "Add necessary check in fft ops to fix crash\n\nThis PR tries to address the issue raised in 55263 where\ntf.single.rfft2d will crash when length contains negative value.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        OP_REQUIRES(\n+            ctx,\n+            fft_length_as_vec(i) >= 0,\n+            errors::InvalidArgument(\n+                \"fft_length[\" , i,\n+                \"] must >= 0, but got: \", fft_length_as_vec(i)));",
    "Label": "Buggy"
},
{
    "Id": 1288,
    "Library": "tensorflow",
    "Date": "2022/02/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/19b2e1b5868a044df4622ef7e26fa5570ca52e5e",
    "Root Cause": "Edge Cases",
    "Bug report": "Only perform scalar check for a tensor shape if it's not empty.\n\nPiperOrigin-RevId: 428100731\nChange-Id: I4b908cf448bcbb853023899f8b501b82c8ac03f0",
    "Number of deleted lines": 1,
    "Deleted lines": "-    DCHECK(weights.shape_.IsScalar());",
    "Added lines": "+    DCHECK(weights.shape_.IsEmpty() || weights.shape_.IsScalar());",
    "Label": "Buggy"
},
{
    "Id": 1289,
    "Library": "tensorflow",
    "Date": "2024/01/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1a1a381b5be7701843c3f1e34aa1846ae2a1d0ce",
    "Root Cause": "Null Value",
    "Bug report": "Fix a SIGSEGV bug in `InferShapeForXlaGatherOp`\n\nSince `ComputeOutputComponent` may return nullptr, we need to check for null attributes explicitly to be safe.\n\nPiperOrigin-RevId: 601487562",
    "Number of deleted lines": 1,
    "Deleted lines": "-             llvm::isa<DenseIntElementsAttr>(it->second)) {",
    "Added lines": "+             llvm::isa_and_nonnull<DenseIntElementsAttr>(it->second)) {",
    "Label": "Buggy"
},
{
    "Id": 1290,
    "Library": "tensorflow",
    "Date": "2023/11/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/20d54796563631c23c27548b321487e8b0c982a9",
    "Root Cause": "Null Value",
    "Bug report": "Add a nil check before init the device_name string, and also assign an empty string as a placeholder.\n\nPiperOrigin-RevId: 579225867",
    "Number of deleted lines": 1,
    "Deleted lines": "-    std::string device_name = std::string([[metal_device_ name] UTF8String]);",
    "Added lines": "+    auto utf8_name = [[metal_device_ name] UTF8String];\n+    const std::string device_name = utf8_name != nil ? utf8_name : \"\";",
    "Label": "Buggy"
},
{
    "Id": 1291,
    "Library": "tensorflow",
    "Date": "2021/11/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/23968a8bf65b009120c43b5ebcceaf52dbc9e943",
    "Root Cause": "Edge Cases",
    "Bug report": "Fix out of bound access in DequantizeOp by adding check for axis < input dimension\n\nPiperOrigin-RevId: 411214268\nChange-Id: I3249d2a69ddc82f182c589a3a5bbfb71543f4b29",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(\n+        ctx, axis_ < input.dims(),\n+        errors::InvalidArgument(\"Axis must be less than input dimension(\",\n+                                input.dims(), \"), got \", axis_));\n+",
    "Label": "Buggy"
},
{
    "Id": 1292,
    "Library": "tensorflow",
    "Date": "2023/03/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2465d4e77654f0d4f7799bc46d5fd5812590acc6",
    "Root Cause": "Edge Cases",
    "Bug report": "Add a check in auto-sharding setup and die if the input mesh shape contains more than two shardable dimensions, which is currently not supported.\n\nPiperOrigin-RevId: 518440655",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (spmd::VectorGreaterThanOneElementCount(device_mesh_shape) > 2) {\n+      return tsl::errors::OutOfRange(\n+          absl::StrCat(\"the auto-sharding pass currently does not support \",\n+                       \"more than two shardable dims: device_mesh_shape=\",\n+                       absl::StrJoin(device_mesh_shape, \",\")));\n+    }",
    "Label": "Buggy"
},
{
    "Id": 1293,
    "Library": "tensorflow",
    "Date": "2022/11/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/258233804f2bc92b4bdb9714b396aed34b53ff0d",
    "Root Cause": "Edge Cases",
    "Bug report": "sanity check of empty tensor on avgpool3d_grad",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      // For empty tensor, avg_pool_3d_grad in oneDNN doesn't handle this case\n+      if (orig_input_tensor.NumElements() == 0 ||\n+          grad_tensor.NumElements() == 0)\n+        return;\n+      ",
    "Label": "Buggy"
},
{
    "Id": 1294,
    "Library": "tensorflow",
    "Date": "2021/11/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/28dacabab5aac2963e37e622f4b157cf00d82662",
    "Root Cause": "Error Message",
    "Bug report": "[tf] Explicitly check that runner index is in bounds and runner is available\n\nPiperOrigin-RevId: 410516176\nChange-Id: I8caecaffb958f3cd5251d85a9f6dc7cbc5c06742",
    "Number of deleted lines": 2,
    "Deleted lines": "-    DCHECK_GT(runners_.size(), index);\n-    DCHECK(result.has_value());",
    "Added lines": "+    // Out of bounds vector access will throw an exception and anyway will crash\n+    // the binary, prefer a more readable error message.\n+    CHECK_GT(runners_.size(), index)  // Crash OK\n+        << \"runner index is out of bounds: index=\" << index\n+        << \" size=\" << runners_.size();\n+    CHECK(result.has_value())  // Crash OK\n+        << \"runner is not available: index=\" << index;",
    "Label": "Buggy"
},
{
    "Id": 1295,
    "Library": "tensorflow",
    "Date": "2023/08/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/40c7fe94824100338ef0c495143b26501b1c367e",
    "Root Cause": "Edge Cases",
    "Bug report": "Return error on invalid input in `tfl.topkv2`\n\nPiperOrigin-RevId: 554830225",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      return kTfLiteError;",
    "Label": "Buggy"
},
{
    "Id": 1296,
    "Library": "tensorflow",
    "Date": "2021/11/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/41ab69692ede0db3422fa70bc5889d470741e69c",
    "Root Cause": "Edge Cases",
    "Bug report": "Check for tensors to be vectors in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->vec` should only happen after checking that the tensor shape implies a vector. Otherwise, we can get denial of service via `CHECK`-fails\n\nPiperOrigin-RevId: 411054482\nChange-Id: Idc7a624dcc9e84685bf328b2d0e4842b904229dd",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(context, TensorShapeUtils::IsVector(feature_values_t->shape()),\n+                errors::InvalidArgument(\n+                    \"feature_values must be a vector, received shape \",\n+                    feature_values_t->shape().DebugString()));",
    "Label": "Buggy"
},
{
    "Id": 1297,
    "Library": "tensorflow",
    "Date": "2023/06/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4377a561b7757ed83757f07532e6564c42c286ba",
    "Root Cause": "Edge Cases",
    "Bug report": "Add a check for group size when sorting grouped AllReduces within a block.\n\nPiperOrigin-RevId: 538255219",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        // Maintain relative order of ALLReduces within the block.\n+                    if (lhs.empty() || rhs.empty()) {\n+                      // Skip order check if either group is empty.\n+                      return false;\n+                    }",
    "Label": "Buggy"
},
{
    "Id": 1298,
    "Library": "tensorflow",
    "Date": "2023/08/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/43fd10302bcc8447e7a7205bae848a3a88624775",
    "Root Cause": "Edge Cases",
    "Bug report": "Return error on invalid input in `tfl.atan2_custom`\n\nPiperOrigin-RevId: 556797683",
    "Number of deleted lines": 1,
    "Deleted lines": "-    default:",
    "Added lines": "+    default: {\n+      return TfLiteStatus::kTfLiteError;\n+    }",
    "Label": "Buggy"
},
{
    "Id": 1299,
    "Library": "tensorflow",
    "Date": "2023/09/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/450dec35448a73b3fcb5d4f82108d5fdcb3f59b4",
    "Root Cause": "Boundary Value",
    "Bug report": "Internal change, add some checks on the sparseTensor format checking.\n\nPiperOrigin-RevId: 568349775",
    "Number of deleted lines": 1,
    "Deleted lines": "-      *(row_ids_before_padding + i) = indices_matrix(i, 0);",
    "Added lines": "+    int32 previous_row_id = -1;\n+      int32 current_row_id = indices_matrix(i, 0);\n+      if (current_row_id < previous_row_id) {\n+        return absl::InvalidArgumentError(\n+            \"Invalid indices_or_row_splits input, indices of SparseTensor need \"\n+            \"to be sorted in ascending order.\");\n+      }\n+      *(row_ids_before_padding + i) = current_row_id;",
    "Label": "Buggy"
},
{
    "Id": 1300,
    "Library": "tensorflow",
    "Date": "2023/05/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/47eaa828a1dd4bf50ec4203ef4bbb348b3ef0dd0",
    "Root Cause": "Null Value",
    "Bug report": "Add nullptr check",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if ((&cc_tensor) == nullptr) {\n+    *tensor = nullptr;\n+    return;\n+  }",
    "Label": "Buggy"
},
{
    "Id": 1301,
    "Library": "tensorflow",
    "Date": "2023/02/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/48d3e51a1bd128554dd129251a51b6e12918a604",
    "Root Cause": "Edge Cases",
    "Bug report": "Add a check to HandleFromInput to ensure that the resource isn't empty.\n\nPiperOrigin-RevId: 510250667",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+// TODO(b/228388547) users of this method should be migrated to the one below.\n+  if (tensor->NumElements() == 0) {\n+    return errors::InvalidArgument(\"Empty resouce handle\");\n+  }",
    "Label": "Buggy"
},
{
    "Id": 1302,
    "Library": "tensorflow",
    "Date": "2021/11/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4f38b1ac8e42727e18a2f0bde06d3bee8e77b250",
    "Root Cause": "Null Value",
    "Bug report": "Prevent null dereference read in `GetInitOp`.\n\nWe have a map of maps. We test that the key exists in the first map but then we don't have any validation that this also means the second map has the needed key. In the scenarios where this is not the case, we'll dereference a nullptr, if we don't have this check\n\nPiperOrigin-RevId: 408739325\nChange-Id: If9bb7ed759aba1f3b56a34913f209508dbaf65ce",
    "Number of deleted lines": 3,
    "Deleted lines": "-    *init_op_name = init_op_sig_it->second.outputs()\n-                        .find(kSavedModelInitOpSignatureKey)\n-                        ->second.name();",
    "Added lines": "+    const auto& sig_def_outputs = init_op_sig_it->second.outputs();\n+    const auto& sig_def_outputs_it =\n+        sig_def_outputs.find(kSavedModelInitOpSignatureKey);\n+    if (sig_def_outputs_it == sig_def_outputs.end()) {\n+      return errors::FailedPrecondition(\"Could not find output \",\n+                                        kSavedModelInitOpSignatureKey);\n+    }\n+    *init_op_name = sig_def_outputs_it->second.name();",
    "Label": "Buggy"
},
{
    "Id": 1303,
    "Library": "tensorflow",
    "Date": "2021/11/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/54c94431e5dd17fc46d99da1a3f132c76414c161",
    "Root Cause": "Edge Cases",
    "Bug report": "Prevent `CHECK`-fail DOS in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.\n\nPiperOrigin-RevId: 410951880\nChange-Id: Id26099f022d68366eec03cc878e57bf6237ecccf",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsMatrix(hessians_t->shape()),\n+        errors::InvalidArgument(\"hessians must be a matrix, received shape \",\n+                                hessians_t->shape().DebugString()));",
    "Label": "Buggy"
},
{
    "Id": 1304,
    "Library": "tensorflow",
    "Date": "2022/05/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/580140611a47413dcf6373deb1250c0ed605e873",
    "Root Cause": "Backend Type",
    "Bug report": "[XLA] Do not check fail in proto copy from if the backend config proto and desired proto type do not match.\n\nPiperOrigin-RevId: 450516623",
    "Number of deleted lines": 2,
    "Deleted lines": "-    proto->CopyFrom(*proto_ptr);\n-    return Status::OK();",
    "Added lines": "+    if (proto_ptr->GetDescriptor() == proto->GetDescriptor()) {\n+      proto->CopyFrom(*proto_ptr);\n+      return Status::OK();\n+    }",
    "Label": "Buggy"
},
{
    "Id": 1305,
    "Library": "tensorflow",
    "Date": "2021/11/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5d96267d907ac2119cbccf1416b749195e8fd8de",
    "Root Cause": "Edge Cases",
    "Bug report": "Prevent `CHECK`-fail DOS in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.\n\nPiperOrigin-RevId: 410951067\nChange-Id: I73a968f2116cc14b3e0e868d8a188aa232b47643",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsMatrix(gradients_t->shape()),\n+        errors::InvalidArgument(\"gradients must be a matrix, received shape \",\n+                                gradients_t->shape().DebugString()));",
    "Label": "Buggy"
},
{
    "Id": 1306,
    "Library": "tensorflow",
    "Date": "2023/02/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5ed3c7881f1f039b1bb502eb68c65250de3bbac8",
    "Root Cause": "Edge Cases",
    "Bug report": "Fix ThreadPoolHandle 0 nthreads argument.\n\nIt was reported that a value of 0 leads to a check failure.  Using 0 to indicate\n`port::MaxParallelism`, for consistency with `Dataset`.\n\nFixes #59162\n\nPiperOrigin-RevId: 508092599",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+    // For consistency with Dataset, use MaxParallelism if 0 threads are\n+    // specified.\n+    if (num_threads_ == 0) {\n+      num_threads_ = port::MaxParallelism();\n+    }",
    "Label": "Buggy"
},
{
    "Id": 1307,
    "Library": "tensorflow",
    "Date": "2023/06/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/62cb54f2caf48480dc6b3c1ce9629eaac4688f83",
    "Root Cause": "Edge Cases",
    "Bug report": "Set 2nd output shape for SparseSegmentReduceGradV2\n\n- Fixes a debug check failure.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (outputs_unique_indices) {\n+    c->set_output(1, c->Vector(InferenceContext::kUnknownDim));\n+  }",
    "Label": "Buggy"
},
{
    "Id": 1308,
    "Library": "tensorflow",
    "Date": "2021/11/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6381a7b127bd276a3817a93e5423b15a06c33419",
    "Root Cause": "Edge Cases",
    "Bug report": "[tf.data] Add a check for ram_budget == 0 to avoid division by 0 exception when ram_budget is not set.\n\nPiperOrigin-RevId: 410071934\nChange-Id: Ida9fb401ba24367e48066c8a899962877429c3da",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (ram_budget == 0) {\n+    return;\n+  }",
    "Label": "Buggy"
},
{
    "Id": 1309,
    "Library": "tensorflow",
    "Date": "2023/09/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/63feaf321165e1e2795f43e3834c007364921df6",
    "Root Cause": "Edge Cases",
    "Bug report": "Add check for raster bits.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    // Stop load if no images are detected or the allocation of the last image\n+    // buffer was failed.\n+    if (gif_file->ImageCount <= 0 ||\n+        gif_file->SavedImages[gif_file->ImageCount - 1].RasterBits == NULL) {\n+    }\n+",
    "Label": "Buggy"
},
{
    "Id": 1310,
    "Library": "tensorflow",
    "Date": "2022/12/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/662128e8ca3411286b234553a7efc1356353d0f5",
    "Root Cause": "Edge Cases",
    "Bug report": "add rank checking for MEAN op\n\nThe MEAN op of NNAPI only supports a tensor with rank <= 4.\nCheck the rank of the input tensor before delegating the op.\n\n```\n...\n12-22 09:22:24.514  6130  6130 E ModelBuilder: Invalid Operation: NN_RET_CHECK failed (packages/modules/NeuralNetworks/common/types/operations/src/SimpleMath.cpp:30): inputRank <= 4u (inputRank = 5, 4u = 4) Unsupported input tensor rank for operation MEAN\n12-22 09:22:24.514  6130  6130 E tflite  : NN API returned error ANEURALNETWORKS_BAD_DATA at line 1131 while adding operation.\n12-22 09:22:24.515  6130  6130 E tflite  : Restored original execution plan after delegate application failure.\n...\n```",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      Expect(context->tensors[node->inputs->data[0]].dims->size <= 4,\n+             NNAPIValidationFailureType::kUnsupportedOperandValue,\n+             \"NNAPI does not support mean of a tensor with rank > 4\",\n+             &val_ctx);",
    "Label": "Buggy"
},
{
    "Id": 1311,
    "Library": "tensorflow",
    "Date": "2023/07/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/66e0cb1d9afd251931f4f920c5d7bd638bc882b4",
    "Root Cause": "Edge Cases",
    "Bug report": "validate clip_norm argument in clip_by_norm API\n\nThe API clip_by_norm have argument clip_norm which accepts  0-D (scalar) `Tensor` > 0 . But if we pass -ve value for this argument then its not raising intended error and converting the input tensor into Negative which IMO is wrong. Hence I am adding validation code for -ve values to raise value error.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if clip_norm < 0:\n+      raise ValueError('clip_norm should be a 0-D (scalar) Tensor > 0')",
    "Label": "Buggy"
},
{
    "Id": 1312,
    "Library": "tensorflow",
    "Date": "2023/03/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/685418cd85e09bc2117fa15bc1b6a75d21248348",
    "Root Cause": "Edge Cases",
    "Bug report": "maxpooling op should check that ksize must be positive.\n\nPiperOrigin-RevId: 520539022",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      OP_REQUIRES(\n+          context,\n+          ksize_[0] > 0 && ksize_[1] > 0 && ksize_[2] > 0 && ksize_[3] > 0,\n+          errors::InvalidArgument(\"Sliding window ksize must be positive.\"));\n+    OP_REQUIRES(\n+        context, ksize[0] > 0 && ksize[1] > 0 && ksize[2] > 0 && ksize[3] > 0,\n+        errors::InvalidArgument(\"Sliding window ksize must be positive.\"));",
    "Label": "Buggy"
},
{
    "Id": 1313,
    "Library": "tensorflow",
    "Date": "2023/12/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6c472f6632c4864da749e7a4aee8c001a905287f",
    "Root Cause": "Device Version",
    "Bug report": "only found `CU_MEM_LOCATION_TYPE_HOST`, `CU_MEM_LOCATION_TYPE_HOST_NUMA` and `CU_MEM_LOCATION_TYPE_HOST_NUMA_CURRENT` in [CUDA version 12.3.1 doc](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TYPES.html), and didn't find the evidence of their existence in formal versions' doc, so suggest to use check `CUDA_VERSION` at `12030` here, for `maxSize`, resolved directly in the same way\n\nPiperOrigin-RevId: 589226039",
    "Number of deleted lines": 4,
    "Deleted lines": "-#if CUDA_VERSION >= 12000\n-#endif  // CUDA_VERSION >= 12000\n-#if CUDA_VERSION >= 12000\n-#endif  // CUDA_VERSION >= 12000",
    "Added lines": "+#if CUDA_VERSION >= 12030\n+#endif  // CUDA_VERSION >= 12030\n+#if CUDA_VERSION >= 12030\n+#endif  // CUDA_VERSION >= 12030",
    "Label": "Buggy"
},
{
    "Id": 1314,
    "Library": "tensorflow",
    "Date": "2023/07/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6e153325b66330dafea4e4e8b67b5d56b1a37852",
    "Root Cause": "Edge Cases",
    "Bug report": "[XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast produces a\nscalar. This avoids crashing within last_dimension when attempting to match.\n\nPiperOrigin-RevId: 548090995",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (bitcast->shape().rank() == 0) {\n+    return true;\n+  }\n+",
    "Label": "Buggy"
},
{
    "Id": 1315,
    "Library": "tensorflow",
    "Date": "2024/01/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/798b2ebda0cc6f12f1ca6460611f760149771a11",
    "Root Cause": "Type Checking",
    "Bug report": "Ensure the allocation type is kTfLiteCustom when doing shallow copies in DeepOrShallowCopyTensorsShapeTypeData.\n\nThis code is correct only under the assumption that the caller has correctly prepared\nthe tensors that get passed in for shallow copying, by setting their allocation\ntypes to kTfLiteCustom. This ensures that those tensors won't be double `free`'d\nlater on. This check simply ensures that that assumption always holds, to ensure\nwe fail early if ever a bug is introduced that breaks that assumption.\n\nPiperOrigin-RevId: 597990125",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      // Make a shallow copy of the data. This is only safe because the caller\n+      // is expected to have previously set dst_tensor->allocation_type to\n+      // kTfLiteCustom, to ensure the buffer is never double-freed later on.\n+      TF_LITE_ENSURE_EQ(context, dst_tensor->allocation_type, kTfLiteCustom);",
    "Label": "Buggy"
},
{
    "Id": 1316,
    "Library": "tensorflow",
    "Date": "2023/03/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/80b65ab79bf8dd6ec03c570b59a1208bb27fec24",
    "Root Cause": "Edge Cases",
    "Bug report": "Small fix to axis check for tfl.pack to tosa\n\nThere was an off-by-one error when checking the axis value\nbased on the input rank.\n\nPiperOrigin-RevId: 516334935",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if ((axis < 0) || (axis > (input_tensor_rank + 1))) {",
    "Added lines": "+  if ((axis < 0) || (axis > input_tensor_rank)) {",
    "Label": "Buggy"
},
{
    "Id": 1317,
    "Library": "tensorflow",
    "Date": "2022/05/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/80bb2f5511e7d2d386c79da52ff517691e19ac54",
    "Root Cause": "Edge Cases",
    "Bug report": "Add check condition for large values of range_max, which is causing session abort.\nFixes: https://github.com/tensorflow/tensorflow/issues/46938\n\nPiperOrigin-RevId: 450688945",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  # Limiting to Max int32 value\n+  if range_max > 2147483647:\n+    raise ValueError(f'Value of range_max:{range_max} is too large to handle')",
    "Label": "Buggy"
},
{
    "Id": 1318,
    "Library": "tensorflow",
    "Date": "2023/11/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/81ff894c113a5912ba52078ac27e36d06831112e",
    "Root Cause": "Boundary Value",
    "Bug report": "[XLA] Add bounds checks to xla::Array::Slice\n\nTo guard against specifying limits that are out of bounds, which ends up\ntouching OOB data.\n\nPiperOrigin-RevId: 578475805",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      CHECK_GE(starts[i], 0);\n+      CHECK_LE(limits[i], dim(i));",
    "Label": "Buggy"
},
{
    "Id": 1319,
    "Library": "tensorflow",
    "Date": "2022/03/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/824af2acfa0cdf897c08d91224aea0958c1afc02",
    "Root Cause": "Edge Cases",
    "Bug report": "Add ndmin check\n\nAdded ndmin check to allow maximum 32 ndmin to make same behavior as numpy.\r\nCurrently it is crashing when very large ndmin is passed.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  max_ndmin = 32\n+  if ndmin > max_ndmin:\n+    raise ValueError('ndmin bigger than allowable number of dimensions: '\n+                     f'{max_ndmin}.')\n+  ",
    "Label": "Buggy"
},
{
    "Id": 1320,
    "Library": "tensorflow",
    "Date": "2023/06/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84a1cf61dd7239aa5d682083d34e0f7c99039734",
    "Root Cause": "Error Message",
    "Bug report": "[XLA] Do not suggest trying to use TF_XLA_FLAGS when failing to parse XLA_FLAGS\n\nThe error can be very misleading, as we never check whether the new flag is actually supported by TF_XLA_FLAGS.\n\nPiperOrigin-RevId: 543680270",
    "Number of deleted lines": 18,
    "Deleted lines": "-\n-    // Some flags are set on XLA_FLAGS, others on TF_XLA_FLAGS.  If we find an\n-    // unrecognized flag, suggest the alternative.\n-    std::string alternate_envvar;\n-    if (envvar == \"TF_XLA_FLAGS\") {\n-      alternate_envvar = \"XLA_FLAGS\";\n-    } else if (envvar == \"XLA_FLAGS\") {\n-      alternate_envvar = \"TF_XLA_FLAGS\";\n-    }\n-    std::string did_you_mean;\n-    if (!alternate_envvar.empty()) {\n-      did_you_mean = absl::StrFormat(\n-          \"\\nPerhaps you meant to specify these on the %s envvar?\",\n-          alternate_envvar);\n-    }\n-\n-                << \" in \" << envvar << \": \" << absl::StrJoin(unknown_flags, \" \")\n-                << did_you_mean;",
    "Added lines": "+                << \" in \" << envvar << \": \"\n+                << absl::StrJoin(unknown_flags, \" \");",
    "Label": "Buggy"
},
{
    "Id": 1321,
    "Library": "tensorflow",
    "Date": "2023/05/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84d7bf6f64fd9c8677f7f26511ce3031fe8d35a6",
    "Root Cause": "Type Checking",
    "Bug report": "Add is_numeric to dtypes.cc to check whether a data type is numeric",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      .def_property_readonly(\n+          \"is_numeric\",\n+          [](tensorflow::DataType self) {\n+            return tensorflow::DataTypeIsNumeric(tensorflow::BaseType(self));\n+          },\n+          \"Returns whether this is a numeric data type.\")",
    "Label": "Buggy"
},
{
    "Id": 1322,
    "Library": "tensorflow",
    "Date": "2022/02/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/86abddb56350bccd95d1b7140b003fb03525b890",
    "Root Cause": "Error Message",
    "Bug report": "Add appropriate error check for nbins in tf.histogram_fixed_width_bins\n\nThis PR tries to address the issue raised in 54415 where\nnbins was not checked for tf.histogram_fixed_width_bins\nand an incorrect result was returned when nbins < 0.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+from tensorflow.python.ops import control_flow_ops\n+    check = control_flow_ops.Assert(\n+        math_ops.greater(nbins, 0), [\"nbins %s must > 0\" % nbins])\n+    nbins = control_flow_ops.with_dependencies([check], nbins)",
    "Label": "Buggy"
},
{
    "Id": 1323,
    "Library": "tensorflow",
    "Date": "2022/02/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/88609e2e22fa5c296de2e27e04d1cc4743b2dfcd",
    "Root Cause": "Type Checking",
    "Bug report": "Add appropriate dtype check for tf.boolean_mask's mask\n\nThis PR tries to address the issue raised in 54412 where\nmask's dtype was checked in tf.boolean_mask and an invalid\nresult has been returned instead.\n\nThis PR fixes 54412.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if mask.dtype != dtypes.bool:\n+      raise TypeError(\"Invalid `mask`: expected bool but got %s.\" % mask.dtype)",
    "Label": "Buggy"
},
{
    "Id": 1324,
    "Library": "tensorflow",
    "Date": "2023/10/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/893aa7518fe3175739ac1ba70d7355a0b091115c",
    "Root Cause": "Null Value",
    "Bug report": "Added a null check in `string_util.cc`\n\nPiperOrigin-RevId: 573300006",
    "Number of deleted lines": 2,
    "Deleted lines": "-#include <limits>\n-",
    "Added lines": "+#include <cstddef>\n+  if (*buffer == nullptr) {\n+    return -1;\n+  }\n+",
    "Label": "Buggy"
},
{
    "Id": 1325,
    "Library": "tensorflow",
    "Date": "2023/04/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8a2e7deb21f02e4072d6b62cf7f447b9264afe01",
    "Root Cause": "Type Checking",
    "Bug report": "Adjust checks for `type(Tensor)` to isinstance or is_eager/is_symbolic_tensor.\n\nPiperOrigin-RevId: 525801792",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if tensors_type is ops.Tensor:",
    "Added lines": "+  if isinstance(tensors, ops.Tensor):",
    "Label": "Buggy"
},
{
    "Id": 1326,
    "Library": "tensorflow",
    "Date": "2023/05/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8b742f8559e88474735d0a2c03e00da65e40b412",
    "Root Cause": "Edge Cases",
    "Bug report": "Fix check error on shape overflow.\n\nFixes #60198, #60275\n\nPiperOrigin-RevId: 529127714",
    "Number of deleted lines": 2,
    "Deleted lines": "-    input_matrix_shapes->emplace_back(\n-        std::initializer_list<int64_t>({num_rows, num_cols}));",
    "Added lines": "+    TensorShape input_shape;\n+    OP_REQUIRES_OK(context, TensorShape::BuildTensorShape({num_rows, num_cols},\n+                                                          &input_shape));\n+    input_matrix_shapes->push_back(std::move(input_shape));",
    "Label": "Buggy"
},
{
    "Id": 1327,
    "Library": "tensorflow",
    "Date": "2023/03/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8c3822edbb31cf71cedaf49f2167e45c1e2d0b83",
    "Root Cause": "Execution Mode",
    "Bug report": "Update the is_dtensor check to only run in eager mode.\n\nPiperOrigin-RevId: 516294602",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+    Raises:\n+      RuntimeError: When not called eagerly.\n+    if not context.executing_eagerly():\n+      raise RuntimeError(\"is_dtensor must be called eagerly.\")",
    "Label": "Buggy"
},
{
    "Id": 1328,
    "Library": "tensorflow",
    "Date": "2021/12/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8c6f391a2282684a25cbfec7687bd5d35261a209",
    "Root Cause": "Edge Cases",
    "Bug report": "[lite] Add check for bias_size is zero to avoid division by zero. This shouldn't happen for properly converted models. Just safety check\n\nPiperOrigin-RevId: 416383645\nChange-Id: If8e508bf696ae8ecfb927e69c139a8ccf7fe60cb",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (bias_size == 0) return;",
    "Label": "Buggy"
},
{
    "Id": 1329,
    "Library": "tensorflow",
    "Date": "2021/11/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8d733ecdb270dd90b2b5f53fd220d5ce17a5e20f",
    "Root Cause": "Edge Cases",
    "Bug report": "Check for tensors to be vectors in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->vec` should only happen after checking that the tensor shape implies a vector. Otherwise, we can get denial of service via `CHECK`-fails\n\nPiperOrigin-RevId: 410960878\nChange-Id: I7b26bec796cbaebde4696862eb855160402b4b0d",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(node_ids_t->shape()),\n+        errors::InvalidArgument(\"node_ids must be a vector, received shape \",\n+                                node_ids_t->shape().DebugString()));",
    "Label": "Buggy"
},
{
    "Id": 1330,
    "Library": "tensorflow",
    "Date": "2023/05/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/924f80a4fdb34230965a7a8a4476901847463645",
    "Root Cause": "Type Checking",
    "Bug report": "Add stricter type checking for tf.math.real\n\nFix for tf.math.real so that it only accepts tensors with numeric entries as input. This makes it consistent with its documentation at https://www.tensorflow.org/api_docs/python/tf/math/real and raises a TypeError saying input must have numeric entries when called incorrectly.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    else:",
    "Added lines": "+    elif tf.debugging.is_numeric_tensor(input):\n+    else:\n+      raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))",
    "Label": "Buggy"
},
{
    "Id": 1331,
    "Library": "tensorflow",
    "Date": "2024/01/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9718fed7b9aba244359b3d38c2a1dc20e50428bd",
    "Root Cause": "Computation Graph",
    "Bug report": "Added size check to avoid memory corruption in GraphDefImporter::ConvertNodeDef.\n\nPiperOrigin-RevId: 601277773",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+  if (op_def->output_arg_size() < 0)\n+    return InvalidArgument(\"Node \", node.name(), \" output arg size < 0\");",
    "Label": "Buggy"
},
{
    "Id": 1332,
    "Library": "tensorflow",
    "Date": "2023/07/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9720b405905dee209a3f7d003de21d388e1aaef4",
    "Root Cause": "Null Value",
    "Bug report": "Avoid nullptr as row offsets to cusparseCreateCsr\n\nAs of CUDA 12.2 additional input validation allows NULL for the row offsets\nonly when rows=0.",
    "Number of deleted lines": 1,
    "Deleted lines": "-                         nullptr, nullptr, nullptr));",
    "Added lines": "+                         c_row_ptr.data(), nullptr, nullptr));",
    "Label": "Buggy"
},
{
    "Id": 1333,
    "Library": "tensorflow",
    "Date": "2023/03/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a0de0ca6a39f3037e1be6ec740829863bcda3e8",
    "Root Cause": "Type Checking",
    "Bug report": "[XLA:GPU] Fix type check in IsMatrixMultiplication\n\nPiperOrigin-RevId: 513638308",
    "Number of deleted lines": 1,
    "Deleted lines": "-       lhs_shape.element_type() == S8);",
    "Added lines": "+       rhs_shape.element_type() == S8);",
    "Label": "Buggy"
},
{
    "Id": 1334,
    "Library": "tensorflow",
    "Date": "2023/05/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a4b6b6bcc7a813162bf0378727950e321aca19c",
    "Root Cause": "Type Checking",
    "Bug report": "Add stricter type checking for tf.math.real (using is_numeric)",
    "Number of deleted lines": 1,
    "Deleted lines": "-    elif tf.debugging.is_numeric_tensor(input):",
    "Added lines": "+    elif input.dtype.is_numeric:",
    "Label": "Buggy"
},
{
    "Id": 1335,
    "Library": "tensorflow",
    "Date": "2022/03/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9c92b50fc4b95985a0749101976d04896bf19bfe",
    "Root Cause": "Edge Cases",
    "Bug report": "[conv3d_transpose] Fix dim check for bias\n\nPer discussion with @thaink, the previous way to do the dim check for bias is not correct. So we need this change.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 4));",
    "Added lines": "+    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 3));",
    "Label": "Buggy"
},
{
    "Id": 1336,
    "Library": "tensorflow",
    "Date": "2023/01/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9d3cce4c7525bad6743f84302e5f6355a3fd8fe5",
    "Root Cause": "Edge Cases",
    "Bug report": "Fix crash in BlockLSTM\n\nThis PR tries to address the issue raised in 58175 in addressing\nthe crash of BlockLSTM when invalid input is provided.\n\nThis PR fixes 58175.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(seq_len_max_tensor->shape()),\n+                errors::InvalidArgument(\"`seq_len_max_tensor` must be rank 0 but is rank \",\n+                                        seq_len_max_tensor->dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(seq_len_max_tensor->shape()),\n+                errors::InvalidArgument(\"`seq_len_max_tensor` must be rank 0 but is rank \",\n+                                        seq_len_max_tensor->dims()));",
    "Label": "Buggy"
},
{
    "Id": 1337,
    "Library": "tensorflow",
    "Date": "2023/03/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a0dc73569fc193c1ce26a7bd2d4a8776e7b813ac",
    "Root Cause": "Edge Cases",
    "Bug report": "add check for empty cs_prev_tensor",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(ctx, \n+        cs_prev_tensor->dim_size(0) > 0 && cs_prev_tensor->dim_size(1) > 0,\n+                errors::InvalidArgument(\"cs_prev_tensor is empty, has shape: (\",\n+                            cs_prev_tensor->dim_size(0), \",\", cs_prev_tensor->dim_size(1), \").\"));",
    "Label": "Buggy"
},
{
    "Id": 1338,
    "Library": "tensorflow",
    "Date": "2023/03/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a21ec782601aca6c7e0461093d72596f26229e44",
    "Root Cause": "Type Checking",
    "Bug report": "Use `getattr` instead of `isinstance` in `tensor_conversion_registry`.\n\nUsing `isinstance` to check if an object is\nan instance of a Python `typing.Protocol` instead\nof using `getattr`/`hasattr` has negative performance\nimplications.\n\nThis change reverts `tensor_conversion_registry.convert()`\nto use `getattr` for this reason.\n\nPiperOrigin-RevId: 513547008",
    "Number of deleted lines": 2,
    "Deleted lines": "-  if isinstance(value, core.TensorProtocol):\n-    return value.__tf_tensor__(dtype, name)",
    "Added lines": "+  overload = getattr(value, \"__tf_tensor__\", None)\n+  if overload is not None:\n+    return overload(dtype, name)  #  pylint: disable=not-callable",
    "Label": "Buggy"
},
{
    "Id": 1339,
    "Library": "tensorflow",
    "Date": "2023/02/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a63f3006f703428ff980748cdbe24d6a13f761e2",
    "Root Cause": "Execution Mode",
    "Bug report": "Skip checking for graph_key in V1 optimizer when running in eager mode.\n\nPiperOrigin-RevId: 509660850",
    "Number of deleted lines": 1,
    "Deleted lines": "-      if variable_object._graph_key == current_graph_key:  # pylint: disable=protected-access",
    "Added lines": "+      # Skip checking for graph key for eager mode since there's only one graph.\n+      # This is necessary because there are cases where _trackable_children() is\n+      # called in a differenr thread from the main thread (e.g., async\n+      # checkpoint) and hence the default graph key would be different.\n+      if (context.executing_eagerly()\n+          or variable_object._graph_key == current_graph_key):  # pylint: disable=protected-access",
    "Label": "Buggy"
},
{
    "Id": 1340,
    "Library": "tensorflow",
    "Date": "2022/05/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a68f57a24203fd49c4a5c4a8f51098d4415a93f8",
    "Root Cause": "Edge Cases",
    "Bug report": "[XNNPACK] Add missing return when output channels do not match in TransposeConvolution\n\nAdd a check that input channels in the filter and tensor match.\n\nPiperOrigin-RevId: 448040780",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      return kTfLiteError;\n+    }\n+    if (input_channels != input_tensor_dims[3]) {\n+      TF_LITE_MAYBE_KERNEL_LOG(\n+          logging_context,\n+          \"transpose convolution kernel input channel dimension (%d) \"\n+          \"doesn't match filter input channel (%d) in node #%d\",\n+          input_channels, input_tensor_dims[3]);\n+      return kTfLiteError;",
    "Label": "Buggy"
},
{
    "Id": 1341,
    "Library": "tensorflow",
    "Date": "2023/02/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ab60b0ee51a8924a0f02b0152cd6a78ba64d3e94",
    "Root Cause": "Boundary Value",
    "Bug report": "[tfg] Fix named-attribute token check.\n\nSince the name tokens are being indexed directly, we should check that list of tokens is not empty to prevent an out-of-bounds error.\n\nPiperOrigin-RevId: 511553573",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include <vector>\n+    TF_RET_CHECK(!name_tokens.empty());",
    "Label": "Buggy"
},
{
    "Id": 1342,
    "Library": "tensorflow",
    "Date": "2023/08/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b4aadb17b7aa5ea926b5220008e41f33e582baed",
    "Root Cause": "Edge Cases",
    "Bug report": "Return error on invalid input in `tfl.where`\n\nPiperOrigin-RevId: 554544503",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      return kTfLiteError;\n+        return kTfLiteError;\n+      return kTfLiteError;",
    "Label": "Buggy"
},
{
    "Id": 1343,
    "Library": "tensorflow",
    "Date": "2023/02/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b677392e4af8095dbde8068b0ceb60bca815e94b",
    "Root Cause": "Null Value",
    "Bug report": "Reject non-PjRt devices in PjRtArray::Reshard()\n\nPjRt buffers traditionally support some degree of interoperability between PjRt\nclients (e.g., CPU and TPU). However, this is not universally true between\narbitrary IFRT clients that may use a non-PjRt-compatible runtime. This change\nadds extra checks to make sure that non-PjRt devices are not accidentally used\nin PjRtArray's destination devices.\n\nIn the future, ifrt::Device (currently aliasing to PjRtDevice) will introduce\nstronger type checking.\n\nPiperOrigin-RevId: 507846317",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      if (new_sharding->devices()[i]->client() == nullptr) {\n+        return InvalidArgument(\n+            \"The destination device is owned by a non-PjRt-compatible client. \"\n+            \"To use this Array on the destination device, the Array must be \"\n+            \"first fetched to the host and then sent to the destination \"\n+            \"device.\");\n+      }",
    "Label": "Buggy"
},
{
    "Id": 1344,
    "Library": "tensorflow",
    "Date": "2021/11/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b73a3c21a224f479af8d3b8af320c220a091906c",
    "Root Cause": "Edge Cases",
    "Bug report": "[XLA] Add check for potential out-of-bound access.\n\nPiperOrigin-RevId: 407330138\nChange-Id: Ib32dfd4ac963bbb79e3dde999334c81c8b87f55f",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TF_RET_CHECK(sort_dim >= 0 && sort_dim < increment.size())\n+      << \"Unexpected out-of-bound sort dimension \" << sort_dim\n+      << \" accessing increment of size \" << increment.size();",
    "Label": "Buggy"
},
{
    "Id": 1345,
    "Library": "tensorflow",
    "Date": "2022/12/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b8431494de404b5f4def7303fb8efd6ba3575ef9",
    "Root Cause": "Error Message",
    "Bug report": "Fix error log messages in data type checks\n\nPiperOrigin-RevId: 492303173",
    "Number of deleted lines": 3,
    "Deleted lines": "-                           \"unsupported zero-point value (%f) for UINT8 tensor \"\n-                           scale, t);\n-                             \"unsupported zero-point value (%f) for INT8 \"",
    "Added lines": "+                           \"unsupported zero-point value (%d) for UINT8 tensor \"\n+                           zero_point, t);\n+                             \"unsupported zero-point value (%d) for INT8 \"",
    "Label": "Buggy"
},
{
    "Id": 1346,
    "Library": "tensorflow",
    "Date": "2023/03/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2fc1f2b5a8b8152c43b81cf31394f3e0a2cb837",
    "Root Cause": "Null Value",
    "Bug report": "Add null pointer check",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  CHECK(a.opaque() != nullptr);\n+",
    "Label": "Buggy"
},
{
    "Id": 1347,
    "Library": "tensorflow",
    "Date": "2023/04/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2ff14318050e26302785a49a1719d29ddcc91b4",
    "Root Cause": "Edge Cases",
    "Bug report": "[XNNPACK] Fix incorrect check in slice node\n\nbegin+size == input dimension is valid, e.g. input size is 3, begin is 2, size is 1.\n\nPiperOrigin-RevId: 523713369",
    "Number of deleted lines": 2,
    "Deleted lines": "-      if (begin[i] + size[i] >= input_shape->data[i]) {\n-                                 \") must be less input \"",
    "Added lines": "+      if (begin[i] + size[i] > input_shape->data[i]) {\n+                                 \") must not be greater than input \"",
    "Label": "Buggy"
},
{
    "Id": 1348,
    "Library": "tensorflow",
    "Date": "2024/02/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c510c1b8b1ef5be1d65971f5b9e21e61becd0bb3",
    "Root Cause": "Type Checking",
    "Bug report": "[XLA] Remove IsCalledComputation from HloComputation.\n\nThe function doesn't do what it seems. There are more types of called instruction that are not accounted in this check.\n\nPiperOrigin-RevId: 605270151",
    "Number of deleted lines": 6,
    "Deleted lines": "-    CHECK(!IsCalledComputation());\n-  // Returns if this computation is invoked by an Hlo instruction.\n-  bool IsCalledComputation() const {\n-    return IsFusionComputation() || IsCustomCallComputation();\n-  }\n-",
    "Added lines": "+    // TODO: Add instruction type for async instructions.\n+    CHECK(instruction_type() == InstructionType::kUnset);",
    "Label": "Buggy"
},
{
    "Id": 1349,
    "Library": "tensorflow",
    "Date": "2022/09/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c676a2d7ce8884aad59ca9cd5f45e9b851574cac",
    "Root Cause": "Edge Cases",
    "Bug report": "[tensorflow] Add a check that strided slice op strides argument has reasonable size\n\nPiperOrigin-RevId: 478036251",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return errors::InvalidArgument(\"Unexpected negative dense.dims\");",
    "Added lines": "+    return errors::InvalidArgument(\"Unexpected negative dense.dims: %d\",\n+                                   dense->dims);\n+  }\n+\n+  if (dense->dims >= 1024) {\n+    // We do not expect to see tensors with rank >= 1024, it must mean that\n+    // there is a bug somewhere.\n+    return errors::InvalidArgument(\"Unexpected large dense.dims: %d\",\n+                                   dense->dims);",
    "Label": "Buggy"
},
{
    "Id": 1350,
    "Library": "tensorflow",
    "Date": "2021/11/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cb164786dc891ea11d3a900e90367c339305dc7b",
    "Root Cause": "Edge Cases",
    "Bug report": "Properly handle the case where `SpecializeType()` returns an error `Status`.\n\nIf the error case in `SpecializeType()` is reached, then we would get a crash when trying to access the value of an errorenous `StatusOr` object\n\nPiperOrigin-RevId: 408380069\nChange-Id: If3c3fc876dcf9384d5ec7a4985adc68c23ea7318",
    "Number of deleted lines": 1,
    "Deleted lines": "-  DCHECK(ret.status().ok()) << \"while instantiating types: \" << ret.status();",
    "Added lines": "+  if (!ret.status().ok()) {\n+    construction_status_ = ret.status();\n+    return;\n+  }",
    "Label": "Buggy"
},
{
    "Id": 1351,
    "Library": "tensorflow",
    "Date": "2023/05/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cd34289b744040974ebe81e1b1e88f1c752d68e0",
    "Root Cause": "Type Checking",
    "Bug report": "Update types.h to check if a data type is numeric",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+// Returns true iff 'dt' is a numeric type.\n+inline bool DataTypeIsNumeric(DataType dt) {\n+  return kNumberTypes.Contains(dt);\n+}\n+",
    "Label": "Buggy"
},
{
    "Id": 1352,
    "Library": "tensorflow",
    "Date": "2021/11/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d23458fdd2655c83ff9d54725062ded31b644ba4",
    "Root Cause": "Edge Cases",
    "Bug report": "[XLA:CPU] Do not check that the size of the XLA parameter buffer is exactly equal to the size of the underlying given buffer\n\nInstead, check that the underlying allocation is \"large enough\". This is also\nmore consistent with XLA:GPU behavior.\n\nThe mismatch can happen when the input comes from tf.where, which is backed by\nan allocation larger than is actually required.\n\nProviding tests is a bit of a chicken-and-an-egg problem: this is tested in\nwhere ops tests, which are disabled due to this issue.\n\nPiperOrigin-RevId: 411923802\nChange-Id: Iee3da305b9692ff37bdecba058629419e3f495c3",
    "Number of deleted lines": 1,
    "Deleted lines": "-    CHECK_EQ(allocation.size(), out.size())",
    "Added lines": "+    CHECK_LE(allocation.size(), out.size())",
    "Label": "Buggy"
},
{
    "Id": 1353,
    "Library": "tensorflow",
    "Date": "2024/01/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d5862d423742ec26c46737d4526eca3b8b8a0d9b",
    "Root Cause": "Edge Cases",
    "Bug report": "[TFLite] Add check in Softmax reference function to ensure exponent is within valid range\n\n* Add check to ensure the exponent does not cause an overflow in gemmlowp::RoundingDivideByPOT",
    "Number of deleted lines": 2,
    "Deleted lines": "-            (shifted_scale * exp_in_0).raw(),\n-            num_bits_over_unit + 31 - (sizeof(OutputT) * 8));",
    "Added lines": "+    const int exponent = num_bits_over_unit + 31 - (sizeof(OutputT) * 8);\n+    TFLITE_CHECK(0 <= exponent && exponent <= 31);\n+\n+            (shifted_scale * exp_in_0).raw(), exponent);",
    "Label": "Buggy"
},
{
    "Id": 1354,
    "Library": "tensorflow",
    "Date": "2023/02/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dd7d791e02396346d98b7b2c58137d7e51756c0c",
    "Root Cause": "Execution Mode",
    "Bug report": "Add isinstance check for eager execution.\n\nPiperOrigin-RevId: 507003564",
    "Number of deleted lines": 2,
    "Deleted lines": "-\n-  if isinstance(v, internal.NativeObject):",
    "Added lines": "+  if isinstance(v, EagerTensor) and not context.executing_eagerly():\n+    return convert_to_tensor(v, as_ref=True).op, None\n+  elif isinstance(v, internal.NativeObject):",
    "Label": "Buggy"
},
{
    "Id": 1355,
    "Library": "tensorflow",
    "Date": "2023/07/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e1ad3b74ad44b883c7b3fdc3a19adcea1d28bfbc",
    "Root Cause": "Edge Cases",
    "Bug report": "[XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast is an\neffective scalar. This short-circuit avoids crashing within last_dimension when\nattempting to match and either the operand or the result of the bitcast has a\nshape with rank 0.\n\nPiperOrigin-RevId: 548645429",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (bitcast->shape().rank() == 0) {",
    "Added lines": "+  if (ShapeUtil::IsEffectiveScalar(bitcast->shape())) {",
    "Label": "Buggy"
},
{
    "Id": 1356,
    "Library": "tensorflow",
    "Date": "2023/11/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e1dbfeba8acb1df8f42dfa6f76262f5cb23e1fa1",
    "Root Cause": "Device Version",
    "Bug report": "[stream_executor] NFC: Guard new features with CUDA_VERSION check\n\nPiperOrigin-RevId: 586180704",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#if CUDA_VERSION >= 12000\n+#else\n+    case GpuDriver::MemLocationType::kHost:\n+    case GpuDriver::MemLocationType::kHostNuma:\n+    case GpuDriver::MemLocationType::kHostNumaCurrent:\n+      return CU_MEM_LOCATION_TYPE_INVALID;\n+#endif  // CUDA_VERSION >= 12000\n+#if CUDA_VERSION >= 12000\n+#endif  // CUDA_VERSION >= 12000",
    "Label": "Buggy"
},
{
    "Id": 1357,
    "Library": "tensorflow",
    "Date": "2023/10/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e44f8a08051baa58bde9130a844a1b82a8179526",
    "Root Cause": "Type Checking",
    "Bug report": "check hasattr on the type, not the instance.\n\nhasattr on the instance triggers __getattr__ which carries very undesirable\neffects, such as running Ops on a donated buffer.\n\nLong term, we may want to audit all uses of hasattr on TensorFlow instances\nthat overrides __getattr__ in nontrival (e.g. running tf Ops) ways. They will\nalmost always cause trouble here and there because TensorFlow is quite far\nfrom being able guarantee if an Op returns or consumes is actually valid in all cases. Things will improve give it time, but if we can avoid such strong assumptions the system tend to get more robust.\n\nPiperOrigin-RevId: 578261984",
    "Number of deleted lines": 7,
    "Deleted lines": "-    # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n-    # object map. Also add TPUEmbedding to separate list for special handling\n-    # with values copy.\n-      if hasattr(t, _TPU_EMBEDDING_ATTR):\n-    if not hasattr(\n-        tpu_embedding, _TPU_EMBEDDING_ATTR\n-    ) or not callable(tpu_embedding._create_copy_for_async_checkpoint):  # pylint: disable=protected-access",
    "Added lines": "+      # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n+      # object map. Also add TPUEmbedding to separate list for special handling\n+      # with values copy.\n+      if hasattr(type(t), _TPU_EMBEDDING_ATTR):\n+    if not hasattr(type(tpu_embedding), _TPU_EMBEDDING_ATTR) or not callable(\n+        tpu_embedding._create_copy_for_async_checkpoint  # pylint: disable=protected-access\n+    ):",
    "Label": "Buggy"
},
{
    "Id": 1358,
    "Library": "tensorflow",
    "Date": "2021/12/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e5b0eec199c2d03de54fd6a7fd9275692218e2bc",
    "Root Cause": "Edge Cases",
    "Bug report": "[lite] Add validation check for dilation height/width to be positive integers.\n\nPiperOrigin-RevId: 416429178\nChange-Id: If7cdcddca54486434d9b2f06e7e2b401d7c3ee25",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TF_LITE_ENSURE(context, params->dilation_height_factor > 0);\n+  TF_LITE_ENSURE(context, params->dilation_width_factor > 0);",
    "Label": "Buggy"
},
{
    "Id": 1359,
    "Library": "tensorflow",
    "Date": "2022/01/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e6390bc13471f28f211cab874cc49a123505dc3e",
    "Root Cause": "Edge Cases",
    "Bug report": "Update histogram_ops.py\n\nAdded the condition to check the negative value of nbins input",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    ValueError: If the value of nbins is negative.\n+  if nbins < 0:\n+    raise ValueError(\"nbins should be a positive number.\")\n+    \n+    ValueError: If the value of nbins is negative.\n+  if nbins < 0:\n+    raise ValueError(\"nbins should be a positive number.\")\n+",
    "Label": "Buggy"
},
{
    "Id": 1360,
    "Library": "tensorflow",
    "Date": "2022/08/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e6df768b81e973f2123bc83a18a60773fc4da99e",
    "Root Cause": "Type Checking",
    "Bug report": "[TFG] Fix IsAdd string type check in tf_op_names\n\nPiperOrigin-RevId: 469316572",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (op_name == add_) return !op->getAttrOfType<StringAttr>(\"T\");",
    "Added lines": "+  if (op_name == add_)\n+    return !op->getAttrOfType<TypeAttr>(\"T\").getValue().isa<StringType>();",
    "Label": "Buggy"
},
{
    "Id": 1361,
    "Library": "tensorflow",
    "Date": "2023/02/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e99e31597c1b5cc9f0cbc8a3dea71674d81c20b1",
    "Root Cause": "Error Message",
    "Bug report": "Fix GRUCellBlockOp message for invalid rank of x\n\nThe validation checks that x is a matrix, so rank must be 2. ff45913\nfixed the crash in #58261 but left this typo in an exception message.\n\nFixes #58261\n\nSigned-off-by: Reid Wahl <nrwahl@protonmail.com>",
    "Number of deleted lines": 2,
    "Deleted lines": "-                errors::InvalidArgument(\"Rank of x must be 2\", x_tensor->dims(),\n-                                        \" vs. 2\"));",
    "Added lines": "+                errors::InvalidArgument(\"Rank of x must be 2, got \",\n+                                        x_tensor->dims()));",
    "Label": "Buggy"
},
{
    "Id": 1362,
    "Library": "tensorflow",
    "Date": "2022/07/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/eb2ddc0debb7e1b0c9ea68c817ca05fd59dc7914",
    "Root Cause": "Edge Cases",
    "Bug report": "In TF2XLA EnsureShape kernel, don't check whether the original tensor has dynamic shapes as it is much more expensive than just blindly clear out dynamic dimension.\n\nPiperOrigin-RevId: 464003792",
    "Number of deleted lines": 6,
    "Deleted lines": "-    // remove the dynamic dimensions in XLA dynamic padder.\n-    std::vector<bool> dynamic_dims;\n-    OP_REQUIRES_OK(ctx,\n-                   ctx->ResolveInputDynamismIntoPredVector(0, &dynamic_dims));\n-      if (expected_shape_.dim_size(i) > 0 && dynamic_dims[i]) {\n-        VLOG(1) << \"RemoveDynamicDimension: \" << i;",
    "Added lines": "+    // remove the dynamic dimensions in XLA dynamic padder. Here we don't check\n+    // whether the original input has dynamic shapes, because\n+    // `ctx->ResolveInputDynamismIntoPredVector` runs a DFS underneath which is\n+    // more expensive.\n+      if (expected_shape_.dim_size(i) > 0) {\n+        VLOG(1) << \"RemoveDynamicDimension: \" << i << \" of shape \"\n+                << shape.DebugString();",
    "Label": "Buggy"
},
{
    "Id": 1363,
    "Library": "tensorflow",
    "Date": "2023/04/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/edd9fb416e04b8ca4398c4eea65f14dc6704a44a",
    "Root Cause": "Edge Cases",
    "Bug report": "TfLiteTensorCopy returns an error status when src and dest bytes are not equal. So we don't need to check them specifically if we ensure the status of the call to copy (which we should do anyways).\n\nPiperOrigin-RevId: 522647125",
    "Number of deleted lines": 2,
    "Deleted lines": "-    TF_LITE_ENSURE_EQ(context, src_tensor->bytes, dst_tensor->bytes);\n-    TfLiteTensorCopy(src_tensor, dst_tensor);",
    "Added lines": "+    TF_LITE_ENSURE_OK(context, TfLiteTensorCopy(src_tensor, dst_tensor));",
    "Label": "Buggy"
},
{
    "Id": 1364,
    "Library": "tensorflow",
    "Date": "2023/04/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ef049bdfc4f307c8b3a9dc480a90a5ff287f3d55",
    "Root Cause": "Edge Cases",
    "Bug report": "Add check for `ResizeOutput` return value in `range.cc`\n\nPiperOrigin-RevId: 524984384",
    "Number of deleted lines": 1,
    "Deleted lines": "-    ResizeOutput(context, start, limit, delta, output);",
    "Added lines": "+    TF_LITE_ENSURE_OK(context,\n+                      ResizeOutput(context, start, limit, delta, output));",
    "Label": "Buggy"
},
{
    "Id": 1365,
    "Library": "tensorflow",
    "Date": "2022/06/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f22ca1dc88c70a0dc5696c37e6a2de6bcf8d60c7",
    "Root Cause": "Edge Cases",
    "Bug report": "Avoid segfault when init_value is not on default_mesh.\n\nTo actually fix the segfault in lower level (e.g. directly users of VarHandleOp),\nI tried to add a validation in SPMD of AssignValueOp, but turns out it only\nknows the resource_layout is an 'empty' layout without any mesh information.\n\nSo there is not enough information to compare if the mesh is correct. We\nshall start tracking mesh of empty layout -- but changing the data model at\nthis point is not very easy to do or to justify.\n\nPiperOrigin-RevId: 453453817",
    "Number of deleted lines": 2,
    "Deleted lines": "-      super(DVariable, self).__init__(\n-          initial_value, *args, dtype=dtype, **kwargs)",
    "Added lines": "+import contextlib\n+      mesh = self.layout.mesh if self.layout else None\n+      with api.run_on(mesh) if mesh else contextlib.nullcontext():\n+        super(DVariable, self).__init__(\n+            initial_value, *args, dtype=dtype, **kwargs)",
    "Label": "Buggy"
},
{
    "Id": 1366,
    "Library": "tensorflow",
    "Date": "2023/08/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f636be3bb1f556c15dba3028e61a8969d90dadd9",
    "Root Cause": "Error Message",
    "Bug report": "Return error on invalid input in `tfl.sign_custom`\n\nPiperOrigin-RevId: 557545596",
    "Number of deleted lines": 5,
    "Deleted lines": "-    default:\n-      TF_LITE_KERNEL_LOG(\n-          context,\n-          \"Unsupported datatype for atan2 output: %s\",\n-          TfLiteTypeGetName(output->type));",
    "Added lines": "+    default: {\n+      TF_LITE_KERNEL_LOG(context, \"Unsupported datatype for sign output: %s\",\n+                         TfLiteTypeGetName(output->type));\n+      return TfLiteStatus::kTfLiteError;\n+    }",
    "Label": "Buggy"
}]
