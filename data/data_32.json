[{
    "Id": 1,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0a7eef9bcf6d1f8b5531102342ffc21f24beb58d",
    "Violation": "unnecessary",
    "Bug report": "[BE] Remove stale CUDA version check from cpp_extension.py. As at least CUDA-11.x is needed to build PyTorch on latest trunk. But still skip `--generate-dependencies-with-compile` if running on ROCm",
    "Number of deleted lines": 5,
    "Deleted lines": "                nvcc = _join_rocm_home('bin', 'hipcc')\n            else:\n                nvcc = _join_cuda_home('bin', 'nvcc')\n        config.append(f'nvcc = {nvcc}')\n\n    if IS_HIP_EXTENSION:\n        post_cflags = COMMON_HIP_FLAGS + post_cflags\n    flags = [f'cflags = {\" \".join(cflags)}']\n    flags.append(f'post_cflags = {\" \".join(post_cflags)}')\n    if with_cuda:\n        flags.append(f'cuda_cflags = {\" \".join(cuda_cflags)}')\n        flags.append(f'cuda_post_cflags = {\" \".join(cuda_post_cflags)}')\n    flags.append(f'cuda_dlink_post_cflags = {\" \".join(cuda_dlink_post_cflags)}')\n    flags.append(f'ldflags = {\" \".join(ldflags)}')\n\n    # Turn into absolute paths so we can emit them into the ninja build\n    # file wherever it is.\n    sources = [os.path.abspath(file) for file in sources]\n\n    # See https://ninja-build.org/build.ninja.html for reference.\n    compile_rule = ['rule compile']\n    if IS_WINDOWS:\n        compile_rule.append(\n            '  command = cl /showIncludes $cflags -c $in /Fo$out $post_cflags')\n        compile_rule.append('  deps = msvc')\n    else:\n        compile_rule.append(\n            '  command = $cxx -MMD -MF $out.d $cflags -c $in -o $out $post_cflags')\n        compile_rule.append('  depfile = $out.d')\n        compile_rule.append('  deps = gcc')\n\n    if with_cuda:\n        cuda_compile_rule = ['rule cuda_compile']\n        nvcc_gendeps = ''\n        # --generate-dependencies-with-compile was added in CUDA 10.2.\n        # Compilation will work on earlier CUDA versions but header file\n        # dependencies are not correctly computed.\n        required_cuda_version = '11.0'\n        if torch.version.cuda is not None and TorchVersion(torch.version.cuda) >= required_cuda_version:\n            cuda_compile_rule.append('  depfile = $out.d')\n            cuda_compile_rule.append('  deps = gcc')\n            # Note: non-system deps with nvcc are only supported\n            # on Linux so use --generate-dependencies-with-compile\n            # to make this work on Windows too.\n            nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'\n        cuda_compile_rule.append(\n            f'  command = $nvcc {nvcc_gendeps} $cuda_cflags -c $in -o $out $cuda_post_cflags')\n\n    # Emit one build rule per source to enable incremental build.\n    build = []\n    for source_file, object_file in zip(sources, objects):\n        is_cuda_source = _is_cuda_file(source_file) and with_cuda\n        rule = 'cuda_compile' if is_cuda_source else 'compile'\n        if IS_WINDOWS:\n            source_file = source_file.replace(':', '$:')\n            object_file = object_file.replace(':', '$:')\n        source_file = source_file.replace(\" \", \"$ \")\n        object_file = object_file.replace(\" \", \"$ \")\n        build.append(f'build {object_file}: {rule} {source_file}')\n\n    if cuda_dlink_post_cflags:\n        devlink_out = os.path.join(os.path.dirname(objects[0]), 'dlink.o')\n        devlink_rule = ['rule cuda_devlink']\n        devlink_rule.append('  command = $nvcc $in -o $out $cuda_dlink_post_cflags')\n        devlink = [f'build {devlink_out}: cuda_devlink {\" \".join(objects)}']\n        objects += [devlink_out]\n    else:\n        devlink_rule, devlink = [], []\n\n    if library_target is not None:\n        link_rule = ['rule link']\n        if IS_WINDOWS:\n            cl_paths = subprocess.check_output(['where',\n                                                'cl']).decode(*SUBPROCESS_DECODE_ARGS).split('\\r\\n')\n            if len(cl_paths) >= 1:"
},
{
    "Id": 2,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/93cea394dee1315c7a85ead7bb7af21363157c4f",
    "Violation": "improper",
    "Bug report": "Currently we compare `CUDA_INCLUDE_DIRS` and expect exact equality with `CUDAToolkit_INCLUDE_DIR` however this fails in the presense of symbolic links or for split installs where there are multiple include paths.",
    "Number of deleted lines": 4,
    "Deleted lines": "find_package(CUDA)\nif(NOT CUDA_FOUND)\n  message(WARNING\n    \"Caffe2: CUDA cannot be found. Depending on whether you are building \"\n    \"Caffe2 or a Caffe2 dependent library, the next warning / error will \"\n    \"give you more info.\")\n  set(CAFFE2_USE_CUDA OFF)\n  return()\nendif()\n\n# Enable CUDA language support\nset(CUDAToolkit_ROOT \"${CUDA_TOOLKIT_ROOT_DIR}\")\n# Pass clang as host compiler, which according to the docs\n# Must be done before CUDA language is enabled, see\n# https://cmake.org/cmake/help/v3.15/variable/CMAKE_CUDA_HOST_COMPILER.html\nif(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n  set(CMAKE_CUDA_HOST_COMPILER \"${CMAKE_C_COMPILER}\")\nendif()\nenable_language(CUDA)\nif(\"X${CMAKE_CUDA_STANDARD}\" STREQUAL \"X\" )\n  set(CMAKE_CUDA_STANDARD ${CMAKE_CXX_STANDARD})\nendif()\nset(CMAKE_CUDA_STANDARD_REQUIRED ON)\n\n# CMP0074 - find_package will respect <PackageName>_ROOT variables\ncmake_policy(PUSH)\nif(CMAKE_VERSION VERSION_GREATER_EQUAL 3.12.0)\n  cmake_policy(SET CMP0074 NEW)\nendif()\n\nfind_package(CUDAToolkit REQUIRED)\n\ncmake_policy(POP)\n\nif(NOT CMAKE_CUDA_COMPILER_VERSION STREQUAL CUDAToolkit_VERSION OR\n    NOT CUDA_INCLUDE_DIRS STREQUAL CUDAToolkit_INCLUDE_DIR)\n  message(FATAL_ERROR \"Found two conflicting CUDA installs:\\n\"\n                      \"V${CMAKE_CUDA_COMPILER_VERSION} in '${CUDA_INCLUDE_DIRS}' and\\n\"\n                      \"V${CUDAToolkit_VERSION} in '${CUDAToolkit_INCLUDE_DIR}'\")\nendif()\n\nif(NOT TARGET CUDA::nvToolsExt)\n  message(FATAL_ERROR \"Failed to find nvToolsExt\")\nendif()\n\nmessage(STATUS \"Caffe2: CUDA detected: \" ${CUDA_VERSION})\nmessage(STATUS \"Caffe2: CUDA nvcc is: \" ${CUDA_NVCC_EXECUTABLE})\nmessage(STATUS \"Caffe2: CUDA toolkit directory: \" ${CUDA_TOOLKIT_ROOT_DIR})\nif(CUDA_VERSION VERSION_LESS 11.0)\n  message(FATAL_ERROR \"PyTorch requires CUDA 11.0 or above.\")\nendif()\n\nif(CUDA_FOUND)\n  # Sometimes, we may mismatch nvcc with the CUDA headers we are\n  # compiling with, e.g., if a ccache nvcc is fed to us by CUDA_NVCC_EXECUTABLE\n  # but the PATH is not consistent with CUDA_HOME.  It's better safe\n  # than sorry: make sure everything is consistent.\n  if(MSVC AND CMAKE_GENERATOR MATCHES \"Visual Studio\")\n    # When using Visual Studio, it attempts to lock the whole binary dir when\n    # `try_run` is called, which will cause the build to fail.\n    string(RANDOM BUILD_SUFFIX)\n    set(PROJECT_RANDOM_BINARY_DIR \"${PROJECT_BINARY_DIR}/${BUILD_SUFFIX}\")\n  else()\n    set(PROJECT_RANDOM_BINARY_DIR \"${PROJECT_BINARY_DIR}\")\n  endif()\n  set(file \"${PROJECT_BINARY_DIR}/detect_cuda_version.cc\")\n  file(WRITE ${file} \"\"\n    \"#include <cuda.h>\\n\"\n    \"#include <cstdio>\\n\"\n    \"int main() {\\n\"\n    \"  printf(\\\"%d.%d\\\", CUDA_VERSION / 1000, (CUDA_VERSION / 10) % 100);\\n\"\n    \"  return 0;\\n\"\n    \"}\\n\"\n    )\n  if(NOT CMAKE_CROSSCOMPILING)"
},
{
    "Id": 3,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6b4c686b9a33a1503a4a4133f9067dd31e0822f7",
    "Violation": "improper",
    "Bug report": "Forward fix a performance regression caused by #110510. When a model is run once, all those kernel pointers are initialized and removing the if-nullptr check will cause those loadKernel be unnecessarily executed again when we rerun the foward function.",
    "Number of deleted lines": 2,
    "Deleted lines": "                    cudaStream_t stream) {\n                CUDA_DRIVER_CHECK(cuLaunchKernel(\n                    func, gridX, gridY, gridZ, 32*numWarps, 1, 1, sharedMemBytes, stream, args, nullptr\n                ));\n            }\n            \"\"\"\n        )\n\n    def write_get_raw_stream(self, index):\n        name = f\"stream{index}\"\n        self.writeline(\n            f\"cudaStream_t {name} = at::cuda::getCurrentCUDAStream({index});\"\n        )\n        return name\n\n    def define_kernel(\n        self, name: str, kernel: str, metadata: Optional[str] = None, cuda=True\n    ):\n        if not cuda:\n            return super().define_kernel(name, kernel, metadata, cuda)\n\n    def generate(self):\n        self.prefix.writeline(\"\\n\")\n        if not V.graph.aot_mode:\n            for kernel in self.src_to_kernel.values():\n                self.prefix.writeline(f\"static CUfunction {kernel} = nullptr;\")\n            self.prefix.writeline(\"\\n\")\n        return super().generate()\n\n    @functools.lru_cache(None)\n    def generate_load_kernel_once(\n        self, name: str, mangled_name: str, cubin_path: str, shared_mem: int\n    ):\n        if V.graph.aot_mode:\n            self.writeline(\n                f\"\"\"kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n            )\n        else:\n            self.writeline(\n                f\"\"\"{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n            )\n\n    def generate_args_decl(self, call_args):\n        dynamic_symbols = V.graph.sizevars.free_symbols()\n        # TODO: only works for constant now, need type info\n        new_args = []\n        for arg in call_args:\n            var_name = f\"var_{next(self.arg_var_id)}\"\n            if isinstance(\n                arg,\n                (\n                    sympy.Integer,\n                    sympy.Symbol,\n                    SymbolicCallArg,\n                ),\n            ):\n                self.writeline(f\"auto {var_name} = {arg};\")\n            elif is_int(arg):\n                self.writeline(f\"int {var_name} = {arg};\")\n            elif is_float(arg):\n                self.writeline(f\"float {var_name} = {arg};\")\n            elif any(str(arg) == s.name for s in dynamic_symbols):\n                self.writeline(f\"auto {var_name} = {arg};\")\n            else:\n                if config.aot_inductor.abi_compatible:\n                    self.writeline(f\"CUdeviceptr {var_name};\")\n                    self.writeline(\n                        f\"AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_get_data_ptr({arg}, reinterpret_cast<void**>(&{var_name})));\"\n                    )\n                else:\n                    self.writeline(\n                        f\"CUdeviceptr {var_name} = reinterpret_cast<CUdeviceptr>({arg}.data_ptr());\"\n                    )\n            new_args.append(f\"&{var_name}\")\n\n        return \", \".join(new_args)\n"
},
{
    "Id": 4,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/45296f87ec865a7a500a6fd98353035c040d0cb7",
    "Violation": "insufficient",
    "Bug report": "Prior to this change ROCm was not exiting check_cuda, causing an exception at packaging.version.parse(torch.version.cuda),",
    "Number of deleted lines": 1,
    "Deleted lines": "        raise VerifyDynamoError(\"CUDA version not found in `nvcc --version` output\")\n\n    cuda_str_version = cuda_version.group(1)\n    return packaging.version.parse(cuda_str_version)\n\n\ndef get_rocm_version():\n    from torch.utils import cpp_extension\n\n    ROCM_HOME = cpp_extension._find_rocm_home()\n    if not ROCM_HOME:\n        raise VerifyDynamoError(\n            \"ROCM was not found on the system, please set ROCM_HOME environment variable\"\n        )\n\n    hipcc = os.path.join(ROCM_HOME, \"bin\", \"hipcc\")\n    hip_version_str = (\n        subprocess.check_output([hipcc, \"--version\"])\n        .strip()\n        .decode(*cpp_extension.SUBPROCESS_DECODE_ARGS)\n    )\n    hip_version = re.search(r\"HIP version: (\\d+[.]\\d+)\", hip_version_str)\n\n    if hip_version is None:\n        raise VerifyDynamoError(\"HIP version not found in `hipcc --version` output\")\n\n    hip_str_version = hip_version.group(1)\n\n    return packaging.version.parse(hip_str_version)\n\n\ndef check_cuda():\n    import torch\n\n    if not torch.cuda.is_available():\n        return None\n\n    torch_cuda_ver = packaging.version.parse(torch.version.cuda)\n\n    # check if torch cuda version matches system cuda version\n    cuda_ver = get_cuda_version()\n    if cuda_ver != torch_cuda_ver:\n        # raise VerifyDynamoError(\n        warnings.warn(\n            f\"CUDA version mismatch, `torch` version: {torch_cuda_ver}, env version: {cuda_ver}\"\n        )\n\n    if torch_cuda_ver < MIN_CUDA_VERSION:\n        # raise VerifyDynamoError(\n        warnings.warn(\n            f\"(`torch`) CUDA version not supported: {torch_cuda_ver} \"\n            f\"- minimum requirement: {MIN_CUDA_VERSION}\"\n        )\n    if cuda_ver < MIN_CUDA_VERSION:\n        # raise VerifyDynamoError(\n        warnings.warn(\n            f\"(env) CUDA version not supported: {cuda_ver} \"\n            f\"- minimum requirement: {MIN_CUDA_VERSION}\"\n        )\n\n    return cuda_ver if torch.version.hip is None else \"None\"\n\n\ndef check_rocm():\n    import torch\n\n    if not torch.cuda.is_available() or torch.version.hip is None:\n        return None\n\n    # Extracts main ROCm version from full string\n    torch_rocm_ver = packaging.version.parse("
},
{
    "Id": 5,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bede7d999523d02e636a8981c0dff233b67f1a62",
    "Violation": "improper",
    "Bug report": "It does not check if `kind` variable fits in array of pointer called `names`",
    "Number of deleted lines": 1,
    "Deleted lines": "#include <string>\n#include <vector>\n\n#include <ATen/core/interned_strings.h>\n\n#include <torch/csrc/WindowsTorchApiMacro.h>\n\nnamespace c10 {\nstruct Type;\nusing TypePtr = std::shared_ptr<Type>;\n} // namespace c10\n\nnamespace torch {\nnamespace jit {\n\nusing ::c10::Symbol;\n\nconstexpr int max_tensor_display_size = 10;\n\nenum class AttributeKind { f, fs, i, is, s, ss, t, ts, g, gs, ty, tys, ival };\nstatic inline const char* toString(AttributeKind kind) {\n  static const char* names[] = {\"f\",\n                                \"fs\",\n                                \"i\",\n                                \"is\",\n                                \"s\",\n                                \"ss\",\n                                \"t\",\n                                \"ts\",\n                                \"g\",\n                                \"gs\",\n                                \"ty\",\n                                \"tys\",\n                                \"ival\"};\n  AT_ASSERT(size_t(kind) < sizeof(names) / sizeof(AttributeKind));\n  return names[int(kind)];\n}\n\nstruct AttributeValue {\n  AttributeValue(Symbol name) : name(name) {}\n  using Ptr = std::unique_ptr<AttributeValue>;\n  Symbol name;\n  virtual AttributeKind kind() const = 0;\n  virtual Ptr clone() const = 0;\n  virtual ~AttributeValue() = default;\n};\n\ntemplate <typename T, AttributeKind Kind>\nstruct ScalarAttributeValue : public AttributeValue {\n  using ConstructorType = T;\n  using ValueType = T;\n  ScalarAttributeValue(Symbol name, ConstructorType value_)\n      : AttributeValue(name), value_(std::move(value_)) {}\n  ValueType& value() {\n    return value_;\n  }\n  Ptr clone() const override {\n    return Ptr(new ScalarAttributeValue(name, value_));\n  }\n  AttributeKind kind() const override {\n    return Kind;\n  }\n\n private:\n  ValueType value_;\n};\n\ntemplate <typename T, AttributeKind Kind>\nstruct VectorAttributeValue : public AttributeValue {\n  using ConstructorType = std::vector<T>;\n  using ValueType = std::vector<T>;"
},
{
    "Id": 6,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f6639359357452de8bfc691430396ded98ea399c",
    "Violation": "improper",
    "Bug report": "`TORCH_CHECK(i<UINT32_MAX)` is always false, it should be `TORCH_CHECK(iterShape[i] < UINT32_MAX)`",
    "Number of deleted lines": 3,
    "Deleted lines": "\nMPSGraphCache* MPSGraphCache::_instance_cache = nullptr;\n\nvoid MPSGraphCache::profileCachedGraph(const CacheEntry& cacheEntry) const {\n  auto& profiler = getMPSProfiler();\n  if (profiler.isOperationProfilingEnabled()) {\n    std::string graphKey = cacheEntry.key_;\n    // for interval-based signpost tracing, we begin the interval here to be able\n    // to measure the time it takes to compile the graphs (if graph newly created),\n    // and also the time potentially spent on gather/scatter of graph's input tensors\n    profiler.beginProfileKernel(cacheEntry.cachedGraph_->graph(), graphKey, true);\n  }\n}\n\nclass MPSGraphCacheCallback : public IMpsAllocatorCallback {\n public:\n  MPSGraphCacheCallback() : graph_cache(MPSGraphCache::getInstance()) {}\n\n  void executeMPSAllocatorCallback(void* ptr, EventType event) override {}\n\n private:\n  MPSGraphCache* graph_cache;\n};\n\nREGISTER_MPS_ALLOCATOR_CALLBACK(\"mps_graph_cache_callback\", MPSGraphCacheCallback);\n\nid<MTLBuffer> generateKernelDataOffsets(id<MTLComputeCommandEncoder> commandEncoder, const TensorIteratorBase& iter) {\n  constexpr uint32_t nOffsets = 3;\n  uint32_t numThreads = iter.numel();\n  const uint32_t nDim = iter.ndim();\n  const IntArrayRef& iterShape = iter.shape();\n  std::vector<uint32_t> iterShapeData(iterShape.size());\n  std::vector<std::array<uint32_t, nOffsets>> strides(nDim);\n  TORCH_INTERNAL_ASSERT(iter.ntensors() >= nOffsets);\n\n  for (const auto i : c10::irange(iterShape.size())) {\n    TORCH_CHECK(i <= UINT32_MAX);\n    iterShapeData[i] = (uint32_t)(iterShape[i]);\n  }\n\n  for (const auto i : c10::irange(nDim)) {\n    for (const auto offset : c10::irange(nOffsets)) {\n      strides[i][offset] = iter.strides(offset)[i];\n    }\n  }\n\n  id<MTLComputePipelineState> kernelDataOffsetsPSO = MPSDevice::getInstance()->metalIndexingPSO(\"kernel_index_offsets\");\n  id<MTLBuffer> kernelDataOffsets = (id<MTLBuffer>)getIMPSAllocator()->allocate(numThreads * sizeof(simd_uint3)).get();\n\n  [commandEncoder setComputePipelineState:kernelDataOffsetsPSO];\n  [commandEncoder setBytes:strides.data() length:sizeof(uint32_t) * nDim * nOffsets atIndex:0];\n  [commandEncoder setBuffer:kernelDataOffsets offset:0 atIndex:1];\n  [commandEncoder setBytes:iterShapeData.data() length:sizeof(uint32_t) * iterShape.size() atIndex:2];\n  [commandEncoder setBytes:&nDim length:sizeof(uint32_t) atIndex:3];\n  [commandEncoder setBytes:&nOffsets length:sizeof(uint32_t) atIndex:4];\n\n  mtl_dispatch1DJob(commandEncoder, kernelDataOffsetsPSO, numThreads);\n\n  return kernelDataOffsets;\n}\n\n} // namespace at::native::mps\n"
},
{
    "Id": 7,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/175ccfc4c8443bcc65c87d9c942272d3ebf16b0b",
    "Violation": "missing",
    "Bug report": "flatbuffer module fields are not initialized",
    "Number of deleted lines": 2,
    "Deleted lines": "    (*extra_files)[extra_file->name()->str()] = extra_file->content()->str();\n  }\n}\n\nvoid parseExtraFiles(\n    mobile::serialization::Module* module,\n    ExtraFilesMap& extra_files) {\n  auto extra_files_offsets = module->extra_files();\n  parseExtraFilesFromVector(extra_files_offsets, &extra_files);\n}\n\nvoid FlatbufferLoader::parseAndPopulate(\n    uint32_t i,\n    const mobile::serialization::IValue* ivalue) {\n  if (const auto* func = ivalue->val_as_Function()) {\n    auto func_ptr = parseFunction(func);\n    all_functions_[i] = func_ptr.get();\n    mcu_->register_function(std::move(func_ptr));\n  } else {\n    all_ivalues_[i] = parseIValue(ivalue);\n  }\n}\n\nmobile::Module FlatbufferLoader::parseModule(\n    mobile::serialization::Module* module,\n    char* end) {\n  module_ = module;\n  all_ivalues_.clear();\n  all_types_.clear();\n  storages_.clear();\n  storage_loaded_.clear();\n  module_parsed_ = false;\n\n  const auto* ivalues = module->ivalues();\n  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")\n  TORCH_CHECK(\n      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\")\n  all_ivalues_.resize(ivalues->size());\n  all_types_.resize(module->object_types()->size());\n  storages_.resize(module->storage_data_size());\n  storage_loaded_.resize(module->storage_data_size(), false);\n\n  mobile_ivalue_size_ = module_->mobile_ivalue_size();\n  if (mobile_ivalue_size_ == 0) {\n    mobile_ivalue_size_ = ivalues->size();\n  }\n\n  for (uint32_t i = 0; i < mobile_ivalue_size_; i++) {\n    const auto* ival = ivalues->Get(i);\n    TORCH_CHECK(\n        reinterpret_cast<const char*>(ival) < end, \"Corrupted ivalue item\")\n    parseAndPopulate(i, ival);\n  }\n  IValue& module_ivalue = getIValue(module->state_obj());\n\n  // register functions\n  for (const auto& f : all_functions_) {\n    uint32_t class_index =\n        ivalues->Get(f.first)->val_as_Function()->class_type();\n    ClassTypePtr class_type = all_types_[class_index];\n    class_type->addMethod(f.second);\n  }\n\n  module_parsed_ = true;\n  auto m = mobile::Module(module_ivalue.toObject(), mcu_);\n  m.set_min_operator_version(module->operator_version());\n  m.set_bytecode_version(module->bytecode_version());\n  return m;\n}\n\nvoid appendUpgraderFunctions(mobile::Function* function) {\n#ifndef DISABLE_UPGRADER\n  for (auto& byteCodeFunctionWithOperator : getUpgraderBytecodeList()) {"
},
{
    "Id": 8,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bde7b81f34925491fbcbb9e355697eb594e36923",
    "Violation": "improper",
    "Bug report": "Back out \"[PyTorch] Don't do extra numel() check in TensorImpl::data()",
    "Number of deleted lines": 1,
    "Deleted lines": "   * Return a void* data pointer to the actual data which this tensor refers to.\n   *\n   * It is invalid to call mutable_data() on a dtype-uninitialized\n   * tensor, even if the size is 0.\n   *\n   * WARNING: The data pointed to by this tensor may not contiguous; do NOT\n   * assume that itemsize() * numel() is sufficient to compute the bytes that\n   * can be validly read from this tensor.\n   */\n  inline void* mutable_data() {\n    return data_impl<void>(\n        [this] { return static_cast<char*>(storage_.mutable_data()); });\n  }\n\n private:\n  /// Shared implementation of data() and mutable_data().\n  ///\n  /// get_data must return a byte-addressed pointer, e.g. char*,\n  /// std::byte const*, etc.\n  template <typename Void, typename Func>\n  Void* data_impl(const Func& get_data) const {\n    TORCH_CHECK(\n        has_storage(),\n        \"Cannot access data pointer of Tensor that doesn't have storage\");\n    TORCH_CHECK(\n        dtype_initialized(),\n        \"Cannot access data pointer of Tensor that doesn't have initialized dtype \"\n        \"(e.g., caffe2::Tensor x(CPU), prior to calling mutable_data<T>() on x)\");\n    auto* data = get_data();\n    static_assert(\n        sizeof(*data) == 1, \"get_data must return a byte-addressed pointer.\");\n    // Computing an offset into an empty tensor would be UB, since an empty\n    // tensor's storage will be nullptr, and adding a nonzero offset to nullptr\n    // is UB.  So we skip the offset computation in this case.\n    if (data == nullptr) {\n      return nullptr;\n    }\n    return data + data_type_.itemsize() * storage_offset_;\n  }\n\n public:\n  /**\n   * Returns the TypeMeta of a tensor, which describes what data type\n   * it is (e.g., int, float, ...)\n   */\n  const caffe2::TypeMeta dtype() const {\n    return data_type_;\n  }\n\n  /**\n   * Return the size of a single element of this tensor in bytes.\n   */\n  size_t itemsize() const {\n    TORCH_CHECK(\n        dtype_initialized(),\n        \"Cannot report itemsize of Tensor that doesn't have initialized dtype \"\n        \"(e.g., caffe2::Tensor x(CPU), prior to calling mutable_data<T>() on x)\");\n    return data_type_.itemsize();\n  }\n\n  void set_backend_meta(intrusive_ptr<c10::BackendMeta> backend_meta) {\n    get_extra_meta().backend_meta_ = std::move(backend_meta);\n  }\n\n  c10::BackendMeta* get_backend_meta() {\n    if (!extra_meta_) {\n      return nullptr;\n    }\n    return extra_meta_->backend_meta_.get();\n  }\n"
},
{
    "Id": 9,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2e224d62b6afecc78d885d0a4e160354950f6424",
    "Violation": "missing",
    "Bug report": "We have environment variable USE_CUDNN with self-explanatory name. However cpp code is compiled based on cpp macro definition AT_CUDNN_ENABLED, even if USE_CUDNN is set to 0, cpp is compiled with cuDNN if cmake finds cuDNN in the system.",
    "Number of deleted lines": 2,
    "Deleted lines": "    add_compile_options(-DUSE_GCC_GET_CPUID)\n  ENDIF()\n\n  FIND_PACKAGE(AVX) # checks AVX and AVX2\n\n  # we don't set -mavx and -mavx2 flags globally, but only for specific files\n  # however, we want to enable the AVX codepaths, so we still need to\n  # add USE_AVX and USE_AVX2 macro defines\n  IF (C_AVX_FOUND)\n    MESSAGE(STATUS \"AVX compiler support found\")\n    add_compile_options(-DUSE_AVX)\n  ENDIF()\n  IF (C_AVX2_FOUND)\n    MESSAGE(STATUS \"AVX2 compiler support found\")\n    add_compile_options(-DUSE_AVX2)\n  ENDIF()\n\n  IF (WIN32 AND NOT CYGWIN)\n    SET(BLAS_INSTALL_LIBRARIES \"OFF\"\n      CACHE BOOL \"Copy the required BLAS DLLs into the TH install dirs\")\n  ENDIF()\n\n  FIND_PACKAGE(LAPACK)\n  IF (LAPACK_FOUND)\n    SET(USE_LAPACK 1)\n  ENDIF()\n\n  if (NOT USE_CUDA)\n    message(\"disabling CUDA because NOT USE_CUDA is set\")\n    SET(AT_CUDA_ENABLED 0)\n  else()\n    SET(AT_CUDA_ENABLED 1)\n  endif()\n\n  IF (NOT AT_CUDA_ENABLED OR NOT CUDNN_FOUND)\n    MESSAGE(STATUS \"CuDNN not found. Compiling without CuDNN support\")\n    set(AT_CUDNN_ENABLED 0)\n  ELSE()\n    include_directories(SYSTEM ${CUDNN_INCLUDE_PATH})\n    set(AT_CUDNN_ENABLED 1)\n  ENDIF()\n\n  IF (NOT USE_ROCM)\n    message(\"disabling ROCM because NOT USE_ROCM is set\")\n    MESSAGE(STATUS \"MIOpen not found. Compiling without MIOpen support\")\n    set(AT_ROCM_ENABLED 0)\n  ELSE()\n    INCLUDE_DIRECTORIES(BEFORE ${MIOPEN_INCLUDE_DIRS})\n    set(AT_ROCM_ENABLED 1)\n  ENDIF()\n\n  SET(AT_MKLDNN_ENABLED 0)\n  SET(CAFFE2_USE_MKLDNN OFF)\n  IF (USE_MKLDNN)\n    INCLUDE(${CMAKE_CURRENT_LIST_DIR}/public/mkldnn.cmake)\n    IF(MKLDNN_FOUND)\n      SET(AT_MKLDNN_ENABLED 1)\n      INCLUDE_DIRECTORIES(AFTER SYSTEM ${MKLDNN_INCLUDE_DIR})\n      IF(BUILD_CAFFE2_OPS)\n        SET(CAFFE2_USE_MKLDNN ON)\n        LIST(APPEND Caffe2_PUBLIC_DEPENDENCY_LIBS caffe2::mkldnn)\n      ENDIF(BUILD_CAFFE2_OPS)\n    ELSE()\n      MESSAGE(WARNING \"MKLDNN could not be found.\")\n    ENDIF()\n  ELSE()\n    MESSAGE(\"disabling MKLDNN because USE_MKLDNN is not set\")\n  ENDIF()\n\n  IF(UNIX AND NOT APPLE)\n     INCLUDE(CheckLibraryExists)\n     # https://github.com/libgit2/libgit2/issues/2128#issuecomment-35649830"
},
{
    "Id": 10,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/30e1c74dc19ae2b622b46ebcdb7972c42775ac80",
    "Violation": "improper",
    "Bug report": "Update cuda amp to also check xla device ",
    "Number of deleted lines": 1,
    "Deleted lines": "        a_float32 = torch.rand((8, 8), device=\"cuda\")\n        b_float32 = torch.rand((8, 8), device=\"cuda\")\n        c_float32 = torch.rand((8, 8), device=\"cuda\")\n        d_float32 = torch.rand((8, 8), device=\"cuda\")\n\n        with autocast():\n            e_float16 = torch.mm(a_float32, b_float32)\n            with autocast(enabled=False):\n                # Calls e_float16.float() to ensure float32 execution\n                # (necessary because e_float16 was created in an autocasted region)\n                f_float32 = torch.mm(c_float32, e_float16.float())\n\n            # No manual casts are required when re-entering the autocast-enabled region.\n            # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n            g_float16 = torch.mm(d_float32, f_float32)\n\n    The autocast state is thread-local.  If you want it enabled in a new thread, the context manager or decorator\n    must be invoked in that thread.  This affects :class:`torch.nn.DataParallel` and\n    :class:`torch.nn.parallel.DistributedDataParallel` when used with more than one GPU per process\n    (see :ref:`Working with Multiple GPUs<amp-multigpu>`).\n\n    Args:\n        device_type(string, required):  Whether to use 'cuda' or 'cpu' device\n        enabled(bool, optional, default=True)\":  Whether autocasting should be enabled in the region.\n        fast_dtype(torch_dtype, optional):  Whether to use torch.float16 or torch.bfloat16\n    \"\"\"\n    def __init__(self, device_type, enabled=True, **kwargs):\n        self.device = device_type\n        if self.device == 'cuda':\n            self.fast_dtype = torch.get_autocast_gpu_dtype()\n        elif self.device == 'cpu':\n            self.fast_dtype = torch.get_autocast_cpu_dtype()\n        else:\n            raise RuntimeError('User specified autocast device_type must be \\'cuda\\' or \\'cpu\\'')\n        if not torch.cuda.is_available() and self.device == 'cuda':\n            warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n            enabled = False\n        for key, value in kwargs.items():\n            if key == 'fast_dtype':\n                self.fast_dtype = value\n            if not (key == 'fast_dtype'):\n                raise RuntimeError('Unrecognized optional argument supplied to autocast context manager: ' + str(key))\n\n        if self.device == 'cpu':\n            supported_dtype = [torch.bfloat16]\n            if self.fast_dtype not in supported_dtype:\n                error_message = 'In CPU autocast, but the target dtype is not supported. Disabling autocast.\\n'\n                error_message += 'CPU Autocast only supports dtype of torch.bfloat16 currently.'\n                warnings.warn(error_message)\n                enabled = False\n        if self.device == 'cuda':\n            if self.fast_dtype == torch.bfloat16 and torch.cuda.get_device_properties(torch.cuda.current_device()).major < 8:\n                raise RuntimeError('Current CUDA Device does not support bfloat16. Switching fast_dtype to float16.')\n        self._enabled = enabled\n\n    def __enter__(self):\n        if self.device == 'cpu':\n            self.prev = torch.is_autocast_cpu_enabled()\n            self.prev_fastdtype = torch.get_autocast_cpu_dtype()\n            torch.set_autocast_cpu_enabled(self._enabled)\n            torch.set_autocast_cpu_dtype(self.fast_dtype)\n            torch.autocast_increment_nesting()\n        else:\n            self.prev = torch.is_autocast_enabled()\n            self.prev_fastdtype = torch.get_autocast_gpu_dtype()\n            torch.set_autocast_gpu_dtype(self.fast_dtype)\n            torch.set_autocast_enabled(self._enabled)\n            torch.autocast_increment_nesting()\n\n    def __exit__(self, *args):\n        # Drop the cache when we exit to a nesting level that's outside any instance of autocast."
},
{
    "Id": 11,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5a63c452e638dad8e077887ad8d2c94ff0e23917",
    "Violation": "missing",
    "Bug report": "This is because there are some hard-to-detect edge cases that will throw exceptions with cudnn 8.0.5 on Nvidia A40 GPU.",
    "Number of deleted lines": 0,
    "Deleted lines": "    }\n    if (tensors.is_input_packed()) {\n      return {tensors.batch_sizes_sum, out_size * rnn.num_directions()};\n    } else {\n      return {tensors.seq_length, tensors.mini_batch, out_size * rnn.num_directions()};\n    }\n  }\n\n  inline bool use_persist_common_heuristics(const RNNDescriptorParams& rnn,\n                                            const TensorDescriptorListParams& tensors) {\n    return rnn.num_layers == 1 &&\n           rnn.hidden_size <= 1024 &&\n           rnn.num_directions() == 1 &&\n           rnn.hidden_size % 128 == 0 &&\n           tensors.input_size % 128 == 0;\n  }\n\n  inline bool use_persist_device_heuristics(const RNNDescriptorParams& rnn,\n                                            const TensorDescriptorListParams& tensors) {\n    auto bsize = tensors.mini_batch;\n    cudaDeviceProp* prop = at::cuda::getCurrentDeviceProperties();\n    if (prop->major == 7) {\n      if (prop->minor == 5) {\n        // Excludes Turing from using persistent rnn.\n        return false;\n      } else {\n        // technically, batch size should be multiple of 8, but there are quite a few multiple-of-8 batchsizes that give bad perf,\n        // weed them out\n        return ((bsize % 16 == 0 && bsize != 80 && bsize !=112) || bsize == 8) &&\n               ((tensors.seq_length >=40 && bsize <=128) ||\n                (tensors.seq_length >=20 && bsize <=96) ||\n                (tensors.seq_length >=10 && bsize <=32));\n      }\n    } else if (prop->major >= 8) {\n      // Based on tests by Vasily Volkov and xwang233.  Vasily only tried bsize <= 128,\n      // so conservatively enable persistence for bsize <= 128 only.\n      // TODO:  Run more tests for bsize > 128.\n      if (rnn.mode == CUDNN_GRU) {\n        // Persistent GRU performance is flakier than other RNN types.  Exclude them for now.\n        // TODO:  Write a more refined GRU heuristic.\n        return false;\n      } else if (rnn.mode == CUDNN_LSTM) {\n        // Persistent LSTMs are comparable to or better than non-persistent for bsize <= 128.\n        return (bsize % 8 == 0) && (bsize <= 128);\n      } else {\n        // Persistent RNN_RELU and TANH show poor performance when bsize >= 96 AND hidden size >= 896.\n        return (bsize % 8 == 0) && (bsize <= 128) && (bsize < 96 || rnn.hidden_size < 896);\n      }\n    } else {\n      return false;\n    }\n  }\n\n  cudnnRNNAlgo_t get_algo(const RNNDescriptorParams& rnn, const TensorDescriptorListParams& tensors, const Tensor input) {\n    // LSTM with projections only works with standard algorithm\n    if (rnn.proj_size != 0) {\n      return CUDNN_RNN_ALGO_STANDARD;\n    }\n\n    if (getCudnnDataType(input) == CUDNN_DATA_HALF &&\n        !tensors.is_input_packed()) {\n      if (use_persist_common_heuristics(rnn, tensors) &&\n          use_persist_device_heuristics(rnn, tensors)) {\n        return CUDNN_RNN_ALGO_PERSIST_STATIC;\n      }\n    }\n    return CUDNN_RNN_ALGO_STANDARD;\n  }\n\n  cudnnDataType_t promote_rnn_math_type(cudnnDataType_t dtype) {"
},
{
    "Id": 12,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/611080a118fff166c85f3200d860f3b059abac6f",
    "Violation": "improper",
    "Bug report": "uda 11.0.x doesn't support sm86.",
    "Number of deleted lines": 2,
    "Deleted lines": "  if(CUDA_VERSION VERSION_LESS \"9.0\")\n    list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"6.2+PTX\")\n    set(CUDA_LIMIT_GPU_ARCHITECTURE \"7.0\")\n  endif()\nendif ()\n\nif(CUDA_VERSION VERSION_GREATER \"8.5\")\n  list(APPEND CUDA_KNOWN_GPU_ARCHITECTURES \"Volta\")\n  list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"7.0\")\n  list(APPEND CUDA_ALL_GPU_ARCHITECTURES \"7.0\" \"7.2\")\n\n  if(CUDA_VERSION VERSION_LESS \"10.0\")\n    list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"7.2+PTX\")\n    set(CUDA_LIMIT_GPU_ARCHITECTURE \"8.0\")\n  endif()\nendif()\n\nif(CUDA_VERSION VERSION_GREATER \"9.5\")\n  list(APPEND CUDA_KNOWN_GPU_ARCHITECTURES \"Turing\")\n  list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"7.5\")\n  list(APPEND CUDA_ALL_GPU_ARCHITECTURES \"7.5\")\n\n  if(CUDA_VERSION VERSION_LESS \"11.0\")\n    set(CUDA_LIMIT_GPU_ARCHITECTURE \"8.0\")\n    list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"7.5+PTX\")\n  endif()\nendif()\n\nif(CUDA_VERSION VERSION_GREATER \"10.5\")\n  list(APPEND CUDA_KNOWN_GPU_ARCHITECTURES \"Ampere\")\n  list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"8.0\")\n  list(APPEND CUDA_ALL_GPU_ARCHITECTURES \"8.0\")\n\n  if(CUDA_VERSION VERSION_LESS \"11.1\")\n    set(CUDA_LIMIT_GPU_ARCHITECTURE \"8.6\")\n    list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"8.0+PTX\")\n  endif()\nendif()\n\nif(CUDA_VERSION VERSION_GREATER \"11.0\")\n  list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"8.6\" \"8.6+PTX\")\n  list(APPEND CUDA_ALL_GPU_ARCHITECTURES \"8.6\")\n\n  if(CUDA_VERSION VERSION_LESS \"12.0\")\n    set(CUDA_LIMIT_GPU_ARCHITECTURE \"9.0\")\n  endif()\nendif()\n\n################################################################################################\n# A function for automatic detection of GPUs installed  (if autodetection is enabled)\n# Usage:\n#   CUDA_DETECT_INSTALLED_GPUS(OUT_VARIABLE)\n#\nfunction(CUDA_DETECT_INSTALLED_GPUS OUT_VARIABLE)\n  if(NOT CUDA_GPU_DETECT_OUTPUT)\n    if(CMAKE_CUDA_COMPILER_LOADED) # CUDA as a language\n      set(file \"${PROJECT_BINARY_DIR}/detect_cuda_compute_capabilities.cu\")\n    else()\n      set(file \"${PROJECT_BINARY_DIR}/detect_cuda_compute_capabilities.cpp\")\n    endif()\n\n    file(WRITE ${file} \"\"\n      \"#include <cuda_runtime.h>\\n\"\n      \"#include <cstdio>\\n\"\n      \"int main()\\n\"\n      \"{\\n\"\n      \"  int count = 0;\\n\"\n      \"  if (cudaSuccess != cudaGetDeviceCount(&count)) return -1;\\n\"\n      \"  if (count == 0) return -1;\\n\"\n      \"  for (int device = 0; device < count; ++device)\\n\"\n      \"  {\\n\"\n      \"    cudaDeviceProp prop;\\n\"\n      \"    if (cudaSuccess == cudaGetDeviceProperties(&prop, device))\\n\"\n      \"      std::printf(\\\"%d.%d \\\", prop.major, prop.minor);\\n\"\n      \"  }\\n\"\n      \"  return 0;\\n\"\n      \"}\\n\")\n"
},
{
    "Id": 13,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/563bbeb8905f4cea0bc5353dc12518c61113128e",
    "Violation": "insufficient",
    "Bug report": "undefined CUDA_VERSION warning",
    "Number of deleted lines": 1,
    "Deleted lines": ""
},
{
    "Id": 14,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8198474eb763c8d526ede3418211479c2f4cbd30",
    "Violation": "missing",
    "Bug report": "Previous to this PR, we only checked TorchScript nodes for scope compatibility, skipping their parent's scope reference check.",
    "Number of deleted lines": 1,
    "Deleted lines": ""
},
{
    "Id": 15,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/37dea0454dd310cfe443859f717862657df6b753",
    "Violation": "missing",
    "Bug report": "add checking for number of args checking observer in same graph",
    "Number of deleted lines": 1,
    "Deleted lines": "    # TODO: support check for standalone module\n    is_weight = node_arg_is_weight(node, arg)\n    is_bias = node_arg_is_bias(node, arg)\n    is_activation = not is_weight and not is_bias\n    if is_activation:\n        input_activation_dtype = dtype_config.get(\"input_activation_dtype\", None)\n        return input_activation_dtype is None or \\\n            node_name_to_target_dtype[node.name][\"input_activation_dtype\"] == input_activation_dtype\n    elif is_weight:\n        weight_dtype = dtype_config.get(\"weight_dtype\", None)\n        return weight_dtype is None or node_name_to_target_dtype[node.name][\"weight_dtype\"] == weight_dtype\n    else:  # bias\n        bias_dtype = dtype_config.get(\"bias_dtype\", None)\n        return bias_dtype is None or node_name_to_target_dtype[node.name][\"bias_dtype\"] == bias_dtype\n\ndef is_output_dtype_supported_by_backend(\n    node: Node,\n    node_name_to_target_dtype: Dict[str, Dict[str, Optional[Union[torch.dtype, type]]]],\n    dtype_config: Dict[str, torch.dtype],\n) -> bool:\n    \"\"\" Check if the configured qconfig for the output\n    is supported by the backend or not\n    \"\"\"\n    output_dtype = dtype_config.get(\"output_dtype\", None)\n    return output_dtype is None or \\\n        output_dtype == node_name_to_target_dtype[node.name][\"output_activation_dtype\"]\n\ndef is_observer_in_same_graph(node, modules, node_name_to_target_dtype):\n    \"\"\" Check if observer in same graph\n    when the node output is not fp32 and input is 'placeholder'\n    the input is assumed to be quantized, so it is observed\n    in a different place rather than not observed.\n    \"\"\"\n    node_output_dtype = get_arg_target_dtype_as_output(node, modules, node_name_to_target_dtype)\n    if isinstance(node.args[0], Node):\n        if node_output_dtype == torch.quint8 and node.args[0].op == 'placeholder':\n            return False\n    return True\n\ndef is_pattern_dtype_config_supported_by_backend(\n    pattern: Optional[Pattern],\n    matched_node_pattern: Optional[NodePattern],\n    node_name_to_target_dtype: Dict[str, Dict[str, Optional[Union[torch.dtype, type]]]],\n    backend_config_dict: Optional[Dict[str, Any]]\n) -> bool:\n    \"\"\" Check is the dtype configuration of a pattern is supported by\n    the backend or not\n    \"\"\"\n    if backend_config_dict is None or pattern is None:\n        return True\n    assert matched_node_pattern is not None and len(matched_node_pattern) >= 1\n    pattern_to_dtype_configs = get_pattern_to_dtype_configs(backend_config_dict)\n    dtype_configs: List[Dict[str, torch.dtype]] = pattern_to_dtype_configs.get(pattern, [])\n\n    # TODO: this only works for one input and one output patterns, need to generalize to multiple\n    # inputs/output\n    root_node = _default_root_node_getter(matched_node_pattern)\n    input_node = root_node\n    output_node = matched_node_pattern[0]\n    for dtype_config in dtype_configs:\n        # check if arg dtype are supported\n        supported = True\n        for arg in input_node.args:\n            supported = supported and \\\n                is_input_arg_dtype_supported_by_backend(\n                    arg, input_node, node_name_to_target_dtype, dtype_config)\n        for k, arg in input_node.kwargs.items():\n            supported = supported and \\\n                is_input_arg_dtype_supported_by_backend(\n                    arg, input_node, node_name_to_target_dtype, dtype_config)\n        # check if output dtype is supported"
},
{
    "Id": 16,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bdbd3ed312e0fc81e75302239ea78b3445fe95e7",
    "Violation": "insufficient",
    "Bug report": "Although `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.",
    "Number of deleted lines": 1,
    "Deleted lines": "        elif isinstance(value, NNModuleVariable):\n            # Equivant of \"self.nn_module is not None\"\n            if truth_fn(value):\n                push and self.push(value)\n                self.jump(inst)\n        elif not isinstance(value, TensorVariable) and value.has_unpack_var_sequence(\n            self\n        ):\n            if truth_fn(len(value.unpack_var_sequence(self))):\n                push and self.push(value)\n                self.jump(inst)\n        elif isinstance(value, DynamicShapeVariable):\n            eval_result = value.evaluate_expr(self.output)\n            if truth_fn(eval_result):\n                push and self.push(value)\n                self.jump(inst)\n        else:\n            unimplemented(f\"generic_jump {typestr(value)}\")\n\n    return inner\n\n\nexplain = False\n\n\ndef break_graph_if_unsupported(*, push):\n    def decorator(inner_fn):\n        @functools.wraps(inner_fn)\n        def wrapper(self: \"InstructionTranslatorBase\", inst: Instruction):\n            state = self.copy_graphstate()\n            reason = None\n            try:\n                return inner_fn(self, inst)\n            except Unsupported as excp:\n                if self.has_backedge():\n                    msg = \"Skipping frame because there is a graph break in a for/while loop\"\n                    log.debug(msg)\n                    raise exc.SkipFrame(msg) from excp\n\n                if not self.should_compile_partial_graph():\n                    raise\n\n                log.debug(\"break_graph_if_unsupported triggered compile\", exc_info=True)\n\n                user_stack = [self.frame_summary()] + list(reversed(excp.real_stack))\n                user_stack_formatted = \"\".join(traceback.format_list(user_stack))\n                frame_loc = (user_stack[-1].filename, user_stack[-1].lineno)\n                # torch._dynamo.explain() formats this a little nicer, and presents a slightly\n                # more actionable user code pointer\n                if (\n                    config.print_graph_breaks\n                    and not explain\n                    and graph_break_dup_warning_checker.add(frame_loc)\n                ):\n                    log.warning(\n                        f\"Graph break: {excp} from user code at {user_stack_formatted}\"\n                    )\n\n                excp.remove_from_stats()\n                excp.add_to_stats(\"graph_break\")\n                reason = GraphCompileReason(excp.msg, user_stack)\n            self.restore_graphstate(state)\n            self.output.compile_subgraph(self, reason=reason)\n            self.popn(push - dis.stack_effect(inst.opcode, inst.arg))\n\n            for _ in range(push):\n                self.push(UnknownVariable())\n\n            resume_call_insts = self.create_call_resume_at(self.next_instruction)\n            # Check if there is a block stack entry with GradModeVariable. And\n            # wrap the instruction causing the graph break inside a try..finally"
},
{
    "Id": 17,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7e9bf2ed860b8b60d252eead4cc457c3fe5f1667",
    "Violation": "insufficient",
    "Bug report": "Although `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.",
    "Number of deleted lines": 1,
    "Deleted lines": "        elif isinstance(value, NNModuleVariable):\n            # Equivant of \"self.nn_module is not None\"\n            if truth_fn(value):\n                push and self.push(value)\n                self.jump(inst)\n        elif not isinstance(value, TensorVariable) and value.has_unpack_var_sequence(\n            self\n        ):\n            if truth_fn(len(value.unpack_var_sequence(self))):\n                push and self.push(value)\n                self.jump(inst)\n        elif isinstance(value, DynamicShapeVariable):\n            eval_result = value.evaluate_expr(self.output)\n            if truth_fn(eval_result):\n                push and self.push(value)\n                self.jump(inst)\n        else:\n            unimplemented(f\"generic_jump {typestr(value)}\")\n\n    return inner\n\n\nexplain = False\n\n\ndef break_graph_if_unsupported(*, push):\n    def decorator(inner_fn):\n        @functools.wraps(inner_fn)\n        def wrapper(self: \"InstructionTranslatorBase\", inst: Instruction):\n            state = self.copy_graphstate()\n            reason = None\n            try:\n                return inner_fn(self, inst)\n            except Unsupported as excp:\n                if self.has_backedge():\n                    msg = \"Skipping frame because there is a graph break in a for/while loop\"\n                    log.debug(msg)\n                    raise exc.SkipFrame(msg) from excp\n\n                if not self.should_compile_partial_graph():\n                    raise\n\n                log.debug(\"break_graph_if_unsupported triggered compile\", exc_info=True)\n\n                user_stack = [self.frame_summary()] + list(reversed(excp.real_stack))\n                user_stack_formatted = \"\".join(traceback.format_list(user_stack))\n                frame_loc = (user_stack[-1].filename, user_stack[-1].lineno)\n                # torch._dynamo.explain() formats this a little nicer, and presents a slightly\n                # more actionable user code pointer\n                if (\n                    config.print_graph_breaks\n                    and not explain\n                    and graph_break_dup_warning_checker.add(frame_loc)\n                ):\n                    log.warning(\n                        f\"Graph break: {excp} from user code at {user_stack_formatted}\"\n                    )\n\n                excp.remove_from_stats()\n                excp.add_to_stats(\"graph_break\")\n                reason = GraphCompileReason(excp.msg, user_stack)\n            self.restore_graphstate(state)\n            self.output.compile_subgraph(self, reason=reason)\n            self.popn(push - dis.stack_effect(inst.opcode, inst.arg))\n\n            for _ in range(push):\n                self.push(UnknownVariable())\n\n            resume_call_insts = self.create_call_resume_at(self.next_instruction)\n            # Check if there is a block stack entry with GradModeVariable. And\n            # wrap the instruction causing the graph break inside a try..finally"
},
{
    "Id": 18,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9234f5026dbaf09a41b82bb6cf5f10ad4eeb03f2",
    "Violation": "improper",
    "Bug report": "at::cuda::CUDAEvent is \"lazy\" and only creates an event when it's first recorded. Until then, at::cuda::CUDAEvent is empty. If we use at::cuda::CUDAEvent::query() this is taken into account (an empty event is always ready), but WorkNCCL extracts the raw cudaEvent_t value from at::cuda::CUDAEvent and calls cudaEventQuery manually and doesn't check this. This could cause a failure. It's unclear if this is ever supposed to happen, but we're seeing that failure, and we want to sort it out in order to see if there's something \"deeper\" going on.",
    "Number of deleted lines": 5,
    "Deleted lines": "  if (exception()) {\n    // Already detected an exception.\n    return false;\n  }\n\n  return !checkForNCCLErrors(ncclComms_) && finishedGPUExecutionInternal();\n}\n\nvoid ProcessGroupNCCL::WorkNCCL::checkAndSetException() {\n  if (exception()) {\n    // We already have an exception.\n    return;\n  }\n\n  auto exception_ptr = checkForNCCLErrors(ncclComms_);\n  std::unique_lock<std::mutex> lock(mutex_);\n  exception_ = exception_ptr;\n}\n\nvoid ProcessGroupNCCL::WorkNCCL::setException(\n    std::exception_ptr exception_ptr) {\n  std::unique_lock<std::mutex> lock(mutex_);\n  exception_ = exception_ptr;\n}\n\n// Helper that checks if the NCCL kernels are completed on the GPUs\nbool ProcessGroupNCCL::WorkNCCL::finishedGPUExecution() {\n  checkAndSetException();\n  return finishedGPUExecutionInternal();\n}\n\nbool ProcessGroupNCCL::WorkNCCL::finishedGPUExecutionInternal() const {\n  for (size_t i = 0; i < devices_.size(); ++i) {\n    // Checking the work's corresponding CUDA events' status\n    auto ret = cudaEventQuery((*cudaEvents_)[i]);\n    if (ret != cudaSuccess && ret != cudaErrorNotReady) {\n      AT_CUDA_CHECK(ret);\n    }\n    if (ret == cudaErrorNotReady) {\n      return false;\n    }\n  }\n  return true;\n}\n\nvoid ProcessGroupNCCL::WorkNCCL::checkAndThrowException() {\n  // Set the appropriate exception if found.\n  checkAndSetException();\n\n  // Throw an exception, only if we have a valid exception.\n  if (exception()) {\n    std::rethrow_exception(exception());\n  }\n}\n\nvoid ProcessGroupNCCL::WorkNCCL::handleNCCLGuard() {\n  std::lock_guard<std::mutex> lock(mutex_);\n  completed_ = true;\n  if (exception_) {\n    auto exceptionMsg = c10::str(\n        \"Some NCCL operations have failed or timed out. Due to the \",\n        \"asynchronous nature of CUDA kernels, subsequent GPU operations \",\n        \"might run on corrupted/incomplete data. To avoid this inconsistency, \",\n        \"we are taking the entire process down.\");\n    LOG(ERROR) << exceptionMsg;\n    C10_LOG_API_USAGE_ONCE(\"ProcessGroupNCCL.WorkNCCL.handleNCCLGuard\");\n    std::rethrow_exception(exception_);\n  }\n}\n\nvoid ProcessGroupNCCL::WorkNCCL::synchronize() {\n  // Call Synchronize without a timeout. We use this method to avoid adding a\n  // timeout argument to the public synchronize API.\n  synchronizeInternal(kNoTimeout);\n}"
},
{
    "Id": 19,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e9c1ccee2247a7746fde202067a7d47b72809968",
    "Violation": "improper",
    "Bug report": "Bug fix: allow std 0 in the meta definition of normal_. All other `normal` variants allow 0.  Looks like a mistake made while copying the check. ",
    "Number of deleted lines": 1,
    "Deleted lines": "  void operator()(TensorIteratorBase& iter, double from, double to, c10::optional<Generator> gen) {\n    uniform_stub(iter.device_type(), iter, from, to, gen);\n  }\n};\n\ntemplate<typename RNG>\nstruct UniformMeta {\n  // No-op!\n  void operator()(TensorIteratorBase& iter, double from, double to, c10::optional<Generator> gen) {\n  }\n};\n\nTensor& uniform_(Tensor& self, double from, double to, c10::optional<Generator> gen) {\n  return at::native::templates::uniform_impl_<UniformStub, Generator>(self, from, to, gen);\n}\n\nTensor& uniform_meta_(Tensor& self, double from, double to, c10::optional<Generator> gen) {\n  return at::native::templates::uniform_impl_<UniformMeta, Generator>(self, from, to, gen);\n}\n\n// ==================================================== Normal ========================================================\n\ntemplate<typename RNG>\nstruct NormalStub {\n  void operator()(Tensor& self, double mean, double std, c10::optional<Generator> gen) {\n    normal_stub(self.device().type(), self, mean, std, gen);\n  }\n};\n\nTensor& normal_(Tensor& self, double mean, double std, c10::optional<Generator> gen) {\n  return at::native::templates::normal_impl_<NormalStub, Generator>(self, mean, std, gen);\n}\n\nTensor& normal_meta_(Tensor& self, double mean, double std, c10::optional<Generator> gen) {\n  TORCH_CHECK(std > 0.0, \"normal_ expects std > 0.0, but found std=\", std);  // TODO: dedupe\n  return self;\n}\n\nTensor& normal_out(const Tensor& mean, double std, c10::optional<Generator> gen, Tensor& output) {\n  return at::native::templates::normal_out_impl<NormalStub, Generator>(output, mean, std, gen);\n}\n\nTensor& normal_out(double mean, const Tensor& std, c10::optional<Generator> gen, Tensor& output) {\n  return at::native::templates::normal_out_impl<NormalStub, Generator>(output, mean, std, gen);\n}\n\nTensor& normal_out(const Tensor& mean, const Tensor& std, c10::optional<Generator> gen, Tensor& output) {\n  return at::native::templates::normal_out_impl<NormalStub, Generator>(output, mean, std, gen);\n}\n\nTensor normal(const Tensor& mean, double std, c10::optional<Generator> gen) {\n  return at::native::templates::normal_impl<NormalStub, Generator>(mean, std, gen);\n}\n\nTensor normal(double mean, const Tensor& std, c10::optional<Generator> gen) {\n  return at::native::templates::normal_impl<NormalStub, Generator>(mean, std, gen);\n}\n\nTensor normal(const Tensor& mean, const Tensor& std, c10::optional<Generator> gen) {\n  return at::native::templates::normal_impl<NormalStub, Generator>(mean, std, gen);\n}\n\n// ==================================================== Random ========================================================\n\ntemplate<typename RNG>\nstruct RandomStub {\n  void operator()(TensorIteratorBase& iter, c10::optional<Generator> gen) {\n    random_stub(iter.device_type(), iter, gen);\n  }\n};\n"
},
{
    "Id": 20,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c99277e177cf16736262251c7e92ea5e9ba2c5c2",
    "Violation": "improper",
    "Bug report": " handle the case in acc_ops.sum when dim == 0, differentiating it from the case when dim is None. handle the case in acc_ops.sum when dim == 0, differentiating it from the case when dim is None",
    "Number of deleted lines": 1,
    "Deleted lines": "        log_node = node.graph.call_function(log, kwargs=log_kwargs)\n        log_node.meta = node.meta.copy()\n        return log_node\n\n@register_custom_acc_mapper_fn(\n    op_and_target=(\"call_method\", \"sum\"),\n    arg_replacement_tuples=[\n        (\"input\", \"input\"),\n        (\"dim\", \"dim\", this_arg_is_optional),\n        (\"keepdim\", \"keepdim\", this_arg_is_optional),\n        (\"dtype\", \"dtype\", this_arg_is_optional),\n    ],\n)\n@register_custom_acc_mapper_fn(\n    op_and_target=(\"call_function\", torch.sum),\n    arg_replacement_tuples=[\n        (\"input\", \"input\"),\n        (\"dim\", \"dim\", this_arg_is_optional),\n        (\"keepdim\", \"keepdim\", this_arg_is_optional),\n        (\"dtype\", \"dtype\", this_arg_is_optional),\n    ],\n)\ndef add_sum_mapper(node: torch.fx.Node, mod: torch.fx.GraphModule) -> torch.fx.Node:\n    with node.graph.inserting_before(node):\n        sum_kwargs = dict(node.kwargs)\n        if \"dim\" in sum_kwargs and isinstance(sum_kwargs[\"dim\"], int):\n            sum_kwargs[\"dim\"] = (sum_kwargs[\"dim\"],)\n        sum_node = node.graph.call_function(sum, kwargs=sum_kwargs)\n        sum_node.meta = node.meta.copy()\n        return sum_node\n\n\n@register_acc_op\ndef sum(*, input, dim=None, keepdim=False, dtype=None):\n    if dim:\n        return torch.sum(**locals())\n    else:\n        return input.sum(dtype=dtype)\n\n\n@register_acc_op_mapping(op_and_target=(\"call_function\", torch.sigmoid))\n@register_acc_op_mapping(op_and_target=(\"call_method\", \"sigmoid\"))\n@register_acc_op\ndef sigmoid(*, input):\n    return torch.sigmoid(**locals())\n\n\n@register_acc_op_mapping(op_and_target=(\"call_function\", torch.sinh))\n@register_acc_op\ndef sinh(*, input):\n    return torch.sinh(**locals())\n\n\n@register_acc_op_mapping(op_and_target=(\"call_function\", torch.cosh))\n@register_acc_op\ndef cosh(*, input):\n    return torch.cosh(**locals())\n\n\n@register_acc_op_mapping(op_and_target=(\"call_function\", torch.tanh))\n@register_acc_op\ndef tanh(*, input):\n    return torch.tanh(**locals())\n\n\n@register_acc_op_mapping(op_and_target=(\"call_function\", torch.asin))\n@register_acc_op\ndef asin(*, input):\n    return torch.asin(**locals())\n\n"
},
{
    "Id": 21,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d72db37c4a6513c0f67f6f69870c9c45bf4880e6",
    "Violation": "unnecessary",
    "Bug report": "In file: combinatorics.py, the comparison of Collection length creates a logical short circuit. if isinstance(self.sampler, Sized) and len(self.sampler) >= 0: Here, the right side of the comparison will always return true. I suggested that the Collection length check should be removed since this is redundant.",
    "Number of deleted lines": 1,
    "Deleted lines": "\n\nclass SamplerIterDataPipe(IterDataPipe[T_co]):\n    r\"\"\"\n    Generates sample elements using the provided ``Sampler`` (defaults to :class:`SequentialSampler`).\n\n    Args:\n        datapipe: IterDataPipe to sample from\n        sampler: Sampler class to generate sample elements from input DataPipe.\n            Default is :class:`SequentialSampler` for IterDataPipe\n    \"\"\"\n    datapipe: IterDataPipe\n    sampler: Sampler\n\n    def __init__(self,\n                 datapipe: IterDataPipe,\n                 sampler: Type[Sampler] = SequentialSampler,\n                 sampler_args: Optional[Tuple] = None,\n                 sampler_kwargs: Optional[Dict] = None\n                 ) -> None:\n        assert isinstance(datapipe, Sized), \\\n            \"Sampler class requires input datapipe implemented `__len__`\"\n        super().__init__()\n        self.datapipe = datapipe\n        self.sampler_args = () if sampler_args is None else sampler_args\n        self.sampler_kwargs = {} if sampler_kwargs is None else sampler_kwargs\n        # https://github.com/python/mypy/pull/9629 will solve\n        self.sampler = sampler(data_source=self.datapipe, *self.sampler_args, **self.sampler_kwargs)  # type: ignore[misc]\n\n    def __iter__(self) -> Iterator[T_co]:\n        return iter(self.sampler)\n\n    def __len__(self) -> int:\n        # Dataset has been tested as `Sized`\n        if isinstance(self.sampler, Sized) and len(self.sampler) >= 0:\n            return len(self.sampler)\n        raise TypeError(\"{} instance doesn't have valid length\".format(type(self).__name__))\n\n\n@functional_datapipe('shuffle')\nclass ShufflerIterDataPipe(IterDataPipe[T_co]):\n    r\"\"\"\n    Shuffles the input DataPipe with a buffer (functional name: ``shuffle``). The buffer\n    with ``buffer_size`` is filled with elements from the datapipe first. Then,\n    each item will be yielded from the buffer by reservoir sampling via iterator.\n\n    ``buffer_size`` is required to be larger than ``0``. For ``buffer_size == 1``, the\n    datapipe is not shuffled. In order to fully shuffle all elements from datapipe,\n    ``buffer_size`` is required to be greater than or equal to the size of datapipe.\n\n    When it is used with :class:`torch.utils.data.DataLoader`, the methods to\n    set up random seed are different based on :attr:`num_workers`.\n\n    For single-process mode (:attr:`num_workers == 0`), the random seed is set before\n    the :class:`~torch.utils.data.DataLoader` in the main process. For multi-process\n    mode (:attr:`num_worker > 0`), `worker_init_fn` is used to set up a random seed\n    for each worker process.\n\n    Args:\n        datapipe: The IterDataPipe being shuffled\n        buffer_size: The buffer size for shuffling (default to ``10000``)\n        unbatch_level: Specifies if it is necessary to unbatch source data before\n            applying the shuffle\n\n    Example:\n        >>> # xdoctest: +SKIP\n        >>> from torchdata.datapipes.iter import IterableWrapper\n        >>> dp = IterableWrapper(range(10))\n        >>> shuffle_dp = dp.shuffle()\n        >>> list(shuffle_dp)\n        [0, 4, 1, 6, 3, 2, 9, 5, 7, 8]"
},
{
    "Id": 22,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c170d395de8ca441d3bedb20c9e45beb666f216c",
    "Violation": "missing",
    "Bug report": "Only check for xnnpack if torch installed. Fixes a bug where collect_env.py was not able to be run without having torch installed",
    "Number of deleted lines": 2,
    "Deleted lines": "\n\ndef get_libc_version():\n    import platform\n    if get_platform() != 'linux':\n        return 'N/A'\n    return '-'.join(platform.libc_ver())\n\n\ndef get_pip_packages(run_lambda):\n    \"\"\"Returns `pip list` output. Note: will also find conda-installed pytorch\n    and numpy packages.\"\"\"\n    # People generally have `pip` as `pip` or `pip3`\n    # But here it is incoved as `python -mpip`\n    def run_with_pip(pip):\n        if get_platform() == 'win32':\n            system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n            findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n            grep_cmd = r'{} /R \"numpy torch mypy\"'.format(findstr_cmd)\n        else:\n            grep_cmd = r'grep \"torch\\|numpy\\|mypy\"'\n        return run_and_read_all(run_lambda, pip + ' list --format=freeze | ' + grep_cmd)\n\n    pip_version = 'pip3' if sys.version[0] == '3' else 'pip'\n    out = run_with_pip(sys.executable + ' -mpip')\n\n    return pip_version, out\n\n\ndef get_cachingallocator_config():\n    ca_config = os.environ.get('PYTORCH_CUDA_ALLOC_CONF', '')\n    return ca_config\n\ndef is_xnnpack_available():\n    import torch.backends.xnnpack\n    return str(torch.backends.xnnpack.enabled)  # type: ignore[attr-defined]\n\ndef get_env_info():\n    run_lambda = run\n    pip_version, pip_list_output = get_pip_packages(run_lambda)\n\n    if TORCH_AVAILABLE:\n        version_str = torch.__version__\n        debug_mode_str = str(torch.version.debug)\n        cuda_available_str = str(torch.cuda.is_available())\n        cuda_version_str = torch.version.cuda\n        if not hasattr(torch.version, 'hip') or torch.version.hip is None:  # cuda version\n            hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'\n        else:  # HIP version\n            cfg = torch._C._show_config().split('\\n')\n            hip_runtime_version = [s.rsplit(None, 1)[-1] for s in cfg if 'HIP Runtime' in s][0]\n            miopen_runtime_version = [s.rsplit(None, 1)[-1] for s in cfg if 'MIOpen' in s][0]\n            cuda_version_str = 'N/A'\n            hip_compiled_version = torch.version.hip\n    else:\n        version_str = debug_mode_str = cuda_available_str = cuda_version_str = 'N/A'\n        hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'\n\n    sys_version = sys.version.replace(\"\\n\", \" \")\n\n    return SystemEnv(\n        torch_version=version_str,\n        is_debug_build=debug_mode_str,\n        python_version='{} ({}-bit runtime)'.format(sys_version, sys.maxsize.bit_length() + 1),\n        python_platform=get_python_platform(),\n        is_cuda_available=cuda_available_str,\n        cuda_compiled_version=cuda_version_str,\n        cuda_runtime_version=get_running_cuda_version(run_lambda),\n        nvidia_gpu_models=get_gpu_info(run_lambda),\n        nvidia_driver_version=get_nvidia_driver_version(run_lambda),\n        cudnn_version=get_cudnn_version(run_lambda),\n        hip_compiled_version=hip_compiled_version,"
},
{
    "Id": 23,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/65faf1a7eb07129d8b1f017fac341e178620dabd",
    "Violation": "missing",
    "Bug report": "Add version check for ProfilingVerbosity bulider config",
    "Number of deleted lines": 1,
    "Deleted lines": "        fp16_mode=True,\n        int8_mode=False,\n        sparse_weights=False,\n        force_fp32_output=False,\n        strict_type_constraints=False,\n        algorithm_selector=None,\n        timing_cache=None,\n        profiling_verbosity=None,\n    ) -> TRTInterpreterResult:\n        # For float outputs, we set their dtype to fp16 only if fp16_mode=True and\n        # force_fp32_output=False.\n        self.output_fp16 = not force_fp32_output and fp16_mode\n\n        if int8_mode and not self.builder.platform_has_fast_int8:\n            warnings.warn(\"Current platform doesn't support fast native int8!\")\n\n        if fp16_mode and not self.builder.platform_has_fast_fp16:\n            warnings.warn(\"Current platform doesn't support fast native fp16!\")\n\n        self.input_specs_iter = 0\n        super().run()\n\n        self.builder.max_batch_size = max_batch_size\n        builder_config = self.builder.create_builder_config()\n        builder_config.max_workspace_size = max_workspace_size\n\n        cache = None\n        if timing_cache:\n            cache_file = numpy.array(timing_cache)\n            cache = builder_config.create_timing_cache(cache_file.tobytes())\n        else:\n            cache = builder_config.create_timing_cache(b\"\")\n        builder_config.set_timing_cache(cache, False)\n\n        builder_config.profiling_verbosity = profiling_verbosity if profiling_verbosity else trt.ProfilingVerbosity.LAYER_NAMES_ONLY\n        if fp16_mode:\n            builder_config.set_flag(trt.BuilderFlag.FP16)\n\n        if int8_mode:\n            builder_config.set_flag(trt.BuilderFlag.INT8)\n\n        if sparse_weights:\n            assert fp16_mode or int8_mode, \"We can only enable sparsity in fp16 or int8 mode.\"\n            builder_config.set_flag(trt.BuilderFlag.SPARSE_WEIGHTS)\n\n        if strict_type_constraints:\n            builder_config.set_flag(trt.BuilderFlag.STRICT_TYPES)\n\n        if self.optimization_profiles:\n            for optimization_profile in self.optimization_profiles:\n                builder_config.add_optimization_profile(optimization_profile)\n\n        if algorithm_selector:\n            builder_config.set_flag(trt.BuilderFlag.DISABLE_TIMING_CACHE)\n            builder_config.algorithm_selector = algorithm_selector\n\n        engine = self.builder.build_engine(self.network, builder_config)\n        assert engine\n\n        serialized_cache = bytearray(cache.serialize()) \\\n            if builder_config.get_timing_cache() else bytearray()\n\n        return TRTInterpreterResult(engine, self._input_names, self._output_names, serialized_cache)\n\n    def run_node(self, n):\n        self._cur_node_name = str(n)\n        # add \"_itensor_to_tensor_meta\"\n        kwargs = dict(n.kwargs)\n        kwargs[\"_itensor_to_tensor_meta\"] = self._itensor_to_tensor_meta\n        n.kwargs = kwargs\n"
},
{
    "Id": 24,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5fdddbbfe8ef17b2c81ed34a48f3b963944aa4c3",
    "Violation": "improper",
    "Bug report": "Fix checking of current mode in PyOperator dispatch",
    "Number of deleted lines": 3,
    "Deleted lines": "                torch.utils._python_dispatch.TorchDispatchMode,\n            ):\n                mode = dispatch_key_or_mode_or_transform\n                assert mode not in self.python_key_mode_table\n                # TODO(voz): Should we replace setting torch._C.DispatchKey.Python entirely with setting mode keys?\n                self.python_key_mode_table[mode] = fn\n                return fn\n\n            if isinstance(\n                dispatch_key_or_mode_or_transform, torch._C._functorch.TransformType\n            ):\n                transform = dispatch_key_or_mode_or_transform\n                self.functorch_table[transform] = fn\n                return fn\n\n            dispatch_key = dispatch_key_or_mode_or_transform\n            assert (\n                dispatch_key != torch._C.DispatchKey.Python\n            ), \"Please register a mode for the torch._C.DispatchKey.Python key instead.\"\n            assert isinstance(dispatch_key, torch._C.DispatchKey)\n            assert dispatch_key not in self.table\n            self.table[dispatch_key] = fn\n            return fn\n\n        return inner\n\n    def dispatch(self, dispatch_key, *args, **kwargs):\n        from torch.utils._python_dispatch import _get_current_dispatch_mode\n\n        if dispatch_key == torch._C.DispatchKey.FuncTorchDynamicLayerFrontMode:\n            return dispatch_functorch(self, args, kwargs)\n\n        if dispatch_key == torch._C.DispatchKey.Python:\n            # TODO(voz): We should walk all the nodes here / turn it into a list, topmode is ok for now.\n            curr_mode = type(_get_current_dispatch_mode())\n            assert (\n                curr_mode is not None\n            ), \"Illegal invocation of dispatch on torch._C.DispatchKey.Python without a mode.\"\n            assert (\n                curr_mode in self.python_key_mode_table\n            ), f\"Current active mode {curr_mode} not registered\"\n            # TODO(voz): The idea behind this is that we do not yet support dispatch by key + mode, only key.\n            return self.python_key_mode_table[curr_mode](*args, **kwargs)\n\n        assert dispatch_key in self.table, dispatch_key\n        return self.table[dispatch_key](*args, **kwargs)\n\n    def __call__(self, *args, **kwargs):\n        flat_args = _to_flat_tuple(args, kwargs)\n        if torch.overrides.has_torch_function(flat_args):\n            return torch.overrides.handle_torch_function(\n                self, flat_args, *args, **kwargs\n            )\n\n        dispatch_key_set = _compute_keyset(args, kwargs)\n        return self.dispatch(dispatch_key_set.highestPriorityTypeId(), *args, **kwargs)\n\n    def name(self):\n        return self.name\n\n    # TODO(voz): Should rewrite fallthrough register as the impl for keys we do not specify\n    # as opposed to being this sort of explicit thing where ops are a little too key aware...\n    def _fallthrough_fn(self, operator, dispatch_key):\n        def inner(*args, **kwargs):\n            all_keys_after_current = torch._C._dispatch_keyset_full_after(dispatch_key)\n            all_keys_after_current_masked = all_keys_after_current & _compute_keyset(\n                args, kwargs\n            )\n            return self.dispatch(\n                all_keys_after_current_masked.highestPriorityTypeId(), *args, **kwargs\n            )\n\n        return inner\n\n\ndef _to_flat_tuple(args, kwargs):\n    flat_args, _ = torch.utils._pytree.tree_flatten(args)\n    flat_kwargs, _ = torch.utils._pytree.tree_flatten(kwargs)\n    flat_all = flat_args + flat_kwargs"
},
{
    "Id": 25,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0c0c9e743e82b398435ed07719e998aa15ac1ce1",
    "Violation": "improper",
    "Bug report": "Fix dimensions check ",
    "Number of deleted lines": 1,
    "Deleted lines": "  for (int colIndex = blockIdx.x; colIndex < cols; colIndex += gridDim.x) {\n    float sum = 0;\n    for (int rowIndex = threadIdx.x; rowIndex < rows; rowIndex += blockDim.x) {\n      sum += data[rowIndex * cols + colIndex];\n    }\n    sum = BlockReduce(temp_storage).Reduce(sum, cub::Sum());\n    if (threadIdx.x == 0) {\n      out[colIndex] = alpha * sum;\n    }\n    __syncthreads();\n  }\n}\n}\n\ntemplate <\n    typename T,\n    class Context = CUDAContext,\n    bool FIRSTDIMS = true,\n    bool NORMALIZE = false>\nclass ReduceDimsOp : public Operator<CUDAContext> {\n public:\n  USE_OPERATOR_CONTEXT_FUNCTIONS;\n  ReduceDimsOp(const OperatorDef& operator_def, Workspace* ws)\n      : Operator<CUDAContext>(operator_def, ws),\n        num_reduce_dims_(\n            OperatorBase::GetSingleArgument<int32_t>(\"num_reduce_dim\", 1)) {}\n\n  ~ReduceDimsOp() {}\n\n  bool RunOnDevice() override {\n    const auto& input = Input(0);\n    const auto* input_data = input.template data<T>();\n    auto* Y = Output(0);\n\n    CHECK_LT(num_reduce_dims_, input.dims().size());\n    const int M = FIRSTDIMS\n        ? input.size_to_dim(num_reduce_dims_)\n        : input.size_to_dim(input.ndim() - num_reduce_dims_);\n    const int N = FIRSTDIMS\n        ? input.size_from_dim(num_reduce_dims_)\n        : input.size_from_dim(input.ndim() - num_reduce_dims_);\n\n    vector<TIndex> output_shape;\n    int start_index = FIRSTDIMS ? num_reduce_dims_ : 0;\n    int end_index = FIRSTDIMS ? input.dims().size()\n                              : input.dims().size() - num_reduce_dims_;\n    for (int i = start_index; i < end_index; ++i) {\n      output_shape.push_back(input.dims()[i]);\n    }\n\n    Y->Resize(output_shape);\n\n    int in_dim = FIRSTDIMS ? M : N;\n\n    T alpha = 1.0;\n    if (NORMALIZE) { // Static if\n      alpha = 1.0 / in_dim;\n    }\n\n    if (FIRSTDIMS) {\n      columnwise_sum_kernel<<<\n          std::min(N, CAFFE_MAXIMUM_NUM_BLOCKS),\n          CAFFE_CUDA_NUM_THREADS,\n          0,\n          context_.cuda_stream()>>>(\n          M, N, alpha, input_data, Y->template mutable_data<T>());\n    } else {\n      rowwise_sum_kernel<<<\n          std::min(M, CAFFE_MAXIMUM_NUM_BLOCKS),\n          CAFFE_CUDA_NUM_THREADS,\n          0,"
},
{
    "Id": 26,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4d0fbb0e6f578bea14f3f52b0a927bcc20f8b109",
    "Violation": "improper",
    "Bug report": "when adding a new axis to concatenate along, allow it to be the last axis. For example, concated 1D columns into a 2D matrix with axis=1, add_axis=1.",
    "Number of deleted lines": 1,
    "Deleted lines": "  for (int i = axis_ + 1; i < input.ndim(); ++i) {\n    after *= input.dim32(i);\n  }\n  if (add_axis_) {\n    output_dims.erase(output_dims.begin() + axis_);\n  }\n  for (int i = 0; i < OutputSize(); ++i) {\n    auto* output = Output(i);\n    auto axis_dim = add_axis_ ? 1 : axis_data[i];\n    if (!add_axis_) {\n      output_dims[axis_] = axis_data[i];\n    }\n    output->Resize(output_dims);\n    math::CopyMatrix<Context>(\n        input.itemsize(),\n        before,\n        axis_dim * after,\n        static_cast<const char*>(input.raw_data()) + input_offset,\n        input.dim32(axis_) * after,\n        output->raw_mutable_data(input.meta()),\n        axis_dim * after,\n        &context_);\n    input_offset += axis_dim * after * input.itemsize();\n  }\n  return true;\n}\n\ntemplate <class Context>\nbool ConcatOp<Context>::RunOnDevice() {\n  auto* output = Output(0);\n  TensorCPU* split = OperatorBase::Output<TensorCPU>(1);\n  split->Resize(vector<TIndex>(1, InputSize()));\n  int* axis_data = split->template mutable_data<int>();\n  auto& input_zero = Input(0);\n  CAFFE_ENFORCE_LT(axis_, input_zero.ndim(), \"Axis not in input ndim range.\");\n  for (int i = 1; i < InputSize(); ++i) {\n    CAFFE_ENFORCE(\n        Input(i).meta() == input_zero.meta(),\n        \"All inputs must have the same type, expected: \",\n        input_zero.meta().name(),\n        \" but got: \",\n        Input(i).meta().name(),\n        \" for input: \",\n        i);\n  }\n\n  int before = 1, after = 1;\n  vector<TIndex> output_dims(input_zero.dims());\n  for (int i = 0; i < input_zero.ndim(); ++i) {\n    if (i == axis_ && !add_axis_) {\n      continue;\n    }\n    int dim = input_zero.dim32(i);\n    if (i < axis_) {\n      before *= dim;\n    } else { // i > axis_ || i == axis_ && add_axis_\n      after *= dim;\n    }\n    // check the input dims are compatible.\n    for (int j = 1; j < InputSize(); ++j) {\n      int dim_j = Input(j).dim32(i);\n      CAFFE_ENFORCE(\n          dim == dim_j,\n          \"Expect dimension = \",\n          dim,\n          \" got \",\n          dim_j,\n          \" at axis = \",\n          i,\n          \" for input: \",\n          j,"
},
{
    "Id": 27,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b2d110447190abe5d66b0b59a775cc4881f3e30e",
    "Violation": "improper",
    "Bug report": "Fixed numpy bool check",
    "Number of deleted lines": 1,
    "Deleted lines": "        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\n        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\n        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\n            Default: ``False``.\n        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\n        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n\n    Examples::\n\n        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n\n    \"\"\"\n    __constants__ = ['batch_first']\n    bias_k: Optional[torch.Tensor]\n    bias_v: Optional[torch.Tensor]\n\n    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\n                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super(MultiheadAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.batch_first = batch_first\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n\n        if self._qkv_same_embed_dim is False:\n            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n            self.register_parameter('in_proj_weight', None)\n        else:\n            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n            self.register_parameter('q_proj_weight', None)\n            self.register_parameter('k_proj_weight', None)\n            self.register_parameter('v_proj_weight', None)\n\n        if bias:\n            self.in_proj_bias = Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\n        else:\n            self.register_parameter('in_proj_bias', None)\n        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\n\n        if add_bias_kv:\n            self.bias_k = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n            self.bias_v = Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\n        else:\n            self.bias_k = self.bias_v = None\n\n        self.add_zero_attn = add_zero_attn\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        if self._qkv_same_embed_dim:\n            xavier_uniform_(self.in_proj_weight)\n        else:\n            xavier_uniform_(self.q_proj_weight)\n            xavier_uniform_(self.k_proj_weight)\n            xavier_uniform_(self.v_proj_weight)\n\n        if self.in_proj_bias is not None:\n            constant_(self.in_proj_bias, 0.)"
},
{
    "Id": 28,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/85cbe0d8258ab06897e2f049e61f74d8aa935240",
    "Violation": "missing",
    "Bug report": "This diff is similar to D14163001. We need to handle the edge case when add_axis=1.",
    "Number of deleted lines": 1,
    "Deleted lines": ")DOC\")\n    .InheritOnnxSchema();\n\nOPERATOR_SCHEMA(SplitByLengths)\n    .NumInputs(2)\n    .NumOutputs(1, INT_MAX)\n    .Input(0, \"input\", \"The tensor to split\")\n    .Input(1, \"legnths\", \"The tensor `l_i` indicates the logic block of input.\")\n    .Arg(\"axis\", \"Which axis to split on\")\n    .Arg(\"order\", \"Either NHWC or NCWH, will split on C axis, defaults to NCHW\")\n    .DeviceInferenceFunction([](const OperatorDef& def) {\n      auto op_device =\n          def.has_device_option() ? def.device_option() : DeviceOption();\n      vector<DeviceOption> in_dev(def.input_size(), op_device);\n      vector<DeviceOption> out_dev(def.output_size(), op_device);\n      // lengths input should be on CPU\n      in_dev[1] = DeviceOption();\n      return std::make_pair(in_dev, out_dev);\n    })\n    .SetDoc(R\"DOC(\nSplit a tensor into a list of tensors, given a lengths input, along the specified\n'axis'. If `K` outputs are provided, the op assumes `len(lengths) % K == 0`.\nThe `input` will be split into `K` parts. Each part of length\n`sum(lengths[i*k:i*k+k))`)DOC\");\n\nOpSchema::Cost CostInferenceForConcat(\n    const OperatorDef& def,\n    const vector<TensorShape>& in) {\n  ArgumentHelper helper(def);\n  const int axis = helper.HasArgument(\"axis\")\n      ? helper.GetSingleArgument<int>(\"axis\", -1)\n      : GetDimFromOrderString(\n            helper.GetSingleArgument<string>(\"order\", \"NCHW\"));\n  bool add_axis = helper.GetSingleArgument<int>(\"add_axis\", 0) != 0;\n  const int canonical_axis = canonical_axis_index_(axis, in[0].dims_size());\n  CAFFE_ENFORCE_GT(in.size(), 0);\n  vector<int> out_shape(in[0].dims().begin(), in[0].dims().end());\n  if (add_axis) {\n    out_shape.insert(out_shape.begin() + canonical_axis, in.size());\n  } else {\n    for (size_t i = 1; i < in.size(); ++i) {\n      out_shape[canonical_axis] += in[i].dims(canonical_axis);\n    }\n  }\n  uint64_t nElemRead = 1;\n  for (int i = 0; i < in.size(); ++i) {\n    nElemRead += nElemFromDim(in[i]);\n  }\n  int size = 1;\n  for (auto& s : out_shape) {\n    size *= s;\n  }\n\n  struct OpSchema::Cost cost;\n  cost.flops = 0;\n  cost.bytes_read = nElemRead * sizeof(in[0].data_type());\n  cost.bytes_written = size * sizeof(in[0].data_type());\n  cost.params_bytes = 0;\n  return cost;\n}\n\nnamespace {\nstd::pair<std::vector<DeviceOption>, std::vector<DeviceOption>>\nconcatOpDevInfer(const OperatorDef& def) {\n  auto op_device =\n      def.has_device_option() ? def.device_option() : DeviceOption();\n  vector<DeviceOption> in_dev(def.input_size(), op_device);\n  vector<DeviceOption> out_dev(def.output_size(), op_device);\n\n  // 2nd output's type is always CPU irrespective of op's device option.\n  CAFFE_ENFORCE_GT(out_dev.size(), 1);"
},
{
    "Id": 29,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4f63f348aef3da8b4d53f61098f4e32bd916c221",
    "Violation": "improper",
    "Bug report": "The bounds check was too conservative by an extra one.",
    "Number of deleted lines": 1,
    "Deleted lines": "  std::vector<int64_t> strides;\n\n  for(int64_t d = 0; d < tensor.dim(); d++) {\n    if(tensor.sizes()[d] != 1) {\n      sizes.push_back(tensor.sizes()[d]);\n      strides.push_back(tensor.strides()[d]);\n    }\n  }\n\n  return std::make_tuple(sizes, strides);\n}\n\nstd::tuple<std::vector<int64_t>, std::vector<int64_t> >\ninferSqueezeGeometry(const Tensor& tensor, int64_t dim) {\n  std::vector<int64_t> sizes;\n  std::vector<int64_t> strides;\n\n  for(int64_t d = 0; d < tensor.dim(); d++) {\n    if(d != dim || tensor.sizes()[dim] != 1) {\n      sizes.push_back(tensor.sizes()[d]);\n      strides.push_back(tensor.strides()[d]);\n    }\n  }\n  return std::make_tuple(sizes, strides);\n}\n\nstd::tuple<std::vector<int64_t>, std::vector<int64_t> >\ninferUnsqueezeGeometry(const Tensor& tensor, int64_t dim) {\n  if (tensor.numel() == 0) {\n    throw std::runtime_error(\"cannot unsqueeze empty tensor\");\n  }\n\n  std::vector<int64_t> sizes(tensor.sizes());\n  std::vector<int64_t> strides(tensor.strides());\n  int64_t new_stride = dim >= tensor.dim() - 1 ? 1 : sizes[dim] * strides[dim];\n  sizes.insert(sizes.begin() + dim, 1);\n  strides.insert(strides.begin() + dim, new_stride);\n\n  return std::make_tuple(sizes, strides);\n}\n\nTensor squeeze(const Tensor& self) {\n  auto g = inferSqueezeGeometry(self);\n  return self.as_strided(std::get<0>(g), std::get<1>(g));\n}\n\nTensor squeeze(const Tensor& self, int64_t dim) {\n  int64_t dims = self.dim();\n  dim = maybe_wrap_dim(dim, dims);\n\n  if (dims == 0 || self.sizes()[dim] != 1) {\n    return self.as_strided(self.sizes().vec(), self.strides().vec());\n  }\n  auto g = inferSqueezeGeometry(self, dim);\n  return self.as_strided(std::get<0>(g), std::get<1>(g));\n}\n\nTensor & squeeze_(Tensor& self) {\n  auto g = inferSqueezeGeometry(self);\n  return self.as_strided_(std::get<0>(g), std::get<1>(g));\n}\n\nTensor & squeeze_(Tensor& self, int64_t dim) {\n  int64_t dims = self.dim();\n  dim = maybe_wrap_dim(dim, self.dim());\n\n  if (dims == 0 || self.sizes()[dim] != 1) {\n    return self.as_strided_(self.sizes().vec(), self.strides().vec());\n  }\n  auto g = inferSqueezeGeometry(self, dim);\n  return self.as_strided_(std::get<0>(g), std::get<1>(g));"
},
{
    "Id": 30,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bf32ea80942ce720b105efcd517fd11182edeb08",
    "Violation": "insufficient",
    "Bug report": " Fix dimension check in 1D instance norm, allowing 2D tensors alongsid e 3D.",
    "Number of deleted lines": 2,
    "Deleted lines": "        where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n        new observed value.\n\n    Args:\n        num_features: :math:`C` from an expected input of size\n            :math:`(N, C, L)` or :math:`L` from input of size :math:`(N, L)`\n        eps: a value added to the denominator for numerical stability. Default: 1e-5\n        momentum: the value used for the running_mean and running_var computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, this module has\n            learnable affine parameters, initialized the same way as done for batch normalization.\n            Default: ``False``.\n        track_running_stats: a boolean value that when set to ``True``, this\n            module tracks the running mean and variance, and when set to ``False``,\n            this module does not track such statistics and always uses batch\n            statistics in both training and eval modes. Default: ``False``\n\n    Shape:\n        - Input: :math:`(N, C, L)`\n        - Output: :math:`(N, C, L)` (same shape as input)\n\n    Examples::\n\n        >>> # Without Learnable Parameters\n        >>> m = nn.InstanceNorm1d(100)\n        >>> # With Learnable Parameters\n        >>> m = nn.InstanceNorm1d(100, affine=True)\n        >>> input = torch.randn(20, 100, 40)\n        >>> output = m(input)\n\n    .. _`Instance Normalization: The Missing Ingredient for Fast Stylization`:\n        https://arxiv.org/abs/1607.08022\n    \"\"\"\n\n    def _check_input_dim(self, input):\n        if input.dim() != 3:\n            raise ValueError('expected 3D input (got {}D input)'\n                             .format(input.dim()))\n\n\nclass InstanceNorm2d(_InstanceNorm):\n    r\"\"\"Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs\n    with additional channel dimension) as described in the paper\n    `Instance Normalization: The Missing Ingredient for Fast Stylization`_ .\n\n    .. math::\n\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n\n    The mean and standard-deviation are calculated per-dimension separately\n    for each object in a mini-batch. :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\n    of size `C` (where `C` is the input size) if :attr:`affine` is ``True``.\n\n    By default, this layer uses instance statistics computed from input data in\n    both training and evaluation modes.\n\n    If :attr:`track_running_stats` is set to ``True``, during training this\n    layer keeps running estimates of its computed mean and variance, which are\n    then used for normalization during evaluation. The running estimates are\n    kept with a default :attr:`momentum` of 0.1.\n\n    .. note::\n        This :attr:`momentum` argument is different from one used in optimizer\n        classes and the conventional notion of momentum. Mathematically, the\n        update rule for running statistics here is\n        :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momemtum} \\times x_t`,\n        where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n        new observed value.\n\n    Args:\n        num_features: :math:`C` from an expected input of size\n            :math:`(N, C, H, W)`\n        eps: a value added to the denominator for numerical stability. Default: 1e-5"
},
{
    "Id": 31,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a9deda5469a6ef73692a9dd796cc4eeba4436d6c",
    "Violation": "improper",
    "Bug report": "The at::native::_validate_sparse_coo_tensor_args only supports checking the indices on CUDA device and CPU device. To extend the function to support more device type.",
    "Number of deleted lines": 1,
    "Deleted lines": "    const Tensor& indices,\n    const Tensor& values_,\n    ArrayRef<int64_t> size) {\n  Tensor values = expand_values_if_needed(values_);\n\n  // the following checks are redundant because they are also checked in\n  // SparseTensorImpl::set_indices_and_values_unsafe but we need to ensure them\n  // in order to infer the shape.\n  TORCH_CHECK(\n      indices.dim() == 2,\n      \"indices must be sparse_dim x nnz, but got: \",\n      indices.sizes())\n  TORCH_CHECK(\n      !indices.is_sparse(),\n      \"expected indices to be a dense tensor, but got indices of layout \",\n      indices.layout());\n  int64_t sparse_dim = indices.size(0);\n  int64_t dense_dim = values.dim() - 1;\n  TORCH_CHECK(\n      static_cast<int64_t>(size.size()) == sparse_dim + dense_dim,\n      \"number of dimensions must be sparse_dim (\",\n      sparse_dim,\n      \") + dense_dim (\",\n      dense_dim,\n      \"), but got \",\n      size.size());\n\n  // Check to make sure all indices are within the boundaries of `size`\n  if (indices.numel() > 0) {\n    Tensor min_indices =\n        std::get</* values */ 0>(indices.min(/* dim */ 1, /* keepdim */ false));\n    Tensor max_indices =\n        std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n    Tensor cpu_min_indices, cpu_max_indices;\n    if (indices.is_cuda()) {\n      cpu_min_indices = min_indices.to(at::DeviceType::CPU);\n      cpu_max_indices = max_indices.to(at::DeviceType::CPU);\n    } else {\n      cpu_min_indices = min_indices;\n      cpu_max_indices = max_indices;\n    }\n    auto cpu_min_indices_accessor = cpu_min_indices.accessor<int64_t, 1>();\n    auto cpu_max_indices_accessor = cpu_max_indices.accessor<int64_t, 1>();\n    for (const auto d : c10::irange(sparse_dim)) {\n      // NB: This used to sync ndim times to access each entry; now we copy\n      // everything to CPU first and then access it.\n      int64_t min_index_in_dim = cpu_min_indices_accessor[d];\n      TORCH_CHECK(\n          min_index_in_dim >= 0,\n          \"found negative index \",\n          min_index_in_dim,\n          \" for dim \",\n          d);\n      int64_t max_index_in_dim = cpu_max_indices_accessor[d];\n      int64_t dim_size = size[static_cast<size_t>(d)];\n      TORCH_CHECK(\n          max_index_in_dim < dim_size,\n          \"size is inconsistent with indices: for dim \",\n          d,\n          \", size is \",\n          dim_size,\n          \" but found index \",\n          max_index_in_dim);\n    }\n  }\n}\n\n// NB: Got rid of the sizes == NULL case\n\nTensor sparse_coo_tensor(const Tensor& indices, const Tensor& values, IntArrayRef size,\n    c10::optional<ScalarType> dtype,"
},
{
    "Id": 32,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d9870d70c12dc59b0f8bce288910422bcb60b044",
    "Violation": "insufficient",
    "Bug report": "Exempt _foreach_norm from autograd_not_implemented_fallback check",
    "Number of deleted lines": 1,
    "Deleted lines": "      0,\n      num_arguments);\n#endif\n  if (aliased_input_idx != -1 || any_is_inplace_output) {\n    at::AutoDispatchBelowAutograd guard;\n    op.redispatchBoxed(dispatch_keys & c10::after_autograd_keyset, stack);\n  } else {\n    // If neither in-place nor view\n    at::AutoDispatchBelowADInplaceOrView guard;\n    op.redispatchBoxed(\n        dispatch_keys & c10::after_ADInplaceOrView_keyset, stack);\n  }\n#ifndef NDEBUG\n  _foreach_tensor(\n      [&](size_t idx_tensor, size_t _, const at::Tensor& t) {\n        if (storage_saved.at(idx_tensor).has_value())\n          TORCH_INTERNAL_ASSERT(\n              storage_saved.at(idx_tensor).value().is_alias_of(t.storage()),\n              op_name);\n        if (impl_saved.at(idx_tensor))\n          TORCH_INTERNAL_ASSERT(\n              impl_saved.at(idx_tensor) == t.getIntrusivePtr(), op_name);\n      },\n      &stack_args_copy,\n      0,\n      num_arguments);\n  _foreach_tensor(\n      [&](size_t idx_tensor, size_t idx_ret, const at::Tensor& t) {\n        if (at::impl::tensor_has_dispatch(t) ||\n            at::impl::dispatch_mode_enabled())\n          return;\n        if (!is_inplace_output[idx_ret])\n          TORCH_INTERNAL_ASSERT(\n              t.use_count() <= 1, op_name); // Okay to return undefined tensor\n        if (!is_aliased_output[idx_ret] && t.has_storage())\n          TORCH_INTERNAL_ASSERT(t.storage().use_count() == 1);\n      },\n      stack,\n      stack->size() - num_returns,\n      num_returns);\n  // There should be only a single base-view pair, make sure their storage is\n  // aliased.\n  if (aliased_input_idx != -1 && aliased_output_idx != -1) {\n    const c10::IValue& aliased_input_iv = stack_args_copy[aliased_input_idx];\n    const c10::IValue& aliased_output_iv =\n        (*stack)[stack->size() - num_returns + aliased_output_idx];\n    TORCH_INTERNAL_ASSERT(aliased_input_iv.isTensor(), op_name);\n    TORCH_INTERNAL_ASSERT(\n        aliased_output_iv.isTensor() || aliased_output_iv.isTensorList(),\n        op_name);\n    const at::Tensor& aliased_input = aliased_input_iv.toTensor();\n    if (aliased_input.has_storage()) {\n      if (aliased_output_iv.isTensor()) {\n        const at::Tensor& aliased_output = aliased_input_iv.toTensor();\n        TORCH_INTERNAL_ASSERT(\n            aliased_input.storage().is_alias_of(aliased_output.storage()),\n            op_name);\n      } else {\n        const auto aliased_output_vec = aliased_output_iv.toTensorVector();\n        for (const auto& aliased_output : aliased_output_vec) {\n          TORCH_INTERNAL_ASSERT(\n              aliased_input.storage().is_alias_of(aliased_output.storage()),\n              op_name);\n        }\n      }\n    }\n  }\n#endif\n\n  if (any_requires_grad) {\n    _foreach_tensor("
},
{
    "Id": 33,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/98f9ff90268ae62ab6d794cce0786121bf17edc9",
    "Violation": "improper",
    "Bug report": "Fix an assertion failure involving Slice. Before this change, exporting a model to ONNX involving Slice crashes at `axes[i]` in line 153 if C++ assertions are enabled:",
    "Number of deleted lines": 1,
    "Deleted lines": "        << \"Constant folding not applied.\" << std::endl;\n    return c10::nullopt;\n  }\n  // Checking validity of 'starts' and 'ends' input\n  if (inputTensorValues[1].sizes().size() != 1 ||\n      inputTensorValues[2].sizes().size() != 1) {\n    std::cerr\n        << \"Warning: Constant folding - Invalid 'starts' or 'ends' inputs found for opset >= 10 onnx::Slice op. \"\n        << \"Constant folding not applied.\" << std::endl;\n    return c10::nullopt;\n  }\n  if (inputTensorValues[1].sizes()[0] != inputTensorValues[2].sizes()[0]) {\n    // Number of elements of 'starts' and 'ends' 1-D input tensors should be the\n    // same\n    return c10::nullopt;\n  }\n  // Checking 'axes' input, if available.\n  std::vector<int64_t> axes;\n  if (inputTensorValues.size() > 3) {\n    if (inputTensorValues[3].sizes().size() != 1) {\n      std::cerr\n          << \"Warning: Constant folding - Invalid 'axes' input found for opset >= 10 onnx::Slice op. \"\n          << \"Constant folding not applied.\" << std::endl;\n      return c10::nullopt;\n    }\n    if (inputTensorValues[3].sizes()[0] != inputTensorValues[1].sizes()[0]) {\n      // Number of elements of 'axes' and 'ends' 1-D input tensors should be the\n      // same\n      std::cerr\n          << \"Warning: Constant folding - Invalid 'axes' or 'ends' inputs found for opset >= 10 onnx::Slice op. \"\n          << \"Constant folding not applied.\" << std::endl;\n      return c10::nullopt;\n    }\n    auto axes_a = inputTensorValues[3].accessor<int64_t, 1>();\n    axes.reserve(inputTensorValues[3].sizes()[0]);\n    // ONNX slice accepts negative axis, fix this for aten op\n    for (const auto i : c10::irange(inputTensorValues[3].sizes()[0])) {\n      axes[i] = axes_a[i] < 0 ? axes_a[i] + inputTensorValues[0].sizes().size()\n                              : axes_a[i];\n    }\n  } else {\n    axes = std::vector<int64_t>(inputTensorValues[1].sizes()[0], 0);\n  }\n  // Checking 'steps' input, if available.\n  if (inputTensorValues.size() > 4) {\n    if (inputTensorValues[4].sizes().size() != 1) {\n      std::cerr\n          << \"Warning: Constant folding - Invalid 'steps' input found for opset >= 10 onnx::Slice op. \"\n          << \"Constant folding not applied.\" << std::endl;\n      return c10::nullopt;\n    }\n    if (inputTensorValues[4].sizes()[0] != inputTensorValues[1].sizes()[0]) {\n      // Number of elements of 'steps' and 'ends' 1-D input tensors should be\n      // the same\n      std::cerr\n          << \"Warning: Constant folding - Invalid 'steps' or 'ends' inputs found for opset >= 10 onnx::Slice op. \"\n          << \"Constant folding not applied.\" << std::endl;\n      return c10::nullopt;\n    }\n    auto steps_a = inputTensorValues[4].accessor<int64_t, 1>();\n    for (const auto i : c10::irange(inputTensorValues[4].sizes()[0])) {\n      // Only steps == 1 are supported for constant-folding.\n      if (steps_a[i] != 1) {\n        std::cerr\n            << \"Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. \"\n            << \"Constant folding not applied.\" << std::endl;\n        return c10::nullopt;\n      }\n    }\n  }\n  auto starts_a = inputTensorValues[1].accessor<int64_t, 1>();"
},
{
    "Id": 34,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/71af538e31547e5b1bc814c9e00323a21905baf3",
    "Violation": "insufficient",
    "Bug report": "Updated assert to remove check on 3rd dim for MHA.  Updated assert statement to remove check on 3rd dimension (features) for keys and values in MultiheadAttention / Transform. The feature dimension for keys and values can now be of different sizes",
    "Number of deleted lines": 1,
    "Deleted lines": "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n          is provided, it will be added to the attention weight.\n        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n\n        Outputs:\n        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n          E is the embedding dimension.\n        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n          L is the target sequence length, S is the source sequence length.\n    \"\"\"\n    if not torch.jit.is_scripting():\n        tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v,\n                    out_proj_weight, out_proj_bias)\n        if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):\n            return handle_torch_function(\n                multi_head_attention_forward, tens_ops, query, key, value,\n                embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias,\n                bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight,\n                out_proj_bias, training=training, key_padding_mask=key_padding_mask,\n                need_weights=need_weights, attn_mask=attn_mask,\n                use_separate_proj_weight=use_separate_proj_weight,\n                q_proj_weight=q_proj_weight, k_proj_weight=k_proj_weight,\n                v_proj_weight=v_proj_weight, static_k=static_k, static_v=static_v)\n    tgt_len, bsz, embed_dim = query.size()\n    assert embed_dim == embed_dim_to_check\n    assert key.size() == value.size()\n\n    head_dim = embed_dim // num_heads\n    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n    scaling = float(head_dim) ** -0.5\n\n    if not use_separate_proj_weight:\n        if torch.equal(query, key) and torch.equal(key, value):\n            # self-attention\n            q, k, v = linear(query, in_proj_weight, in_proj_bias).chunk(3, dim=-1)\n\n        elif torch.equal(key, value):\n            # encoder-decoder attention\n            # This is inline in_proj function with in_proj_weight and in_proj_bias\n            _b = in_proj_bias\n            _start = 0\n            _end = embed_dim\n            _w = in_proj_weight[_start:_end, :]\n            if _b is not None:\n                _b = _b[_start:_end]\n            q = linear(query, _w, _b)\n\n            if key is None:\n                assert value is None\n                k = None\n                v = None\n            else:\n\n                # This is inline in_proj function with in_proj_weight and in_proj_bias\n                _b = in_proj_bias\n                _start = embed_dim\n                _end = None\n                _w = in_proj_weight[_start:, :]\n                if _b is not None:\n                    _b = _b[_start:]\n                k, v = linear(key, _w, _b).chunk(2, dim=-1)\n"
},
{
    "Id": 35,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b8ab3080b1043a610ba2825a2be406a1833b1d70",
    "Violation": "improper",
    "Bug report": "If kernel sizes were specified via \"kernel_w\" and \"kernel_h\", tensor size inference was incorrect in InferShapesAndTypes(): it was checking for \"helper_w\" instead of \"kernel_w\".",
    "Number of deleted lines": 1,
    "Deleted lines": "    c.flops = (X_h - kernel_h + 1) * (X_w - kernel_w + 1) * kernel_w *\n        kernel_h * in_channels * out_channels * 2;\n    return c;\n  }\n\n  static vector<TensorShape> TensorInferenceForSchema(\n      const OperatorDef& def,\n      const vector<TensorShape>& in,\n      int output_channel) {\n    ArgumentHelper helper(def);\n    CAFFE_ENFORCE_GT(in.size(), 0);\n    CAFFE_ENFORCE_GT(in[0].dims_size(), 0);\n    int N = in[0].dims(0);\n    bool channel_first;\n\n    vector<int> pads = helper.GetRepeatedArgument<int>(\"pads\");\n    vector<int> kernel = helper.GetRepeatedArgument<int>(\"kernels\");\n    vector<int> strides = helper.GetRepeatedArgument<int>(\"strides\");\n    vector<int> dilations = helper.GetRepeatedArgument<int>(\"dilation\");\n\n    if (helper.HasArgument(\"pad\")) {\n      pads.resize(4, helper.GetSingleArgument<int>(\"pad\", 0));\n    } else if (\n        helper.HasArgument(\"pad_t\") && helper.HasArgument(\"pad_l\") &&\n        helper.HasArgument(\"pad_b\") && helper.HasArgument(\"pad_r\")) {\n      pads.push_back(helper.GetSingleArgument<int>(\"pad_t\", 0));\n      pads.push_back(helper.GetSingleArgument<int>(\"pad_l\", 0));\n      pads.push_back(helper.GetSingleArgument<int>(\"pad_b\", 0));\n      pads.push_back(helper.GetSingleArgument<int>(\"pad_r\", 0));\n    }\n\n    if (helper.HasArgument(\"kernel\")) {\n      kernel.resize(2, helper.GetSingleArgument<int>(\"kernel\", 1));\n    } else if (\n        helper.HasArgument(\"kernel_h\") && helper.HasArgument(\"helper_w\")) {\n      kernel.push_back(helper.GetSingleArgument<int>(\"kernel_h\", 1));\n      kernel.push_back(helper.GetSingleArgument<int>(\"kernel_w\", 1));\n    }\n\n    if (helper.HasArgument(\"stride\")) {\n      strides.resize(2, helper.GetSingleArgument<int>(\"stride\", 1));\n    } else if (\n        helper.HasArgument(\"stride_h\") && helper.HasArgument(\"stride_w\")) {\n      strides.push_back(helper.GetSingleArgument<int>(\"stride_h\", 1));\n      strides.push_back(helper.GetSingleArgument<int>(\"stride_w\", 1));\n    }\n\n    if (helper.HasArgument(\"dilation\")) {\n      strides.resize(2, helper.GetSingleArgument<int>(\"dilation\", 1));\n    } else if (\n        helper.HasArgument(\"dilation_h\") && helper.HasArgument(\"dilation_w\")) {\n      strides.push_back(helper.GetSingleArgument<int>(\"dilation_h\", 1));\n      strides.push_back(helper.GetSingleArgument<int>(\"dilation_w\", 1));\n    }\n\n    auto check_and_set_default_value = [](\n        vector<int>& vec, int size, int value) {\n      if (vec.size() == 0) {\n        vec.resize(size, value);\n      }\n    };\n\n    check_and_set_default_value(pads, 4, 0);\n    check_and_set_default_value(kernel, 2, 1);\n    check_and_set_default_value(strides, 2, 1);\n    check_and_set_default_value(dilations, 2, 1);\n\n    vector<int> output_dims;\n    ConvPoolOpBase<CPUContext>::InferOutputSize(\n        GetDimsVector(in[0]),\n        output_channel,"
},
{
    "Id": 36,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/be253b8ee8a104997773d11ed28928a48193217d",
    "Violation": "improper",
    "Bug report": "The existing check isn't safe for 32-bit `size_t` because the max 64-bit int will overflow.",
    "Number of deleted lines": 1,
    "Deleted lines": "  int64_t expected_stride = 1;\n  bool contig_if_nonempty = true;\n  for (int64_t i = dim - 1; i >= 0; i--) {\n    if (sizes[i] == 0) {\n      return true;\n    }\n    if (contig_if_nonempty) {\n      if (sizes[i] != 1 && strides[i] != expected_stride) {\n        contig_if_nonempty = false;\n      }\n      expected_stride *= sizes[i];\n    }\n  }\n  return contig_if_nonempty;\n}\n\nbool TensorGeometry::is_contiguous() const {\n  if (numel_ == 0) {\n    return true;\n  }\n  return at::geometry_is_contiguous(sizes_, strides_);\n}\n\n} // namespace at\n"
},
{
    "Id": 37,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/128dd6b1502ad3687ffb79484b72d4cfafec496e",
    "Violation": "insufficient",
    "Bug report": "Relax the check that makes sure number of outs equals number of returns. We are trying to add an out variant for an existing operator, Notice the out argument is a mutable list of tensors. Given the fact that we don't support mutable tensor list as a return type and it seems not useful to add such a return type. The solution I'm proposing is to relax the constraint that the number of outs needs to be the same as the number of returns, so we can return a `void`.",
    "Number of deleted lines": 3,
    "Deleted lines": "        ops, args = func_decl.split(\"(\", 1)\n        assert args[-1] == \")\", \"Expecting closing )\"\n        args = args[:-1]\n        name = OperatorName.parse(ops)\n        arguments = Arguments.parse(args)\n        returns = parse_returns(return_decl)\n        r = FunctionSchema(name=name, arguments=arguments, returns=returns)\n        assert str(r) == func, f\"{str(r)} != {func}\"\n        return r\n\n    def __post_init__(self) -> None:\n        for arg, ret in zip(self.arguments.out, self.returns):\n            assert arg.annotation == ret.annotation, (\n                \"Out arguments must have matching return Tensor; furthermore, \"\n                \"the ith-argument needs to correspond to the ith return\"\n            )\n        # Invariant: we expect out arguments to appear as keyword arguments in the schema.\n        # This means that all mutable returns should be aliased to a keyword argument\n        # (except for \"self\", which we explicitly don't treat as an out argument because of its use in methods)\n        # See Note [is_out_fn]\n        out_and_self = list(self.arguments.out) + [\n            arg for arg in self.arguments.flat_positional if arg.name == \"self\"\n        ]\n        mutable_returns = [\n            ret\n            for ret in self.returns\n            if ret.annotation is not None and ret.annotation.is_write\n        ]\n        for ret in mutable_returns:\n            assert any([ret.annotation == arg.annotation for arg in out_and_self]), (\n                'All mutable returns must be aliased either to a keyword argument, or to \"self\". '\n                \"Did you forget to mark an out argument as keyword-only?\"\n            )\n        if self.arguments.out:\n            assert len(self.arguments.out) == len(\n                self.returns\n            ), \"Must return as many arguments as there are out arguments\"\n        if self.name.name.inplace:\n            # TODO: fixme\n            if not is_foreach_op(str(self.name)):\n                assert len(self.returns) == 1\n\n    def is_out_fn(self) -> bool:\n        # Note [is_out_fn]\n        #\n        # out functions are the variants which take an explicit out= argument\n        # to populate into.  We need to know if a schema corresponds to an\n        # out function for several reasons:\n        #\n        #   - They codegen differently in C++ API\n        #       - codegen to at::add_out rather than at::add\n        #       - out argument is moved to front of C++ argument list\n        #\n        # out functions are DEFINED to be any function with a keyword-only\n        # argument that is mutable.  In principle, this could lead to a\n        # false positive if you define a function that mutates a\n        # kwarg only argument, but this isn't the \"true\" output of this\n        # function.  A more robust definition that would work in this\n        # case would also look at:\n        #\n        #   - The output types.  Out functions take in the arguments\n        #     they mutate and then return them again; this is sort\n        #     of \"definitionally\" what makes something an out function.\n        #     Historically, we DO check this for consistency.\n        #   - Correspondence with pure variant.  An out function\n        #     should have a signature equivalent to its pure variant,\n        #     but just with extra kwargs for the output elements.  This\n        #     is difficult to actually check for and historically\n        #     we only do this check in tools/\n        return bool(self.arguments.out)\n\n    def kind(self) -> SchemaKind:\n        \"\"\""
},
{
    "Id": 38,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9c3cb6e652e6033e355cef92229fde5b0baf271b",
    "Violation": "improper",
    "Bug report": "Fix stride checks in gemm dispatch.  lda: \"When transa = 'N' or 'n', then lda must be at least max(1, m), otherwise lda must be at least max(1, k). When transb = 'N' or 'n', then ldb must be at least max(1, k), otherwise ldb must be at least max(1, n).",
    "Number of deleted lines": 2,
    "Deleted lines": "}\n\nvoid THBlas_(gemm)(char transa, char transb, int64_t m, int64_t n, int64_t k, real alpha, real *a, int64_t lda, real *b, int64_t ldb, real beta, real *c, int64_t ldc)\n{\n  int transa_ = ((transa == 't') || (transa == 'T'));\n  int transb_ = ((transb == 't') || (transb == 'T'));\n\n  if(n == 1)\n    ldc = m;\n\n  if(transa_)\n  {\n    if(m == 1)\n      lda = k;\n  }\n  else\n  {\n    if(k == 1)\n      lda = m;\n  }\n\n  if(transb_)\n  {\n    if(k == 1)\n      ldb = n;\n  }\n  else\n  {\n    if(n == 1)\n      ldb = k;\n  }\n\n#if defined(USE_BLAS) && (defined(TH_REAL_IS_DOUBLE) || defined(TH_REAL_IS_FLOAT))\n  if( (m <= INT_MAX) && (n <= INT_MAX) && (k <= INT_MAX) &&\n      (lda >= THMax(1, (transa_ ? m : k))) && (lda <= INT_MAX) &&\n      (ldb >= THMax(1, (transb_ ? k : n))) && (ldb <= INT_MAX) &&\n      (ldc >= THMax(1, n)) && (ldc <= INT_MAX) )\n  {\n    int i_m = (int)m;\n    int i_n = (int)n;\n    int i_k = (int)k;\n    int i_lda = (int)lda;\n    int i_ldb = (int)ldb;\n    int i_ldc = (int)ldc;\n\n#if defined(TH_REAL_IS_DOUBLE)\n    dgemm_(&transa, &transb, &i_m, &i_n, &i_k, &alpha, a, &i_lda, b, &i_ldb, &beta, c, &i_ldc);\n#else\n    sgemm_(&transa, &transb, &i_m, &i_n, &i_k, &alpha, a, &i_lda, b, &i_ldb, &beta, c, &i_ldc);\n#endif\n    return;\n  }\n#endif\n  {\n    int64_t i, j, l;\n    if(!transa_ && !transb_)\n    {\n      real *a_ = a;\n      for(i = 0; i < m; i++)\n      {\n        real *b_ = b;\n        for(j = 0; j < n; j++)\n        {\n          real sum = 0;\n          for(l = 0; l < k; l++)\n            sum += a_[l*lda]*b_[l];\n          b_ += ldb;\n\t  if (beta == 0)\n\t    c[j*ldc+i] = alpha*sum;\n\t  else\n\t    c[j*ldc+i] = beta*c[j*ldc+i]+alpha*sum;\n        }"
},
{
    "Id": 39,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7f125bca1cd42ebd8e07c97f1bd1682dff5cf387",
    "Violation": "insufficient",
    "Bug report": "Add pin_memory check in empty_strided. Add the false checking if pin_memory has been specified to `False`",
    "Number of deleted lines": 1,
    "Deleted lines": "  if (src.device().type() == at::kCPU && dst.device().type() == at::kMetal) {\n    return copy_to_metal_(dst, src);\n  }\n  TORCH_INTERNAL_ASSERT(\n      src.device().type() == DeviceType::Metal,\n      \"metal_copy_ is implemented only for CPU,Strided,float->Metal; Metal->CPU,Strided,float\");\n  return dst;\n}\n\n#pragma mark - ATen Ops\n\nTensor empty(\n    IntArrayRef size,\n    const TensorOptions& options,\n    c10::optional<MemoryFormat> memory_format) {\n  TORCH_CHECK(\n      !options.has_pinned_memory(),\n      \"'pin_memory' argument is incompatible with Metal tensor\");\n  TORCH_CHECK(\n      !options.has_memory_format() && !memory_format,\n      \"'memory_format' argument is incompatible with Metal tensor\");\n  MetalTensor mt{size.vec()};\n  return MetalTensor::toTensor(\n      std::move(mt), at::device(at::kMetal).dtype(options.dtype()));\n};\n\nat::Tensor empty_strided(\n    IntArrayRef size,\n    IntArrayRef stride,\n    optional<ScalarType> dtype,\n    optional<Layout> layout,\n    optional<Device> device,\n    optional<bool> pin_memory) {\n  TORCH_CHECK(\n      !pin_memory.has_value(),\n      \"'pin_memory' argument is incompatible with Metal tensor\");\n  MetalTensor mt{size.vec(), stride.vec()};\n  return MetalTensor::toTensor(\n      std::move(mt), at::device(at::kMetal).dtype(dtype));\n}\n\nTensor addmm(\n    const Tensor& bias,\n    const Tensor& input,\n    const Tensor& weight,\n    Scalar beta,\n    Scalar alpha) {\n  TORCH_CHECK(input.is_metal());\n  TORCH_CHECK(input.dim() == 2 && weight.dim() == 2);\n  TORCH_CHECK(beta.toFloat() == 1.0f);\n  TORCH_CHECK(alpha.toFloat() == 1.0f);\n  auto&& sizes = weight.sizes();\n  at::Tensor transposedWeight = weight.t().contiguous();\n  at::Tensor mWeight =\n      transposedWeight.view({sizes[1], sizes[0], 1, 1}).contiguous();\n  return mpscnn::addmm(bias, input, mWeight);\n}\n\nTensor conv2d(\n    const Tensor& input,\n    const Tensor& weight,\n    const c10::optional<at::Tensor>& bias,\n    IntArrayRef stride,\n    IntArrayRef padding,\n    IntArrayRef dilation,\n    int64_t groups) {\n  TORCH_CHECK(input.is_metal());\n  Conv2DParams params{\n      input.sizes(), weight.sizes(), padding, stride, dilation, groups};\n  TORCH_INTERNAL_ASSERT(input.dim() == 4, \"Expected 4-dimensional input\");\n  TORCH_INTERNAL_ASSERT(weight.dim() == 4, \"Expected 4-dimensional weight\");"
},
{
    "Id": 40,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f02b7a9c36dd6182da694bc47a5c345285dfd951",
    "Violation": "improper",
    "Bug report": "don't error when unused fill value is zero. In the python version of `F.pad`, checking that the fill value was left as default was done by comparing against zero. So if someone does explicitly pass in a zero-value, then this `TORCH_CHECK` was an accidental BC-break.",
    "Number of deleted lines": 4,
    "Deleted lines": "  // Only in cases where padding values are > 0 are when additional copying\n  // is required.\n  for (const auto i : c10::irange(ndim)) {\n    const auto dim = ndim - i + 1;\n    const auto pad_l = padding[2*i + 0];\n    const auto pad_r = padding[2*i + 1];\n\n    if (pad_l > 0) {\n      out_slice = out.slice(dim, 0, pad_l);\n      in_slice = out.slice(dim,\n                           out_shape[dim] - pad_l - std::max(pad_r, zero),\n                           out_shape[dim] - std::max(pad_r, zero));\n      out_slice.copy_(in_slice);\n    }\n\n    if (pad_r > 0) {\n      out_slice = out.slice(dim, out_shape[dim] - pad_r, out_shape[dim]);\n      in_slice = out.slice(dim, std::max(pad_l, zero), std::max(pad_l, zero) + pad_r);\n      out_slice.copy_(in_slice);\n    }\n  }\n\n  return out;\n}\n\nTensor _pad_enum(const Tensor &self, IntArrayRef pad, int64_t mode_int, c10::optional<double> value) {\n  const auto input_dim = self.dim();\n  TORCH_CHECK(pad.size() % 2 == 0, \"Padding length must be divisible by 2\");\n  TORCH_CHECK(static_cast<int64_t>(pad.size()) <= input_dim * 2, \"Padding length too large\");\n  auto mode = static_cast<at::padding_mode>(mode_int);\n\n  if (mode == at::padding_mode::constant) {\n    return at::constant_pad_nd(self, pad, value.value_or(0.0));\n  }\n  TORCH_CHECK(\n      !value.has_value(), \"Padding mode \\\"\",\n      padding_mode_string(mode),\n      \"\\\" doesn't take in value argument\");\n\n  if (pad.size() == 2 && (input_dim == 2 || input_dim == 3)) {\n    switch (mode) {\n      case at::padding_mode::reflect: return at::reflection_pad1d(self, pad);\n      case at::padding_mode::replicate: return at::replication_pad1d(self, pad);\n      case at::padding_mode::circular: return at::_pad_circular(self, pad);\n      default: {}\n    }\n  } else if(pad.size() == 4 && (input_dim == 3 || input_dim == 4)) {\n    switch (mode) {\n      case at::padding_mode::reflect: return at::reflection_pad2d(self, pad);\n      case at::padding_mode::replicate: return at::replication_pad2d(self, pad);\n      case at::padding_mode::circular: return at::_pad_circular(self, pad);\n      default: {}\n    }\n  } else if (pad.size() == 6 && (input_dim == 4 || input_dim == 5)) {\n    switch (mode) {\n      case at::padding_mode::reflect: return at::reflection_pad3d(self, pad);\n      case at::padding_mode::replicate: return at::replication_pad3d(self, pad);\n      case at::padding_mode::circular: return at::_pad_circular(self, pad);\n      default: {}\n    }\n  }\n  C10_THROW_ERROR(NotImplementedError,\n      \"Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now\");\n}\n\nTensor pad(const Tensor &self, IntArrayRef pad, c10::string_view mode, c10::optional<double> value) {\n  const auto mode_enum = [&] {\n    if (mode == \"reflect\") {\n      return at::padding_mode::reflect;\n    } else if (mode == \"constant\") {\n      return at::padding_mode::constant;\n    } else if (mode == \"replicate\") {\n      return at::padding_mode::replicate;\n    } else if (mode == \"circular\") {"
},
{
    "Id": 41,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/75be4f9cdb503d6eff189b2bc5c05d96bff66653",
    "Violation": "insufficient",
    "Bug report": " check tensor has storage before refer to tensor data ptr. In the exporter dedupe initializers passes, check the tensor has storage before reference to tensor's data_ptr, otherwise it will result in a crash.",
    "Number of deleted lines": 1,
    "Deleted lines": "  std::vector<Value*> uniqueVals;\n  std::vector<size_t> inputsIndicesToRemove;\n  auto b = g->block();\n\n  for (auto i : c10::irange(b->inputs().size())) {\n    auto v = g->inputs().at(i);\n    if (valsToParamsMap.find(v) == valsToParamsMap.end()) {\n      // Skip model inputs\n      continue;\n    }\n    auto it = std::find_if(\n        uniqueVals.begin(), uniqueVals.end(), is_same_tensor_as(v));\n    if (it == uniqueVals.end()) {\n      uniqueVals.emplace_back(v);\n    } else {\n      inputsIndicesToRemove.emplace_back(i);\n      auto id_node = g->create(onnx::Identity);\n      id_node->insertAfter(g->block()->param_node());\n      id_node->addInput(*it);\n      id_node->output()->copyMetadata(v);\n      id_node->copyMetadata(g->block()->param_node());\n      v->replaceAllUsesWith(id_node->output());\n    }\n  }\n  for (auto it = inputsIndicesToRemove.rbegin();\n       it != inputsIndicesToRemove.rend();\n       ++it) {\n    valsToParamsMap.erase(g->inputs().at(*it));\n    g->eraseInput(*it);\n  }\n}\n\nbool DeduplicateInitializersByDataPtr(at::Tensor& t1, at::Tensor& t2) {\n  return t1.sizes().equals(t2.sizes()) && t1.strides().equals(t2.strides()) &&\n      (t1.data_ptr() == t2.data_ptr());\n}\n\nbool DeduplicateInitializersByValue(at::Tensor& t1, at::Tensor& t2) {\n  if (t1.dtype() != t2.dtype() || !t1.sizes().equals(t2.sizes()) ||\n      !t1.strides().equals(t2.strides())) {\n    return false;\n  }\n\n  if (t1.device() != t2.device()) {\n    return t1.to(\"cpu\").equal(t2.to(\"cpu\"));\n  }\n\n  return t1.equal(t2);\n}\n\nvoid DeduplicateInitializers(\n    std::shared_ptr<Graph>& g,\n    std::map<std::string, IValue>& paramsDict,\n    bool is_train) {\n  auto valsToParamsMap = buildValueToParamsMap(g->block(), paramsDict);\n  // ONNX spec does not support parameters with shared memory.\n  // This pass de-duplicate those parameters. Training is not affected.\n  DeduplicateInitializers(g, valsToParamsMap, DeduplicateInitializersByDataPtr);\n  if (!is_train) {\n    // More aggressive parameters de-duplication based on tensor values.\n    // Producing more compact model for inference.\n    // For training, this pass is disabled,\n    // because parameters may be updated differently.\n    DeduplicateInitializers(g, valsToParamsMap, DeduplicateInitializersByValue);\n  }\n  buildParamsMapFromValueToParamsMap(valsToParamsMap, paramsDict);\n}\n\n} // namespace jit\n} // namespace torch\n"
},
{
    "Id": 42,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a3701b674046bcefb5927a6643364b186f77dbcf",
    "Violation": "unnecessary",
    "Bug report": "fix backward bug for custom device. In the backward on some device , it may get an error to get device index because of exchange a new thread. So just set_device and check the device index in `setDevice`  func may be better for some many kinds of devices. For CUDA, the device index check is also included in `setDevice`  func",
    "Number of deleted lines": 2,
    "Deleted lines": "\nstatic variable_list call_tensor_pre_hooks(Node& fn, variable_list inputs) {\n  for (const auto& hook : fn.tensor_pre_hooks()) {\n    inputs = (*hook)(inputs);\n  }\n  for (const auto& pair : fn.retains_grad_hooks()) {\n    inputs = (*pair.second)(inputs);\n  }\n  return inputs;\n}\n\nstatic variable_list call_post_hooks(\n    Node& fn,\n    variable_list outputs,\n    const variable_list& inputs) {\n  for (const auto& hook : fn.post_hooks()) {\n    outputs = (*hook)(outputs, inputs);\n  }\n  return outputs;\n}\n\nvoid set_device(int device) {\n  // NB: We MUST NOT construct the guard for device CPU,\n  // as in some settings we compile with cuda, but\n  // have lazy stubs for CUDA functionality (so actually\n  // attempting to setup a guard(CPU_DEVICE) will cause an\n  // error, because it will still query cudaGetDevice).\n  //\n  // Don't use DeviceGuard here because its destructor may be called before the\n  // device is reset. This is fine because the device is thread local.\n  if (device != CPU_DEVICE) {\n    for (const auto i : c10::irange(static_cast<size_t>(\n             c10::DeviceType::COMPILE_TIME_MAX_DEVICE_TYPES))) {\n      auto* impl = c10::impl::device_guard_impl_registry[i].load();\n      if (impl && device < impl->deviceCount() &&\n          impl->getDevice().index() != device) {\n        impl->setDevice(at::Device(static_cast<c10::DeviceType>(i), device));\n      }\n    }\n  }\n  worker_device = device;\n}\n\nvoid validate_outputs(\n    const edge_list& edges,\n    variable_list& grads,\n    const std::function<std::string(const std::string&)>& format_error) {\n  if (grads.size() != edges.size()) {\n    std::stringstream ss;\n    ss << \"invalid number of gradients - expected \";\n    ss << edges.size() << \", but got \" << grads.size();\n    AT_ERROR(format_error(ss.str()));\n  }\n  for (const auto i : c10::irange(grads.size())) {\n    const auto& edge = edges[i];\n    if (!edge.is_valid())\n      continue;\n\n    const auto& metadata = edge.function->input_metadata(edge.input_nr);\n    auto& grad = grads[i];\n    if (!grad.defined()) {\n      // FIXME: TestJit.test_ge_optimized fails this assertion.\n      // std::stringstream ss;\n      // ss << \"undefined gradient at index \" << i;\n      // AT_ERROR(format_error(ss.str()));\n      continue;\n    }\n\n    if (!metadata.is_same_shape(grad)) {\n      if (metadata.is_expandable_to_shape(grad)) {\n        grad = metadata.reduce_grad(grad);\n      } else {"
},
{
    "Id": 43,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6f5945e4bb1258d39a2878a08a910fcc8f659d5e",
    "Violation": "improper",
    "Bug report": "triton supports devices < 7.0, not 6.0. triton is still buggy with Pascal devices, so make the error checker reflect that. Also, this < 6.0 never worked, as the `has_triton` definition in utils.py was checking >= 7.0.",
    "Number of deleted lines": 2,
    "Deleted lines": "    def flush(self):\n        for backend in self.backends.values():\n            backend.flush()\n        self.free_buffers()\n\n    def codegen_extern_call(self, scheduler_node: ExternKernelSchedulerNode):\n        assert isinstance(scheduler_node, ExternKernelSchedulerNode)\n        scheduler_node.allocate()\n        node = scheduler_node.node\n        node.codegen(V.graph.wrapper_code)\n        self.free_buffers()\n\n    def codegen_template_call(\n        self, scheduler_node: Union[FusedSchedulerNode, TemplateSchedulerNode]\n    ):\n        from .codegen.triton_template import template_codegen\n\n        node, *epilogue = scheduler_node.get_nodes()\n        node.allocate()\n        template_codegen(self, node, epilogue)\n        self.free_buffers()\n\n    def create_backend(self, device: torch.device):\n        assert (\n            device.type != \"cuda\" or device.index is not None\n        ), f\"{device} should have been normalized in lowering\"\n        V.graph.device_types.add(device.type)\n        if device.type == \"cpu\":\n            from .codegen.cpp import CppScheduling\n\n            return CppScheduling(self)\n        else:\n            if not has_triton():\n                device_props = torch.cuda.get_device_properties(device)\n                if device_props.major < 6:\n                    raise RuntimeError(\n                        f\"Found {device_props.name} which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 6.0, but your device is of CUDA capability {device_props.major}.{device_props.minor}\"  # noqa: B950\n                    )\n                else:\n                    raise RuntimeError(\n                        \"Cannot find a working triton installation. More information on installing Triton can be found at https://github.com/openai/triton\"  # noqa: B950\n                    )\n            from .codegen.triton import TritonScheduling\n\n            return TritonScheduling(self)\n\n    def get_backend(self, device: torch.device):\n        if device not in self.backends:\n            self.backends[device] = self.create_backend(device)\n        return self.backends[device]\n\n    @dynamo_utils.dynamo_timed\n    def codegen(self):\n        for node in self.nodes:\n            self.buffer_names_no_longer_needed.update(node.last_usage)\n\n            if not isinstance(node, NopKernelSchedulerNode):\n                device = node.get_device()\n                if (\n                    device != self.current_device\n                    or node.is_extern()\n                    or node.is_template()\n                ):\n                    self.flush()\n                    self.current_device = device\n\n            self.buffer_names_to_free.update(node.last_usage)\n\n            if node.is_template():\n                self.codegen_template_call(node)\n            elif node.is_extern():\n                self.codegen_extern_call(node)\n            elif isinstance(node, (FusedSchedulerNode, SchedulerNode)):"
},
{
    "Id": 44,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/871b5caae76185cff141c522b3133e7543c8dabf",
    "Violation": "improper",
    "Bug report": "Fix hpu deserialization bug. fix hpu deserialization bug. It should check hpu model if and only if location start with hpu. Otherwise, it always raise an AssertError if hpu is not imported. This break the serialization/desirialization functionality abourt other third-party like IPEX. only assert hpu model when start with hpu",
    "Number of deleted lines": 2,
    "Deleted lines": "    return device\n\n\ndef _cuda_deserialize(obj, location):\n    if location.startswith('cuda'):\n        device = validate_cuda_device(location)\n        if getattr(obj, \"_torch_load_uninitialized\", False):\n            with torch.cuda.device(device):\n                return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n        else:\n            return obj.cuda(device)\n\n\ndef validate_hpu_device(location):\n    hpu = getattr(torch, \"hpu\", None)\n    assert hpu is not None, \"HPU device module is not loaded\"\n    device = hpu._utils._get_device_index(location, optional=True)\n\n    if not hpu.is_available():\n        raise RuntimeError('Attempting to deserialize object on a HPU '\n                           'device but torch.hpu.is_available() is False. '\n                           'If you are running on a CPU-only machine, '\n                           'please use torch.load with map_location=torch.device(\\'cpu\\') '\n                           'to map your storages to the CPU.')\n    device_count = hpu.device_count()\n    if device >= device_count:\n        raise RuntimeError('Attempting to deserialize object on HPU device '\n                           f'{device} but torch.hpu.device_count() is {device_count}. Please use '\n                           'torch.load with map_location to map your storages '\n                           'to an existing device.')\n    return device\n\n\ndef _hpu_deserialize(obj, location):\n    hpu = getattr(torch, \"hpu\", None)\n    assert hpu is not None, \"HPU device module is not loaded\"\n    if location.startswith('hpu'):\n        device = validate_hpu_device(location)\n        if getattr(obj, \"_torch_load_uninitialized\", False):\n            with hpu.device(device):\n                return torch.UntypedStorage(obj.nbytes(), device=torch.device(location))\n        else:\n            return obj.hpu(device)\n\n\ndef _mps_deserialize(obj, location):\n    if location.startswith('mps'):\n        return obj.mps()\n\n\ndef _meta_deserialize(obj, location):\n    if location == 'meta':\n        return torch.UntypedStorage(obj.nbytes(), device='meta')\n\n\ndef _validate_privateuse1_device(location, backend_name):\n    device = torch.device(location)\n    device_index = device.index if device.index else 0\n    if not hasattr(torch, backend_name):\n        raise RuntimeError(f'The {backend_name.upper()} device module is not registered. '\n                           'If you are running on a CPU-only machine, '\n                           'please use torch.load with map_location=torch.device(\\'cpu\\') '\n                           'to map your storages to the CPU.')\n    device_module = getattr(torch, backend_name)\n    if hasattr(device_module, 'is_available') and not device_module.is_available():\n        raise RuntimeError(f'Attempting to deserialize object on a {backend_name.upper()} '\n                           f'device but torch.{backend_name}.is_available() is False. '\n                           'If you are running on a CPU-only machine, '\n                           'please use torch.load with map_location=torch.device(\\'cpu\\') '\n                           'to map your storages to the CPU.')\n    if hasattr(device_module, 'device_count'):\n        device_count = device_module.device_count()\n        if device_index >= device_count:"
},
{
    "Id": 45,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6592259ea52f45e1fc9a633ccb5b154ba5099334",
    "Violation": "insufficient",
    "Bug report": "As per torch.jit.load documentation, all previously saved modules, irrespective of their device, are first loaded onto CPU, and then are moved to the devices they were saved from. So far, supported devices included CPU and CUDA only. To enable torch.jit.load for HPU, additional check for HPU is introduced.",
    "Number of deleted lines": 2,
    "Deleted lines": "        at::DataPtr storage_ptr;\n        if (numel > 0) {\n          // If there are no elements in the tensor, there's no point in\n          // reading a zero (0) byte file from the input stream and paying\n          // that cost.\n          storage_ptr = read_record_(key);\n        }\n\n        storage = at::Storage(\n            c10::Storage::use_byte_size_t(),\n            numel * dtype.itemsize(),\n            std::move(storage_ptr),\n            /*allocator=*/nullptr,\n            /*resizable=*/false); // NB: we didn't set any allocator for the\n                                  // tensor\n        if (storage_context_ != nullptr) {\n          storage_context_->addStorage(key, storage);\n        }\n      }\n\n      auto options = at::CPU(type).options();\n      if (use_storage_device_) {\n        options = options.device(storage.device());\n        device = storage.device();\n      }\n\n      at::Tensor tensor;\n      if (options.backend() == c10::Backend::QuantizedCPU) {\n        tensor = at::_empty_affine_quantized({}, options, 0, 0)\n                     .set_(storage, 0, {}, {});\n      } else {\n        tensor = at::empty({0}, options).set_(storage);\n      }\n\n      if (device.is_cuda() || device.is_xpu() || device.is_meta()) {\n        tensor = tensor.to(device, tensor.scalar_type());\n      } else if (device.type() != DeviceType::CPU) {\n        AT_ERROR(\n            \"supported devices include CPU and CUDA, however got \",\n            DeviceTypeName(device.type(), false));\n      }\n      stack_.emplace_back(std::move(tensor));\n    } break;\n    default: {\n      AT_ERROR(\n          \"Unknown opcode for unpickling at \",\n          reinterpret_cast<void*>(opcode),\n          \": \",\n          int(static_cast<uint8_t>(opcode)));\n    } break;\n  }\n  return opcode;\n}\n\nvoid Unpickler::readGlobal(\n    const std::string& module_name,\n    const std::string& class_name) {\n  // TODO [unpickler refactor] __main__ isn't used by the pickler anymore, this\n  // is only here for bc-compatibility reasons\n  if (module_name == \"__main__\") {\n    if (class_name == \"TensorID\") {\n      globals_.emplace_back([this] {\n        auto setitem_data = stack_.back();\n        stack_.pop_back();\n        TORCH_INTERNAL_ASSERT(\n            !tensor_table_.empty(),\n            \"Pickler tried to write a tensor but had no tensor table to write to\");\n        stack_.emplace_back(tensor_table_.at(setitem_data.toInt()));\n      });\n    } else if (class_name == \"IntList\") {\n      globals_.emplace_back([this] {\n        stack_.back().toList().unsafeSetElementType(IntType::get());\n      });\n    } else {\n      AT_ERROR(\"Unknown pickler class id\", class_name);"
},
{
    "Id": 46,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1becd2c314f45bded8d3fbec91d785e7190b4afe",
    "Violation": "insufficient",
    "Bug report": "Align checks in _use_cudnn_ctc_loss with those in _cudnn_ctc_loss.This PR is intended to fix the following problem: When using `CTCLoss`, there is a cudnn path gated by a call to [`_use_cudnn_ctc_loss`] which checks some conditions. However, there are more checks in `_cudnn_ctc_loss`.  some of which are not present in `_use_cudnn_ctc_loss` (e.g. the check that `targets` is on CPU which will cause a RuntimeError after dispatching to `_cudnn_ctc_loss`). Instead, these checks should be in `_use_cudnn_ctc_loss` so that the normal `_ctc_loss` path will be used if the checks are not met)",
    "Number of deleted lines": 1,
    "Deleted lines": "    const Tensor& log_probs,\n    const Tensor& targets,\n    const Tensor& input_lengths,\n    const Tensor& target_lengths,\n    int64_t BLANK,\n    bool deterministic,\n    bool zero_infinity) {\n  AT_ERROR(\"cudnn_ctc_loss: ATen not compiled with cuDNN >= 7 support\");\n}\n\n}}\n\n#else // AT_CUDNN_ENABLED\n\n#include <ATen/cudnn/Descriptors.h>\n#include <ATen/cudnn/Types.h>\n#include <ATen/cudnn/Utils.h>\n\n#include <ATen/TensorUtils.h>\n#include <c10/util/irange.h>\n\nnamespace at { namespace native {\n\nbool _use_cudnn_ctc_loss(\n    const Tensor& log_probs,\n    const Tensor& targets,\n    IntArrayRef input_lengths,\n    IntArrayRef target_lengths,\n    int64_t BLANK) {\n  auto& ctx = at::globalContext();\n\n  bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n      (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n      (targets.scalar_type() == at::kInt) &&\n      (log_probs.device().type() == at::kCUDA);\n\n  if (use_cudnn) {\n    // we don't know that input_lengths and target_lengths have the same size\n    // (they should, but we didn't check yet)\n    int64_t max_input_length = log_probs.size(0);\n    for (const auto input_length : input_lengths) {\n      use_cudnn = use_cudnn && ((input_length == max_input_length) ? 1 : 0);\n    }\n    for (const auto b : c10::irange(target_lengths.size())) {\n      // target length < 256 is documented, but we see illegal memory accesses\n      // when target lengths > input lengths for CuDNN\n      use_cudnn =\n          use_cudnn && (target_lengths[b] < 256) && (target_lengths[b] <= input_lengths[b]);\n    }\n  }\n  return use_cudnn;\n}\n\nbool _use_cudnn_ctc_loss_tensor(\n    const Tensor& log_probs,\n    const Tensor& targets,\n    const Tensor& input_lengths,\n    const Tensor& target_lengths,\n    int64_t BLANK) {\n  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();\n  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());\n  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());\n  return at::_use_cudnn_ctc_loss(\n      log_probs, targets, il, tl, BLANK);\n}\n\nstd::tuple<Tensor, Tensor> _cudnn_ctc_loss(const Tensor& log_probs_t, const Tensor& targets_t, IntArrayRef input_lengths_, IntArrayRef target_lengths_, int64_t BLANK, bool deterministic, bool zero_infinity) {\n  (void)zero_infinity; // only used for backward\n  const CheckedFrom c = \"cudnn_ctc_loss\";\n  const TensorArg log_probs { log_probs_t, \"log_probs\", 1 };"
},
{
    "Id": 47,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a076a74f1118da171cf70d00d1de4abbe27cf85d",
    "Violation": "insufficient",
    "Bug report": " Add xpu device in assertion for nested tensor creation",
    "Number of deleted lines": 2,
    "Deleted lines": "  int64_t *offsets_ptr = offsets.mutable_data_ptr<int64_t>();\n  // nesting scalars has easy offsets\n  if (orig_dim == 0) {\n    std::iota(offsets_ptr, offsets_ptr + ntensors, 0);\n    return offsets;\n  }\n  const int64_t* sizes_ptr = sizes.data_ptr<int64_t>();\n  offsets_ptr[0] = 0;\n  for (const auto i : c10::irange(ntensors - 1)) {\n    const int64_t row_product = std::accumulate(sizes_ptr, sizes_ptr + orig_dim, 1, std::multiplies<int64_t>());\n    offsets_ptr[i + 1] = offsets_ptr[i] + row_product;\n    sizes_ptr += orig_dim;\n  }\n  return offsets;\n}\n\nNestedTensorImpl::NestedTensorImpl(\n    Storage storage,\n    c10::DispatchKeySet key_set,\n    const caffe2::TypeMeta data_type,\n    at::Tensor nested_sizes,\n    at::Tensor nested_strides,\n    at::Tensor storage_offsets)\n    : TensorImpl(std::move(storage), key_set, data_type),\n      nested_sizes_(std::move(nested_sizes)),\n      nested_strides_(std::move(nested_strides)),\n      storage_offsets_(std::move(storage_offsets)),\n      opt_sizes_(c10::nullopt) {\n  C10_LOG_API_USAGE_ONCE(\"torch.NestedTensor\");\n  TORCH_WARN_ONCE(\n      \"The PyTorch API of nested tensors is in prototype stage and will change \"\n      \"in the near future.\");\n  auto storage_device = storage_.device();\n  TORCH_INTERNAL_ASSERT(\n      storage_device.is_cpu() || storage_device.is_cuda() || storage_device.is_privateuseone(),\n      \"NestedTensorImpl storage must be either CUDA, CPU or \", get_privateuse1_backend(), \" but got \",\n      storage_device);\n  validate_nested_tensor_metadata(nested_sizes_, nested_strides_, storage_offsets_);\n  refresh_dim();\n  set_custom_sizes_strides(c10::TensorImpl::SizesStridesPolicy::CustomSizes);\n}\n\nNestedTensorImpl::NestedTensorImpl(\n    at::Tensor buffer,\n    at::Tensor nested_sizes,\n    at::Tensor nested_strides,\n    at::Tensor storage_offsets)\n    : NestedTensorImpl(\n          buffer.storage(),\n          generate_nested_key_set_from_buffer(buffer),\n          buffer.dtype(),\n          nested_sizes,\n          nested_strides,\n          storage_offsets) {\n\n  TORCH_INTERNAL_ASSERT(\n      buffer.dim() == 1,\n      \"NestedTensorImpl buffer is required to be 1 dimensional but got a buffer with \",\n      buffer.dim(),\n      \" dimensions.\");\n}\n\n// assume contiguous, `nested_strides` and `offsets`\n// can be infered from `nested_sizes`\nNestedTensorImpl::NestedTensorImpl(\n    at::Tensor buffer,\n    at::Tensor nested_sizes)\n    : NestedTensorImpl(\n          buffer,\n          nested_sizes,\n          construct_nested_strides(nested_sizes),\n          construct_offsets(nested_sizes))"
},
{
    "Id": 48,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/097defb1608827d82b18b27adeec0a98b72a9281",
    "Violation": "insufficient",
    "Bug report": "only check when world size > num_devices per host",
    "Number of deleted lines": 1,
    "Deleted lines": "        self.mesh = (\n            mesh.detach()\n            if isinstance(mesh, torch.Tensor)\n            else torch.tensor(mesh, dtype=torch.int)\n        )\n        self.mesh_dim_names = mesh_dim_names\n\n        # private field to pre-generate DeviceMesh's hash\n        self._flatten_mesh_list = tuple(self.mesh.flatten().tolist())\n        self._hash = hash((self._flatten_mesh_list, self.mesh.shape))\n        # always try to create default (world) pg, even if it is not initialized\n        # already. The world pg is used for device mesh identity (rank) on each\n        # process (we need to know if the current global rank is in the mesh or not)\n        self._get_or_create_default_group()\n        if _init_process_groups:\n            self._init_process_groups(_validate_mesh)\n\n    def _get_or_create_default_group(self):\n        default_initialized = is_initialized()\n        if not default_initialized:\n            init_process_group()\n\n        world_size = get_world_size()\n        if self.mesh.numel() > world_size:\n            raise RuntimeError(\n                f\"Mesh should not be bigger than default world size, but found {self.mesh.numel()} ranks!\"\n            )\n\n        device_handle = _get_device_handle(self.device_type)\n        # TODO: if user want to pass pg_options, offer a way to do it\n        if not default_initialized and device_handle:\n            # automatically set the current cuda/cuda-like device base on num of gpu devices available in each host\n            # NOTE: This device selection would only work for homogeneous hardware.\n            num_devices_per_host = device_handle.device_count()\n            if world_size % num_devices_per_host != 0:\n                raise RuntimeError(\n                    f\"DeviceMesh only support homogeneous hardware, but found \"\n                    f\"{world_size} ranks and {num_devices_per_host} {self.device_type} devices!\"\n                )\n            device_handle.set_device(get_rank() % num_devices_per_host)\n\n        # calculate the coordinates of the current global rank on the mesh\n        rank_coords = (self.mesh == get_rank()).nonzero()\n        assert rank_coords.size(0) in (0, 1)\n        self._coordinate_on_dim: Optional[List[int]] = (\n            rank_coords[0].tolist() if rank_coords.size(0) > 0 else None\n        )\n        return _get_default_group()\n\n    def _validate_mesh(self):\n        # check mesh tensor validity\n        unique_mesh_values = self.mesh.unique(sorted=True)\n        if unique_mesh_values.numel() != self.mesh.numel():\n            raise RuntimeError(\n                f\"DeviceMesh cannot have duplicate values, but found {self.mesh.tolist()}\"\n            )\n\n        # validate that all calling ranks pass in the same `mesh` argument.\n        self_mesh = self.mesh.to(self.device_type).contiguous()\n        mesh_tensor = funcol.all_gather_tensor(\n            self_mesh, gather_dim=0, group=_get_default_group()\n        )\n        mesh_tensor_chunked = torch.chunk(mesh_tensor, get_world_size())\n        for other_rank, other_mesh in enumerate(mesh_tensor_chunked):\n            if not torch.equal(self_mesh, other_mesh):\n                raise RuntimeError(\n                    f\"DeviceMesh initialization does not allow different mesh argument:\"\n                    f\"rank {get_rank()} has mesh {self_mesh} while rank {other_rank}\"\n                    f\"has mesh {other_mesh}!\"\n                )\n"
},
{
    "Id": 49,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/57af1ec14594a73c8f2b73bf70c04ba7efeb6eab",
    "Violation": "improper",
    "Bug report": "observers: use torch.all to check for valid min and max values. Using `torch.all` instead of `torch.sum` and length check. It's unclear whether the increase in perf (~5% for small inputs) is real, but should be a net benefit, especially for larger channel inputs.",
    "Number of deleted lines": 1,
    "Deleted lines": "                    quant_min, quant_max = -128, 127\n            else:\n                if self.reduce_range:\n                    quant_min, quant_max = 0, 127\n                else:\n                    quant_min, quant_max = 0, 255\n        return quant_min, quant_max\n\n    @torch.jit.export\n    def _calculate_qparams(self, min_val, max_val):\n        # type: (Tensor, Tensor) -> Tuple[Tensor, Tensor]\n        r\"\"\"Calculates the quantization parameters, given min and max\n        value tensors. Works for both per tensor and per channel cases\n\n        Args:\n            min_val: Minimum values per channel\n            max_val: Maximum values per channel\n\n        Returns:\n            scales: Scales tensor of shape (#channels,)\n            zero_points: Zero points tensor of shape (#channels,)\n        \"\"\"\n        if min_val.numel() == 0 or max_val.numel() == 0:\n            warnings.warn(\n                \"must run observer before calling calculate_qparams.\\\n                                    Returning default scale and zero point \"\n            )\n            return torch.tensor([1.0]), torch.tensor([0])\n\n        if min_val.dim() == 0 or max_val.dim() == 0:\n            assert min_val <= max_val, \"min {} should be less than max {}\".format(\n                min_val, max_val\n            )\n        else:\n            assert torch.sum(min_val <= max_val) == len(min_val), \"min {} should be less than max {}\".format(\n                min_val, max_val\n            )\n\n        quant_min, quant_max = self._calculate_qmin_qmax()\n        min_val_neg = torch.min(min_val, torch.zeros_like(min_val))\n        max_val_pos = torch.max(max_val, torch.zeros_like(max_val))\n\n        scale = torch.ones(min_val_neg.size(), dtype=torch.float32)\n        zero_point = torch.zeros(min_val_neg.size(), dtype=torch.int64)\n        device = 'cuda' if min_val_neg.is_cuda else 'cpu'\n\n        if self.qscheme == torch.per_tensor_symmetric or self.qscheme == torch.per_channel_symmetric:\n            max_val_pos = torch.max(-min_val_neg, max_val_pos)\n            scale = max_val_pos / (float(quant_max - quant_min) / 2)\n            scale = torch.max(scale, self.eps)\n            if self.dtype == torch.quint8:\n                if self.has_customized_qrange:\n                    # When customized quantization range is used, down-rounded midpoint of the range is chosen.\n                    zero_point = zero_point.new_full(zero_point.size(), (quant_min + quant_max) // 2)\n                else:\n                    zero_point = zero_point.new_full(zero_point.size(), 128)\n        elif self.qscheme == torch.per_channel_affine_float_qparams:\n            scale = (max_val - min_val) / float(quant_max - quant_min)\n            scale = torch.where(scale > self.eps, scale, torch.ones_like(scale))\n            # We use the quantize function\n            # xq = Round(Xf * inv_scale + zero_point),\n            # setting zero_point to (-1 * min *inv_scale) we get\n            # Xq = Round((Xf - min) * inv_scale)\n            zero_point = -1 * min_val / scale\n        else:\n            scale = (max_val_pos - min_val_neg) / float(quant_max - quant_min)\n            scale = torch.max(scale, self.eps)\n            zero_point = quant_min - torch.round(min_val_neg / scale)\n            zero_point = torch.clamp(zero_point, quant_min, quant_max)\n\n        # For scalar values, cast them to Tensors of size 1 to keep the shape"
},
{
    "Id": 50,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/07f0f383fa23e63eca164036ab58ab983e9437eb",
    "Violation": "unnecessary",
    "Bug report": "update tensor-like to check instance for torch function impl. tensor like should check the instance for a torch function impl, not the type",
    "Number of deleted lines": 1,
    "Deleted lines": "def is_tensor_like(inp):\n    \"\"\"\n    Returns ``True`` if the passed-in input is a Tensor-like.\n\n    Currently, this occurs whenever there's a ``__torch_function__``\n    attribute on the type of the input.\n\n    Examples\n    --------\n    A subclass of tensor is generally a Tensor-like.\n\n    >>> class SubTensor(torch.Tensor): ...\n    >>> is_tensor_like(SubTensor([0]))\n    True\n\n    Built-in or user types aren't usually Tensor-like.\n\n    >>> is_tensor_like(6)\n    False\n    >>> is_tensor_like(None)\n    False\n    >>> class NotATensor: ...\n    >>> is_tensor_like(NotATensor())\n    False\n\n    But, they can be made Tensor-like by implementing __torch_function__.\n\n    >>> class TensorLike:\n    ...     @classmethod\n    ...     def __torch_function__(cls, func, types, args, kwargs):\n    ...         return -1\n    >>> is_tensor_like(TensorLike())\n    True\n    \"\"\"\n    return type(inp) is torch.Tensor or hasattr(type(inp), \"__torch_function__\")\n\nclass TorchFunctionMode:\n    \"\"\"\n    A ``TorchFunctionMode`` allows you to override the meaning of all\n    ``__torch_function__`` overrideable functions within a dynamic scope,\n    without having to actually create a tensor subclass or manually\n    monkey-patch functions in the PyTorch API.  Some common situations\n    where you should use a mode:\n\n        * You want to override the meaning of factory functions, or other\n          functions that do not otherwise take a tensor as an argument\n          (these cannot be overridden with tensor subclasses).\n\n        * You want to override the behavior of all functions without needing\n          to wrap your inputs in tensor subclasses; e.g., if you are just\n          interested in logging intermediate computations.\n\n        * You want to control the order of execution of various tensor\n          subclasses explicitly, rather than implicitly via the return of\n          ``NotImplemented``.\n\n    Independent subclasses of :class:`TorchFunctionMode` are compositional:\n    modes can be pushed onto a stack using ``with MyMode():``.\n    When you call functions in the PyTorch API inside your\n    ``__torch_function__`` implementation, by default, they will forward on to\n    the next mode on the mode stack.  If you want recursively call back into\n    your current ``__torch_function__`` implementation, either explicitly\n    invoke ``self.__torch_function__(...)``, or use the context manager\n    ``enable_torch_function_mode(self, replace=self.inner)`` to make PyTorch\n    API self-referential (beware of infinite loops, in this case!)\n    \"\"\"\n    inner: \"TorchFunctionMode\"\n\n    # Force metaclass to generate constructor at the base of the hierarchy\n    def __init__(self):\n        pass"
},
{
    "Id": 51,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b90db4a78f8d760377a81a5a64d03ab4b67599de",
    "Violation": "improper",
    "Bug report": "Fix type checking to accept both Iter and Map DataPipe",
    "Number of deleted lines": 1,
    "Deleted lines": "            return _utils.fetch._MapDatasetFetcher(dataset, auto_collation, collate_fn, drop_last)\n        else:\n            return _utils.fetch._IterableDatasetFetcher(dataset, auto_collation, collate_fn, drop_last)\n\n\nclass _InfiniteConstantSampler(Sampler):\n    r\"\"\"Analogous to ``itertools.repeat(None, None)``.\n    Used as sampler for :class:`~torch.utils.data.IterableDataset`.\n\n    Args:\n        data_source (Dataset): dataset to sample from\n    \"\"\"\n\n    def __init__(self):\n        super(_InfiniteConstantSampler, self).__init__(None)\n\n    def __iter__(self):\n        while True:\n            yield None\n\n\ndef _get_distributed_settings():\n    if dist.is_available() and dist.is_initialized():\n        return dist.get_world_size(), dist.get_rank()\n    else:\n        return 1, 0\n\n\ndef _sharding_worker_init_fn(worker_init_fn, world_size, rank_id, worker_id):\n    global_worker_id = worker_id\n    info = torch.utils.data.get_worker_info()\n    assert info is not None\n    total_workers = info.num_workers\n    datapipe = info.dataset\n    assert isinstance(datapipe, IterDataPipe)\n    # To distribute elements across distributed process evenly, we should shard data on distributed\n    # processes first then shard on worker processes\n    total_workers *= world_size\n    global_worker_id = global_worker_id * world_size + rank_id\n    torch.utils.data.graph_settings.apply_sharding(datapipe, total_workers, global_worker_id)\n    if worker_init_fn is not None:\n        worker_init_fn(worker_id)\n\n\ndef _share_dist_seed(generator, pg):\n    _shared_seed = torch.empty((), dtype=torch.int64).random_(generator=generator)\n    if isinstance(pg, dist.ProcessGroup):\n        dist.broadcast(_shared_seed, src=0, group=pg)\n    return _shared_seed.item()\n\n\nclass DataLoader(Generic[T_co]):\n    r\"\"\"\n    Data loader. Combines a dataset and a sampler, and provides an iterable over\n    the given dataset.\n\n    The :class:`~torch.utils.data.DataLoader` supports both map-style and\n    iterable-style datasets with single- or multi-process loading, customizing\n    loading order and optional automatic batching (collation) and memory pinning.\n\n    See :py:mod:`torch.utils.data` documentation page for more details.\n\n    Args:\n        dataset (Dataset): dataset from which to load the data.\n        batch_size (int, optional): how many samples per batch to load\n            (default: ``1``).\n        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n            at every epoch (default: ``False``).\n        sampler (Sampler or Iterable, optional): defines the strategy to draw\n            samples from the dataset. Can be any ``Iterable`` with ``__len__``\n            implemented. If specified, :attr:`shuffle` must not be specified."
},
{
    "Id": 52,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/63cbdc92a750a667ffdcfbdac563d02db6fd9559",
    "Violation": "improper",
    "Bug report": "switching the exact check to isinstance check. Simplifying a type check if an object is a SymIntNode in `is_symint_node`",
    "Number of deleted lines": 2,
    "Deleted lines": "#include <torch/csrc/Exceptions.h>\n#include <torch/csrc/Generator.h>\n#include <torch/csrc/Layout.h>\n#include <torch/csrc/MemoryFormat.h>\n#include <torch/csrc/QScheme.h>\n#include <torch/csrc/Stream.h>\n#include <torch/csrc/autograd/python_variable.h>\n#include <torch/csrc/autograd/variable.h>\n#include <torch/csrc/jit/frontend/tracer.h>\n#include <torch/csrc/python_dimname.h>\n#include <torch/csrc/tensor/python_tensor.h>\n#include <torch/csrc/utils/disable_torch_function.h>\n#include <torch/csrc/utils/object_ptr.h>\n#include <torch/csrc/utils/pybind.h>\n#include <torch/csrc/utils/python_numbers.h>\n#include <torch/csrc/utils/python_strings.h>\n#include <torch/csrc/utils/six.h>\n\n#include <ATen/PythonTorchFunctionTLS.h>\n#include <ATen/core/Tensor.h>\n#include <c10/util/Exception.h>\n#include <c10/util/irange.h>\n\n#include <c10/core/SymIntNodeImpl.h>\n#include <array>\n#include <cstddef>\n#include <memory>\n#include <sstream>\n#include <string>\n#include <vector>\n\nnamespace torch {\ninline bool is_symint_node(py::handle obj) {\n  auto static tp_symn = py::type::of<c10::SymIntNodeImpl>();\n  // TODO: switch this to `isinstance`\n  if (obj.get_type().equal(tp_symn)) {\n    TORCH_CHECK(\n        !jit::tracer::isTracing(), \"JIT tracing of SymInts isn't supported!\");\n    return true;\n  }\n  return false;\n}\n} // namespace torch\n\nnamespace pybind11 {\nnamespace detail {\ntemplate <>\nstruct type_caster<c10::SymInt> {\n public:\n  PYBIND11_TYPE_CASTER(c10::SymInt, _(\"SymInt\"));\n  bool load(py::handle src, bool) {\n    if (torch::is_symint_node(src)) {\n      value = src.cast<c10::SymIntNodeImpl*>()->toSymInt();\n      return true;\n    }\n\n    auto raw_obj = src.ptr();\n    if (THPUtils_checkIndex(raw_obj)) {\n      value = c10::SymInt{THPUtils_unpackIndex(raw_obj)};\n      return true;\n    }\n    return false;\n  }\n\n  static py::handle cast(\n      c10::SymInt si,\n      return_value_policy /* policy */,\n      handle /* parent */) {\n    return si.is_symbolic() ? py::cast(si.toSymIntNodeImpl()).release()\n                            : py::cast(si.expect_int()).release();\n  }\n};"
},
{
    "Id": 53,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/92ebb04f9206882e6d312a8b91318545f43a53c2",
    "Violation": "insufficient",
    "Bug report": "added check for NumberType",
    "Number of deleted lines": 3,
    "Deleted lines": "            getAugOp(stmt, sliceable->type()),\n            {rhs},\n            {},\n            slicedArg);\n      } else {\n        // Special case: we tried to do \"advanced indexing\". Lower this expr\n        // into `index` and `index_put_` ops with tensordices of Tensor?[]\n        const auto indices = graph\n                                 ->insertNode(graph->createList(\n                                     OptionalType::ofTensor(), tensorIndices))\n                                 ->output();\n        const auto indexed =\n            graph->insert(aten::index, {slicedArg, indices}, {}, stmt.range());\n        const auto augmented = emitBuiltinCall(\n            stmt.range(),\n            *method.graph(),\n            getAugOp(stmt, sliceable->type()),\n            {rhs},\n            {},\n            indexed);\n        graph->insert(\n            aten::index_put_,\n            {slicedArg, indices, augmented},\n            {},\n            stmt.range());\n      }\n    } else {\n      emitAugAssignmentGeneric(stmt, lhs, sliceable);\n    }\n  }\n\n  NamedValue emitValueToTensor(\n      const NamedValue& value,\n      const NamedValue& matchTypeOf) {\n    // Add implicit conversion of int/float/bool types to tensors\n    // Used in emitSubscriptAssign to convert:\n    //   `tensor(...)[x] = 99` to `tensor(...)[x] = tensor(99)`\n    // Mirrors the `valueToTensor` behavior in python_variable_indexing.cpp\n    const auto kind = value.type()->kind();\n    if (kind == c10::TypeKind::IntType || kind == c10::TypeKind::BoolType ||\n        kind == c10::TypeKind::FloatType) {\n      auto dtype = graph->insert(prim::dtype, {matchTypeOf}, {});\n      auto device = graph->insert(prim::device, {matchTypeOf}, {});\n      auto converted = graph->insert(\n          aten::tensor,\n          {value},\n          {NamedValue(\"dtype\", dtype), NamedValue(\"device\", device)});\n      return NamedValue(value.loc(), converted);\n    }\n\n    return value;\n  }\n\n  // Emit mutating assignments like `foo[0] = bar`\n  void emitSubscriptAssign(\n      const SourceRange& stmtRange,\n      const Subscript& lhs,\n      const Expr& rhs) {\n    emitSubscriptAssign(stmtRange, lhs, NamedValue(rhs.range(), emitExpr(rhs)));\n  }\n\n  void emitSubscriptAssign(\n      const SourceRange& stmtRange,\n      const Subscript& lhs,\n      const NamedValue& rhs) {\n    // First check the base value.\n    auto sliceable = emitExpr(lhs.value());\n\n    // If it's a tensor, copy the RHS data into it\n    if (sliceable->type()->isSubtypeOf(TensorType::get())) {\n      std::vector<Value*> tensorIndices;\n      Value* sliced;\n      // Handle multi-dimensional slicing: first emit int/slice indexing\n      // TODO: the Python equivalent code has special-cased copy_to\n      // broadcasting to match NumPy semantics (see PR#4853). We can't\n      // replicate that without knowing the size of the Tensor; so really that\n      // code should be moved into the aten function"
},
{
    "Id": 54,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fe6aa0844466e5dd2669092eac5edde153108b28",
    "Violation": "improper",
    "Bug report": "Don't delegate to `operator=` for construction. Catch hypothetical addition of a new Scalar type via debug assertion rather than checking in prod.",
    "Number of deleted lines": 5,
    "Deleted lines": "  bool isPyObject() const {\n    return tag == Tag::PyObject;\n  }\n  c10::intrusive_ptr<ivalue::PyObjectHolder> toPyObjectHolder() &&;\n  c10::intrusive_ptr<ivalue::PyObjectHolder> toPyObjectHolder() const&;\n  PyObject* toPyObject() const;\n\n  // Enum\n  explicit IValue(c10::intrusive_ptr<ivalue::EnumHolder> v);\n  bool isEnum() const {\n    return tag == Tag::Enum;\n  }\n  c10::intrusive_ptr<ivalue::EnumHolder> toEnumHolder() &&;\n  c10::intrusive_ptr<ivalue::EnumHolder> toEnumHolder() const&;\n\n  // None\n  IValue() : tag(Tag::None) {}\n  bool isNone() const {\n    return Tag::None == tag;\n  }\n  std::string toNone() const {\n    AT_ASSERT(isNone());\n    return \"None\";\n  }\n\n  static IValue uninitialized() {\n    auto i = IValue();\n    i.tag = Tag::Uninitialized;\n    return i;\n  }\n\n  // Scalar, which gets encoded as either an Int, a Double or a ComplexDouble\n  IValue(const at::Scalar& s) : IValue() {\n    if (s.isFloatingPoint()) {\n      *this = s.toDouble();\n    } else if (s.isComplex()) {\n      *this = s.toComplexDouble();\n    } else if (s.isBoolean()) {\n      *this = s.toBool();\n    } else if (s.isIntegral(false)) {\n      *this = s.toLong();\n    } else {\n      TORCH_CHECK(false, \"Unknown type in Scalar\");\n    }\n  }\n\n  bool isScalar() const {\n    return isDouble() || isInt() || isComplexDouble() || isBool();\n  }\n\n  at::Scalar toScalar() const {\n    if (isDouble())\n      return toDouble();\n    else if (isInt())\n      return toInt();\n    else if (isComplexDouble())\n      return toComplexDouble();\n    else if (isBool())\n      return toBool();\n    throw std::runtime_error(\"IValue is not a Scalar\");\n  }\n\n  // Device\n  IValue(c10::Device d) : tag(Tag::Device) {\n    payload.u.as_device.type = d.type();\n    payload.u.as_device.index = d.index();\n  }\n  bool isDevice() const {\n    return Tag::Device == tag;\n  }\n  c10::Device toDevice() const {\n    AT_ASSERT(isDevice());\n    return c10::Device(payload.u.as_device.type, payload.u.as_device.index);\n  }\n\n  //Stream\n  IValue(c10::Stream stream)\n    : tag(Tag::Stream) {\n    payload.u.as_int = stream.pack();"
},
{
    "Id": 55,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/35e7ac3fa17bc20d982ea69440d30cd9658dff25",
    "Violation": "insufficient",
    "Bug report": "This fixes a bug in singleCheckErrors introduced by #69437 (thanks Lezcano for the catch). Checking existence of a substring in a larger string is done with (name.find(text) != name.npos) but we omitted the second half of the check.",
    "Number of deleted lines": 3,
    "Deleted lines": "}\n\nstatic inline void checkFloatingOrComplex(const Tensor& t, const char* const f_name) {\n  TORCH_CHECK((at::isFloatingType(t.scalar_type()) || at::isComplexType(t.scalar_type())),\n              f_name, \": Expected a floating point or complex tensor as input. Got \", toString(t.scalar_type()));\n}\n\n/*\n * Given a info int, obtained after a single operation, this function check if the computation\n * has been successful (info = 0) or not, and report in case of the latter.\n */\nstatic inline void singleCheckErrors(int64_t info, const c10::string_view name, int64_t batch_id=-1) {\n  std::string batch_string{\"\"};\n  if (batch_id >= 0) {\n    batch_string = \": (Batch element \" + std::to_string(batch_id) + \")\";\n  }\n  if (info < 0) {\n    TORCH_INTERNAL_ASSERT(false, name, batch_string,\n        \": Argument \", -info, \" has illegal value. Most certainly there is a bug in the implementation calling the backend library.\");\n  } else if (info > 0) {\n    if (name.find(\"inv\") != name.npos) {\n      // inv, inverse, cholesky_inverse, etc.\n      TORCH_CHECK_LINALG(false, name, batch_string,\n          \": The diagonal element \", info, \" is zero, the inversion could not be completed because the input matrix is singular.\");\n    } else if (name.find(\"solve\") != name.npos) {\n      // solve, linalg_solve, cholesky_solve, etc.\n      TORCH_CHECK_LINALG(false, name, batch_string,\n          \": The diagonal element \", info, \" is zero, the solve could not be completed because the input matrix is singular.\");\n    } else if (name.find(\"cholesky\") != name.npos) {\n      TORCH_CHECK_LINALG(false, name, batch_string,\n          \": The factorization could not be completed because the input is not positive-definite (the leading minor of order \", info, \" is not positive-definite).\");\n    } else if (name.find(\"svd\") != name.npos) {\n      TORCH_CHECK_LINALG(false, name, batch_string,\n          \": The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated singular values (error code: \", info, \").\");\n    } else if (name.find(\"eig\") || name.find(\"syevd\")) {\n      TORCH_CHECK_LINALG(false, name, batch_string,\n          \": The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: \", info, \").\");\n    } else if (name.find(\"lstsq\")) {\n      TORCH_CHECK_LINALG(false, name, batch_string,\n          \": The least squares solution could not be computed because the input matrix does not have full rank (error code: \", info, \").\");\n    } else if (name.find(\"lu_factor\")) {\n      TORCH_CHECK(false, name, batch_string,\n          \": U[\", info, \",\", info, \"] is zero and using it on lu_solve would result in a division by zero. \"\n          \"If you still want to perform the factorization, consider calling linalg.lu(A, pivot) or \"\n          \"linalg.lu_factor_ex(A, pivot)\");\n    } else {\n      TORCH_INTERNAL_ASSERT(false, name, \": Unknown error code: \", info, \".\");\n    }\n  }\n}\n\n/*\n * Given a vector of int64_t infos, obtained after a batch operations,\n * this function checks if the computation over all these batches has been\n * successful (info = 0) or not, and report in case of the latter.\n */\nstatic inline void batchCheckErrors(const std::vector<int64_t>& infos, const c10::string_view name) {\n  for (size_t i = 0; i < infos.size(); i++) {\n    auto info = infos[i];\n    singleCheckErrors(info, name, i);\n  }\n}\n\n/*\n * This is an overloaded case of the previous function for a tensor of infos.\n */\nstatic inline void batchCheckErrors(const Tensor& infos, const c10::string_view name) {\n  auto infos_cpu = infos.to(at::kCPU);\n  auto infos_data = infos_cpu.data_ptr<int>();\n  for (int64_t i = 0; i < infos.numel(); i++) {\n    auto info = infos_data[i];\n    singleCheckErrors(info, name, i);\n  }\n}\n\n// Checks if all the Tensors in a TensorList are of the same dimensions\nstatic inline void checkAllSameDim(TensorList tensors, int64_t dim) {"
},
{
    "Id": 56,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a7cc6531399300f999a404718827e2a94c115aaf",
    "Violation": "insufficient",
    "Bug report": "Summary:GCC version check is currently being skipped when using the newly released CUDA 9.1. This will also handle other CUDA 9.x minor releases if any, reducing our work if there are such releases like 9.2. This assumes that the next major CUDA version will be 10.0, needing adjustment only after such major version is released.",
    "Number of deleted lines": 3,
    "Deleted lines": "        message(WARNING \"OpenMPI found, but it is not built with CUDA support.\")\n        set(CAFFE2_FORCE_FALLBACK_CUDA_MPI 1)\n      endif()\n    endif()\n  else()\n    message(WARNING \"Not compiling with MPI. Suppress this warning with -DUSE_MPI=OFF\")\n    set(USE_MPI OFF)\n  endif()\nendif()\n\n# ---[ OpenMP\nif(USE_OPENMP)\n  find_package(OpenMP)\n  if(OPENMP_FOUND)\n    message(STATUS \"Adding \" ${OpenMP_CXX_FLAGS})\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}\")\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${OpenMP_EXE_LINKER_FLAGS}\")\n  else()\n    message(WARNING \"Not compiling with OpenMP. Suppress this warning with -DUSE_OPENMP=OFF\")\n    set(USE_OPENMP OFF)\n  endif()\nendif()\n\n\n# ---[ Android specific ones\nif(ANDROID)\n  list(APPEND Caffe2_DEPENDENCY_LIBS log)\nendif()\n\n# ---[ CUDA\nif(USE_CUDA)\n  include(cmake/Cuda.cmake)\n  if(HAVE_CUDA)\n    # CUDA 9.0 requires GCC version <= 6\n    if (CUDA_VERSION VERSION_EQUAL 9.0)\n      if (CMAKE_C_COMPILER_ID STREQUAL \"GNU\" AND\n          NOT CMAKE_C_COMPILER_VERSION VERSION_LESS 7.0 AND\n          CUDA_HOST_COMPILER STREQUAL CMAKE_C_COMPILER)\n        message(FATAL_ERROR\n          \"CUDA 9.0 is not compatible with GCC version >= 7. \"\n          \"Use the following option to use another version (for example): \\n\"\n          \"  -DCUDA_HOST_COMPILER=/usr/bin/gcc-6\\n\")\n      endif()\n    # CUDA 8.0 requires GCC version <= 5\n    elseif (CUDA_VERSION VERSION_EQUAL 8.0)\n      if (CMAKE_C_COMPILER_ID STREQUAL \"GNU\" AND\n          NOT CMAKE_C_COMPILER_VERSION VERSION_LESS 6.0 AND\n          CUDA_HOST_COMPILER STREQUAL CMAKE_C_COMPILER)\n        message(FATAL_ERROR\n          \"CUDA 8.0 is not compatible with GCC version >= 6. \"\n          \"Use the following option to use another version (for example): \\n\"\n          \"  -DCUDA_HOST_COMPILER=/usr/bin/gcc-5\\n\")\n      endif()\n    endif()\n  endif()\n  # ---[ CUDNN\n  if(HAVE_CUDA)\n    find_package(CuDNN REQUIRED)\n    if(CUDNN_FOUND)\n      caffe2_include_directories(${CUDNN_INCLUDE_DIRS})\n      list(APPEND Caffe2_CUDA_DEPENDENCY_LIBS ${CUDNN_LIBRARIES})\n    endif()\n  else()\n    message(WARNING \"Not compiling with CUDA. Suppress this warning with -DUSE_CUDA=OFF\")\n    set(USE_CUDA OFF)\n  endif()\nendif()\n\n# ---[ NCCL\nif(USE_NCCL)\n  if(NOT USE_CUDA)\n    message(WARNING \"If not using cuda, one should not use NCCL either.\")\n    set(USE_NCCL OFF)\n  elseif(NOT ${CMAKE_SYSTEM_NAME} STREQUAL \"Linux\")\n    message(WARNING \"NCCL is currently only supported under Linux.\")\n    set(USE_NCCL OFF)"
},
{
    "Id": 57,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3f5dc95b57496c4ea938be381efcdc2ea92bb4cc",
    "Violation": "insufficient",
    "Bug report": "Some of the tests don't specify `device` in the input configs so filter by device won't work for them. This diff fixes that issue.",
    "Number of deleted lines": 1,
    "Deleted lines": "                break\n\n            # Re-estimate the hopefully-sufficient\n            # iteration count, and run the benchmark again...\n            iters = self._predict_num_iter_needed(iters)\n        reported_run_time_us = np.percentile(np.array(time_trace), 50)\n        return reported_run_time_us\n\n    def _check_keep(self, test_flag, cmd_flag):\n        return (cmd_flag is None or test_flag == cmd_flag)\n\n    def _check_keep_list(self, test_flag, cmd_flag_list):\n        if (cmd_flag_list is None or\n                any(test_flag == cmd_flag for cmd_flag in cmd_flag_list)):\n            return True\n        return False\n\n    def _keep_test(self, test_case):\n        # TODO: consider regex matching for test filtering.\n        # Currently, this is a sub-string matching.\n        op_test_config = test_case.test_config\n\n        if self.args.framework:\n            frameworks = benchmark_utils.process_arg_list(self.args.framework)\n\n        operators = benchmark_utils.process_arg_list(self.args.operators) if self.args.operators else None\n\n        # Filter framework, operator, test_name, tag, forward_only\n        if (self._check_keep(op_test_config.test_name, self.args.test_name) and\n            self._check_keep_list(test_case.op_bench.module_name(), operators) and\n            self._check_keep_list(test_case.framework, frameworks) and\n                (self.args.tag_filter == 'all' or\n                    self._check_keep(op_test_config.tag, self.args.tag_filter)) and\n                (not self.args.forward_only or op_test_config.run_backward != self.args.forward_only) and\n                (self.args.device == 'None' or self.args.device in op_test_config.test_name)):\n            return True\n\n        return False\n\n    def _print_test_case_info(self, test_case):\n        # Print out the test name and skip the real execution\n        if self.args.list_tests:\n            print(\"# {}\".format(test_case.test_config.test_name))\n            return True\n        elif self.args.list_ops:\n            if self.args.operators is None:\n                op_name = test_case.op_bench.module_name()\n\n                if op_name not in self.printed_ops_list:\n                    print(\"# {}\".format(op_name))\n                    self.printed_ops_list.add(op_name)\n            return True\n\n        return False\n\n    def run(self):\n        self._print_header()\n\n        for test_metainfo in BENCHMARK_TESTER:\n            for test in _build_test(*test_metainfo):\n                full_test_id, test_case = test\n                op_test_config = test_case.test_config\n\n                if self._print_test_case_info(test_case):\n                    continue\n\n                if not self._keep_test(test_case):\n                    continue\n\n                # To reduce variance, fix a numpy randseed to the test case,\n                # so that the randomly generated input tensors remain the"
},
{
    "Id": 58,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1c02be1b6a0f6d02d3a0ae19c13d51a3e59a55ae",
    "Violation": "insufficient",
    "Bug report": "In PyTorch 1.5, when running `torch.cuda.reset_peak_memory_stats()` on a machine where `torch.cuda.is_available() is False`, I would get: AssertionError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from. With this patch, we get a more sensible:",
    "Number of deleted lines": 1,
    "Deleted lines": "        # NOTE [ Python Traceback Reference Cycle Problem ]\n        if exc_info is None:\n            exc_info = sys.exc_info()\n        self.exc_type = exc_info[0]\n        self.exc_msg = \"\".join(traceback.format_exception(*exc_info))\n        self.where = where\n\n    def reraise(self):\n        r\"\"\"Reraises the wrapped exception in the current thread\"\"\"\n        # Format a message such as: \"Caught ValueError in DataLoader worker\n        # process 2. Original Traceback:\", followed by the traceback.\n        msg = \"Caught {} {}.\\nOriginal {}\".format(\n            self.exc_type.__name__, self.where, self.exc_msg)\n        if self.exc_type == KeyError:\n            # KeyError calls repr() on its argument (usually a dict key). This\n            # makes stack traces unreadable. It will not be changed in Python\n            # (https://bugs.python.org/issue2651), so we work around it.\n            msg = KeyErrorMessage(msg)\n        elif getattr(self.exc_type, \"message\", None):\n            # Some exceptions have first argument as non-str but explicitly\n            # have message field\n            raise self.exc_type(message=msg)\n        raise self.exc_type(msg)\n\n\ndef _get_available_device_type():\n    if torch.cuda.is_available():\n        return \"cuda\"\n    # add more available device types here\n    return None\n\n\ndef _get_device_attr(get_member):\n    device_type = _get_available_device_type()\n    if device_type.lower() == \"cuda\":\n        return get_member(torch.cuda)\n    # add more available device types here\n    return None\n\n\ndef _get_current_device_index():\n    # current device index\n    return _get_device_attr(lambda m: m.current_device())\n\n\ndef _get_all_device_indices():\n    # all device index\n    return _get_device_attr(lambda m: list(range(m.device_count())))\n\n\ndef _get_devices_properties(device_ids):\n    # all device properties\n    return [_get_device_attr(lambda m: m.get_device_properties(i)) for i in device_ids]\n\n\ndef _get_device_index(device, optional=False, allow_cpu=False) -> int:\n    r\"\"\"Gets the device index from :attr:`device`, which can be a torch.device\n    object, a Python integer, or ``None``.\n\n    If :attr:`device` is a torch.device object, returns the device index if it\n    has index. Note that for a device without a specified index,\n    i.e., ``torch.device('xxx')``, this will return the current default\n    device of that type if :attr:`optional` is ``True``. If :attr:`allow_cpu` is ``True``,\n    CPU devices will be accepted and ``-1`` will be returned in this case.\n\n    If :attr:`device` is a Python integer, it is returned as is.\n\n    If :attr:`device` is ``None``, this will return the current default\n    device of the supported runtime platform if :attr:`optional` is ``True``.\n    i.e., the current default CUDA device will be returned if CUDA runtime is supported.\n    \"\"\""
},
{
    "Id": 59,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/29b702144bf5bb96dfd8fcbd04b6562a27ca5385",
    "Violation": "improper",
    "Bug report": " Fix issue in s_addmm_out_sparse_dense_cpu only supporting CUDA device checking. ## Motivation The at::native::s_addmm_out_sparse_dense_cpu only supports the CPU tensors. But it only checks whether the tensor is on CUDA device which is not enough. ## Solution Change the tensor device type checkging from is_cuda to !is_cpu to protect other backends than the CUDA.",
    "Number of deleted lines": 4,
    "Deleted lines": "  scalar_t* r_ptr = r.data_ptr<scalar_t>();\n\n  int64_t dense_stride0 = dense.stride(0);\n  int64_t dense_stride1 = dense.stride(1);\n  int64_t r_stride0 = r.stride(0);\n  int64_t r_stride1 = r.stride(1);\n  for (auto i: c10::irange(nnz)) {\n    scalar_t val = values_accessor[i];\n    int64_t row = indices_accessor[0][i];\n    int64_t col = indices_accessor[1][i];\n    if (col >= 0 && col < dim_j && row >= 0 && row < dim_i) {\n      at::native::cpublas::axpy<scalar_t>(dim_k,\n            cast_alpha * val,\n            dense_ptr + col * dense_stride0, dense_stride1,\n            r_ptr + row * r_stride0, r_stride1);\n    } else {\n      if (col < 0 || col >= dim_j) {\n        AT_ERROR(\"addmm: index out of column bound: \", col, \" not between 1 and \", dim_j);\n      } else {\n        AT_ERROR(\"addmm: index out of row bound: \", row, \" not between 1 and \", dim_i);\n      }\n    }\n  }\n};\n\nTensor& s_addmm_out_sparse_dense_cpu(\n    Tensor& r,\n    const Tensor& t,\n    const SparseTensor& sparse_,\n    const Tensor& dense,\n    const Scalar& beta,\n    const Scalar& alpha\n) {\n  // TODO: This error message seems awfully opaque\n  TORCH_CHECK(!t.is_cuda(),  \"Expected all tensors to be on the same device. addmm expected 't' to be CPU tensor, but got CUDA tensor\");\n  TORCH_CHECK(!r.is_cuda(), \"Expected all tensors to be on the same device. addmm: expected 'out' to be CPU tensor, but got CUDA tensor\");\n  TORCH_CHECK(!sparse_.is_cuda(), \"Expected all tensors to be on the same device. addmm: expected 'mat1' to be a CPU tensor, but got a CUDA tensor\");\n  TORCH_CHECK(!dense.is_cuda(), \"Expected all tensors to be on the same device. addmm: expected 'mat2' to be a CPU tensor, but got a CUDA tensor\");\n\n  TORCH_CHECK(sparse_.sparse_dim() == 2, \"addmm: matrices expected, got \", sparse_.sparse_dim(), \"D tensor\");\n  TORCH_CHECK(sparse_.dense_dim() == 0, \"addmm: scalar values expected, got \", sparse_.dense_dim(), \"D values\");\n  TORCH_CHECK(dense.dim() == 2, \"addmm: matrices expected, got \", dense.dim(), \"D tensor\");\n\n  // ixj * jxk = ixk\n  int64_t dim_i = sparse_.size(0);\n  int64_t dim_j = sparse_.size(1);\n  int64_t dim_k = dense.size(1);\n\n  TORCH_CHECK(dense.size(0) == dim_j,\n      \"addmm: Argument #3 (dense): Expected dim 0 size \", dim_j, \", got \", dense.size(0));\n  TORCH_CHECK(t.size(0) == dim_i,\n      \"addmm: Argument #1 (t): Expected dim 0 size \", dim_i, \", got \", t.size(0));\n  TORCH_CHECK(t.size(1) == dim_k,\n      \"addmm: Argument #1 (t): Expected dim 1 size \", dim_k, \", got \", t.size(1));\n\n  r.resize_({dim_i, dim_k});\n\n  int64_t nnz        = sparse_._nnz();\n\n  if (nnz == 0) {\n    at::mul_out(r, t, at::scalar_tensor(beta, r.options()));\n    return r;\n  }\n\n  Tensor indices = sparse_._indices();\n  Tensor values      = sparse_._values();\n\n  AT_DISPATCH_ALL_TYPES_AND_COMPLEX(\n      values.scalar_type(), \"addmm_sparse_dense\", [&] {\n        s_addmm_out_sparse_dense_worker<scalar_t>(nnz, dim_i, dim_j, dim_k, r, beta, t, alpha, indices, values, dense);\n      }\n  );\n\n  return r;"
},
{
    "Id": 60,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bbc7c79b20e67da450dd9b7de70cc6b68e656714",
    "Violation": "missing",
    "Bug report": "add device checks for sparse csr",
    "Number of deleted lines": 0,
    "Deleted lines": "                    ]__device__(int64_t irow) {\n                      for (index_t batch_idx = 0; batch_idx < batch_count; batch_idx++) {\n                        index_t start_index = crow_indices_accessor[batch_idx*crow_stride0 + irow];\n                        index_t end_index = crow_indices_accessor[batch_idx*crow_stride0 + irow + 1];\n\n                        for (index_t i = start_index; i < end_index; ++i) {\n                            auto icol = col_indices_accessor[batch_idx*col_stride0 + i];\n                            auto index = batch_idx * out_strides0 + irow * out_strides1 + icol;\n                            out_ptr[index] += cast_value * values_accessor[batch_idx*val_stride0 + i];\n                        }\n                      }\n                    });\n              });\n      });\n  if (output.scalar_type() != commonDtype) {\n    output.copy_(resultBuffer);\n  }\n  return output;\n}\n\nTensor& add_out_sparse_csr_cuda(\n    const Tensor& self,\n    const SparseCsrTensor& other,\n    const Scalar& alpha,\n    SparseCsrTensor& out) {\n  if (self.layout() == kStrided) {\n    add_out_dense_sparse_csr_cuda(out, self, other, alpha);\n  } else {\n    TORCH_CHECK(\n        self.sizes().equals(other.sizes()),\n        \"torch.add: Expected input tensors to have the same shape, but got tensor `self` with shape \",\n        self.sizes(),\n        \" and tensor `other` with shape \",\n        other.sizes());\n\n    if (only_sparse_compressed_add_trivial_cases(self, other, alpha, out)) {\n      return out;\n    }\n\n    at::native::resize_as_sparse_compressed_(out, self);\n    sparse::impl::cuda::add_out_sparse_csr(self, other, Scalar(1), alpha, out);\n  }\n  return out;\n}\n\nTORCH_IMPL_FUNC(_convert_indices_from_coo_to_csr_structured_cuda) (\n  const Tensor& input, const int64_t size, const bool out_int32, const Tensor& result\n) {\n  if (out_int32) {\n    AT_DISPATCH_INTEGRAL_TYPES(input.scalar_type(), \"convert_indices_from_coo_to_csr_cuda\", [&] {\n      convert_indices_from_coo_to_csr_cuda<scalar_t, int>(result, input, size);\n    });\n  } else {\n    AT_DISPATCH_INTEGRAL_TYPES(input.scalar_type(), \"convert_indices_from_coo_to_csr_cuda\", [&] {\n      convert_indices_from_coo_to_csr_cuda<scalar_t, int64_t>(result, input, size);\n    });\n  }\n}\n\nTORCH_IMPL_FUNC(_convert_indices_from_csr_to_coo_structured_cuda) (\n  const Tensor& crow_indices, const Tensor& col_indices, const bool out_int32, const bool transpose, const Tensor& result\n) {\n  if (out_int32) {\n    AT_DISPATCH_INTEGRAL_TYPES(crow_indices.scalar_type(), \"convert_indices_from_csr_to_coo_cuda\", [&] {\n      convert_indices_from_csr_to_coo_cuda<scalar_t, int32_t>(result, crow_indices, col_indices, transpose);\n    });\n  } else {\n    AT_DISPATCH_INTEGRAL_TYPES(crow_indices.scalar_type(), \"convert_indices_from_csr_to_coo_cuda\", [&] {\n      convert_indices_from_csr_to_coo_cuda<scalar_t, int64_t>(result, crow_indices, col_indices, transpose);\n    });"
},
{
    "Id": 61,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/faa7eb81c634492b70fcc0327622bb0aa812cacd",
    "Violation": "misleading",
    "Bug report": "change error_message for XPU Autocast data type check. XPU autocast supports bf16 and fp16 data types, we are going to change the error_message for that.",
    "Number of deleted lines": 1,
    "Deleted lines": "            message += \"registered a module or  the module miss some necessary funcs. The backend should register \"\n            message += \"a module by `torch._register_device_module`, and the module must have these funcs: \\n\"\n            message += \"`is_autocast_enabled() -> bool`, `set_autocast_enabled(bool) -> None`, \"\n            message += \"`get_autocast_dtype() -> torch.dtype`, `set_autocast_dtype(torch.dtype) \"\n            message += \"-> None` and `get_amp_supported_dtype() -> List[torch.dtype]`. \\n\"\n\n            assert hasattr(torch, self.custom_backend_name), message\n            self.custom_device_mod = getattr(torch, self.custom_backend_name)\n            for func in necessary_funcs:\n                assert hasattr(self.custom_device_mod, func), message + f\"But the func `{func}` is missing. \\n\"\n\n            self.fast_dtype = self.custom_device_mod.get_autocast_dtype()\n        else:\n            raise RuntimeError('User specified autocast device_type must be \\'cuda\\' or \\'cpu\\'')\n        self._cache_enabled = torch.is_autocast_cache_enabled()\n        if enabled and torch.cuda.amp.common.amp_definitely_not_available() and self.device == 'cuda':\n            warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n            enabled = False\n        if dtype is not None:\n            self.fast_dtype = dtype\n        if cache_enabled is not None:\n            self._cache_enabled = cache_enabled\n\n        if self.device == 'cpu':\n            supported_dtype = [torch.bfloat16]\n            if self.fast_dtype not in supported_dtype:\n                error_message = 'In CPU autocast, but the target dtype is not supported. Disabling autocast.\\n'\n                error_message += 'CPU Autocast only supports dtype of torch.bfloat16 currently.'\n                warnings.warn(error_message)\n                enabled = False\n        elif self.device == 'xpu':\n            supported_dtype = [torch.bfloat16, torch.float16]\n            if self.fast_dtype not in supported_dtype:\n                error_message = 'In XPU autocast, but the target dtype is not supported. Disabling autocast.\\n'\n                error_message += 'XPU Autocast only supports dtype of torch.bfloat16 currently.'\n                warnings.warn(error_message)\n                enabled = False\n        elif self.device == 'hpu':\n            supported_dtype = [torch.bfloat16, torch.float16]\n            if self.fast_dtype not in supported_dtype:\n                error_message = 'In HPU autocast, but the target dtype is not supported. Disabling autocast.\\n'\n                error_message += 'HPU Autocast only supports dtypes of torch.bfloat16 and torch.float16 currently.'\n                warnings.warn(error_message)\n                enabled = False\n        elif self.device == self.custom_backend_name:\n            supported_dtype = self.custom_device_mod.get_amp_supported_dtype()\n            if self.fast_dtype not in supported_dtype:\n                error_message = f\"In {self.custom_backend_name} autocast, but the target dtype is not supported. \"\n                error_message += f\"Disabling autocast.\\n {self.custom_backend_name} Autocast only supports dtypes of \"\n                error_message += \", \".join(str(dtype) for dtype in supported_dtype) + \" currently.\"\n                warnings.warn(error_message)\n                enabled = False\n        elif self.device == 'cuda':\n            if enabled and self.fast_dtype == torch.bfloat16 and not torch.cuda.is_bf16_supported():\n                raise RuntimeError('Current CUDA Device does not support bfloat16. Please switch dtype to float16.')\n        self._enabled = enabled\n\n    def __enter__(self):\n        if torch._jit_internal.is_scripting():\n            assert self.fast_dtype is not None\n            return self\n\n        self.prev_cache_enabled = torch.is_autocast_cache_enabled()\n        if self.device == 'cpu':\n            self.prev = torch.is_autocast_cpu_enabled()\n            self.prev_fastdtype = torch.get_autocast_cpu_dtype()\n            torch.set_autocast_cpu_enabled(self._enabled)\n            torch.set_autocast_cpu_dtype(self.fast_dtype)  # type: ignore[arg-type]\n            torch.autocast_increment_nesting()\n        elif self.device == 'xpu':\n            self.prev = torch.xpu.is_autocast_xpu_enabled()    # type: ignore[attr-defined]"
},
{
    "Id": 62,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/48a49b2683ffa21eb1b472e503c129c043c18f87",
    "Violation": "misleading",
    "Bug report": "use more informative error message for ConstandPad2d/3d.  the current error message for `torch.nn.ConstantPad2d` and `torch.nn.ConstantPad3d` is misleading, this PR fixes the problem.",
    "Number of deleted lines": 1,
    "Deleted lines": "  out_slice.copy_(in_slice);\n\n  // The following steps first pad the beginning of the tensor (left side),\n  // and then pad the end of the tensor (right side).\n  // Note: Corners will be written more than once when ndim > 1.\n  //\n  // Only in cases where padding values are > 0 are when additional copying\n  // is required.\n  for (const auto i : c10::irange(ndim)) {\n    const auto dim = ndim - i + 1;\n    const auto& pad_l = padding[2*i + 0];\n    const auto& pad_r = padding[2*i + 1];\n\n    if (pad_l > 0) {\n      out_slice = out.slice_symint(dim, 0, pad_l);\n      in_slice = out.slice_symint(dim,\n                           out_shape[dim] - pad_l - std::max(pad_r, zero),\n                           out_shape[dim] - std::max(pad_r, zero));\n      out_slice.copy_(in_slice);\n    }\n\n    if (pad_r > 0) {\n      out_slice = out.slice_symint(dim, out_shape[dim] - pad_r, out_shape[dim]);\n      in_slice = out.slice_symint(dim, std::max(pad_l, zero), std::max(pad_l, zero) + pad_r);\n      out_slice.copy_(in_slice);\n    }\n  }\n\n  return out;\n}\n\nTensor _pad_enum_symint(const Tensor &self, c10::SymIntArrayRef pad, int64_t mode_int, c10::optional<double> value) {\n  const auto input_dim = self.dim();\n  TORCH_CHECK(pad.size() % 2 == 0, \"Padding length must be divisible by 2\");\n  TORCH_CHECK(static_cast<int64_t>(pad.size()) <= input_dim * 2, \"Padding length too large\");\n  auto mode = static_cast<at::padding_mode>(mode_int);\n\n  if (mode == at::padding_mode::constant) {\n    return at::constant_pad_nd_symint(self, pad, value.value_or(0.0));\n  }\n  TORCH_CHECK(!value.has_value() || *value == 0,\n              \"Padding mode \\\"\", padding_mode_string(mode),\n              \"\\\" doesn't take in value argument\");\n\n  if (pad.size() == 2 && (input_dim == 2 || input_dim == 3)) {\n    switch (mode) {\n      case at::padding_mode::reflect: return at::reflection_pad1d_symint(self, pad);\n      case at::padding_mode::replicate: return at::replication_pad1d_symint(self, pad);\n      case at::padding_mode::circular: return at::_pad_circular_symint(self, pad);\n      default: {}\n    }\n  } else if(pad.size() == 4 && (input_dim == 3 || input_dim == 4)) {\n    switch (mode) {\n      case at::padding_mode::reflect: return at::reflection_pad2d_symint(self, pad);\n      case at::padding_mode::replicate: return at::replication_pad2d_symint(self, pad);\n      case at::padding_mode::circular: return at::_pad_circular_symint(self, pad);\n      default: {}\n    }\n  } else if (pad.size() == 6 && (input_dim == 4 || input_dim == 5)) {\n    switch (mode) {\n      case at::padding_mode::reflect: return at::reflection_pad3d_symint(self, pad);\n      case at::padding_mode::replicate: return at::replication_pad3d_symint(self, pad);\n      case at::padding_mode::circular: return at::_pad_circular_symint(self, pad);\n      default: {}\n    }\n  }\n  C10_THROW_ERROR(NotImplementedError,\n      \"Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now\");\n}\n\nTensor pad_symint(const Tensor &self, c10::SymIntArrayRef pad, c10::string_view mode, c10::optional<double> value) {"
},
{
    "Id": 63,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4ab1588d9919bc1a62219a5c2393e0784ddaae70",
    "Violation": "misleading",
    "Bug report": "Enhance error message for dependency check, If python development library is missing when building pytorch from source cmake will raise the error like: CMake Error at cmake/Dependencies.cmake:1079 (if): if given arguments: \"VERSION_LESS\" \"3\"  Unknown arguments specified ```it's quite a misleading information that user would consider it's a syntax error or cmake version problem. This PR add a check to ensure `PYTHONLIBS_VERSION_STRING` exist before using.",
    "Number of deleted lines": 0,
    "Deleted lines": "\n  # Seed PYTHON_INCLUDE_DIR and PYTHON_LIBRARY to be consistent with the\n  # executable that we already found (if we didn't actually find an executable\n  # then these will just use \"python\", but at least they'll be consistent with\n  # each other).\n  if(NOT PYTHON_INCLUDE_DIR)\n    # TODO: Verify that sysconfig isn't inaccurate\n    pycmd_no_exit(_py_inc _exitcode \"import sysconfig; print(sysconfig.get_path('include'))\")\n    if(\"${_exitcode}\" EQUAL 0 AND IS_DIRECTORY \"${_py_inc}\")\n      set(PYTHON_INCLUDE_DIR \"${_py_inc}\")\n      message(STATUS \"Setting Python's include dir to ${_py_inc} from sysconfig\")\n    else()\n      message(WARNING \"Could not set Python's include dir to ${_py_inc} from sysconfig\")\n    endif()\n  endif(NOT PYTHON_INCLUDE_DIR)\n\n  if(NOT PYTHON_LIBRARY)\n    pycmd_no_exit(_py_lib _exitcode \"import sysconfig; print(sysconfig.get_path('stdlib'))\")\n    if(\"${_exitcode}\" EQUAL 0 AND EXISTS \"${_py_lib}\" AND EXISTS \"${_py_lib}\")\n      set(PYTHON_LIBRARY \"${_py_lib}\")\n      if(MSVC)\n        string(REPLACE \"Lib\" \"libs\" _py_static_lib ${_py_lib})\n        link_directories(${_py_static_lib})\n      endif()\n      message(STATUS \"Setting Python's library to ${PYTHON_LIBRARY}\")\n    endif()\n  endif(NOT PYTHON_LIBRARY)\n\n  # These should fill in the rest of the variables, like versions, but resepct\n  # the variables we set above\n  set(Python_ADDITIONAL_VERSIONS ${PYTHON_VERSION} 3.8)\n  find_package(PythonInterp 3.0)\n  find_package(PythonLibs 3.0)\n\n  if(${PYTHONLIBS_VERSION_STRING} VERSION_LESS 3)\n    message(FATAL_ERROR\n      \"Found Python libraries version ${PYTHONLIBS_VERSION_STRING}. Python 2 has reached end-of-life and is no longer supported by PyTorch.\")\n  endif()\n  if(${PYTHONLIBS_VERSION_STRING} VERSION_LESS 3.8)\n    message(FATAL_ERROR\n      \"Found Python libraries version ${PYTHONLIBS_VERSION_STRING}. Python < 3.8 is no longer supported by PyTorch.\")\n  endif()\n\n  # When building pytorch, we pass this in directly from setup.py, and\n  # don't want to overwrite it because we trust python more than cmake\n  if(NUMPY_INCLUDE_DIR)\n    set(NUMPY_FOUND ON)\n  elseif(USE_NUMPY)\n    find_package(NumPy)\n    if(NOT NUMPY_FOUND)\n      message(WARNING \"NumPy could not be found. Not building with NumPy. Suppress this warning with -DUSE_NUMPY=OFF\")\n    endif()\n  endif()\n\n  if(PYTHONINTERP_FOUND AND PYTHONLIBS_FOUND)\n    add_library(python::python INTERFACE IMPORTED)\n    target_include_directories(python::python SYSTEM INTERFACE ${PYTHON_INCLUDE_DIRS})\n    if(WIN32)\n      target_link_libraries(python::python INTERFACE ${PYTHON_LIBRARIES})\n    endif()\n\n    caffe2_update_option(USE_NUMPY OFF)\n    if(NUMPY_FOUND)\n      caffe2_update_option(USE_NUMPY ON)\n      add_library(numpy::numpy INTERFACE IMPORTED)\n      target_include_directories(numpy::numpy SYSTEM INTERFACE ${NUMPY_INCLUDE_DIR})\n    endif()\n    # Observers are required in the python build\n    caffe2_update_option(USE_OBSERVERS ON)\n  else()"
},
{
    "Id": 64,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/577e90ae9bf257040acb68da3626d9a64d07bf7a",
    "Violation": "misleading",
    "Bug report": " Improve error message for missing ops. The current error message is ill formed",
    "Number of deleted lines": 2,
    "Deleted lines": "}\n\nstd::string operator_str(const c10::OperatorName& opname) {\n  std::string result = opname.name;\n  if (!opname.overload_name.empty()) {\n    result += \".\" + opname.overload_name;\n  }\n  return result;\n}\n\nbool Function::initialize_operators(bool should_check_operators) {\n  if (code_.initialized) {\n    return true;\n  }\n  std::unordered_set<std::string> unsupported_op_names;\n  code_.operators_.resize(code_.op_names_.size());\n  bool all_ops_supported = true;\n  for (int i = 0; i < code_.op_names_.size(); i++) {\n    const auto& opname = code_.op_names_[i];\n    int num_args = code_.operator_input_sizes_[i];\n    c10::optional<int> num_specified_args =\n        num_args < 0 ? c10::nullopt : c10::optional<int>(num_args);\n    auto func = makeOperatorFunction(opname, num_specified_args);\n    if (!func.has_value()) {\n      unsupported_op_names.insert(operator_str(opname));\n      all_ops_supported = false;\n      break;\n    } else {\n      code_.operators_[i] = *func;\n    }\n  }\n  if (should_check_operators) {\n    TORCH_CHECK(\n        unsupported_op_names.empty(),\n        \"Following ops cannot be found. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/\",\n        c10::Join(\", \", unsupported_op_names));\n  }\n  code_.initialized = all_ops_supported;\n  return all_ops_supported;\n}\n\nvoid Function::append_constant(const c10::IValue& constant) {\n  code_.constants_.push_back(constant);\n}\n\nvoid Function::append_type(const at::TypePtr& type) {\n  code_.types_.push_back(type);\n}\n\nvoid Function::append_function(mobile::Function& function) {\n  code_.functions_.push_back(&function);\n}\n\nvoid Function::set_register_size(size_t size) {\n  code_.register_size_ = size;\n}\n\nint64_t Function::get_debug_handle(size_t pc) const {\n  TORCH_CHECK(\n      pc < code_.debug_handles_.size(),\n      \"Module debug info index out of boundary.\");\n  return code_.debug_handles_[pc];\n}\n\ntorch::jit::Function& Function::setSchema(c10::FunctionSchema schema) {\n  schema_ = std::move(schema);\n  return *this;\n}\n\nbool Function::hasSchema() const {\n  return schema_.has_value();\n}"
},
{
    "Id": 65,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/22044c6f7cbdafdd340714bbe220b621e1927826",
    "Violation": "misleading",
    "Bug report": "Use TORCH_CHECK instead of AT_ASSERT in torch::cuda::gather(). The error message produced by AT_ASSERT() in gather() encouraged users to file a bug report (\"please report a bug to PyTorch...\"). The assertion should be a regular argument check since it can be triggered by passing tensors with different dimensionality, e.g. `torch.cuda.comm.gather([torch.rand(1, device='cuda'), torch.rand(1, 1, device='cuda')])`.",
    "Number of deleted lines": 1,
    "Deleted lines": "    const auto device_index = static_cast<int16_t>(devices[chunk]);\n    if (streams && (*streams)[chunk]) {\n      TORCH_CHECK(\n          (*streams)[chunk]->device_index() == device_index,\n          \"Expected the device associated with the stream at index \",\n          chunk, \" (was \", (*streams)[chunk]->device_index(), \") \",\n          \"to match the device supplied at that index \",\n          \"(expected \", device_index, \")\");\n      cuda_guard.reset_stream(*(*streams)[chunk]);\n    }\n    chunks[chunk] =\n        chunks[chunk].to(\n            {at::DeviceType::CUDA, device_index},\n            /*non_blocking=*/true,\n            /*copy=*/false,\n            /*memory_format=*/at::MemoryFormat::Preserve);\n  }\n  return chunks;\n}\n\nat::Tensor gather(\n    at::TensorList tensors,\n    int64_t dim,\n    c10::optional<int32_t> destination_index) {\n  TORCH_CHECK(!tensors.empty(), \"Expected at least one tensor to gather from\");\n  at::Tensor result;\n  int64_t total_size = 0;\n  auto& first = tensors.front();\n  const auto first_size = first.sizes();\n  std::vector<int64_t> expected_size(first_size.begin(), first_size.end());\n  bool all_channels_last = true;\n  for (const auto& tensor : tensors) {\n    TORCH_CHECK(\n        tensor.is_cuda(), \"Gather expects all inputs to have CUDA type\");\n    AT_ASSERT(tensor.ndimension() == static_cast<int64_t>(expected_size.size()));\n    expected_size[dim] = tensor.size(dim);\n    for (size_t dimension = 0; dimension < expected_size.size(); ++dimension) {\n      TORCH_CHECK(\n          expected_size[dimension] == tensor.size(dimension),\n          \"Gather got an input of invalid size: got \",\n          tensor.sizes(), \", but expected \", at::IntArrayRef(expected_size));\n    }\n    total_size += tensor.size(dim);\n    all_channels_last = all_channels_last &&\n        tensor.suggest_memory_format() == MemoryFormat::ChannelsLast;\n  }\n  expected_size[dim] = total_size;\n  at::Device device(at::DeviceType::CPU);\n  if (!destination_index || *destination_index != -1) {\n    device = at::Device(at::DeviceType::CUDA, destination_index ? *destination_index : -1);\n  }\n\n  auto memory_format = MemoryFormat::Contiguous;\n  if (all_channels_last) {\n    memory_format = MemoryFormat::ChannelsLast;\n  }\n  result =\n      at::empty(expected_size, first.options().device(device), memory_format);\n\n  int64_t chunk_start = 0;\n  for (const auto& tensor : tensors) {\n    result.narrow(dim, chunk_start, tensor.size(dim))\n        .copy_(tensor, /*non_blocking=*/true);\n    chunk_start += tensor.size(dim);\n  }\n  return result;\n}\n}} // namespace torch::cuda\n"
},
{
    "Id": 66,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc0d68a1ee3800ed4024762d018f85256e80f5ad",
    "Violation": "misleading",
    "Bug report": "Print out interface mismatch for prim::ModuleDictIndex. This commit augments the module interface subtyping check that is done before the emission of the `prim::ModuleDictIndex` operator so that the error message that is printed if the subtyping check fails provides more information on which methods do not match.",
    "Number of deleted lines": 2,
    "Deleted lines": "  }\n  throw ErrorReport(loc)\n      << \"Only ModuleList or Sequential modules can be used as tuple\";\n}\n\nSugaredValuePtr ModuleValue::getitem(\n    const SourceRange& loc,\n    Function& m,\n    Value* idx,\n    TypePtr type_hint) {\n  if (concreteType_->getIterableModuleKind() == IterableModuleKind::LIST) {\n    return getSugaredDict(loc, m)->getModules()->getitem(\n        loc, m, idx, type_hint);\n  } else if (\n      concreteType_->getIterableModuleKind() == IterableModuleKind::DICT) {\n    if (auto ivalue = toIValue(idx)) {\n      auto sd = getSugaredDict(loc, m);\n      auto idx_str = ivalue->toStringRef();\n      auto keys_iter = sd->keys_;\n      auto module_values_iter = sd->modules_;\n      for (size_t i = 0; i < keys_iter->tup_.size(); ++i) {\n        auto key = keys_iter->tup_.at(i);\n        auto key_str = toIValue(key->asValue(loc, m))->toStringRef();\n        if (key_str == idx_str) {\n          return module_values_iter->tup_.at(i);\n        }\n      }\n      throw ErrorReport(loc) << \"Key Error, \" << idx_str;\n    } else if (type_hint) {\n      // Check that all submodules comply with the type hint.\n      const auto& self_type = concreteType_->getJitType()->expect<ClassType>();\n      for (size_t i = 0; i < self_type->numAttributes(); ++i) {\n        const auto& attr_type = self_type->getAttribute(i);\n        if (attr_type->is_module()) {\n          if (!attr_type->isSubtypeOf(type_hint)) {\n            auto loc = self_->node()->sourceRange();\n            throw ErrorReport(loc)\n                << \"Attribute \" << self_type->getAttributeName(i)\n                << \" is not of annotated type \" << type_hint->annotation_str();\n          }\n        }\n      }\n\n      // Emit a prim::ModuleDictIndex operator. This is needed because it's\n      // difficult to construct a dict in the graph representing the ModuleDict\n      // and use aten::__getitem__ ops to index into it because any call to\n      // ModuleDict.setAttr would invalidate that emitted dict.\n      auto graph = m.graph();\n      auto* getitem_node =\n          graph->insertNode(graph->create(prim::ModuleDictIndex, {self_, idx}));\n      getitem_node->output(0)->setType(type_hint);\n      return std::make_shared<SimpleValue>(getitem_node->output(0));\n    }\n    throw ErrorReport(loc)\n        << \"Unable to extract string literal index. \"\n        << \"ModuleDict indexing is only supported with string literals. \"\n        << \"Enumeration of ModuleDict is supported, e.g. 'for k, v in self.items(): ...'\";\n  }\n  throw ErrorReport(loc)\n      << \"Only ModuleList, Sequential, and ModuleDict modules are subscriptable\";\n}\n\nvoid checkInterface(\n    const SourceRange& loc,\n    Function& m,\n    std::shared_ptr<ModuleValue> self,\n    const std::string& field) {\n  if (self->asValue(loc, m)->type()->cast<InterfaceType>()) {\n    throw ErrorReport(loc)\n        << \"Could not compile \" << field\n        << \"() because module is an interface type. Please file issue.\";\n  }\n}\n\nvoid recurseThroughNestedModules("
},
{
    "Id": 67,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c9548176965557a76526ba0db23ff5c9facd3e97",
    "Violation": "misleading",
    "Bug report": "print matrix dims in torch cuda matrix multiply error.  trying to improve the error message for torch matrix multiply dimension mismatch",
    "Number of deleted lines": 1,
    "Deleted lines": "    ld_tensor = tensor_strides[fast_dim];\n  } else {\n    transpose_tensor = !transpose_result;\n    // gemm call requires leading dimension and stride parameters to be non-zero\n    bool is_stride_non_zero = tensor.stride(1) != 0 && tensor.stride(2) != 0;\n    if (tensor.is_contiguous() && is_stride_non_zero) {\n      tensor_ = tensor;\n    } else {\n      tensor_ = tensor.clone(at::MemoryFormat::Contiguous);\n    }\n    ld_tensor = tensor_.stride(1);\n  }\n\n  return tensor_;\n}\n\nnamespace {\n\nTensor& addmm_out_cuda_impl(Tensor& result, const Tensor& self, const Tensor& mat1, const Tensor& mat2, Scalar beta, Scalar alpha) {\n  TORCH_CHECK(mat1.dim() == 2 && mat2.dim() == 2, \"tensors must be 2-D\");\n\n  TensorArg args[]{{result, \"out\", 0}, {self, \"self\", 1}, {mat1, \"mat1\", 2}, {mat2, \"mat2\", 3}};\n  checkAllSameGPU(\"addmm\", args);\n\n  Tensor self_;\n  if (&result != &self) {\n    std::tie(self_) = expand_size(self, {mat1.size(0), mat2.size(1)}, \"addmm\");\n  } else {\n    self_ = self;\n  }\n\n  IntArrayRef mat1_sizes = mat1.sizes();\n  IntArrayRef mat2_sizes = mat2.sizes();\n  IntArrayRef self__sizes = self_.sizes();\n  TORCH_CHECK(mat1_sizes[1] == mat2_sizes[0], \"mat1 dim 1 must match mat2 dim 0\");\n  TORCH_CHECK(self__sizes[0] == mat1_sizes[0], \"self_ dim 0 must match mat1 dim 0\");\n  TORCH_CHECK(self__sizes[1] == mat2_sizes[1], \"self_ dim 1 must match mat2 dim 1\");\n\n  if (&result != &self) {\n    at::native::resize_as_(result, self_);\n    if (beta.toComplexDouble() != 0.0) {\n      at::native::copy_(result, self_);\n    }\n  }\n\n  TORCH_CHECK(result.dim() == 2 && self_.dim() == 2, \"tensors must be 2-D\");\n\n  IntArrayRef result_sizes = result.sizes();\n  if ((result_sizes[0] == 0) || (result_sizes[1] == 0)) {\n    return result;\n  }\n\n  bool transpose_result;\n  Tensor result_ = prepare_matrix_for_cublas(result, transpose_result);\n  bool transpose_mat1;\n  bool transpose_mat2;\n  Tensor mat1_ = transpose_result ? mat2 : mat1;\n  Tensor mat2_ = transpose_result ? mat1 : mat2;\n  mat1_ = prepare_matrix_for_cublas(mat1_, transpose_mat1);\n  mat2_ = prepare_matrix_for_cublas(mat2_, transpose_mat2);\n\n  if (transpose_result) {\n    transpose_mat1 = !transpose_mat1;\n    transpose_mat2 = !transpose_mat2;\n    mat1_sizes = mat1_.sizes();\n    mat2_sizes = mat2_.sizes();\n  }\n\n  int64_t m = mat1_sizes[transpose_result ? 1 : 0];\n  int64_t k = mat1_sizes[transpose_result ? 0 : 1];\n  int64_t n = mat2_sizes[transpose_result ? 0 : 1];"
},
{
    "Id": 68,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/93256617c8622760181dacf03c41cc0577ac0ea6",
    "Violation": "misleading",
    "Bug report": "corrected messages for check of default options. Modified messages in the check of default options for the Adam optimizer.",
    "Number of deleted lines": 3,
    "Deleted lines": "  AdamOptions(double lr = 1e-3);\n  TORCH_ARG(double, lr) = 1e-3;\n  typedef std::tuple<double, double> betas_t;\n  TORCH_ARG(betas_t, betas) = std::make_tuple(0.9, 0.999);\n  TORCH_ARG(double, eps) = 1e-8;\n  TORCH_ARG(double, weight_decay) = 0;\n  TORCH_ARG(bool, amsgrad) = false;\npublic:\n  void serialize(torch::serialize::InputArchive& archive) override;\n  void serialize(torch::serialize::OutputArchive& archive) const override;\n  TORCH_API friend bool operator==(const AdamOptions& lhs, const AdamOptions& rhs);\n  ~AdamOptions() = default;\n};\n\nstruct TORCH_API AdamParamState : public OptimizerCloneableParamState<AdamParamState> {\n  TORCH_ARG(int64_t, step) = 0;\n  TORCH_ARG(torch::Tensor, exp_avg);\n  TORCH_ARG(torch::Tensor, exp_avg_sq);\n  TORCH_ARG(torch::Tensor, max_exp_avg_sq) = {};\n\npublic:\n  void serialize(torch::serialize::InputArchive& archive) override;\n  void serialize(torch::serialize::OutputArchive& archive) const override;\n  TORCH_API friend bool operator==(const AdamParamState& lhs, const AdamParamState& rhs);\n  ~AdamParamState() = default;\n};\n\nclass TORCH_API Adam : public Optimizer {\n public:\n   explicit Adam(std::vector<OptimizerParamGroup> param_groups,\n       AdamOptions defaults = {}) : Optimizer(std::move(param_groups), std::make_unique<AdamOptions>(defaults)) {\n     TORCH_CHECK(defaults.lr() >= 0, \"Invalid learning rate: \", defaults.lr());\n     TORCH_CHECK(defaults.eps() >= 0, \"Invalid epsilon value: \", defaults.eps());\n     auto betas = defaults.betas();\n     TORCH_CHECK(std::get<0>(betas) >= 0, \"Invalid learning rate: \", std::get<0>(betas));\n     TORCH_CHECK(std::get<1>(betas) >= 0, \"Invalid learning rate: \", std::get<1>(betas));\n     TORCH_CHECK(defaults.weight_decay() >= 0, \"Invalid learning rate: \", defaults.weight_decay());\n   }\n   explicit Adam(\n       std::vector<Tensor> params,\n       AdamOptions defaults = {}) : Adam({std::move(OptimizerParamGroup(params))}, defaults) {}\n\n  torch::Tensor step(LossClosure closure = nullptr) override;\n  void save(serialize::OutputArchive& archive) const override;\n  void load(serialize::InputArchive& archive) override;\n\n private:\n  template <typename Self, typename Archive>\n  static void serialize(Self& self, Archive& archive) {\n    _TORCH_OPTIM_SERIALIZE_WITH_TEMPLATE_ARG(Adam);\n  }\n};\n} // namespace optim\n} // namespace torch\n"
},
{
    "Id": 69,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/99f06c0cc2a907d8fbf613768356838548f1f8c0",
    "Violation": "misleading",
    "Bug report": "update errors to be more descriptive we call `_check_single_tensor` and `_check_tensor_list` as validation but don't print out the param types that were invalid",
    "Number of deleted lines": 5,
    "Deleted lines": "# TODO: remove this once the ecosystem moves away from it.\ndef _get_global_rank(group, rank) -> int:\n    \"\"\"Use get_global_rank as this method is deprecated.\"\"\"\n    warnings.warn(\n        \"torch.distributed.distributed_c10d._get_global_rank is deprecated \"\n        \"please use torch.distributed.distributed_c10d.get_global_rank instead\"\n    )\n    return get_global_rank(group, rank)\n\n\ndef get_process_group_ranks(group: ProcessGroup) -> List[int]:\n    \"\"\"\n    Get all ranks associated with ``group``.\n\n    Args:\n        group (ProcessGroup): ProcessGroup to get all ranks from.\n\n    Returns:\n        List of global ranks ordered by group rank.\n    \"\"\"\n    return list(_world.pg_group_ranks[group].keys())\n\ndef _get_group_size(group) -> int:\n    \"\"\"Get a given group's world size.\"\"\"\n    if group is GroupMember.WORLD or group is None:\n        default_pg = _get_default_group()\n        return default_pg.size()\n    return group.size()\n\n\ndef _check_single_tensor(param, param_name) -> None:\n    \"\"\"Check that the parameter ``param_name`` is a single tensor.\"\"\"\n    if not isinstance(param, torch.Tensor):\n        raise TypeError(\n            f\"Invalid function argument. Expected parameter `{param_name}` to be of type torch.Tensor.\"\n        )\n\n\ndef _check_tensor_list(param, param_name) -> None:\n    \"\"\"Check that the parameter ``param_name`` is a list of tensors.\"\"\"\n    if not isinstance(param, list) or not all(\n        isinstance(p, torch.Tensor) for p in param\n    ):\n        raise TypeError(\n            f\"Invalid function argument. Expected parameter `{param_name}` to be of type List[torch.Tensor].\"\n        )\n\ndef _as_iterable(obj) -> collections.abc.Iterable:\n    return obj if isinstance(obj, list) else (obj,)\n\ndef _ensure_all_tensors_same_dtype(*tensors) -> None:\n    last_dtype = None\n    for tensor in itertools.chain(*map(_as_iterable, tensors)):\n        tensor_dtype = tensor.dtype\n        # Mixing complex and its element type is allowed\n        if tensor_dtype.is_complex:\n            tensor_dtype = torch.float32 if tensor_dtype == torch.complex64 else torch.complex128\n\n        if last_dtype is None:\n            last_dtype = tensor_dtype\n        else:\n            if last_dtype != tensor_dtype:\n                raise ValueError(\n                    \"Invalid usage of tensors with different dtypes\"\n                    f\"Found {last_dtype} and  {tensor.dtype}\"\n                )\n\n\ndef _check_op(op) -> None:\n    \"\"\"Check that the ``op`` is either isend or irecv.\"\"\"\n    if op not in [isend, irecv]:\n        raise ValueError(\n            \"Invalid ``op``. Expected ``op`` \"\n            \"to be of type ``torch.distributed.isend`` or \"\n            \"``torch.distributed.irecv``.\"\n        )\n\n\ndef _check_p2p_op_list(p2p_op_list) -> None:\n    \"\"\"\n    Check that the ``p2p_op_list`` is a list of P2POp instances.\n\n    Also, check that all ops use the same group."
},
{
    "Id": 70,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9a9eadacc6ac3b734a6d607ae6f63ec1a0d1438d",
    "Violation": "missing",
    "Bug report": "explicitly check device for grid_sampler",
    "Number of deleted lines": 0,
    "Deleted lines": "\n\ndef grid_sampler(input, grid, padding_mode):\n    if cudnn.is_acceptable(input.data) \\\n       and padding_mode == 'zeros' \\\n       and input.dim() == 4 \\\n       and input.size(1) <= 1024:  # as of cudnn 7102, will not work for larger than 1024\n        return torch.cudnn_grid_sampler(input, grid)\n    else:\n        return GridSampler.apply(input, grid, padding_mode)\n\n\ndef affine_grid_generator(theta, size):\n    if theta.data.is_cuda:\n        if not cudnn.enabled:\n            raise RuntimeError(\"AffineGridGenerator needs CuDNN for \"\n                               \"processing CUDA inputs, but CuDNN is not enabled\")\n        if not cudnn.is_acceptable(theta.data):\n            raise RuntimeError(\"AffineGridGenerator generator theta not acceptable for CuDNN\")\n        N, C, H, W = size\n        return torch.cudnn_affine_grid_generator(theta, N, C, H, W)\n    else:\n        return AffineGridGenerator.apply(theta, size)\n\n\n# TODO: Port these completely into C++\n\n\nclass GridSampler(Function):\n\n    @staticmethod\n    def forward(ctx, input, grid, padding_mode='zeros'):\n        ctx.save_for_backward(input, grid)\n\n        if padding_mode == 'zeros':\n            ctx.padding_mode = MODE_ZEROS\n        elif padding_mode == 'border':\n            ctx.padding_mode = MODE_BORDER\n        else:\n            raise ValueError(\"padding_mode needs to be 'zeros' or 'border', but got {}\".format(padding_mode))\n\n        grid_sz = grid.size()\n        backend = type2backend[input.type()]\n        if input.dim() == 4:\n            output = input.new(grid_sz[0], input.size(1), grid_sz[1], grid_sz[2])\n            backend.SpatialGridSamplerBilinear_updateOutput(backend.library_state, input, grid,\n                                                            output, ctx.padding_mode)\n        elif input.dim() == 5:\n            output = input.new(grid_sz[0], input.size(1), grid_sz[1], grid_sz[2], grid_sz[3])\n            backend.VolumetricGridSamplerBilinear_updateOutput(backend.library_state, input, grid,\n                                                               output, ctx.padding_mode)\n        else:\n            raise ValueError(\"input has to be 4d or 5d but got input of shape: {}\".format(input.shape))\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        input, grid = ctx.saved_tensors\n        padding_mode = ctx.padding_mode\n\n        backend = type2backend[input.type()]\n        grad_input = input.new(input.size())\n        grad_grid = grid.new(grid.size())\n        if input.dim() == 4:\n            backend.SpatialGridSamplerBilinear_updateGradInput(\n                backend.library_state, input, grad_input,\n                grid, grad_grid, grad_output, padding_mode)\n        elif input.dim() == 5:\n            backend.VolumetricGridSamplerBilinear_updateGradInput("
},
{
    "Id": 71,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/157d478a30f27fd9d866c1235841721a559c8d0b",
    "Violation": "improper",
    "Bug report": "Fix omission of shape in size check in index. ",
    "Number of deleted lines": 1,
    "Deleted lines": "        Qt_shape[-1] = m\n        Q = input.new_empty(Qt_shape)\n        Q.transpose_(-2, -1)\n    else:\n        Q = input.new_empty(0)\n    Rt_shape = list(input.size())\n    Rt_shape[-2] = n\n    Rt_shape[-1] = mn if reduced_mode or not compute_q else m\n    R = input.new_empty(Rt_shape)\n    R.transpose_(-2, -1)\n    return (Q, R)\n\n@torch.library.impl(meta_lib, \"index.Tensor\")\ndef meta_index_Tensor(self, indices):\n    check(indices, lambda: \"at least one index must be provided\")\n    # aten::index is the internal advanced indexing implementation\n    # checkIndexTensorTypes and expandTensors\n    result: List[Optional[Tensor]] = []\n    for i, index in enumerate(indices):\n        if index is not None:\n            check(\n                index.dtype in [torch.long, torch.int8, torch.bool],\n                lambda: \"tensors used as indices must be long, byte or bool tensors\"\n            )\n            if index.dtype in [torch.int8, torch.bool]:\n                nonzero = index.nonzero()\n                k = len(result)\n                check(\n                    k + index.ndim <= self.ndim,\n                    lambda: f\"too many indices for tensor of dimension {self.ndim}\",\n                    IndexError\n                )\n                for j in range(index.ndim):\n                    check(\n                        index[j] <= self.shape[k + j],\n                        lambda: f\"The shape of the mask {index.shape} at index {i} \"\n                                f\"does not match the shape of the indexed tensor {self.shape} at index {k + j}\",\n                        IndexError\n                    )\n                    result.append(nonzero.select(1, j))\n            else:\n                result.append(index)\n        else:\n            result.append(index)\n    indices = result\n    check(len(indices) <= self.ndim, lambda: f\"too many indices for tensor of dimension {self.ndim} (got {len(indices)})\")\n    # expand_outplace\n    import torch._refs as refs  # avoid import cycle in mypy\n    indices = list(refs._maybe_broadcast(*indices))\n    # add missing null tensors\n    while len(indices) < self.ndim:\n        indices.append(None)\n\n    # hasContiguousSubspace\n    #   true if all non-null tensors are adjacent\n    # See:\n    # https://numpy.org/doc/stable/user/basics.indexing.html#combining-advanced-and-basic-indexing\n    # https://stackoverflow.com/questions/53841497/why-does-numpy-mixed-basic-advanced-indexing-depend-on-slice-adjacency\n    state = 0\n    has_contiguous_subspace = False\n    for index in indices:\n        if state == 0:\n            if index is not None:\n                state = 1\n        elif state == 1:\n            if index is None:\n                state = 2\n        else:\n            if index is not None:\n                break\n    else:"
},
{
    "Id": 72,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f14887a63f83b931df9fc5d841c7d3829141ff58",
    "Violation": "missing",
    "Bug report": "check for exact shape match before loading. Use RuntimeError instead of ValueError to keep it consistent with other errors",
    "Number of deleted lines": 0,
    "Deleted lines": "        this module, but not its descendants. This is called on every submodule\n        in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n        module in input :attr:`state_dict` is at ``state_dict._metadata[prefix]``.\n        Subclasses can achieve class-specific backward compatible loading using\n        the version number at ``state_dict._metadata[prefix][\"version\"]``.\n\n        .. note::\n            :attr:`state_dict` is not the same object as the input\n            :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n            it can be modified.\n\n        Arguments:\n            state_dict (dict): a dict containing parameters and\n                persistent buffers.\n            prefix (str): the prefix for parameters and buffers used in this\n                module\n            strict (bool): whether to strictly enforce that the keys in\n                :attr:`state_dict` with :attr:`prefix` match the names of\n                parameters and buffers in this module\n            missing_keys (list of str): if ``strict=False``, add missing keys to\n                this list\n            unexpected_keys (list of str): if ``strict=False``, add unexpected\n                keys to this list\n            error_msgs (list of str): error messages should be added to this\n                list, and will be reported together in\n                :meth:`~torch.nn.Module.load_state_dict`\n        \"\"\"\n        local_name_params = itertools.chain(self._parameters.items(), self._buffers.items())\n        local_state = {k: v.data for k, v in local_name_params if v is not None}\n\n        for name, param in local_state.items():\n            key = prefix + name\n            if key in state_dict:\n                input_param = state_dict[key]\n                if isinstance(input_param, Parameter):\n                    # backwards compatibility for serialized parameters\n                    input_param = input_param.data\n                try:\n                    param.copy_(input_param)\n                except Exception:\n                    error_msgs.append('While copying the parameter named \"{}\", '\n                                      'whose dimensions in the model are {} and '\n                                      'whose dimensions in the checkpoint are {}.'\n                                      .format(key, param.size(), input_param.size()))\n            elif strict:\n                missing_keys.append(key)\n\n        if strict:\n            for key, input_param in state_dict.items():\n                if key.startswith(prefix):\n                    input_name = key[len(prefix):]\n                    input_name = input_name.split('.', 1)[0]  # get the name of param/buffer/child\n                    if input_name not in self._modules and input_name not in local_state:\n                        unexpected_keys.append(key)\n\n    def load_state_dict(self, state_dict, strict=True):\n        r\"\"\"Copies parameters and buffers from :attr:`state_dict` into\n        this module and its descendants. If :attr:`strict` is ``True``, then\n        the keys of :attr:`state_dict` must exactly match the keys returned\n        by this module's :meth:`~torch.nn.Module.state_dict` function.\n\n        Arguments:\n            state_dict (dict): a dict containing parameters and\n                persistent buffers.\n            strict (bool, optional): whether to strictly enforce that the keys\n                in :attr:`state_dict` match the keys returned by this module's\n                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n        \"\"\"\n        missing_keys = []\n        unexpected_keys = []"
},
{
    "Id": 73,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/02e2158e754bafda46e663052c838aeb6ab6b560",
    "Violation": "missing",
    "Bug report": " Fix for out of bounds read in mobile interpreter INTERFACE_CALL opcode handler. The INTERFACE_CALL opcode for the mobile TorchScript interpreter contained an out of bounds read issue leading to memory corruption. This change adds an explicit check that the number of inputs passed to the format method called when handling the INTERFACE_CALL opcode is a valid and within bounds of the stack.",
    "Number of deleted lines": 0,
    "Deleted lines": "              mobile_debug_info->setOpIdx(pc);\n            }\n          }\n          if (inst.X < 0 ||\n              static_cast<size_t>(inst.X) >= code.op_names_.size() ||\n              static_cast<size_t>(inst.X) >= code.operators_.size()) {\n            TORCH_CHECK(false, \"Can't load op with index: \", inst.X);\n          }\n          RECORD_EDGE_SCOPE_WITH_DEBUG_HANDLE_AND_INPUTS(\n              code.op_names_[inst.X].name, debug_handle, stack);\n          code.operators_[inst.X](stack);\n          frame.step();\n        } break;\n        case OPN: {\n          if (inst.X < 0 ||\n              static_cast<size_t>(inst.X) >= code.op_names_.size() ||\n              static_cast<size_t>(inst.X) >= code.operators_.size()) {\n            TORCH_CHECK(false, \"Can't load op with index: \", inst.X);\n          }\n          stack.emplace_back(inst.N);\n          RECORD_EDGE_SCOPE_WITH_DEBUG_HANDLE_AND_INPUTS(\n              code.op_names_[inst.X].name, debug_handle, stack);\n          code.operators_[inst.X](stack);\n          frame.step();\n        } break;\n        case CALL: {\n          auto& function = *frame.getCode().functions_.at(inst.X);\n          callFunction(function, stack);\n        } break;\n        case INTERFACE_CALL: {\n          if (inst.X < 0 ||\n              static_cast<size_t>(inst.X) >= code.constants_.size()) {\n            TORCH_CHECK(false, \"Can't load constant with index: \", inst.X);\n          }\n          torch::jit::Function& method =\n              peek(stack, 0, inst.N)\n                  .toObject()\n                  ->type()\n                  ->getMethod(code.constants_[inst.X].toStringRef());\n          RECORD_EDGE_SCOPE_WITH_DEBUG_HANDLE_AND_INPUTS(\n              method.name(), debug_handle, stack);\n          callFunction(method, stack);\n        } break;\n        case LOAD:\n          stack.emplace_back(reg(inst.X));\n          frame.step();\n          break;\n        case MOVE:\n          stack.emplace_back(std::move(reg(inst.X)));\n          frame.step();\n          break;\n        case STORE:\n          reg(inst.X) = pop(stack);\n          frame.step();\n          break;\n        case STOREN:\n          for (size_t i = inst.N; i > 0; --i) {\n            reg(inst.X + i - 1) = pop(stack);\n          }\n          frame.step();\n          break;\n        case DROP:\n          pop(stack);\n          frame.step();\n          break;\n        case DROPR:\n          reg(inst.X) = IValue();\n          frame.step();\n          break;\n        case LOADC:"
},
{
    "Id": 74,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4d07428edee863e7f5920f0672957a9711a9f0b5",
    "Violation": "missing",
    "Bug report": "Fix for out of bounds read in mobile interpreter FORMAT opcode handler. Summary: The FORMAT opcode for the mobile TorchScript interpreter contained an out of bounds read issue leading to memory corruption. This change adds an explicit check that the number of inputs passed to the format method called when handling the FORMAT opcode is a valid and within bounds of the stack.",
    "Number of deleted lines": 0,
    "Deleted lines": "          \"%\",\n          key,\n          \" requires an int or char for formatting, but got \",\n          ival.tagKind());\n      if (ival.isInt()) {\n        ss << static_cast<char>(ival.toInt());\n      } else {\n        ss << ival.toStringRef();\n      }\n      break;\n    case 's':\n      if (ival.isString()) {\n        ss << ival.toStringRef();\n      } else {\n        ss << ival;\n      }\n      break;\n    default:\n      TORCH_CHECK(\n          false,\n          \"The specifier %\",\n          key,\n          \" is not supported in TorchScript format strings\");\n  }\n}\n\n} // namespace\n\nvoid tupleUnpack(Stack& stack) {\n  auto tuple = pop(stack).toTuple();\n  stack.insert(stack.end(), tuple->elements().begin(), tuple->elements().end());\n}\n\nvoid format(Stack& stack, size_t num_inputs) {\n  // static const std::regex unsupported_options(\"\\\\{(.*?)\\\\}\");\n  auto format = peek(stack, 0, num_inputs).toStringRef();\n  // // Temporally comment out the warning message because of\n  // // \"StdRegexIsAwful\" internal Lint error, to prevent sev\n  // // of std::regex from PT mobile.\n  // if (std::regex_search(format, unsupported_options)) {\n  //   TORCH_WARN(\"Format options are not supported.\");\n  // }\n\n  auto args = last(stack, num_inputs - 1);\n  std::stringstream ss;\n  for (size_t begin = 0, used_args = 0; true; ++used_args) {\n    size_t loc = format.find(\"{}\", begin);\n    if (loc == std::string::npos) {\n      ss << format.substr(begin);\n      break;\n    }\n    ss << format.substr(begin, loc - begin);\n    if (used_args >= args.size()) {\n      AT_ERROR(\"Too few arguments for format string: \", format);\n    }\n    ss << args[used_args];\n    begin = loc + 2;\n  }\n\n  drop(stack, num_inputs);\n  push(stack, ss.str());\n}\n\nvoid einsum(Stack& stack, size_t num_inputs) {\n  TORCH_CHECK(\n      num_inputs >= 2,\n      \"einsum(): must specify the equation string and at least one operand, \",\n      \"or at least one operand and its subscripts list\");\n\n  const auto args = last(stack, num_inputs);"
},
{
    "Id": 75,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b3ace213f240dc0f0f2a738f825f46e0d0dffca4",
    "Violation": "missing",
    "Bug report": "The error occurs because there is not check in `deserialize_source` that `text_table_` size can be less than `fnameIndex`. To prevent the error the corresponding check must be located.",
    "Number of deleted lines": 0,
    "Deleted lines": "  c10::IValue serialize_source(const std::shared_ptr<Source>& s);\n  std::unordered_map<std::shared_ptr<Source>, c10::IValue> serialized_sources;\n\n  int64_t store_text_and_get_index(const std::string& text_view);\n\n  std::vector<c10::IValue> texts_;\n  std::unordered_map<c10::string_view, int64_t> text_to_idx_;\n};\n\nSourceRange SourceRangeDeserializer::deserialize(const c10::IValue& iv) {\n  const auto& tup_elems = iv.toTupleRef().elements();\n  TORCH_INTERNAL_ASSERT(tup_elems.size() == 3);\n  std::shared_ptr<Source> source_ = deserialize_source(tup_elems[0]);\n  int64_t start_ = tup_elems[1].toInt();\n  int64_t end_ = tup_elems[2].toInt();\n  return SourceRange(source_, start_, end_);\n}\n\nstd::shared_ptr<Source> SourceRangeDeserializer::deserialize_source(\n    const c10::IValue& iv) {\n  auto tup = iv.toTuple();\n  auto it = cached_sources.find(tup);\n  if (it != cached_sources.end()) {\n    return it->second;\n  }\n  std::shared_ptr<Source> source;\n  const auto& tup_elems = tup->elements();\n  TORCH_INTERNAL_ASSERT(tup_elems.size() == 3);\n  if (!text_table_.empty()) {\n    const auto& textIndex = tup_elems[0].toIntList();\n    int64_t fnameIndex = tup_elems[1].toInt();\n    int64_t starting_line_no_ = tup_elems[2].toInt();\n    c10::optional<std::string> filename = c10::nullopt;\n\n    filename = *text_table_[fnameIndex];\n\n    std::vector<c10::string_view> pieces;\n    std::vector<std::shared_ptr<std::string>> strs;\n\n    for (int64_t i : textIndex) {\n      pieces.emplace_back(*text_table_[i]);\n      strs.emplace_back(text_table_[i]);\n    }\n\n    StringCordView str_cord(std::move(pieces), std::move(strs));\n\n    source = std::make_shared<Source>(str_cord, filename, starting_line_no_);\n  } else {\n    std::string text_ = tup_elems[0].toStringRef();\n    c10::optional<std::string> filename_ =\n        tup_elems[1].toOptional<std::string>();\n    int64_t starting_line_no_ = tup_elems[2].toInt();\n    source = std::make_shared<Source>(\n        std::move(text_), std::move(filename_), starting_line_no_);\n  }\n  cached_sources[tup] = source;\n  return source;\n}\n\nc10::IValue SourceRangeSerializer::serialize(const SourceRange& sr) {\n  return c10::ivalue::Tuple::create(\n      serialize_source(sr.source()), (int64_t)sr.start(), (int64_t)sr.end());\n}\n\nint64_t SourceRangeSerializer::store_text_and_get_index(\n    const std::string& text_view) {\n  auto text_iter = text_to_idx_.find(text_view);\n  if (text_iter == text_to_idx_.end()) {\n    int64_t text_pos = static_cast<int64_t>(texts_.size());\n    texts_.emplace_back(text_view);"
},
{
    "Id": 76,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d8466964b348b6172317f70b8e52de02402bad54",
    "Violation": "missing",
    "Bug report": "Add range check to multi margin loss target ",
    "Number of deleted lines": 0,
    "Deleted lines": ""
},
{
    "Id": 77,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c22ac14969a863a00b5ebb04a3453610c7a27713",
    "Violation": "missing",
    "Bug report": "The diff sets the upper boundary on border element when presenting the error message. This is required in order to avoid unnecessary log contamination",
    "Number of deleted lines": 0,
    "Deleted lines": "    In the example above, trainer 1's failure (written into error.json) is\n    the root cause and should be reported to the scheduler's init process.\n    The torchelastic agent raises a ``ChildFailedError(\"trainer\", {1: \"trainer_1/error.json\"})``\n    upon detecting trainer 1's failure which would propagate the contents\n    of trainer 1's error file to the scheduler's init process.\n    \"\"\"\n\n    def __init__(self, name: str, failures: Dict[GlobalRank, ProcessFailure]):\n        self.name = name\n        self.failures = failures\n        assert (\n            self.failures\n        )  # does not make sense to create a ChildFaileError with no failures\n        super().__init__(self.format_msg())\n\n    def get_first_failure(self) -> Tuple[GlobalRank, ProcessFailure]:\n        rank = min(self.failures.keys(), key=lambda r: self.failures[r].timestamp)\n        return rank, self.failures[rank]\n\n    def format_msg(self, boarder_delim=\"*\", section_delim=\"=\"):\n        title = f\"  {self.name} FAILED  \"\n        root_rank, root_failure = self.get_first_failure()\n\n        root_failure_fmt: str = \"\"\n        other_failures_fmt: List[str] = []\n        width = len(title)\n        for idx, (rank, failure) in enumerate(self.failures.items()):\n            fmt, w = self._format_failure(idx, rank, failure)\n            width = max(width, w)\n            if rank == root_rank:\n                root_failure_fmt = fmt\n            else:\n                other_failures_fmt.append(fmt)\n\n        return Template(_MSG_FORMAT_TEMPLATE).substitute(\n            boarder=boarder_delim * width,\n            title=title.center(width),\n            section=section_delim * width,\n            root_failure=root_failure_fmt,\n            other_failures=\"\\n\".join(other_failures_fmt or [\"  <NO_OTHER_FAILURES>\"]),\n        )\n\n    def _format_failure(\n        self, idx: int, rank: int, failure: ProcessFailure\n    ) -> Tuple[str, int]:\n        fmt = Template(_FAILURE_FORMAT_TEMPLATE).substitute(\n            idx=idx,\n            time=failure.timestamp_isoformat(),\n            rank=rank,\n            local_rank=failure.local_rank,\n            exitcode=failure.exitcode,\n            pid=failure.pid,\n            error_file=failure.error_file,\n            message=failure.message,\n        )\n        width = 0\n        for line in fmt.split(\"\\n\"):\n            width = max(width, len(line))\n        return fmt, width\n\n\ndef _no_error_file_warning_msg(rank: int, failure: ProcessFailure) -> str:\n    msg = [\n        \"CHILD PROCESS FAILED WITH NO ERROR_FILE\",\n        f\"Child process {failure.pid} (local_rank {rank}) FAILED (exitcode {failure.exitcode})\",\n        f\"Error msg: {failure.message}\",\n        f\"Without writing an error file to {failure.error_file}.\",\n        \"While this DOES NOT affect the correctness of your application,\",\n        \"no trace information about the error will be available for inspection.\",\n        \"Consider decorating your top level entrypoint function with\","
},
{
    "Id": 78,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b7bb34d7625d95e5088638721dcc07c2bc5e2ade",
    "Violation": "missing",
    "Bug report": "[MPS] Add version check Use `instancesRespondToSelector:` to test the presence of `optimizationLevel` in `MPSGraphCompilationDescriptor`",
    "Number of deleted lines": 0,
    "Deleted lines": "at::Allocator* GetMPSAllocator(bool useSharedAllocator) {\n  return useSharedAllocator ? getMPSSharedAllocator() : GetAllocator(DeviceType::MPS);\n}\n\nbool is_available() {\n  return MPSDevice::getInstance()->device() != nil;\n}\n\n} // namespace mps\n} // namespace at\n"
},
{
    "Id": 79,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b1f08e7426a56a323e6928365918093b65aa4fb6",
    "Violation": "missing",
    "Bug report": " Call uncheckedSetDevice in ~InlineDeviceGuard only when device index are different. Setting device could be expensive, especially when a debugger is present. We should check the device are different before we set.",
    "Number of deleted lines": 2,
    "Deleted lines": "\n#include <c10/cuda/CUDAException.h>\n#include <c10/cuda/CUDAStream.h>\n#include <c10/cuda/CUDAFunctions.h>\n\n#include <cuda_runtime_api.h>\n\nnamespace c10 {\nnamespace cuda {\nnamespace impl {\n\nstruct CUDAGuardImpl final : public c10::impl::DeviceGuardImplInterface {\n  static constexpr DeviceType static_type = DeviceType::CUDA;\n\n  CUDAGuardImpl() {}\n  explicit CUDAGuardImpl(DeviceType t) {\n    TORCH_INTERNAL_ASSERT(t == DeviceType::CUDA);\n  }\n  DeviceType type() const override {\n    return DeviceType::CUDA;\n  }\n  Device exchangeDevice(Device d) const override {\n    TORCH_INTERNAL_ASSERT(d.type() == DeviceType::CUDA);\n    Device old_device = getDevice();\n    if (old_device.index() != d.index()) {\n      C10_CUDA_CHECK(cudaSetDevice(d.index()));\n    }\n    return old_device;\n  }\n  Device getDevice() const override {\n    int device;\n    C10_CUDA_CHECK(cudaGetDevice(&device));\n    return Device(DeviceType::CUDA, device);\n  }\n  void setDevice(Device d) const override {\n    TORCH_INTERNAL_ASSERT(d.type() == DeviceType::CUDA);\n    C10_CUDA_CHECK(cudaSetDevice(d.index()));\n  }\n  void uncheckedSetDevice(Device d) const noexcept override {\n    C10_CUDA_CHECK_WARN(cudaSetDevice(d.index()));\n  }\n  Stream getStream(Device d) const noexcept override {\n    return getCurrentCUDAStream(d.index()).unwrap();\n  }\n  Stream getDefaultStream(Device d) const override {\n    return getDefaultCUDAStream(d.index());\n  }\n  // NB: These do NOT set the current device\n  Stream exchangeStream(Stream s) const noexcept override {\n    CUDAStream cs(s);\n    auto old_stream = getCurrentCUDAStream(s.device().index());\n    setCurrentCUDAStream(cs);\n    return old_stream.unwrap();\n  }\n  DeviceIndex deviceCount() const noexcept override {\n    return device_count();\n  }\n\n  // Event-related functions\n  void createEvent(\n    cudaEvent_t* cuda_event,\n    const EventFlag flag) const {\n    // Maps PyTorch's Event::Flag to CUDA flag\n    auto cuda_flag = cudaEventDefault;\n    switch (flag) {\n      case EventFlag::PYTORCH_DEFAULT:\n      case EventFlag::CUDA_EVENT_DISABLE_TIMING:\n        cuda_flag = cudaEventDisableTiming;\n        break;\n      case EventFlag::BACKEND_DEFAULT:\n      case EventFlag::CUDA_EVENT_DEFAULT:\n        cuda_flag = cudaEventDefault;\n        break;\n      default:\n        TORCH_CHECK(false, \"CUDA event received unknown flag\");\n    }"
},
{
    "Id": 80,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/53953316444485c8ee250022988ef87778ae1352",
    "Violation": "missing",
    "Bug report": "Stacks recorded when tensors are being freed during exit could try to acquire the GIL. Py_IsInitialized can be used to check if we are post Python exit and should not attempt to acquire the GIL.",
    "Number of deleted lines": 0,
    "Deleted lines": ""
},
{
    "Id": 81,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8269c4f3d30ad950a873d900f7de0880cdd38878",
    "Violation": "missing",
    "Bug report": "Added nullptr check for pthradpool_get_threads_count. We get seg fault without this in using XNNPACK.",
    "Number of deleted lines": 0,
    "Deleted lines": "    delete threadpool;\n  }\n}\n"
},
{
    "Id": 82,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/13121598efc7d86cb7ae6e05322bb95c1d0f16bc",
    "Violation": "missing",
    "Bug report": "Bug fix to update requantization and zp parameters of input. Also sneaking in change to check for realloc failure for packed activation buffer. In dynamic quantization input's quantization scale and zero point can be different on every iterations. Thus requantization scale needs to be recomputed. Earlier bug that calculated those only at op creation time results in wrong results on subsequent runs.",
    "Number of deleted lines": 0,
    "Deleted lines": "      const size_t group_input_channels = op->group_input_channels;\n      const size_t group_output_channels = op->group_output_channels;\n      uint32_t mr, log2_mr, nr, kr, log2_row_block_size;\n      pytorch_q8gemm_sparse_packA_ukernel_function prepack_kernel;\n      pytorch_q8gemm_dq_sparse_packedA_ukernel_function compute_kernel;\n      if (op->sparse_matrix.row_block_size == 1 &&\n          op->sparse_matrix.col_block_size == 4) {\n        mr = pytorch_qnnp_params.q8gemm_sparse_c1x4.mr;\n        log2_mr = pytorch_qnnp_params.q8gemm_sparse_c1x4.log2_mr;\n        log2_row_block_size = 0;\n        nr = pytorch_qnnp_params.q8gemm_sparse_c1x4.nr;\n        kr = pytorch_qnnp_params.q8gemm_sparse_c1x4.kr;\n        compute_kernel =\n          pytorch_qnnp_params.q8gemm_sparse_c1x4.packedA_gemm_dq;\n        prepack_kernel = pytorch_qnnp_params.q8gemm_sparse_c1x4.packA;\n      } else if (op->sparse_matrix.row_block_size == 8 &&\n          op->sparse_matrix.col_block_size == 1) {\n        mr = pytorch_qnnp_params.q8gemm_sparse_c8x1.mr;\n        log2_mr = pytorch_qnnp_params.q8gemm_sparse_c8x1.log2_mr;\n        log2_row_block_size = 3;\n        nr = pytorch_qnnp_params.q8gemm_sparse_c8x1.nr;\n        kr = pytorch_qnnp_params.q8gemm_sparse_c8x1.kr;\n        compute_kernel =\n          pytorch_qnnp_params.q8gemm_sparse_c8x1.packedA_gemm_dq;\n        prepack_kernel = pytorch_qnnp_params.q8gemm_sparse_c8x1.packA;\n      } else {\n        return pytorch_qnnp_status_invalid_parameter;\n      }\n\n      const size_t output_size = op->output_height * op->output_width;\n      const size_t k_stride = (group_input_channels + (kr - 1)) & -kr;\n      const size_t m_stride = (output_size + (mr - 1)) & -mr;\n      op->prepacked_a =\n        (uint8_t*)realloc((void*)op->prepacked_a, k_stride * m_stride);\n\n      struct q8gemm_prepackA_sparse_dq_context\n        q8gemm_prepack_sparse_dq_context = {\n          .k = group_input_channels,\n          .a = op->input,\n          .a_stride = op->input_pixel_stride,\n          .a_packed = op->prepacked_a,\n          .a_packed_stride = k_stride * mr,\n          .log2_mr = log2_mr,\n          .log2_row_block_size = log2_row_block_size,\n          .kernel_col_indices = op->sparse_matrix.col_indices,\n          .kernel_row_values = op->sparse_matrix.row_values,\n          .kernel_values = op->sparse_matrix.values,\n          .bias = (const float*)op->bias,\n          .c = (float*)op->output,\n          .c_stride = op->output_pixel_stride,\n          .quantization_params = op->dynamic_conv_quantization_params,\n          .ukernel = compute_kernel,\n          .prepack_ukernel = prepack_kernel,\n      };\n\n      // This batch size is not the actual batch size of the op\n      // The batch size is modified in fully-connected-sparse.c\n      if (groups != 1 || batch_size != 1) {\n        pytorch_qnnp_log_error(\"pytorch_qnnp_ukernel_type_gemm_prepackA_sparse_dq \"\n            \"works with group size = 1, batch_size = 1.\\n\");\n        return pytorch_qnnp_status_invalid_parameter;\n      }\n\n      pthreadpool_compute_4d_tiled(\n          threadpool,\n          (pthreadpool_function_4d_tiled_t)compute_q8gemm_prepack_a_sparse,\n          &q8gemm_prepack_sparse_dq_context,\n          1,\n          1,\n          output_size,"
},
{
    "Id": 83,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e8e29690ef0306da25b5f191623476001d29a18b",
    "Violation": "missing",
    "Bug report": " Add has_debug_def() check to net's debug_def() ",
    "Number of deleted lines": 2,
    "Deleted lines": "   *\n   * This function returns a vector of float recording the number of milli-\n   * seconds spent during the benchmark. The 0-th item is the time spent per\n   * each network run, and if a net instantiation supports run_individual,\n   * the remainder of the vector returns the number of milliseconds spent per\n   * opeartor.\n   */\n  virtual vector<float> TEST_Benchmark(\n      const int /*warmup_runs*/,\n      const int /*main_runs*/,\n      const bool /*run_individual*/) {\n    LOG(ERROR) << \"Benchmark not implemented for this net type.\";\n    return vector<float>();\n  }\n\n  inline const vector<string>& external_output() const {\n    return external_output_;\n  }\n\n  inline const vector<string>& external_input() const {\n    return external_input_;\n  }\n\n  /* Used to attach Observers to operators of a Net\n   *\n   * Returns pointers to objects owned with unique_ptrs.\n   * Use with caution.\n   */\n  virtual vector<OperatorBase*> GetOperators() const = 0;\n\n  const string& Name() const {\n    return name_;\n  }\n\n  inline const std::shared_ptr<const NetDef> debug_def() const {\n    return net_def_;\n  }\n\n protected:\n  virtual bool DoRunAsync() = 0;\n\n  vector<string> external_input_;\n  vector<string> external_output_;\n  string name_;\n  vector<const Event*> events_;\n  std::shared_ptr<const NetDef> net_def_;\n  DISABLE_COPY_AND_ASSIGN(NetBase);\n};\n\nCAFFE_DECLARE_REGISTRY(\n    NetRegistry,\n    NetBase,\n    const std::shared_ptr<const NetDef>&,\n    Workspace*);\n#define REGISTER_NET_CREATOR(key, ...) \\\n  CAFFE_REGISTER_CREATOR(NetRegistry, key, __VA_ARGS__)\n#define REGISTER_NET(name, ...) \\\n  CAFFE_REGISTER_CLASS(NetRegistry, name, __VA_ARGS__)\n\n/**\n * @brief Creates a network, accessing / creating blobs in the given workspace.\n *\n * Note that this is different from Workspace::CreateNet. The latter adds the\n * created net object to the workspace's net map, while this function returns\n * a standalone net object.\n */\nunique_ptr<NetBase> CreateNet(const NetDef& net_def, Workspace* ws);\nunique_ptr<NetBase> CreateNet(\n    const std::shared_ptr<const NetDef>& net_def,\n    Workspace* ws);\n\nvoid SetGlobalNetObserverCreator(NetObserverCreator creator);"
},
{
    "Id": 84,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/eed22921237eb4c1f4399af177ce912147a885c3",
    "Violation": "missing",
    "Bug report": " check for null commonworld in DestroyCommonWorld. Summary: Check for nullptr before closing a common world.",
    "Number of deleted lines": 0,
    "Deleted lines": " private:\n  bool handleException(std::exception& ex) {\n    if (status_blob_ != \"\") {\n      signalFailure(ws_->GetBlob(status_blob_), ex);\n      return false;\n    } else {\n      throw ex;\n    }\n  }\n\n  std::shared_ptr<::gloo::transport::Device> createDevice();\n\n  const int size_;\n  const int rank_;\n  const bool sync_;\n\n  std::string name_;\n  std::shared_ptr<::gloo::transport::Device> device_;\n\n  Workspace* ws_;\n  std::string status_blob_;\n\n  INPUT_TAGS(STORE_HANDLER, EXISTING_CW);\n  OUTPUT_TAGS(COMM);\n};\n\nclass DestroyCommonWorld final : public Operator<CPUContext> {\n public:\n  DestroyCommonWorld(const OperatorDef& operator_def, Workspace* ws)\n      : Operator<CPUContext>(operator_def, ws) {\n    cw_name_ = operator_def.input(0);\n  }\n\n  bool RunOnDevice() override {\n    const auto& context =\n        OperatorBase::Input<std::shared_ptr<::gloo::Context>>(0);\n\n    if (context) {\n      LOG(INFO) << \"Closing connections: \" << cw_name_;\n      context->closeConnections();\n    }\n    return true;\n  }\n\n private:\n  std::string cw_name_;\n};\n\n} // namespace gloo\n} // namespace caffe2\n"
},
{
    "Id": 85,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d471eaeb1d2fbc7efcde6408d7d1e513b969af25",
    "Violation": "missing",
    "Bug report": "fix inline_container.cc inplace loading",
    "Number of deleted lines": 1,
    "Deleted lines": "  mz_zip_reader_extract_to_mem(ar_.get(), key, dst, stat.m_uncomp_size, 0);\n  valid(\"reading file \", name.c_str());\n\n  return stat.m_uncomp_size;\n}\n\nsize_t PyTorchStreamReader::getRecord(\n    const std::string& name,\n    void* dst,\n    size_t n,\n    size_t chunk_size,\n    void* buf,\n    const std::function<void(void*, const void*, size_t)>& memcpy_func) {\n  std::lock_guard<std::mutex> guard(reader_lock_);\n  if ((!load_debug_symbol_) && c10::string_view(name).ends_with(kDebugPklSuffix)) {\n    return 0;\n  }\n  size_t key = getRecordID(name);\n  mz_zip_archive_file_stat stat;\n  mz_zip_reader_file_stat(ar_.get(), key, &stat);\n  TORCH_CHECK(\n      n == stat.m_uncomp_size,\n      \"record size \",\n      stat.m_uncomp_size,\n      \" mismatch with dst size \",\n      n);\n  valid(\"retrieving file meta-data for \", name.c_str());\n\n  mz_zip_reader_extract_iter_state* iter =\n      mz_zip_reader_extract_iter_new(ar_.get(), key, 0);\n  TORCH_CHECK(\n      iter != nullptr,\n      \"Failed to create zip reader iter: \",\n      mz_zip_get_error_string(mz_zip_get_last_error(ar_.get())));\n\n  for (size_t offset = 0; offset < stat.m_uncomp_size; offset += chunk_size) {\n    size_t want_size =\n        std::min(chunk_size, (size_t)stat.m_uncomp_size - offset);\n    size_t read_size =\n        mz_zip_reader_extract_iter_read(iter, buf, want_size);\n    TORCH_CHECK(\n        read_size == want_size,\n        \"Failed to advance zip reader iter: \",\n        mz_zip_get_error_string(mz_zip_get_last_error(ar_.get())));\n    memcpy_func((char*)dst + offset, buf, read_size);\n  }\n  valid(\"reading file \", name.c_str());\n  mz_zip_reader_extract_iter_free(iter);\n\n  return stat.m_uncomp_size;\n}\n\nstatic int64_t read_le_16(uint8_t* buf) {\n  return buf[0] + (buf[1] << 8);\n}\n\nsize_t PyTorchStreamReader::getRecordOffset(const std::string& name) {\n  std::lock_guard<std::mutex> guard(reader_lock_);\n  mz_zip_archive_file_stat stat;\n  mz_zip_reader_file_stat(ar_.get(), getRecordID(name), &stat);\n  valid(\"retrieving file meta-data for \", name.c_str());\n  // NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,modernize-avoid-c-arrays)\n  uint8_t local_header[MZ_ZIP_LOCAL_DIR_HEADER_SIZE];\n  in_->read(\n      stat.m_local_header_ofs,\n      local_header,\n      MZ_ZIP_LOCAL_DIR_HEADER_SIZE,\n      \"reading file header\");\n  size_t filename_len = read_le_16(local_header + MZ_ZIP_LDH_FILENAME_LEN_OFS);\n  size_t extra_len = read_le_16(local_header + MZ_ZIP_LDH_EXTRA_LEN_OFS);\n  return stat.m_local_header_ofs + MZ_ZIP_LOCAL_DIR_HEADER_SIZE + filename_len + extra_len;"
},
{
    "Id": 86,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a5ca445f7953711bc90c111c3cad2ec87f02e74a",
    "Violation": "missing",
    "Bug report": "Check for corrupted ivalues. The error occurs because the `ivalues` field of flatbuffer module can be null, so the corresponding check must be inserted.",
    "Number of deleted lines": 0,
    "Deleted lines": "    const auto* extra_file = files->Get(i);\n    (*extra_files)[extra_file->name()->str()] = extra_file->content()->str();\n  }\n}\n\nvoid parseExtraFiles(\n    mobile::serialization::Module* module,\n    ExtraFilesMap& extra_files) {\n  auto extra_files_offsets = module->extra_files();\n  parseExtraFilesFromVector(extra_files_offsets, &extra_files);\n}\n\nvoid FlatbufferLoader::parseAndPopulate(\n    uint32_t i,\n    const mobile::serialization::IValue* ivalue) {\n  if (const auto* func = ivalue->val_as_Function()) {\n    auto func_ptr = parseFunction(func);\n    all_functions_[i] = func_ptr.get();\n    mcu_->register_function(std::move(func_ptr));\n  } else {\n    all_ivalues_[i] = parseIValue(ivalue);\n  }\n}\n\nmobile::Module FlatbufferLoader::parseModule(\n    mobile::serialization::Module* module) {\n  module_ = module;\n  all_ivalues_.clear();\n  all_types_.clear();\n  storages_.clear();\n  storage_loaded_.clear();\n  module_parsed_ = false;\n\n  const auto* ivalues = module->ivalues();\n  all_ivalues_.resize(ivalues->size());\n  all_types_.resize(module->object_types()->size());\n  storages_.resize(module->storage_data_size());\n  storage_loaded_.resize(module->storage_data_size(), false);\n\n  mobile_ivalue_size_ = module_->mobile_ivalue_size();\n  if (mobile_ivalue_size_ == 0) {\n    mobile_ivalue_size_ = ivalues->size();\n  }\n\n  for (uint32_t i = 0; i < mobile_ivalue_size_; i++) {\n    const auto* ival = ivalues->Get(i);\n    parseAndPopulate(i, ival);\n  }\n  IValue& module_ivalue = getIValue(module->state_obj());\n\n  // register functions\n  for (const auto& f : all_functions_) {\n    uint32_t class_index =\n        ivalues->Get(f.first)->val_as_Function()->class_type();\n    ClassTypePtr class_type = all_types_[class_index];\n    class_type->addMethod(f.second);\n  }\n\n  module_parsed_ = true;\n  auto m = mobile::Module(module_ivalue.toObject(), mcu_);\n  m.set_min_operator_version(module->operator_version());\n  m.set_bytecode_version(module->bytecode_version());\n  return m;\n}\n\nvoid appendUpgraderFunctions(mobile::Function* function) {\n#ifndef DISABLE_UPGRADER\n  for (auto& byteCodeFunctionWithOperator : getUpgraderBytecodeList()) {\n    function->append_function(byteCodeFunctionWithOperator.function);\n  }"
},
{
    "Id": 87,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6cc0f1c20c2f87a6c7b0e4abd5419e5007920999",
    "Violation": "missing",
    "Bug report": "Checking for nullptr in get_model_bytecode_version . One-liner commit to check that the ptr is not null. Just had `test_jit` that had a segfault there.",
    "Number of deleted lines": 0,
    "Deleted lines": "  in.seekg(0, in.beg);\n  std::shared_ptr<char> data;\n  size_t size = 0;\n  std::tie(data, size) = get_stream_content(in);\n  in.seekg(orig_pos, in.beg);\n  return _get_model_bytecode_version_from_bytes(data.get(), size);\n}\n\nuint64_t _get_model_bytecode_version(const std::string& filename) {\n  std::ifstream ifile(filename);\n  return _get_model_bytecode_version(ifile);\n}\n\nuint64_t _get_model_bytecode_version(\n    std::shared_ptr<ReadAdapterInterface> rai) {\n  std::shared_ptr<char> data;\n  size_t size = 0;\n  std::tie(data, size) = get_rai_content(rai.get());\n  return _get_model_bytecode_version_from_bytes(data.get(), size);\n}\n\nstatic uint64_t _get_model_bytecode_version_zip(\n    std::shared_ptr<ReadAdapterInterface> rai) {\n  if (!check_zip_file(rai)) {\n    TORCH_CHECK(\n        false,\n        \"Failed to open .ptl file please ensure the model was exported for mobile\");\n  }\n  PyTorchStreamReader reader(std::move(rai));\n  auto bytecode_values = get_bytecode_ivalues(reader);\n  return _get_model_bytecode_version(bytecode_values);\n}\n\nuint64_t _get_model_bytecode_version_from_bytes(char* data, size_t size) {\n  TORCH_CHECK(size >= kFileFormatHeaderSize, \"Unrecognized data format\");\n  auto format = getFileFormat(data);\n  switch (format) {\n    case FileFormat::FlatbufferFileFormat: {\n      return get_bytecode_version_from_bytes(data);\n    }\n    case FileFormat::ZipFileFormat: {\n      auto rai =\n          std::make_unique<caffe2::serialize::MemoryReadAdapter>(data, size);\n      auto version = _get_model_bytecode_version_zip(std::move(rai));\n      return version;\n    }\n\n    default:\n      TORCH_CHECK(false, \"Unrecognized data format\");\n  }\n}\n\nuint64_t _get_model_bytecode_version(\n    const std::vector<IValue>& bytecode_ivalues) {\n  if (!bytecode_ivalues.empty() && bytecode_ivalues[0].isInt()) {\n    int64_t model_version = bytecode_ivalues[0].toInt();\n    TORCH_CHECK(\n        model_version > 0,\n        \"Expected model bytecode version > 0 got \",\n        model_version);\n    return static_cast<uint64_t>(model_version);\n  }\n  TORCH_CHECK(false, \"Failed to get bytecode version.\");\n}\n\n/********************** Operator Version **********************/\n\nuint64_t _get_model_operator_version(\n    PyTorchStreamReader& reader); // Forward Declare\n"
},
{
    "Id": 88,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/61b9d8fccd3361f21e1f3548c2a9538b62cc7525",
    "Violation": "improper",
    "Bug report": "It is apparently undefined behavior to do pointer arithmetic on nullptr. In the case of AppendOnlyList, `next_` will only be null if `end_` is also null and thus the `memcpy` path will only be triggered if `n == 0`. Nonetheless, it is U to `memcpy(0, 0, 0)`. The extra null check is in a `C10_LIKELY` block so the extra cost should be negligible, and indeed after dusting off the component microbenchmarks there's no observable difference.",
    "Number of deleted lines": 2,
    "Deleted lines": "// Performance drops off for larger values, so testing on a case-by-case basis\n// is recommended if performance is absolutely critical.\n\ntemplate <\n    typename T,\n    size_t ChunkSize,\n    template <typename U, size_t N> class block_t = std::array>\nclass AppendOnlyList {\n public:\n  using array_t = block_t<T, ChunkSize>;\n  static_assert(\n      std::is_base_of<std::array<T, ChunkSize>, array_t>::value,\n      \"AppendOnlyList expects raw low level pointer storage.\");\n  static_assert(ChunkSize > 0, \"Block cannot be empty.\");\n\n  AppendOnlyList() : buffer_last_{buffer_.before_begin()} {}\n  AppendOnlyList(const AppendOnlyList&) = delete;\n  AppendOnlyList& operator=(const AppendOnlyList&) = delete;\n\n  size_t size() const {\n    return n_blocks_ * ChunkSize + (size_t)(next_ - end_);\n  }\n\n  template <class... Args>\n  T* emplace_back(Args&&... args) {\n    maybe_grow();\n    *next_ = {std::forward<Args>(args)...};\n    return next_++;\n  }\n\n  template <typename T0>\n  typename std::enable_if<\n      std::is_same<T0, T>::value && std::is_trivially_copyable<T>::value>::type\n  copy(c10::ArrayRef<T0> src) {\n    int n = src.size();\n    if (C10_LIKELY(next_ + n <= end_)) {\n      std::memcpy((void*)next_, (void*)src.begin(), n * sizeof(T0));\n      next_ += n;\n    } else {\n      // We could chunk this into several `memcpy`s, but because we expect this\n      // fallback to be infrequent (n << ChunkSize) the performance impact is\n      // negligible.\n      for (auto i : src) {\n        emplace_back(i);\n      }\n    }\n  }\n\n  void clear() {\n    buffer_.clear();\n    buffer_last_ = buffer_.before_begin();\n    n_blocks_ = 0;\n    next_ = nullptr;\n    end_ = nullptr;\n  }\n\n  struct Iterator {\n    using iterator_category = std::forward_iterator_tag;\n    using difference_type = std::ptrdiff_t;\n    using value_type = T;\n    using pointer = T*;\n    using reference = T&;\n\n    Iterator(std::forward_list<array_t>& buffer, const size_t size)\n        : block_{buffer.begin()}, size_{size} {}\n\n    // End iterator.\n    Iterator() = default;\n\n    bool exhausted() const {\n      return current_ >= size_;\n    }"
},
{
    "Id": 89,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c06dfd7c26102ac2436ca25609c92fa794e972ca",
    "Violation": "missing",
    "Bug report": "Check input device in TRTModule. Add a check to ensure all the inputs are on cuda device.",
    "Number of deleted lines": 0,
    "Deleted lines": "    def _on_state_dict(self, state_dict, prefix, local_metadata):\n        state_dict[prefix + \"engine\"] = bytearray(self.engine.serialize())\n        state_dict[prefix + \"input_names\"] = self.input_names\n        state_dict[prefix + \"output_names\"] = self.output_names\n        state_dict[prefix + \"fp16_output\"] = self.fp16_output\n\n    def _load_from_state_dict(\n        self,\n        state_dict,\n        prefix,\n        local_metadata,\n        strict,\n        missing_keys,\n        unexpected_keys,\n        error_msgs,\n    ):\n        engine_bytes = state_dict[prefix + \"engine\"]\n\n        with trt.Logger() as logger, trt.Runtime(logger) as runtime:\n            self.engine = runtime.deserialize_cuda_engine(engine_bytes)\n            self.context = self.engine.create_execution_context()\n\n        self.input_names = state_dict[prefix + \"input_names\"]\n        self.output_names = state_dict[prefix + \"output_names\"]\n\n    def forward(self, *inputs):\n        assert len(inputs) == len(\n            self.input_names\n        ), f\"Wrong number of inputs, expect {len(self.input_names)} get {len(inputs)}.\"\n        batch_size = inputs[0].shape[0]\n        contiguous_inputs: List[torch.Tensor] = [i.contiguous() for i in inputs]\n        bindings: List[Any] = [None] * (len(self.input_names) + len(self.output_names))\n\n        for i, input_name in enumerate(self.input_names):\n            idx = self.engine.get_binding_index(input_name)\n            bindings[idx] = contiguous_inputs[i].data_ptr()\n\n            if not self.engine.has_implicit_batch_dimension:\n                self.context.set_binding_shape(idx, tuple(contiguous_inputs[i].shape))\n\n        # create output tensors\n        outputs: List[torch.Tensor] = []\n        for idx in range(len(inputs), len(inputs) + len(self.output_names)):\n            dtype = torch_dtype_from_trt(self.engine.get_binding_dtype(idx))\n\n            if self.engine.has_implicit_batch_dimension:\n                shape = (batch_size,) + tuple(self.engine.get_binding_shape(idx))\n            else:\n                shape = tuple(self.context.get_binding_shape(idx))\n\n            output = torch.empty(size=shape, dtype=dtype, device=\"cuda\")\n            outputs.append(output)\n            bindings[idx] = output.data_ptr()\n\n        if self.engine.has_implicit_batch_dimension:\n            self.context.execute_async(\n                batch_size, bindings, torch.cuda.current_stream().cuda_stream\n            )\n        else:\n            self.context.execute_async_v2(\n                bindings, torch.cuda.current_stream().cuda_stream\n            )\n\n        if len(outputs) == 1:\n            return outputs[0]\n\n        return tuple(outputs)\n\n    def enable_profiling(self):\n        raise RuntimeError("
},
{
    "Id": 90,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d3de37609f2f052a7efb098ab69540458ebaaa6c",
    "Violation": "insufficient",
    "Bug report": "Support fused_dropout with XPU backend. ## Motivation Enable the fused dropout optimization on XPU devices. ## Solution Add XPU device in the fused dropout acceptable checking.",
    "Number of deleted lines": 1,
    "Deleted lines": ""
},
{
    "Id": 91,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7bf195f3608e0f28c30ffb6e2fecd74a1d4ee50a",
    "Violation": "missing",
    "Bug report": "fix kernel launch check in cross kernel",
    "Number of deleted lines": 1,
    "Deleted lines": "    const auto* x2_row = x2 + offsets[2];\n\n    const T val0 = (x1_row[1 * x1stride] * x2_row[2 * x2stride] -\n                    x1_row[2 * x1stride] * x2_row[1 * x2stride]);\n\n    const T val1 = (x1_row[2 * x1stride] * x2_row[0 * x2stride] -\n                    x1_row[0 * x1stride] * x2_row[2 * x2stride]);\n\n    const T val2 = (x1_row[0 * x1stride] * x2_row[1 * x2stride] -\n                    x1_row[1 * x1stride] * x2_row[0 * x2stride]);\n\n\n    out_row[0 * ostride] = val0;\n    out_row[1 * ostride] = val1;\n    out_row[2 * ostride] = val2;\n  }\n}\n\nvoid launch_cross_kernel(const TensorIteratorBase& iter, int64_t ostride,\n                         int64_t x1stride, int64_t x2stride) {\n  const auto N = iter.numel();\n  auto offset_calculator = make_element_offset_calculator<3>(iter);\n  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(N > 0 && N <= std::numeric_limits<int32_t>::max());\n  int64_t grid = (N + NUM_THREADS - 1) / NUM_THREADS;\n  auto stream = at::cuda::getCurrentCUDAStream();\n\n  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND(kHalf, iter.common_dtype(), \"cross_cuda\", [&] {\n    auto out = static_cast<scalar_t*>(iter.data_ptr(0));\n    auto x1 = static_cast<const scalar_t*>(iter.data_ptr(1));\n    auto x2 = static_cast<const scalar_t*>(iter.data_ptr(2));\n    constexpr int64_t int_max = std::numeric_limits<int>::max();\n    if (ostride * 2 > int_max || x1stride * 2 > int_max || x2stride * 2 > int_max) {\n      cross_kernel<<<grid, num_threads, 0, stream>>>(\n          N, out, x1, x2, offset_calculator, ostride, x1stride, x2stride);\n    } else {\n      cross_kernel<<<grid, num_threads, 0, stream>>>(\n          N, out, x1, x2, offset_calculator,\n          static_cast<int>(ostride),\n          static_cast<int>(x1stride),\n          static_cast<int>(x2stride));\n    }\n  });\n  C10_CUDA_KERNEL_LAUNCH_CHECK();\n}\n\nvoid cross_impl(Tensor& result, const Tensor& x1, const Tensor& x2, int64_t dim) {\n  const int64_t ostride = result.stride(dim);\n  const int64_t x1stride = x1.stride(dim);\n  const int64_t x2stride = x2.stride(dim);\n\n  auto iter = TensorIteratorConfig()\n      .add_output(result)\n      .add_input(x1)\n      .add_input(x2)\n      .resize_outputs(false)\n      .declare_static_shape(result.sizes(), /*squash_dims=*/dim)\n      .build();\n\n  if (iter.numel() == 0) {\n    return;\n  }\n\n  if (iter.can_use_32bit_indexing()) {\n    launch_cross_kernel(iter, ostride, x1stride, x2stride);\n  } else {\n    for (auto&& sub_iter: iter.with_32bit_indexing()) {\n      launch_cross_kernel(sub_iter, ostride, x1stride, x2stride);\n    }\n  }\n}\n\nREGISTER_DISPATCH(cross_stub, &cross_impl);\n\n}}\n"
},
{
    "Id": 92,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/15dbc566c57eedbd0245e786912e94586eba0fd2",
    "Violation": "missing",
    "Bug report": "Add missing cuda kernel launch check",
    "Number of deleted lines": 0,
    "Deleted lines": "              data_data_ptr,\n              output_data_ptr,\n              segment_count,\n              offsets_data_ptr,\n              offsets_data_ptr + 1,\n              max_op,\n              initial_value,\n              at::cuda::getCurrentCUDAStream());\n        } else if (reduction == SegmentReductionType::MEAN) {\n          CustomSum sum_op{};\n          scalar_t initial_value = initial.has_value()\n              ? initial.value().to<scalar_t>()\n              : (scalar_t)0;\n          CUB_WRAPPER(\n              cub::DeviceSegmentedReduce::Reduce,\n              data_data_ptr,\n              output_data_ptr,\n              segment_count,\n              offsets_data_ptr,\n              offsets_data_ptr + 1,\n              sum_op,\n              initial_value,\n              at::cuda::getCurrentCUDAStream());\n\n          post_sum_div_kernel<scalar_t>\n              <<<num_blocks,\n                 threads_per_block,\n                 0,\n                 at::cuda::getCurrentCUDAStream()>>>(\n                  output_data_ptr,\n                  lengths_data_ptr,\n                  segment_count,\n                  initial.has_value(),\n                  initial_value);\n        }\n      });\n\n  return output;\n}\n\nREGISTER_DISPATCH(_segment_reduce_stub, &_segment_reduce_cuda_kernel);\n\n} // namespace native\n} // namespace at\n"
},
{
    "Id": 93,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a8653f35de02c7fb038e3c184dda6e67a12a39e2",
    "Violation": "missing",
    "Bug report": "Perf win by check which device tensors are on",
    "Number of deleted lines": 0,
    "Deleted lines": "\n// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ fill ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nTensor& fill_out(Tensor& self, const Scalar& value) {\n  if (self.device() == at::kCPU && self.numel() == 1) {\n    return at::detail::scalar_fill(self, value);\n  }\n  auto iter = TensorIteratorConfig()\n    .set_check_mem_overlap(false)  // Fill is idempotent, so overlap is okay\n    .check_all_same_dtype(false)\n    .add_output(self)\n    .resize_outputs(false)\n    .build();\n  fill_stub(iter.device_type(), iter, value);\n  return self;\n}\n\nstatic Tensor& fill_out_quantized(Tensor& self, const Scalar& value) {\n  at::Tensor out = at::ones(self.sizes()).to(kFloat) * value;\n  out = out.to(self.device()).to(self.suggest_memory_format());\n  // Trust the `copy_` to handle the quantization and the boundary checks.\n  self.copy_(out);\n  return self;\n}\n\nTensor& fill_(Tensor& self, const Scalar& value) {\n  return fill_out(self, value);\n}\n\nTensor& fill_quantized_(Tensor& self, const Scalar& value) {\n  return fill_out_quantized(self, value);\n}\n\nTensor& fill_(Tensor& self, const Tensor& value) {\n  TORCH_CHECK(value.dim() == 0, \"fill_ only supports 0-dimension value tensor but got tensor with \", value.dim(), \" dimensions.\");\n  // Check if value is a view of self and if it is we clone\n  // it to avoid overwriting self prematurely\n  if(self.is_alias_of(value)) {\n    self.copy_(value.clone());\n  } else{\n    self.copy_(value);\n  }\n  return self;\n}\n\nTensor& fill_quantized_(Tensor& self, const Tensor& value) {\n  TORCH_CHECK(value.dim() == 0, \"fill_ only supports 0-dimension value tensor but got tensor with \", value.dim(), \" dimensions.\");\n  return fill_out_quantized(self, value.item());\n}\n\nTensor& fill_meta_(Tensor& self, const Scalar& value) {\n  return self;\n}\n\nTensor& fill_meta_(Tensor& self, const Tensor& value) {\n  TORCH_CHECK(value.dim() == 0, \"fill_ only supports 0-dimension value tensor but got tensor with \", value.dim(), \" dimensions.\");\n  return self;\n}\n\nTensor fill(const Tensor& self, const Scalar& value) {\n  return at::empty_like(self).fill_(value);\n}\n\nTensor fill(const Tensor& self, const Tensor& value) {\n  return at::empty_like(self).fill_(value);\n}\n\nDEFINE_DISPATCH(fill_stub);\n\n// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ fill_diagonal ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
},
{
    "Id": 94,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8b37821813b60a3ce2ae92e7a06057183578a450",
    "Violation": "insufficient",
    "Bug report": "if we want to use dp on other device ranther than \"cuda\", this balance  check will raise error, so I make the balance check only effective for `cuda`",
    "Number of deleted lines": 1,
    "Deleted lines": "    \"\"\"\n\n    # TODO: update notes/cuda.rst when this class handles 8+ GPUs well\n\n    def __init__(\n        self,\n        module: T,\n        device_ids: Optional[Sequence[Union[int, torch.device]]] = None,\n        output_device: Optional[Union[int, torch.device]] = None,\n        dim: int = 0,\n    ) -> None:\n        super().__init__()\n        torch._C._log_api_usage_once(\"torch.nn.parallel.DataParallel\")\n        device_type = _get_available_device_type()\n        if device_type is None:\n            self.module = module\n            self.device_ids = []\n            return\n\n        if device_ids is None:\n            device_ids = _get_all_device_indices()\n\n        if device_ids is None:\n            raise RuntimeError(\"no available devices were found\")\n\n        if output_device is None:\n            output_device = device_ids[0]\n\n        self.dim = dim\n        self.module = module\n        self.device_ids = [_get_device_index(x, True) for x in device_ids]\n        self.output_device = _get_device_index(output_device, True)\n        self.src_device_obj = torch.device(device_type, self.device_ids[0])\n\n        _check_balance(self.device_ids)\n\n        if len(self.device_ids) == 1:\n            self.module.to(self.src_device_obj)\n\n    def forward(self, *inputs: Any, **kwargs: Any) -> Any:\n        with torch.autograd.profiler.record_function(\"DataParallel.forward\"):\n            if not self.device_ids:\n                return self.module(*inputs, **kwargs)\n\n            for t in chain(self.module.parameters(), self.module.buffers()):\n                if t.device != self.src_device_obj:\n                    raise RuntimeError(\"module must have its parameters and buffers \"\n                                       \"on device {} (device_ids[0]) but found one of \"\n                                       \"them on device: {}\".format(self.src_device_obj, t.device))\n\n            inputs, module_kwargs = self.scatter(inputs, kwargs, self.device_ids)\n            # for forward function without any inputs, empty list and dict will be created\n            # so the module can be executed on one device which is the first one in device_ids\n            if not inputs and not module_kwargs:\n                inputs = ((),)\n                module_kwargs = ({},)\n\n            if len(self.device_ids) == 1:\n                return self.module(*inputs[0], **module_kwargs[0])\n            replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n            outputs = self.parallel_apply(replicas, inputs, module_kwargs)\n            return self.gather(outputs, self.output_device)\n\n    def replicate(self, module: T, device_ids: Sequence[Union[int, torch.device]]) -> List[T]:\n        return replicate(module, device_ids, not torch.is_grad_enabled())\n\n    def scatter(\n        self,\n        inputs: Tuple[Any, ...],\n        kwargs: Optional[Dict[str, Any]],\n        device_ids: Sequence[Union[int, torch.device]],"
},
{
    "Id": 95,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/59a3759d9787091e75d939de603981a6d32505c8",
    "Violation": "missing",
    "Bug report": "When we need to link extra libs, we should notice that 64-bit CUDA may be installed in \"lib\", not in \"lib64\".",
    "Number of deleted lines": 1,
    "Deleted lines": "            extra_ldflags.append('-INCLUDE:?warp_size@cuda@at@@YAHXZ')\n        extra_ldflags.append('torch.lib')\n        extra_ldflags.append(f'/LIBPATH:{TORCH_LIB_PATH}')\n        if not is_standalone:\n            extra_ldflags.append('torch_python.lib')\n            extra_ldflags.append(f'/LIBPATH:{python_lib_path}')\n\n    else:\n        extra_ldflags.append(f'-L{TORCH_LIB_PATH}')\n        extra_ldflags.append('-lc10')\n        if with_cuda:\n            extra_ldflags.append('-lc10_hip' if IS_HIP_EXTENSION else '-lc10_cuda')\n        extra_ldflags.append('-ltorch_cpu')\n        if with_cuda:\n            extra_ldflags.append('-ltorch_hip' if IS_HIP_EXTENSION else '-ltorch_cuda')\n        extra_ldflags.append('-ltorch')\n        if not is_standalone:\n            extra_ldflags.append('-ltorch_python')\n\n        if is_standalone and \"TBB\" in torch.__config__.parallel_info():\n            extra_ldflags.append('-ltbb')\n\n        if is_standalone:\n            extra_ldflags.append(f\"-Wl,-rpath,{TORCH_LIB_PATH}\")\n\n    if with_cuda:\n        if verbose:\n            print('Detected CUDA files, patching ldflags', file=sys.stderr)\n        if IS_WINDOWS:\n            extra_ldflags.append(f'/LIBPATH:{_join_cuda_home(\"lib\", \"x64\")}')\n            extra_ldflags.append('cudart.lib')\n            if CUDNN_HOME is not None:\n                extra_ldflags.append(f'/LIBPATH:{os.path.join(CUDNN_HOME, \"lib\", \"x64\")}')\n        elif not IS_HIP_EXTENSION:\n            extra_ldflags.append(f'-L{_join_cuda_home(\"lib64\")}')\n            extra_ldflags.append('-lcudart')\n            if CUDNN_HOME is not None:\n                extra_ldflags.append(f'-L{os.path.join(CUDNN_HOME, \"lib64\")}')\n        elif IS_HIP_EXTENSION:\n            assert ROCM_VERSION is not None\n            extra_ldflags.append(f'-L{_join_rocm_home(\"lib\")}')\n            extra_ldflags.append('-lamdhip64' if ROCM_VERSION >= (3, 5) else '-lhip_hcc')\n    return extra_ldflags\n\n\ndef _get_cuda_arch_flags(cflags: Optional[List[str]] = None) -> List[str]:\n    r'''\n    Determine CUDA arch flags to use.\n\n    For an arch, say \"6.1\", the added compile flag will be\n    ``-gencode=arch=compute_61,code=sm_61``.\n    For an added \"+PTX\", an additional\n    ``-gencode=arch=compute_xx,code=compute_xx`` is added.\n\n    See select_compute_arch.cmake for corresponding named and supported arches\n    when building with CMake.\n    '''\n    # If cflags is given, there may already be user-provided arch flags in it\n    # (from `extra_compile_args`)\n    if cflags is not None:\n        for flag in cflags:\n            if 'arch' in flag:\n                return []\n\n    # Note: keep combined names (\"arch1+arch2\") above single names, otherwise\n    # string replacement may not do the right thing\n    named_arches = collections.OrderedDict([\n        ('Kepler+Tesla', '3.7'),\n        ('Kepler', '3.5+PTX'),\n        ('Maxwell+Tegra', '5.3'),\n        ('Maxwell', '5.0;5.2+PTX'),"
},
{
    "Id": 96,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e856a4d66bead8997a83f8714547c09fcbcdc263",
    "Violation": "missing",
    "Bug report": " Add an env var to skip cudnn version compatibility check. skip the check by setting `PYTORCH_SKIP_CUDNN_COMPATIBILITY_CHECK=1`",
    "Number of deleted lines": 0,
    "Deleted lines": "from torch.backends import ContextProp, PropModule, __allow_nonbracketed_mutation\n\ntry:\n    from torch._C import _cudnn\nexcept ImportError:\n    _cudnn = None  # type: ignore[assignment]\n\n# Write:\n#\n#   torch.backends.cudnn.enabled = False\n#\n# to globally disable CuDNN/MIOpen\n\n__cudnn_version = None\n\nif _cudnn is not None:\n    def _init():\n        global __cudnn_version\n        if __cudnn_version is None:\n            __cudnn_version = _cudnn.getVersionInt()\n            runtime_version = _cudnn.getRuntimeVersion()\n            compile_version = _cudnn.getCompileVersion()\n            runtime_major, runtime_minor, _ = runtime_version\n            compile_major, compile_minor, _ = compile_version\n            # Different major versions are always incompatible\n            # Starting with cuDNN 7, minor versions are backwards-compatible\n            # Not sure about MIOpen (ROCm), so always do a strict check\n            if runtime_major != compile_major:\n                cudnn_compatible = False\n            elif runtime_major < 7 or not _cudnn.is_cuda:\n                cudnn_compatible = runtime_minor == compile_minor\n            else:\n                cudnn_compatible = runtime_minor >= compile_minor\n            if not cudnn_compatible:\n                base_error_msg = (f'cuDNN version incompatibility: '\n                                  f'PyTorch was compiled  against {compile_version} '\n                                  f'but found runtime version {runtime_version}. '\n                                  f'PyTorch already comes bundled with cuDNN. '\n                                  f'One option to resolving this error is to ensure PyTorch '\n                                  f'can find the bundled cuDNN.')\n\n                if 'LD_LIBRARY_PATH' in os.environ:\n                    ld_library_path = os.environ.get('LD_LIBRARY_PATH', '')\n                    if any(substring in ld_library_path for substring in ['cuda', 'cudnn']):\n                        raise RuntimeError(f'{base_error_msg}'\n                                           f'Looks like your LD_LIBRARY_PATH contains incompatible version of cudnn'\n                                           f'Please either remove it from the path or install cudnn {compile_version}')\n                    else:\n                        raise RuntimeError(f'{base_error_msg}'\n                                           f'one possibility is that there is a '\n                                           f'conflicting cuDNN in LD_LIBRARY_PATH.')\n                else:\n                    raise RuntimeError(base_error_msg)\n\n        return True\nelse:\n    def _init():\n        return False\n\n\ndef version():\n    \"\"\"Returns the version of cuDNN\"\"\"\n    if not _init():\n        return None\n    return __cudnn_version\n\n\nCUDNN_TENSOR_DTYPES = {\n    torch.half,\n    torch.float,"
},
{
    "Id": 97,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0fc110cdd19363f2eb5de68b6eeb82dadc933be0",
    "Violation": "missing",
    "Bug report": "The bug in libcuda.so that required is fixed for libcuda.so versions >= 11.4.This PR changes replay() to sync after each launch only if the process's in-use libcuda.so is < 11.4. With all the \"enhanced\" and \"forward\" compatibility promises flying around, and the fact that \"driver\" sometimes means kernel-mode driver and sometimes means user-mode driver (libcuda.so), I wasn't sure if this PR's check suffices to trigger the sync iff the in-use libcuda.so is 11.4, but Cuda people say what I wrote is reasonable.",
    "Number of deleted lines": 5,
    "Deleted lines": "              \"Default CUDA RNG generator on current device at capture end \"\n              \"is different from default generator on current device \"\n              \"when capture began\");\n  wholegraph_increment_ = gen->capture_epilogue();\n\n  // Now that we've instantiated graph_ into graph_exec_,\n  // we don't need graph_ anymore.\n  AT_CUDA_CHECK(cudaGraphDestroy(graph_));\n  has_graph_ = false;\n#else\n  TORCH_CHECK(false, \"CUDA graphs may only be used in Pytorch built with CUDA >= 11.0\");\n#endif\n}\n\nvoid CUDAGraph::replay() {\n#if CUDA_VERSION >= 11000\n  TORCH_CHECK(has_graph_exec_,\n              \"Called CUDAGraph::replay without a preceding successful capture.\");\n\n  c10::OptionalDeviceGuard device_guard{capture_stream_.device()};\n\n  // Just like any RNG consumer kernel!\n  auto* gen = get_generator_or_default<CUDAGeneratorImpl>(\n      c10::nullopt, cuda::detail::getDefaultCUDAGenerator());\n  PhiloxCudaState rng_engine_inputs;\n  {\n    std::lock_guard<std::mutex> lock(gen->mutex_);\n    rng_engine_inputs = gen->philox_cuda_state(wholegraph_increment_);\n  }\n  offset_extragraph_.fill_(int64_t(rng_engine_inputs.offset_.val));\n\n  // graph_exec_ may be replayed in any stream.\n  AT_CUDA_CHECK(cudaGraphLaunch(graph_exec_, at::cuda::getCurrentCUDAStream()));\n\n  // Temporary workaround for bug in libcuda.so that causes replayed graphs\n  // with certain topologies to be corrupted (kernels elided, internal syncs\n  // ignored) when replayed back to back without a sync in between.\n  // I hate to use a hard sync, but it's the only surefire workaround at the moment.\n  cudaDeviceSynchronize();\n#else\n  TORCH_CHECK(false, \"CUDA graphs may only be used in Pytorch built with CUDA >= 11.0\");\n#endif\n}\n\nvoid CUDAGraph::reset() {\n#if CUDA_VERSION >= 11000\n  // I'd prefer these checks throw exceptions, not print warnings,\n  // but the destructor calls reset(), and at least one CI build\n  // refuses to compile with a throwing destructor.\n  //\n  // Instead of calling reset() in the destructor to clean up, I could\n  // call reset() in the __del__ method of a thin Python wrapper,\n  // in which case reset would be allowed to throw exceptions.\n  // But Stackoverflow does not like user-defined __del__.\n  // __del__ prevents Graph instances from EVER being garbage collected\n  // if they participate in a reference cycle.\n  // And exceptions thrown in __del__ only print a warning anyway.\n  //\n  // Calling reset() in the C++ destructor, with warnings instead of exceptions\n  // if calls fail, is the compromise we chose.\n  //\n  // If capture_begin, the capture, or capture_end failed at some point, this CUDAGraph, the generator,\n  // and the allocator could end up in all kinds of weird states depending where failure occurred.\n  // If the user catches the failure exception in a script, or is running in REPL or (god forbid)\n  // a Juptyer notebook, I don't see an easy way for reset() to gracefully fix all such possible error states.\n  if (has_graph_ || has_graph_exec_) {\n    // notifyCaptureDestroy may throw. How should we handle this?\n    c10::cuda::CUDACachingAllocator::notifyCaptureDestroy(capture_dev_, mempool_id_);\n  }\n  if (has_graph_) {\n    C10_CUDA_CHECK_WARN(cudaGraphDestroy(graph_));\n  }\n  if (has_graph_exec_) {\n    C10_CUDA_CHECK_WARN(cudaGraphExecDestroy(graph_exec_));\n  }"
},
{
    "Id": 98,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ef44faece2cd4045f58cbbac6c74842b84ac6c45",
    "Violation": "insufficient",
    "Bug report": " check attribute existence in torch.legay.nn.SpatialFullConvolution in method type",
    "Number of deleted lines": 2,
    "Deleted lines": "        return self.gradInput\n\n    def accGradParameters(self, input, gradOutput, scale=1):\n        inputTensor = input\n        adjW, adjH = self.adjW, self.adjH\n\n        # The input can be a table where the second element indicates the target\n        # output size, in which case the adj factors are computed automatically\n        if isinstance(inputTensor, list):\n            inputTensor = input[0]\n            targetTensor = input[1]\n            tDims = targetTensor.dim()\n            tH = targetTensor.size(tDims - 2)\n            tW = targetTensor.size(tDims - 1)\n            adjW = calculateAdj(tW, self.kW, self.padW, self.dW)\n            adjH = calculateAdj(tH, self.kH, self.padH, self.dH)\n\n        inputTensor, gradOutput = self._makeContiguous(inputTensor, gradOutput)\n        self._backend.SpatialFullConvolution_accGradParameters(\n            self._backend.library_state,\n            inputTensor,\n            gradOutput,\n            self.gradWeight,\n            self.gradBias,\n            self.finput,\n            self.fgradInput,\n            self.kW, self.kH,\n            self.dW, self.dH,\n            self.padW, self.padH,\n            adjW, adjH,\n            scale\n        )\n\n    def type(self, type=None, tensorCache=None):\n        if self.finput is not None:\n            self.finput = torch.Tensor()\n        if self.fgradInput is not None:\n            self.fgradInput = torch.Tensor()\n        return super(SpatialFullConvolution, self).type(type, tensorCache)\n\n    def __repr__(self):\n        s = super(SpatialFullConvolution, self).__repr__()\n        s += '({} -> {}, {}x{}'.format(self.nInputPlane, self.nOutputPlane, self.kW, self.kH)\n        if self.dW != 1 or self.dH != 1 or self.padW != 0 or self.padH != 0:\n            s += ', {}, {}'.format(self.dW, self.dH)\n\n        if (self.padW or self.padH) and (self.padW != 0 or self.padH != 0):\n            s += ', {}, {}'.format(self.padW, self.padH)\n\n        if (self.adjW or self.adjH) and (self.adjW != 0 or self.adjH != 0):\n            s += ', {}, {}'.format(self.adjW, self.adjH)\n\n        s += ')'\n        if self.bias is None:\n            s += ' without bias'\n        return s\n\n    def clearState(self):\n        clear(self, 'finput', 'fgradInput', '_input', '_gradOutput')\n        return super(SpatialFullConvolution, self).clearState()\n"
},
{
    "Id": 99,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5c93ca258bab7bd74a8ec94d64647e48c8ad8797",
    "Violation": "insufficient",
    "Bug report": "check attribute existence in SpatialFullConvolution",
    "Number of deleted lines": 4,
    "Deleted lines": "    def _makeContiguous(self, input, gradOutput=None):\n        if not input.is_contiguous():\n            if self._input is None:\n                self._input = input.new()\n            self._input.resize_as_(input).copy_(input)\n            input = self._input\n\n        if gradOutput is not None:\n            if not gradOutput.is_contiguous():\n                if self._gradOutput is None:\n                    self._gradOutput = gradOutput.new()\n                self._gradOutput.resize_as_(gradOutput).copy_(gradOutput)\n                gradOutput = self._gradOutput\n            return input, gradOutput\n\n        return input\n\n    def _calculateAdj(self, targetSize, ker, pad, stride):\n        return (targetSize + 2 * pad - ker) % stride\n\n    def updateOutput(self, input):\n        inputTensor = input\n        adjW, adjH = self.adjW, self.adjH\n\n        # The input can be a table where the second element indicates the target\n        # output size, in which case the adj factors are computed automatically\n        if isinstance(input, list):\n            inputTensor = input[0]\n            targetTensor = input[1]\n            tDims = targetTensor.dim()\n            tH = targetTensor.size(tDims - 2)\n            tW = targetTensor.size(tDims - 1)\n            adjW = self._calculateAdj(tW, self.kW, self.padW, self.dW)\n            adjH = self._calculateAdj(tH, self.kH, self.padH, self.dH)\n            if self.finput is None:\n                self.finput = input[0].new()\n            if self.fgradInput is None:\n                self.fgradInput = input[0].new()\n        else:\n            if self.finput is None:\n                self.finput = input.new()\n            if self.fgradInput is None:\n                self.fgradInput = input.new()\n\n        inputTensor = self._makeContiguous(inputTensor)\n        self._backend.SpatialFullConvolution_updateOutput(\n            self._backend.library_state,\n            inputTensor,\n            self.output,\n            self.weight,\n            self.bias,\n            self.finput,\n            self.fgradInput,\n            self.kW, self.kH,\n            self.dW, self.dH,\n            self.padW, self.padH,\n            adjW, adjH\n        )\n        return self.output\n\n    def updateGradInput(self, input, gradOutput):\n        if self.gradInput is None:\n            return\n        inputTensor = input\n        adjW, adjH = self.adjW, self.adjH\n\n        # The input can be a table where the second element indicates the target\n        # output size, in which case the adj factors are computed automatically\n        if isinstance(input, list):\n            inputTensor = input[0]\n            targetTensor = input[1]\n            tDims = targetTensor.dim()\n            tH = targetTensor.size(tDims - 2)\n            tW = targetTensor.size(tDims - 1)\n            adjW = self._calculateAdj(tW, self.kW, self.padW, self.dW)\n            adjH = self._calculateAdj(tH, self.kH, self.padH, self.dH)\n        # Momentarily extract the gradInput tensor\n        if isinstance(self.gradInput, list):"
},
{
    "Id": 100,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c5fdcd85c7570b654eec45b6cba7cc75b0cf8f6b",
    "Violation": "insufficient",
    "Bug report": "check pruned attributes before deleting. I copyed a pruned model after deleteing the derived tensors. In order to be able to reparameter the model, we should check the existence of the tensors here.",
    "Number of deleted lines": 1,
    "Deleted lines": "                ``default_mask``).\n            default_mask (torch.Tensor, optional): mask from previous pruning\n                iteration, if any. To be considered when determining what\n                portion of the tensor that pruning should act on. If None,\n                default to a mask of ones.\n\n        Returns:\n            pruned version of tensor ``t``.\n        \"\"\"\n        if default_mask is None:\n            default_mask = torch.ones_like(t)\n        return t * self.compute_mask(t, default_mask=default_mask)\n\n    def remove(self, module):\n        r\"\"\"Removes the pruning reparameterization from a module. The pruned\n        parameter named ``name`` remains permanently pruned, and the parameter\n        named ``name+'_orig'`` is removed from the parameter list. Similarly,\n        the buffer named ``name+'_mask'`` is removed from the buffers.\n\n        Note:\n            Pruning itself is NOT undone or reversed!\n        \"\"\"\n        # before removing pruning from a tensor, it has to have been applied\n        assert (\n            self._tensor_name is not None\n        ), \"Module {} has to be pruned\\\n            before pruning can be removed\".format(\n            module\n        )  # this gets set in apply()\n\n        # to update module[name] to latest trained weights\n        weight = self.apply_mask(module)  # masked weights\n\n        # delete and reset\n        delattr(module, self._tensor_name)\n        orig = module._parameters[self._tensor_name + \"_orig\"]\n        orig.data = weight.data\n        del module._parameters[self._tensor_name + \"_orig\"]\n        del module._buffers[self._tensor_name + \"_mask\"]\n        setattr(module, self._tensor_name, orig)\n\n\nclass PruningContainer(BasePruningMethod):\n    \"\"\"Container holding a sequence of pruning methods for iterative pruning.\n    Keeps track of the order in which pruning methods are applied and handles\n    combining successive pruning calls.\n\n    Accepts as argument an instance of a BasePruningMethod or an iterable of\n    them.\n    \"\"\"\n\n    def __init__(self, *args):\n        self._pruning_methods = tuple()\n        if not isinstance(args, Iterable):  # only 1 item\n            self._tensor_name = args._tensor_name\n            self.add_pruning_method(args)\n        elif len(args) == 1:  # only 1 item in a tuple\n            self._tensor_name = args[0]._tensor_name\n            self.add_pruning_method(args[0])\n        else:  # manual construction from list or other iterable (or no args)\n            for method in args:\n                self.add_pruning_method(method)\n\n    def add_pruning_method(self, method):\n        r\"\"\"Adds a child pruning ``method`` to the container.\n\n        Args:\n            method (subclass of BasePruningMethod): child pruning method\n                to be added to the container.\n        \"\"\"\n        # check that we're adding a pruning method to the container"
},
{
    "Id": 101,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/678c08bb55eef0c2e707a17d0cd6e50f5b9bd427",
    "Violation": "missing",
    "Bug report": "_ProcessGroupWrapper check needs to be gated on Gloo availability, this fails when gloo is not avail_ProcessGroupWrapper check needs to be gated on Gloo availability, this fails when gloo is not avail.",
    "Number of deleted lines": 4,
    "Deleted lines": "    input_tensor_list = [\n        t if not t.is_complex() else torch.view_as_real(t) for t in input_tensor_list\n    ]\n\n    if group is None:\n        default_pg = _get_default_group()\n        work = default_pg.allgather(output_tensor_lists, input_tensor_list)\n    else:\n        work = group.allgather(output_tensor_lists, input_tensor_list)\n\n    if async_op:\n        return work\n    else:\n        work.wait()\n\n\ndef _object_to_tensor(obj):\n    f = io.BytesIO()\n    _pickler(f).dump(obj)\n    byte_storage = torch.ByteStorage.from_buffer(f.getvalue())  # type: ignore[attr-defined]\n    # Do not replace `torch.ByteTensor` or `torch.LongTensor` with torch.tensor and specifying dtype.\n    # Otherwise, it will casue 100X slowdown.\n    # See: https://github.com/pytorch/pytorch/issues/65696\n    byte_tensor = torch.ByteTensor(byte_storage)\n    local_size = torch.LongTensor([byte_tensor.numel()])\n    return byte_tensor, local_size\n\n\ndef _tensor_to_object(tensor, tensor_size):\n    buf = tensor.numpy().tobytes()[:tensor_size]\n    return _unpickler(io.BytesIO(buf)).load()\n\ndef _check_for_nccl_backend(group):\n    pg = group or _get_default_group()\n    # It is not expected for PG to be wrapped many times, but support it just\n    # in case\n    while isinstance(pg, _ProcessGroupWrapper):\n        pg = pg.wrapped_pg\n\n    return (\n        is_nccl_available() and\n        isinstance(pg, ProcessGroupNCCL)\n    )\n\ndef all_gather_object(object_list, obj, group=None):\n    \"\"\"\n    Gathers picklable objects from the whole group into a list. Similar to\n    :func:`all_gather`, but Python objects can be passed in. Note that the object\n    must be picklable in order to be gathered.\n\n    Args:\n        object_list (list[Any]): Output list. It should be correctly sized as the\n            size of the group for this collective and will contain the output.\n        object (Any): Pickable Python object to be broadcast from current process.\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used. Default is ``None``.\n\n    Returns:\n        None. If the calling rank is part of this group, the output of the\n        collective will be populated into the input ``object_list``. If the\n        calling rank is not part of the group, the passed in ``object_list`` will\n        be unmodified.\n\n    .. note:: Note that this API differs slightly from the :func:`all_gather`\n        collective since it does not provide an ``async_op`` handle and thus\n        will be a blocking call.\n\n    .. note:: For NCCL-based processed groups, internal tensor representations\n        of objects must be moved to the GPU device before communication takes\n        place. In this case, the device used is given by\n        ``torch.cuda.current_device()`` and it is the user's responsiblity to\n        ensure that this is set so that each rank has an individual GPU, via\n        ``torch.cuda.set_device()``.\n"
},
{
    "Id": 102,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/db1ac4e29b0f557711190c8d49d4afb5da1844e8",
    "Violation": "insufficient",
    "Bug report": "Summary: We should explicitly check for the gloo backend instead of relying on the shard's device, because user might pass a GPU tensor as input and a process group gloo as the pg, and expect that should work.",
    "Number of deleted lines": 1,
    "Deleted lines": "captured in the graph and the backend should implement the op however it prefers.\n\"\"\"\n# TODO assert if ranks has duplicated entries\ndef _all_reduce(self, reduceOp, tag, ranks, group_size):\n    op = _str_to_reduce_op(reduceOp)\n    group = c10d._find_or_create_pg_by_ranks_and_tag(tag, ranks, group_size)\n    assert group is not None\n\n    inplace_tensor = self.clone(memory_format=torch.contiguous_format)\n    work = dist.all_reduce(inplace_tensor, op=op, group=group, async_op=True)\n    _register_tensor_work(inplace_tensor, work)\n\n    return inplace_tensor\n\ndef _all_reduce_coalesced(self, reduceOp, tag, ranks, group_size):\n    op = _str_to_reduce_op(reduceOp)\n    group = c10d._find_or_create_pg_by_ranks_and_tag(tag, ranks, group_size)\n    assert group is not None\n\n    inplace_tensor_list = [t.clone(memory_format=torch.contiguous_format) for t in self]\n    work = dist.all_reduce_coalesced(inplace_tensor_list, op=op, group=group, async_op=True)\n    _register_tensor_work(inplace_tensor_list, work)\n\n    return inplace_tensor_list\n\ndef _all_gather_into_tensor(shard, tag, ranks, group_size):\n    # TODO add dim support?\n    group = c10d._find_or_create_pg_by_ranks_and_tag(tag, ranks, group_size)\n    assert group is not None\n    out_size = list(shard.size())\n    out_size[0] *= group_size\n    out_tensor = shard.new_empty(out_size)\n    assert out_tensor.is_contiguous()\n    # FIXME gloo doesn't support _allgather_base\n    if shard.is_cpu:\n        tensor_list = list(torch.chunk(out_tensor, group_size))\n        work = dist.all_gather(tensor_list, shard, group=group, async_op=True)\n    else:\n        work = dist.all_gather_into_tensor(out_tensor, shard, group=group, async_op=True)\n    _register_tensor_work(out_tensor, work)\n\n    return out_tensor\n\n\ndef _all_gather_into_tensor_coalesced(self, tag, rankset, group_size):\n    group = c10d._find_or_create_pg_by_ranks_and_tag(tag, rankset, group_size)\n    assert group is not None\n\n    def mk_out_tensor(shard):\n        out_size = list(shard.size())\n        out_size[0] *= group_size\n        out_tensor = shard.new_empty(out_size)\n        assert out_tensor.is_contiguous()\n        return out_tensor\n\n    out_tensors = [mk_out_tensor(t) for t in self]\n\n    work_list = _all_gather_into_tensor_coalesced_fallback(\n        output_tensors=out_tensors,\n        input_tensors=self,\n        group=group,\n        async_op=False)\n\n\n    _register_tensor_work(out_tensors, work_list)\n    return out_tensors\n\n\ndef _reduce_scatter_tensor(\n    input: torch.Tensor,\n    reduceOp: str,"
},
{
    "Id": 103,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3ef4d697df5bfdbd27dfc7a79c0679da2b87e3af",
    "Violation": "missing",
    "Bug report": "default backend need to check for nccl availability. As titled, we can only initialize nccl backend when NCCL is available",
    "Number of deleted lines": 4,
    "Deleted lines": "            )\n        assert name.upper() not in Backend._plugins, (\n            f\"{name.upper()} c10d backend creator function already exist\"\n        )\n\n        setattr(Backend, name.upper(), name.upper())\n        Backend.backend_list.append(name.lower())\n\n        # Update device capability matrix in Backend class\n        if devices is None:\n            # This is more of a backward support for groups like `threaded`:\n            # assume default devices \"cpu\" and \"cuda\", but warn\n            warnings.warn(\n                f\"Device capability of {name} unspecified, assuming `cpu` and \"\n                \"`cuda`. Please specify it via the `devices` argument of \"\n                \"`register_backend`.\"\n            )\n            Backend.backend_capability[name.lower()] = [\"cpu\", \"cuda\"]\n        elif isinstance(devices, str):\n            # Single device string specified. Simply convert to list.\n            Backend.backend_capability[name.lower()] = [devices]\n        else:\n            Backend.backend_capability[name.lower()] = devices\n\n        Backend._plugins[name.upper()] = Backend._BackendPlugin(func, extended_api)\n\nclass BackendConfig:\n\n    def __init__(self, backend: Union[str, Backend]):\n        self.device_backend_map: Dict[torch.device, Backend] = {}\n\n        if backend == Backend.UNDEFINED:\n            # default config when backend is not specified\n            # supported since PyTorch 2.0\n            self.device_backend_map = {\n                \"cpu\": Backend.GLOO,\n                \"cuda\": Backend.NCCL,\n            }\n        elif backend.lower() in Backend.backend_list:\n            # Cases for when backend is a single string (without device types)\n            # e.g. \"nccl\", \"gloo\", \"ucc\", \"mpi\"\n            supported_devices = Backend.backend_capability[backend.lower()]\n            backend_val = Backend(backend)\n            self.device_backend_map = {\n                device : backend_val for device in supported_devices\n            }\n        elif \":\" in backend.lower():\n            # Backend specified in \"device:backend\" format\n            # make sure the backend string is in the correct format\n            # \"{device_type1}:{backend1},{device_type2}:{backend2}\"\n            # e.g. \"cpu:gloo,cuda:nccl\"\n            backend_str_error_message = f\"\"\"The custom backend string argument is invalid: {backend}.\n                Custom backend string is an experimental feature where the backend string must be in the format:\n                \"<device_type1>:<backend1>,<device_type2>:<backend2>...\". e.g. 'cpu:gloo,cuda:nccl'\"\"\"\n\n            # parse the backend string and populate the device_backend_map\n            for device_backend_pair_str in backend.lower().split(\",\"):\n                device_backend_pair = device_backend_pair_str.split(\":\")\n                if len(device_backend_pair) != 2:\n                    raise ValueError(f\"Invalid device:backend pairing: \\\n                                     {device_backend_pair_str}. {backend_str_error_message}\")\n                device, backend = device_backend_pair\n                if device in self.device_backend_map:\n                    raise ValueError(f\"Duplicate device type {device} \\\n                                     in backend string: {backend}. {backend_str_error_message}\")\n                self.device_backend_map[device] = Backend(backend)\n        else:\n            # User specified a single backend name whose device capability is\n            # unknown, assuming it can support the default devices of PyTorch\n            # (cpu and cuda)\n            warnings.warn(\n                f\"Device capability of {backend} unknown, assuming `cpu` and \"\n                \"`cuda`. You can specify it in `device:backend` format in \"\n                \"`init_process_group` call.\""
},
{
    "Id": 104,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9bb1371cc20a14907dbc47bc98e3ac5de866e34b",
    "Violation": "missing",
    "Bug report": "Disable RDYNAMIC check with MSVC. Summary: When testing with clang-cl, the flag is added though it is unsupported and that generates a few warnings.",
    "Number of deleted lines": 4,
    "Deleted lines": "     // check avx512f\n     __m512 addConstant(__m512 arg) {\n       return _mm512_add_ps(arg, _mm512_set1_ps(1.f));\n     }\n     // check avx512dq\n     __m512 andConstant(__m512 arg) {\n       return _mm512_and_ps(arg, _mm512_set1_ps(1.f));\n     }\n     int main() {\n       __m512i a = _mm512_set1_epi32(1);\n       __m256i ymm = _mm512_extracti64x4_epi64(a, 0);\n       ymm = _mm256_abs_epi64(ymm); // check avx512vl\n       __mmask16 m = _mm512_cmp_epi32_mask(a, a, _MM_CMPINT_EQ);\n       __m512i r = _mm512_andnot_si512(a, a);\n     }\" CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS)\nif(CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS)\n  message(STATUS \"Current compiler supports avx512f extension. Will build fbgemm.\")\n  # Also see CMakeLists.txt under caffe2/perfkernels.\n  set(CAFFE2_PERF_WITH_AVX512 1)\nendif()\ncmake_pop_check_state()\n\n# ---[ Checks if compiler supports -fvisibility=hidden\ncheck_cxx_compiler_flag(\"-fvisibility=hidden\" COMPILER_SUPPORTS_HIDDEN_VISIBILITY)\ncheck_cxx_compiler_flag(\"-fvisibility-inlines-hidden\" COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY)\nif(${COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY})\n  set(CAFFE2_VISIBILITY_FLAG \"-fvisibility-inlines-hidden\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${CAFFE2_VISIBILITY_FLAG}\")\nendif()\n\n# ---[ Checks if linker supports -rdynamic. `-rdynamic` tells linker\n# -to add all (including unused) symbols into the dynamic symbol\n# -table. We need this to get symbols when generating backtrace at\n# -runtime.\ncheck_cxx_compiler_flag(\"-rdynamic\" COMPILER_SUPPORTS_RDYNAMIC)\nif(${COMPILER_SUPPORTS_RDYNAMIC})\n  set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -rdynamic\")\n  set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -rdynamic\")\nendif()\n\n# ---[ If we are using msvc, set no warning flags\n# Note(jiayq): if you are going to add a warning flag, check if this is\n# totally necessary, and only add when you see fit. If it is needed due to\n# a third party library (like Protobuf), mention it in the comment as\n# \"THIRD_PARTY_NAME related\"\n# From https://docs.microsoft.com/en-us/cpp/error-messages/compiler-warnings/\nif(${CMAKE_CXX_COMPILER_ID} STREQUAL \"MSVC\")\n  add_compile_options(\n      ##########################################\n      # Protobuf related. Cannot remove.\n      # This is directly copied from\n      #     https://github.com/google/protobuf/blob/master/cmake/README.md\n      ##########################################\n      /wd4018 # 'expression' : signed/unsigned mismatch\n      /wd4065 # (3): switch with default but no case.\n      /wd4146 # unary minus operator applied to unsigned type, result still unsigned\n      /wd4244 # Conversion from 'type1' to 'type2', possible loss of data.\n      /wd4251 # 'identifier' : class 'type' needs to have dll-interface to be used by clients of class 'type2'\n      /wd4267 # Conversion from 'size_t' to 'type', possible loss of data.\n      /wd4305 # 'identifier' : truncation from 'type1' to 'type2'\n      /wd4355 # 'this' : used in base member initializer list\n      /wd4506 # (1): no definition for inline function. Protobuf related.\n      /wd4661 # No suitable definition provided for explicit template instantiation request\n      /wd4800 # 'type' : forcing value to bool 'true' or 'false' (performance warning)\n      /wd4996 # 'function': was declared deprecated\n      ##########################################\n      # Third party related. Cannot remove.\n      ##########################################\n      /wd4141 # (1): inline used twice. google benchmark related.\n      /wd4503 # (1): decorated name length exceeded, name was truncated.\n              #      Eigen related.\n      /wd4554 # (3): check operator precedence for possible error.\n              # Eigen related.\n      /wd4805 # (1): Unsafe mix of types in gtest/gtest.h. Gtest related."
},
{
    "Id": 105,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/48e675ac7519666ed5e96d8d49c468dfc15a5d66",
    "Violation": "missing",
    "Bug report": "fx quant: fix subtle bug in BinaryOpQuantizeHanlder logic in matching. When matching a pattern to `BinaryOpQuantizeHandler`, we need to make sure we check for dtype support on the base node, instead of the current node.  This is important in cases such as `add-relu` and `mul-relu`, when the current node is `relu`, but the base node is `add|mul`.",
    "Number of deleted lines": 3,
    "Deleted lines": "            custom_module_classes = []\n\n        if standalone_module_classes is None:\n            standalone_module_classes = []\n\n        if standalone_module_names is None:\n            standalone_module_names = []\n\n        match_map: Dict[str, MatchResult] = {}\n        all_matched : Set[str] = set()\n\n        def record_match(pattern, node, matched):\n            if isinstance(pattern, tuple):\n                s, *args = pattern\n                record_match(s, node, matched)\n                if pattern[0] is not getattr:\n                    for subpattern, arg in zip(args, node.args):\n                        record_match(subpattern, arg, matched)\n            else:\n                matched.append(node)\n\n        cache_for_no_tensor_check: Dict[Node, bool] = dict()\n        for node in reversed(graph.nodes):\n            if node.name not in match_map and node.name not in all_matched:\n                for pattern, value in patterns.items():\n                    if is_match(modules, node, pattern):\n                        skip_this_match = False\n                        if value is BinaryOpQuantizeHandler:\n                            use_copy_node = all_node_args_have_no_tensors(node, modules, cache_for_no_tensor_check)\n                            if use_copy_node:\n                                # TODO(future PR): update the pattern to quantize\n                                # handler logic to take this into account.\n                                value = CopyNodeQuantizeHandler  # type: ignore\n\n                            this_node_qconfig = self.qconfig_map[node.name]\n                            if this_node_qconfig:\n                                dtypes = get_qconfig_dtypes(this_node_qconfig)\n                                # TODO(future PR): update the pattern to quantize\n                                # handler logic to take this into account.\n                                skip_this_match = (\n                                    (node.target in binary_op_supported_dtypes) and\n                                    (dtypes not in binary_op_supported_dtypes[node.target])\n                                )\n\n                        if not skip_this_match:\n                            matched: List[Any] = []\n                            record_match(pattern, node, matched)\n                            for n in matched:\n                                match_map[n.name] = (\n                                    node, matched, pattern, value(self, node),  # type: ignore\n                                    self.qconfig_map[n.name])\n                                all_matched.add(n.name)\n                            # break after finding the first match\n                            break\n\n        # add custom module instances to the match result\n        assert self.modules is not None\n        for node in graph.nodes:\n            if node.op == 'call_module' and \\\n               type(self.modules[node.target]) in custom_module_classes:\n                custom_module_qconfig = self.qconfig_map[node.name]\n                match_map[node.name] = (\n                    node, [node], None, CustomModuleQuantizeHandler(self, node),\n                    custom_module_qconfig)\n\n        def is_standalone_module(node_target):\n            assert self.modules is not None\n            return (\n                node_target in standalone_module_names or  # type: ignore\n                type(self.modules[node_target]) in standalone_module_classes  # type: ignore\n            )\n\n        # add standalone modules to the match\n        for node in graph.nodes:\n            if node.op == 'call_module' and \\\n               (is_standalone_module(node.target) or\n                    is_observed_standalone_module(self.modules[node.target])):\n                # add node to matched nodes"
},
{
    "Id": 106,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/62732bdcdb8b6112e01366d4ad1c2a50e61da1ed",
    "Violation": "insufficient",
    "Bug report": "quick fix for invalid nodes. Summary: As title.Need to check whether node is valid before fusion",
    "Number of deleted lines": 0,
    "Deleted lines": "                    args=(cat_biases, batch_input, transposed_weights),\n                )\n            else:\n                fused_lhs = graph.call_function(\n                    torch.mm,\n                    args=(batch_input, transposed_weights),\n                )\n            fused_lhs_list = graph.call_function(\n                torch.split, args=((fused_lhs, split_sections, 1))\n            )\n\n        for i, node in enumerate(batch_nodes):\n            with graph.inserting_after(fused_lhs_list):\n                new_node = graph.call_function(\n                    operator.getitem, args=(fused_lhs_list, i)\n                )\n            node.replace_all_uses_with(new_node)\n            new_node.meta.update(node.meta)\n            graph.erase_node(node)\n\n\ndef is_node_meta_valid(node):\n    if node is None:\n        return True\n    if \"example_value\" not in node.meta:\n        return False\n    return True\n\n\ndef is_linear_node_can_be_fused(node):\n    input = get_arg_value(node, 0, \"input\")\n    weight = get_arg_value(node, 1, \"weight\")\n    return (\n        is_node_meta_valid(node)\n        and len(input.meta[\"example_value\"].shape) == 2\n        and len(weight.meta[\"example_value\"].shape) == 2\n    )\n\n\nclass BatchLinearFusion(BatchFusion):\n    \"\"\"\n    Batch linear fusion in pre grad pass.\n    Fuse linear with same size with torch.baddmm\n    \"\"\"\n\n    def _getitem_args(self, getitem_node: torch.fx.Node):\n        if getitem_node.target != operator.__getitem__ or (\n            getitem_node.op != \"call_function\"\n        ):\n            return None\n        return getitem_node.args[0]\n\n    def match(self, node):\n        if CallFunctionVarArgs(torch.nn.functional.linear).match(\n            node\n        ) and is_linear_node_can_be_fused(node):\n            input = get_arg_value(node, 0, \"input\")\n            weight = get_arg_value(node, 1, \"weight\")\n            bias = get_arg_value(node, 2, \"bias\")\n            group_key = (\n                \"batch_linear_pre_grad\",\n                self._getitem_args(input),\n                str(input.meta[\"example_value\"].shape),\n                str(weight.meta[\"example_value\"].shape),\n                bias is None,\n            )\n        else:\n            group_key = None\n        return group_key\n"
},
{
    "Id": 107,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/aab55d6d0d7b958e32cfdbb69794e107cfceb6bc",
    "Violation": "missing",
    "Bug report": "Remove all the dequant nodes when the ref module has multi input args. When converting a ref module into a quant module, `_lower_static_weighted_ref_module` pass assumes the `ref_node` only has 1 input node, and only remove the first `dequant` node. We add a check in this PR to ensure this is the case for `_lower_static_weighted_ref_module` pass.",
    "Number of deleted lines": 0,
    "Deleted lines": "    modules = dict(model.named_modules(remove_duplicate=False))\n    nodes = list(model.graph.nodes)\n    for n in model.graph.nodes:\n        # Step 0: Find nodes that match this pattern (dequantize - ref module - quantize)\n        matching_modules = list(STATIC_LOWER_MODULE_MAP.keys()) + list(STATIC_LOWER_FUSED_MODULE_MAP.keys())\n        (q_node, relu_node, ref_node) = _match_static_pattern(\n            n, modules, qconfig_map, matching_modules, dequantize_node_arg_indices=[0])  # type: ignore[arg-type]\n        if q_node is None:\n            continue\n        assert(ref_node is not None)\n        (_, scale_node, zero_point_node, _) = q_node.args\n        ref_module = _get_module(ref_node, modules)\n        ref_class = type(ref_module)\n        assert(isinstance(scale_node, Node))\n        assert(isinstance(zero_point_node, Node))\n        assert(issubclass(ref_class, nn.Module))\n\n        # Step 1: Change this pattern to use the corresponding quantized module\n        # For fused modules, we also check whether the inner module is a reference module\n        # If so, we replace the entire fused module with the corresponding quantized module\n        if ref_class in STATIC_LOWER_FUSED_MODULE_MAP:\n            inner_ref_class, q_class = STATIC_LOWER_FUSED_MODULE_MAP[ref_class]\n            if type(ref_module[0]) != inner_ref_class:  # type: ignore[index]\n                continue\n        else:\n            q_class = STATIC_LOWER_MODULE_MAP[ref_class]\n        output_scale = getattr(model, scale_node.target)\n        output_zero_point = getattr(model, zero_point_node.target)\n        q_module = q_class.from_reference(ref_module, output_scale, output_zero_point)\n        # replace reference module with quantized module\n        parent_name, module_name = _parent_name(ref_node.target)\n        setattr(modules[parent_name], module_name, q_module)\n\n        # Step 2: Reroute around dq_node, and remove q_node and its args\n        dq_node = ref_node.args[0]\n        assert(isinstance(dq_node, Node))\n        ref_node.replace_input_with(dq_node, dq_node.args[0])\n        q_node.replace_all_uses_with(ref_node)\n        model.graph.erase_node(q_node)\n        model.graph.erase_node(scale_node)\n        model.graph.erase_node(zero_point_node)\n\ndef _lower_dynamic_weighted_ref_module(model: QuantizedGraphModule):\n    \"\"\"\n    Traverse the graph and find quantize_per_tensor_dynamic - dequantize - ref_module patterns\n    and replace them with the dynamically quantized version of the ref module.\n    \"\"\"\n    named_modules = dict(model.named_modules(remove_duplicate=False))\n    for n in model.graph.nodes:\n        if n.op != \"call_module\" or \\\n           type(named_modules[str(n.target)]) not in \\\n           set(DYNAMIC_LOWER_MODULE_MAP.keys()).union(\n               set(DYNAMIC_LOWER_FUSED_MODULE_MAP.keys())):\n            continue\n        ref_node = n\n        dq_node = ref_node.args[0]\n        if dq_node.op != \"call_method\" or dq_node.target != \"dequantize\":\n            continue\n\n        input_dynamic_q_node = dq_node.args[0]\n\n        if input_dynamic_q_node.op != \"call_function\" or \\\n           input_dynamic_q_node.target != torch.quantize_per_tensor_dynamic:\n            continue\n\n        activation_dtype = input_dynamic_q_node.args[1]\n        is_fp16 = activation_dtype == torch.float16\n        is_int8 = activation_dtype in [torch.quint8, torch.qint8]\n        if not is_int8 and not is_fp16:\n            continue"
},
{
    "Id": 108,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bae895cef0c12df5f64afa155ce5462e06f0e04a",
    "Violation": "missing",
    "Bug report": "Added check for kHIP in ATen/native/Copy.cpp",
    "Number of deleted lines": 0,
    "Deleted lines": "    at::_copy_from(src, self, non_blocking);\n    return self;\n  }\n\n  if (self.is_quantized() && !src.is_quantized()) {\n    return quantized_copy_from_float_cpu_(self, src);\n  }\n\n  if (self.is_quantized() && src.is_quantized()) {\n    TORCH_CHECK(self.qscheme() == src.qscheme(),\n                \"Quantized Copy only works with same qscheme\");\n    TORCH_CHECK(self.scalar_type() == src.scalar_type());\n    self.set_quantizer_(src.quantizer());\n  }\n\n  if (!self.is_quantized() && src.is_quantized()) {\n    TORCH_CHECK(false, \"Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor\");\n  }\n\n  auto iter = TensorIterator();\n  iter.set_check_mem_overlap(true);\n  iter.add_output(self);\n  iter.add_input(src);\n  iter.dont_resize_outputs();\n  iter.dont_compute_common_dtype();\n  iter.build();\n\n  if (iter.numel() == 0) {\n    return self;\n  }\n\n  DeviceType device_type = iter.device_type(0);\n  if (iter.device_type(1) == kCUDA) {\n    device_type = kCUDA;\n  }\n\n  // TODO: if we need to, we can also enable this path for quantized tensor\n  if (device_type == kCPU && copy_transpose_valid(self, src) && !self.is_quantized()) {\n    copy_same_type_transpose_(self, src);\n    return self;\n  }\n\n  copy_stub(device_type, iter, non_blocking);\n  return self;\n}\n\nTensor& copy_(Tensor& self, const Tensor& src, bool non_blocking) {\n  auto maybe_outnames = namedinference::compute_broadcast_outnames(self, src);\n  {\n    NoNamesGuard guard;\n    copy_impl(self, src, non_blocking);\n  }\n  namedinference::propagate_names_if_nonempty(self, maybe_outnames);\n  return self;\n}\n\nTORCH_LIBRARY_IMPL(aten, CatchAll, m) { m.impl_UNBOXED(\"copy_\", copy_); }\nDEFINE_DISPATCH(copy_stub);\n\n} // namespace native\n} // namespace at\n"
},
{
    "Id": 109,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cf348bcdeecfe0b47a2245d95eaa8ef37fb7b53e",
    "Violation": "missing",
    "Bug report": "tighten hasCUDA check",
    "Number of deleted lines": 0,
    "Deleted lines": "Context::Context()\n: thc_state(nullptr) {\n\n  THSetDefaultErrorHandler(errorHandler,nullptr);\n  THSetDefaultArgErrorHandler(argErrorHandler,nullptr);\n\n  generator_registry[static_cast<int>(Backend::CPU)]\n    .reset(new CPUGenerator(this));\n  Type::registerAll(this);\n}\nvoid Context::doInitCUDA() {\n#ifdef AT_CUDA_ENABLED\n  thc_state = THCState_alloc();\n  THCState_setDeviceAllocator(thc_state, THCCachingAllocator_get());\n  thc_state->cudaHostAllocator = &THCCachingHostAllocator;\n  THCudaInit(thc_state);\n  generator_registry[static_cast<int>(Backend::CUDA)]\n    .reset(new CUDAGenerator(this));\n#endif\n}\nContext::~Context() {\n#ifdef AT_CUDA_ENABLED\n  if(thc_state)\n    THCState_free(thc_state);\n#endif\n}\n\nContext & globalContext() {\n  static Context globalContext_;\n  return globalContext_;\n}\n\nbool Context::hasCUDA() const {\n#ifdef AT_CUDA_ENABLED\n  return true;\n#else\n  return false;\n#endif\n}\n\n\n}\n"
},
{
    "Id": 110,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/027c0d7f8e37e583c02b372df5331d73793c06b1",
    "Violation": "missing",
    "Bug report": "fixed compilations on xla tensor prin. This is done to avoid compilations during tensor printing. Torch performs some tensor operations like slicing to make the tensor readable. These operations result in compilations. Hence to avoid the compilations, copying the tensor to cpu before printing. Returning from this function would have resulted in 63 compiles, since PDB prints the value of the return output. In this case it is a xla tensor. Now with the current change, there is no compilation.",
    "Number of deleted lines": 0,
    "Deleted lines": "    if dim == 1:\n        if self.size(0) > 2 * PRINT_OPTS.edgeitems:\n            return torch.cat((self[:PRINT_OPTS.edgeitems], self[-PRINT_OPTS.edgeitems:]))\n        else:\n            return self\n    if self.size(0) > 2 * PRINT_OPTS.edgeitems:\n        start = [self[i] for i in range(0, PRINT_OPTS.edgeitems)]\n        end = ([self[i]\n               for i in range(len(self) - PRINT_OPTS.edgeitems, len(self))])\n        return torch.stack([get_summarized_data(x) for x in (start + end)])\n    else:\n        return torch.stack([get_summarized_data(x) for x in self])\n\ndef _str_intern(inp):\n    prefix = 'tensor('\n    indent = len(prefix)\n    suffixes = []\n\n    # This is used to extract the primal value and thus disable the forward AD\n    # within this function.\n    # TODO(albanD) This needs to be updated when more than one level is supported\n    self, tangent = torch.autograd.forward_ad.unpack_dual(inp)\n\n    # Note [Print tensor device]:\n    # A general logic here is we only print device when it doesn't match\n    # the device specified in default tensor type.\n    # Currently torch.set_default_tensor_type() only supports CPU/CUDA, thus\n    # torch._C._get_default_device() only returns either cpu or cuda.\n    # In other cases, we don't have a way to set them as default yet,\n    # and we should always print out device for them.\n    if self.device.type != torch._C._get_default_device()\\\n            or (self.device.type == 'cuda' and torch.cuda.current_device() != self.device.index):\n        suffixes.append('device=\\'' + str(self.device) + '\\'')\n\n    # TODO: add an API to map real -> complex dtypes\n    _default_complex_dtype = torch.cdouble if torch.get_default_dtype() == torch.double else torch.cfloat\n    has_default_dtype = self.dtype in (torch.get_default_dtype(), _default_complex_dtype, torch.int64, torch.bool)\n    if self.is_sparse:\n        suffixes.append('size=' + str(tuple(self.shape)))\n        suffixes.append('nnz=' + str(self._nnz()))\n        if not has_default_dtype:\n            suffixes.append('dtype=' + str(self.dtype))\n        indices_prefix = 'indices=tensor('\n        indices = self._indices().detach()\n        indices_str = _tensor_str(indices, indent + len(indices_prefix))\n        if indices.numel() == 0:\n            indices_str += ', size=' + str(tuple(indices.shape))\n        values_prefix = 'values=tensor('\n        values = self._values().detach()\n        values_str = _tensor_str(values, indent + len(values_prefix))\n        if values.numel() == 0:\n            values_str += ', size=' + str(tuple(values.shape))\n        tensor_str = indices_prefix + indices_str + '),\\n' + ' ' * indent + values_prefix + values_str + ')'\n    elif self.is_sparse_csr:\n        suffixes.append('size=' + str(tuple(self.shape)))\n        suffixes.append('nnz=' + str(self._nnz()))\n        if not has_default_dtype:\n            suffixes.append('dtype=' + str(self.dtype))\n        crow_indices_prefix = 'crow_indices=tensor('\n        crow_indices = self.crow_indices().detach()\n        crow_indices_str = _tensor_str(crow_indices, indent + len(crow_indices_prefix))\n        if crow_indices.numel() == 0:\n            crow_indices_str += ', size=' + str(tuple(crow_indices.shape))\n        col_indices_prefix = 'col_indices=tensor('\n        col_indices = self.col_indices().detach()\n        col_indices_str = _tensor_str(col_indices, indent + len(col_indices_prefix))\n        if col_indices.numel() == 0:\n            col_indices_str += ', size=' + str(tuple(col_indices.shape))\n        values_prefix = 'values=tensor('\n        values = self.values().detach()"
},
{
    "Id": 111,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ee91c328da5739ce03b3127cd7c542ce505212b8",
    "Violation": "improper",
    "Bug report": "Fix cuda/cpu check on NoneType ",
    "Number of deleted lines": 1,
    "Deleted lines": "            # this case will fail anyway, but at least they'll get a useful error message.\n            why_not_fast_path = f\"dtypes of query ({query.dtype}) and self.in_proj_weight ({self.in_proj_weight.dtype}) don't match\"\n        elif self.training:\n            why_not_fast_path = \"training is enabled\"\n        elif not self.batch_first:\n            why_not_fast_path = \"batch_first was not True\"\n        elif self.bias_k is not None:\n            why_not_fast_path = \"self.bias_k was not None\"\n        elif self.bias_v is not None:\n            why_not_fast_path = \"self.bias_v was not None\"\n        elif self.add_zero_attn:\n            why_not_fast_path = \"add_zero_attn was enabled\"\n        elif not self._qkv_same_embed_dim:\n            why_not_fast_path = \"_qkv_same_embed_dim was not True\"\n        elif query.is_nested and (key_padding_mask is not None or attn_mask is not None):\n            why_not_fast_path = \"supplying both src_key_padding_mask and src_mask at the same time \\\n                                 is not supported with NestedTensor input\"\n        elif torch.is_autocast_enabled():\n            why_not_fast_path = \"autocast is enabled\"\n\n        if not why_not_fast_path:\n            tensor_args = (\n                query,\n                key,\n                value,\n                self.in_proj_weight,\n                self.in_proj_bias,\n                self.out_proj.weight,\n                self.out_proj.bias,\n            )\n            # We have to use list comprehensions below because TorchScript does not support\n            # generator expressions.\n            if torch.overrides.has_torch_function(tensor_args):\n                why_not_fast_path = \"some Tensor argument has_torch_function\"\n            elif not all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):\n                why_not_fast_path = \"some Tensor argument is neither CUDA nor CPU\"\n            elif torch.is_grad_enabled() and any([x.requires_grad for x in tensor_args]):\n                why_not_fast_path = (\"grad is enabled and at least one of query or the \"\n                                     \"input/output projection weights or biases requires_grad\")\n            if not why_not_fast_path:\n                merged_mask, mask_type = self.merge_masks(attn_mask, key_padding_mask, query)\n\n                return torch._native_multi_head_attention(\n                    query,\n                    key,\n                    value,\n                    self.embed_dim,\n                    self.num_heads,\n                    self.in_proj_weight,\n                    self.in_proj_bias,\n                    self.out_proj.weight,\n                    self.out_proj.bias,\n                    merged_mask,\n                    need_weights,\n                    average_attn_weights,\n                    mask_type)\n\n        any_nested = query.is_nested or key.is_nested or value.is_nested\n        assert not any_nested, (\"MultiheadAttention does not support NestedTensor outside of its fast path. \" +\n                                f\"The fast path was not hit because {why_not_fast_path}\")\n\n        if self.batch_first and is_batched:\n            # make sure that the transpose op does not affect the \"is\" property\n            if key is value:\n                if query is key:\n                    query = key = value = query.transpose(1, 0)\n                else:\n                    query, key = [x.transpose(1, 0) for x in (query, key)]\n                    value = key\n            else:\n                query, key, value = [x.transpose(1, 0) for x in (query, key, value)]"
},
{
    "Id": 112,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cdab6c8df9ff9331126f69ea59c23f06109f03d7",
    "Violation": "missing",
    "Bug report": "Support specifying None for obs_or_fq_ctr in target_dtype_info. It is cleaner for quantizer to say what does not need observation instead of putting fp32 observers. This diff add support for that by checking if target_dtype_info contains none for specific observers and if so skip inserting observers for those.",
    "Number of deleted lines": 0,
    "Deleted lines": "        # we'll default to False, this makes configuring this field optional for users\n        reuse_input_obs_or_fq = node.meta[\"target_dtype_info\"].get(\"reuse_input_obs_or_fq\", False)\n\n        act_post_process_ctr = node.meta[\"target_dtype_info\"][\"weight_obs_or_fq_ctr\"] if is_weight else \\\n            node.meta[\"target_dtype_info\"][\"input_act_obs_or_fq_ctr\"]\n\n        arg_as_output_target_dtype = _get_arg_target_dtype_as_output(arg, named_modules)\n        arg_as_input_target_dtype = _get_arg_target_dtype_as_input_to_node(arg, node, named_modules)\n        arg_as_input_target_is_dynamic = \\\n            _get_arg_target_is_dynamic_as_input_to_node(arg, node, named_modules)  # type: ignore[arg-type]\n        needs_obs = \\\n            (\n                # the following code block is for static quantization\n                (not arg_as_input_target_is_dynamic) and\n                # if the dtypes are different, we need an observer\n                (arg_as_output_target_dtype != arg_as_input_target_dtype) and\n                # except if the second dtype is float, a dequant will be inserted\n                # without an observer in convert\n                # TODO(future PR): change this so a placeholder is inserted for\n                # future dequants, to make the logic easier to understand\n                (arg_as_input_target_dtype != torch.float) and\n                # if arg output dtype is in _DO_NOT_OBS_DTYPE_LIST do not insert observer\n                (arg_as_output_target_dtype not in _DO_NOT_OBS_DTYPE_LIST) and\n                # we won't insert extra observer for input if the nodes configured with\n                # reuse_input_obs_or_fq\n                not reuse_input_obs_or_fq\n            ) or (\n                # need to add input observer for dynamic quantization\n                # only add observer for first input for now, we may need to extend\n                # qconfig_dict and backend_config to support more general configurations\n                # of dynamic quantization, e.g. dynamically quantizing second input, third\n                # input etc.\n                arg_as_input_target_is_dynamic and arg is node.args[0]\n            )\n\n    else:\n        assert qconfig is not None\n        # custom flow for standalone modules\n        _, _, sm_prepare_custom_config, _ = \\\n            _get_standalone_module_configs(\n                node, named_modules, prepare_custom_config, qconfig, backend_config)\n        sm_input_quantized_idxs = sm_prepare_custom_config.input_quantized_indexes\n\n        # for args, this is set to the index of the current arg\n        # for kwargs, this is left at None\n        cur_input_idx = None\n        for arg_idx, arg_to_check in enumerate(node.args):\n            if arg_to_check is arg:\n                cur_input_idx = arg_idx\n                break\n\n        if cur_input_idx is None:\n            needs_obs = False\n        else:\n            arg_as_output_target_dtype = _get_arg_target_dtype_as_output(arg, named_modules)\n            arg_as_input_target_dtype = torch.quint8 if cur_input_idx in sm_input_quantized_idxs \\\n                else torch.float\n            needs_obs = (\n                (arg_as_output_target_dtype != arg_as_input_target_dtype) and\n                (arg_as_input_target_dtype != torch.float)\n            )\n\n        act_post_process_ctr = qconfig.activation\n\n    if needs_obs:\n\n        new_obs_mod = act_post_process_ctr()\n        existing_obs_node = None\n\n        # Before using the new observer, check if an observer"
},
{
    "Id": 113,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7f5737392d637a22d555a88a8546d8fc7ab31084",
    "Violation": "insufficient",
    "Bug report": "fix for fsdp exec order pre fwd record. When the sharding_strategy is set to SHARD_GRAD_OP and forward_prefetch=True, during direct validation run, self.is_first_iter will always be True (because training=False, iter+1 is not executed). Additionally, the _pre_forward_order_index of the first handle entering the record_pre_forward function is 0. This causes the handle to have a False result in the if condition at line 166 when entering the record_pre_forward function again (the expected value should be True because _pre_forward_order_index has actually been assigned a value). As a result, the first handle is repetitively added to handles_pre_forward_order, leading to incorrect prefetching order.",
    "Number of deleted lines": 1,
    "Deleted lines": "        Records ``handles`` in the post-forward order, where ``handles`` should\n        be a group of handles used in the same module's forward. If ``handles``\n        is empty, then it is omitted.\n\n        Unlike :meth:`record_pre_forward`, this records the order *every*\n        iteration with the expectation that the recorded order is reset in\n        :meth:`next_iter`.\n        \"\"\"\n        if not handle:\n            return\n        # Only record the first usage of a handles key\n        if handle._post_forward_index:\n            self.handles_post_forward_order.append(handle)\n            return\n        index = len(self.handles_post_forward_order)\n        handle._post_forward_index = index\n        self.handles_post_forward_order.append(handle)\n\n    def record_pre_forward(\n        self, handle: Optional[FlatParamHandle], is_training: bool\n    ) -> None:\n        \"\"\"\n        Records ``handles`` in the pre-forward order, where ``handles`` should\n        be a group of handles used in the same module's forward. If ``handles``\n        is empty, then it is omitted.\n\n        On the first iteration, this checks the execution order across ranks.\n        See :meth:`_check_order` for details.\n        \"\"\"\n        if not handle:\n            return\n        self._check_order(handle, is_training)\n        # Fix the order after the first iteration and only record the first\n        # usage of a handles key\n        if not self.is_first_iter or handle._pre_forward_order_index:\n            return\n        index = len(self.handles_pre_forward_order)\n        handle._pre_forward_order_index = index\n        self.handles_pre_forward_order.append(handle)\n\n    def _check_order(self, handle: FlatParamHandle, is_training: bool) -> None:\n        \"\"\"\n        Checks the forward execution order as long as ``is_training`` is\n        ``True`` since checking in eval mode is not supported. This only checks\n        if the distributed debug level is DETAIL.\n\n        - On the first iteration, this uses all-gathers to check that all ranks\n        are all-gathering the same handles and hence ``FlatParameter`` s,\n        raising an error if not.\n        - On subsequent iterations, this checks that each rank is locally\n        consistent with its own forward order from the first iteration, issuing\n        a warning if not. This issues a warning on the first deviating\n        iteration and stops warning thereafter.\n        \"\"\"\n        # Do not check order in eval mode since the post-backward callback does\n        # not run so it cannot be used to mark the end of an iteration\n        if not is_training or not self._checking_order:\n            return\n        if self.is_first_iter:\n            msg_prefix = \"Forward order differs across ranks:\"\n            optional_local_indices: Tuple[\n                Optional[int], ...\n            ] = self._get_handle_indices(handle)\n            device = handle.device  # guaranteed to be non-CPU\n            num_valid_indices = sum(\n                (index is not None) for index in optional_local_indices\n            )\n            tensor_kwargs: Dict[str, Union[torch.dtype, torch.device]] = {\n                \"dtype\": torch.int32,\n                \"device\": device,\n            }"
},
{
    "Id": 114,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dcac4dd58edefb6951a60266e53d8767dc9be002",
    "Violation": "missing",
    "Bug report": "Add int32_t range check in packed_accessor32 in PyTorch TensorBase.  Summary: As ajtulloch suggested, we can make tensor.packed_accessor32<...>() raise an exception if tensor.numel() > std::numeric_limits<uint32_t>::max(). Trade-off: run-time check overhead (one-time) when doing `packed_accessor32` accessor.",
    "Number of deleted lines": 0,
    "Deleted lines": "\n  template <typename T>\n  T * data_ptr() const;\n\n  // Purposely not defined here to avoid inlining\n  void print() const;\n\n  // Return a `TensorAccessor` for CPU `Tensor`s. You have to specify scalar type and\n  // dimension.\n  template<typename T, size_t N>\n  TensorAccessor<T,N> accessor() const& {\n    static_assert(N > 0, \"accessor is used for indexing tensor, for scalars use *data_ptr<T>()\");\n    TORCH_CHECK(dim() == N, \"TensorAccessor expected \", N, \" dims but tensor has \", dim());\n    return TensorAccessor<T,N>(data_ptr<T>(),sizes().data(),strides().data());\n  }\n  template<typename T, size_t N>\n  TensorAccessor<T,N> accessor() && = delete;\n\n  // Return a `GenericPackedTensorAccessor` for CUDA `Tensor`s. You have to specify scalar type and\n  // dimension. You can optionally specify RestrictPtrTraits as a template parameter to\n  // cast the data pointer to a __restrict__ pointer.\n  // In order to use this, your CUDA kernel has to take a corresponding GenericPackedTensorAccessor\n  // as an argument.\n  template<typename T, size_t N, template <typename U> class PtrTraits = DefaultPtrTraits, typename index_t = int64_t>\n  GenericPackedTensorAccessor<T,N,PtrTraits,index_t> generic_packed_accessor() const& {\n    static_assert(N > 0, \"accessor is used for indexing tensor, for scalars use *data_ptr<T>()\");\n    TORCH_CHECK(dim() == N, \"TensorAccessor expected \", N, \" dims but tensor has \", dim());\n    return GenericPackedTensorAccessor<T,N,PtrTraits,index_t>(static_cast<typename PtrTraits<T>::PtrType>(data_ptr<T>()),sizes().data(),strides().data());\n  }\n  template<typename T, size_t N, template <typename U> class PtrTraits = DefaultPtrTraits, typename index_t = int64_t>\n  GenericPackedTensorAccessor<T,N> generic_packed_accessor() && = delete;\n\n  template<typename T, size_t N, template <typename U> class PtrTraits = DefaultPtrTraits>\n  PackedTensorAccessor32<T,N,PtrTraits> packed_accessor32() const& {\n    return generic_packed_accessor<T,N,PtrTraits,int32_t>();\n  }\n  template<typename T, size_t N, template <typename U> class PtrTraits = DefaultPtrTraits>\n  PackedTensorAccessor32<T,N,PtrTraits> packed_accessor32() && = delete;\n\n  template<typename T, size_t N, template <typename U> class PtrTraits = DefaultPtrTraits>\n  PackedTensorAccessor64<T,N,PtrTraits> packed_accessor64() const& {\n    return generic_packed_accessor<T,N,PtrTraits,int64_t>();\n  }\n  template<typename T, size_t N, template <typename U> class PtrTraits = DefaultPtrTraits>\n  PackedTensorAccessor64<T,N,PtrTraits> packed_accessor64() && = delete;\n\n  // ~~~~~ Autograd API ~~~~~\n\n  /// \\fn bool is_leaf() const;\n  ///\n  /// All Tensors that have `requires_grad()` which is ``false`` will be leaf Tensors by convention.\n  ///\n  /// For Tensors that have `requires_grad()` which is ``true``, they will be leaf Tensors if they were\n  /// created by the user. This means that they are not the result of an operation and so\n  /// `grad_fn()` is `nullptr`.\n  ///\n  /// Only leaf Tensors will have their `grad()` populated during a call to `backward()`.\n  /// To get `grad()` populated for non-leaf Tensors, you can use `retain_grad()`.\n  ///\n  /// Example:\n  /// @code\n  /// auto a = torch::rand(10, torch::requires_grad());\n  /// std::cout << a.is_leaf() << std::endl; // prints `true`\n  ///\n  /// auto b = torch::rand(10, torch::requires_grad()).to(torch::kCUDA);\n  /// std::cout << b.is_leaf() << std::endl; // prints `false`\n  /// // b was created by the operation that cast a cpu Tensor into a cuda Tensor\n  ///\n  /// auto c = torch::rand(10, torch::requires_grad()) + 2;\n  /// std::cout << c.is_leaf() << std::endl; // prints `false`"
},
{
    "Id": 115,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/027e3b7910fade8950038fb5044a548319510600",
    "Violation": "insufficient",
    "Bug report": "check if source is None when using tensor out variants",
    "Number of deleted lines": 1,
    "Deleted lines": "                    data_arg = args[0]\n                elif \"data\" in kwargs:\n                    data_arg = kwargs[\"data\"]\n\n                if isinstance(data_arg, ListVariable) and check_any_unspec(data_arg):\n                    unimplemented(\"torch.tensor call with list of unspec\")\n            tensor_variable = wrap_fx_proxy(\n                tx=tx,\n                proxy=tx.output.create_proxy(\n                    \"call_function\",\n                    fn_,\n                    *proxy_args_kwargs(args, kwargs),\n                ),\n                **options,\n            )\n\n            if \"out\" in kwargs and not (\n                isinstance(kwargs[\"out\"], variables.ConstantVariable)\n                and kwargs[\"out\"].as_python_constant() is None\n            ):\n                # out variants of torch operators like torch.sort and\n                # torch.sigmoid mutate the tensors in the out field. Track such\n                # tensors and rewrite the symbolic locals.\n                if isinstance(tensor_variable, TupleVariable):\n                    assert isinstance(kwargs[\"out\"], (TupleVariable, ListVariable))\n                    output_tensor_names = [\n                        tx.find_symbolic_locals_name(x) for x in kwargs[\"out\"].items\n                    ]\n                    for idx, name in enumerate(output_tensor_names):\n                        if name in tx.symbolic_locals:\n                            tx.symbolic_locals[name] = tensor_variable.items[idx]\n                elif isinstance(tensor_variable, TensorVariable):\n                    assert isinstance(kwargs[\"out\"], TensorVariable)\n                    if (\n                        kwargs[\"out\"] in tx.output.graphargs\n                        and kwargs[\"out\"].size != tensor_variable.size\n                    ):\n                        # It's hard to get out variants with resizing on graph inputs work\n                        # properly across dynamo/aot/inductor, just fall back.\n                        unimplemented(\"out variants with resizing on graph inputs\")\n                    name = tx.find_symbolic_locals_name(kwargs[\"out\"])\n                    if name in tx.symbolic_locals:\n                        tx.symbolic_locals[name] = tensor_variable\n                else:\n                    unimplemented(f\"out variant of {type(kwargs['out'])}\")\n\n            return tensor_variable\n\n    def _call_cross_entropy_loss(self, tx, args, kwargs, options):\n        \"\"\"\n        functional: input, target, weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean',\n        label_smoothing=0.0\n\n        non functional ctor: weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean',\n        label_smoothing=0.0\n\n        non functional loss call: input, target, optional_output\n        \"\"\"\n        from . import ConstantVariable\n\n        def normalize_args(\n            weight=ConstantVariable(None),\n            size_average=ConstantVariable(None),\n            ignore_index=ConstantVariable(-100),\n            reduce=ConstantVariable(None),\n            reduction=ConstantVariable(\"mean\"),\n            label_smoothing=ConstantVariable(0.0),\n        ):\n            return (\n                weight,\n                size_average,"
},
{
    "Id": 116,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/91066559a8c8e5978ed4de722317576b222267c5",
    "Violation": "improper",
    "Bug report": "truthy check for empty string in NameScope(). As in name. LATTE translation team moving some code from Python 2 to 3 uncovered a case where comparison between unicode and str types leads NameScope('') to prepend a separator to the beginning of blob names. This fixes it.",
    "Number of deleted lines": 1,
    "Deleted lines": "\nimport contextlib\nimport threading\nfrom past.builtins import basestring\n\nfrom caffe2.proto import caffe2_pb2\n\n\n# The name scope and device scope when creating a new operator.\n_NAMESCOPE_SEPARATOR = '/'\n\n_threadlocal_scope = threading.local()\n\n\ndef CurrentNameScope():\n    global _threadlocal_scope\n    if not hasattr(_threadlocal_scope, \"namescope\"):\n        _threadlocal_scope.namescope = ''\n    return _threadlocal_scope.namescope\n\n\ndef CurrentDeviceScope():\n    global _threadlocal_scope\n    if not hasattr(_threadlocal_scope, \"devicescope\"):\n        _threadlocal_scope.devicescope = None\n    return _threadlocal_scope.devicescope\n\n\n@contextlib.contextmanager\ndef NameScope(prefix, reset=False):\n    global _threadlocal_scope\n    assert isinstance(prefix, basestring), \\\n        \"NameScope takes in a string as its argument.\"\n    old_scope = CurrentNameScope()\n    prefix = prefix + _NAMESCOPE_SEPARATOR if prefix is not '' else ''\n    if reset:\n        _threadlocal_scope.namescope = prefix\n    else:\n        _threadlocal_scope.namescope = _threadlocal_scope.namescope + prefix\n\n    try:\n        yield\n    finally:\n        assert _threadlocal_scope.namescope.endswith(prefix), \\\n            \"The namescope variable is changed from outside NameScope() calls.\"\n        _threadlocal_scope.namescope = old_scope\n\n\n@contextlib.contextmanager\ndef DeviceScope(scope, node_name=None):\n    new_scope = caffe2_pb2.DeviceOption()\n    if scope:\n        assert isinstance(scope, caffe2_pb2.DeviceOption), \\\n            \"DeviceScope takes in a caffe2_pb2.DeviceOption as its argument.\"\n        new_scope.CopyFrom(scope)\n    else:\n        assert node_name, \"At least one argument should be non-null in DeviceScope\"\n\n    # rewrite node_name if it is explicitly given\n    if node_name:\n        new_scope.node_name = node_name\n    global _threadlocal_scope\n    old_scope = CurrentDeviceScope()\n    # nested scope should inherit the node_name if it is not explicitly set\n    if old_scope and old_scope.HasField('node_name') and \\\n            not new_scope.HasField('node_name'):\n        new_scope.node_name = old_scope.node_name\n    _threadlocal_scope.devicescope = new_scope\n    try:\n        yield\n    finally:"
},
{
    "Id": 117,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e3542d2c12d8aaaccf8a53873e480c20dc6b7338",
    "Violation": "missing",
    "Bug report": "avoid unnecessary call to empty_tensor_restride in empty(). Our empty benchmark makes this call unconditionally. If MemoryFormat::Contiguous is indeed a common case (or if workloads are likely to use a consistent-ish memory format), then I'd expect checking first to be a win.",
    "Number of deleted lines": 2,
    "Deleted lines": "    c10::optional<Layout> layout_opt,\n    c10::optional<Device> device_opt,\n    c10::optional<bool> pin_memory_opt,\n    c10::optional<c10::MemoryFormat> memory_format_opt) {\n  Device device = device_or_default(device_opt);\n\n  TORCH_CHECK(device.type() == DeviceType::CPU);\n  check_size_nonnegative(size);\n\n  bool pin_memory = pinned_memory_or_default(pin_memory_opt);\n  c10::Allocator* allocator;\n  if (pin_memory) {\n    allocator = detail::getCUDAHooks().getPinnedMemoryAllocator();\n  } else {\n    allocator = at::getCPUAllocator();\n  }\n\n  int64_t nelements = prod_intlist(size);\n  caffe2::TypeMeta dtype = scalarTypeToTypeMeta(dtype_or_default(dtype_opt));\n  int64_t size_bytes = nelements * dtype.itemsize();\n  auto storage_impl = c10::make_intrusive<StorageImpl>(\n      c10::StorageImpl::use_byte_size_t(),\n      size_bytes,\n      allocator->allocate(size_bytes),\n      allocator,\n      /*resizeable=*/true);\n\n  auto tensor = detail::make_tensor<TensorImpl>(\n      std::move(storage_impl), at::DispatchKey::CPU, dtype);\n  // Default TensorImpl has size [0]\n  if (size.size() != 1 || size[0] != 0) {\n    tensor.unsafeGetTensorImpl()->set_sizes_contiguous(size);\n  }\n\n  auto memory_format = memory_format_opt.value_or(MemoryFormat::Contiguous);\n  tensor.unsafeGetTensorImpl()->empty_tensor_restride(memory_format);\n\n  return tensor;\n}\n\ntemplate <typename T>\nTensor tensor_cpu(ArrayRef<T> values, const TensorOptions& options) {\n  auto result = at::empty(values.size(), options);\n  AT_ASSERT(result.is_contiguous());\n  AT_DISPATCH_ALL_TYPES_AND_COMPLEX(result.scalar_type(), \"tensor_cpu\", [&] {\n    std::copy(\n        values.begin(), values.end(), result.template data_ptr<scalar_t>());\n  });\n  return result;\n}\n\ntemplate <typename T>\nTensor tensor_backend(ArrayRef<T> values, const TensorOptions& options) {\n  auto cpu_tensor = tensor_cpu(values, options.device(DeviceType::CPU));\n  return cpu_tensor.to(options.device());\n}\n\ntemplate <typename T>\nTensor tensor_complex_cpu(ArrayRef<T> values, const TensorOptions& options) {\n  auto result = at::empty(values.size(), options);\n  AT_ASSERT(result.is_contiguous());\n  AT_DISPATCH_COMPLEX_TYPES(result.scalar_type(), \"tensor_cpu\", [&] {\n    std::copy(\n        values.begin(), values.end(), result.template data_ptr<scalar_t>());\n  });\n  return result;\n}\n\ntemplate <typename T>\nTensor tensor_complex_backend(\n    ArrayRef<T> values,\n    const TensorOptions& options) {"
},
{
    "Id": 118,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/45d5b3248b430aca70111316accd165954464589",
    "Violation": "missing",
    "Bug report": "Fixed C++ BatchNorm pretty_print() with optional momentum. Summary : Inserted a check for the momentum and print  \"None\" in case is not defined.",
    "Number of deleted lines": 1,
    "Deleted lines": "      \"expected 5D input (got \", input.dim(), \"D input)\");\n}\n\ntemplate class BatchNormImplBase<1, BatchNorm1dImpl>;\ntemplate class BatchNormImplBase<2, BatchNorm2dImpl>;\ntemplate class BatchNormImplBase<3, BatchNorm3dImpl>;\n\n} // namespace nn\n} // namespace torch\n"
},
{
    "Id": 119,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/666ff0ae220e1a5c406b0bc5cd43283e1b18b38e",
    "Violation": "missing",
    "Bug report": "Update _create_c10d_store to check port value. Port number is int in python, but needs to be uint16_t when called for TCPStore constructor.",
    "Number of deleted lines": 0,
    "Deleted lines": "    if \"rank\" not in query:\n        raise _error(\"rank parameter missing\")\n    if \"world_size\" not in query:\n        raise _error(\"world size parameter missing\")\n\n    rank = int(query[\"rank\"])\n    world_size = int(query[\"world_size\"])\n    store = FileStore(path, world_size)\n    yield (store, rank, world_size)\n\n    # If this configuration is invalidated, there is nothing we can do about it\n    raise RuntimeError(\"Unable to perform rerendezvous using file:// method\")\n\n\ndef _torchelastic_use_agent_store() -> bool:\n    return os.environ.get(\"TORCHELASTIC_USE_AGENT_STORE\", None) == str(True)\n\n\ndef _create_c10d_store(hostname, port, rank, world_size, timeout) -> Store:\n    \"\"\"\n    Smartly creates a c10d Store object on ``rank`` based on whether\n    we need to re-use agent store. The TCPStore server is assumed to be hosted\n    on ``hostname:port``.\n\n    If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that\n    the agent leader (node rank 0) hosts the TCPStore server (for which the\n    endpoint is specified by the given ``hostname:port``). Hence\n    ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).\n\n    If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host\n    the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname\n    and port are correctly passed via ``hostname`` and ``port``. All\n    non-zero ranks will create and return a TCPStore client.\n    \"\"\"\n\n    if _torchelastic_use_agent_store():\n        attempt = os.environ[\"TORCHELASTIC_RESTART_COUNT\"]\n        tcp_store = TCPStore(hostname, port, world_size, False, timeout)\n        return PrefixStore(f\"/worker/attempt_{attempt}\", tcp_store)\n    else:\n        start_daemon = rank == 0\n        return TCPStore(\n            hostname, port, world_size, start_daemon, timeout, multi_tenant=True\n        )\n\n\ndef _tcp_rendezvous_handler(\n    url: str, timeout: timedelta = default_pg_timeout, **kwargs\n):\n    def _error(msg):\n        return _rendezvous_error(\"tcp:// rendezvous: \" + msg)\n\n    result = urlparse(url)\n    if not result.port:\n        raise _error(\"port number missing\")\n    query: Dict[str, Union[int, str]]\n    # mypy doesn't allow dict() to accept List of values (#257)\n    query = dict(pair.split(\"=\") for pair in filter(None, result.query.split(\"&\")))  # type: ignore[misc, arg-type]\n    if \"rank\" not in query:\n        raise _error(\"rank parameter missing\")\n    if \"world_size\" not in query:\n        raise _error(\"world size parameter missing\")\n\n    rank = int(query[\"rank\"])\n    world_size = int(query[\"world_size\"])\n    assert result.hostname is not None\n\n    store = _create_c10d_store(result.hostname, result.port, rank, world_size, timeout)\n\n    yield (store, rank, world_size)"
},
{
    "Id": 120,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ba59d720cd5c5c81601b53d2c3397c46c1f87883",
    "Violation": "missing",
    "Bug report": "Change error message for torch.linspace(). Basically moves the error checking from the device-specific function to the native function.",
    "Number of deleted lines": 0,
    "Deleted lines": "\nTensor& full_out(Tensor& result, IntArrayRef size, Scalar fill_value) {\n  if (result.is_sparse()) {\n    AT_ERROR(\"full(...) is not implemented for sparse layout\");\n  }\n  result.resize_(size);\n  return result.fill_(fill_value);\n}\n\nTensor full_like(const Tensor& self, Scalar fill_value) {\n  return native::full_like(self, fill_value, self.options());\n}\n\nTensor full_like(const Tensor& self, Scalar fill_value, const TensorOptions& options) {\n  return native::full(self.sizes(), fill_value, options);\n}\n\nTensor new_full(\n    const Tensor& self,\n    IntArrayRef size,\n    Scalar fill_value,\n    const TensorOptions& options\n    ) {\n  return at::full(size, fill_value, self.options().merge_in(options));\n}\n\n\n// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ linspace ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTensor linspace(\n    Scalar start,\n    Scalar end,\n    int64_t steps,\n    const TensorOptions& options) {\n  Tensor result = at::empty({steps}, options);\n  return at::linspace_out(result, start, end, steps);\n}\n\n// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ logspace ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTensor logspace(\n    Scalar start,\n    Scalar end,\n    int64_t steps,\n    double base,\n    const TensorOptions& options) {\n  Tensor result = at::empty({steps}, options);\n  return at::logspace_out(result, start, end, steps, base);\n}\n\n// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ones ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTensor ones(IntArrayRef size, const TensorOptions& options) {\n  return native::full(size, /*fill_value=*/1, options);\n}\n\nTensor& ones_out(Tensor& result, IntArrayRef size) {\n  return native::full_out(result, size, /*fill_value=*/1);\n}\n\nTensor ones_like(const Tensor& self) {\n  return native::ones(self.sizes(), self.options());\n}\n\nTensor ones_like(const Tensor& self, const TensorOptions& options) {\n  return native::ones(self.sizes(), options);\n}\n\n// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ scalar_tensor ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
},
{
    "Id": 121,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/babb28d2a3a755424f72518bc360d9f511a24463",
    "Violation": "improper",
    "Bug report": "Change DHCECK to CAFFE_ENFORCE in softmax_with_loss_op.cc. Summary: Based on discussion on the post in Caffe2 users. Changing DCHECK that works only in debug mode to CAFFE_ENFORCE that throws exception and is a better option. Update: Also correct the check for label_data >= 0, did not check for all elements previously. Moved it to inner loop.",
    "Number of deleted lines": 5,
    "Deleted lines": "      context_,\n      N,\n      D,\n      X.data<float>(),\n      Pdata,\n      losses_.mutable_data<float>(),\n      sum_multiplier_.data<float>(),\n      !label_prob_mode_,\n      rowmax_.mutable_data<float>());\n\n  // Then compute cross entropy\n  float loss_sum = 0.0;\n  float weight_sum = 0.0;\n  if (!label_prob_mode_) {\n    const int* label_data = T.data<int>();\n    const float* Xdata = X.data<float>();\n\n    for (int i = 0; i < N; ++i) {\n      CAFFE_ENFORCE(\n          label_data[i] < D && label_data[i] >= 0,\n          \"Label seems incorrect: label value larger than number of classes: \",\n          label_data[i],\n          \" vs \",\n          D);\n      float weight = weights ? weights[i] : 1.0;\n      float l = -Pdata[i * D + label_data[i]] * weight;\n      loss_sum += l;\n      weight_sum += weight;\n    }\n    math::Exp(N * D, Pdata, Pdata, &context_);\n  } else {\n    const float* label_data = T.data<float>();\n\n    for (int i = 0; i < N; ++i) {\n      CAFFE_ENFORCE(\n          label_data[i] >= 0,\n          \"Label prob seems incorrect: label prob value must be nonnegative: \",\n          label_data[i]);\n      float l = 0.0;\n      float total_prob = 0.0;\n      float weight = weights ? weights[i] : 1.0;\n      for (int j = 0; j < D; ++j) {\n        l += -log(std::max(Pdata[i * D + j], 1e-20f)) * label_data[i * D + j] *\n            weight;\n        total_prob += label_data[i * D + j];\n      }\n      loss_sum += l;\n      DCHECK(std::abs(total_prob - 1.) < 1e-5f);\n      weight_sum += weight;\n    }\n  }\n\n  avg_loss->Resize(vector<TIndex>());\n  float* avg_loss_data = avg_loss->mutable_data<float>();\n  if (weight_sum != 0.0) {\n    avg_loss_data[0] = loss_sum * scale_ / weight_sum;\n  } else {\n    avg_loss_data[0] = 0.0;\n  }\n  return true;\n}\n\ntemplate <>\nbool SoftmaxWithLossGradientOp<float, CPUContext>::RunOnDevice() {\n  auto& X = Input(0); // Logits\n  auto& T = Input(1); // Labels / targets\n  // Input(2) is weights if given\n  auto& P = Input(InputSize() - 2); // Probabilities from softmax\n  auto& d_avg_loss = Input(InputSize() - 1); // Gradient w.r.t. avg loss\n  auto* dX = Output(0);\n  const float* weights = (InputSize() > 4 ? Input(2).data<float>() : nullptr);\n\n  const auto canonical_axis = X.canonical_axis_index(axis_);\n  int N, D;\n  N = X.size_to_dim(canonical_axis); // batch size\n  D = X.size_from_dim(canonical_axis);\n  dX->ResizeLike(X);\n\n  if (label_prob_mode_) {\n    DCHECK_GE(T.ndim(), 2);\n    DCHECK_EQ(T.size_to_dim(canonical_axis), N);\n    DCHECK_EQ(T.size_from_dim(canonical_axis), D);\n  } else {\n    if (T.ndim() == canonical_axis) {"
},
{
    "Id": 122,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bc371a2cd03ce573f3ad4f7be141364136028905",
    "Violation": "missing",
    "Bug report": "Add additional checks when tracing back during maybe share output observer function. Summary: Currently in `maybe_make_input_output_share_observers`  we trace back from a node to find the activation_post_process of the input node, we have internal use case which would error out during tracing back, this PR is adding a guard during this process to return False early when the node doesn't have any input",
    "Number of deleted lines": 0,
    "Deleted lines": "    # we need to navigate up to the first observer\n    iteration_guard = 0\n    while not is_activation_post_process_node(first_arg_arg, modules):\n        if not isinstance(first_arg_arg, Node):\n            return False\n        # did not find an activation_post_process for the op\n        if first_arg_arg.op == \"placeholder\":\n            return False\n        # trace back the args until we found the first Tensor/Node\n        trace_back_node = None\n        for i in range(len(first_arg_arg.args)):\n            trace_back_node = first_arg_arg.args[i]\n            if isinstance(trace_back_node, Node):\n                break\n        if trace_back_node is None:\n            return False\n        first_arg_arg = trace_back_node\n\n        iteration_guard += 1\n        if iteration_guard > 10000:\n            raise AssertionError('Unable to find observer of previous node')\n\n    assert isinstance(first_arg_arg, Node)\n    target_to_use = first_arg_arg.target\n    assert isinstance(target_to_use, str)\n    obs_mod_to_use = modules[target_to_use]\n\n    if isinstance(first_arg, (list, tuple)):\n        # set all other input observer nodes to use that module\n        for input_idx, input_arg in enumerate(first_arg):\n            if input_idx == 0:\n                continue\n            iteration_guard = 0\n            while not is_activation_post_process_node(input_arg, modules):\n                input_arg = input_arg.args[0]\n                iteration_guard += 1\n                if iteration_guard > 10000:\n                    raise AssertionError('Unable to find observer of previous node')\n\n            parent_name, name = _parent_name(input_arg.target)\n            setattr(modules[parent_name], name, obs_mod_to_use)\n\n    # set the output observer node to use that module\n    for output_obs_node, _ in node.users.items():\n        assert is_activation_post_process_node(output_obs_node, modules)\n        parent_name, name = _parent_name(output_obs_node.target)\n        setattr(modules[parent_name], name, obs_mod_to_use)\n\n    # TODO(future PR): delete the orphaned observer modules\n    return True\n\ndef remove_output_observer(\n        node: Node,\n        model: torch.nn.Module,\n        modules: Dict[str, torch.nn.Module]):\n    items = list(node.users.items())\n    for output_obs_node, _ in items:\n        assert is_activation_post_process_node(output_obs_node, modules)\n        output_obs_node.replace_all_uses_with(node)\n        model.graph.erase_node(output_obs_node)  # type: ignore[union-attr, operator]\n\ndef swap_custom_module_to_observed(\n        node: Node,\n        qconfig: QConfigAny,\n        modules: Dict[str, torch.nn.Module],\n        prepare_custom_config_dict: Dict[str, Any]):\n    custom_module = modules[node.target]  # type: ignore[index]\n    custom_module_class_mapping = prepare_custom_config_dict.get(\n        \"float_to_observed_custom_module_class\", {})\n    observed_custom_module_class = \\"
},
{
    "Id": 123,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3aeaa21eb02953a9cbc62b3e61215572fc28453e",
    "Violation": "missing",
    "Bug report": "Revert \"Remove parent device mesh check",
    "Number of deleted lines": 0,
    "Deleted lines": "        )\n        # we shard across intra-node\n        state.process_group = intra_node_group\n        # save _inter_node_pg to allreduce across.\n        state._inter_node_pg = inter_node_group\n    else:\n        # Check type and assign state.process_group and state._inter_node_pg.\n        if _is_valid_hybrid_shard_pg_type(process_group):\n            # Assuming that user passed in as intra node group and inter node group\n            # as documented.\n            state.process_group, state._inter_node_pg = process_group\n        else:\n            raise ValueError(\n                \"Expected process_group to be passed in as either None or \"\n                f\"Tuple[dist.ProcessGroup, dist.ProcessGroup] but got {type(process_group)}\"\n            )\n    # Create state for allreduce\n    state._inter_node_state = _get_default_comm_hook_state(\n        process_group=state._inter_node_pg,\n    )\n    return state\n\n\n@no_type_check\ndef _is_valid_hybrid_shard_pg_type(process_group: Any) -> bool:\n    return (\n        isinstance(process_group, tuple)\n        and len(process_group) == 2\n        and all(isinstance(pg, dist.ProcessGroup) for pg in process_group)\n    )\n\n\n@no_type_check\ndef _is_valid_hybrid_shard_device_mesh(device_mesh: DeviceMesh) -> bool:\n    return isinstance(device_mesh, DeviceMesh) and device_mesh.ndim == 2\n\n\n@no_type_check\ndef _init_intra_node_process_group(num_devices_per_node: int) -> dist.ProcessGroup:\n    \"\"\"\n    Return a process group across the current node.\n\n    For example, given each row is a distinct node:\n    0 1 2 3 4 5 6 7 8\n    9 10 11 12 13 14 15\n    This API would return an intra-node subgroup across\n    [0, 7] or [8, 15] depending on the process's rank.\n    For example, rank 3 would get [0, 7].\n    \"\"\"\n    intra_node_subgroup, _ = dist.new_subgroups(num_devices_per_node)\n    return intra_node_subgroup\n\n\n@no_type_check\ndef _init_inter_node_process_group(\n    global_process_group: dist.ProcessGroup,\n    num_devices_per_node: int,\n) -> dist.ProcessGroup:\n    \"\"\"\n    Return an inter-node process group where each contained rank has the same local rank.\n\n    For example, given each row is a distinct node:\n    0 1 2 3 4 5 6 7 8\n    9 10 11 12 13 14 15\n    This API would return inter-node process group {0, 8}, {1, 9}, {2, 10}, and so forth\n    depending on the process's rank. For example, rank 1 would get {1, 9}, rank 5\n    would get {5, 13}.\n    \"\"\"\n    # the inter-node pg that is returned\n    inter_node_pg = None"
},
{
    "Id": 124,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5fc122bf3973504e619cd677ad4a7fc1011642cd",
    "Violation": "missing",
    "Bug report": "tensor.numpy() checks that no positional arguments are passed. * tensor.numpy() checks that no arguments are passed.",
    "Number of deleted lines": 0,
    "Deleted lines": "  if (fd == -1) {\n    THPUtils_setError(\"write_file could not retrieve file descriptor from given object\");\n    return NULL;\n  }\n  THStorage *storage = ((THPStorage*)PyTuple_GET_ITEM(args, 1))->cdata;\n  return THPTensor_(New)(THPTensor_(newWithMetadataFileRaw)(fd, storage));\n}\n#endif\n\n\n[[\n  name: THPTensor_(toNumpy)\n  python_name: numpy\n  only_register: True\n]]\n// Adapted from fblualib\nPyObject * THPTensor_(toNumpy)(THPTensor *self, PyObject *args) {\n#if !defined(WITH_NUMPY)\n  THPUtils_setError(\"PyTorch was compiled without numpy support\\n\");\n  return NULL;\n#elif defined(THC_GENERIC_FILE)\n  THPUtils_setError(\"can't convert CUDA tensor to numpy (it doesn't support GPU arrays). \"\n    \"Use .cpu() to move the tensor to host memory first.\");\n  return NULL;\n#elif !defined(NUMPY_TYPE_ENUM)\n  THPUtils_setError(\"numpy conversion for %s is not supported\\n\", THPUtils_typename(self));\n  return NULL;\n#else\n  npy_intp zero = 0;\n  int ndim;\n  npy_intp* sizes_ptr;\n  std::unique_ptr<npy_intp[]> sizes;\n  std::unique_ptr<npy_intp[]> strides;\n\n  // Numpy and Torch disagree on empty tensors. In Torch, an empty tensor\n  // is a tensor with zero dimensions. In Numpy, a tensor with zero dimensions\n  // is a scalar (with one element). So we'll convert an empty Torch tensor\n  // to a 1d Numpy tensor of shape [0]. Also see pushTensor in PythonToLua.cpp.\n  ndim = THTensor_(nDimension)(LIBRARY_STATE self->cdata);\n  if (ndim != 0) {\n\n    sizes.reset(new npy_intp[ndim]);\n    std::copy(self->cdata->size, self->cdata->size + ndim, sizes.get());\n    sizes_ptr = sizes.get();\n\n    if (!THTensor_(isContiguous)(LIBRARY_STATE self->cdata)) {\n      strides.reset(new npy_intp[ndim]);\n      // Numpy strides use bytes; Torch strides use element counts.\n      for (int i = 0; i < ndim; ++i) {\n        strides[i] = self->cdata->stride[i] * sizeof(real);\n      }\n    }\n  } else {\n    ndim = 1;\n    sizes_ptr = &zero;\n  }\n\n  THPObjectPtr array(PyArray_New(\n      &PyArray_Type, ndim, sizes_ptr, NUMPY_TYPE_ENUM,\n      strides.get(), self->cdata->storage->data + self->cdata->storageOffset,\n      0, NPY_ARRAY_ALIGNED | NPY_ARRAY_WRITEABLE | NPY_ARRAY_C_CONTIGUOUS, nullptr));\n  if (!array) {\n    THPUtils_setError(\"an error occurred during conversion to numpy array\");\n    return NULL;\n  }\n\n  // Create a PythonStorage object to hold the reference count.\n  // PyArray_SetBaseObject steals the reference to the base object.\n  // See Note [Numpy memory management]\n  Py_INCREF(self);"
},
{
    "Id": 125,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/647154f82ac2c57769f080c41452b3e5960ab94f",
    "Violation": "missing",
    "Bug report": "Assert tensor isn't sparse in enforce_invariants. There's no reason we can't check this, but I'm punting on implementing it for now.  But it currently segfaults, so this is an improvement",
    "Number of deleted lines": 0,
    "Deleted lines": "  if (defined()) {\n    std::cerr << \"[\" << type().toString() << \" \" << sizes() << \"]\" << std::endl;\n  } else {\n    std::cerr << \"[UndefinedTensor]\" << std::endl;\n  }\n}\n\nconst char * Tensor::toString() const {\n  return type().toString();\n}\n\n} // namespace at\n"
},
{
    "Id": 126,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2c9dd886afc656a8bfe5d8bbcb601ee5877cee21",
    "Violation": "missing",
    "Bug report": "Modify torch.movedim to handle scalar as no-op. Summary: `torch.movedim` directly handle the case of a scalar tensor (0-dim) in input as a no-op by returning a view of the input tensor (after all the usual checks for the other parameters)",
    "Number of deleted lines": 0,
    "Deleted lines": "\nTensor diagonal_backward(const Tensor & grad, IntArrayRef input_sizes, int64_t offset, int64_t dim1, int64_t dim2) {\n  auto grad_input = at::zeros(input_sizes, grad.options());\n  auto diag = grad_input.diagonal(offset, dim1, dim2);\n  diag.copy_(grad);\n  return grad_input;\n}\n\nTensor movedim(const Tensor& self, IntArrayRef src, IntArrayRef dst) {\n  TORCH_CHECK(src.size() == dst.size(), \"movedim: Invalid source or destination dims: source (\",\n              src, \" dims ) should contain the same number of dims as destination (\", dst, \" dims)\");\n\n  size_t self_dim = self.dim();\n  DimVector normalized_src(src.size());\n  DimVector normalized_dst(dst.size());\n\n  auto wrap_dims = [&self_dim](const IntArrayRef& vec, DimVector& normalized_vec) {\n    for (const auto i : c10::irange(vec.size())) {\n      normalized_vec[i] = maybe_wrap_dim(vec[i], self_dim);\n    }\n  };\n\n  wrap_dims(src, normalized_src);\n  wrap_dims(dst, normalized_dst);\n\n  auto all_unique = [](const DimVector& dims) {\n    DimVector copy = dims;\n    std::sort(copy.begin(), copy.end());\n    auto duplicate = std::adjacent_find(copy.begin(), copy.end());\n    return duplicate == copy.end();\n  };\n  TORCH_CHECK(all_unique(normalized_src), \"movedim: repeated dim in `source` (\", src, \")\");\n  TORCH_CHECK(all_unique(normalized_dst), \"movedim: repeated dim in `destination` (\", dst, \")\");\n\n  // TODO: The algorithm below can probably be optimized.\n  // Reference: https://github.com/pytorch/pytorch/pull/41480#discussion_r456100505\n\n  // Algorithm Walkthrough\n  // Example Input\n  // Variable State:\n  //     normalized_src = 0, 1\n  //     normalized_dst = 2, 4\n  //     self_dim = 5\n  DimVector order(self_dim);\n  DimVector source_dims(self_dim);\n  DimVector destination_dims(self_dim);\n\n  // We initialize two vectors to track update to the dims\n  // `order` contains the final order of the dim positions.\n  // Variable State:\n  //     order = NA, NA, NA, NA, NA\n  //     source_dims = 0, 1, 2, 3, 4\n  //     destination_dims = 0, 1, 2, 3, 4\n  std::iota(source_dims.begin(), source_dims.end(), 0);\n  std::iota(destination_dims.begin(), destination_dims.end(), 0);\n\n  // We mark and update position for the dim provided by user\n  // i.e. `normalized_src` and `normalized_dims`\n  // Variable State:\n  //     order = NA, NA, 0, NA, 1\n  //     source_dims = -1, -1, 2, 3, 4\n  //     destination_dims = 0, 1, -1, 3, -1\n  for (const auto i : c10::irange(src.size())) {\n      order[normalized_dst[i]] = normalized_src[i];\n      source_dims[normalized_src[i]] = -1;\n      destination_dims[normalized_dst[i]] = -1;\n  }\n\n  // Remove the dims whose position we already know,\n  // the ones marked with -1 in previous step"
},
{
    "Id": 127,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/52e76a305677dfaf26cd5d59bd1aa239375f833c",
    "Violation": "missing",
    "Bug report": "fix ShardedTensor.gather when shard is empty. current ShardedTensor.gather is not working as expectation when the shard is empty on any rank The root cause is identified that when a sharded tensor has no placement on a specific rank, the metadata doesn't include that rank's placement which introduces KeyError in :```shard_offset = shard_placement[shard. Metadata][1]``` It's fixed by adding an empty tensor check.",
    "Number of deleted lines": 0,
    "Deleted lines": "\n        local_shards = self.local_shards()\n        world_size = dist.get_world_size(self._process_group)\n        rank_sizes = [0 for _ in range(world_size)]\n        max_rank_size = 0\n        shard_placement: Dict[ShardMetadata, Tuple[int, int]] = {}\n        # collect sizes\n        for shard_md in self.metadata().shards_metadata:\n            shard_rank = cast(_remote_device, shard_md.placement).rank()\n            assert shard_rank is not None\n\n            shard_placement[shard_md] = (shard_rank, rank_sizes[shard_rank])\n            rank_sizes[shard_rank] += shard_size(shard_md)\n            max_rank_size = max(max_rank_size, rank_sizes[shard_rank])\n\n        gather_list: Optional[List[torch.Tensor]]\n        if rank == dst:\n            assert out is not None\n            if enforce_dtype:\n                # enforce_dtype is deprecated.  Do it for backward compatibility.\n                dtype = out.dtype\n            # TODO make it as a view of out tensor\n            gather_list = [torch.empty((max_rank_size,), device=out.device, dtype=dtype) for _ in range(world_size)]\n        else:\n            gather_list = None\n\n        with torch.no_grad():\n            if enforce_dtype and len(local_shards) > 0:\n                # enforce_dtype is deprecated.  Do it for backward compatibility.\n                dtype = local_shards[0].tensor.dtype\n            data = torch.empty(max_rank_size, device=self._get_preferred_device(), dtype=dtype)\n\n            for shard in local_shards:\n                src = shard.tensor.flatten()\n                shard_offset = shard_placement[shard.metadata][1]\n                data[shard_offset: shard_offset + src.numel()].copy_(src)\n\n        dist.gather(\n            tensor=data,\n            gather_list=gather_list,\n            dst=dst,\n            group=self._process_group,\n        )\n        if rank != dst:\n            return\n        # In _validate_output_tensor_for_gather, we raise if out == None and rank == dst\n        out = cast(torch.Tensor, out)\n        assert gather_list is not None\n\n        full_size = self.metadata().size\n        dims = len(full_size)\n        for shard_md in self.metadata().shards_metadata:\n            rank, rank_offset = shard_placement[shard_md]\n            tensor = gather_list[rank]\n            tensor = tensor[rank_offset : rank_offset + shard_size(shard_md)]\n            tensor = tensor.view(shard_md.shard_sizes)\n\n            out_narrow_view = out\n            for dim in range(dims):\n                out_narrow_view = out_narrow_view.narrow(\n                    dim,\n                    shard_md.shard_offsets[dim],\n                    shard_md.shard_sizes[dim],\n                )\n\n            out_narrow_view.copy_(tensor)\n\n    def cpu(\n        self,\n        memory_format=torch.preserve_format,"
},
{
    "Id": 128,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4ee179c9528c8c6aae17a01f2b0d7e8235219219",
    "Violation": "insufficient",
    "Bug report": "Fix ConstantVariable init method if NumPy is missing. By adding `np is not None` check before `isinstance(value, np.number)`",
    "Number of deleted lines": 1,
    "Deleted lines": "from typing import Dict, List\n\nimport torch\n\nfrom .. import variables\nfrom ..exc import unimplemented\nfrom ..utils import istype, np\nfrom .base import typestr, VariableTracker\n\n\n_type_to_assert_reason = {\n    # NB - We CAN have ConstantVariable(set) because of how sets interact with guards.\n    # A locally created set should always become a SetVariable, as the items in the set will already either be sourced\n    # from somewhere else, or unsourced. An input set would imply sources derived from set contents. For example, an\n    # input list's contents will have a source like some_list[0], some_list[1][1], etc. For a set, arbitrary access is\n    # not possible. This is a solvable problem, but one we have not taken on yet. As such, input sets are not allowed to\n    # become SetVariables. The solution here is to create a ConstantSetVariable that is more like a ConstantVariable.\n    # As this does not exist, we cannot add sets to this invariant.\n    list: \"List types must use ListVariable.\",\n    dict: \"Dict types must use ConstDictVariable.\",\n    torch.Tensor: \"Tensor types must use TensorVariable.\",\n    torch.SymInt: \"SymInts must use SymNodeVariable. \"\n    \"If the underlying value is static, we will create a ConstantVariable and specialize.\",\n    torch.SymFloat: \"SymInts must use SymNodeVariable\",\n}\n\n\nclass ConstantVariable(VariableTracker):\n    def __init__(self, value, **kwargs):\n        super().__init__(**kwargs)\n        if not ConstantVariable.is_literal(value):\n            for disallowed_type, reason in _type_to_assert_reason.items():\n                assert not isinstance(value, disallowed_type), reason\n\n        if isinstance(value, np.number):\n            self.value = value.item()\n        else:\n            self.value = value\n\n    def as_proxy(self):\n        return self.value\n\n    def __str__(self):\n        # return f\"ConstantVariable({self.value})\"\n        return f\"ConstantVariable({type(self.value).__name__})\"\n\n    def python_type(self):\n        return type(self.value)\n\n    def as_python_constant(self):\n        return self.value\n\n    @property\n    def items(self):\n        \"\"\"\n        Need this when adding a BaseListVariable and a ConstantVariable together.\n        Happens in detectron2.\n        \"\"\"\n        return self.unpack_var_sequence(tx=None)\n\n    def getitem_const(self, arg: VariableTracker):\n        return ConstantVariable(\n            self.value[arg.as_python_constant()],\n            **VariableTracker.propagate([self, arg]),\n        )\n\n    @staticmethod\n    def is_literal(obj):\n        if type(obj) in (int, float, bool, type(None), str):\n            return True\n        if type(obj) in (list, tuple, set, frozenset):"
},
{
    "Id": 129,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ba766ef39a4fff2d8856e17747393d469e409775",
    "Violation": "missing",
    "Bug report": "Fix BN size check in eval mode",
    "Number of deleted lines": 3,
    "Deleted lines": "          0.6597  0.0350  0.0837\n          0.5521  0.9447  0.0498\n\n        (1 ,.,.) =\n          0.6597  0.0350  0.0837\n         -0.1527  0.0877  0.4260\n          0.8393 -0.6062 -0.3348\n         -0.8738 -0.9054  0.4281\n        [torch.FloatTensor of size 2x4x3]\n\n        >>> # example with padding_idx\n        >>> embedding_matrix = Variable(torch.rand(10, 3))\n        >>> embedding_matrix[0].zero_()\n        >>> input = Variable(torch.LongTensor([[0,2,0,5]]))\n        >>> torch.nn.functional.embedding(input, embedding_matrix)\n\n        Variable containing:\n        (0 ,.,.) =\n          0.0000  0.0000  0.0000\n          0.3452  0.4937 -0.9361\n          0.0000  0.0000  0.0000\n          0.0706 -2.1962 -0.6276\n        [torch.FloatTensor of size 1x4x3]\n\n    \"\"\"\n    return torch.nn.backends.thnn.backend.Embedding.apply(\n        input, embedding_matrix,\n        -1, max_norm, norm_type,\n        scale_grad_by_freq, sparse\n    )\n\n\ndef batch_norm(input, running_mean, running_var, weight=None, bias=None,\n               training=False, momentum=0.1, eps=1e-5):\n    size = list(input.size())\n    if reduce(mul, size[2:], size[0]) == 1:\n        raise ValueError('Expected more than 1 value per channel, got input size {}'.format(size))\n    f = torch._C._functions.BatchNorm(running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled)\n    return f(input, weight, bias)\n\n\n# loss\n\ndef nll_loss(input, target, weight=None, size_average=True, ignore_index=-100):\n    r\"\"\"The negative log likelihood loss.\n\n    See :class:`~torch.nn.NLLLoss` for details.\n\n    Args:\n        input: :math:`(N, C)` where `C = number of classes` or `(N, C, H, W)`\n            in case of 2D - Loss\n        target: :math:`(N)` where each value is `0 <= targets[i] <= C-1`\n        weight (Variable, optional): a manual rescaling weight given to each\n            class. If given, has to be a Variable of size \"nclasses\"\n        size_average (bool, optional): By default, the losses are averaged\n            over observations for each minibatch. If size_average\n            is False, the losses are summed for each minibatch. Default: True\n        ignore_index (int, optional): Specifies a target value that is ignored\n            and does not contribute to the input gradient. When size_average is\n            True, the loss is averaged over non-ignored targets. Default: -100\n\n    Example::\n\n        >>> # input is of size nBatch x nClasses = 3 x 5\n        >>> input = autograd.Variable(torch.randn(3, 5))\n        >>> # each element in target has to have 0 <= value < nclasses\n        >>> target = autograd.Variable(torch.LongTensor([1, 0, 4]))\n        >>> output = F.nll_loss(F.log_softmax(input), target)\n        >>> output.backward()\n    \"\"\"\n    dim = input.dim()\n    if dim == 2:\n        return _functions.thnn.NLLLoss.apply(input, target, weight, size_average, ignore_index)"
},
{
    "Id": 130,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b287cb816c1ac52165920a121c98643c08d31ff7",
    "Violation": "insufficient",
    "Bug report": " inductor: make the vec_transpose's tiling stride doesn't depend on out_idx and tiling_idex. ",
    "Number of deleted lines": 2,
    "Deleted lines": "    and store are generated into kernel.preloads and kernel.poststores buffers.\n\n    The loop structure looks like below:\n    for ...\n      for i_outer ...\n        for ...\n          for inner_most ...\n            // generated by CppTile2DKernel\n            float tmp0[16*16]; at::vec::transpose_mxn<...>(tmp0, in_ptr0 + ..., ...); // into kernel.preloads\n            float tmp1[16*16]; // into kernel.preloads\n            for i_inner ... { // the kernel inner loop\n              vectorized loads/compute/stores (e.g., load tmp0, store tmp1) // into kernel.loads/compute/stores\n            }\n            at::vec::transpose_mxn(out_ptr0 + ..., tmp1, ...) // into kernel.poststores\n          for inner_most ... (tail)\n            // generated by CppVecKernel\n            ...\n      for i_outer ... (tail)\n        for ...\n          for ...\n            // generated by CppKernel\n            ...\n    \"\"\"\n\n    def __init__(self, args, num_threads, tiling_factor, tiling_indices, tiling_dtype):\n        super().__init__(\n            args, num_threads, tiling_factor, tiling_indices[1], tiling_dtype\n        )\n        self.tiling_indices = tiling_indices\n\n    def inner_itervar(self):\n        return sympy_symbol(f\"{self.itervars[self.outer_idx]}_inner\")\n\n    def need_vec_transpose(self, index):\n        return stride_at(self.itervars[self.outer_idx], index) == 1 and index.has(\n            self.itervars[self.tiling_idx]\n        )\n\n    def gen_transposed_tile_load_store(self, name, var, index, is_store):\n        # transposed tile load/store outside the kernel inner loop\n        dtype = V.graph.get_dtype(name)\n        factor = self.tiling_factor\n        src = f\"{var} + {cexpr_index(index)}\"\n        dst = \"__place_holder__\"\n        ld_src = f\"{cexpr_index(stride_at(self.itervars[self.tiling_idx], index))}\"\n        ld_dst = f\"{factor}\"\n        if is_store:\n            src, dst = dst, src\n            ld_src, ld_dst = ld_dst, ld_src\n\n        need_define = True\n        load_or_store = f\"at::vec::transpose_mxn<{DTYPE_TO_CPP[dtype]},{factor},{factor}>({src}, {ld_src}, {dst}, {ld_dst});\"\n        if is_store:\n            tile_var = self.cse.newvar()\n        elif load_or_store not in self.cse.cache:\n            tile_var = self.cse.generate(self.preloads, load_or_store, write=False)\n        else:\n            need_define = False\n            tile_var = self.cse.cache[load_or_store]\n\n        if need_define:\n            define_line = f\"{DTYPE_TO_CPP[dtype]} {tile_var}[{factor}*{factor}] __attribute__ ((aligned ({factor})));\"\n            self.preloads.writeline(define_line)\n\n        load_or_store = load_or_store.replace(\"__place_holder__\", str(tile_var))\n        if is_store:\n            self.poststores.writeline(DeferredLine(name, load_or_store))\n        else:\n            self.preloads.writeline(load_or_store)\n\n        return tile_var\n"
},
{
    "Id": 131,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/999bae0f54108ffc5b7cf2524a02a83901554b16",
    "Violation": "insufficient",
    "Bug report": "Add padding check for use_nnpack. nnp_convolution_output doesn't support the case of input padding > = kernel_size.",
    "Number of deleted lines": 1,
    "Deleted lines": "    if (transposed && is_output_padding_big()) {\n      return false;\n    }\n    if (transposed && groups > 1 && at::symint::size<T>(input, 1) == groups) {\n      return false;\n    }\n    if (input.device().is_cpu() && input.scalar_type() == kBFloat16 && mkldnn_bf16_device_check()) {\n      return true;\n    }\n    return (input.is_mkldnn()) || // input is mkldnn Tensor\n      (input.device().is_cpu() &&\n       input.scalar_type() == kFloat && // only on CPU Float Tensors\n       // For 1x1 filters, MKLDNN is faster than THNN when multi-threaded,\n       // but THNN is faster when single-threaded.\n       (is_strided() || is_dilated() || at::symint::size<T>(input, 0) >= 16 ||\n        at::symint::size<T>(weight, -1) != 1 || at::symint::size<T>(weight, -2) != 1 || at::get_num_threads() > 1) &&\n       (groups > 1\n        || (at::symint::size<T>(weight, -1) > 3 && at::symint::size<T>(weight, -2) > 3)\n        || at::symint::size<T>(input, 0) > 1\n        || at::symint::size<T>(input, 0)*at::symint::size<T>(input, 1)*at::symint::size<T>(input, 2)*at::symint::size<T>(input, 3) > 20480) // for some case, native is faster\n        );\n\n#endif\n    return false;\n  }\n  bool use_nnpack(const at::Tensor& input, const at::Tensor& weight) const  {\n#if AT_NNPACK_ENABLED()\n    return at::_nnpack_available() &&\n           input.device().is_cpu() &&\n           input.scalar_type() == kFloat && // only on CPU Float Tensors\n           !is_dilated() && // or dilation\n           !transposed &&   // or transposed tensors\n           input.ndimension() == 4 && // must be in NCHW format\n           weight.ndimension() == 4 &&\n           (at::symint::size<T>(weight, 2) < 17) && (at::symint::size<T>(weight, 3) < 17) // NNPACK only supports kernels up to 16x16\n#if !defined(C10_MOBILE)\n           && at::symint::size<T>(input, 0) >= 16 // ensure large enough batch size to ensure perf, tuneable\n#endif\n       ;\n#endif\n    return false;\n  }\n  bool use_xnnpack(const at::Tensor& input, const at::Tensor& weight,\n                   const at::OptionalArrayRef<T> bias_sizes_opt) const {\n#if defined(C10_MOBILE)\n    if (!transposed) {\n      // NB: for the call here, it MATTERS that we are templated. If you\n      // untemplate this to always use SymInt, the function\n      // xnnpack_use_convolution2d will always return false\n      return (at::symint::size<T>(input, 1) == groups) &&\n              xnnpack_use_convolution2d(\n                  input,\n                  weight,\n                  bias_sizes_opt,\n                  padding,\n                  stride,\n                  dilation,\n                  groups,\n                  transposed);\n    }\n#endif\n    return false;\n  }\n\n  bool use_mps(const at::Tensor& input, const at::Tensor& weight) const {\n    // These checks need to be expanded. Currently we have very limited set of\n    // checks for MPS.\n#ifdef USE_MPS\n    if (needs_64bit_indexing_no_split(input, weight)) {\n      return false;\n    }"
},
{
    "Id": 132,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/66b04e3cb707d30c4698b269c83cb6221848f17a",
    "Violation": "missing",
    "Bug report": "nullptr profiling name. Sometimes profiling name can be a nullptr, which throws on conversion to std::string. This adds a check.",
    "Number of deleted lines": 1,
    "Deleted lines": "    c10::SmallVector<int, 4> input_dims_;\n    c10::SmallVector<int, 4> output_dims_;\n    c10::SmallVector<int64_t, 8> sizes_; // flattened from inputs, outputs\n    bool retired_ = false; // is this work entry no longer in the workMetaList_?\n                           // a retired but not completed event has timed out\n  };\n\n  bool enabled_ = false;\n  bool capture_cpp_stack_ = false;\n  std::mutex mutex_;\n  std::vector<Entry> entries_;\n  size_t max_entries_ = 0;\n  size_t next_ = 0;\n  size_t id_ = 0;\n\n  c10::optional<size_t> record(\n      size_t pg_id,\n      size_t seq_id,\n      const char* profiling_name,\n      const std::vector<at::Tensor>& inputs,\n      const std::vector<at::Tensor>& outputs,\n      EventList* start,\n      EventList* end) {\n    if (!enabled_) {\n      return c10::nullopt;\n    }\n    auto traceback =\n        torch::CapturedTraceback::gather(true, true, capture_cpp_stack_);\n    std::lock_guard<std::mutex> guard(mutex_);\n\n    auto te = Entry{\n        id_,\n        pg_id,\n        seq_id,\n        profiling_name,\n        std::move(traceback),\n        std::move(start),\n        std::move(end),\n        c10::getTime()};\n\n    for (const auto& input : inputs) {\n      c10::IntArrayRef sizes = input.sizes();\n      te.input_dims_.push_back(sizes.size());\n      te.sizes_.insert(te.sizes_.end(), sizes.begin(), sizes.end());\n    }\n\n    for (const auto& output : outputs) {\n      c10::IntArrayRef sizes = output.sizes();\n      te.output_dims_.push_back(sizes.size());\n      te.sizes_.insert(te.sizes_.end(), sizes.begin(), sizes.end());\n    }\n\n    if (entries_.size() < max_entries_) {\n      entries_.emplace_back(std::move(te));\n    } else {\n      entries_[next_++] = std::move(te);\n      if (next_ == max_entries_) {\n        next_ = 0;\n      }\n    }\n    return id_++;\n  }\n\n  void update_state(Entry& r) {\n    if (r.start_ != nullptr) {\n      bool started = true;\n      for (auto& ev : *r.start_) {\n        if (!ev.query()) {\n          started = false;\n          break;\n        }"
},
{
    "Id": 133,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/14c47fb211bc929ae4e505e7e13411faa2339f00",
    "Violation": "missing",
    "Bug report": " fix invalid-null-argument UBSAN error in math_cpu.cc. Add an if statement to check if the destination buffer is not nullptr.",
    "Number of deleted lines": 0,
    "Deleted lines": "      vst1q_f32_aligned(image + i + 8, v2);\n      vst1q_f32_aligned(image + i + 12, v3);\n      vst1q_f32_aligned(image + i + 16, v4);\n      vst1q_f32_aligned(image + i + 20, v5);\n      vst1q_f32_aligned(image + i + 24, v6);\n      vst1q_f32_aligned(image + i + 28, v7);\n    }\n\n    // Non-vectorizable epilogue\n    for (; i < image_size; ++i) {\n      image[i] += b;\n    }\n#else\n    // Non-NEON CPU implementation\n    for (int i = 0; i < image_size; ++i) {\n      image[i] += b;\n    }\n#endif // __ARM_NEON__\n\n    image += image_size;\n  }\n}\n\ntemplate <>\nvoid CopyMatrix<CPUContext>(\n    const size_t itemsize,\n    const int M,\n    const int N,\n    const void* A,\n    const int lda,\n    void* B,\n    const int ldb,\n    CPUContext* /*context*/,\n    TypeMeta::TypedCopy copy) {\n  if (lda == N && ldb == N) {\n    // can coalese to a single memcpy of size M * N\n    if (copy) {\n      copy(static_cast<const char*>(A), static_cast<char*>(B), N * M);\n    } else {\n      memcpy(\n          static_cast<char*>(B), static_cast<const char*>(A), itemsize * N * M);\n    }\n    return;\n  }\n\n  for (int i = 0; i < M; ++i) {\n    if (copy) {\n      copy(\n          static_cast<const char*>(A) + lda * i * itemsize,\n          static_cast<char*>(B) + ldb * i * itemsize,\n          N);\n    } else {\n      memcpy(\n          static_cast<char*>(B) + ldb * i * itemsize,\n          static_cast<const char*>(A) + lda * i * itemsize,\n          itemsize * N);\n    }\n  }\n}\n\n#define CAFFE2_SPECIALIZED_COPYVECTOR(T)                            \\\n  template <>                                                       \\\n  void CopyVector<T, CPUContext>(                                   \\\n      const int N, const T* src, T* dst, CPUContext* /*context*/) { \\\n    if (src != dst && N > 0) {                                      \\\n      memcpy(dst, src, sizeof(T) * N);                              \\\n    }                                                               \\\n  }\nCAFFE2_SPECIALIZED_COPYVECTOR(float)\n#undef CAFFE2_SPECIALIZED_COPYVECTOR"
},
{
    "Id": 134,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f77f88fbc7511b405c4e493bdd74634b633f63d1",
    "Violation": "missing",
    "Bug report": "X86 qengine always uses fbgemm kernels on OS other than Linux. X86 quantization backend (qengine) with oneDNN kernels has not been validated on OS other than Linux. So, let it fall back to fbgemm if OS is not Linux. This makes sure the behavior is the same on Windows/Mac as the previous default fbgemm qengine on x86 CPUs.",
    "Number of deleted lines": 1,
    "Deleted lines": "    }\n    t.set_scale(scales);\n  }\n}\n\n// ONEDNN requires symmetric quantization of weight\n// Use this util function to check.\nstatic bool is_weight_symmetric_quant(\n      const at::Tensor& weight,\n      bool is_transposed_conv) {\n  bool is_symmetric = true;\n  const auto qtype = weight.qscheme();\n  if (qtype == c10::kPerTensorAffine) {\n    is_symmetric &= (weight.q_zero_point() == 0);\n  } else if (qtype == c10::kPerChannelAffine) {\n    if (is_transposed_conv) {\n      // This case is currently not supported in PyTorch\n      // but we do not want to raise an error in this util function.\n      is_symmetric = false;\n    } else {\n      auto output_channels = weight.size(0);\n      for (int i = 0; i < output_channels; ++i) {\n        auto zp = weight.q_per_channel_zero_points()[i].item<int32_t>();\n        is_symmetric &= (zp == 0);\n      }\n    }\n  } else {\n    // This case is currently not supported in PyTorch\n      // but we do not want to raise an error in this util function.\n    is_symmetric = false;\n  }\n  return is_symmetric;\n}\n\n// Check if onednn should be used w.r.t fbgemm\nstatic bool should_use_onednn_quant(\n    const at::Tensor& weight,\n    bool is_transposed_conv,\n    int groups,\n    torch::List<int64_t> output_padding) {\n  bool vnni_available = cpuinfo_has_x86_avx512vnni();\n  bool w_sym_quant =\n      is_weight_symmetric_quant(weight, is_transposed_conv);\n  bool opad_all_zero =\n      std::all_of(output_padding.begin(), output_padding.end(), [](int i) { return i==0; });\n  return vnni_available && (groups <= 100) && w_sym_quant && opad_all_zero;\n}\n\n} // onednn_utils\n\n#endif // #if AT_MKLDNN_ENABLED()\n"
},
{
    "Id": 135,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/acd51e13f727af22e6c9e579518362898f1b12e6",
    "Violation": "missing",
    "Bug report": "TorchScript add check if quantized",
    "Number of deleted lines": 0,
    "Deleted lines": "            return graph_diff_errors, tensor_compare_errors\n\n        def wrap_retval(x):\n            return x if isinstance(x, tuple) else (x,)\n\n        def run_mod_and_filter_tensor_outputs(mod, inputs, running_what):\n            try:\n                outs = wrap_retval(mod(*_clone_inputs(inputs)))\n                outs = [out for out in outs if isinstance(out, torch.Tensor)]\n                return outs\n            except Exception as e:\n                raise TracingCheckError(*graph_diagnostic_info(),\n                                        extra_msg='Encountered an exception while running the ' + running_what +\n                                                  ' with test inputs.\\nException:\\n' + indent(str(e)))\n\n        has_warned = [False]\n\n        def maybe_warn_nondeterministic():\n            if has_warned[0]:\n                return\n            has_warned[0] = True\n            nondeterm_ops = [op for op in traced_func.graph.nodes() if op.isNondeterministic()]\n            if len(nondeterm_ops) > 0:\n                nondeterministic_ops_warning = \"Trace had nondeterministic nodes. \"\n                nondeterministic_ops_warning += \"Did you forget call .eval() on your model? Nodes:\\n\"\n                nondeterministic_ops_warning += \"\\n\".join([indent(str(op)) for op in nondeterm_ops][:20])\n                nondeterministic_ops_warning += \"\\nThis may cause errors in trace checking. To disable trace checking,\"\\\n                                                \" pass check_trace=False to torch.jit.trace()\"\n                warnings.warn(nondeterministic_ops_warning, category=TracerWarning, stacklevel=5)\n\n        def compare_outputs(original, reference, match_what):\n            all_ok = True\n            for i, (orig, ref) in enumerate(zip(original, reference)):\n                try:\n                    torch.testing.assert_allclose(orig.double(), ref.double(), rtol=check_tolerance,\n                                                  atol=torch.testing._get_default_tolerance(orig, ref)[1])\n                except AssertionError as e:\n                    maybe_warn_nondeterministic()\n                    warnings.warn('Output nr ' + str(i + 1) + '. of the traced function does not match '\n                                  'the corresponding output of the ' + match_what + '. Detailed error:\\n' + str(e),\n                                  category=TracerWarning, stacklevel=4)\n                    all_ok = False\n\n            return all_ok\n\n        traced_outs = run_mod_and_filter_tensor_outputs(traced_func, inputs, 'trace')\n        fn_outs = run_mod_and_filter_tensor_outputs(func, inputs, 'Python function')\n        if compare_outputs(traced_outs, fn_outs, 'Python function'):\n            check_outs = run_mod_and_filter_tensor_outputs(check_mod_func, inputs, 'repeated trace')\n            compare_outputs(traced_outs, check_outs, 'repeated trace')\n\n        diag_info = graph_diagnostic_info()\n        if any(info is not None for info in diag_info):\n            raise TracingCheckError(*diag_info)\n\n\nclass TracerWarning(Warning):\n    @staticmethod\n    def ignore_lib_warnings():\n        # We ignore warnings from all submodules excluding the JIT, because we need them e.g. for _check_trace\n        warnings.filterwarnings('ignore', category=TracerWarning, module='torch.(?!jit)')\n\n\n# We ignore the tracer warnings coming form inside the library, because all our shape\n# checks in nn will trigger them.\nTracerWarning.ignore_lib_warnings()\ntorch._C._tracer_warn_use_python()\n\n\ndef make_tuple(example_inputs):"
},
{
    "Id": 136,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0584fd9339af7c939ab7d955db05743ba58ff86d",
    "Violation": "improper",
    "Bug report": "Only insert observers for fixed qparam ops. Fixed a condition check for fixed qparam ops, previously we were including CopyNodes as well",
    "Number of deleted lines": 1,
    "Deleted lines": "                and model.training:\n            # we only insert fake quantize module in qat\n            assert pattern is not None\n            if activation_dtype(qconfig) == torch.float16:\n                activation_post_process_ctr = qconfig.activation\n            else:\n                activation_post_process_ctr = \\\n                    get_default_output_activation_post_process_map().get(\n                        pattern, None)\n            assert activation_post_process_ctr is not None, \\\n                \"activation_post_process constructor not provided \" + \\\n                \"for pattern:\" + str(pattern)\n            insert_observer(\n                node, activation_post_process_ctr(),\n                model, activation_post_process_map, env, observed_graph,\n                load_arg, observed_node_names_set)\n        elif (isinstance(quantize_handler,\n                         FixedQParamsOpQuantizeHandler) and\n              not model.training) or \\\n                isinstance(quantize_handler, CopyNode):\n            # inserting observers for output of observed module, or\n            # mark the output as observed\n            assert node.op in [\n                'call_module',\n                'call_function',\n                'call_method'], \\\n                'CopyNode of type ' + node.op + ' is not handled'\n\n            def is_observed(input_arg):\n                if isinstance(input_arg, Node):\n                    return input_arg.name in observed_node_names_set\n                elif isinstance(input_arg, list):\n                    return all(map(is_observed, input_arg))\n\n            if activation_dtype(qconfig) == torch.float16:\n                insert_observer(\n                    node, qconfig.activation(),\n                    model, activation_post_process_map, env, observed_graph,\n                    load_arg, observed_node_names_set)\n            else:\n                # propagate observed property from input\n                if is_observed(node.args[0]):\n                    observed_node_names_set.add(node.name)\n        elif (isinstance(quantize_handler, BinaryOp) and\n              quantize_handler.num_node_args == 1):\n            assert matched_nodes is not None\n            input_node = matched_nodes[-1]  # first node in the sequence\n\n            def input_is_observed(arg):\n                return (isinstance(arg, Node) and\n                        arg.name in observed_node_names_set)\n            # This is checking if one of the argument of add/mul\n            # is an observed node\n            # If both of the inputs are number,\n            # we will not consider the output to be observed\n            if (input_is_observed(input_node.args[0]) or\n                    input_is_observed(input_node.args[1])):\n                observed_node_names_set.add(node.name)\n\n            if activation_dtype(qconfig) == torch.float16:\n                # observer for outputs\n                new_observer = qconfig.activation()\n                insert_observer(\n                    node, new_observer, model,\n                    activation_post_process_map, env, observed_graph,\n                    load_arg, observed_node_names_set)\n        elif isinstance(quantize_handler,\n                        StandaloneModuleQuantizeHandler):\n            assert node.op == \"call_module\"\n            assert isinstance(node.target, str)\n            sm_out_qidxs = modules[node.target]._standalone_module_output_quantized_idxs.tolist()  # type: ignore"
},
{
    "Id": 137,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/97245a06e14a5b87a0bca1908d7200603aff2c8c",
    "Violation": "missing",
    "Bug report": "TORCH_INTERNAL_ASSERT_DEBUG_ONLY won't be enabled during non-debug builds, but for 1 dimension Tensors the check is cheap enough and not catching this can slow down development a lot.",
    "Number of deleted lines": 5,
    "Deleted lines": "#pragma once\n\n#include <ATen/Dispatch.h>\n#include <ATen/NestedTensorImpl.h>\n#include <ATen/Parallel.h>\n#include <ATen/core/Tensor.h>\n#include <c10/core/DispatchKeySet.h>\n#include <c10/core/TensorImpl.h>\n#include <c10/macros/Macros.h>\n#include <c10/util/Exception.h>\n\n#ifndef AT_PER_OPERATOR_HEADERS\n\n#include <ATen/Functions.h>\n#include <ATen/NativeFunctions.h>\n#else\n#include <ATen/ops/cat.h>\n#include <ATen/ops/empty.h>\n#include <ATen/ops/ones_native.h>\n#include <ATen/ops/prod.h>\n#include <ATen/ops/stack_native.h>\n#include <ATen/ops/tensor.h>\n#endif\n\n#include <utility>\n#include <vector>\n\nnamespace at {\nnamespace native {\nstruct NestedTensorImpl;\n\n// The following functions are used to construct nested tensors from buffers and\n// metadata.\n\ninline at::Tensor wrap_buffer(\n    at::Tensor buffer,\n    at::Tensor nested_sizes) {\n  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(\n      buffer.is_contiguous(), \"Given buffer must be contiguous.\");\n  return at::detail::make_tensor<NestedTensorImpl>(\n      std::move(buffer), std::move(nested_sizes));\n}\n\n// TODO: Figure out if we need a non-moving wrap_buffer()\ninline at::Tensor wrap_buffer(\n    at::Tensor buffer,\n    at::Tensor nested_sizes,\n    at::Tensor nested_strides,\n    at::Tensor storage_offsets) {\n  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(\n      buffer.is_contiguous(), \"Given buffer must be contiguous.\");\n  return at::detail::make_tensor<NestedTensorImpl>(\n      std::move(buffer),\n      std::move(nested_sizes),\n      std::move(nested_strides),\n      std::move(storage_offsets));\n}\n\ninline at::Tensor get_buffer(const at::Tensor& tensor) {\n  return get_nested_tensor_impl(tensor)->get_buffer();\n}\n\n/**\n * Create a new nested tensor that is a view of a base nested tensor\n *\n * create_view_tensor calls a specialized constructor that copys the\n * the keys from base onto the new view tensor being created.\n * The storage is shared between the base and the returned view tensor\n *\n * All callers of this helper must:\n * - Only return a view of the input\n * - Must be explicit and define a derivative\n *\n * @param base Base tensor to construct view from.\n * @param nested_sizes View tensors' sizes."
},
{
    "Id": 138,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9bcb4de16878073896d8743fbd70d5abe28b595a",
    "Violation": "insufficient",
    "Bug report": "check parameter k and l ",
    "Number of deleted lines": 1,
    "Deleted lines": "} // namespace c10\n\nnamespace c10 {\n\n/**\n * A utility function to convert vector<int> to vector<int64_t>.\n */\ninline std::vector<int64_t> ToVectorint64_t(const ArrayRef<int>& src) {\n  return std::vector<int64_t>(src.begin(), src.end());\n}\n\n/**\n * Return product of all dimensions starting from k\n */\ninline int64_t size_from_dim_(int k, IntArrayRef dims) {\n  int64_t r = 1;\n  for (const auto i : c10::irange(k, dims.size())) {\n    r *= dims[i];\n  }\n  return r;\n}\n\n// Product of all dims up to k (not including dims[k])\ninline int64_t size_to_dim_(int k, IntArrayRef dims) {\n  TORCH_CHECK((unsigned)k <= dims.size());\n  int64_t r = 1;\n  for (const auto i : c10::irange(k)) {\n    r *= dims[i];\n  }\n  return r;\n}\n\n// Product of all dims between k and l (not including dims[k] and dims[l])\ninline int64_t size_between_dim_(int k, int l, IntArrayRef dims) {\n  TORCH_CHECK((unsigned)l < dims.size());\n  int64_t r = 1;\n  if (k < l) {\n    for (int i = k + 1; i < l; ++i) {\n      r *= dims[i];\n    }\n  } else {\n    for (int i = l + 1; i < k; ++i) {\n      r *= dims[i];\n    }\n  }\n  return r;\n}\n\n// Wrap around axis_index if it is negative, s.t., -1 is the last dim\ninline int canonical_axis_index_(int axis_index, int ndims) {\n  TORCH_CHECK(axis_index >= -ndims);\n  TORCH_CHECK(axis_index < ndims);\n  if (axis_index < 0) {\n    return axis_index + ndims;\n  }\n  return axis_index;\n}\n\nusing PlacementDtor = void (*)(void*, size_t);\n\n/*\n * A Context that will call extra placement deleter during\n * deconstruction.\n *\n * Accept a already constructed DataPtr and store it as member\n * during destruction, we'll call extra deleter on the underlying\n * data pointer before the DataPtr is destructed.\n * `data_ptr_` owns the memory.\n */\nstruct C10_API PlacementDeleteContext {\n  DataPtr data_ptr_;"
},
{
    "Id": 139,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/65496e4e67b42e52b3428b0cf2d994e0aa1a9902",
    "Violation": "missing",
    "Bug report": "Bug fix in bound shape inferencer. Accessing dims() without boundary check is not good.",
    "Number of deleted lines": 1,
    "Deleted lines": "  }\n}\n\nvoid BoundShapeInferencer::InferReshape(const OperatorDef& op) {\n  InferCommonOp(op);\n  // old_shape should be a constant\n  if (op.output_size() > 1 && shape_info_.count(op.output(1))) {\n    shape_info_[op.output(1)].dim_type = ShapeInfo::DimType::CONSTANT;\n  }\n}\n\nvoid BoundShapeInferencer::InferConcatInputs(const OperatorDef& op) {\n  ArgumentHelper helper(op);\n  const auto add_axis = helper.GetSingleArgument<int32_t>(\"add_axis\", 0);\n  if (add_axis) {\n    return;\n  } else if (op.output_size() == 0 || !shape_info_.count(op.output(0))) {\n    return;\n  }\n\n  const auto axis = helper.HasArgument(\"axis\")\n      ? helper.GetSingleArgument<int32_t>(\"axis\", -1)\n      : GetDimFromOrderString(\n            helper.GetSingleArgument<string>(\"order\", \"NCHW\"));\n\n  const auto& shape_info = shape_info_.at(op.output(0));\n  int output_channel = shape_info.shape.dims(axis);\n  int missing_shape_infos = 0;\n  int channel_acc = 0;\n  std::string input_to_infer;\n  for (const auto& i : op.input()) {\n    const auto it = shape_info_.find(i);\n    if (it != shape_info_.end()) {\n      const auto& current_input_shape = it->second;\n      channel_acc += current_input_shape.shape.dims(axis);\n    } else if (missing_shape_infos) {\n      LOG(INFO) << \"More than one missing shapes, previous one: \"\n                << input_to_infer;\n      // We can only infer one missing input shape info\n      return;\n    } else {\n      ++missing_shape_infos;\n      input_to_infer = i;\n    }\n  }\n\n  if (missing_shape_infos && !input_to_infer.empty()) {\n    auto input_shape_info = shape_info;\n    input_shape_info.shape.set_dims(axis, output_channel - channel_acc);\n    shape_info_.emplace(input_to_infer, std::move(input_shape_info));\n\n    // Infer the shape of the second output of Concat\n    InferCommonOp(op);\n    if (op.output_size() > 1 && shape_info_.count(op.output(1))) {\n      shape_info_[op.output(1)].dim_type = ShapeInfo::DimType::CONSTANT;\n    }\n  }\n}\n\n// For concat net, if some inputs are missing and we have add_axis argument, it\n// means that all the inputs should be of the same dimension. In this case, we\n// can infer the shape of the missing inputs\nvoid BoundShapeInferencer::InferConcat(const OperatorDef& op) {\n  ArgumentHelper helper(op);\n  auto add_axis = helper.GetSingleArgument<int32_t>(\"add_axis\", 0);\n  if (add_axis) {\n    ShapeInfo* ref_input_shape = nullptr;\n    std::string ref_name;\n    std::unordered_set<std::string> missing_shape_inputs;\n    for (const auto& i : op.input()) {\n      const auto it = shape_info_.find(i);"
},
{
    "Id": 140,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/260f66c3165ce0c48dd1514a916da6971d981578",
    "Violation": "missing",
    "Bug report": "Fix concat dimension check bug",
    "Number of deleted lines": 1,
    "Deleted lines": "  auto op_device =\n      def.has_device_option() ? def.device_option() : DeviceOption();\n  vector<DeviceOption> in_dev(def.input_size(), op_device);\n  vector<DeviceOption> out_dev(def.output_size(), op_device);\n\n  // 2nd output's type is always CPU irrespective of op's device option.\n  CAFFE_ENFORCE_GT(out_dev.size(), 1);\n  out_dev[1] = DeviceOption();\n  return std::make_pair(in_dev, out_dev);\n}\n} // namespace\n\nREGISTER_CPU_OPERATOR(Concat, ConcatOp<CPUContext>);\nOPERATOR_SCHEMA(Concat)\n    .NumInputs(1, INT_MAX)\n    .NumOutputs(2)\n    .Arg(\"axis\", \"*(type: int; default: -1)* Axis to concatenate on.\")\n    .Arg(\n        \"order\",\n        \"*(type: string; default='NCHW')* Order of blob dimensions. Concats on the C dimension.\")\n    .Arg(\n        \"add_axis\",\n        \"*(type: int)* Pass non-zero integer to add the axis specified in `axis` to all input tensors.\")\n    .TensorInferenceFunction(OpSchema::NeedsAllInputShapes([](const OperatorDef&\n                                                                  def,\n                                                              const vector<\n                                                                  TensorShape>&\n                                                                  in) {\n      ArgumentHelper helper(def);\n      const int axis = helper.HasArgument(\"axis\")\n          ? helper.GetSingleArgument<int>(\"axis\", -1)\n          : GetDimFromOrderString(\n                helper.GetSingleArgument<string>(\"order\", \"NCHW\"));\n      bool add_axis = helper.GetSingleArgument<int>(\"add_axis\", 0) != 0;\n      const int canonical_axis = canonical_axis_index_(axis, in[0].dims_size());\n      CAFFE_ENFORCE_GT(in.size(), 0);\n      vector<int> split_shape(1, in.size());\n      vector<int> out_shape(in[0].dims().begin(), in[0].dims().end());\n      if (add_axis) {\n        for (int i = 1; i < in.size(); ++i) {\n          CAFFE_ENFORCE_EQ(\n              in[0].dims().size(),\n              in[i].dims().size(),\n              \"All inputs of Concat should have same dims when add_axis = 1. \"\n              \"Got different sizes for inputs 0 and \",\n              i);\n          for (int j = 0; j < in[0].dims().size(); ++j) {\n            CAFFE_ENFORCE_EQ(\n                in[0].dims(j),\n                in[i].dims(j),\n                \"All inputs of Concat should have same dims when add_axis = 1. \"\n                \"Got different dims for inputs 0 and \",\n                i,\n                \". At dim: \",\n                j);\n          }\n        }\n        out_shape.insert(out_shape.begin() + canonical_axis, in.size());\n      } else {\n        for (int i = 1; i < in.size(); ++i) {\n          CAFFE_ENFORCE_EQ(\n              in[0].dims().size(),\n              in[i].dims().size(),\n              \"All inputs of Concat should have same dims except \"\n              \"canonical_axis dim that is equal to \",\n              canonical_axis,\n              \"Got different sizes for inputs 0 and \",\n              i);\n          for (int j = 0; j < in[0].dims().size(); ++j) {\n            if (j == canonical_axis) {\n              continue;"
},
{
    "Id": 141,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc07102b17915f21170fae9a9d52c6f2d59726ca",
    "Violation": "missing",
    "Bug report": "Check dim size preventively when doing shape inference for BatchMatMul. We check input(0) but not input(1) in BatchMatMul. This may result in a protobuf exception which won't be caught by upstream and causing termination of the program. Check that with `CAFFE_ENFORCE` will be caught by upstream inference function. Plus, it will print out clean stack tracing showing where went wrong.",
    "Number of deleted lines": 0,
    "Deleted lines": ""
},
{
    "Id": 142,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/77523df413ff7f8a336b6481cfa47967c234a149",
    "Violation": "missing",
    "Bug report": "Add more check on softmax ONNX exporting logic. * Add more check on softmax exporting logic * Add more comments about axis and dim",
    "Number of deleted lines": 0,
    "Deleted lines": "            if size == 1:\n                dims.append(i)\n    else:\n        dims = [dim]\n    return g.op(\"Squeeze\", self, axes_i=dims)\n\n\ndef prelu(g, self, weight):\n    return g.op(\"PRelu\", self, weight)\n\n\ndef threshold(g, self, threshold, value):\n    # See Note [Export inplace]\n    if _scalar(threshold) != 0:\n        return _unimplemented(\"threshold\", \"non-zero threshold\")\n    if _scalar(value) != 0:\n        return _unimplemented(\"threshold\", \"non-zero value\")\n    return g.op(\"Relu\", self)\n\n\ndef leaky_relu(g, input, negative_slope, inplace=False):\n    # See Note [Export inplace]\n    # TODO: Talk to ONNX about unconditional cast of scalar to float\n    return g.op(\"LeakyRelu\", input, alpha_f=_scalar(negative_slope))\n\n\ndef glu(g, input, dim):\n    assert input.type().sizes()[dim] % 2 == 0\n\n    first, second = g.op('Split', input, axis_i=dim, outputs=2)\n    return g.op('Mul', first, g.op('Sigmoid', second))\n\n\ndef softmax(g, input, dim=None):\n    return g.op('Softmax', input, axis_i=dim)\n\n\ndef softplus(g, self, beta, threshold):\n    if beta != 1:\n        return _unimplemented(\"beta\", \"has to be 1\")\n    return g.op('Softplus', self)\n\n\ndef max_pool1d(g, input, kernel_size, stride, padding, dilation, ceil_mode):\n    if ceil_mode:\n        return _unimplemented(\"max_pool1d\", \"ceil_mode\")\n    if set(_single(dilation)) != {1}:\n        return _unimplemented(\"max_pool1d\", \"dilation\")\n    if stride is None:\n        stride = kernel_size\n    r = g.op(\"MaxPool\", input,\n             kernel_shape_i=_single(kernel_size),\n             pads_i=_single(padding) * 2,\n             strides_i=_single(stride))\n    return r, None\n\n\ndef max_pool2d(g, input, kernel_size, stride, padding, dilation, ceil_mode):\n    if ceil_mode:\n        return _unimplemented(\"max_pool2d\", \"ceil_mode\")\n    if set(_pair(dilation)) != {1}:\n        return _unimplemented(\"max_pool2d\", \"dilation\")\n    if not stride:\n        stride = kernel_size\n    r = g.op(\"MaxPool\", input,\n             kernel_shape_i=_pair(kernel_size),\n             pads_i=_pair(padding) * 2,\n             strides_i=_pair(stride))\n    return r, None\n"
},
{
    "Id": 143,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a53f4b0f9bbc007c0a92e4fd28dd22af027e24a8",
    "Violation": "missing",
    "Bug report": " add dimension check to NHWC2NCHW shape inference. Summary: To prevent assertion from protobuffer when accessing the dims.",
    "Number of deleted lines": 0,
    "Deleted lines": "  }\n  return true;\n}\n\ntemplate <>\nbool NCHW2NHWCOp<float, CPUContext>::RunOnDevice() {\n  auto& X = Input(0);\n  auto* Y = Output(0);\n  CAFFE_ENFORCE(X.ndim() == 4);\n  const int N = X.dim32(0), C = X.dim32(1), H = X.dim32(2), W = X.dim32(3);\n  Y->Resize(N, H, W, C);\n  const float* Xdata = X.data<float>();\n  float* Ydata = Y->mutable_data<float>();\n  for (int n = 0; n < N; ++n) {\n    for (int c = 0; c < C; ++c) {\n      for (int h = 0; h < H; ++h) {\n        for (int w = 0; w < W; ++w) {\n          Ydata[((n * H + h) * W + w) * C + c] = *(Xdata++);\n        }\n      }\n    }\n  }\n  return true;\n}\n\n\nREGISTER_CPU_OPERATOR(NHWC2NCHW, NHWC2NCHWOp<float, CPUContext>);\nREGISTER_CPU_OPERATOR(NCHW2NHWC, NCHW2NHWCOp<float, CPUContext>);\n\nOPERATOR_SCHEMA(NHWC2NCHW)\n    .NumInputs(1)\n    .NumOutputs(1)\n    .TensorInferenceFunction([](const OperatorDef& /*unused*/ /*def*/,\n                                const vector<TensorShape>& in) {\n      vector<TensorShape> out(1);\n      out[0].add_dims(in[0].dims(0));\n      out[0].add_dims(in[0].dims(3));\n      out[0].add_dims(in[0].dims(1));\n      out[0].add_dims(in[0].dims(2));\n      return out;\n    })\n    .SetDoc(R\"DOC(\nThe operator switches the order of data in a tensor from NHWC- sample index N,\nheight H, width H and channels C, to the NCHW order.\n)DOC\")\n    .Input(0, \"data\", \"The input data (Tensor<float>) in the NHWC order.\")\n    .Output(\n        0,\n        \"output\",\n        \"The output tensor (Tensor<float>) in the NCHW order.\");\n\nOPERATOR_SCHEMA(NCHW2NHWC).NumInputs(1).NumOutputs(1)\n  .SetDoc(R\"DOC(\nThe operator switches the order of data in a tensor from NCHW- sample index N,\nchannels C, height H and width W, to the NHWC order.\n)DOC\")\n  .Input(0, \"data\", \"The input data (Tensor<float>) in the NCHW order.\")\n  .Output(0, \"output\", \"The output tensor (Tensor<float>) in the NHWC order.\");\n\n\nclass GetNHWC2NCHWGradient : public GradientMakerBase {\n  using GradientMakerBase::GradientMakerBase;\n  vector<OperatorDef> GetGradientDefs() override {\n    return SingleGradientDef(\n        \"NCHW2NHWC\", \"\",\n        vector<string>{GO(0)},\n        vector<string>{GI(0)});\n  }\n};\nREGISTER_GRADIENT(NHWC2NCHW, GetNHWC2NCHWGradient);"
},
{
    "Id": 144,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ecd3c252b4da3056797f8a505c9ebe8d68db55c4",
    "Violation": "missing",
    "Bug report": "Suport all length one SLS op lowering: C2 part. We check the input shape of lengths and indices of SLS and add an attribute if they are the same.",
    "Number of deleted lines": 0,
    "Deleted lines": "  }\n  AddArgument(\"max_batch_size\", opts_.bound_shape_spec.max_batch_size, &op);\n  AddArgument(\"max_seq_size\", opts_.bound_shape_spec.max_seq_size, &op);\n  AddArgument(\"nominal_batch_idx\", nominal_batch_idx, &op);\n\n  return op;\n}\n\nNetDef OnnxifiTransformer::SubnetToOnnxifiOpViaC2(\n    const caffe2::NetDef& net,\n    const std::unordered_set<std::string>& weights_in_ws,\n    const ShapeInfoMap& shape_hints) {\n  int onnxifi_op_id = onnxifi_op_id_;\n  if (opts_.debug) {\n    WriteProtoToTextFile(\n        net, \"debug_original_net_\" + c10::to_string(onnxifi_op_id) + \".pb_txt\");\n  }\n  if (opts_.min_ops > net.op_size()) {\n    return net;\n  }\n  // We already have all the ops and external inputs and outputs!\n  NetDef onnxifi_net(net);\n\n  // Remove the second output of Concat/Reshape from external_output. In\n  // addition, we remove those outputs from the Onnxifi op too.\n  // TODO: This approach is a bit hacky as we assume that the second output is\n  // never used. A more appropriate approach can be learned from the ONNX path,\n  // where we statically computes the split_info given input shape and insert a\n  // GivenTensorIntFill op\n  std::unordered_set<std::string> split_infos;\n  for (auto& op : *onnxifi_net.mutable_op()) {\n    if ((op.type() == \"Concat\" || op.type() == \"Reshape\") &&\n        op.output_size() == 2) {\n      split_infos.emplace(op.output(1));\n    }\n  }\n  onnxifi_net.clear_external_output();\n  for (const auto& o : net.external_output()) {\n    if (!split_infos.count(o)) {\n      onnxifi_net.add_external_output(o);\n    }\n  }\n\n  // Figure out weights and add it to external_inputs too\n  std::unordered_set<std::string> initialization_list;\n  std::vector<std::string> total_inputs_vec;\n  getWeightsAndInputs(\n      net,\n      weights_in_ws,\n      std::vector<std::string>(),\n      &initialization_list,\n      &total_inputs_vec);\n  auto* shape_arg = onnxifi_net.add_arg();\n  auto* qshape_arg = onnxifi_net.add_arg();\n  shape_arg->set_name(\"input_shape_info\");\n  qshape_arg->set_name(\"input_qshape_info\");\n  onnxifi_net.clear_external_input();\n  for (const auto& i : total_inputs_vec) {\n    onnxifi_net.add_external_input(i);\n    auto info = shape_hints.at(i);\n    if (!info.is_quantized) {\n      shape_arg->mutable_tensors()->Add()->CopyFrom(\n          wrapShapeInfoIntoTensorProto(i, shape_hints.at(i)));\n    } else {\n      qshape_arg->mutable_qtensors()->Add()->CopyFrom(\n          wrapShapeInfoIntoQTensorProto(i, shape_hints.at(i)));\n    }\n  }\n\n  // Compute output shape hints"
},
{
    "Id": 145,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/55092b1cc604fad3d70d31e71bbdd3a43a279423",
    "Violation": "missing",
    "Bug report": "Validate matching input shapes in Int8Add operator. Default engine doesn't support broadcast semantics in Int8Add operator. This patch adds a check that shapes are equivalent.",
    "Number of deleted lines": 0,
    "Deleted lines": "#ifndef CAFFE2_OPERATORS_INT8_ADD_OP_H_\n#define CAFFE2_OPERATORS_INT8_ADD_OP_H_\n\n#include <qnnpack.h>\n\n#include \"caffe2/core/context.h\"\n#include \"caffe2/core/operator.h\"\n#include \"caffe2/core/tensor_int8.h\"\n#include \"caffe2/operators/quantized/int8_utils.h\"\n\nnamespace caffe2 {\n\nnamespace int8 {\n\ntemplate <Activation Ac>\nclass Int8AddOp final : public Operator<CPUContext> {\n public:\n  Int8AddOp(const OperatorDef& operator_def, Workspace* ws)\n      : Operator<CPUContext>(operator_def, ws),\n        ws_(ws) {}\n\n  ~Int8AddOp() {\n    if (this->qnnpackOperator_ != nullptr) {\n      qnnp_delete_operator(this->qnnpackOperator_);\n      this->qnnpackOperator_ = nullptr;\n    }\n  }\n\n  bool RunOnDevice() override {\n    CAFFE_ENFORCE_EQ(Inputs().size(), 2);\n    const auto& A = Inputs()[0]->template Get<Int8TensorCPU>();\n    const auto& B = Inputs()[1]->template Get<Int8TensorCPU>();\n    auto* Y = Outputs()[0]->template GetMutable<Int8TensorCPU>();\n\n    /*\n     * Record quantization parameters for A and B inputs, because if the op is\n     * in-place, we may overwrite these parameters later, when we set\n     * quantization parameters for Y tensor.\n     */\n    const uint8_t A_zero_point = A.zero_point;\n    const uint8_t B_zero_point = B.zero_point;\n    const float A_scale = A.scale;\n    const float B_scale = B.scale;\n\n    const int32_t Y_zero_point =\n      this->template GetSingleArgument<int>(\"Y_zero_point\", 0);\n    const float Y_scale =\n      this->template GetSingleArgument<float>(\"Y_scale\", 1);\n    Y->t.ResizeLike(A.t);\n    Y->zero_point = Y_zero_point;\n    Y->scale = Y_scale;\n\n    initQNNPACK();\n\n    pthreadpool_t threadpool =\n        reinterpret_cast<pthreadpool_t>(ws_->GetThreadPool());\n\n    if (this->qnnpackOperator_ == nullptr) {\n      const qnnp_status createStatus = qnnp_create_add_nc_q8(\n        1 /* channels */,\n        A_zero_point, A_scale,\n        B_zero_point, B_scale,\n        static_cast<uint8_t>(Y_zero_point), Y_scale,\n        activationLimits(Y_scale, Y_zero_point, Ac).first,\n        activationLimits(Y_scale, Y_zero_point, Ac).second,\n        &qnnpackOperator_);\n      CAFFE_ENFORCE(\n          createStatus == qnnp_status_success,\n          \"failed to create QNNPACK add operator\");\n      CAFFE_ENFORCE(this->qnnpackOperator_ != nullptr);"
},
{
    "Id": 146,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1f819ee965894b8332cb364a67c91855c91c9dcc",
    "Violation": "missing",
    "Bug report": "Add check for no grad in transformer encoder nestedtensor conversion.  Before, we allowed inputs with grad to be converted to NestedTensors. Autograd attempts to find the size of the NestedTensor, but NestedTensor throws an exception for its size function. This causes all calls to nn.TransformerEncoder with grad enabled to fail. Fix: we add a check for no grad in transformer encoder so we do not convert tensor with grad to nestedtensor.",
    "Number of deleted lines": 3,
    "Deleted lines": "        Args:\n            src: the sequence to the encoder (required).\n            mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n        output = src\n        convert_to_nested = False\n        first_layer = self.layers[0]\n        if isinstance(first_layer, torch.nn.TransformerEncoderLayer):\n            if (not first_layer.norm_first and not first_layer.training and\n                    first_layer.self_attn.batch_first and\n                    first_layer.self_attn._qkv_same_embed_dim and first_layer.activation_relu_or_gelu and\n                    first_layer.norm1.eps == first_layer.norm2.eps and\n                    src.dim() == 3 and self.enable_nested_tensor) :\n                if src_key_padding_mask is not None and not output.is_nested and mask is None:\n                    tensor_args = (\n                        src,\n                        first_layer.self_attn.in_proj_weight,\n                        first_layer.self_attn.in_proj_bias,\n                        first_layer.self_attn.out_proj.weight,\n                        first_layer.self_attn.out_proj.bias,\n                        first_layer.norm1.weight,\n                        first_layer.norm1.bias,\n                        first_layer.norm2.weight,\n                        first_layer.norm2.bias,\n                        first_layer.linear1.weight,\n                        first_layer.linear1.bias,\n                        first_layer.linear2.weight,\n                        first_layer.linear2.bias,\n                    )\n                    if not torch.overrides.has_torch_function(tensor_args):\n                        if output.is_cuda or 'cpu' in str(output.device):\n                            convert_to_nested = True\n                            output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())\n\n        for mod in self.layers:\n            if convert_to_nested:\n                output = mod(output, src_mask=mask)\n            else:\n                output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n\n        if convert_to_nested:\n            output = output.to_padded_tensor(0.)\n\n        if self.norm is not None:\n            output = self.norm(output)\n\n        return output\n\n\nclass TransformerDecoder(Module):\n    r\"\"\"TransformerDecoder is a stack of N decoder layers\n\n    Args:\n        decoder_layer: an instance of the TransformerDecoderLayer() class (required).\n        num_layers: the number of sub-decoder-layers in the decoder (required).\n        norm: the layer normalization component (optional).\n\n    Examples::\n        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n        >>> transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n        >>> memory = torch.rand(10, 32, 512)\n        >>> tgt = torch.rand(20, 32, 512)\n        >>> out = transformer_decoder(tgt, memory)\n    \"\"\"\n    __constants__ = ['norm']\n\n    def __init__(self, decoder_layer, num_layers, norm=None):\n        super(TransformerDecoder, self).__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)"
},
{
    "Id": 147,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc43ad428603539a2051940c09b191825f66203d",
    "Violation": "missing",
    "Bug report": " add is_grad_enabled check in runtime_wrapper before running with torch.no_grad. We observed that `with torch.no_grad()` in runtime_wrapper introduced ~10% (0.06ms->0.066ms) inference performance regression on lennard_jones on cpu. For inference tasks in benchmark, grad has been disabled, but in the current runtime_wrapper, no_grad is set again and its time is counted into the running time. Therefore, we add `is_grad_enabled` check in runtime_wrapper before running with torch.no_grad. If grad has been disabled, there is no need to set no_grad. ",
    "Number of deleted lines": 1,
    "Deleted lines": "# epilogue for a forward-only inference graph, or for an autograd.Function.apply function.\n# This is because there are some minor differences in how we treat these cases at runtime:\n# - resize_() is currently handled in the inference case, but not fully handled in the autograd case.\n# - the autograd cases inserts TensorAlias wrapper objects for outputs that alias inputs\ndef create_runtime_wrapper(\n    compiled_fn,\n    *,\n    runtime_metadata: ViewAndMutationMeta,\n    indices_of_inps_to_detach: List[int],\n    trace_joint: bool,\n    keep_input_mutations: bool,\n    disable_amp: bool,\n):\n    if not hasattr(compiled_fn, \"_boxed_call\"):\n        compiled_fn = make_boxed_func(compiled_fn)\n\n    def runtime_wrapper(*args):\n        if trace_joint:\n            args_ = list(args)\n            # See Note [Detaching inputs that never need gradients]\n            for idx in indices_of_inps_to_detach:\n                if isinstance(args_[idx], torch.Tensor):\n                    args_[idx] = args_[idx].detach()\n            with torch.autograd._force_original_view_tracking(True):\n                all_outs = call_func_at_runtime_with_args(\n                    compiled_fn,\n                    args_,\n                    disable_amp=disable_amp,\n                )\n        else:\n            # When we have an inference graph, we run with torch.no_grad.\n            # It's possible to get an inference graph with inputs that require grad,\n            # in which case we want to make sure autograd is disabled\n            # (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\n            with torch.no_grad():\n                all_outs = call_func_at_runtime_with_args(\n                    compiled_fn,\n                    args,\n                    disable_amp=disable_amp,\n                )\n\n        num_mutated_runtime_inps = runtime_metadata.num_mutated_inp_runtime_indices\n        num_intermediate_bases = runtime_metadata.num_intermediate_bases\n\n        if keep_input_mutations and trace_joint:\n            num_input_mutations_handled_by_autograd = (\n                runtime_metadata.num_mutated_graph_handled_indices_seen_by_autograd\n            )\n            # autograd.Function requires us to return the mutated inputs as extra outputs to the autograd.Function.forward\n            if num_input_mutations_handled_by_autograd > 0:\n                all_outs = all_outs[:-num_input_mutations_handled_by_autograd]\n\n        assert (\n            len(all_outs)\n            == num_mutated_runtime_inps\n            + runtime_metadata.num_outputs\n            + num_intermediate_bases\n        )\n\n        # Step 3: After running the compiled fw, apply updates to mutated inputs\n        num_mutations_to_apply = runtime_metadata.num_mutated_inp_runtime_indices\n        if num_mutations_to_apply > 0:\n            updated_inputs = all_outs[:num_mutations_to_apply]\n            fw_outs = all_outs[num_mutations_to_apply:]\n\n            for i, inpt_idx in enumerate(runtime_metadata.mutated_inp_runtime_indices):\n                meta = runtime_metadata.input_info[inpt_idx]\n                if not meta.mutates_data and not meta.mutates_metadata:\n                    continue\n                original_inpt = args[inpt_idx]\n                updated_inpt = updated_inputs[i]"
},
{
    "Id": 148,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c1c4882367c592d49e15268a0b99631c207d662e",
    "Violation": "insufficient",
    "Bug report": "Based on discussions with Sherlock + Zhengxu in D51118067, updated the internal thrift schema to match the OSS schema.. So to bypass these failures I did the following hacks(?): Before creating the exported program in deserialization, populate nodes w/o meta[\"val\"] with meta[\"val\"] = None * Add torch.autograd.grad_mode.set_grad_enabled to the skip opset * Duplicated ExportGraphSignature into aot_export.py so that the graph signature checks will be skipped",
    "Number of deleted lines": 1,
    "Deleted lines": "\n    @final\n    def check(self, ep: ExportedProgram) -> None:\n        if not isinstance(ep.graph_signature, ExportGraphSignature):\n            # TODO Enforce type checking in the constructor.\n            return\n        self._check_graph_module(ep.graph_module)\n        try:\n            _verify_exported_program_signature(ep)\n        except SpecViolationError as e:\n            # TODO Remove this branch.\n            if ep.dialect == \"EDGE\":  # !!! Don't change this allowlist. !!!\n                pass\n            else:\n                raise e\n\n    @final\n    def _check_graph_module(self, gm: torch.fx.GraphModule) -> None:\n        def _allowed_getattr_types() -> Tuple[Type[Any], ...]:\n            ret = self.allowed_getattr_types()\n            assert not any(t is object for t in ret)\n            return ret\n\n        def _check_valid_op(op) -> None:\n            def _allowed_builtin_ops() -> List:\n                ret = self.allowed_builtin_ops()\n                assert all(inspect.isbuiltin(op) for op in ret)\n                return ret\n\n            def _allowed_op_types() -> Tuple[Type[Any], ...]:\n                ret = self.allowed_op_types()\n                assert not any(t is object for t in ret)\n                return ret\n\n            if not isinstance(op, _allowed_op_types()):\n                if op not in _allowed_builtin_ops():\n                    raise SpecViolationError(\n                        f\"Operator '{op}' is not an allowed operator type: {_allowed_op_types()}\\n\"\n                        f\"Valid builtin ops: {_allowed_builtin_ops()}\"\n                    )\n\n            if isinstance(op, OpOverload):\n                # All ops functional\n                if not is_functional(op):\n                    raise SpecViolationError(\n                        f\"operator '{op}' is not functional\"\n                    )\n            self.check_valid_op(op)\n\n        for mod in gm.modules():\n            if not isinstance(mod, torch.fx.GraphModule):\n                continue\n\n            mod.graph.lint()\n            for node in mod.graph.nodes:\n                # TODO(T140410192): should have fake tensor for all dialects\n                if node.op in {\"call_module\", \"call_method\"}:\n                    raise SpecViolationError(\n                        f\"call_module is not valid: got a class '{node.target}' \",\n                    )\n\n                elif node.op == \"call_function\":\n                    _check_val(node)\n\n                    _check_valid_op(node.target)\n\n                elif node.op == \"get_attr\":\n                    if not isinstance(node.target, str):\n                        raise SpecViolationError(\n                            f\"Expected get_attr target to be string, but got {type(node.target)}\"\n                        )\n\n                    attr = getattr(mod, node.target)\n                    if isinstance(attr, torch.nn.Module):\n                        def _is_type(name, ty):"
},
{
    "Id": 149,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f3a2094065c8b4b7bae426e71c923a8a8abb74b5",
    "Violation": "insufficient",
    "Bug report": "Mitigate legacy issue that aten op as export entrance function. This is not supported any more, now the top level ```torch.export``` only support ```nn.Module```, but there are still some tests using the internal APIs and caused the ```trace_rules.check``` assertion error. This PR is going to mitigate such cases.",
    "Number of deleted lines": 0,
    "Deleted lines": "        # optimize_assert since user program may mutate the inputs.\n        flat_args, in_spec = pytree.tree_flatten((args, kwargs))\n\n        remove_from_cache(f)\n        constraint_violation_error = None\n        if tracing_mode != \"symbolic\":\n            assume_static_by_default = True\n        with config.patch(\n            specialize_int=True,\n            assume_static_by_default=assume_static_by_default,\n            automatic_dynamic_shapes=False,\n            capture_dynamic_output_shape_ops=True,\n            capture_scalar_outputs=True,\n        ):\n            opt_f = optimize_assert(\n                dynamo_normalization_capturing_compiler,\n                hooks=Hooks(\n                    guard_export_fn=guard_export_print,\n                    guard_fail_fn=None,\n                ),\n                export=True,\n                export_constraints=constraints,\n            )(f)\n            # TODO(voz): We may have instances of `f` that mutate inputs, we should track sideeffects and reject.\n            try:\n                result_traced = opt_f(*args, **kwargs)\n            except ConstraintViolationError as e:\n                constraint_violation_error = e\n        remove_from_cache(f)\n\n        if (\n            not disable_constraint_solver\n            and (shape_env := getattr(fake_mode, \"shape_env\", None)) is not None\n            and (dim_constraints := shape_env.dim_constraints) is not None\n            and not trace_rules.check(call_to_inspect)\n        ):\n            dim_constraints.solve()\n            dim_constraints.remove_redundant_dynamic_results()\n            forced_specializations = dim_constraints.forced_specializations()\n            msg = dim_constraints.prettify_results(\n                original_signature, constraint_violation_error, forced_specializations\n            )\n            if constraint_violation_error:\n                constraint_violation_error.args = (\n                    constraint_violation_error.args[0] + msg,\n                )\n            else:\n                if forced_specializations:\n                    constraint_violation_error = ConstraintViolationError(msg)\n                else:\n                    log.info(\n                        \"Summary of dimension constraints:%s\",\n                        msg,\n                    )\n\n            # Error if we have any constraints on static values\n            for k in shape_env.var_to_range.keys():\n                if isinstance(k, sympy.Integer):\n                    constraint_violation_error = ConstraintViolationError(\n                        f\"{''.join(traceback.format_list(shape_env.var_to_stack[k]))}\\n\"\n                        \"It appears that you're trying to set a constraint on a \"\n                        f\"value which we evaluated to have a static value of {k}. \"\n                        'Set TORCH_LOGS=\"+export\" for more information.'\n                    )\n        if constraint_violation_error:\n            raise constraint_violation_error\n\n        assert (\n            graph is not None\n        ), \"Failed to produce a graph during tracing as no tensor operations were found.\""
},
{
    "Id": 150,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cc6a51c9f3ee97a06ff9c0b84477e88e33e31137",
    "Violation": "missing",
    "Bug report": "added shape checking to WeightedRandomSampler",
    "Number of deleted lines": 1,
    "Deleted lines": "    def __len__(self) -> int:\n        return len(self.indices)\n\n\nclass WeightedRandomSampler(Sampler[int]):\n    r\"\"\"Samples elements from ``[0,..,len(weights)-1]`` with given probabilities (weights).\n\n    Args:\n        weights (sequence)   : a sequence of weights, not necessary summing up to one\n        num_samples (int): number of samples to draw\n        replacement (bool): if ``True``, samples are drawn with replacement.\n            If not, they are drawn without replacement, which means that when a\n            sample index is drawn for a row, it cannot be drawn again for that row.\n        generator (Generator): Generator used in sampling.\n\n    Example:\n        >>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n        [4, 4, 1, 4, 5]\n        >>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n        [0, 1, 4, 3, 2]\n    \"\"\"\n    weights: Tensor\n    num_samples: int\n    replacement: bool\n\n    def __init__(self, weights: Sequence[float], num_samples: int,\n                 replacement: bool = True, generator=None) -> None:\n        if not isinstance(num_samples, int) or isinstance(num_samples, bool) or \\\n                num_samples <= 0:\n            raise ValueError(\"num_samples should be a positive integer \"\n                             \"value, but got num_samples={}\".format(num_samples))\n        if not isinstance(replacement, bool):\n            raise ValueError(\"replacement should be a boolean value, but got \"\n                             \"replacement={}\".format(replacement))\n        self.weights = torch.as_tensor(weights, dtype=torch.double)\n        self.num_samples = num_samples\n        self.replacement = replacement\n        self.generator = generator\n\n    def __iter__(self) -> Iterator[int]:\n        rand_tensor = torch.multinomial(self.weights, self.num_samples, self.replacement, generator=self.generator)\n        yield from iter(rand_tensor.tolist())\n\n    def __len__(self) -> int:\n        return self.num_samples\n\n\nclass BatchSampler(Sampler[List[int]]):\n    r\"\"\"Wraps another sampler to yield a mini-batch of indices.\n\n    Args:\n        sampler (Sampler or Iterable): Base sampler. Can be any iterable object\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If ``True``, the sampler will drop the last batch if\n            its size would be less than ``batch_size``\n\n    Example:\n        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    \"\"\"\n\n    def __init__(self, sampler: Union[Sampler[int], Iterable[int]], batch_size: int, drop_last: bool) -> None:\n        # Since collections.abc.Iterable does not check for `__getitem__`, which\n        # is one way for an object to be an iterable, we don't do an `isinstance`\n        # check here.\n        if not isinstance(batch_size, int) or isinstance(batch_size, bool) or \\\n                batch_size <= 0:\n            raise ValueError(\"batch_size should be a positive integer value, \"\n                             \"but got batch_size={}\".format(batch_size))"
},
{
    "Id": 151,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8ee59280d78a4fefc4de0da04b287e067c28de0d",
    "Violation": "insufficient",
    "Bug report": "Bug - check config for dynamic",
    "Number of deleted lines": 2,
    "Deleted lines": "\n        tx.output.frame_state[name] = curr_sizes\n\n        # TODO: index export_constraints ahead of time so we don't have to\n        # do a linear scan every time here\n        t_id = id(e)\n        dim2constraint = {}\n        if tx.output.export_constraints:\n            for constraint in tx.output.export_constraints:\n                if constraint.t_id == t_id:\n                    if constraint.dim in dim2constraint:\n                        from torch.fx.experimental.symbolic_shapes import (\n                            StrictMinMaxConstraint,\n                        )\n\n                        dim2constraint[constraint.dim] = StrictMinMaxConstraint(\n                            vr=constraint.constraint_range.vr\n                            & dim2constraint[constraint.dim].vr,\n                            warn_only=False,\n                        )\n                    else:\n                        dim2constraint[constraint.dim] = constraint.constraint_range\n\n        dynamic_dims = None\n        constraint_dims = None\n        if tx.fake_mode.shape_env is not None:\n            dynamic_dims = []\n            constraint_dims = []\n            for i in range(e.dim()):\n                # NB: mark dynamic has precedence over static\n                marked_dynamic = i in getattr(e, \"_dynamo_dynamic_indices\", set())\n                marked_static = i in getattr(e, \"_dynamo_static_indices\", set())\n\n                # NB: both static and dynamic have precedence over\n                automatic_dynamic = curr_sizes is None or curr_sizes[i] is None\n\n                # We will process constraints first, as they will imply that we\n                # have a dynamic dimension\n                # Precedence: export constraints > eager constraints\n                constraint = dim2constraint.get(i)\n                if constraint is None:\n                    if marked_dynamic and not config.allow_ignore_mark_dynamic:\n                        constraint = RelaxedUnspecConstraint(warn_only=False)\n                    elif not marked_static and automatic_dynamic:\n                        constraint = RelaxedUnspecConstraint(warn_only=True)\n                constraint_dims.append(constraint)\n\n                # Now, figure out if the dim is dynamic/duck/static\n                if constraint is not None or marked_dynamic:\n                    # NB: We could assert static_shapes is False here, but it\n                    # seems better to allow the user to override policy in this\n                    # case\n                    dynamic = DimDynamic.DYNAMIC\n                elif static_shapes or config.assume_static_by_default or marked_static:\n                    dynamic = DimDynamic.STATIC\n                else:\n                    dynamic = DimDynamic.DUCK\n                dynamic_dims.append(dynamic)\n\n        fake_e = wrap_fake_exception(\n            lambda: tx.fake_mode.from_tensor(\n                e,\n                ignore_subclass=ignore_subclass,\n                source=source,\n                dynamic_dims=dynamic_dims,\n                constraint_dims=constraint_dims,\n            )\n        )\n        if is_tensor and not (static_shapes and source.is_nn_module()):\n            tx.output.tracked_fakes.append(TrackedFake(fake_e, source, constraint_dims))\n        return fake_e\n    else:"
},
{
    "Id": 152,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e9e125475e94aabfb34ee239fadc760615eef429",
    "Violation": "missing",
    "Bug report": "Add schema check to aten::repeat and fb::fast_gather",
    "Number of deleted lines": 0,
    "Deleted lines": "        at::Tensor& bag_size = p_node->Output(2).toTensor();\n        at::native::make_bag_size_out(\n            bag_size, offsets, indices, mode, include_last_offset, false);\n\n        if (p_node->Output(3).isNone()) {\n          p_node->Output(3) = at::empty(bag_size.sizes(), offsets.options());\n        }\n        at::Tensor& max_indices = p_node->Output(3).toTensor();\n        at::native::make_max_indices_out(\n            max_indices,\n            weight,\n            indices,\n            offsets,\n            bag_size,\n            mode,\n            include_last_offset);\n\n        at::native::_embedding_bag_cpu_impl_out(\n            output,\n            offset2bag,\n            bag_size,\n            max_indices,\n            weight,\n            indices,\n            offsets,\n            mode,\n            per_sample_weights,\n            include_last_offset,\n            padding_idx.value_or(-1));\n      };\n    });\n\n// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\nREGISTER_OPERATOR_FUNCTOR(aten::repeat, aten_repeat, [](Node* n) -> SROperator {\n  return [](ProcessedNode* p_node) {\n    const auto& self = p_node->Input(0).toTensor();\n    const auto repeats = p_node->Input(1).toIntVector();\n\n    if (p_node->Output(0).isNone()) {\n      p_node->Output(0) = create_empty_from(self);\n    }\n    at::Tensor& output = p_node->Output(0).toTensor();\n    at::native::repeat_out(output, self, repeats);\n  };\n});\n\nREGISTER_OPERATOR_FUNCTOR(aten::div, aten_div, [](Node* n) -> SROperator {\n  if (n->inputs().size() != 2 && n->inputs().size() != 3) {\n    return nullptr;\n  }\n  return [](ProcessedNode* p_node) {\n    const auto& in0_t = p_node->Input(0).toTensor();\n    c10::optional<std::string> rounding_mode = c10::nullopt;\n    if (p_node->inputs().size() > 2) {\n      rounding_mode = p_node->Input(2).toOptional<std::string>();\n    }\n\n    if (p_node->Output(0).isNone()) {\n      p_node->Output(0) = create_empty_from(in0_t);\n    }\n    auto& out_t = p_node->Output(0).toTensor();\n    fastResizeToZero(out_t);\n\n    const auto& in1_t = p_node->Input(1).isTensor()\n        ? p_node->Input(1).toTensor()\n        : at::native::wrapped_scalar_tensor(p_node->Input(1).toScalar());\n    at::cpu::div_out(out_t, in0_t, in1_t, rounding_mode);\n  };\n});\n"
},
{
    "Id": 153,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7ea6559658a6f650363f8b96f462bbc047e29124",
    "Violation": "missing",
    "Bug report": "Add size checks to torch.stack. Checks the size of each tensor passed to `torch.stack` before calling `cat` to address #29510. This is done in the `get_stack_input` function as that is a common path. The function now compares the size of each tensor in the TensorList to the size of the first tensor and throws an exception when the sizes are not equal.",
    "Number of deleted lines": 1,
    "Deleted lines": "    num_splits = std::max<int64_t>((dim_size + split_size - 1) / split_size, 1);\n  }\n  std::vector<Tensor> splits(num_splits);\n  int64_t last_split_size = split_size - (split_size * num_splits - dim_size);\n\n  for (int64_t i = 0; i < num_splits; ++i) {\n    auto length = i < num_splits - 1 ? split_size : last_split_size;\n    splits[i] = self.narrow(dim, i * split_size, length);\n  }\n  return splits;\n}\n\nstd::vector<Tensor> split_with_sizes(const Tensor& self, IntArrayRef split_sizes, int64_t dim) {\n  TORCH_CHECK(self.dim() != 0, \"split expects at least a 1-dimensional tensor\");\n  int64_t dim_size = self.size(dim);\n  int64_t num_splits = split_sizes.size();\n  std::vector<Tensor> splits(num_splits);\n  int64_t start_idx = 0;\n  int64_t i;\n\n  for (i = 0; i < num_splits; ++i) {\n    auto length = split_sizes[i];\n    TORCH_CHECK(length >= 0,\n             \"split_with_sizes expects split_sizes have only non-negative \",\n             \"entries, but got split_sizes=\", split_sizes);\n    splits[i] = self.narrow(dim, start_idx, length);\n    start_idx += length;\n  }\n  TORCH_CHECK(start_idx == dim_size,\n           \"split_with_sizes expects split_sizes to sum exactly to \", dim_size,\n           \" (input tensor's size at dimension \", dim, \"), \", \"but got split_sizes=\", split_sizes);\n  return splits;\n}\n\nstatic inline std::vector<Tensor> get_stack_inputs(TensorList tensors, int64_t dim) {\n  std::vector<Tensor> inputs(tensors.size());\n  for (size_t i = 0; i < tensors.size(); ++i) {\n    inputs[i] = tensors[i].unsqueeze(dim);\n  }\n  return inputs;\n}\n\nTensor stack(TensorList tensors, int64_t dim) {\n  TORCH_CHECK(tensors.size() > 0,\n           \"stack expects a non-empty TensorList\");\n  dim = maybe_wrap_dim(dim, tensors[0].dim() + 1);\n  return at::cat(get_stack_inputs(tensors, dim), dim);\n}\n\nTensor& stack_out(Tensor& result, TensorList tensors, int64_t dim) {\n  TORCH_CHECK(tensors.size() > 0,\n           \"stack expects a non-empty TensorList\");\n  dim = maybe_wrap_dim(dim, tensors[0].dim() + 1);\n  return at::cat_out(result, get_stack_inputs(tensors, dim), dim);\n}\n\nstatic inline Tensor & sparse_transpose_(Tensor & self, int64_t dim0, int64_t dim1) {\n  int64_t nsparse_dim = self.sparse_dim();\n  TORCH_CHECK(dim0 < nsparse_dim && dim1 < nsparse_dim,\n           \"sparse transpose: transposed dimensions must be sparse \",\n           \"Got sparse_dim: \", nsparse_dim, \", d0: \", dim0, \", d1: \", dim1);\n\n  if (self._indices().numel() == 0 && self._values().numel() == 0) {\n    auto sizes = self.sizes().vec();\n    std::swap(sizes[dim0], sizes[dim1]);\n\n    at::sparse::get_sparse_impl(self)->raw_resize_(self.sparse_dim(), self.dense_dim(), sizes);\n  } else {\n    auto indices = self._indices();\n    auto row0 = indices.select(0, dim0);\n    auto row1 = indices.select(0, dim1);\n\n    // swap row0 and row1"
},
{
    "Id": 154,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bbb5e106ad6228953df6c7f5c8916b26dc0cb457",
    "Violation": "improper",
    "Bug report": "Improve error checking of CUDALoops. Same change as was applied to CPU loops -- separate out checking of the inputs and outputs.",
    "Number of deleted lines": 1,
    "Deleted lines": "  case 4:\n    vectorized_elementwise_kernel<4, func_t, array_t><<<grid, num_threads, 0, stream>>>(N, f, data);\n    break;\n  case 2:\n    vectorized_elementwise_kernel<2, func_t, array_t><<<grid, num_threads, 0, stream>>>(N, f, data);\n    break;\n  case 1:\n    unrolled_elementwise_kernel<func_t, array_t><<<grid, num_threads, 0, stream>>>(N, f, data, input_calc, output_calc);\n    break;\n  default:\n    TORCH_INTERNAL_ASSERT(false, \"Unexpected vectorization size\");\n  }\n  AT_CUDA_CHECK(cudaGetLastError());\n}\n\ntemplate<typename func_t, typename array_t, typename inp_calc_t, typename out_calc_t>\nstatic inline void launch_unrolled_kernel(int64_t N, const func_t& f, array_t data, inp_calc_t ic, out_calc_t oc) {\n  TORCH_INTERNAL_ASSERT(N > 0 && N <= std::numeric_limits<int32_t>::max());\n  int64_t grid = (N + block_work_size - 1) / block_work_size;\n  auto stream = at::cuda::getCurrentCUDAStream();\n  unrolled_elementwise_kernel<func_t, array_t><<<grid, num_threads, 0, stream>>>(N, f, data, ic, oc);\n  AT_CUDA_CHECK(cudaGetLastError());\n}\n\n} // namespace modern\n\n\ntemplate <typename func_t>\nvoid gpu_kernel_impl(TensorIterator& iter, const func_t& f) {\n  using traits = function_traits<func_t>;\n  using arg0_t = typename traits::result_type;\n  constexpr int ntensors = traits::arity + 1;\n\n  TORCH_INTERNAL_ASSERT(iter.can_use_32bit_indexing());\n  TORCH_INTERNAL_ASSERT(iter.ntensors() == traits::arity + 1);\n\n  at::detail::Array<char*, ntensors> data;\n  for (int i = 0; i < ntensors; i++) {\n    data[i] = (char*)iter.data_ptr(i);\n  }\n\n  int64_t numel = iter.numel();\n\n  bool contiguous = iter.is_contiguous();\n  bool dynamic_casting = needs_dynamic_casting<func_t>::check(iter);\n\n  if (contiguous && !dynamic_casting) {\n    modern::launch_vectorized_kernel(numel, f, data);\n    return;\n  }\n\n  if (!dynamic_casting) {\n    // !contiguous\n    auto input_offset_calculator = make_input_offset_calculator<traits::arity>(iter);\n    auto output_offset_calculator = make_output_offset_calculator(iter);\n    modern::launch_unrolled_kernel(numel, f, data, input_offset_calculator, output_offset_calculator);\n    return;\n  }\n\n  at::detail::Array<ScalarType, ntensors> dtypes;\n  for (int i = 0; i < ntensors; i++) {\n    dtypes[i] = iter.tensor(i).scalar_type();\n  }\n\n  if (iter.is_trivial_1d()) {\n    auto inner_strides = iter.get_inner_strides();\n    at::detail::Array<int, ntensors> strides;\n    for (int i = 0; i < ntensors; i++) {\n      strides[i] = inner_strides[i];\n    }\n"
},
{
    "Id": 155,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d3bf6803b62c79f1dafd1eec49b4bd65d5a27697",
    "Violation": "missing",
    "Bug report": "add sanity check that we do not wrap tracked tensors ",
    "Number of deleted lines": 0,
    "Deleted lines": "        elif unspec and type(value) is int:\n            # unspecializing int by default, but still\n            # specialize for the following conditions\n            if not TracingContext.get().force_unspec_int_unbacked_size_like and (\n                value in self._common_constants()\n                # Assume integers from global variables want to be specialized\n                or not self.source.guard_source().is_local()\n                # Assume that integers that came from NN modules want to be\n                # specialized (as we don't expect users to be changing the\n                # NN modules on the fly)\n                or self.source.guard_source().is_nn_module()\n            ):\n                return ConstantVariable.create(\n                    value=value,\n                    guards=self.make_guards(GuardBuilder.CONSTANT_MATCH),\n                )\n            else:\n                return self.wrap_unspecialized_primitive(value)\n        else:\n            return ConstantVariable.create(\n                value=value,\n                guards=self.make_guards(GuardBuilder.CONSTANT_MATCH),\n            )\n\n    def assert_not_wrapped_by_this_graph(self, value: torch.Tensor):\n        if is_fake(value) and maybe_get_fake_mode(value) is self.tx.fake_mode:\n            raise InternalTorchDynamoError(\n                \"Cannot wrap a Tensor that has already been\",\n                \"wrapped by this instance of Dynamo\",\n            )\n\n    def wrap_tensor(self, value: torch.Tensor):\n        source = self.get_source()\n\n        if (\n            source.guard_source().is_nn_module()\n            or get_static_address_type(value) is not None\n        ) and not source.guard_source().is_fsdp_module():\n            self.assert_not_wrapped_by_this_graph(value)\n            return self.tx.output.register_attr_or_module(\n                value,\n                self.name,\n                source=source,\n                # Guards are done inside register_attr_or_module\n                # guards=self.make_guards(GuardBuilder.TENSOR_MATCH),\n            )\n\n        if is_constant_source(source):\n            self.assert_not_wrapped_by_this_graph(value)\n            return self.tx.output.register_attr_or_module(\n                value,\n                re.sub(r\"[^a-zA-Z0-9]+\", \"_\", self.name),\n                source=source,\n                # Guards are added inside register_attr_or_module\n            )\n\n        if type(value) in config.traceable_tensor_subclasses:\n            # Ordinarily, we would fakeify a tensor so that it can get dynamic\n            # shapes and be computed on without triggering actual operations.\n            # However, how can we fakeify a tensor subclass?  Ordinary\n            # inheritance (nor multiple inheritance) won't work work.\n            #\n            # Instead, our plan is to *manually simulate* the tensor subclass\n            # inheriting from a fake tensor with dynamo.  This means our\n            # data representation for a tensor subclass will be a fake tensor\n            # + tensor subclass type + any extra data the subclass may have\n            # been storing on the tensor.  Because all Python accesses are\n            # mediated through TensorWithTFOverrideVariable, we can ensure\n            # that we dispatch differently, e.g., according to\n            # __torch_function__"
},
{
    "Id": 156,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/794e3971ab90611b4a63166589368a737843c8bc",
    "Violation": "missing",
    "Bug report": "Add size check before calling stack_.at(dict_pos) in unpickler.cpp",
    "Number of deleted lines": 0,
    "Deleted lines": "\n      auto options = at::CPU(type).options();\n      if (use_storage_device_) {\n        options = options.device(storage.device());\n        device = storage.device();\n      }\n\n      at::Tensor tensor;\n      if (options.backend() == c10::Backend::QuantizedCPU) {\n        tensor = at::_empty_affine_quantized({}, options, 0, 0)\n                     .set_(storage, 0, {}, {});\n      } else {\n        tensor = at::empty({0}, options).set_(storage);\n      }\n\n      if (device.is_cuda() || device.is_xpu() || device.is_meta() ||\n          device.is_hpu() || device.is_privateuseone()) {\n        tensor = tensor.to(device, tensor.scalar_type());\n      } else if (device.type() != DeviceType::CPU) {\n        AT_ERROR(\n            \"supported devices include CPU, CUDA, HPU and \",\n            c10::get_privateuse1_backend(),\n            \" however got \",\n            DeviceTypeName(device.type(), false));\n      }\n      stack_.emplace_back(std::move(tensor));\n    } break;\n    case PickleOpCode::SETITEM: {\n      // At this OpCode, stack looks like\n      // | Stack Bottom |\n      // | ......       |\n      // | Dict         | -> (stack_size - 3)\n      // | Key          | -> (stack_size - 2)\n      // | Value        | -> (stack_size - 1)\n      auto stack_size = stack_.size();\n      auto dict_pos = stack_size - 3;\n      auto key_pos = stack_size - 2;\n      auto val_pos = stack_size - 1;\n      auto dict = stack_.at(dict_pos).toGenericDict();\n      dict.insert_or_assign(stack_.at(key_pos), stack_.at(val_pos));\n      stack_.erase(stack_.begin() + (key_pos), stack_.end());\n    } break;\n    default: {\n      AT_ERROR(\n          \"Unknown opcode for unpickling at \",\n          reinterpret_cast<void*>(opcode),\n          \": \",\n          int(static_cast<uint8_t>(opcode)));\n    } break;\n  }\n  return opcode;\n}\n\nvoid Unpickler::readGlobal(\n    const std::string& module_name,\n    const std::string& class_name) {\n  if (this->skip_next_read_global) {\n    // See [NOTE] skip_next_read_global\n    this->skip_next_read_global--;\n    if (this->skip_next_read_global == 1) {\n      // Pass through to the correct handler\n    } else if (this->skip_next_read_global == 0) {\n      // Corresponds to the type of `Tensor` being unpickled\n      if (module_name != \"torch\" || class_name != \"Tensor\") {\n        TORCH_WARN(\n            \"Trying to load a Subclassed Tensor, it will be converted to at::Tensor in C++\");\n      }\n      stack_.emplace_back(int64_t(globals_.size() - 1));\n      return;\n    } else {\n      TORCH_CHECK(false, \"INVALID VALUES\")\n    }\n  }\n  // TODO [unpickler refactor] __main__ isn't used by the pickler anymore, this"
},
{
    "Id": 157,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7684044b713761abd4f51225dc5d83ce5869562a",
    "Violation": "missing",
    "Bug report": "Add size check before calling .back() in rpc/script_call.cpp",
    "Number of deleted lines": 0,
    "Deleted lines": "  }\n\n  if (hasOp()) {\n    TORCH_CHECK(\n        !hasQualifiedName(),\n        \"It is builtin operator call, qualifiedName_ should not be set.\");\n    // TODO: replace this with a real overload_name when FunctionSchema supports\n    // that.\n    ivalues.emplace_back(toString((*op_)->schema()));\n    // insert qualified name\n    auto opName = (*op_)->schema().name();\n    TORCH_CHECK(\n        opName.find(\"::\") == opName.rfind(\"::\") &&\n            opName.rfind(ATEN_PREFIX_) == 0,\n        \"Unexpected operator name \",\n        opName);\n    // aten::add -> torch.ops.aten.add\n    opName.replace(0, ATEN_PREFIX_.length(), BUILTIN_OP_NAMESPACE_);\n    ivalues.emplace_back(std::move(opName));\n  } else if (hasQualifiedName()) {\n    ivalues.emplace_back(isAsyncExecution());\n    TORCH_CHECK(\n        !hasOp(),\n        \"It is TorchScript function call, operator should not be set.\");\n    ivalues.emplace_back((*qualifiedName_).qualifiedName());\n  } else {\n    TORCH_INTERNAL_ASSERT(\n        false,\n        \"Either builtin operator or TorchScript function name should be set.\");\n  }\n}\n\nstd::unique_ptr<ScriptCall> ScriptCall::fromIValues(\n    std::vector<at::IValue>& ivalues) {\n  // Last element in the vector is always qualifiedName for both\n  // builitin operator and TorchScript function\n  // If the qualifiedName is not a builtin operator name, then treat it\n  // as TorchScript function name\n  const std::string& qualifiedName = ivalues.back().toStringRef();\n\n  if (qualifiedName.rfind(BUILTIN_OP_NAMESPACE_) == 0) {\n    ivalues.pop_back();\n    const std::string& str_schema = ivalues.back().toStringRef();\n    auto op = matchOperator(str_schema);\n\n    ivalues.pop_back();\n    // remove str_schema from ivalues\n    return std::make_unique<ScriptCall>(op, std::move(ivalues));\n  } else {\n    ivalues.pop_back();\n    bool isAsyncExecution = ivalues.back().toBool();\n    ivalues.pop_back();\n    return std::make_unique<ScriptCall>(\n        c10::QualifiedName(qualifiedName),\n        std::move(ivalues),\n        isAsyncExecution);\n  }\n}\n\nc10::intrusive_ptr<Message> ScriptCall::toMessageImpl() && {\n  std::vector<IValue> ivalues;\n  toIValues(ivalues);\n\n  std::vector<torch::Tensor> tensor_table;\n  auto payload = jit::pickle(\n      c10::ivalue::Tuple::create(std::move(ivalues)), &tensor_table);\n\n  return c10::make_intrusive<Message>(\n      std::move(payload), std::move(tensor_table), MessageType::SCRIPT_CALL);\n}"
},
{
    "Id": 158,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fb25fd6f865ed0532caf710ca130b6cc23a772a8",
    "Violation": "missing",
    "Bug report": "Replaced neg dim normalization with assert in helper. I think we can still leave the check for negative shard dimension in `compute_local_shape_and_global_offset` and replace the normalization logic with an assert. This should provide us a stack trace to see which user-facing API did not normalize the dim as expected.",
    "Number of deleted lines": 2,
    "Deleted lines": "    tensor: torch.Tensor, mesh: DeviceMesh, placements: Sequence[Placement]\n) -> Tuple[List[int], List[int]]:\n    \"\"\"\n    Compute the global size and stride of a DTensor from the given local tensor.\n    The local size is multiplited by `world_size` per Sharding dim.\n    The local stride is multiplited by `world_size` per Sharding dim, as long as the\n    dimension is outside sharding dim.\n\n    For example, if we have a local tensor with size (4, 8, 2) and stride (16, 1, 8).\n    If the DTensor placements are [Shard(2)] and world_size is 2;\n    then the global size is (4, 8, 4) and stride is (16 * 2, 1, 8).\n\n    Args:\n        tensor (:class:`torch.Tensor`):\n            Local tensor which DTensor will be constructed from.\n        mesh (:class:`DeviceMesh`):\n            Object which describes the mesh topology\n            of devices for the DTensor.\n        placements (Sequence[:class:`Placement`]]):\n            The attribute of the DTensor that describes its layout\n            on the mesh topology.\n\n    Return:\n        tensor_shape: A List of int which specifies the size of DTensor which build\n            on top of the local tensor.\n        tensor_stride: A List of int which specifies the stride of DTensor.\n    \"\"\"\n    tensor_shape = list(tensor.size())\n    tensor_stride = list(tensor.stride())\n    for idx, placement in enumerate(placements):\n        mesh_dim_size = mesh.size(idx)\n        if placement.is_shard():\n            shard_placement = cast(Shard, placement)\n            if shard_placement.dim < 0:\n                # normalize shard dim to be positive\n                shard_placement.dim += len(tensor_shape)\n            shard_dim = shard_placement.dim\n\n            assert (\n                shard_dim < tensor.ndim\n            ), f\"Sharding dim {shard_dim} greater than tensor ndim {tensor.ndim} for placement number {idx}.\"\n\n            local_dim_size = tensor_shape[shard_dim]\n            tensor_shape[shard_dim] = local_dim_size * mesh_dim_size\n\n            # recover tensor stride by modifying the stride that larger than\n            # the current stride on the shard_dim\n            for i in range(len(tensor_stride)):\n                if i != shard_dim and tensor_stride[i] >= tensor_stride[shard_dim]:\n                    # rescale the stride by the shard size\n                    tensor_stride[i] = tensor_stride[i] * mesh_dim_size\n        elif not isinstance(placement, (Replicate, _Partial)):\n            raise RuntimeError(f\"placement type {type(placement)} not supported!\")\n    return tensor_shape, tensor_stride\n"
},
{
    "Id": 159,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/55c19a3c6d38a49fe34e008c4c566445c43810f0",
    "Violation": "missing",
    "Bug report": "Increase multiplier to 3 for Inductor AMP benchmark correctness check. we find some of the models have failed the benchmark's correctness check. However, the end-to-end model's accuracy. when comparing AMP with FP32 is within a difference of less than 0.1%. Thus, it's possible that the correctness check failures for these models are false alarms. We use multiplier of 3 instead of 2 in this PR to avoid these false alarms.",
    "Number of deleted lines": 1,
    "Deleted lines": "                )\n                if not r:\n                    log_error(\"Accuracy failed: uint8 tensor did not match\")\n                return r\n\n        if cos_similarity:\n            ref = ref.flatten().to(torch.float32)\n            res = res.flatten().to(torch.float32)\n            if torch.allclose(ref, res, atol=tol, rtol=tol, equal_nan=True):\n                # early exit that handles zero/nan better\n                # cosine_similarity(zeros(10), zeros(10), dim=0) is 0\n                return True\n            score = torch.nn.functional.cosine_similarity(ref, res, dim=0, eps=1e-6)\n            if score < 0.99:\n                log.warning(\"Similarity score=%s\", score.cpu().detach().item())\n            return score >= 0.99\n        else:\n            if not exact_dtype:\n                ref = ref.to(res.dtype)\n\n            # First try usual allclose\n            if torch.allclose(ref, res, atol=tol, rtol=tol, equal_nan=equal_nan):\n                return True\n\n            # Check error from fp64 version\n            if fp64_ref.dtype == torch.float64:\n                ref_error = rmse(fp64_ref, ref).item()\n                # ref unable to produce this with stable numerics in this precision, ignore\n                if math.isnan(ref_error):\n                    log.warning(\n                        \"Found nan in reference. Consider running in higher precision.\"\n                    )\n\n                res_error = rmse(fp64_ref, res).item()\n                multiplier = 2.0\n\n                if (\n                    fp64_ref.numel() < 1000\n                    or (ref.ndim == 4 and ref.shape[-1] == ref.shape[-2] == 1)\n                    # large tol means a benchmark has been specified as REQUIRE_HIGHER_TOLERANCE\n                    or tol >= 2 * 1e-2\n                ):\n                    # In the presence of noise, noise might dominate our error\n                    # metric for smaller tensors.\n                    # Similary, for 1x1 kernels, there seems to be high noise with amp.\n                    multiplier = 3.0\n\n                passes_test = res_error <= (multiplier * ref_error + tol / 10.0)\n                if not passes_test:\n                    log_error(\n                        \"RMSE (res-fp64): %.5f, (ref-fp64): %.5f and shape=%s\",\n                        res_error,\n                        ref_error,\n                        res.size(),\n                    )\n                    # import pdb; pdb.set_trace()\n                return passes_test\n\n            if ignore_non_fp:\n                return True\n\n            log_error(\"Accuracy failed: allclose not within tol=%s\", tol)\n            return False\n    elif isinstance(ref, (str, int, type(None), bool, torch.device)):\n        if ignore_non_fp:\n            return True\n        r = ref == res\n        if not r:\n            log_error(\"Accuracy failed (%s): %s != %s\", type(ref), ref, res)\n        return r\n    elif is_numpy_int_type(ref) or is_numpy_float_type(ref):"
},
{
    "Id": 160,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6e78592cbb81138ce13ad65a5f549d65b191526c",
    "Violation": "missing",
    "Bug report": "Added type checking for ExportedProgram. Added type checking for ExportedProgram in save function. ",
    "Number of deleted lines": 0,
    "Deleted lines": "         implement write and flush) or a string containing a file name.\n\n        extra_files (Optional[Dict[str, Any]]): Map from filename to contents\n         which will be stored as part of f.\n\n        opset_version (Optional[Dict[str, int]]): A map of opset names\n         to the version of this opset\n\n\n    Example::\n\n        import torch\n        import io\n\n        class MyModule(torch.nn.Module):\n            def forward(self, x):\n                return x + 10\n\n        ep = torch.export.export(MyModule(), (torch.randn(5),))\n\n        # Save to file\n        torch.export.save(ep, 'exported_program.pt2')\n\n        # Save to io.BytesIO buffer\n        buffer = io.BytesIO()\n        torch.export.save(ep, buffer)\n\n        # Save with extra files\n        extra_files = {'foo.txt': b'bar'.decode('utf-8')}\n        torch.export.save(ep, 'exported_program.pt2', extra_files=extra_files)\n\n    \"\"\"\n    from torch._export import save\n\n    save(ep, f, extra_files=extra_files, opset_version=opset_version)\n\n\ndef load(\n    f: Union[str, os.PathLike, io.BytesIO],\n    *,\n    extra_files: Optional[Dict[str, Any]] = None,\n    expected_opset_version: Optional[Dict[str, int]] = None,\n) -> ExportedProgram:\n    \"\"\"\n\n    .. warning::\n        Under active development, saved files may not be usable in newer versions\n        of PyTorch.\n\n    Loads an :class:`ExportedProgram` previously saved with\n    :func:`torch.export.save <torch.export.save>`.\n\n    Args:\n        ep (ExportedProgram): The exported program to save.\n\n        f (Union[str, os.PathLike, io.BytesIO): A file-like object (has to\n         implement write and flush) or a string containing a file name.\n\n        extra_files (Optional[Dict[str, Any]]): The extra filenames given in\n         this map would be loaded and their content would be stored in the\n         provided map.\n\n        expected_opset_version (Optional[Dict[str, int]]): A map of opset names\n         to expected opset versions\n\n    Returns:\n        An :class:`ExportedProgram` object\n\n    Example::\n"
},
{
    "Id": 161,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/828a6a3b3943a0a0701ecacacd2bcc34fc03fe03",
    "Violation": "missing",
    "Bug report": "Use proper isnan check ",
    "Number of deleted lines": 1,
    "Deleted lines": "\n/* I cut and pasted (slightly adapted) the quicksort code from\n   Sedgewick's 1978 \"Implementing Quicksort Programs\" article\n   http://www.csie.ntu.edu.tw/~b93076/p847-sedgewick.pdf\n\n   It is the state of the art existing implementation. The macros\n   are here to make as close a match as possible to the pseudocode of\n   Program 2 p.851\n\n   Note that other partition schemes exist, and are typically presented\n   in textbook, but those are less efficient. See e.g.\n   http://cs.stackexchange.com/questions/11458/quicksort-partitioning-hoare-vs-lomuto\n\n   Julien, November 12th 2013\n*/\n#define MAX_LEVELS  300\n#define M_SMALL 10 /* Limit for small subfiles */\n\n#define ARR(III) arr[(III)*stride]\n#define IDX(III) idx[(III)*stride]\n\n#define LONG_SWAP(AAA, BBB) swap = AAA; AAA = BBB; BBB = swap\n#define REAL_SWAP(AAA, BBB) rswap = AAA; AAA = BBB; BBB = rswap\n\n#define ARR_SWAP(III, JJJ) \\\n  REAL_SWAP(ARR(III), ARR(JJJ));\n\n#define BOTH_SWAP(III, JJJ) \\\n  REAL_SWAP(ARR(III), ARR(JJJ)); \\\n  LONG_SWAP(IDX(III), IDX(JJJ))\n\n/* Emulate NumPy behavior of putting NaNs\n * at the end of an ascending list. */\n#define GT_OR_NAN(x, y) \\\n  ((x != x && y == y) || (x > y))\n\nstatic void THTensor_(quicksortascend)(scalar_t *arr, int64_t *idx, int64_t elements, int64_t stride)\n{\n  int64_t beg[MAX_LEVELS], end[MAX_LEVELS], i, j, L, R, P, swap, pid, stack = 0, sz_right, sz_left;\n  scalar_t rswap, piv;\n  unsigned char done = 0;\n\n  /* beg[0]=0; end[0]=elements; */\n  stack = 0;\n  L = 0; R = elements-1;\n  done = elements-1 <= M_SMALL;\n\n  while(!done) {\n      /* Use median of three for pivot choice */\n    P=(L+R)>>1;\n    BOTH_SWAP(P, L+1);\n    if (GT_OR_NAN(ARR(L+1), ARR(R))) { BOTH_SWAP(L+1, R); }\n    if (GT_OR_NAN(ARR(L), ARR(R))) { BOTH_SWAP(L, R); }\n    if (GT_OR_NAN(ARR(L+1), ARR(L))) { BOTH_SWAP(L+1, L); }\n\n    i = L+1; j = R; piv = ARR(L); pid = IDX(L);\n\n    do {\n      do { i = i+1; } while(GT_OR_NAN(piv, ARR(i)));\n      do { j = j-1; } while(GT_OR_NAN(ARR(j), piv));\n      if (j < i)\n          break;\n      BOTH_SWAP(i, j);\n    } while(1);\n    BOTH_SWAP(L, j);\n    /* Left subfile is (L, j-1) */\n    /* Right subfile is (i, R) */\n    sz_left = j-L;\n    sz_right = R-i+1;\n    if (sz_left <= M_SMALL && sz_right <= M_SMALL) {\n      /* both subfiles are small */"
},
{
    "Id": 162,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/47c531b6e80e36282dbaec60d239ae1b9f816f43",
    "Violation": "missing",
    "Bug report": "Compare object identity first in ClassType::operator==. This check is much cheaper than anything involving actually inspecting object fields (i.e., the cost is low), and if it succeeds we can skip the expensive (e.g., it involves locking a weak_ptr and then destroying the resulting shared_ptr)  function body. It almost entirely eliminates time spent in this function during model loading according to perf.",
    "Number of deleted lines": 0,
    "Deleted lines": "\n  private:\n  AttributeKind kind_;\n  TypePtr attributeType_;\n  std::string attributeName_;\n};\n\n/**\n * User Defined Types\n */\n\nstruct ClassType;\nusing ClassTypePtr = std::shared_ptr<ClassType>;\nusing ::torch::jit::CompilationUnit;\n\n// This represents a class in TorchScript.\nstruct TORCH_API ClassType : public NamedType {\n  // This represents an attribute of a class; a name associated with an attribute, and a\n  // getter and (optional) setter for that attribute.\n  struct Property {\n    std::string name;\n    torch::jit::Function* getter;\n    torch::jit::Function* setter;\n  };\n\n  // Create a class type with name `name` and its methods stored in `cu`.\n  static ClassTypePtr create(\n      c10::optional<QualifiedName> qualifiedName,\n      std::weak_ptr<CompilationUnit> cu,\n      bool is_module = false,\n      std::string doc_string = \"\",\n      std::vector<std::string> unresolved_class_attributes = {});\n\n  bool operator==(const Type& rhs) const override {\n    if (auto user_rhs = rhs.castRaw<ClassType>()) {\n      const auto& lhs_name = name().value();\n      const auto& rhs_name = user_rhs->name().value();\n\n      return lhs_name == rhs_name &&\n          this->compilation_unit() == user_rhs->compilation_unit();\n    }\n    return false;\n  }\n\n  std::string str() const override {\n     return annotation_str();\n  }\n\n  std::string repr_str() const override {\n    std::stringstream ss;\n    ss << str()\n       << \" (of Python compilation unit at: \" << compilation_unit().get() << \")\";\n    return ss.str();\n  }\n\n  const std::vector<torch::jit::Function*>& methods() const;\n\n  TypePtr findAttribute(const std::string& name) const {\n    size_t pos = 0;\n    for (const auto& attr : attributes_) {\n      if (name == attr.getName()) {\n        break;\n      }\n      ++pos;\n    }\n\n    if (pos >= attributes_.size()) {\n      return nullptr;\n    }\n    return attributes_[pos].getType();"
},
{
    "Id": 163,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3611d26a25bd889627403a808ea667ac99c09904",
    "Violation": "missing",
    "Bug report": "Optimize FunctionSchema::checkArg for the Tensor case. The Tensor case is one of the most common and the existing check can be made faster. This results in a ~21% improvement on DeepAndWide model and would improve other models as well.",
    "Number of deleted lines": 0,
    "Deleted lines": "      return false;\n    }\n  }\n\n  // Make sure that all the old arguments have their corresponding backward\n  // compatible arguments in this schema.\n  for (size_t i = 0; i < old.arguments().size(); ++i) {\n    if (!arguments().at(i).isBackwardCompatibleWith(\n          old.arguments().at(i), why_not)) {\n      return false;\n    }\n  }\n\n  // Validate that all new arguments provided a default value.\n  for (size_t i = old.arguments().size(); i < arguments().size(); ++i) {\n    if (!arguments().at(i).default_value()) {\n      if (why_not) {\n        *why_not\n            << \"Function schema not backward compatible since the new argument '\"\n            << arguments().at(i).name() << \"' of type \"\n            << arguments().at(i).type()->str()\n            << \" did not provide a default value.\";\n      }\n      return false;\n    }\n  }\n\n  return true;\n}\n\ninline void FunctionSchema::checkArg(\n    const IValue& value,\n    const Argument& argument,\n    optional<size_t> pos) const {\n  if (!value.type()->isSubtypeOf(argument.type())) {\n    TORCH_CHECK(\n        false,\n        formatTypeMismatchMsg(\n            argument, value.type()->repr_str(), pos));\n  }\n}\n\ninline std::string FunctionSchema::findErrorInKwargs(const std::vector<std::string>& kwargs) const {\n  // First check if any of the kwargs are unknown, i.e. don't match the name of\n  // any argument in the schema.\n  for (const auto& kwarg : kwargs) {\n    if (!std::count_if(\n            arguments().begin(),\n            arguments().end(),\n            [&kwarg](const Argument& argument) {\n              return argument.name() == kwarg;\n            })) {\n      return c10::str(\n          \"Unknown keyword argument '\",\n          kwarg,\n          \"' for operator '\",\n          name(),\n          \"'. Schema: \",\n          *this);\n    }\n  }\n  // If there are unconsumed kwargs but none of them were unknown, the first\n  // positional argument present in the kwargs is duplicated.\n  for (const auto& argument : arguments()) {\n    if (std::find(kwargs.begin(), kwargs.end(), argument.name()) != kwargs.end()) {\n      AT_ASSERT(!argument.default_value());\n      return c10::str(\n          \"Argument '\",\n          argument.name(),\n          \"' specified both as positional and \","
},
{
    "Id": 164,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/324dc1623e2f91892038fb1b151450a7c6529dd9",
    "Violation": "missing",
    "Bug report": "add dtype checking for gather and scatter. in the `cpu_scatter_gather_base_kernel`, it interpret a pointer as `int64_t` regardless the actual dtype. add a index dtype checking will avoid the nasty index out of bound error. As using `int64_t` is convention in ATen code (a.k.a, a limitation), no further fix is needed at the moment.",
    "Number of deleted lines": 0,
    "Deleted lines": "      auto result_data_ptr = result.data_ptr<scalar_t>();\n      auto self_numel = self.numel();\n      for (auto i = 0; i < numel; i++) {\n        auto self_i = index_data[i];\n        TORCH_CHECK_INDEX((self_i >= 0) && (self_i < self_numel), \"index out of range in self\");\n        scalar_t *self_ip = self_data_ptr + self_i * self_stride;\n        *(result_data_ptr + i * result_stride) = *self_ip;\n      }\n    });\n  }\n\n  return result;\n}\n\nTensor index_select_cpu_(const Tensor & self, int64_t dim, const Tensor & index) {\n  Tensor result = at::empty({0}, self.options());\n  return index_select_out_cpu_(result, self, dim, index);\n}\n\nTensor & index_fill_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {\n  TORCH_CHECK(source.dim() == 0, \"index_fill_ only supports a 0-dimensional value tensor, but got tensor \"\n      \"with \", source.dim(), \" dimension(s).\");\n  return self.index_fill_(dim, index, source.item());\n}\n\nTensor index_fill(const Tensor & self, int64_t dim, const Tensor & index, Scalar source) {\n  return self.clone(at::MemoryFormat::Preserve).index_fill_(dim, index, source);\n}\n\nTensor index_fill(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {\n  return self.clone(at::MemoryFormat::Preserve).index_fill_(dim, index, source);\n}\n\nTensor & gather_out_cpu(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad) {\n  result.resize_(index.sizes());\n  gather_stub(result.device().type(), result, self, dim, index);\n  return result;\n}\n\nTensor gather_cpu(const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad) {\n  Tensor result = at::empty({0}, self.options());\n  return gather_out_cpu(result, self, dim, index, sparse_grad);\n}\n\nTensor & scatter_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) {\n  scatter_stub(self.device().type(), self, dim, index, src);\n  return self;\n}\n\nTensor & scatter_fill_(Tensor & self, int64_t dim, const Tensor & index, Scalar src) {\n  scatter_fill_stub(self.device().type(), self, dim, index, src);\n  return self;\n}\n\nTensor scatter(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {\n  return self.clone(at::MemoryFormat::Preserve).scatter_(dim, index, source);\n}\n\nTensor scatter(const Tensor & self, int64_t dim, const Tensor & index, Scalar source) {\n  return self.clone(at::MemoryFormat::Preserve).scatter_(dim, index, source);\n}\n\nTensor & scatter_add_cpu_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) {\n  scatter_add_stub(self.device().type(), self, dim, index, src);\n  return self;\n}\n\nTensor scatter_add(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {\n  return self.clone(at::MemoryFormat::Preserve).scatter_add_(dim, index, source);\n}\n\nTensor masked_scatter(const Tensor & self, const Tensor & mask, const Tensor & source) {\n  Tensor _mask, _self;\n  std::tie(_mask, _self) = expand_outplace(mask, self);\n  return _self.clone(at::MemoryFormat::Contiguous).masked_scatter_(_mask, source);\n}\n\nstatic Tensor & masked_fill_impl_cpu(Tensor & self, const Tensor & mask, Scalar value) {\n  NoNamesGuard guard;\n  if (mask.dtype() == ScalarType::Byte) {\n    TORCH_WARN(\"masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,\" \\\n            \"please use a mask with dtype torch.bool instead.\");\n  }\n\n  auto iter = TensorIterator();\n  iter.dont_compute_common_dtype();\n  iter.dont_resize_outputs();\n  iter.add_output(self);\n  iter.add_input(mask);\n  iter.build();\n\n  masked_fill_stub(iter.device_type(), iter, value);\n  return self;\n}\n\nTensor & masked_fill__cpu(Tensor& self, const Tensor & mask, Scalar value) {\n  auto maybe_outnames = namedinference::broadcast_to_outnames(self, mask, \"masked_fill_\");\n\n  masked_fill_impl_cpu(self, mask, value);\n  namedinference::propagate_names_if_nonempty(self, maybe_outnames);\n  return self;\n}\n\nTensor & masked_fill__cpu(Tensor& self, const Tensor & mask, const Tensor & value) {"
},
{
    "Id": 165,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5e50993be72bec4ad939993328dd02691ef7777d",
    "Violation": "missing",
    "Bug report": "Better type checking for pack_padded_sequence symbolic",
    "Number of deleted lines": 0,
    "Deleted lines": "    ``input[:,0]`` should be the longest sequence, and ``input[:,B-1]`` the\n    shortest one.\n\n    Note:\n        This function accepts any input that has at least two dimensions. You\n        can apply it to pack the labels, and use the output of the RNN with\n        them to compute the loss directly. A Tensor can be retrieved from\n        a :class:`PackedSequence` object by accessing its ``.data`` attribute.\n\n    Arguments:\n        input (Tensor): padded batch of variable length sequences.\n        lengths (Tensor): list of sequences lengths of each batch element.\n        batch_first (bool, optional): if ``True``, the input is expected in ``B x T x *``\n            format.\n\n    Returns:\n        a :class:`PackedSequence` object\n    \"\"\"\n    if isinstance(lengths, list):\n        lengths = torch.LongTensor(lengths)\n\n    data, batch_sizes = PackPadded.apply(input, lengths, batch_first)\n\n    return PackedSequence(data, batch_sizes)\n\n\ndef _symbolic_pack_padded_sequence(g, input, lengths, batch_first=False, padding_value=0.0):\n    # There currently is no PackPadded operator in ONNX. We rely on an\n    # optimization pass to remove this later. It is an error if all\n    # PackPadded operators cannot be optimized out.\n\n    def _onnx_symbolic_pack_padded_sequence(g, input, lengths):\n        if batch_first:\n            input = g.op('Transpose', input, perm_i=[1, 0, 2])\n        return g.op(\"prim::PackPadded\", input, lengths, outputs=2)\n\n    def pack_padded_sequence_trace_wrapper(input, lengths):\n        return pack_padded_sequence(input, lengths, batch_first=batch_first)\n\n    outputs = g.wrapPyFuncWithSymbolic(\n        pack_padded_sequence_trace_wrapper, [input, lengths], 2,\n        _onnx_symbolic_pack_padded_sequence)\n    return tuple(o for o in outputs)\n\n\npack_padded_sequence = torch.onnx.symbolic_override_first_arg_based(\n    _symbolic_pack_padded_sequence)(pack_padded_sequence)\n\n\ndef pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None):\n    r\"\"\"Pads a packed batch of variable length sequences.\n\n    It is an inverse operation to :func:`pack_padded_sequence`.\n\n    The returned Tensor's data will be of size ``T x B x *``, where `T` is the length\n    of the longest sequence and `B` is the batch size. If ``batch_first`` is True,\n    the data will be transposed into ``B x T x *`` format.\n\n    Batch elements will be ordered decreasingly by their length.\n\n    .. note::\n        :attr:`total_length` is useful to implement the\n        ``pack sequence -> recurrent network -> unpack sequence`` pattern in a\n        :class:`~torch.nn.Module` wrapped in :class:`~torch.nn.DataParallel`.\n        See :ref:`this FAQ section <pack-rnn-unpack-with-data-parallelism>` for\n        details.\n\n    Arguments:\n        sequence (PackedSequence): batch to pad\n        batch_first (bool, optional): if ``True``, the output will be in ``B x T x *``"
},
{
    "Id": 166,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/df475aa1dc4310abc273cf26b14b6ac1cdb7dfa4",
    "Violation": "missing",
    "Bug report": "Update Vulkan runner in benchmark binary to handle non-tensor inputs. Some models may take in a list of tensors as inputs, thus the bundled inputs will contain `IValues` that are of the type `c10::List`. For Vulkan models, every tensor in the `IValue` list has to be converted to a vulkan tensor first, and this case is not currently handled by the Vulkan model wrapper in the benchmark binary. This diff introduces `IValue` type checking to the input processor of the Vulkan model wrapper, and adds support for Tensor and List types.",
    "Number of deleted lines": 1,
    "Deleted lines": "            at::TensorOptions(input_type).\n            memory_format(input_memory_format)));\n  }\n\n  if (FLAGS_pytext_len > 0) {\n    auto stensor = FLAGS_pytext_len * at::ones({1}, torch::kI64);\n    inputs.push_back(stensor);\n  }\n\n  return inputs;\n}\n\ntemplate<class T>\nclass Runner {\n public:\n  virtual ~Runner() = default;\n  virtual c10::IValue run(\n      T& module,\n      const std::vector<c10::IValue>& inputs) {\n    return module.forward(inputs);\n  }\n};\n\ntemplate<class T>\nclass vkRunner final : public Runner<T> {\n public:\n  virtual ~vkRunner() = default;\n  virtual c10::IValue run(\n      T& module,\n      const std::vector<c10::IValue>& inputs) override {\n    // Upload the input tensor(s) to GPU memory.\n    inputs_.clear();\n    inputs_.reserve(inputs.size());\n    for (const auto& input : inputs) {\n      inputs_.emplace_back(input.toTensor().vulkan());\n    }\n\n    // Run, and download the output tensor to system memory.\n    return module.forward(inputs_).toTensor().cpu();\n  }\n\n private:\n  std::vector<c10::IValue> inputs_;\n};\n\n} // namespace\n\nint main(int argc, char** argv) {\n  c10::SetUsageMessage(\n    \"Run speed benchmark for pytorch model.\\n\"\n    \"Example usage:\\n\"\n    \"./speed_benchmark_torch\"\n    \" --model=<model_file>\"\n    \" --use_bundled_input=0\"\n    \" --warmup=5\"\n    \" --iter=20\");\n  if (!c10::ParseCommandLineFlags(&argc, &argv)) {\n    std::cerr << \"Failed to parse command line flags!\" << std::endl;\n    return 1;\n  }\n\n  std::vector<c10::IValue> inputs = create_inputs();\n\n  c10::InferenceMode mode;\n#if BUILD_LITE_INTERPRETER\n  auto module = torch::jit::_load_for_mobile(FLAGS_model);\n#else\n  torch::jit::GraphOptimizerEnabledGuard no_optimizer_guard(false);\n  auto module = torch::jit::load(FLAGS_model);\n#endif\n"
},
{
    "Id": 167,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e31038d574712d383fdc4c2f1bb63fc82f256ed0",
    "Violation": "missing",
    "Bug report": "Check results dtype in index_out. This logic exists for index_put and index_add, but for some reason not for `index.out` Skip testing, as this function is not technically exposed on the Python level.",
    "Number of deleted lines": 0,
    "Deleted lines": "  }\n  iter.build(config);\n}\n\nstatic void check_indices_on_cpu_or_selfdevice(\n    const Tensor& self,\n    const at::MaterializedIOptTensorListRef& indices) {\n  auto dev = self.device();\n  bool indices_on_cpu_or_dev = std::all_of(\n      indices.begin(), indices.end(), [=](const at::OptionalTensorRef& opt) {\n        return opt.has_value() ? (opt->is_cpu() || opt->device() == dev) : true;\n      });\n  TORCH_CHECK(\n      indices_on_cpu_or_dev,\n      \"indices should be either on \", kCPU,\n      \" or on the same device as the indexed tensor (\", dev, \")\");\n}\n\nTORCH_PRECOMPUTE_META_FUNC2(index, Tensor)\n(const Tensor& self, at::IOptTensorListRef indices) {\n  auto materialized = indices.materialize();\n\n  TORCH_CHECK_INDEX(\n      materialized.size() <= (size_t)self.dim(),\n      \"too many indices for tensor of dimension \",\n      self.dim(), \" (got \", materialized.size(), \")\");\n\n  // Only allow: `dev_tensor[{cpu,dev}_tensor]`.\n  // See: https://github.com/pytorch/pytorch/pull/69607\n  check_indices_on_cpu_or_selfdevice(self, materialized);\n\n  const auto& result = maybe_get_output();\n\n  if (result.defined()) {\n    at::assert_no_internal_overlap(result);\n    at::assert_no_overlap(result, self);\n    for (const at::OptionalTensorRef& index : materialized) {\n      if (index.has_value()) {\n        at::assert_no_overlap(result, *index);\n      }\n    }\n  }\n\n  auto info = at::native::make_info(self, std::move(indices));\n  build_index_op(*this, info, result);\n  return TORCH_PRECOMPUTE_STRUCT2(index, Tensor)()\n      .set_sizes(std::move(info.indexed_sizes))\n      .set_strides(std::move(info.indexed_strides));\n}\n\n} // namespace meta\n\nnamespace native {\n\nDEFINE_DISPATCH(index_stub);\nDEFINE_DISPATCH(index_fill_stub);\nDEFINE_DISPATCH(index_copy_stub);\nDEFINE_DISPATCH(index_put_stub);\nDEFINE_DISPATCH(index_put_with_sort_stub);\nDEFINE_DISPATCH(put_stub);\nDEFINE_DISPATCH(take_stub);\nDEFINE_DISPATCH(masked_fill_stub);\nREGISTER_NO_CPU_DISPATCH(index_put_with_sort_stub);\nREGISTER_NO_CPU_DISPATCH(index_put_with_sort_quantized_stub);\nDEFINE_DISPATCH(masked_select_serial_stub);\nDEFINE_DISPATCH(masked_select_stub);\nDEFINE_DISPATCH(masked_scatter_stub);\n\nDEFINE_DISPATCH(gather_stub);\nDEFINE_DISPATCH(scatter_stub);"
},
{
    "Id": 168,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a69f427f957a37eee9c1dd5df681f30ab38ed3e4",
    "Violation": "improper",
    "Bug report": "aten: Ensure dim is size_t",
    "Number of deleted lines": 1,
    "Deleted lines": ""
},
{
    "Id": 169,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8340762211e3b55caa178bac748bd902249f6fc0",
    "Violation": "missing",
    "Bug report": "Update lr_scheduler.py to check the type of eta_min. Add float assertion to `eta_min` parameter in `CosineAnnealingWarmRestarts`.",
    "Number of deleted lines": 0,
    "Deleted lines": "class CosineAnnealingWarmRestarts(LRScheduler):\n    r\"\"\"Set the learning rate of each parameter group using a cosine annealing\n    schedule, where :math:`\\eta_{max}` is set to the initial lr, :math:`T_{cur}`\n    is the number of epochs since the last restart and :math:`T_{i}` is the number\n    of epochs between two warm restarts in SGDR:\n\n    .. math::\n        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n        \\cos\\left(\\frac{T_{cur}}{T_{i}}\\pi\\right)\\right)\n\n    When :math:`T_{cur}=T_{i}`, set :math:`\\eta_t = \\eta_{min}`.\n    When :math:`T_{cur}=0` after restart, set :math:`\\eta_t=\\eta_{max}`.\n\n    It has been proposed in\n    `SGDR: Stochastic Gradient Descent with Warm Restarts`_.\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        T_0 (int): Number of iterations for the first restart.\n        T_mult (int, optional): A factor increases :math:`T_{i}` after a restart. Default: 1.\n        eta_min (float, optional): Minimum learning rate. Default: 0.\n        last_epoch (int, optional): The index of last epoch. Default: -1.\n        verbose (bool): If ``True``, prints a message to stdout for\n            each update. Default: ``False``.\n\n    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n        https://arxiv.org/abs/1608.03983\n    \"\"\"\n\n    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False):\n        if T_0 <= 0 or not isinstance(T_0, int):\n            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n        if T_mult < 1 or not isinstance(T_mult, int):\n            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n        self.T_0 = T_0\n        self.T_i = T_0\n        self.T_mult = T_mult\n        self.eta_min = eta_min\n        self.T_cur = last_epoch\n        super().__init__(optimizer, last_epoch, verbose)\n\n    def get_lr(self):\n        if not self._get_lr_called_within_step:\n            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n                          \"please use `get_last_lr()`.\", UserWarning)\n\n        return [self.eta_min + (base_lr - self.eta_min) * (1 + math.cos(math.pi * self.T_cur / self.T_i)) / 2\n                for base_lr in self.base_lrs]\n\n    def step(self, epoch=None):\n        \"\"\"Step could be called after every batch update\n\n        Example:\n            >>> # xdoctest: +SKIP(\"Undefined vars\")\n            >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n            >>> iters = len(dataloader)\n            >>> for epoch in range(20):\n            >>>     for i, sample in enumerate(dataloader):\n            >>>         inputs, labels = sample['inputs'], sample['labels']\n            >>>         optimizer.zero_grad()\n            >>>         outputs = net(inputs)\n            >>>         loss = criterion(outputs, labels)\n            >>>         loss.backward()\n            >>>         optimizer.step()\n            >>>         scheduler.step(epoch + i / iters)\n\n        This function can be called in an interleaved way.\n\n        Example:\n            >>> # xdoctest: +SKIP(\"Undefined vars\")"
},
{
    "Id": 170,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ae55619a2bb73ebcdc80b02a6ccd72275a9ce23e",
    "Violation": "missing",
    "Bug report": "Add check for same dtype in tensordot implementation",
    "Number of deleted lines": 0,
    "Deleted lines": "  const Tensor& bias = *bias_maybe_owned;\n\n  TORCH_CHECK(input1.dim() == input2.dim(), \"bilinear(): input dimensions do not match: got \", input1.dim(), \" and \", input2.dim());\n  for (const auto i : c10::irange(input1.dim() - 1)) {\n    TORCH_CHECK(input1.size(i) == input2.size(i),\n              \"bilinear(): input batch dimensions do not match at dim \", i, \": got \", input1.size(i), \" and \", input2.size(i));\n  }\n  TORCH_CHECK(input1.size(input1.dim() - 1) == weight.size(1),\n            \"bilinear(): input1 size does not match weight size: got \",\n            input1.size(input1.dim() - 1), \" but expected \", weight.size(1));\n  TORCH_CHECK(input2.size(input2.dim() - 1) == weight.size(2),\n            \"bilinear(): input2 size does not match weight size: got \",\n            input2.size(input2.dim() - 1), \" but expected \", weight.size(2));\n  TORCH_CHECK(!bias.defined() || bias.size(0) == weight.size(0),\n            \"bilinear(): bias size does not match weight size: got \",\n            bias.size(0), \" but expected \", weight.size(0));\n\n  std::vector<int64_t> output_size;\n  auto size1 = input1.sizes();\n  output_size.insert(output_size.end(), size1.begin(), size1.end() - 1);\n  output_size.push_back(weight.size(0));\n  auto input1_flattened = input1.reshape({-1, input1.size(-1)});\n  auto input2_flattened = input2.reshape({-1, input2.size(-1)});\n  Tensor output = at::_trilinear(input1_flattened, weight, input2_flattened, {1,3}, {0}, {1,2}, {2,3}).reshape(output_size);\n  if (bias.defined()) {\n    output = output + bias;\n  }\n  return output;\n}\n\n// implements tensordot, a matrix-multiplication-like contraction, but the dimensions given\n// in the two dimension lists\nTensor tensordot(const Tensor& input1, const Tensor& input2, IntArrayRef dims1, IntArrayRef dims2) {\n  TORCH_CHECK(dims1.size() == dims2.size(), \"both dimension lists should have same length\");\n  int64_t csize = 1;  // total size of the contracted dimensions\n  Tensor t1 = input1;\n  Tensor t2 = input2;\n  for (const auto i : c10::irange(dims1.size())) {\n    int s1 = input1.size(dims1[i]);\n    int s2 = input2.size(dims2[i]);\n    if (s2 == 1) { // broadcasted dimensions can be summed right away\n      t1 = t1.sum(dims1[i], true);\n    } else if (s1 == 1) {\n      t2 = t2.sum(dims2[i], true);\n    } else {\n      TORCH_CHECK(s1 == s2, \"contracted dimensions need to match, but first has size \", s1, \" in dim \", dims1[i],\n               \" and second has size \", s2, \" in dim \", dims2[i]);\n      csize *= s1;\n    }\n  }\n\n  auto cdims1 = at::dim_list_to_bitset(dims1, input1.dim());\n  auto cdims2 = at::dim_list_to_bitset(dims2, input2.dim());\n  std::vector<int64_t> p1, p2, rsizes;  // p1, p2: input permutations, rsizes: sizes of the result\n  p1.reserve(input1.dim());\n  p2.reserve(input2.dim());\n  rsizes.reserve(input1.dim() + input2.dim() - (int64_t) dims1.size());\n  int64_t size1 = 1; // number of non-contracted elements in input1\n  int64_t size2 = 1; // number of non-contracted elements in input2\n\n  // fill the permutations and compute sizes\n  for (const auto i : c10::irange(input1.dim())) {\n    if (! cdims1[i]) {\n      p1.emplace_back(i);\n      size1 *= t1.size(i);\n      rsizes.emplace_back(t1.size(i));\n    }\n  }\n  for (const auto x : dims1) {\n    p1.emplace_back(x);"
},
{
    "Id": 171,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b6920405da340bbd3397b80bf16d9c360b0c48d2",
    "Violation": "improper",
    "Bug report": "reorder checks to shave 1 us off no-op dispatch time ",
    "Number of deleted lines": 5,
    "Deleted lines": "  ~PyInterpreterHolder() {\n    impl_->disarm();\n  }\n  c10::impl::PyInterpreter* get() const noexcept {\n    return impl_;\n  }\n\n private:\n  c10::impl::PyInterpreter* impl_;\n};\nPyInterpreterHolder self_interpreter;\n\n} // anonymous namespace\n\nc10::impl::PyInterpreter* getPyInterpreter() {\n  return self_interpreter.get();\n}\n\nPyObject *THPVariableClass = nullptr;\n\nPyObject *ParameterClass = nullptr;\n\nstatic PyObject* THPVariable_NewWithVar(\n    PyTypeObject* type,\n    Variable _var,\n    c10::impl::PyInterpreterStatus status);\n\n// clang-tidy gets confused by static const\nstatic const char* VOLATILE_WARNING =\n    \"volatile was removed and now has no effect. Use \"\n    \"`with torch.no_grad():` instead.\";\n\nstatic bool check_has_torch_dispatch(PyObject *obj) {\n  PyTypeObject *tp = Py_TYPE(obj);\n  py::object attr = PyObject_FastGetAttrString(obj, \"__torch_dispatch__\");\n  return (\n    !THPVariable_CheckTypeExact(tp) &&\n    // TODO: test if Python key is disabled\n    attr.ptr() != nullptr &&\n    attr.ptr() != torch::disabled_torch_dispatch_impl()\n  );\n}\n\n// NOLINTNEXTLINE\nstatic PyObject* device_to_py_class_ [static_cast<size_t>(c10::DeviceType::COMPILE_TIME_MAX_DEVICE_TYPES)];\n\nvoid registerPythonTensorClass(const std::string& device, PyObject* python_tensor_class) {\n  c10::Device dev(device);\n\n  TORCH_CHECK(dev.type() == kXLA, \"Only the python class for XLA can be overriden\");\n  if (device_to_py_class_[static_cast<size_t>(dev.type())] != nullptr) {\n    TORCH_WARN(\"Overriding a previously registered python class for \", dev.str());\n  }\n\n  device_to_py_class_[static_cast<size_t>(dev.type())] = python_tensor_class;\n}\n\nstatic PyObject* getPythonTensorClass(c10::Device d) {\n  return device_to_py_class_[static_cast<size_t>(d.type())];\n}\n\n// TODO: Make this take Variable by const reference\nPyObject * THPVariable_Wrap(at::TensorBase var)\n{\n  if (!var.defined()) {\n    Py_RETURN_NONE;\n  }\n\n  c10::optional<PyObject*> mb_obj =\n      var.unsafeGetTensorImpl()->check_pyobj(self_interpreter.get());\n  c10::impl::PyInterpreterStatus status;\n  if (mb_obj.has_value()) {\n    auto obj = *mb_obj;\n    if (obj) {\n      if (var.unsafeGetTensorImpl()->owns_pyobj()) {\n        // C++ owns the Python object; this implies there weren't any other"
},
{
    "Id": 172,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/18eeccc7e8cba16d71efdd2eca831983c4abde15",
    "Violation": "missing",
    "Bug report": "Fix Optional type check",
    "Number of deleted lines": 0,
    "Deleted lines": "        self.builder.max_batch_size = max_batch_size\n        builder_config = self.builder.create_builder_config()\n        builder_config.max_workspace_size = max_workspace_size\n        if fp16_mode:\n            builder_config.set_flag(trt.BuilderFlag.FP16)\n\n        if int8_mode:\n            builder_config.set_flag(trt.BuilderFlag.INT8)\n\n        if strict_type_constraints:\n            builder_config.set_flag(trt.BuilderFlag.STRICT_TYPES)\n\n        if self.optimization_profiles:\n            for optimization_profile in self.optimization_profiles:\n                builder_config.add_optimization_profile(optimization_profile)\n\n        engine = self.builder.build_engine(self.network, builder_config)\n        assert(engine)\n        return engine, self._input_names, self._output_names\n\n    def run_node(self, n):\n        self._cur_node_name = str(n)\n        return super().run_node(n)\n\n    def placeholder(self, target, args, kwargs):\n        self._input_names.append(target)\n        shape, dtype, _, shape_ranges, has_batch_dim = self.input_specs[self.input_specs_iter]\n        self.input_specs_iter += 1\n\n        if self.network.has_implicit_batch_dimension:\n            if has_batch_dim:\n                shape = shape[1:]\n        else:\n            for i, shape_range in enumerate(shape_ranges):\n                self.optimization_profiles[i].set_shape(target, *shape_range)\n\n        return self.network.add_input(name=target, shape=tuple(shape), dtype=torch_dtype_to_trt(dtype))\n\n    def call_module(self, target, args, kwargs):\n        assert isinstance(target, str)\n        submod = self.fetch_attr(target)\n        converter = CONVERTERS.get(type(submod))\n\n        if not converter:\n            raise RuntimeError(f'Conversion of module of type {type(submod)} not currently supported!')\n\n        return converter(self.network, submod, args, kwargs, self._cur_node_name)\n\n    def call_function(self, target, args, kwargs):\n        converter = CONVERTERS.get(target)\n\n        if not converter:\n            raise RuntimeError(f'Conversion of function {torch.typename(target)} not currently supported!')\n\n        return converter(self.network, target, args, kwargs, self._cur_node_name)\n\n    def call_method(self, target, args, kwargs):\n        assert isinstance(target, str)\n        converter = CONVERTERS.get(target)\n\n        if not converter:\n            raise RuntimeError(f'Conversion of method {target} not currently supported!')\n\n        return converter(self.network, target, args, kwargs, self._cur_node_name)\n\n    def output(self, target, args, kwargs):\n        assert len(args) == 1\n        outputs = args[0] if isinstance(args[0], tuple) else (args[0],)\n\n        if not all(isinstance(output, trt.tensorrt.ITensor) for output in outputs):"
},
{
    "Id": 173,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4dad00b64b396ef81f16bdb896175688fc629f4d",
    "Violation": "missing",
    "Bug report": "special case tensor type check when getting RRef ",
    "Number of deleted lines": 1,
    "Deleted lines": "  // Currently, there are two call sites.\n  // (1) The creator user in python_functions.cpp\n  // (2) The callee user in RRefContext::notifyOwnerAndParentOfFork.\n  //\n  // The reason for not adding the pending user here is to put addPendingUser()\n  // close to where the RPC occurs, and it is more clear to pair it with\n  // deletePendingUser() in the response callback at the call site.\n  return c10::make_intrusive<UserRRef>(ownerId, rrefId, forkId, type);\n}\n\nvoid RRefContext::delUser(\n    const worker_id_t owner,\n    const RRefId& rrefId,\n    const ForkId& forkId) {\n  std::lock_guard<std::mutex> lock(destroyedMutex_);\n  if (!destroyed_) {\n    auto fm = agent_->send(\n        agent_->getWorkerInfo(owner),\n        RRefUserDelete(rrefId, forkId).toMessage());\n\n    fm->addCallback([](const Message& /* unused */,\n                       const c10::optional<utils::FutureError>& futErr) {\n      RRefContext::handleException(futErr);\n    });\n  }\n}\n\nc10::intrusive_ptr<RRef> RRefContext::getOrCreateRRef(\n    const RRefForkData& rrefForkData,\n    const TypePtr& type) {\n  auto& ownerId = rrefForkData.ownerId_;\n  auto& rrefId = rrefForkData.rrefId_;\n  auto& forkId = rrefForkData.forkId_;\n  if (ownerId == getWorkerId()) {\n    auto ownerRRef = getOwnerRRef(rrefId);\n    TORCH_INTERNAL_ASSERT(ownerRRef->type() == type);\n    return ownerRRef;\n  } else {\n    return createUserRRef(ownerId, rrefId, forkId, type);\n  }\n}\n\nc10::intrusive_ptr<OwnerRRef> RRefContext::getOrCreateOwnerRRef(\n    const RRefId& rrefId,\n    const TypePtr& type) {\n  std::lock_guard<std::mutex> lock(mutex_);\n  const auto iter = owners_.find(rrefId);\n  if (iter == owners_.end()) {\n    // Scenario (1) the first time this owner knows about this RRef\n    //\n    // NB: cannot use make_shared here as the constructor of OwnerRRef is\n    // private.\n    auto rref =\n        c10::make_intrusive<OwnerRRef>(getWorkerId(), rrefId, type);\n    owners_[rref->rrefId()] = rref;\n    ownerCV_.notify_all();\n    return rref;\n  } else {\n    // Scenario (2) retrieving an existing RRef\n    auto ownerRRef = c10::static_intrusive_pointer_cast<OwnerRRef>(iter->second);\n    TORCH_INTERNAL_ASSERT(ownerRRef->type() == type);\n    return ownerRRef;\n  }\n}\n\nc10::intrusive_ptr<OwnerRRef> RRefContext::createOwnerRRef(const TypePtr& type) {\n  // Don't add this OnwerRRef to the owners_ map yet, otherwise\n  // it will never be removed from there. Instead, only add it to the\n  // map in prepareChildFork, in case this local RRef is being passed\n  // to another worker.\n  return c10::make_intrusive<OwnerRRef>(getWorkerId(), genGloballyUniqueId(), type);\n}"
},
{
    "Id": 174,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4cc6e6bbbe1fb114e7d7fb207ef2deb567950102",
    "Violation": "missing",
    "Bug report": "Adding scalar to the c10 registration type check",
    "Number of deleted lines": 0,
    "Deleted lines": "        tracer::recordSourceLocation(node);\n        const auto& args = op.schema().arguments();\n        int i = 0;\n        for (auto iter = stack.end() - input_size; iter != stack.end();\n             ++iter, ++i) {\n          // TODO we need to refactor graph APIs (e.g., addInputs)\n          // appropriately; after that, we can get rid of the giant if-else\n          // block we will clean this tech debt together in the following PRs\n          auto type = args[i].type();\n          if (type->kind() == TypeKind::OptionalType) {\n            if (iter->isNone()) {\n              Value* none = graph->insertNode(graph->createNone())->output();\n              node->addInput(none);\n              continue;\n            } else {\n              type = type->expect<OptionalType>()->getElementType();\n            }\n          }\n          if (type->isSubtypeOf(TensorType::get())) {\n            AT_ASSERT(iter->isTensor());\n            tracer::addInputs(node, args[i].name().c_str(), iter->toTensor());\n          } else if (type->kind() == TypeKind::FloatType) {\n            AT_ASSERT(iter->isDouble());\n            tracer::addInputs(node, args[i].name().c_str(), iter->toDouble());\n          } else if (type->kind() == TypeKind::IntType) {\n            AT_ASSERT(iter->isInt());\n            tracer::addInputs(node, args[i].name().c_str(), iter->toInt());\n          } else if (type->kind() == TypeKind::BoolType) {\n            AT_ASSERT(iter->isBool());\n            tracer::addInputs(node, args[i].name().c_str(), iter->toBool());\n          } else if (type->kind() == TypeKind::StringType) {\n            AT_ASSERT(iter->isString());\n            tracer::addInputs(\n                node, args[i].name().c_str(), iter->toStringRef());\n          } else if (type->kind() == TypeKind::ListType) {\n            const auto& elem_type = type->expect<ListType>()->getElementType();\n            if (elem_type->isSubtypeOf(TensorType::get())) {\n              AT_ASSERT(iter->isTensorList());\n              auto list = iter->toTensorVector();\n              tracer::addInputs(node, args[i].name().c_str(), list);\n            } else if (elem_type->kind() == TypeKind::FloatType) {\n              AT_ASSERT(iter->isDoubleList());\n              // NB: now, tracer doesn't support tracing double list. We add special\n              // handling here, since in our case, we assume that all the doubles\n              // in the list are constants\n              auto value = iter->toDoubleVector();\n              std::vector<Value*> info(value.size());\n              for (size_t value_index = 0; value_index < value.size(); ++value_index) {\n                info[value_index] = graph->insertConstant(value[value_index]);\n                tracer::recordSourceLocation(info[value_index]->node());\n              }\n              node->addInput(\n                  graph->insertNode(graph->createList(jit::FloatType::get(), info))->output());\n            } else if (elem_type->kind() == TypeKind::IntType) {\n              AT_ASSERT(iter->isIntList());\n              tracer::addInputs(\n                  node, args[i].name().c_str(), iter->toIntVector());\n            } else if (elem_type->kind() == TypeKind::BoolType) {\n              AT_ASSERT(iter->isBoolList());\n              tracer::addInputs(\n                  node, args[i].name().c_str(), iter->toBoolList().vec());\n            } else {\n              throw std::runtime_error(\n                  \"unsupported input list type: \" + elem_type->str());\n            }\n          } else {\n            throw std::runtime_error(\"unsupported input type: \" + type->str());\n          }\n        }\n        graph->insertNode(node);"
},
{
    "Id": 175,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4b1ebd2f65e49d251ac2cfdb635794c7c6eb362f",
    "Violation": "missing",
    "Bug report": "Fast path for serializing large floating-point tensors to protobuf. Summary: Our existing serialization routines take a significant amount of time for large numpy arrays in order to verify the type of each element in the array as well as converting each element to a canonical type.  For large floating-point tensors, such as model parameters, this checking and converting takes a significant amount of time.  Adding a fast track path for just float32 arrays as this is the most common use case to worry about.",
    "Number of deleted lines": 0,
    "Deleted lines": "            tensor.double_data, dtype=np.int).reshape(tensor.dims)\n    else:\n        # TODO: complete the data type.\n        raise RuntimeError(\n            \"Tensor data type not supported yet: \" + str(tensor.data_type))\n\n\ndef NumpyArrayToCaffe2Tensor(arr, name=None):\n    tensor = caffe2_pb2.TensorProto()\n    tensor.dims.extend(arr.shape)\n    if name:\n        tensor.name = name\n    if arr.dtype == np.float32:\n        tensor.data_type = caffe2_pb2.TensorProto.FLOAT\n        tensor.float_data.extend(list(arr.flatten().astype(float)))\n    elif arr.dtype == np.float64:\n        tensor.data_type = caffe2_pb2.TensorProto.DOUBLE\n        tensor.double_data.extend(list(arr.flatten().astype(np.float64)))\n    elif arr.dtype == np.int:\n        tensor.data_type = caffe2_pb2.TensorProto.INT32\n        tensor.int32_data.extend(list(arr.flatten().astype(np.int)))\n    else:\n        # TODO: complete the data type.\n        raise RuntimeError(\n            \"Numpy data type not supported yet: \" + str(arr.dtype))\n    return tensor\n\n\ndef MakeArgument(key, value):\n    \"\"\"Makes an argument based on the value type.\"\"\"\n    argument = caffe2_pb2.Argument()\n    argument.name = key\n    iterable = isinstance(value, collections.Iterable)\n\n    if isinstance(value, np.ndarray):\n        value = value.flatten().tolist()\n    elif isinstance(value, np.generic):\n        # convert numpy scalar to native python type\n        value = np.asscalar(value)\n\n    if type(value) is float:\n        argument.f = value\n    elif type(value) in integer_types or type(value) is bool:\n        # We make a relaxation that a boolean variable will also be stored as\n        # int.\n        argument.i = value\n    elif isinstance(value, binary_type):\n        argument.s = value\n    elif isinstance(value, text_type):\n        argument.s = value.encode('utf-8')\n    elif isinstance(value, Message):\n        argument.s = value.SerializeToString()\n    elif iterable and all(type(v) in [float, np.float_] for v in value):\n        argument.floats.extend(\n            v.item() if type(v) is np.float_ else v for v in value\n        )\n    elif iterable and all(\n        type(v) in integer_types or type(v) in [bool, np.int_] for v in value\n    ):\n        argument.ints.extend(\n            v.item() if type(v) is np.int_ else v for v in value\n        )\n    elif iterable and all(\n        isinstance(v, binary_type) or isinstance(v, text_type) for v in value\n    ):\n        argument.strings.extend(\n            v.encode('utf-8') if isinstance(v, text_type) else v\n            for v in value\n        )\n    elif iterable and all(isinstance(v, Message) for v in value):"
},
{
    "Id": 176,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/65dfe1203ffab064d4e32fa8f76833042369d2f5",
    "Violation": "missing",
    "Bug report": "add an assertion to check the param num. Introduce this check to see whether it will break any existing workflow",
    "Number of deleted lines": 0,
    "Deleted lines": "        if isinstance(example_outputs, torch.Tensor):\n            example_outputs = [example_outputs]\n        try:\n            method = model.__getattr__('forward')\n            graph = method.propagate_and_assign_input_and_output_shapes(\n                args, example_outputs, False, propagate)\n            # Erase number types to bring the graph to a pre-NumberType state\n            params = method.initial_ivalues()\n        except AttributeError:\n            # TODO: just trace it\n            raise RuntimeError('\\'forward\\' method must be a script method')\n    else:\n        graph, torch_out = _trace_and_get_graph_from_model(model, args, training)\n        state_dict = _unique_state_dict(model)\n        params = list(state_dict.values())\n        if _retain_param_name:\n            graph_inputs = list(graph.inputs())\n            user_input_num = len(graph_inputs) - len(state_dict)\n            param_names = list(state_dict.keys())\n            for i, inp in enumerate(graph_inputs):\n                if i >= user_input_num:\n                    inp.setUniqueName(param_names[i - user_input_num])\n\n    graph = _optimize_graph(graph, operator_export_type)\n\n    # NB: ONNX requires complete information about output types, which might be\n    # erased by some optimizations, so we need to set it explicitly again.\n    if torch_out is not None:\n        output_tensors, _ = torch._C._jit_flatten(torch_out)\n        for output, tensor in zip(graph.outputs(), output_tensors):\n            output.inferTypeFrom(tensor)\n\n    _set_input_and_output_names(graph, input_names, output_names)\n\n    input_and_param_names = [val.uniqueName() for val in graph.inputs()]\n    param_names = input_and_param_names[len(input_and_param_names) - len(params):]\n    params_dict = dict(zip(param_names, params))\n\n    if verbose:\n        print(graph)\n\n    return graph, params_dict, torch_out\n\n\ndef export_to_pretty_string(model, args, f, export_params=True, verbose=False, training=False,\n                            input_names=None, output_names=None, aten=False, export_raw_ir=False,\n                            operator_export_type=None, export_type=ExportTypes.PROTOBUF_FILE,\n                            example_outputs=None, propagate=False, google_printer=False,\n                            opset_version=None, _retain_param_name=True):\n    if aten or export_raw_ir:\n        assert operator_export_type is None\n        assert aten ^ export_raw_ir\n        operator_export_type = OperatorExportTypes.ATEN if aten else OperatorExportTypes.RAW\n    elif operator_export_type is None:\n        operator_export_type = OperatorExportTypes.ONNX\n    return _export_to_pretty_string(model, args, f, export_params, verbose, training,\n                                    input_names, output_names, operator_export_type,\n                                    export_type, example_outputs, propagate, google_printer,\n                                    opset_version, _retain_param_name)\n\n\ndef _export_to_pretty_string(model, args, f, export_params=True, verbose=False, training=False,\n                             input_names=None, output_names=None, operator_export_type=OperatorExportTypes.ONNX,\n                             export_type=ExportTypes.PROTOBUF_FILE, example_outputs=None, propagate=False,\n                             google_printer=False, opset_version=None, _retain_param_name=False):\n    from torch.onnx.symbolic import _default_onnx_opset_version, _set_opset_version\n    if opset_version is None:\n        opset_version = _default_onnx_opset_version\n    _set_opset_version(opset_version)\n    graph, params, torch_out = _model_to_graph(model, args, f, verbose,"
},
{
    "Id": 177,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4fd98dfe69287914fd29b38fbccaf7ac4d7261ee",
    "Violation": "unnecessary",
    "Bug report": "Don't only apply DDP optimizer on forward frames. Previously a check would only apply DDP optimizer on frames named \"forward\".",
    "Number of deleted lines": 1,
    "Deleted lines": "        super().__init__(\n            callback=callback,\n            on_enter=on_enter,\n            backend_ctx_ctor=backend_ctx_ctor,\n            patch_fn=TorchPatcher.patch,\n            first_ctx=first_ctx,\n        )\n\n\nclass RunOnlyContext(_TorchDynamoContext):\n    def __init__(self):\n        super().__init__(callback=False)\n\n\nclass DisableContext(_TorchDynamoContext):\n    def __init__(self):\n        super().__init__(callback=None)\n\n\ndef catch_errors_wrapper(callback):\n    @functools.wraps(callback)\n    def catch_errors(frame, cache_size):\n        try:\n            if frame.f_lasti >= 0 or skipfiles.check(frame.f_code.co_filename):\n                log.debug(f\"skipping {frame.f_code.co_name} {frame.f_code.co_filename}\")\n                return None\n            if (\n                frame.f_code.co_filename == \"<string>\"\n                and frame.f_code.co_name == \"__new__\"\n            ):\n                # nametuple constructor\n                return None\n            if config.optimize_ddp:\n                ddp_module = DistributedDataParallel._get_active_ddp_module()\n                if ddp_module and frame.f_code.co_name == \"forward\":\n                    with compile_lock:\n                        ddp_optimizer = DDPOptimizer(\n                            bucket_bytes_cap=ddp_module.bucket_bytes_cap,\n                            parameters_to_ignore=ddp_module.parameters_to_ignore,\n                            backend_compile_fn=callback._torchdynamo_orig_callable,\n                        )\n                        hijacked_callback = convert_frame.convert_frame(\n                            ddp_optimizer.compile_fn, guard_export_fn=None\n                        )\n                        return hijacked_callback(frame, cache_size)\n\n            with compile_lock:\n                return callback(frame, cache_size)\n        except Exception:\n            log.exception(\"Error while processing frame\")\n            raise\n\n    catch_errors._torchdynamo_orig_callable = callback\n    return catch_errors\n\n\ndef _optimize_catch_errors(compile_fn, backend_ctx_ctor=null_context):\n    return OptimizeContext(\n        catch_errors_wrapper(compile_fn),\n        backend_ctx_ctor=backend_ctx_ctor,\n        first_ctx=True,\n    )\n\n\nclass WrapperBackend:\n    def __init__(self, backend=None):\n        self.backend = backend\n\n    @property\n    def example_inputs(self):\n        return clone_inputs(self.original_example_inputs)"
},
{
    "Id": 178,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8a644f0c136cb12cf200050c2ae6875ec487d174",
    "Violation": "improper",
    "Bug report": "Summary: Sometimes first dim of X in FC is BATCH_OF_FEATURE_MAX instead of BATCH. This caused an issue in f207899183 (when first dim of X is 64 but is set to 1 in inferFC). Change the check from `!= BATCH` to `== UNKNOWN`",
    "Number of deleted lines": 1,
    "Deleted lines": "    ArgumentHelper helper(op);\n    auto axis = helper.GetSingleArgument<int32_t>(\"axis\", 1);\n    auto axis_w = helper.GetSingleArgument<int32_t>(\"axis_w\", 1);\n    const TensorShape w_shape = w_shape_info.shape;\n    bool transposed = (op.type() == \"FCTransposed\") ? true : false;\n    const int canonical_axis_w =\n        canonical_axis_index_(axis_w, w_shape.dims().size());\n    const int64_t K = transposed ? SizeToDim(w_shape, canonical_axis_w)\n                                 : SizeFromDim(w_shape, canonical_axis_w);\n    std::vector<int64_t> dims;\n    std::vector<TensorBoundShape::DimType> dimTypes;\n    for (int i = 0; i < axis - 1; ++i) {\n      dims.push_back(1);\n      dimTypes.push_back(TensorBoundShape_DimType_CONSTANT);\n    }\n    dims.push_back(spec_.max_batch_size);\n    dimTypes.push_back(TensorBoundShape_DimType_BATCH);\n    dims.push_back(K);\n    dimTypes.push_back(TensorBoundShape_DimType_CONSTANT);\n    current_dim_type_ = TensorBoundShape_DimType_BATCH;\n    current_max_batch_size_ = spec_.max_batch_size;\n    TensorProto::DataType w_data_type;\n    if (fp16) {\n      w_data_type = TensorProto_DataType_FLOAT;\n    } else if (int8_fc) {\n      w_data_type = TensorProto_DataType_UINT8;\n    } else {\n      w_data_type = w_shape.data_type();\n    }\n    // Note: for FbFCPacked, weight is fp16 but activations are in fp32\n    CheckAndSetTensorBoundShape(\n        op.input(0), dimTypes, dims, w_data_type, int8_fc ? true : false);\n  } else {\n    ShapeInfo& x_shape_info = x_it->second;\n    if (x_shape_info.getDimType(0) != TensorBoundShape_DimType_BATCH) {\n      CAFFE_ENFORCE_GE(x_shape_info.shape.dims_size(), 1);\n      x_shape_info.shape.set_dims(0, spec_.max_batch_size);\n      x_shape_info.setDimType(0, TensorBoundShape_DimType_BATCH);\n    }\n  }\n\n  // Standard shape inference for outputs\n  std::vector<TensorShape> input_shapes{\n      shape_info_[op.input(0)].shape, w_shape_info.shape, b_shape_info.shape};\n  std::vector<TensorShape> output_shapes = InferOutput(op, input_shapes);\n  CAFFE_ENFORCE_EQ(output_shapes.size(), 1);\n  TensorProto::DataType output_data_type;\n  if (fp16) {\n    output_data_type = TensorProto_DataType_FLOAT;\n  } else if (int8_fc) {\n    output_data_type = TensorProto_DataType_UINT8;\n  } else {\n    output_data_type = output_shapes.front().data_type();\n  }\n  CheckAndSetTensorBoundShape(\n      op.output(0),\n      setDimTypeWithFirst(\n          TensorBoundShape_DimType_BATCH, output_shapes.front().dims().size()),\n      ConvertToVec(output_shapes[0].dims()),\n      output_data_type,\n      int8_fc ? true : false);\n}\n\n// Infers shapes for operators which are used to transform non-quantized\n// operators (e.g. SparseLengthsSum) into quantized operators (e.g.\n// SparseLengthsSumFused8BitRowwise) at model training time. If we're doing\n// quantization for CONSTANTS (eg. embedding tables), current_dim_type_ should\n// be set to CONSTANT.\nvoid BoundShapeInferencer::InferQuantizationTransformation(\n    const OperatorDef& op) {\n  bool all_constant = true;"
},
{
    "Id": 179,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5023995292f5119c447de15c20a375b7e3aa2d0b",
    "Violation": "improper",
    "Bug report": " fix output size adjustment for onnxifi_op. Summary: this breaks if we cut the net at certain int8 ops boundary.",
    "Number of deleted lines": 1,
    "Deleted lines": "    input_shape_info_[input_names_[i]] =\n        ShapeInfo(dim_type, std::move(shape), quantized);\n  }\n  bound_shape_inferencer->InferBoundShapeAndType(\n      netdef_, input_shape_info_, nullptr, false);\n  const auto& shape_info = bound_shape_inferencer->shape_info();\n  for (int i = 0; i < OutputSize(); ++i) {\n    const auto it = shape_info.find(output_names_[i]);\n    CAFFE_ENFORCE(it != shape_info.end());\n    const auto& real_shape = it->second.shape;\n    const auto& max_shape = output_shapes_[i];\n    CAFFE_ENFORCE_EQ(real_shape.dims_size(), max_shape.size());\n    const auto dim_size = real_shape.dims_size();\n    auto& begin = output_reshape_info.begins[i];\n    begin.Resize(dim_size);\n    int32_t* begin_ptr = begin.template mutable_data<int32_t>();\n    auto& end = output_reshape_info.ends[i];\n    end.Resize(dim_size);\n    int32_t* end_ptr = end.template mutable_data<int32_t>();\n    int32_t mismatch = 0;\n    for (int j = 0; j < dim_size; ++j) {\n      CAFFE_ENFORCE_GE(\n          max_shape[j],\n          real_shape.dims(j),\n          \"It is weird that max shape of \",\n          output_names_[i],\n          \" is smaller than real shape at dim \",\n          j,\n          \" (\",\n          max_shape[j],\n          \" vs \",\n          real_shape.dims(j),\n          \")\");\n      begin_ptr[j] = 0;\n      if (max_shape[j] > real_shape.dims(j)) {\n        end_ptr[j] = real_shape.dims(j);\n        mismatch += j;\n      } else {\n        end_ptr[j] = -1;\n      }\n    }\n    output_reshape_info.fast_path[i] = !mismatch;\n  }\n  return current_batch_size;\n}\n\ntemplate <>\nvoid OnnxifiOp<CPUContext>::adjustOutputBatchSizes(int current_batch_size) {\n  auto it = output_reshape_info_.find(current_batch_size);\n  CAFFE_ENFORCE(\n      it != output_reshape_info_.end(),\n      \"Cannot find current_batch_size \",\n      current_batch_size,\n      \" in output_reshape_info_\");\n  const auto& output_reshape_info = it->second;\n  CPUContext context;\n  Tensor tmp(CPU);\n  for (int i = 0; i < OutputSize(); ++i) {\n    Tensor* output_tensor = quantized_outputs_[i]\n        ? (&this->template Output<int8::Int8TensorCPU>(i)->t)\n        : Output(i);\n    const auto& end = output_reshape_info.ends[i];\n    if (output_reshape_info.fast_path[i]) {\n      output_tensor->ShrinkTo(end.data<int32_t>()[0]);\n    } else {\n      // We need to use generic Slice\n      SliceImpl<int32_t, CPUContext>(\n          &tmp, *output_tensor, output_reshape_info.begins[i], end, &context);\n      output_tensor->CopyFrom(tmp);\n    }\n  }"
},
{
    "Id": 180,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a1edf5f63c62d88230d1f7feb26edb059551ae71",
    "Violation": "improper",
    "Bug report": "Do hook sizes check with SymInt. I don't think this matters for any uses right now, but I found it during an audit; might as well fix it.",
    "Number of deleted lines": 1,
    "Deleted lines": "      input_vars,\n      std::move(inputs_mapping),\n      raw_outputs,\n      outputs,\n      non_differentiable,\n      dirty_inputs,\n      std::move(jvp_user_function));\n\n  return outputs;\n}\n\nvoid check_variable_result(\n    const at::TensorBase& original,\n    const at::TensorBase& result,\n    std::string hook_name) {\n  if (!original.options().type_equal(result.options())) {\n    std::stringstream ss;\n    ss << \"hook '\" << hook_name << \"' has changed the type of value (\";\n    ss << \"was \" << original.toString() << \" got \";\n    ss << result.toString() << \")\";\n    throw std::runtime_error(ss.str());\n  }\n\n  if (original.is_cuda() != result.is_cuda()) {\n    std::stringstream ss;\n    ss << \"hook '\" << hook_name << \"' has changed the type of value\";\n    if (original.is_cuda()) {\n      ss << \" (was CUDA tensor got CPU tensor)\";\n    } else {\n      ss << \" (was CPU tensor got CUDA tensor)\";\n    }\n    throw std::runtime_error(ss.str());\n  }\n\n  if (original.sizes().vec() != result.sizes().vec()) {\n    std::stringstream ss;\n    ss << \"hook '\" << hook_name << \"' has changed the size of value\";\n    throw std::runtime_error(ss.str());\n  }\n}\n\nvoid AutogradContext::save_for_backward(variable_list to_save) {\n  to_save_ = std::move(to_save);\n}\n\n// The logic for handling saved variables here is the same as\n// python_function.cpp See _save_variables() and unpack_saved_variables()\nvoid AutogradContext::save_variables() {\n  saved_variables_.clear();\n  auto ptr = grad_fn_.lock();\n\n  for (const auto& var : to_save_) {\n    // Allow empty variables to be saved\n    if (var.defined()) {\n      bool is_output = var.grad_fn().get() == ptr.get();\n      saved_variables_.emplace_back(var, is_output);\n    } else {\n      saved_variables_.emplace_back();\n    }\n  }\n  to_save_.clear();\n}\n\nvariable_list AutogradContext::get_saved_variables() const {\n  TORCH_CHECK(!has_freed_buffers_, ERR_BACKWARD_TWICE);\n  variable_list saved;\n  saved.reserve(saved_variables_.size());\n  auto ptr = grad_fn_.lock();\n  TORCH_INTERNAL_ASSERT(ptr);\n  for (auto& var : saved_variables_) {\n    saved.push_back(var.unpack(ptr));"
},
{
    "Id": 181,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f6e137598ddf0b990c423b1d6412502b62e095b2",
    "Violation": "unnecessary",
    "Bug report": "ns for fx: fix nit in default qlinear weight extraction function. Removes the assert for node type in default qlinear weight extraction function. Without the assert, user defined functions can now use this util function without failing this check.",
    "Number of deleted lines": 1,
    "Deleted lines": "    # weight -> to(torch.float16) -> dequantize -> linear\n    linear_second_arg = node.args[1]\n    assert isinstance(linear_second_arg, Node)\n\n    if linear_second_arg.op == 'call_module':\n        # weight -> obs -> linear\n        weight_arg_node = node.args[1]\n        assert isinstance(weight_arg_node, Node)\n        weight_node = weight_arg_node.args[0]\n        assert isinstance(weight_node, Node)\n        assert weight_node.op == 'get_attr'\n        weight = getattr_from_fqn(gm, weight_node.target)  # type: ignore[arg-type]\n        return weight.detach()\n    elif linear_second_arg.op == 'call_method':\n        # weight -> to(torch.float16) -> dequantize -> linear\n        assert linear_second_arg.op == 'call_method'\n        dequant_node = node.args[1]\n        assert isinstance(dequant_node, Node)\n        to_fp16_node = dequant_node.args[0]\n        assert isinstance(to_fp16_node, Node)\n        # extract the dtype, so we can cast to it before returning\n        target_dtype = to_fp16_node.args[1]\n        weight_node = to_fp16_node.args[0]\n        assert isinstance(weight_node, Node)\n        assert weight_node.op == 'get_attr'\n        weight = getattr_from_fqn(gm, weight_node.target)  # type: ignore[arg-type]\n        # return the weight with fp16 cast\n        return weight.detach().to(target_dtype)\n    else:\n        assert linear_second_arg.op == 'get_attr'\n        weight = getattr_from_fqn(gm, linear_second_arg.target)  # type: ignore[arg-type]\n        return weight.detach()\n\ndef get_qlinear_fun_weight(node: Node, gm: GraphModule) -> torch.Tensor:\n    assert node.target in (toq.linear, toq.linear_relu)\n    # packed weight is arg 1\n    packed_weight_node = node.args[1]\n    assert isinstance(packed_weight_node, Node)\n    assert packed_weight_node.op == 'get_attr'\n    packed_weight = getattr_from_fqn(gm, packed_weight_node.target)  # type: ignore[arg-type]\n    # TODO(future PR): why does packed_weight.unpack() not work?\n    (weight, _bias), _name = packed_weight.__getstate__()\n    return weight\n\ndef get_op_to_type_to_weight_extraction_fn() -> Dict[str, Dict[Callable, Callable]]:\n\n    op_to_type_to_weight_extraction_fn: Dict[str, Dict[Callable, Callable]] = {\n        'call_module': {\n            # Conv\n            nn.Conv1d: mod_weight_detach,\n            nn.Conv2d: mod_weight_detach,\n            nn.Conv3d: mod_weight_detach,\n            nni.ConvReLU1d: mod_0_weight_detach,\n            nni.ConvReLU2d: mod_0_weight_detach,\n            nni.ConvReLU3d: mod_0_weight_detach,\n            nnq.Conv1d: mod_weight_bias_0,\n            nniqat.ConvBn1d: mod_weight_detach,\n            nniqat.ConvBnReLU1d: mod_weight_detach,\n            nniq.ConvReLU1d: mod_weight_bias_0,\n            nnq.Conv2d: mod_weight_bias_0,\n            nnqat.Conv2d: mod_weight_detach,\n            nniqat.ConvBn2d: mod_weight_detach,\n            nniqat.ConvBnReLU2d: mod_weight_detach,\n            nniqat.ConvReLU2d: mod_weight_detach,\n            nniq.ConvReLU2d: mod_weight_bias_0,\n            nnq.Conv3d: mod_weight_bias_0,\n            nnqat.Conv3d: mod_weight_detach,\n            nniqat.ConvBn3d: mod_weight_detach,\n            nniqat.ConvBnReLU3d: mod_weight_detach,\n            nniqat.ConvReLU3d: mod_weight_detach,\n            nniq.ConvReLU3d: mod_weight_bias_0,"
},
{
    "Id": 182,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5a20c56ebce3426397210e91693fbbeade8b46ba",
    "Violation": "unnecessary",
    "Bug report": "emove hasOperation() check. by removing the hasOperation() check, the Operation gets successfully materialized, and static runtime enables successfully and runs ok. Will check that the outputs match with jit interpreter",
    "Number of deleted lines": 1,
    "Deleted lines": "    // the tensor size from the previous run to allocate tensors for the next\n    // run (following C2 tradition), exploiting the fact that tensor storage\n    // size does not have to match that of real tensor size. The following logic\n    // records the tensor storage size for the next run.\n    ms.first = max;\n    managed_bytes_ += max;\n  }\n\n  // for unmanaged ivalues (either tensor or non-tensor), we reset the *iv so\n  // that the objects pointed to by *iv may be reclaimed by reference counting\n  for (auto& iv : unmanaged_ivalues_) {\n    *iv = IValue();\n  }\n  buffer_ = {};\n}\n\nProcessedNode::ProcessedNode(\n    Node* node,\n    std::vector<const IValue*>&& inputs,\n    bool enable_out_variant)\n    : node_(node), inputs_(std::move(inputs)) {\n  // TODO leverage type information\n  outputs_.resize(node->outputs().size());\n\n  if (enable_out_variant && (fn_ = getOutOfPlaceOperation(node))) {\n    VLOG(1) << \"Switch to out variant for node: \" << PrintNode(node);\n    return;\n  }\n  if (!fn_ && (native_fn_ = getNativeOperation(node))) {\n    VLOG(1) << \"Switch to native impl for node: \" << PrintNode(node);\n    return;\n  }\n  {\n    const Operator& op = node->getOperator();\n    TORCH_CHECK(op.hasOperation());\n    op_ = op.getOperation(node);\n    VLOG(1) << \"Fallback interpreter for node: \" << PrintNode(node);\n  }\n}\n\nvoid ProcessedNode::run() {\n  if (fn_) {\n    fn_(this);\n  } else if (native_fn_) {\n    native_fn_(this);\n  } else {\n    std::vector<IValue> stack;\n    const size_t size = node_->inputs().size();\n    stack.reserve(size);\n    for (const auto i : c10::irange(size)) {\n      stack.emplace_back(Input(i));\n    }\n\n    DCHECK(op_);\n    op_->operator()(&stack);\n\n    DCHECK_EQ(stack.size(), node_->outputs().size());\n    for (const auto i : c10::irange(node_->outputs().size())) {\n      Output(i) = std::move(stack[i]);\n    }\n  }\n}\n\n} // namespace jit\n} // namespace torch\n"
},
{
    "Id": 183,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/012829eb3657aff2d58cead0bd166089c6e90c7f",
    "Violation": "unnecessary",
    "Bug report": "Do not crash when target device is unsupported by fuser. The `canFuseOnDevice` function now crashes when the device is not covered (i.e., CPU, GPU, XPU). However, now we have some devices, such as XLA and Lazy, that could perform fusion by themselves. This checker then prevents these devices from working on the models partially implemented in `jit.script`. This PR proposes to remove this checker and simply return false for all uncovered cases.",
    "Number of deleted lines": 2,
    "Deleted lines": "    // TODO: Relax the checks to support dynamic shapes\n    for (Value* input : node->inputs()) {\n      if (!shapeIsKnown(input)) {\n        return false;\n      }\n      if (input->node()->kind() == prim::ListConstruct) {\n        if (!allShapesAreKnown(input->node())) {\n          return false;\n        }\n      }\n    }\n    for (Value* output : node->outputs()) {\n      if (!shapeIsKnown(output)) {\n        return false;\n      }\n    }\n    return true;\n  }\n\n  bool canFuseOnDevice(Value* v) {\n    auto type = v->type()->cast<TensorType>();\n    if (!type) {\n      return true;\n    }\n    auto device = type->device();\n    if (!device) {\n      return false;\n    }\n    if (device->is_cpu()) {\n      return canFuseOnCPU();\n    } else if (device->is_cuda()) {\n      return canFuseOnGPU();\n    } else if (device->is_xpu()) {\n      return false;\n    } else {\n      TORCH_CHECK_NOT_IMPLEMENTED(false, \"Unknown device for tensorexpr fuser\")\n    }\n  }\n\n  bool isFusableOnDevice(Node* node) {\n    for (const auto& input : node->inputs()) {\n      if (input->node()->kind() == prim::ListConstruct) {\n        if (!isFusableOnDevice(input->node())) {\n          return false;\n        }\n      }\n      if (!canFuseOnDevice(input)) {\n        return false;\n      }\n    }\n    return true;\n  }\n\n  bool typesAreSupported(Node* node) {\n    // clang-format off\n    // breaks up the schema strings so they are no longer discoverable with ctrl-F\n    static const OperatorSet float_only_operator_set{\n      \"aten::fmod.Scalar(Tensor self, Scalar other) -> Tensor\",\n      \"aten::fmod.Tensor(Tensor self, Tensor other) -> Tensor\",\n      \"aten::remainder.Scalar(Tensor self, Scalar other) -> Tensor\",\n      \"aten::remainder.Tensor(Tensor self, Tensor other) -> Tensor\",\n    };\n    static const OperatorSet int_only_operator_set{\n      \"aten::__lshift__.Scalar(Tensor self, Scalar other) -> Tensor\",\n      \"aten::__lshift__.Tensor(Tensor self, Tensor other) -> Tensor\",\n      \"aten::__rshift__.Scalar(Tensor self, Scalar other) -> Tensor\",\n      \"aten::__rshift__.Tensor(Tensor self, Tensor other) -> Tensor\",\n    };\n    static const OperatorSet cpu_compute_heavy_set{\n      \"aten::conv2d(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1, int groups=1) -> Tensor\",\n      \"aten::matmul(Tensor self, Tensor other) -> Tensor\",\n    };\n    static const OperatorSet gpu_only_operator_set{"
},
{
    "Id": 184,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f386312ec936a94bfb1abe44acdd61d498f4272b",
    "Violation": "improper",
    "Bug report": "Don't do extra numel() check in TensorImpl::data(). `is_empty()` checks `numel() == 0`, but we don't need to access `numel_` at all (or the policy that `numel()` checks) in our happy path -- we just need the data pointer from `storage_`. Let's do the check we need to do using only the data we strictly need, rather than adding instructions loading other pieces of data.",
    "Number of deleted lines": 4,
    "Deleted lines": "  inline T* data_ptr_impl() const {\n    TORCH_CHECK(\n        has_storage(),\n        \"Cannot access data pointer of Tensor that doesn't have storage\");\n    TORCH_CHECK(\n        storage_initialized(),\n        \"The tensor has a non-zero number of elements, but its data is not allocated yet. \"\n        \"Caffe2 uses a lazy allocation, so you will need to call \"\n        \"mutable_data() or raw_mutable_data() to actually allocate memory.\");\n    // Caller does the type check.\n    return storage_.unsafe_data<T>() + storage_offset_;\n  }\n\n  /**\n   * Return a void* data pointer to the actual data which this tensor refers to.\n   *\n   * It is invalid to call data() on a dtype-uninitialized tensor, even if the\n   * size is 0.\n   *\n   * WARNING: The data pointed to by this tensor may not contiguous; do NOT\n   * assume that itemsize() * numel() is sufficient to compute the bytes that\n   * can be validly read from this tensor.\n   */\n  inline void* data() const {\n    TORCH_CHECK(\n        has_storage(),\n        \"Cannot access data pointer of Tensor that doesn't have storage\");\n    TORCH_CHECK(\n        dtype_initialized(),\n        \"Cannot access data pointer of Tensor that doesn't have initialized dtype \"\n        \"(e.g., caffe2::Tensor x(CPU), prior to calling mutable_data<T>() on x)\");\n    // Computing an offset into an empty tensor would be UB, since an empty\n    // tensor's storage will be nullptr, and adding a nonzero offset to nullptr\n    // is UB.  So we skip the offset computation in this case.\n    if (is_empty()) {\n      return nullptr;\n    }\n    return static_cast<void*>(\n        static_cast<char*>(storage_.data()) +\n        data_type_.itemsize() * storage_offset_);\n  }\n\n  /**\n   * Like data<T>(), but performs no checks.  You are responsible for ensuring\n   * that all invariants required by data() are upheld here.\n   */\n  template <typename T>\n  inline T* unsafe_data() const {\n    return storage_.unsafe_data<T>() + storage_offset_;\n  }\n\n  /**\n   * Returns the TypeMeta of a tensor, which describes what data type\n   * it is (e.g., int, float, ...)\n   */\n  const caffe2::TypeMeta dtype() const {\n    return data_type_;\n  }\n\n  /**\n   * Return the size of a single element of this tensor in bytes.\n   */\n  size_t itemsize() const {\n    TORCH_CHECK(\n        dtype_initialized(),\n        \"Cannot report itemsize of Tensor that doesn't have initialized dtype \"\n        \"(e.g., caffe2::Tensor x(CPU), prior to calling mutable_data<T>() on x)\");\n    return data_type_.itemsize();\n  }\n\n protected:\n  /**\n   * Returns the human-readable name of the actual type of this object (e.g.,\n   * TensorImpl, BatchedTensorImpl, etc.). Used for error messages.\n   */\n  virtual const char* tensorimpl_type_name() const {"
},
{
    "Id": 185,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fc304bec9f550075d4c899aa0fc5b1a0a573c1e5",
    "Violation": "unnecessary",
    "Bug report": "Remove redundant checks from canHandle in TE fuser",
    "Number of deleted lines": 3,
    "Deleted lines": "      });\n  return result;\n}\n\nbool allShapesAreKnown(Value* v) {\n  if (!v->type()->cast<TensorType>()) {\n    return true;\n  }\n  return v->isCompleteTensor();\n}\n\nbool allShapesAreKnown(Node* node) {\n  // TODO: Relax the checks to support dynamic shapes\n  for (torch::jit::Value* output : node->outputs()) {\n    if (!allShapesAreKnown(output)) {\n      return false;\n    }\n  }\n  for (torch::jit::Value* input : node->inputs()) {\n    if (!allShapesAreKnown(input)) {\n      return false;\n    }\n  }\n  return true;\n}\n\nbool canHandle(Node* node) {\n  if (node->kind() == prim::Constant) {\n    if (node->output()->type()->cast<TensorType>()) {\n      // TODO: add support for tensor constants.\n      return false;\n    }\n    return true;\n  }\n  if (node->kind() == prim::Loop) {\n    return false; // TODO\n  }\n  if (!allShapesAreKnown(node)) {\n    return false;\n  }\n\n  // Don't include nodes whose inputs are tensor constants - we cannot handle\n  // them at the moment.\n  // TODO: actually support tensor constants and remove this.\n  for (torch::jit::Value* input : node->inputs()) {\n    if (input->node()->kind() == prim::Constant &&\n        input->type()->cast<TensorType>()) {\n      return false;\n    }\n  }\n  return tensorexpr::isSupported(node);\n}\n\nNode* getOrCreateTensorExprSubgraph(Node* n) {\n  if (n->hasAttribute(attr::Subgraph) && n->kind() == getTensorExprSymbol()) {\n    return n;\n  }\n  auto te_group =\n      SubgraphUtils::createSingletonSubgraph(n, getTensorExprSymbol());\n  GRAPH_UPDATE(\"getOrCreateTensorExprSubgraph: \", *te_group);\n  return te_group;\n}\n\nclass TensorExprFuser {\n public:\n  TensorExprFuser(std::shared_ptr<Graph> graph) : graph_(std::move(graph)) {}\n\n  void run() {\n    aliasDb_ = torch::make_unique<AliasDb>(graph_);\n    createFusionGroups();\n  }\n\n private:"
},
{
    "Id": 186,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5b7c72101ca8e9d4edba1d16b6121ad900ca3936",
    "Violation": "unnecessary",
    "Bug report": "Removed check for is_quantized in dequantize_cpu_or_cuda. This particular PR isn't dispatcher related but does remove the extraneous torch check for a quant tensor since the dispatcher already handles a quantized backend for this particular function",
    "Number of deleted lines": 1,
    "Deleted lines": "    const Tensor& scale,\n    const Tensor& zero_point,\n    ScalarType dtype) {\n  auto quantizer = make_per_tensor_affine_quantizer(scale.item().toDouble(), zero_point.item().toLong(), dtype);\n  return quantizer->quantize(self);\n}\n\nstd::vector<Tensor> quantize_per_tensor_list_cpu(\n    TensorList tensors,\n    const Tensor& scales,\n    const Tensor& zero_points,\n    ScalarType dtype) {\n  std::vector<Tensor> quantized_tensors;\n  for (const auto i : c10::irange(tensors.size())) {\n    quantized_tensors.push_back(at::quantize_per_tensor(\n        tensors[i],\n        scales[i].item<double>(),\n        zero_points[i].item<int64_t>(),\n        dtype));\n  }\n  return quantized_tensors;\n}\n\nTensor quantize_per_channel(\n    const Tensor& self,\n    const Tensor& scales,\n    const Tensor& zero_points,\n    int64_t axis,\n    ScalarType dtype) {\n  auto quantizer = make_per_channel_affine_quantizer(scales, zero_points, axis, dtype);\n  return quantizer->quantize(self);\n}\n\nTensor dequantize_cpu_or_cuda(const Tensor& self) {\n  TORCH_CHECK(!self.is_quantized());\n  return self.to(at::kFloat);\n}\n\nTensor dequantize_quantized(const Tensor& self) {\n  return get_qtensorimpl(self)->quantizer()->dequantize(self);\n}\n\nstd::vector<Tensor> dequantize_tensors_quantized_cpu(TensorList tensors) {\n  std::vector<Tensor> dequantized_tensors;\n  for (const auto & tensor : tensors) {\n    dequantized_tensors.push_back(tensor.dequantize());\n  }\n  return dequantized_tensors;\n}\n\ndouble q_scale_quant(const Tensor& self) {\n  auto quantizer = get_qtensorimpl(self)->quantizer();\n  TORCH_CHECK(quantizer->qscheme() == kPerTensorAffine);\n  return static_cast<PerTensorAffineQuantizer*>(quantizer.get())->scale();\n}\n\nint64_t q_zero_point_quant(const Tensor& self) {\n  auto quantizer = get_qtensorimpl(self)->quantizer();\n  TORCH_CHECK(quantizer->qscheme() == kPerTensorAffine);\n  return static_cast<PerTensorAffineQuantizer*>(quantizer.get())->zero_point();\n}\n\nTensor q_per_channel_scales(const Tensor& self) {\n  auto quantizer = get_qtensorimpl(self)->quantizer();\n  TORCH_CHECK(quantizer->qscheme() == kPerChannelAffine || quantizer->qscheme() == kPerChannelAffineFloatQParams);\n  return static_cast<PerChannelAffineQuantizer*>(quantizer.get())->scales();\n}\n\nTensor q_per_channel_zero_points(const Tensor& self) {\n  auto quantizer = get_qtensorimpl(self)->quantizer();\n  TORCH_CHECK(quantizer->qscheme() == kPerChannelAffine || quantizer->qscheme() == kPerChannelAffineFloatQParams);"
},
{
    "Id": 187,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2597d5d72272d196b4cb5442ffc8cde376d1f785",
    "Violation": "insufficient",
    "Bug report": " TorchDynamo: always convert flexiblelayout to be FixedLayout when given a stride_order. For convolution, we always call **require_stride_order** to convert the input to the target stride order,  if the original input's layout is flexiblelayout, there always have a memory copy because the **is_stride_order_storage_and_layout** only checks the init stride order,  I think for flexiblelayout, means it's layout can be changed, if the user gives a stride order, I think we always need to convert the flexiblelayout to be FixedLayout using given strider order.",
    "Number of deleted lines": 3,
    "Deleted lines": "        if isinstance(x, ReinterpretView):\n            return x\n        if isinstance(x, BaseView):\n            x.realize()\n            if is_storage_and_layout(x.unwrap_view()) and not isinstance(\n                x.unwrap_view().data, ExternKernelAlloc\n            ):\n                try:\n                    return cls.convert_to_reinterpret_view(x)\n                except NotImplementedError:\n                    pass\n        if isinstance(x, StorageBox):\n            # TODO(jansel): impose layout preference on realized buffer\n            x.realize()\n            return x\n        return cls.copy_input(x)\n\n    @classmethod\n    def require_stride1(cls, x):\n        if is_storage_and_layout(x):\n            if len(x.get_stride()) == 0:\n                return x\n            for stride in x.get_stride():\n                if stride == 1:\n                    return x\n        return cls.copy_input(x)\n\n    @classmethod\n    def require_stride_order(cls, x, order):\n        if x.get_numel() == 0:  # Layout doesn't matter\n            return x\n\n        # require x to have the layout as strided_ordered as order\n        if is_storage_and_layout(x):\n            if isinstance(\n                x.get_layout(), FlexibleLayout\n            ) and is_stride_order_storage_and_layout(x, order):\n                # fix flexiblelayout to be FixedLayout with stride_order\n                as_storage_and_layout(\n                    x, freeze=True, want_contiguous=False, stride_order=order\n                )\n                return x\n            elif isinstance(\n                x.get_layout(), FixedLayout\n            ) and x.get_layout().is_stride_ordered(order):\n                return x\n            elif isinstance(x.get_layout(), MutationLayout):\n                if isinstance(x.get_layout().real_layout(), FlexibleLayout):\n                    raise AssertionError(\n                        \"the MutationLayout's real layout shouldn't be FlexibleLayout\"\n                    )\n                elif isinstance(\n                    x.get_layout().real_layout(), FixedLayout\n                ) and x.get_layout().real_layout().is_stride_ordered(order):\n                    return x\n\n        # TODO - Storage to InputBuffer\n        if isinstance(x, InputBuffer) and x.get_layout().is_stride_ordered(order):\n            return x\n        x = cls.copy_input(x)\n        as_storage_and_layout(x, freeze=True, want_contiguous=False, stride_order=order)\n        assert is_stride_order_storage_and_layout(x, order)\n        return x\n\n    @classmethod\n    def require_contiguous(cls, x):\n        return cls.require_stride_order(x, list(reversed(range(len(x.get_size())))))\n\n    def apply_constraint(self):\n        pass\n\n    def codegen_args(self):\n        args = [x.codegen_reference() for x in self.inputs]"
},
{
    "Id": 188,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/201f7d330ac8c33a7bedb8f0a66954415d1d27db",
    "Violation": "unnecessary",
    "Bug report": "Remove duplicate check in distributions arg validation ",
    "Number of deleted lines": 2,
    "Deleted lines": "            value (bool): Whether to enable validation.\n        \"\"\"\n        if value not in [True, False]:\n            raise ValueError\n        Distribution._validate_args = value\n\n    def __init__(self, batch_shape=torch.Size(), event_shape=torch.Size(), validate_args=None):\n        self._batch_shape = batch_shape\n        self._event_shape = event_shape\n        if validate_args is not None:\n            self._validate_args = validate_args\n        if self._validate_args:\n            try:\n                arg_constraints = self.arg_constraints\n            except NotImplementedError:\n                arg_constraints = {}\n                warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n                              'Please set `arg_constraints = {}` or initialize the distribution ' +\n                              'with `validate_args=False` to turn off validation.')\n            for param, constraint in arg_constraints.items():\n                if constraints.is_dependent(constraint):\n                    continue  # skip constraints that cannot be checked\n                if param not in self.__dict__ and isinstance(getattr(type(self), param), lazy_property):\n                    continue  # skip checking lazily-constructed args\n                value = getattr(self, param)\n                valid = constraint.check(value)\n                if not valid.all():\n                    raise ValueError(\n                        f\"Expected parameter {param} \"\n                        f\"({type(value).__name__} of shape {tuple(value.shape)}) \"\n                        f\"of distribution {repr(self)} \"\n                        f\"to satisfy the constraint {repr(constraint)}, \"\n                        f\"but found invalid values:\\n{value}\"\n                    )\n                if not constraint.check(getattr(self, param)).all():\n                    raise ValueError(\"The parameter {} has invalid values\".format(param))\n        super(Distribution, self).__init__()\n\n    def expand(self, batch_shape, _instance=None):\n        \"\"\"\n        Returns a new distribution instance (or populates an existing instance\n        provided by a derived class) with batch dimensions expanded to\n        `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n        the distribution's parameters. As such, this does not allocate new\n        memory for the expanded distribution instance. Additionally,\n        this does not repeat any args checking or parameter broadcasting in\n        `__init__.py`, when an instance is first created.\n\n        Args:\n            batch_shape (torch.Size): the desired expanded size.\n            _instance: new instance provided by subclasses that\n                need to override `.expand`.\n\n        Returns:\n            New distribution instance with batch dimensions expanded to\n            `batch_size`.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def batch_shape(self):\n        \"\"\"\n        Returns the shape over which parameters are batched.\n        \"\"\"\n        return self._batch_shape\n\n    @property\n    def event_shape(self):\n        \"\"\"\n        Returns the shape of a single sample (without batching).\n        \"\"\"\n        return self._event_shape"
},
{
    "Id": 189,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a47cc18254ade6dee1fe4a3c4eb5aca7ba40c77c",
    "Violation": "unnecessary",
    "Bug report": "remove unnecessary tuple check on tensor types",
    "Number of deleted lines": 1,
    "Deleted lines": ""
},
{
    "Id": 190,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2512017814fb2e3d6f3ae9dd3b315692ffc8fc71",
    "Violation": "unnecessary",
    "Bug report": "Fix for out of bounds read in torch mobile flatbuffer loader ",
    "Number of deleted lines": 2,
    "Deleted lines": "    std::vector<IValue>* constants) {\n  AT_ASSERT(\n      module_parsed_,\n      \"Need to first parse a flatbuffer file before extracting jit_sources\");\n\n  const auto* ivalues = module_->ivalues();\n  for (uint32_t i = mobile_ivalue_size_; i < ivalues->size(); i++) {\n    const auto* ival = ivalues->Get(i);\n    parseAndPopulate(i, ival);\n  }\n  // register functions\n  for (const auto& f : all_functions_) {\n    if (f.first >= mobile_ivalue_size_) {\n      uint32_t class_index =\n          ivalues->Get(f.first)->val_as_Function()->class_type();\n      ClassTypePtr class_type = all_types_[class_index];\n      class_type->addMethod(f.second);\n    }\n  }\n  const auto* jit_constants = module_->jit_constants();\n  for (const auto i : c10::irange(jit_constants->size())) {\n    constants->emplace_back(getIValue(jit_constants->Get(i)));\n  }\n  parseExtraFilesFromVector(module_->jit_sources(), jit_sources);\n}\n\n} // namespace\n\nmobile::Module parse_and_initialize_mobile_module(\n    void* data,\n    size_t size,\n    c10::optional<at::Device>,\n    ExtraFilesMap* extra_files,\n    bool should_copy_tensor_memory) {\n  TORCH_CHECK(\n      mobile::serialization::ModuleBufferHasIdentifier(data), \"Format error\");\n  // TODO(T128189662): If not copying, enforce that data is aligned to\n  // kFlatbufferDataAlignmentBytes, and add unit tests.\n\n  // Validate Flatbuffer module before parsing.\n  flatbuffers::Verifier verifier(reinterpret_cast<uint8_t*>(data), size);\n  TORCH_CHECK(\n      mobile::serialization::VerifyModuleBuffer(verifier),\n      \"Malformed Flatbuffer module\");\n\n  FlatbufferLoader loader;\n  loader.setShouldCopyTensorMemory(should_copy_tensor_memory);\n\n  // Flatbuffer doesn't seem to have a way to provide the buffer size when\n  // interacting with the buffer.\n  auto* flatbuffer_module = mobile::serialization::GetMutableModule(data);\n  auto* end = static_cast<char*>(data) + size;\n  mobile::Module m = loader.parseModule(flatbuffer_module, end);\n  if (extra_files != nullptr) {\n    parseExtraFiles(flatbuffer_module, *extra_files);\n  }\n  return m;\n}\n\nmobile::Module parse_and_initialize_mobile_module(\n    std::shared_ptr<char> data,\n    size_t size,\n    c10::optional<at::Device> device,\n    ExtraFilesMap* extra_files) {\n  mobile::Module m = parse_and_initialize_mobile_module(\n      data.get(),\n      size,\n      device,\n      extra_files,\n      /*should_copy_tensor_memory=*/false);\n  m.set_delete_memory(std::move(data));\n  return m;"
},
{
    "Id": 191,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e7fc7c732cbde822f9490840704b1f57fe86c50a",
    "Violation": "insufficient",
    "Bug report": "Bugfix for fusion device check",
    "Number of deleted lines": 1,
    "Deleted lines": "  }\n\n  void removeOutputsUsedOnlyInSize(Node* fusion_group) {\n    if (fusion_group->kind() != prim::FusionGroup)\n      return;\n    auto subgraph = fusion_group->g(attr::Subgraph);\n\n    auto shape_of = buildShapeExpressions(fusion_group);\n    auto outputs = fusion_group->outputs().vec();\n    auto soutputs = subgraph->outputs().vec();\n    // XXX: Iterating in this order is not only good for performance reasons!\n    // It is also crucial for correctness (i has to reflect the current true\n    // index of outputs[i])!\n    for (int64_t i = static_cast<int64_t>(outputs.size()) - 1; i >= 0; --i) {\n      auto output = outputs[i];\n      auto soutput = soutputs[i];\n      if (usedOnlyInSize(output) && shape_of.count(soutput) > 0) {\n        auto uses = output->uses();\n        for (Use u : uses) {\n          AT_ASSERT(u.user->matches(\"aten::size(Tensor self) -> int[]\"));\n          u.user->output()->replaceAllUsesWith(shape_of.at(soutput));\n          u.user->destroy();\n        }\n        fusion_group->eraseOutput(i);\n        subgraph->eraseOutput(i);\n      }\n    }\n  }\n\n  void refreshAliasDb() {\n    aliasDb_ = torch::make_unique<AliasDb>(graph_);\n  }\n\n  bool canFuseWithConcat(Value* producer, Node* before_check) {\n    if (!isFusable(producer->node())) {\n      return false;\n    }\n    // NB: it is important that this check happens after isFusable, which checks\n    // that the blocks match, and it's not a special node like prim::Param\n    if (!aliasDb_->couldMoveBeforeTopologically(\n            producer->node(), before_check)) {\n      return false;\n    }\n\n    // If the number of kernel args could exceed the limit, skip.\n    if ((before_check->inputs().size() +\n         before_check->outputs().size() +\n         producer->node()->inputs().size() +\n         producer->node()->outputs().size())\n        > fusion_kernel_args_limit) {\n      return false;\n    }\n\n    // Fusion groups can be merged with concat's group if and only if\n    // - the value they produce isn't already coming from a concat and\n    // - the fusion group does not contain GradSumToSize\n    if (producer->node()->kind() == prim::FusionGroup) {\n      auto subgraph = producer->node()->g(attr::Subgraph);\n      auto* node = subgraph->outputs().at(producer->offset())->node();\n      return node->kind() != prim::FusedConcat &&\n          !containsGradSumToSize(producer->node());\n    }\n    return true;\n  }\n\n  Node* createFusedConcat(Node* node) {\n    AT_ASSERT(node->kind() == aten::cat);\n\n    Graph* graph = node->owningGraph();\n    Node* list_construct = node->namedInput(attr::tensors)->node();\n    int64_t dim = node->get<int64_t>(attr::dim).value();"
},
{
    "Id": 192,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4a343043dbc6ce229b4dcf2258f7b6352db32b64",
    "Violation": "improper",
    "Bug report": "Fix hasattr check in saved_model_cli ",
    "Number of deleted lines": 2,
    "Deleted lines": "      metavar='SIGNATURE_DEF_KEY',\n      help='key of SignatureDef to run')\n  msg = ('Loading inputs from files, in the format of \\'<input_key>=<filename>,'\n         ' or \\'<input_key>=<filename>[<variable_name>]\\', separated by \\';\\'.'\n         ' The file format can only be from .npy, .npz or pickle.')\n  parser_run.add_argument('--inputs', type=str, default='', help=msg)\n  msg = ('Specifying inputs by python expressions, in the format of'\n         ' \"<input_key>=\\'<python expression>\\'\", separated by \\';\\'. '\n         'numpy module is available as \\'np\\'. '\n         'Will override duplicate input_keys from --inputs option.')\n  parser_run.add_argument('--input_exprs', type=str, default='', help=msg)\n  parser_run.add_argument(\n      '--outdir',\n      type=str,\n      default=None,\n      help='if specified, output tensor(s) will be saved to given directory')\n  parser_run.add_argument(\n      '--overwrite',\n      action='store_true',\n      help='if set, output file will be overwritten if it already exists.')\n  parser_run.add_argument(\n      '--tf_debug',\n      action='store_true',\n      help='if set, will use TensorFlow Debugger (tfdbg) to watch the '\n           'intermediate Tensors and runtime GraphDefs while running the '\n           'SavedModel.')\n  parser_run.set_defaults(func=run)\n\n  return parser\n\n\ndef main():\n  parser = create_parser()\n  args = parser.parse_args()\n  if not hasattr(args.func):\n    parser.error(\"too few arguments\")\n  args.func(args)\n\n\nif __name__ == '__main__':\n  sys.exit(main())\n"
},
{
    "Id": 193,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/73f25fc34c69878c83ee2eeb8f030cb79a76472f",
    "Violation": "insufficient",
    "Bug report": "This fix tries to address the issue by adding an `hasattr()` check so that AttributeError is not thrown.",
    "Number of deleted lines": 1,
    "Deleted lines": "    import_str = format_import(source_module_name, source_name, dest_name)\n    if import_str in self.module_imports[dest_module_name]:\n      return\n\n    # Check if we are trying to expose two different symbols with same name.\n    full_api_name = dest_name\n    if dest_module_name:\n      full_api_name = dest_module_name + '.' + full_api_name\n    if full_api_name in self._seen_api_names:\n      raise SymbolExposedTwiceError(\n          'Trying to export multiple symbols with same name: %s.' %\n          full_api_name)\n    self._seen_api_names.add(full_api_name)\n\n    self.module_imports[dest_module_name].append(import_str)\n\n\ndef get_api_imports():\n  \"\"\"Get a map from destination module to formatted imports.\n\n  Returns:\n    A dictionary where\n      key: (string) destination module (for e.g. tf or tf.consts).\n      value: List of strings representing module imports\n          (for e.g. 'from foo import bar') and constant\n          assignments (for e.g. 'FOO = 123').\n  \"\"\"\n  module_imports_builder = _ModuleImportsBuilder()\n  visited_symbols = set()\n\n  # Traverse over everything imported above. Specifically,\n  # we want to traverse over TensorFlow Python modules.\n  for module in sys.modules.values():\n    # Only look at tensorflow modules.\n    if not module or 'tensorflow.' not in module.__name__:\n      continue\n    # Do not generate __init__.py files for contrib modules for now.\n    if '.contrib.' in module.__name__ or module.__name__.endswith('.contrib'):\n      continue\n\n    for module_contents_name in dir(module):\n      attr = getattr(module, module_contents_name)\n      if id(attr) in visited_symbols:\n        continue\n\n      # If attr is _tf_api_constants attribute, then add the constants.\n      if module_contents_name == _API_CONSTANTS_ATTR:\n        for exports, value in attr:\n          for export in exports:\n            names = export.split('.')\n            dest_module = '.'.join(names[:-1])\n            module_imports_builder.add_import(\n                dest_module, module.__name__, value, names[-1])\n        continue\n\n      _, attr = tf_decorator.unwrap(attr)\n      # If attr is a symbol with _tf_api_names attribute, then\n      # add import for it.\n      if hasattr(attr, '__dict__') and _API_NAMES_ATTR in attr.__dict__:\n        # If the same symbol is available using multiple names, only create\n        # imports for it once.\n        if id(attr) in visited_symbols:\n          continue\n        visited_symbols.add(id(attr))\n\n        for export in attr._tf_api_names:  # pylint: disable=protected-access\n          names = export.split('.')\n          dest_module = '.'.join(names[:-1])\n          module_imports_builder.add_import(\n              dest_module, module.__name__, module_contents_name, names[-1])\n"
},
{
    "Id": 194,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cdc36a3b1f7d227984bb5e415b555ed334737f82",
    "Violation": "missing",
    "Bug report": "cosmetic fix to MonitoredSession.__del__ AttributeError. This CL prevents this message by checking that the underlying _sess object has the __del__ method defined before calling it.",
    "Number of deleted lines": 1,
    "Deleted lines": "        1) direct the wrapper session to perform a specified action (e.g., run\n          with or without debug tensor watching, invoking the stepper.)\n        2) debug URLs used to watch the tensors.\n    \"\"\"\n\n  @abc.abstractmethod\n  def on_run_end(self, request):\n    \"\"\"Callback invoked on run() calls to the debug-wrapper session.\n\n    This is a blocking callback.\n    The invocation happens right before the wrapper exits its run() call.\n\n    Args:\n      request: (`OnRunEndRequest`) callback request object carrying information\n        such as the actual action performed by the session wrapper for the\n        run() call.\n\n    Returns:\n      An instance of `OnRunStartResponse`.\n    \"\"\"\n\n  def as_default(self):\n    return ops.default_session(self)\n\n  def __enter__(self):\n    if self._default_session_context_manager is None:\n      self._default_session_context_manager = self.as_default()\n    return self._default_session_context_manager.__enter__()\n\n  def __exit__(self, exec_type, exec_value, exec_tb):\n    self._default_session_context_manager.__exit__(\n        exec_type, exec_value, exec_tb)\n\n  def __del__(self):\n    self._sess.__del__()\n\n  def close(self):\n    self._sess.close()\n\n  # TODO(cais): Add _node_name_regex_whitelist and\n  #   _node_op_type_regex_whitelist.\n\n  @abc.abstractmethod\n  def invoke_node_stepper(self,\n                          node_stepper,\n                          restore_variable_values_on_exit=True):\n    \"\"\"Callback invoked when the client intends to step through graph nodes.\n\n    Args:\n      node_stepper: (stepper.NodeStepper) An instance of NodeStepper to be used\n        in this stepping session.\n      restore_variable_values_on_exit: (bool) Whether any variables whose values\n        have been altered during this node-stepper invocation should be restored\n        to their old values when this invocation ends.\n\n    Returns:\n      The same return values as the `Session.run()` call on the same fetches as\n        the NodeStepper.\n    \"\"\"\n\n  def should_stop(self):\n    if hasattr(self._sess, \"should_stop\"):\n      return self._sess.should_stop()\n    else:\n      raise ValueError(\n          \"The wrapped session %r does not have a method called 'should_stop'. \"\n          \"Do you intend to wrap a tf.MonitoredSession instead?\" % self._sess)\n\n\nclass WatchOptions(object):\n  \"\"\"Type for return values of watch_fn.\"\"\""
},
{
    "Id": 195,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/67b6c880e39ba02ba53c7d499e45fd136090ee32",
    "Violation": "missing",
    "Bug report": " In tf.map_fn: skip sanity check for shape of first value in elems if it doesn't have a shape attribute. (E.g., this can happen if it's a CompsiteTensor.)",
    "Number of deleted lines": 4,
    "Deleted lines": "  if fn_output_signature is None:\n    # If fn_output_signature was not specified, then assume that it matches the\n    # input signature.\n    result_flat_signature = [\n        _most_general_compatible_type(s)._unbatch()  # pylint: disable=protected-access\n        for s in elems_flat_signature\n    ]\n    result_unflatten = elems_unflatten\n  else:\n    result_flat_signature = [\n        _dtype_to_spec(d) for d in nest.flatten(fn_output_signature)\n    ]\n    result_unflatten = lambda x: nest.pack_sequence_as(fn_output_signature, x)\n\n  with ops.name_scope(name, \"map\", elems_flat):\n    # TODO(akshayka): Remove the in_graph_mode check once caching devices are\n    # supported in Eager\n    if in_graph_mode:\n      # Any get_variable calls in fn will cache the first call locally\n      # and not issue repeated network I/O requests for each iteration.\n      varscope = vs.get_variable_scope()\n      varscope_caching_device_was_none = False\n      if varscope.caching_device is None:\n        # TODO(ebrevdo): Change to using colocate_with here and in other\n        # methods.\n        varscope.set_caching_device(lambda op: op.device)\n        varscope_caching_device_was_none = True\n\n    elems_flat = [\n        ops.convert_to_tensor_or_composite(t, name=\"elem\") for t in elems_flat\n    ]\n\n    # Check that inputs are not scalars.\n    first_elem = elems_flat[0]\n    elems_static_shape = first_elem.shape\n    if elems_static_shape.ndims is not None and elems_static_shape.ndims < 1:\n      raise ValueError(\n          \"Elements in elems must be 1+ dimensional Tensors, not scalars\")\n\n    # Box any composite tensors into tensor lists.\n    elems_batchable = _elems_flat_to_batchable(elems_flat)\n\n    # Find the number of iterations, n.  (may be known statically.)\n    n_static = tensor_shape.Dimension(\n        tensor_shape.dimension_value(\n            elems_batchable[0].get_shape().with_rank_at_least(1)[0]))\n    for tensor in elems_batchable[1:]:\n      n_static.assert_is_compatible_with(\n          tensor_shape.Dimension(\n              tensor_shape.dimension_value(\n                  tensor.get_shape().with_rank_at_least(1)[0])))\n    n = n_static.value or array_ops.shape(elems_batchable[0])[0]\n\n    # Convert elems to tensor array.\n    # TODO(edloper): Should we set infer_shape=False for composite tensors?\n    elems_batchable_ta = [\n        tensor_array_ops.TensorArray(\n            dtype=t.dtype, size=n, dynamic_size=False, infer_shape=True)\n        for t in elems_batchable\n    ]\n    # Unpack elements\n    elems_batchable_ta = [\n        ta.unstack(t) for (ta, t) in zip(elems_batchable_ta, elems_batchable)\n    ]\n\n    i = constant_op.constant(0)\n\n    # Prepare result tensor array.\n    # TODO(edloper): Should we set infer_shape=False for composite tensors?\n    result_batchable_tensor_spec = (\n        _result_flat_signature_to_batchable_tensor_spec(result_flat_signature))\n    result_batchable_ta = []\n    for spec in result_batchable_tensor_spec:\n      result_batchable_ta.append("
},
{
    "Id": 196,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dcdca11bcbab4b2474e7bf4d21d1806e6c2790a3",
    "Violation": "insufficient",
    "Bug report": " Check both output name and output slot in duplicate scope id sanity check. Before this change, we would throw an error if different outputs of a node were committed to different scope ids.  Since that is legal, this change fixes the bug by making the check based on both output name and output index.",
    "Number of deleted lines": 1,
    "Deleted lines": "}\n\n}  // namespace\n\nvoid ScopedAllocatorOptimizer::ExtendNodeAttr(StringPiece name,\n                                              const std::vector<int32>& values,\n                                              NodeDef* node_def) {\n  if (HasNodeAttr(*node_def, name)) {\n    VLOG(2) << \"extending\";\n    AttrValue* existing = &(*node_def->mutable_attr())[string(name)];\n    for (int32 i : values) {\n      existing->mutable_list()->add_i(i);\n    }\n  } else {\n    VLOG(2) << \"setting new attr value\";\n    AddNodeAttr(name, values, node_def);\n  }\n}\n\nclass UnaryElementwiseRewriter : public ScopedAllocatorOptimizer::Rewriter {\n public:\n  ~UnaryElementwiseRewriter() override {}\n\n  // Return non-OK if any input is already committed to a ScopedAllocator.\n  //\n  // We insert an identity to ensure that inputs are not committed to different\n  // scope ids in `MaybeRewriteInput`, so this function is basically a sanity\n  // check.\n  Status CheckExistingScopedAllocator(const std::vector<InputDesc>& inputs) {\n    for (const InputDesc& nd : inputs) {\n      VLOG(2) << \"get attrs for \" << nd.from_node_def->name();\n      AttrSlice n_attrs = AttrSlice(*nd.from_node_def);\n      std::vector<int32> scope_ids;\n      Status ss = GetNodeAttr(n_attrs, kScopedAllocatorAttrName, &scope_ids);\n      if (ss.ok()) {\n        LOG(INFO) << \"Abandoning ScopedAllocatorOptimizer because input \"\n                  << nd.from_node_def->name() << \" output \" << scope_ids[0]\n                  << \" is already assigned to scope_id \" << scope_ids[1];\n        return errors::Internal(\n            \"Abandoning ScopedAllocatorOptimizer because input \",\n            nd.from_node_def->name(), \" output \", scope_ids[0], \" is already \",\n            \"assigned to scope_id \", scope_ids[1]);\n      }\n    }\n    return Status::OK();\n  }\n\n  // Return non-OK if any input is a member of op_set.\n  Status CheckInternalDataDependency(const std::set<string>& op_set,\n                                     const std::vector<InputDesc>& inputs) {\n    for (const InputDesc& nd : inputs) {\n      if (op_set.find(nd.from_node_def->name()) != op_set.end()) {\n        if (nd.output_slot != tensorflow::Graph::kControlSlot) {\n          return errors::Internal(\"Data edge exists bewtween \",\n                                  nd.from_node_def->name(),\n                                  \" and another \"\n                                  \"node in the set\");\n        }\n      }\n    }\n    return Status::OK();\n  }\n\n  // Remove all control edges between members of ops.\n  void ClearInternalControlInputs(const std::set<string>& op_set,\n                                  const std::vector<NodeDef*>& ops,\n                                  NodeMap* node_map) {\n    for (NodeDef* n : ops) {\n      for (const auto& input_name : n->input()) {\n        if (IsControlInput(input_name)) {\n          int position = 0;"
},
{
    "Id": 197,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a607eb012b1bc4f6dbe263ad99caa76d84ae3ab2",
    "Violation": "insufficient",
    "Bug report": "fix output shape check for strided slice always failing when stride != 1 ",
    "Number of deleted lines": 3,
    "Deleted lines": "    Value<TensorRef<BHWC>>* input;\n    RETURN_IF_ERROR(reader->ReadValue(0, &input));\n    RETURN_IF_ERROR(graph->AddConsumer(node->id, input->id));\n\n    Tensor<Linear, DataType::INT32> tmp;\n    RETURN_IF_ERROR(reader->ReadTensor(1, &tmp));\n\n    bool read_without_batch = tmp.data.size() == 3;\n    bool read_with_batch = tmp.data.size() == 4;\n    if (!read_without_batch && !read_with_batch) {\n      return UnimplementedError(\n          \"Slicing is supported for 3 or 4 dimensional tensors only.\");\n    }\n\n    const auto* tf_options = reinterpret_cast<const TfLiteStridedSliceParams*>(\n        tflite_node->builtin_data);\n    auto out_shape = graph->FindOutputs(node->id)[0]->tensor.shape;\n    if (!tf_options) {\n      return InternalError(\"Missing tflite params\");\n    }\n    RETURN_IF_ERROR(CheckOptionsSupport(tf_options));\n\n    SliceAttributes attr;\n    if (read_without_batch) {\n      RETURN_IF_ERROR(ReadAttribsWithoutBatch(reader, tf_options,\n                                              input->tensor.shape, &attr));\n    }\n    if (read_with_batch) {\n      RETURN_IF_ERROR(\n          ReadAttribsWithBatch(reader, tf_options, input->tensor.shape, &attr));\n    }\n    if (attr.strides.h < 0 || attr.strides.w < 0 || attr.strides.c < 0) {\n      return UnimplementedError(\"Reverse slices are not supported.\");\n    }\n    if (attr.ends.h - attr.starts.h != out_shape.h) {\n      return UnimplementedError(\"Output height doesn't match\");\n    }\n    if (attr.ends.w - attr.starts.w != out_shape.w) {\n      return UnimplementedError(\"Output width doesn't match\");\n    }\n    if (attr.ends.c - attr.starts.c != out_shape.c) {\n      return UnimplementedError(\"Output channels don't match\");\n    }\n    node->operation.attributes = attr;\n    return OkStatus();\n  }\n\n private:\n  Status UpdateWithMask(const TfLiteStridedSliceParams* tf_options,\n                        const BHWC& input_shape, int ignore_h, int ignore_w,\n                        int ignore_c, SliceAttributes* attr) {\n    if (tf_options->begin_mask & ignore_h) {\n      attr->starts.h = 0;\n    }\n    if (tf_options->begin_mask & ignore_w) {\n      attr->starts.w = 0;\n    }\n    if (tf_options->begin_mask & ignore_c) {\n      attr->starts.c = 0;\n    }\n\n    if (tf_options->end_mask & ignore_h) {\n      attr->ends.h = input_shape.h;\n    }\n    if (tf_options->end_mask & ignore_w) {\n      attr->ends.w = input_shape.w;\n    }\n    if (tf_options->end_mask & ignore_c) {\n      attr->ends.c = input_shape.c;\n    }\n    return OkStatus();\n  }\n\n  Status UpdateIfNegative(const BHWC& input_shape, SliceAttributes* attr) {\n    if (attr->ends.h < 0) {\n      attr->ends.h = input_shape.h + attr->ends.h;\n    }"
},
{
    "Id": 198,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2bf2799ee80791107d4fe587ff9b6c7cf6c8b418",
    "Violation": "missing",
    "Bug report": "Fail gracefully if the serialized graph would be too large. Without this explicit check, a large graph would trigger an assertion failure in the protobuf codebase",
    "Number of deleted lines": 1,
    "Deleted lines": "    dst_len -= consumed;\n  }\n  if (dst != base + size) {\n    status->status = InvalidArgument(\n        \"invalid string tensor encoding (decoded \", (dst - base),\n        \" bytes, but the tensor is encoded in \", size, \" bytes\");\n    delete[] base;\n    return nullptr;\n  }\n\n  auto dims = src.shape().dim_sizes();\n  std::vector<tensorflow::int64> dimvec(dims.size());\n  for (size_t i = 0; i < dims.size(); ++i) {\n    dimvec[i] = dims[i];\n  }\n  static_assert(sizeof(int64_t) == sizeof(tensorflow::int64),\n                \"64-bit int types should match in size\");\n  return TF_NewTensor(TF_STRING,\n                      reinterpret_cast<const int64_t*>(dimvec.data()),\n                      dimvec.size(), base, size, DeleteArray, base);\n}\n\nStatus MessageToBuffer(const tensorflow::protobuf::Message& in,\n                       TF_Buffer* out) {\n  if (out->data != nullptr) {\n    return InvalidArgument(\"Passing non-empty TF_Buffer is invalid.\");\n  }\n  const size_t proto_size = in.ByteSizeLong();\n  void* buf = tensorflow::port::Malloc(proto_size);\n  if (buf == nullptr) {\n    return tensorflow::errors::ResourceExhausted(\n        \"Failed to allocate memory to serialize message of type '\",\n        in.GetTypeName(), \"' and size \", proto_size);\n  }\n  in.SerializeToArray(buf, proto_size);\n  out->data = buf;\n  out->length = proto_size;\n  out->data_deallocator = [](void* data, size_t length) {\n    tensorflow::port::Free(data);\n  };\n  return Status::OK();\n}\n\nvoid RecordMutation(TF_Graph* graph, const TF_Operation& op,\n                    const char* mutation_type) {\n  // If any session has already run this node_id, mark this session as\n  // unrunnable.\n  for (auto it : graph->sessions) {\n    mutex_lock session_lock(it.first->mu);\n    if (it.first->last_num_graph_nodes > op.node.id()) {\n      it.second = strings::StrCat(\n          \"Operation '\", op.node.DebugString(), \"' was changed by \",\n          mutation_type,\n          \" after it was run by a session. This mutation will have no effect, \"\n          \"and will trigger an error in the future. Either don't modify \"\n          \"nodes after running them or create a new session.\");\n    }\n  }\n}\n\nnamespace {\n\n// Helper method that creates a shape handle for a shape described by dims.\ntensorflow::shape_inference::ShapeHandle ShapeHandleFromDims(\n    tensorflow::shape_inference::InferenceContext* ic, int num_dims,\n    const int64_t* dims) {\n  if (num_dims != -1) {\n    std::vector<tensorflow::shape_inference::DimensionHandle> dim_vec;\n    dim_vec.reserve(num_dims);\n    for (int i = 0; i < num_dims; ++i) {\n      dim_vec.push_back(ic->MakeDim(dims[i]));"
},
{
    "Id": 199,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0197a2d8a3070af763cb67227835ee63df095e6d",
    "Violation": "missing",
    "Bug report": " Add a check to catch out-of-bound access on invalid Graphs. The existing Check trying to catch malformed graph is not robust when an op is registered with an expected number of inputs but has data edges beyond this.",
    "Number of deleted lines": 0,
    "Deleted lines": "  GraphDef ret;\n  ToGraphDef(&ret);\n  return ret;\n}\n\nvoid Graph::ToGraphDefSubRange(GraphDef* graph_def, int from_node_id) const {\n  graph_def->Clear();\n  *graph_def->mutable_versions() = versions();\n  *graph_def->mutable_library() = ops_.ToProto();\n\n  graph_def->mutable_node()->Reserve(std::max(1, num_nodes() - from_node_id));\n\n  std::vector<const Edge*>\n      inputs;  // Construct this outside the loop for speed.\n  for (auto id = from_node_id; id < num_node_ids(); ++id) {\n    const Node* node = FindNodeId(id);\n    if (node == nullptr || !node->IsOp()) continue;\n    NodeDef* node_def = graph_def->add_node();\n    *node_def = node->def();\n\n    // Use the node's assigned device, if any, instead of the device requested\n    // in the NodeDef.\n    if (!node->assigned_device_name().empty()) {\n      node_def->set_device(node->assigned_device_name());\n    }\n\n    // Get the inputs for this Node.  We make sure control inputs are\n    // after data inputs, as required by GraphDef.\n    inputs.clear();\n    inputs.resize(node->num_inputs(), nullptr);\n    for (const Edge* edge : node->in_edges()) {\n      if (edge->IsControlEdge()) {\n        inputs.push_back(edge);\n      } else {\n        CHECK(inputs[edge->dst_input()] == nullptr)\n            << \"Edge \" << edge->src()->DebugString() << \":\"\n            << edge->dst()->DebugString() << \" with dst_input \"\n            << edge->dst_input() << \" and had pre-existing input edge \"\n            << inputs[edge->dst_input()]->src()->DebugString() << \":\"\n            << inputs[edge->dst_input()]->dst()->DebugString();\n\n        inputs[edge->dst_input()] = edge;\n      }\n    }\n    // Sort the control inputs for more predictable serialization.\n    std::sort(inputs.begin() + node->num_inputs(), inputs.end(),\n              [](const Edge* a, const Edge* b) -> bool {\n                return a->src()->name() < b->src()->name();\n              });\n    node_def->clear_input();\n    node_def->mutable_input()->Reserve(inputs.size());\n\n    for (size_t i = 0; i < inputs.size(); ++i) {\n      const Edge* edge = inputs[i];\n      if (edge == nullptr) {\n        if (i < node->requested_inputs().size()) {\n          node_def->add_input(node->requested_inputs()[i]);\n        } else {\n          node_def->add_input(\"\");\n        }\n      } else {\n        const Node* src = edge->src();\n        if (!src->IsOp()) continue;\n        AddInput(node_def, src->name(), edge->src_output());\n      }\n    }\n  }\n}\n\nstring Graph::NewName(StringPiece prefix) {"
},
{
    "Id": 200,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/314d9cd9b607460f8bfea80fc828b1521ca18443",
    "Violation": "missing",
    "Bug report": "Fix segfault in MacOS when GPU is not available ",
    "Number of deleted lines": 1,
    "Deleted lines": "  auto stripped_kernel_version =\n      port::StripSuffixString(kernel_version, \".ld64\");\n  return StringToDriverVersion(stripped_kernel_version);\n}\n\nvoid Diagnostician::WarnOnDsoKernelMismatch(\n    port::StatusOr<DriverVersion> dso_version,\n    port::StatusOr<DriverVersion> kernel_version) {\n  if (kernel_version.ok() && dso_version.ok() &&\n      dso_version.ValueOrDie() == kernel_version.ValueOrDie()) {\n    LOG(INFO) << \"kernel version seems to match DSO: \"\n              << DriverVersionToString(kernel_version.ValueOrDie());\n  } else {\n    LOG(ERROR) << \"kernel version \"\n               << DriverVersionStatusToString(kernel_version)\n               << \" does not match DSO version \"\n               << DriverVersionStatusToString(dso_version)\n               << \" -- cannot find working devices in this configuration\";\n  }\n}\n\n\nport::StatusOr<DriverVersion> Diagnostician::FindKernelDriverVersion() {\n#if defined(__APPLE__)\n  CFStringRef kext_ids[1];\n  kext_ids[0] = kDriverKextIdentifier;\n  CFArrayRef kext_id_query = CFArrayCreate(nullptr, (const void**)kext_ids, 1, &kCFTypeArrayCallBacks);\n  CFDictionaryRef kext_infos = KextManagerCopyLoadedKextInfo(kext_id_query, nullptr);\n  CFRelease(kext_id_query);\n\n  CFDictionaryRef cuda_driver_info = nullptr;\n  if (CFDictionaryGetValueIfPresent(kext_infos, kDriverKextIdentifier, (const void**)&cuda_driver_info)) {\n    // NOTE: OSX CUDA driver does not currently store the same driver version\n    // in kCFBundleVersionKey as is returned by cuDriverGetVersion\n    const char * version = CFStringGetCStringPtr((CFStringRef)CFDictionaryGetValue(cuda_driver_info, kCFBundleVersionKey), kCFStringEncodingUTF8);\n    CFRelease(kext_infos);\n    return StringToDriverVersion(version);\n  }\n  CFRelease(kext_infos);\n  auto status =\n    port::Status{port::error::INTERNAL,\n                 port::StrCat(\"failed to read driver bundle version: \",\n                              CFStringGetCStringPtr(kDriverKextIdentifier, kCFStringEncodingUTF8))\n    };\n  return status;\n#else\n  FILE *driver_version_file = fopen(kDriverVersionPath, \"r\");\n  if (driver_version_file == nullptr) {\n    return port::Status{\n        port::error::PERMISSION_DENIED,\n        port::StrCat(\"could not open driver version path for reading: \",\n                     kDriverVersionPath)};\n  }\n\n  static const int kContentsSize = 1024;\n  port::InlinedVector<char, 4> contents(kContentsSize);\n  size_t retcode =\n      fread(contents.begin(), 1, kContentsSize - 2, driver_version_file);\n  if (retcode < kContentsSize - 1) {\n    contents[retcode] = '\\0';\n  }\n  contents[kContentsSize - 1] = '\\0';\n\n  if (retcode != 0) {\n    LOG(INFO) << \"driver version file contents: \\\"\\\"\\\"\" << contents.begin()\n              << \"\\\"\\\"\\\"\";\n    fclose(driver_version_file);\n    return FindKernelModuleVersion(string{contents.begin()});\n  }\n\n  auto status =\n      port::Status{port::error::INTERNAL,"
},
{
    "Id": 201,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/abd645085b1dd1496df847b05a1934d471a2f2c0",
    "Violation": "missing",
    "Bug report": " Use the correct device ordinal to check whether the device the executable was built for is equivalent to the device the it will run on. Before this patch, if the device to run on was provided via a stream without setting the device ordinal in the ExecutableRunOptions, we would check the default device against the device the executable was built for.",
    "Number of deleted lines": 5,
    "Deleted lines": "          \"Argument does not match host shape or layout of computation \"\n          \"parameter \"\n          \"%d: want %s, got %s\",\n          i,\n          ShapeUtil::HumanString(computation_layout.parameter_layout(i).shape())\n              .c_str(),\n          ShapeUtil::HumanString(arguments[i]->on_host_shape()).c_str());\n    }\n  }\n\n  if (run_options.stream() != nullptr) {\n    if (!run_options.stream()->ok()) {\n      return InvalidArgument(\"stream is uninitialized or in an error state\");\n    }\n\n    // Check stream matches service platform.\n    const se::Platform* stream_platform =\n        run_options.stream()->parent()->platform();\n    if (stream_platform != backend_->platform()) {\n      return InvalidArgument(\n          \"stream is for platform %s, but service targets platform %s\",\n          stream_platform->Name().c_str(),\n          backend_->platform()->Name().c_str());\n    }\n\n    // Cannot specify device_ordinal with a stream. The stream determines these\n    // values.\n    if (run_options.device_ordinal() != -1) {\n      return InvalidArgument(\n          \"cannot set both device ordinal and stream options in \"\n          \"ExecutableRunOptions; the stream determines the device ordinal\");\n    }\n  }\n\n  // Verify that the device the executable was built for is equivalent to the\n  // device it will run on.\n  int run_device_ordinal = run_options.device_ordinal() == -1\n                               ? backend_->default_device_ordinal()\n                               : run_options.device_ordinal();\n  TF_ASSIGN_OR_RETURN(bool devices_equivalent,\n                      backend_->devices_equivalent(\n                          run_device_ordinal, build_options_.device_ordinal()));\n  if (!devices_equivalent) {\n    TF_ASSIGN_OR_RETURN(se::StreamExecutor * run_executor,\n                        backend_->stream_executor(run_device_ordinal));\n    TF_ASSIGN_OR_RETURN(se::StreamExecutor * build_executor,\n                        backend_->stream_executor(build_device_ordinal()));\n    return InvalidArgument(\n        \"executable is built for device %s of type \\\"%s\\\"; cannot run it on \"\n        \"device %s of type \\\"%s\\\"\",\n        backend_->device_name(build_device_ordinal()).c_str(),\n        build_executor->GetDeviceDescription().name().c_str(),\n        backend_->device_name(run_device_ordinal).c_str(),\n        run_executor->GetDeviceDescription().name().c_str());\n  }\n\n  if (!run_options.allocator()) {\n    return InvalidArgument(\"an allocator must be provided to ExecuteLocally\");\n  }\n\n  if (run_options.allocator()->platform() != backend.platform()) {\n    return InvalidArgument(\n        \"allocator platform (%s) does not match service platform (%s)\",\n        run_options.allocator()->platform()->Name().c_str(),\n        backend.platform()->Name().c_str());\n  }\n\n  return Status::OK();\n}\n\nStatusOr<ScopedShapedBuffer> LocalExecutable::Run(\n    const tensorflow::gtl::ArraySlice<const ShapedBuffer*> arguments,\n    ExecutableRunOptions run_options) {\n  TF_RETURN_IF_ERROR(\n      ValidateExecutionOptions(arguments, run_options, *backend_));"
},
{
    "Id": 202,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/083fd8c4b23104f6b27a871c6469629ace4ee9c3",
    "Violation": "insufficient",
    "Bug report": " Don't check soname on Windows. This allow users to specify a certain CUDA version on Windows again.",
    "Number of deleted lines": 1,
    "Deleted lines": "        static: True the library is static or False if it is a shared object.\n\n      Returns:\n        The platform-specific name of the library.\n      \"\"\"\n    version = \"\" if not version else \".\" + version\n    if cpu_value in (\"Linux\", \"FreeBSD\"):\n        if static:\n            return \"lib%s.a\" % base_name\n        return \"lib%s.so%s\" % (base_name, version)\n    elif cpu_value == \"Windows\":\n        return \"%s.lib\" % base_name\n    elif cpu_value == \"Darwin\":\n        if static:\n            return \"lib%s.a\" % base_name\n        return \"lib%s%s.dylib\" % (base_name, version)\n    else:\n        auto_configure_fail(\"Invalid cpu_value: %s\" % cpu_value)\n\ndef find_lib(repository_ctx, paths, check_soname = True):\n    \"\"\"\n      Finds a library among a list of potential paths.\n\n      Args:\n        paths: List of paths to inspect.\n\n      Returns:\n        Returns the first path in paths that exist.\n    \"\"\"\n    objdump = repository_ctx.which(\"objdump\")\n    mismatches = []\n    for path in [repository_ctx.path(path) for path in paths]:\n        if not path.exists:\n            continue\n        if check_soname and objdump != None:\n            output = repository_ctx.execute([objdump, \"-p\", str(path)]).stdout\n            output = [line for line in output.splitlines() if \"SONAME\" in line]\n            sonames = [line.strip().split(\" \")[-1] for line in output]\n            if not any([soname == path.basename for soname in sonames]):\n                mismatches.append(str(path))\n                continue\n        return path\n    if mismatches:\n        auto_configure_fail(\n            \"None of the libraries match their SONAME: \" + \", \".join(mismatches),\n        )\n    auto_configure_fail(\"No library found under: \" + \", \".join(paths))\n\ndef _find_cuda_lib(\n        lib,\n        repository_ctx,\n        cpu_value,\n        basedir,\n        version,\n        static = False):\n    \"\"\"Finds the given CUDA or cuDNN library on the system.\n\n      Args:\n        lib: The name of the library, such as \"cudart\"\n        repository_ctx: The repository context.\n        cpu_value: The name of the host operating system.\n        basedir: The install directory of CUDA or cuDNN.\n        version: The version of the library.\n        static: True if static library, False if shared object.\n\n      Returns:\n        Returns the path to the library.\n      \"\"\"\n    file_name = lib_name(lib, cpu_value, version, static)\n    return find_lib(repository_ctx, [\n        \"%s/%s%s\" % (basedir, path, file_name)"
},
{
    "Id": 203,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/867a918bd3d40afeca6b96430671a098134e7905",
    "Violation": "missing",
    "Bug report": " CUDA Driver: do better error reporting if checking the pointer properties failed. There are many reasons why an operation can fail, propagate the error instead of assuming the cause.",
    "Number of deleted lines": 0,
    "Deleted lines": "// stack-limited threads (such as those spawned by a default-argument\n// thread::ThreadPool on some platforms), we run certain routines in this pool\n// and wait for completion.\nport::ThreadPool* GetDriverExecutor() {\n  static port::ThreadPool* thread_pool = new port::ThreadPool(\n      port::Env::Default(), port::ThreadOptions(), \"cuda_driver\", 1);\n  return thread_pool;\n}\n\n}  // namespace\n\nstring MemorySpaceString(MemorySpace memory_space) {\n  switch (memory_space) {\n    case MemorySpace::kHost:\n      return \"host\";\n    case MemorySpace::kDevice:\n      return \"device\";\n    default:\n      LOG(FATAL) << \"impossible memory space\";\n  }\n}\n\nnamespace {\n\n// Checks that the pointer is to a location on the device it purports to be.\n// PtrT is one of CUdeviceptr or void*.  If it's a CUdeviceptr, then\n// cudaPointerGetAttributes should not fail, and return a memoryType of\n// cudaMemoryTypeDevice.\ntemplate <typename PtrT>\nvoid CheckPointerIsValid(const PtrT ptr, absl::string_view name) {\n  bool is_host_ptr = !std::is_same<PtrT, CUdeviceptr>::value;\n  cudaPointerAttributes attributes;\n  cudaError_t err =\n      cudaPointerGetAttributes(&attributes, reinterpret_cast<const void*>(ptr));\n  // If we failed, reset cuda error status to avoid poisoning cuda streams.\n  if (err != cudaSuccess) cudaGetLastError();\n  bool points_to_host_memory = (err == cudaErrorInvalidValue ||\n                                attributes.memoryType != cudaMemoryTypeDevice);\n  CHECK_EQ(is_host_ptr, points_to_host_memory) << absl::StreamFormat(\n      \"%s pointer is not actually on %s: %p\", name, is_host_ptr ? \"CPU\" : \"GPU\",\n      reinterpret_cast<const void*>(ptr));\n}\n\n// Call cuCtxtSynchronize and crash if it doesn't succeed.\nvoid SynchronizeOrDie() {\n  auto res = cuCtxSynchronize();\n  if (res != CUDA_SUCCESS) {\n    LOG(FATAL) << \"Synchronize found \" << ToString(res)\n               << \" :: \" << port::CurrentStackTrace();\n  }\n}\n\nstruct ThreadLocalData {\n  int64 id;\n  GpuContext* context;  // Only valid if id == a known good context.\n  int depth;\n};\n\nSE_STATIC_THREAD_LOCAL_POD(ThreadLocalData, tls_data);\n\n}  // namespace\n\nScopedActivateContext::ScopedActivateContext(GpuContext* cuda_context) {\n  if (FLAGS_gpuexec_cuda_sync_around_driver_calls) SynchronizeOrDie();\n\n  auto* tls = &tls_data.get();\n  tls->depth++;\n  if (tls->id == cuda_context->id()) {\n    if (kVerifyGpuContext) {\n      CHECK_EQ(CurrentContext(), cuda_context->context());"
},
{
    "Id": 204,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1a73fdfa83bd50695a7d374d14a5cb3835d94d9e",
    "Violation": "missing",
    "Bug report": " Add extra check incase segmenter does not exclude CPU in order to prevent segfault",
    "Number of deleted lines": 4,
    "Deleted lines": "}\n\nstruct EdgePtrCompare {\n  bool operator()(const tensorflow::Edge* lhs,\n                  const tensorflow::Edge* rhs) const {\n    return lhs->id() < rhs->id();\n  }\n};\n\n// Function to get subsegment information structure.\ntensorflow::Status GetEngineInfo(\n    const tensorflow::Graph* g,\n    const tensorflow::grappler::GraphProperties& graph_properties,\n    const std::set<string>& segment_nodes,\n    const std::unordered_map<string, tensorflow::Node*>& node_map,\n    const std::vector<tensorflow::Node*>& reverse_topo_order,\n    EngineInfo* info) {\n  std::vector<int> subgraph_node_ids;  // Topologically sorted node ids.\n  std::set<string> subgraph_node_names = segment_nodes;\n  std::set<int> added_const_node_ids;  // Used to prevent double insertion.\n  std::set<string> segment_devices;\n\n  // Map from src_node_name+port to the unique port numbers of the TRT op, where\n  // the src_node_name is the name of the source node of the input/output\n  // edge, thus there must not be any duplicates since source nodes of\n  // input/output edges must be in different split of the graph.\n  // TODO(aaroey): consider using node id and port instead.\n  // TODO(aaroey): using topo order instead of reverting reverse topo order.\n  std::unordered_map<string, int> input_to_engine_port, output_to_engine_port;\n  for (auto it = reverse_topo_order.rbegin(); it != reverse_topo_order.rend();\n       ++it) {\n    const auto& node_name = (*it)->name();\n    if (segment_nodes.count(node_name) == 0) continue;\n    auto node = *it;\n    // TODO: check for CPU device here\n    // If device is CPU, we should've caught that in the segmenter. Fall back here.\n\n    auto node_device = node->requested_device();\n    if (!node_device.empty()) {\n      segment_devices.insert(node_device);\n    } else {\n      if (node->has_assigned_device_name()) {\n        segment_devices.insert(node->assigned_device_name());\n      } else {\n        VLOG(2) << \"Node \" << node->name()\n                << \" neither have requested device nor assigned device\";\n      }\n    }\n    const int node_id = node->id();\n    subgraph_node_ids.push_back(node_id);\n    // Create input connections. Sort edges first to make determnistic since\n    // in_edges is a set of pointers.\n    std::vector<const tensorflow::Edge*> in_edges(node->in_edges().begin(),\n                                                  node->in_edges().end());\n    std::sort(in_edges.begin(), in_edges.end(), EdgePtrCompare());\n    for (const auto edge : in_edges) {\n      auto input_node = edge->src();\n      if (input_node->IsSource() || segment_nodes.count(input_node->name())) {\n        continue;\n      }\n      if (edge->IsControlEdge()) {\n        // Control input.\n        info->connections.emplace_back(input_node->name(), input_node->id(),\n                                       node_name, node_id,\n                                       /*input_edge=*/true);\n      } else if (input_node->type_string() == \"Const\") {\n        // Add constant data input nodes into the segment graphdef (thus also in\n        // the engine). We don't care if it has other output edges going into\n        // other engines or TF nodes. Since we add it only to the segment\n        // graphdef, not the segment itself, it won't be removed from the graph.\n        // If it doesn't have any edges, TF will prune it out.\n        //\n        // Note that the segmenter already ensure that the constant data input\n        // is valid and suppported by the engine.\n        if (!added_const_node_ids.insert(input_node->id()).second) {\n          // Already added before."
},
{
    "Id": 205,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b234ff0ee4ce87d21a3e5306b678e1fb4b1fedfc",
    "Violation": "missing",
    "Bug report": " Fixed division by zero, by checking the number of GPUs in GenericLayoutOptimizer.",
    "Number of deleted lines": 0,
    "Deleted lines": "  }\n  return {num_gpus, num_volta};\n}\n\ninline bool NumConv2DOnDeviceWithDataTypeOverThreshold(\n    const TransposeContext& context, absl::string_view device,\n    const DataType& data_type) {\n  int num_conv2d_gpu = 0;\n  int num_conv2d_gpu_fp16 = 0;\n\n  for (const auto& node : context.graph_view->GetNodes()) {\n    const auto* node_def = node.node();\n    if (!IsConv2D(*node_def)) {\n      continue;\n    }\n    const string& device_name =\n        GetDeviceName(context.virtual_placer.get(), *node_def);\n    string device_type;\n    string task;\n    if (!DeviceNameUtils::SplitDeviceName(device_name, &task, &device_type) ||\n        !absl::StrContains(absl::AsciiStrToLower(device_type),\n                           absl::AsciiStrToLower(device))) {\n      continue;\n    }\n    num_conv2d_gpu++;\n    const auto* t_attr = node.GetAttr(\"T\");\n    if (t_attr == nullptr) {\n      continue;\n    }\n    if (t_attr->type() == data_type) {\n      num_conv2d_gpu_fp16++;\n    }\n  }\n\n  return (static_cast<float>(num_conv2d_gpu_fp16) /\n          static_cast<float>(num_conv2d_gpu)) >= kConv2DGPUFP16Threshold;\n}\n\ninline std::pair<string, string> GetSrcAndDstDataFormats(\n    const TransposeContext& context, int num_gpus, int num_voltas) {\n  string src_format = kNHWC;\n  string dst_format = kNCHW;\n  if (((static_cast<float>(num_voltas) / static_cast<float>(num_gpus)) >=\n       kVoltaGPURatioThreshold) &&\n      NumConv2DOnDeviceWithDataTypeOverThreshold(context, kGPU, DT_HALF)) {\n    std::swap(src_format, dst_format);\n  }\n  return {src_format, dst_format};\n}\n\nStatus ExpandLayoutSensitiveOp(TransposeContext* context,\n                               TransposerFactory* transposer_factory) {\n  const int num_nodes = context->num_nodes;\n  for (int i = 0; i < num_nodes; ++i) {\n    auto* node_view = context->graph_view->GetNode(i);\n    auto* node_def = node_view->node();\n    if (IsLayoutSensitiveOp(*node_def)) {\n      std::shared_ptr<Transposer> transposer =\n          transposer_factory->GetTransposer(*node_def);\n      if (transposer == nullptr) {\n        return Status(\n            error::NOT_FOUND,\n            absl::StrCat(\n                \"Layout sensitive operation should have a transposer. Node: \",\n                node_def->DebugString()));\n      }\n      TF_RETURN_IF_ERROR(transposer->TransposeNode(context, node_view));\n    }\n  }\n  return Status::OK();"
},
{
    "Id": 206,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9f8ad5ff118166537d42f87f1ee254f83ba553f0",
    "Violation": "improper",
    "Bug report": "Fix CUDA version check (format is 1000 * major + 10 * minor). ",
    "Number of deleted lines": 1,
    "Deleted lines": "You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/stream_executor/gpu/asm_compiler.h\"\n#include \"tensorflow/stream_executor/gpu/gpu_diagnostics.h\"\n#include \"tensorflow/stream_executor/gpu/gpu_driver.h\"\n\nnamespace stream_executor {\n\n#define RETURN_IF_CUDA_ERROR(expr)                                            \\\n  do {                                                                        \\\n    CUresult _status = expr;                                                  \\\n    if (!SE_PREDICT_TRUE(_status == CUDA_SUCCESS)) {                          \\\n      const char* error_string;                                               \\\n      cuGetErrorString(_status, &error_string);                               \\\n      std::ostringstream oss;                                                 \\\n      oss << error_string << \"\\nin \" << __FILE__ << \"(\" << __LINE__ << \"): '\" \\\n          << #expr << \"'\";                                                    \\\n      return port::Status(port::error::UNKNOWN, oss.str().c_str());           \\\n    }                                                                         \\\n  } while (false)\n\nport::StatusOr<std::vector<uint8>> LinkGpuAsm(\n    gpu::GpuContext* context, std::vector<CubinOrPTXImage> images) {\n  const bool linking_supported = [] {\n    if (CUDA_VERSION < 11300) {\n      return true;\n    }\n    auto version_or_status = gpu::Diagnostician::FindKernelDriverVersion();\n    if (!version_or_status.ok()) {\n      LOG(WARNING) << \"Couldn't read CUDA driver version.\";\n      return false;\n    }\n    return std::get<0>(*version_or_status) >= 465;\n  }();\n\n  if (!linking_supported) {\n    return tensorflow::errors::Unimplemented(\"Linking is unsupported\");\n  }\n\n  gpu::ScopedActivateContext activation(context);\n\n  CUlinkState link_state;\n  RETURN_IF_CUDA_ERROR(cuLinkCreate(0, nullptr, nullptr, &link_state));\n  for (auto& image : images) {\n    auto status = cuLinkAddData(link_state, CU_JIT_INPUT_CUBIN,\n                                static_cast<void*>(image.bytes.data()),\n                                image.bytes.size(), \"\", 0, nullptr, nullptr);\n    if (status != CUDA_SUCCESS) {\n      LOG(ERROR) << \"cuLinkAddData fails. This is usually caused by stale \"\n                    \"driver version.\";\n    }\n    RETURN_IF_CUDA_ERROR(status);\n  }\n  void* cubin_out;\n  size_t cubin_size;\n  RETURN_IF_CUDA_ERROR(cuLinkComplete(link_state, &cubin_out, &cubin_size));\n  std::vector<uint8> cubin(static_cast<uint8*>(cubin_out),\n                           static_cast<uint8*>(cubin_out) + cubin_size);\n  RETURN_IF_CUDA_ERROR(cuLinkDestroy(link_state));\n  return std::move(cubin);\n}"
},
{
    "Id": 207,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/89334fb15c688e7dbd81878745755db01579ea70",
    "Violation": "missing",
    "Bug report": " [NVIDIA] Update VersionCheck APIs for CuDNN. This PR updates the cudnnXXXVersionCheck to the latest for the next CUDNN release.",
    "Number of deleted lines": 2,
    "Deleted lines": "\ntsl::StatusOr<int> GetCudnnProperty(libraryPropertyType type) {\n  int value;\n  RETURN_IF_CUDNN_ERROR(cudnnGetProperty(type, &value));\n  return value;\n}\n\ncudnnRNNAlgo_t ToCudnnRNNAlgo(std::optional<dnn::AlgorithmDesc> algorithm) {\n  if (!algorithm.has_value()) {\n    return CUDNN_RNN_ALGO_STANDARD;\n  }\n  cudnnRNNAlgo_t algo = static_cast<cudnnRNNAlgo_t>(algorithm->algo_id());\n  switch (algo) {\n    case CUDNN_RNN_ALGO_STANDARD:\n    case CUDNN_RNN_ALGO_PERSIST_STATIC:\n    case CUDNN_RNN_ALGO_PERSIST_DYNAMIC:\n      return algo;\n    default:\n      LOG(FATAL) << \"Unsupported Cudnn RNN algorithm: \" << algorithm->algo_id();\n  }\n}\n\ntsl::StatusOr<dnn::VersionInfo> GetLoadedCudnnVersion() {\n  TF_ASSIGN_OR_RETURN(int major, GetCudnnProperty(MAJOR_VERSION));\n  TF_ASSIGN_OR_RETURN(int minor, GetCudnnProperty(MINOR_VERSION));\n  TF_ASSIGN_OR_RETURN(int patch_level, GetCudnnProperty(PATCH_LEVEL));\n  return dnn::VersionInfo(major, minor, patch_level);\n}\n\nenum class PreloadCudnnType { ConvFwd, ConvBwdFilter, ConvBwdData, Rnn };\n\n// Preload sub libs for cudnn 8.0.4+ to make sure that the loading time isn't\n// measured in the autotuning.\nvoid PreloadCudnnSubLibs(PreloadCudnnType type) {\n#if CUDNN_VERSION >= 8004\n  switch (type) {\n    case PreloadCudnnType::ConvBwdFilter:\n    case PreloadCudnnType::ConvBwdData: {\n      cudnnOpsTrainVersionCheck();\n      cudnnCnnTrainVersionCheck();\n      [[clang::fallthrough]];\n    }\n    case PreloadCudnnType::ConvFwd: {\n      cudnnOpsInferVersionCheck();\n      cudnnCnnInferVersionCheck();\n      break;\n    }\n    case PreloadCudnnType::Rnn: {\n      cudnnOpsInferVersionCheck();\n      cudnnAdvInferVersionCheck();\n      cudnnOpsTrainVersionCheck();\n      cudnnAdvTrainVersionCheck();\n      break;\n    }\n  }\n#endif  // CUDNN_VERSION >= 8004\n}\n\nvoid PreloadCudnnSubLibsHelper(dnn::ConvolutionKind kind) {\n  switch (kind) {\n    case dnn::ConvolutionKind::FORWARD:\n    case dnn::ConvolutionKind::FORWARD_GRAPH: {\n      PreloadCudnnSubLibs(PreloadCudnnType::ConvFwd);\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_DATA: {\n      PreloadCudnnSubLibs(PreloadCudnnType::ConvBwdData);\n      break;\n    }\n    case dnn::ConvolutionKind::BACKWARD_FILTER: {\n      PreloadCudnnSubLibs(PreloadCudnnType::ConvBwdFilter);\n      break;\n    }\n    default: {\n      LOG(WARNING) << \"Unsupported dnn::ConvolutionKind: \"\n                   << static_cast<int>(kind) << \" for cuDNN preload.\";\n      break;\n    }\n  }\n}\n\n}  // namespace\n\nCudnnSupport::CudnnSupport(GpuExecutor* parent) : parent_(parent) {}\n\ntsl::Status CudnnSupport::Init() {\n  ScopedActivateExecutorContext context(parent_);\n\n  // Peek at the last error to give more information in cases of errors.\n  cudaError_t cuda_error = cudaPeekAtLastError();\n  if (cuda_error != cudaSuccess) {\n    // Printing the cuda_error value is useful when cudaGetErrorName doesn't"
},
{
    "Id": 208,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e5cfbd0eceb4dca98b388b13acff499a5420f863",
    "Violation": "improper",
    "Bug report": "Fix more for cuda version check. ",
    "Number of deleted lines": 1,
    "Deleted lines": "    hash_code_ = Hash64Combine(hash_code_, dtype);\n    hash_code_ = Hash64Combine(hash_code_, device_id);\n  }\n  bool operator==(const ConvParameters& other) const {\n    return this->get_data_as_tuple() == other.get_data_as_tuple();\n  }\n\n  bool operator!=(const ConvParameters& other) const {\n    return !(*this == other);\n  }\n  uint64 hash() const { return hash_code_; }\n\n  string ToString() const {\n    // clang-format off\n    return strings::StrCat(\n        batch_, \", \", in_depths_, \", \",\n        \"(\", str_util::Join(in_, \", \"), \"), \",\n        out_depths_, \", \",\n        \"(\", str_util::Join(filter_, \", \"), \"), \",\n        \"(\", str_util::Join(dilation_, \", \"), \"), \",\n        \"(\", str_util::Join(stride_, \", \"), \"), \",\n        \"(\", str_util::Join(padding_, \", \"), \"), \",\n        dtype_, \", \",\n        device_id_);\n    // clang-format on\n  }\n\n  // The purpose of this function is to disable winograd nonfused conv algorithm\n  // for certain input parameters so as to avoid a bug in cuDNNv5 and cuDNNv6.\n  template <typename T>\n  bool ShouldIncludeWinogradNonfusedAlgo(\n      perftools::gputools::StreamExecutor* stream_exec) const {\n    // Skip this check for cuDNN 7 and newer.\n    auto version = stream_exec->AsDnn()->GetVersion();\n    if (version.ok() && std::get<0>(version.ValueOrDie()) >= 7) {\n      return true;\n    }\n    return ShouldIncludeWinogradNonfusedAlgoPreCudnn7<T>();\n  }\n\n protected:\n  using ParameterDataType =\n      std::tuple<int64, int64, SpatialArray, int64, SpatialArray, SpatialArray,\n                 SpatialArray, SpatialArray, DataType, int>;\n\n  ParameterDataType get_data_as_tuple() const {\n    return std::make_tuple(batch_, in_depths_, in_, out_depths_, filter_,\n                           dilation_, stride_, padding_, dtype_, device_id_);\n  }\n\n  uint64 hash_code_;\n\n private:\n  friend struct ConvParametersPeer;  // For testing purposes.\n\n  template <typename T>\n  bool ShouldIncludeWinogradNonfusedAlgoPreCudnn7() const {\n    int64 total_size = 16 * std::ceil(batch_ / 16.0) *\n                       std::max(in_depths_, out_depths_) * in_[0] * in_[1] *\n                       sizeof(T);\n    int64 threshold = 1LL << 31;\n    if (total_size >= threshold) {\n      return false;\n    } else {\n      return true;\n    }\n  }\n\n  int64 batch_;\n  int64 in_depths_;\n  int64 out_depths_;"
},
{
    "Id": 209,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e009644f034fa0ca4df910a812432cab3458d440",
    "Violation": "missing",
    "Bug report": "Add one error check in cuda_dnn for int8 to float convolution. ",
    "Number of deleted lines": 0,
    "Deleted lines": "    // memory. See nvbugs/2138754, b/80018418.\n    if (CUDNN_VERSION < 7300) {\n      if (algorithm_desc.algo_id() != CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING) {\n        return port::Status::OK();\n      }\n      if (input_descriptor.ndims() < 3) {\n        return port::Status::OK();\n      }\n      // Checks that a*b is within the valid range (as provided by NVIDIA).\n      const auto check_sizes = [](size_t a, size_t b) {\n        if ((a * b * 4608 - 1) >> 31 == 0) {\n          return port::Status::OK();\n        }\n        return port::Status(\n            port::error::FAILED_PRECONDITION,\n            \"This configuration potentially accesses illegal memory.\");\n      };\n      SE_RETURN_IF_ERROR(check_sizes(input_descriptor.feature_map_count(),\n                                     output_descriptor.feature_map_count()));\n      SE_RETURN_IF_ERROR(check_sizes(input_descriptor.count(),\n                                     input_descriptor.feature_map_count()));\n      SE_RETURN_IF_ERROR(check_sizes(input_descriptor.count(),\n                                     output_descriptor.feature_map_count()));\n      return port::Status::OK();\n    }\n    if (algorithm_desc.algo_id() ==\n            CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED &&\n        !ShouldIncludeWinogradNonfusedAlgo(input_descriptor,\n                                           output_descriptor)) {\n      return port::Status(\n          port::error::FAILED_PRECONDITION,\n          \"This configuration has potential integer overflow in \"\n          \"cuDNNv5 and cuDNNv6. See b/68264959.\");\n    }\n    return port::Status::OK();\n  };\n\n  auto get_bwd_data_bugs = [&]() -> port::Status {\n    if (algorithm_desc.algo_id() ==\n            CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED &&\n        !ShouldIncludeWinogradNonfusedAlgo(input_descriptor,\n                                           output_descriptor)) {\n      return port::Status(\n          port::error::FAILED_PRECONDITION,\n          \"This configuration has potential integer overflow in \"\n          \"cuDNNv5 and cuDNNv6. See b/68264959.\");\n    }\n\n    // Cudnn 7.1.4 has a bug if the workspace of the following convolution is\n    // not zero-initialized, nvbugs/2254619.\n    if (CUDNN_VERSION >= 7000 && CUDNN_VERSION < 7300 &&\n        algorithm_desc.algo_id() == CUDNN_CONVOLUTION_BWD_DATA_ALGO_1 &&\n        cudnn_type == CUDNN_DATA_HALF && algorithm_desc.tensor_ops_enabled() &&\n        input_descriptor.layout() == dnn::DataLayout::kBatchYXDepth &&\n        filter_descriptor.layout() == dnn::FilterLayout::kOutputInputYX &&\n        output_descriptor.layout() == dnn::DataLayout::kBatchDepthYX &&\n        (convolution_descriptor.vertical_filter_stride() > 1 ||\n         convolution_descriptor.horizontal_filter_stride() > 1)) {\n      stream->ThenMemZero(&scratch_memory, scratch_memory.size());\n    }\n    return port::Status::OK();\n  };\n\n  const auto get_bwd_filter_bugs = [&]() -> port::Status {\n    // Report an error if we might be hitting a cuDNN bug that produces\n    // incorrect results. See nvbugs/2072856\n    if (CUDNN_VERSION < 7300) {\n      SE_RETURN_IF_ERROR([&] {\n        if (algorithm_desc.algo_id() !=\n            CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING) {"
},
{
    "Id": 210,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/70ade1b64f65d0a2275672d27129627ff116a997",
    "Violation": "missing",
    "Bug report": " Fix defect: shuffle_batch gives ZeroDivisionError when computing capacity stat. * Fix defect: shuffle_batch gives ZeroDivisionError when computing capacity stat. * Cover < case in error checking",
    "Number of deleted lines": 0,
    "Deleted lines": "  tensor_list_list = _as_tensor_list_list(tensors_list)\n  with ops.name_scope(name, \"batch_join\",\n                      _flatten(tensor_list_list) + [keep_input]) as name:\n    tensor_list_list = _validate_join(tensor_list_list)\n    keep_input = _validate_keep_input(keep_input, enqueue_many)\n    tensor_list_list, sparse_info = _store_sparse_tensors_join(\n        tensor_list_list, enqueue_many, keep_input)\n    types = _dtypes(tensor_list_list)\n    shapes = _shapes(tensor_list_list, shapes, enqueue_many)\n    # TODO(josh11b,mrry): Switch to BatchQueue once it is written.\n    queue = _which_queue(dynamic_pad)(\n        capacity=capacity, dtypes=types, shapes=shapes, shared_name=shared_name)\n    _enqueue_join(queue, tensor_list_list, enqueue_many, keep_input)\n    summary.scalar(\"fraction_of_%d_full\" % capacity,\n                   math_ops.cast(queue.size(), dtypes.float32) *\n                   (1. / capacity))\n\n    if allow_smaller_final_batch:\n      dequeued = queue.dequeue_up_to(batch_size, name=name)\n    else:\n      dequeued = queue.dequeue_many(batch_size, name=name)\n    dequeued = _restore_sparse_tensors(dequeued, sparse_info)\n    # tensors_list was validated to not be empty.\n    return _as_original_type(tensors_list[0], dequeued)\n\n\ndef _shuffle_batch(tensors, batch_size, capacity, min_after_dequeue,\n                   keep_input, num_threads=1, seed=None, enqueue_many=False,\n                   shapes=None, allow_smaller_final_batch=False,\n                   shared_name=None, name=None):\n  \"\"\"Helper function for `shuffle_batch` and `maybe_shuffle_batch`.\"\"\"\n  tensor_list = _as_tensor_list(tensors)\n  with ops.name_scope(name, \"shuffle_batch\",\n                      list(tensor_list) + [keep_input]) as name:\n    tensor_list = _validate(tensor_list)\n    keep_input = _validate_keep_input(keep_input, enqueue_many)\n    tensor_list, sparse_info = _store_sparse_tensors(\n        tensor_list, enqueue_many, keep_input)\n    types = _dtypes([tensor_list])\n    shapes = _shapes([tensor_list], shapes, enqueue_many)\n    queue = data_flow_ops.RandomShuffleQueue(\n        capacity=capacity, min_after_dequeue=min_after_dequeue, seed=seed,\n        dtypes=types, shapes=shapes, shared_name=shared_name)\n    _enqueue(queue, tensor_list, num_threads, enqueue_many, keep_input)\n    full = (math_ops.cast(math_ops.maximum(0, queue.size() - min_after_dequeue),\n                          dtypes.float32) *\n            (1. / (capacity - min_after_dequeue)))\n    # Note that name contains a '/' at the end so we intentionally do not place\n    # a '/' after %s below.\n    summary_name = (\n        \"fraction_over_%d_of_%d_full\" %\n        (min_after_dequeue, capacity - min_after_dequeue))\n    summary.scalar(summary_name, full)\n\n    if allow_smaller_final_batch:\n      dequeued = queue.dequeue_up_to(batch_size, name=name)\n    else:\n      dequeued = queue.dequeue_many(batch_size, name=name)\n    dequeued = _restore_sparse_tensors(dequeued, sparse_info)\n    return _as_original_type(tensors, dequeued)\n\n\ndef _shuffle_batch_join(tensors_list, batch_size, capacity,\n                        min_after_dequeue, keep_input, seed=None,\n                        enqueue_many=False, shapes=None,\n                        allow_smaller_final_batch=False, shared_name=None,\n                        name=None):\n  \"\"\"Helper function for `shuffle_batch_join` and `maybe_shuffle_batch_join`.\"\"\"\n  tensor_list_list = _as_tensor_list_list(tensors_list)\n  with ops.name_scope(name, \"shuffle_batch_join\","
},
{
    "Id": 211,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a21ec782601aca6c7e0461093d72596f26229e44",
    "Violation": "improper",
    "Bug report": " Use getattr instead of isinstance in tensor_conversion_registry. Using `isinstance` to check if an object is an instance of a Python `typing.Protocol` instead of using `getattr`/`hasattr` has negative performance implications. This change reverts `tensor_conversion_registry.convert()` to use `getattr` for this reason.",
    "Number of deleted lines": 2,
    "Deleted lines": "    value: An object whose type has a registered `Tensor` conversion function.\n    dtype: Optional element type for the returned tensor. If missing, the type\n      is inferred from the type of `value`.\n    name: Optional name to use if a new `Tensor` is created.\n    as_ref: Optional boolean specifying if the returned value should be a\n      reference-type `Tensor` (e.g. Variable). Pass-through to the registered\n      conversion function. Defaults to `False`.\n    preferred_dtype: Optional element type for the returned tensor.\n      Used when dtype is None. In some cases, a caller may not have a dtype\n      in mind when converting to a tensor, so `preferred_dtype` can be used\n      as a soft preference. If the conversion to `preferred_dtype` is not\n      possible, this argument has no effect.\n    accepted_result_types: Optional collection of types as an allow-list\n      for the returned value. If a conversion function returns an object\n      which is not an instance of some type in this collection, that value\n      will not be returned.\n\n  Returns:\n    A `Tensor` converted from `value`.\n\n  Raises:\n    ValueError: If `value` is a `Tensor` and conversion is requested\n      to a `Tensor` with an incompatible `dtype`.\n    TypeError: If no conversion function is registered for an element in\n      `values`.\n    RuntimeError: If a registered conversion function returns an invalid\n      value.\n  \"\"\"\n\n  if dtype is not None:\n    dtype = dtypes.as_dtype(dtype)\n  if preferred_dtype is not None:\n    preferred_dtype = dtypes.as_dtype(preferred_dtype)\n\n  if isinstance(value, core.TensorProtocol):\n    return value.__tf_tensor__(dtype, name)\n\n  for base_type, conversion_func in get(type(value)):\n    # If dtype is None but preferred_dtype is not None, we try to\n    # cast to preferred_dtype first.\n    ret = None\n    if dtype is None and preferred_dtype is not None:\n      try:\n        ret = conversion_func(\n            value, dtype=preferred_dtype, name=name, as_ref=as_ref)\n      except (TypeError, ValueError):\n        # Could not coerce the conversion to use the preferred dtype.\n        pass\n      else:\n        if (ret is not NotImplemented and\n            ret.dtype.base_dtype != preferred_dtype.base_dtype):\n          raise RuntimeError(\n              _add_error_prefix(\n                  f\"Conversion function {conversion_func!r} for type \"\n                  f\"{base_type} returned incompatible dtype: requested = \"\n                  f\"{preferred_dtype.base_dtype.name}, \"\n                  f\"actual = {ret.dtype.base_dtype.name}\",\n                  name=name))\n\n    if ret is None:\n      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n\n    if ret is NotImplemented:\n      continue\n\n    if not isinstance(ret, accepted_result_types):\n      raise RuntimeError(\n          _add_error_prefix(\n              f\"Conversion function {conversion_func!r} for type \"\n              f\"{base_type} returned non-Tensor: {ret!r}\",\n              name=name))\n    if dtype and not dtype.is_compatible_with(ret.dtype):"
},
{
    "Id": 212,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e1ad3b74ad44b883c7b3fdc3a19adcea1d28bfbc",
    "Violation": "improper",
    "Bug report": " [XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast is an effective scalar. This short-circuit avoids crashing within last_dimension when attempting to match and either the operand or the result of the bitcast has a shape with rank 0.",
    "Number of deleted lines": 1,
    "Deleted lines": "                                  const GpuVersion& gpu_version) {\n  // TODO(bchetioui): expand with non-trivial instructions.\n  if (instr->IsElementwise()) {\n    if (instr->opcode() == HloOpcode::kConvert &&\n        (instr->operand(0)->shape().element_type() == BF16 ||\n         instr->shape().element_type() == BF16) &&\n        !std::get<se::CudaComputeCapability>(gpu_version)\n             .IsAtLeast(stream_executor::CudaComputeCapability::AMPERE)) {\n      return false;\n    }\n    return IsTritonSupportedElementwise(instr->opcode(),\n                                        instr->shape().element_type());\n  }\n\n  switch (instr->opcode()) {\n    case HloOpcode::kBitcast:\n    case HloOpcode::kParameter:\n      return true;\n    default:\n      return false;\n  }\n}\n\n// Returns true if a trivially connected producer of 'consumer' with opcode\n// 'opcode' exists. If such an instruction is found, the value of 'producer' is\n// set to it. The definition of \"trivial\" operations is as given in\n// 'IsTriviallyFusible'.\nbool TrivialEdge(HloInstruction** producer, HloInstruction* consumer,\n                 HloOpcode opcode, const GpuVersion& gpu_version);\n\nbool BitcastIsTilingNoop(HloInstruction* bitcast,\n                         const GpuVersion& gpu_version) {\n  CHECK_EQ(bitcast->opcode(), HloOpcode::kBitcast);\n\n  if (bitcast->shape().rank() == 0) {\n    return true;\n  }\n\n  // In the Softmax rewriter for now, tiling is derived from a hero reduction\n  // operation, which should be reducing its input on the last axis. Therefore,\n  // a bitcast is always a no-op with regards to a tile if\n  //   (1) it does not change the size of the reduction dimension of its input\n  //       (the last one); if its input is already reduced, then (1) is true\n  //       by default\n  //   (2) the layout of its output is ordered in the same way as the layout of\n  //       its input. This is a fuzzy definition, but since we assume fusible\n  //       ops to always have a default layout, we can just check if both the\n  //       bitcast and its input have a default layout\n  auto last_dimension = [](const HloInstruction* instr) {\n    return instr->shape().dimensions().back();\n  };\n\n  HloInstruction* reduce = nullptr;\n  TrivialEdge(&reduce, bitcast->mutable_operand(0), HloOpcode::kReduce,\n              gpu_version);\n\n  return (HasDefaultLayout(bitcast->shape()) &&\n          HasDefaultLayout(bitcast->operand(0)->shape()) &&\n          (reduce != nullptr ||\n           last_dimension(bitcast->operand(0)) == last_dimension(bitcast)));\n}\n\nbool IsTriviallyFusible(HloInstruction* instr, const GpuVersion& gpu_version,\n                        int num_allowed_users = 1) {\n  // Checks whether an op is trivially fusible. An op is said to be trivially\n  // fusible if it does not increase the amount of memory read/written by the\n  // resulting fusion, is compatible with any chosen tiling, and can be\n  // codegen'd using Triton. The op is allowed to have up to num_allowed_users\n  // users.\n  if (instr->user_count() > num_allowed_users ||\n      !HasDefaultLayout(instr->shape())) {"
},
{
    "Id": 213,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2f3b69e4976d3b14eaa6ae070eb68f37d1556d98",
    "Violation": "improper",
    "Bug report": "Changed empty check ",
    "Number of deleted lines": 3,
    "Deleted lines": "          self._checkpoint.slot_restorations,))\n    if self._checkpoint.unused_attributes:\n      raise AssertionError(\n          (\"Unused attributes in these objects (the attributes exist in the \"\n           \"checkpoint but not in the objects): %s\") % (\n               self._checkpoint.unused_attributes.items(),))\n    return self\n\n  def assert_existing_objects_matched(self):\n    \"\"\"Asserts that checkpointable Python objects have been matched.\n\n    Note that this is a weaker assertion than `assert_consumed`. It will only\n    fail for existing Python objects which are (transitive) dependencies of the\n    root object and which do not have an entry in the checkpoint.\n\n    It will not fail, for example, if a `tf.keras.Layer` object has not yet been\n    built and so has not created any `tf.Variable` objects.\n\n    Returns:\n      `self` for chaining.\n\n    Raises:\n      AssertionError: If a Python object exists in the transitive dependencies\n        of the root object but does not have a value in the checkpoint.\n    \"\"\"\n    for node_id, node in enumerate(self._checkpoint.object_graph_proto.nodes):\n      checkpointable = self._checkpoint.object_by_proto_id.get(node_id, None)\n      if (checkpointable is not None\n          and checkpointable._update_uid < self._checkpoint.restore_uid):  # pylint: disable=protected-access\n        raise AssertionError(\n            \"Object not assigned a value from checkpoint: %s\" % (node,))\n    for checkpointable_object in list_objects(self._root_checkpointable):\n      # Remove data structures that do not contain any variables from\n      # restoration checks.\n      if (isinstance(checkpointable_object,\n                     data_structures.CheckpointableDataStructure) and\n              len(checkpointable_object.variables) == 0):\n        continue\n      self._checkpoint.all_python_objects.add(checkpointable_object)\n    unused_python_objects = (\n        _ObjectIdentitySet(self._checkpoint.all_python_objects)\n        - _ObjectIdentitySet(self._checkpoint.object_by_proto_id.values()))\n    if unused_python_objects:\n      raise AssertionError(\n          (\"Some Python objects were not bound to checkpointed values, likely \"\n           \"due to changes in the Python program: %s\")\n          % (list(unused_python_objects),))\n    return self\n\n  def assert_nontrivial_match(self):\n    \"\"\"Raises an exception if only the root object matched.\"\"\"\n    for checkpointable_object in list_objects(self._root_checkpointable):\n      self._checkpoint.all_python_objects.add(checkpointable_object)\n    if len(self._checkpoint.object_by_proto_id) <= 1:\n      unused_python_objects = (\n          _ObjectIdentitySet(self._checkpoint.all_python_objects)\n          - _ObjectIdentitySet(self._checkpoint.object_by_proto_id.values()))\n      if unused_python_objects:\n        raise AssertionError(\n            (\"Nothing except the root object matched a checkpointed value. \"\n             \"Typically this means that the checkpoint does not match the \"\n             \"Python program. The following objects have no matching \"\n             \"checkpointed value: %s\") % (list(unused_python_objects),))\n      else:\n        raise AssertionError(\n            \"Nothing to load. No dependencies have been added to %s yet.\" % (\n                self._root_checkpointable,))\n    return self\n\n  def run_restore_ops(self, session=None):\n    \"\"\"Run operations to restore objects in the dependency graph.\"\"\"\n    if context.executing_eagerly():\n      return  # Run eagerly"
},
{
    "Id": 214,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5ed3c7881f1f039b1bb502eb68c65250de3bbac8",
    "Violation": "missing",
    "Bug report": "Fix ThreadPoolHandle 0 nthreads argument. It was reported that a value of 0 leads to a check failure.  Using 0 to indicate `port::MaxParallelism`, for consistency with `Dataset`.",
    "Number of deleted lines": 0,
    "Deleted lines": "      thread_pool_.Schedule(std::move(fn));\n    } else {\n      thread_pool_.Schedule(std::bind(\n          [this](std::function<void()> bound_fn) {\n            // TODO(mrry): Consider moving this thread-local configuration to\n            // the threads themselves.\n            ScopedPerThreadMaxParallelism scope(max_intra_op_parallelism_);\n            bound_fn();\n          },\n          std::move(fn)));\n    }\n  }\n\n  int32 NumThreads() { return thread_pool_.NumThreads(); }\n\n  string DebugString() const override { return \"ThreadPoolResource\"; }\n\n private:\n  thread::ThreadPool thread_pool_;\n  const int max_intra_op_parallelism_;\n};\n\n// Creates a handle to a ThreadPool resource. Note that we don't use\n// ResourceOpKernel here because the ThreadPoolResource constructor requires\n// access to `OpKernelContext::env()`, which isn't provided by\n// `ResourceOpKernel<T>::CreateResource()`.\nclass ThreadPoolHandleOp : public OpKernel {\n public:\n  explicit ThreadPoolHandleOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"display_name\", &display_name_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"num_threads\", &num_threads_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"max_intra_op_parallelism\",\n                                     &max_intra_op_parallelism_));\n    OP_REQUIRES_OK(ctx, ValidateNumThreads(num_threads_));\n  }\n\n  // The resource is deleted from the resource manager only when it is private\n  // to kernel. Ideally the resource should be deleted when it is no longer held\n  // by anyone, but it would break backward compatibility.\n  ~ThreadPoolHandleOp() override {\n    if (cinfo_.resource_is_private_to_kernel()) {\n      if (!cinfo_.resource_manager()\n               ->Delete<ThreadPoolResource>(cinfo_.container(), cinfo_.name())\n               .ok()) {\n        // Do nothing; the resource can have been deleted by session resets.\n      }\n    }\n  }\n\n  void Compute(OpKernelContext* ctx) override TF_LOCKS_EXCLUDED(mu_) {\n    mutex_lock l(mu_);\n    if (!initialized_) {\n      ResourceMgr* mgr = ctx->resource_manager();\n      OP_REQUIRES_OK(ctx, cinfo_.Init(mgr, def()));\n      ThreadPoolResource* resource;\n      OP_REQUIRES_OK(ctx, mgr->LookupOrCreate<ThreadPoolResource>(\n                              cinfo_.container(), cinfo_.name(), &resource,\n                              [this, ctx](ThreadPoolResource** ret)\n                                  TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n                                    *ret = new ThreadPoolResource(\n                                        ctx->env(), {}, display_name_,\n                                        num_threads_,\n                                        /*low_latency_hint=*/false,\n                                        max_intra_op_parallelism_);\n                                    return OkStatus();\n                                  }));\n      initialized_ = true;\n    }\n    OP_REQUIRES_OK(ctx, MakeResourceHandleToOutput(\n                            ctx, 0, cinfo_.container(), cinfo_.name(),"
},
{
    "Id": 215,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6381a7b127bd276a3817a93e5423b15a06c33419",
    "Violation": "missing",
    "Bug report": " [tf.data] Add a check for ram_budget == 0 to avoid division by 0 exception when ram_budget is not set.",
    "Number of deleted lines": 0,
    "Deleted lines": "\n#include \"tensorflow/core/framework/model.h\"\n\n#include <memory>\n\n#include \"absl/time/clock.h\"\n#include \"tensorflow/core/framework/cancellation.h\"\n#include \"tensorflow/core/framework/model.pb.h\"\n#include \"tensorflow/core/lib/gtl/cleanup.h\"\n#include \"tensorflow/core/lib/strings/str_util.h\"\n#include \"tensorflow/core/platform/host_info.h\"\n#include \"tensorflow/core/platform/mem.h\"\n\nnamespace tensorflow {\nnamespace data {\nnamespace model {\n\nconstexpr int64_t Model::kOptimizationPeriodMinMs;\nconstexpr int64_t Model::kOptimizationPeriodMaxMs;\n\nnamespace {\n\n// Returns true if all parameters have reached their max values.\nbool AreAllParametersMax(const Model::ModelParameters& parameters) {\n  for (const auto& pair : parameters) {\n    if (pair.second->value < pair.second->max) {\n      return false;\n    }\n  }\n  return true;\n}\n\n// Records the ram usage of hill climbing algorithm.\nvoid RecordAutotuneRamUsage(int64 ram_budget, double max_buffered_bytes) {\n  const auto memory_info = port::GetMemoryInfo();\n  // Records ratio of memory used since RootDataset was created over the ram\n  // budget.\n  const auto original_free_memory = ram_budget / kRamBudgetShare;\n  const auto current_free_memory = memory_info.free;\n  metrics::RecordTFDataAutotuneUsedRamBudgetRatio(\n      (original_free_memory - current_free_memory) / ram_budget);\n  // Records ratio of maximum buffer bytes tf.data could use over the ram\n  // budget.\n  metrics::RecordTFDataAutotuneMaxBufferBudgetRatio(\n      max_buffered_bytes / static_cast<double>(ram_budget));\n}\n\n// Helper function for node traversal that doesn't skip any nodes.\ninline bool IsAnyNode(const std::shared_ptr<Node> node) { return true; }\n\n// Helper function for node traversal that filters out nodes for which\n// autotuning is disabled.\ninline bool IsAutotuneNode(const std::shared_ptr<Node> node) {\n  return node->autotune();\n}\n\n// Wrapper for the square function to reduce verbosity.\ninline double Square(double x) { return x * x; }\n\n// Collects \"essential\" parallelism parameters and buffer size parameters in the\n// tree rooted in the given node. Which parallelism parameters are essential is\n// determined by the relative processing time spent in the corresponding\n// transformation. The collected parameters are returned via maps that map node\n// names to their respective parameters.\ninline void CollectParameters(std::shared_ptr<Node> node,\n                              const Node::ModelParameters& parameters,\n                              Node::ModelParameters* parallelism_parameters,\n                              Node::ModelParameters* buffer_size_parameters) {\n  // Parallelism parameter is considered to be essential if the corresponding\n  // transformations's processing time is greater than essential rate times the"
},
{
    "Id": 216,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a0fe44410e875e8e7775c6c256496bafb1a41b25",
    "Violation": "improper",
    "Bug report": " Remove the check of NodeItem exists in unfinished_nodes_ in node callback. This fixes the failure of RemoteAsyncTest.test_out_of_range_with_while_loop in DEBUG mode.",
    "Number of deleted lines": 1,
    "Deleted lines": "  last_eager_client_ = nullptr;\n  nodes_pending_.notify_all();\n}\n\nvoid EagerExecutor::NodeDone(const core::RefCountPtr<NodeItem>& item,\n                             const Status& status, bool from_queue) {\n  DVLOG(3) << \"Node Done: [id \" << item->id << \"] \" << item->node->DebugString()\n           << \" with status: \" << status.ToString();\n  DCHECK(item->state != NodeState::kDONE);\n  item->state = NodeState::kDONE;\n\n  bool async = item->node->AsAsync() != nullptr;\n  // If executing synchronously we don't need to notify if status is OK since\n  // the node  was never added to the unfinished_nodes_ list and nobody should\n  // ever be waiting for it.\n  if (status.ok() && !from_queue && !async) {\n    return;\n  }\n\n  std::forward_list<core::RefCountPtr<NodeItem>> items_to_destroy;\n  {\n    mutex_lock l(node_queue_mutex_);\n    if (!status_.ok()) return;\n\n    bool need_notification = from_queue;\n    if (from_queue) {\n      // Since this was from the async queue, pop it from the front of the queue\n      DCHECK(!node_queue_.empty() && item.get() == node_queue_.front().get());\n      node_queue_.pop();\n    } else if (async) {\n      // If it is an Async node then we will find the node in the unfinished\n      // nodes list. However we only notify if we are at the front of the list\n      // since we don't want to notify any waiters of earlier nodes.\n      need_notification = item->id == unfinished_nodes_.begin()->first;\n      auto result = unfinished_nodes_.erase(item->id);\n      DCHECK_GT(result, 0);\n    }\n\n    if (!status.ok() && item->node->Fatal()) {\n      // Since we received an error, broadcast to any waiters.\n      need_notification = true;\n      status_ = status;\n      ok_ = false;\n      if (Async()) {\n        // We remove any pending ops so that we don't try to execute them if\n        // ClearError is called.\n        errors::AppendToMessage(&status_,\n                                \"Encountered when executing an operation using \"\n                                \"EagerExecutor. This error cancels all future \"\n                                \"operations and poisons their output tensors.\");\n      }\n      while (!node_queue_.empty()) {\n        items_to_destroy.push_front(std::move(node_queue_.front()));\n        node_queue_.pop();\n      }\n      for (auto& it : unfinished_nodes_) {\n        items_to_destroy.push_front(std::move(it.second));\n      }\n      unfinished_nodes_.clear();\n    }\n    if (need_notification) {\n      NotifyWaiters(item->id);\n    }\n  }\n\n  for (auto& item : items_to_destroy) {\n    item->node->Abort(status);\n  }\n  // nodes_to_destroy will be destructed here, while not holding\n  // node_queue_mutex_. This is important because, unfortunately, some nodes'\n  // destructors can enqueue more operations onto this executor and cause\n  // a deadlock."
},
{
    "Id": 217,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bd1f1ac1fec05d38f1b8fc98f650c1c55ac06790",
    "Violation": "improper",
    "Bug report": "Fix operator check ",
    "Number of deleted lines": 1,
    "Deleted lines": "\n  # The property is preserved under composition when the operators commute.\n  if operator_a.is_self_adjoint and operator_b.is_self_adjoint:\n    return True\n\n  # The property is not preserved when an operator with the property is composed\n  # with an operator without the property.\n\n  # pylint:disable=g-bool-id-comparison\n  if ((operator_a.is_self_adjoint is True and\n       operator_b.is_self_adjoint is False) or\n      (operator_a.is_self_adjoint is False and\n       operator_b.is_self_adjoint is True)):\n    return False\n  # pylint:enable=g-bool-id-comparison\n\n  # The property is not known when operators are not known to have the property\n  # or both operators don't have the property (the property for the complement\n  # class is not closed under composition).\n  return None\n\n\ndef is_square(operator_a, operator_b):\n  \"\"\"Return a hint to whether the composition is square.\"\"\"\n  if operator_a.is_square and operator_b.is_square:\n    return True\n  if operator_a.is_square is False and operator_b.is_square is False:  # pylint:disable=g-bool-id-comparison\n    # Let A have shape [B, M, N], B have shape [B, N, L].\n    m = operator_a.range_dimension\n    l = operator_b.domain_dimension\n    if m is not None and l is not None:\n      return m == l\n\n  if (operator_a.is_square != operator_b.is_square) and (\n      operator_a.is_square is not None and operator_a.is_square is not None):\n    return False\n\n  return None\n\n\n# Note: Positive definiteness is only guaranteed to be preserved\n# when the operators commute and are symmetric. Only use this method in\n# commuting cases.\ndef combined_commuting_positive_definite_hint(operator_a, operator_b):\n  \"\"\"Get combined PD hint for compositions.\"\"\"\n  # pylint:disable=g-bool-id-comparison\n  if (operator_a.is_positive_definite is True and\n      operator_a.is_self_adjoint is True and\n      operator_b.is_positive_definite is True and\n      operator_b.is_self_adjoint is True):\n    return True\n  # pylint:enable=g-bool-id-comparison\n\n  return None\n\n\ndef combined_non_singular_hint(operator_a, operator_b):\n  \"\"\"Get combined hint for when .\"\"\"\n  # If either operator is not-invertible the composition isn't.\n\n  # pylint:disable=g-bool-id-comparison\n  if (operator_a.is_non_singular is False or\n      operator_b.is_non_singular is False):\n    return False\n  # pylint:enable=g-bool-id-comparison\n\n  return operator_a.is_non_singular and operator_b.is_non_singular\n"
},
{
    "Id": 218,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5e0c9fff657498f9a74da38b2ce1b4721698a388",
    "Violation": "missing",
    "Bug report": " Add bounds checks to jpeg parsing code. ",
    "Number of deleted lines": 2,
    "Deleted lines": "// -----------------------------------------------------------------------------\n// We emulate the same error-handling as fill_input_buffer() from jdatasrc.c,\n// for coherency's sake.\nboolean MemFillInputBuffer(j_decompress_ptr cinfo) {\n  static const JOCTET kEOIBuffer[2] = {0xff, JPEG_EOI};\n  MemSourceMgr *src = reinterpret_cast<MemSourceMgr *>(cinfo->src);\n  if (src->pub.bytes_in_buffer == 0 && src->pub.next_input_byte == src->data) {\n    // empty file -> treated as an error.\n    ERREXIT(cinfo, JERR_INPUT_EMPTY);\n    return FALSE;\n  } else if (src->pub.bytes_in_buffer) {\n    // if there's still some data left, it's probably corrupted\n    return src->try_recover_truncated_jpeg ? TRUE : FALSE;\n  } else if (src->pub.next_input_byte != kEOIBuffer &&\n             src->try_recover_truncated_jpeg) {\n    // In an attempt to recover truncated files, we insert a fake EOI\n    WARNMS(cinfo, JWRN_JPEG_EOF);\n    src->pub.next_input_byte = kEOIBuffer;\n    src->pub.bytes_in_buffer = 2;\n    return TRUE;\n  } else {\n    // We already inserted a fake EOI and it wasn't enough, so this time\n    // it's really an error.\n    ERREXIT(cinfo, JERR_FILE_READ);\n    return FALSE;\n  }\n}\n\n// -----------------------------------------------------------------------------\nvoid MemTermSource(j_decompress_ptr cinfo) {}\n\n// -----------------------------------------------------------------------------\nvoid MemSkipInputData(j_decompress_ptr cinfo, long jump) {\n  MemSourceMgr *src = reinterpret_cast<MemSourceMgr *>(cinfo->src);\n  src->pub.bytes_in_buffer -= jump;\n  src->pub.next_input_byte += jump;\n}\n\n// -----------------------------------------------------------------------------\nvoid SetSrc(j_decompress_ptr cinfo, const void *data,\n            unsigned long int datasize, bool try_recover_truncated_jpeg) {\n  MemSourceMgr *src;\n\n  cinfo->src = reinterpret_cast<struct jpeg_source_mgr *>(\n      (*cinfo->mem->alloc_small)(reinterpret_cast<j_common_ptr>(cinfo),\n                                 JPOOL_PERMANENT, sizeof(MemSourceMgr)));\n\n  src = reinterpret_cast<MemSourceMgr *>(cinfo->src);\n  src->pub.init_source = MemInitSource;\n  src->pub.fill_input_buffer = MemFillInputBuffer;\n  src->pub.skip_input_data = MemSkipInputData;\n  src->pub.resync_to_restart = jpeg_resync_to_restart;\n  src->pub.term_source = MemTermSource;\n  src->data = reinterpret_cast<const unsigned char *>(data);\n  src->datasize = datasize;\n  src->pub.bytes_in_buffer = 0;\n  src->pub.next_input_byte = NULL;\n  src->try_recover_truncated_jpeg = try_recover_truncated_jpeg;\n}\n\n}  // namespace jpeg\n}  // namespace tensorflow\n"
},
{
    "Id": 219,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/28dacabab5aac2963e37e622f4b157cf00d82662",
    "Violation": "insufficient",
    "Bug report": " [tf] Explicitly check that runner index is in bounds and runner is available",
    "Number of deleted lines": 2,
    "Deleted lines": "    // BEF file.\n    return H::combine(std::move(h), key.loc_.data, key.loc_.GetHandler());\n  }\n\n  friend bool operator==(const OpLocationKey& x, const OpLocationKey& y) {\n    return x.loc_.data == y.loc_.data &&\n           x.loc_.GetHandler() == y.loc_.GetHandler();\n  }\n\n private:\n  tfrt::Location loc_;\n};\n\n// OpKernelRunnerTable for keeping OpKernelRunner instances to avoid expensive\n// reinstantiation of OpKernel and other repeated setup per kernel execution.\n// OpKernelRunnerTable is thread-compatible.\nclass OpKernelRunnerTable {\n public:\n  OpKernelRunnerTable() = default;\n\n  // Return true if it successfully inserts `runner`. `index` is supposed to be\n  // dense.\n  bool Insert(int64_t index, OpKernelRunner runner) {\n    if (runners_.size() <= index) runners_.resize(index + 1);\n    if (runners_[index].has_value()) return false;\n    runners_[index] = std::move(runner);\n    return true;\n  }\n\n  // Return the OpKernelRunner at the corresponding `index` in the table. The\n  // result can never be nullptr. It is a fatal error to use an index that is\n  // not in the table. Note that the returned pointer will be invalidated if\n  // Insert() is called.\n  const OpKernelRunner* Get(int64_t index) const {\n    DCHECK_GT(runners_.size(), index);\n    auto& result = runners_.at(index);\n    DCHECK(result.has_value());\n    return &(*result);\n  }\n\n private:\n  std::vector<absl::optional<OpKernelRunner>> runners_;\n};\n\n// OpKernelRunnerCache is similar to OpKernelRunnerTable but thread-safe.\nclass OpKernelRunnerCache {\n public:\n  OpKernelRunnerCache();\n\n  tfrt::StatusOr<OpKernelRunner*> GetOrCreate(\n      tfrt::Location loc, absl::string_view op_name,\n      absl::string_view device_name, int num_args,\n      const std::function<llvm::Error(tensorflow::AttrValueMap*)>& attr_builder,\n      const KernelFallbackCompatRequestState& fallback_request_state);\n\n private:\n  mutable mutex mu_;\n  absl::flat_hash_map<OpLocationKey, std::unique_ptr<OpKernelRunner>> map_\n      TF_GUARDED_BY(mu_);\n};\n\n}  // namespace tfd\n}  // namespace tensorflow\n\n#endif  // TENSORFLOW_CORE_RUNTIME_FALLBACK_KERNEL_OP_KERNEL_RUNNER_H_\n"
},
{
    "Id": 220,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bb6a0383ed553c286f87ca88c207f6774d5c4a8f",
    "Violation": "missing",
    "Bug report": " Prevent heap OOB read in TFLite's gather_nd.cc. Passing negative indices is illegal but there was a missing check so that resulted in OOB accesses.",
    "Number of deleted lines": 0,
    "Deleted lines": "  const int output_rank = indices_rank + params_rank - indices_nd - 1;\n  TfLiteIntArray* output_shape = TfLiteIntArrayCreate(output_rank);\n  int output_index = 0;\n  for (int i = 0; i < indices_rank - 1; ++i) {\n    output_shape->data[output_index++] = indices->dims->data[i];\n  }\n  for (int i = indices_nd; i < params_rank; ++i) {\n    output_shape->data[output_index++] = params->dims->data[i];\n  }\n  return context->ResizeTensor(context, output, output_shape);\n}\n\ntemplate <typename ParamsT, typename IndicesT>\nTfLiteStatus GatherNd(const TfLiteTensor* params, const TfLiteTensor* indices,\n                      TfLiteTensor* output) {\n  reference_ops::GatherNd(\n      GetTensorShape(params), GetTensorData<ParamsT>(params),\n      GetTensorShape(indices), GetTensorData<IndicesT>(indices),\n      GetTensorShape(output), GetTensorData<ParamsT>(output));\n  return kTfLiteOk;\n}\n\ntemplate <typename IndicesT>\nTfLiteStatus GatherNdString(const TfLiteTensor* params,\n                            const TfLiteTensor* indices, TfLiteTensor* output) {\n  reference_ops::GatherNdString(\n      GetTensorShape(params), params, GetTensorShape(indices),\n      GetTensorData<IndicesT>(indices), GetTensorShape(output), output);\n  return kTfLiteOk;\n}\n\ntemplate <typename IndicesT>\nTfLiteStatus EvalGatherNd(TfLiteContext* context, const TfLiteTensor* params,\n                          const TfLiteTensor* indices, TfLiteTensor* output) {\n  switch (params->type) {\n    case kTfLiteFloat32:\n      return GatherNd<float, IndicesT>(params, indices, output);\n    case kTfLiteUInt8:\n      return GatherNd<uint8_t, IndicesT>(params, indices, output);\n    case kTfLiteInt8:\n      return GatherNd<int8_t, IndicesT>(params, indices, output);\n    case kTfLiteInt16:\n      return GatherNd<int16_t, IndicesT>(params, indices, output);\n    case kTfLiteInt32:\n      return GatherNd<int32_t, IndicesT>(params, indices, output);\n    case kTfLiteInt64:\n      return GatherNd<int64_t, IndicesT>(params, indices, output);\n    case kTfLiteString:\n      return GatherNdString<IndicesT>(params, indices, output);\n    default:\n      context->ReportError(context,\n                           \"Params type '%s' are not supported by gather_nd.\",\n                           TfLiteTypeGetName(params->type));\n      return kTfLiteError;\n  }\n}\n\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* params;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kParams, &params));\n  const TfLiteTensor* indices;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kIndices, &indices));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  // Prevent division by 0 in the helper\n  TF_LITE_ENSURE(context, NumElements(params) > 0);\n\n  switch (indices->type) {"
},
{
    "Id": 221,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/22783fdf812b700f7de9980038ab41ee0a4a2284",
    "Violation": "missing",
    "Bug report": "Add checks recently removed ",
    "Number of deleted lines": 0,
    "Deleted lines": "namespace gpu {\n\n// If quantized tensors exist in the graph & quant_conversion_map is non-null,\n// the mapping between the original tensors (fixed-point) & GPU values (fp) is\n// stored in quant_conversion_map.\nclass ObjectReader {\n public:\n  static absl::Status ReadNonConstantTensor(\n      TfLiteContext* context, absl::flat_hash_map<int, Value*>* tensor_to_value,\n      absl::flat_hash_map<int, int>* quant_conversion_map, GraphFloat32* graph,\n      uint32_t tensor_idx, Value** value = nullptr);\n\n  ObjectReader(GraphFloat32* graph, TfLiteContext* context,\n               const TfLiteNode* node,\n               absl::flat_hash_map<int, Value*>* tensor_to_value,\n               absl::flat_hash_map<int, int>* quant_conversion_map = nullptr)\n      : graph_(graph),\n        context_(context),\n        node_(node),\n        tensor_to_value_(tensor_to_value),\n        quant_conversion_map_(quant_conversion_map) {}\n\n  absl::Status ReadValue(uint32_t idx, Value** value);\n\n  absl::Status ReadValueByTensorIdx(uint32_t tensor_idx, Value** value);\n\n  int GetNumberOfRuntimeInputs() const;\n\n  absl::Status GetTensorId(uint32_t input_id, int* tensor_id) const;\n\n  absl::Status GetTensorDims(uint32_t idx, TfLiteIntArray* dimensions) const;\n\n  template <typename TensorT>\n  absl::Status ReadTensor(uint32_t index, TensorT* tensor) const {\n    const int32_t tensor_id = node_->inputs->data[index];\n    const TfLiteTensor* tflite_tensor = context_->tensors + tensor_id;\n    tensor->data.resize(NumElements(tflite_tensor));\n    if (tflite_tensor->sparsity) {\n      std::vector<int> dims;\n      dims.reserve(tflite_tensor->dims->size);\n      for (int i = 0; i < tflite_tensor->dims->size; ++i) {\n        dims.push_back(tflite_tensor->dims->data[i]);\n      }\n      switch (tflite_tensor->type) {\n        case kTfLiteFloat32: {\n          optimize::sparsity::FormatConverter<float> converter(\n              dims, *tflite_tensor->sparsity);\n          converter.SparseToDense(\n              static_cast<const float*>(tflite_tensor->data.data));\n          const std::vector<float> out = converter.GetData();\n          std::memcpy(&tensor->data[0], out.data(), out.size() * sizeof(float));\n          break;\n        }\n        case kTfLiteFloat16: {\n          optimize::sparsity::FormatConverter<Eigen::half> converter(\n              dims, *tflite_tensor->sparsity);\n          converter.SparseToDense(\n              static_cast<const Eigen::half*>(tflite_tensor->data.data));\n          const std::vector<Eigen::half> out = converter.GetData();\n          std::memcpy(&tensor->data[0], out.data(),\n                      out.size() * sizeof(Eigen::half));\n          break;\n        }\n        default: {\n          return absl::InvalidArgumentError(\n              \"Unexpected data type in sparse tensor\");\n        }\n      }\n    } else {\n      RETURN_IF_ERROR(CreateVectorCopyData(*tflite_tensor, &tensor->data[0]));\n    }"
},
{
    "Id": 222,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/459b4bfe1f73737fae23aa1499b06a69605d0f65",
    "Violation": "missing",
    "Bug report": "Added a check in EagerExecutor to avoid getting invalid range. ",
    "Number of deleted lines": 0,
    "Deleted lines": "      }\n      while (!node_queue_.empty()) {\n        items_to_destroy.push_front(std::move(node_queue_.front()));\n        node_queue_.pop();\n      }\n      for (auto& it : unfinished_nodes_) {\n        items_to_destroy.push_front(std::move(it.second));\n      }\n      unfinished_nodes_.clear();\n    }\n    if (need_notification) {\n      NotifyWaiters(item->id);\n    }\n  }\n\n  for (auto& item : items_to_destroy) {\n    item->node->Abort(status);\n  }\n  // nodes_to_destroy will be destructed here, while not holding\n  // node_queue_mutex_. This is important because, unfortunately, some nodes'\n  // destructors can enqueue more operations onto this executor and cause\n  // a deadlock.\n}\n\nvoid EagerExecutor::NotifyWaiters(uint64 id) {\n  if (!node_done_notifications_.empty()) {\n    uint64 upperbound_id = 0;\n    if (!unfinished_nodes_.empty()) {\n      upperbound_id = unfinished_nodes_.begin()->first - 1;\n    } else if (!node_queue_.empty()) {\n      upperbound_id = node_queue_.front()->id - 1;\n    } else {\n      upperbound_id = next_node_id_ - 1;\n    }\n    DVLOG(3) << \"Notify node done: [id \" << id << \" to \" << upperbound_id\n             << \"] \";\n    // Note that we notify all waiting threads in case an error has\n    // occurred. These calling threads are responsible for checking status_\n    // before proceeding.\n    const auto range =\n        status_.ok()\n            ? make_pair(node_done_notifications_.lower_bound(id),\n                        node_done_notifications_.upper_bound(upperbound_id))\n            : make_pair(node_done_notifications_.begin(),\n                        node_done_notifications_.end());\n    for (auto it = range.first; it != range.second; ++it) {\n      it->second->notify_all();\n    }\n    node_done_notifications_.erase(range.first, range.second);\n  }\n}\n\nvoid EagerExecutor::Run() {\n  auto thread_exited_notifier =\n      gtl::MakeCleanup([this] { thread_exited_notification_.Notify(); });\n  while (true) {\n    core::RefCountPtr<NodeItem> curr_item;\n    {\n      tensorflow::mutex_lock l(node_queue_mutex_);\n      while (node_queue_.empty() || !status_.ok()) {\n        if (state_ == ExecutorState::kShutDown) return;\n        nodes_pending_.wait(l);\n      }\n      // Obtain raw pointer since we don't want to remove from the queue until\n      // the node has been run. Otherwise, WaitForAllPendingNodes can return\n      // too early.\n      // Note, we don't std::move from the here because the front of the queue\n      // will then contain a nullptr. This can be a problem in\n      // WaitForAllPendingNodes where we get the top EagerNode pointer\n      // and register a notification for its completion."
},
{
    "Id": 223,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cddca76312f5ae4fb92a101e79eeff6d5ac16932",
    "Violation": "missing",
    "Bug report": "Add check for reading input tensors at an index that is out of range. ",
    "Number of deleted lines": 0,
    "Deleted lines": "\nnamespace tflite {\nnamespace gpu {\n\n// If quantized tensors exist in the graph & quant_conversion_map is non-null,\n// the mapping between the original tensors (fixed-point) & GPU values (fp) is\n// stored in quant_conversion_map.\nclass ObjectReader {\n public:\n  static absl::Status ReadNonConstantTensor(\n      TfLiteContext* context, std::unordered_map<int, Value*>* tensor_to_value,\n      std::unordered_map<int, int>* quant_conversion_map, GraphFloat32* graph,\n      uint32_t tensor_idx, Value** value = nullptr);\n\n  ObjectReader(GraphFloat32* graph, TfLiteContext* context,\n               const TfLiteNode* node,\n               std::unordered_map<int, Value*>* tensor_to_value,\n               std::unordered_map<int, int>* quant_conversion_map = nullptr)\n      : graph_(graph),\n        context_(context),\n        node_(node),\n        tensor_to_value_(tensor_to_value),\n        quant_conversion_map_(quant_conversion_map) {}\n\n  absl::Status ReadValue(uint32_t idx, Value** value);\n\n  absl::Status ReadValueByTensorIdx(uint32_t tensor_idx, Value** value);\n\n  int GetNumberOfRuntimeInputs() const;\n\n  absl::Status GetTensorDims(uint32_t idx, TfLiteIntArray* dimensions) const;\n\n  template <typename TensorT>\n  absl::Status ReadTensor(uint32_t idx, TensorT* t) const {\n    const int32_t tensor_idx = node_->inputs->data[idx];\n    if (tensor_idx < 0) {\n      return absl::InvalidArgumentError(\n          \"Invalid data index found. Possibly an unset optional tensor is \"\n          \"being read.\");\n    }\n\n    const TfLiteTensor* tflite_tensor = context_->tensors + tensor_idx;\n    t->data.resize(NumElements(tflite_tensor));\n    RETURN_IF_ERROR(CreateVectorCopyData(*tflite_tensor, &t->data[0]));\n\n    // Axis and data layout depend on operation this tensor is used in. So,\n    // postpone resolutions until operations are parsed.\n    t->id = tensor_idx;\n    return SetAllDimensions(tflite_tensor->dims, &t->shape);\n  }\n\n  absl::Status AddOutput(const Node* node, int id);\n\n  absl::Status AddOutputs(const Node* node);\n\n  absl::Status AddInput(const Node* node, uint32_t idx);\n\n  TfLiteTensor* GetInputTensor(int index) const;\n\n  TfLiteTensor* GetOutputTensor(int index) const;\n\n  absl::Status VerifyInputsConstsOutputs(const TfLiteNode* node,\n                                         int runtime_inputs, int const_inputs,\n                                         int outputs);\n\n private:\n  GraphFloat32* graph_;\n  TfLiteContext* context_;\n  const TfLiteNode* node_;\n  std::unordered_map<int, Value*>* tensor_to_value_;"
},
{
    "Id": 224,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/823b694639a3f49b6adbf9e73a08c529d583878e",
    "Violation": "missing",
    "Bug report": "Add bounds checking when looking at the stack in TF Registry. ",
    "Number of deleted lines": 2,
    "Deleted lines": "_TYPE_TAG = \"type\"\n\n\nclass Registry(object):\n  \"\"\"Provides a registry for saving objects.\"\"\"\n\n  def __init__(self, name):\n    \"\"\"Creates a new registry.\"\"\"\n    self._name = name\n    self._registry = dict()\n\n  def register(self, candidate, name=None):\n    \"\"\"Registers a Python object \"candidate\" for the given \"name\".\n\n    Args:\n      candidate: The candidate object to add to the registry.\n      name: An optional string specifying the registry key for the candidate.\n            If None, candidate.__name__ will be used.\n    Raises:\n      KeyError: If same name is used twice.\n    \"\"\"\n    if not name:\n      name = candidate.__name__\n    if name in self._registry:\n      (filename, line_number, function_name, _) = (\n          self._registry[name][_LOCATION_TAG])\n      raise KeyError(\"Registering two %s with name '%s'! \"\n                     \"(Previous registration was in %s %s:%d)\" %\n                     (self._name, name, function_name, filename, line_number))\n\n    logging.vlog(1, \"Registering %s (%s) in %s.\", name, candidate, self._name)\n    # stack trace is [this_function, Register(), user_function,...]\n    # so the user function is #2.\n    stack = tf_stack.extract_stack()\n    user_function = stack[2]\n    location_tag = tf_stack.convert_stack([user_function])[0]\n    self._registry[name] = {_TYPE_TAG: candidate, _LOCATION_TAG: location_tag}\n\n  def list(self):\n    \"\"\"Lists registered items.\n\n    Returns:\n      A list of names of registered objects.\n    \"\"\"\n    return self._registry.keys()\n\n  def lookup(self, name):\n    \"\"\"Looks up \"name\".\n\n    Args:\n      name: a string specifying the registry key for the candidate.\n    Returns:\n      Registered object if found\n    Raises:\n      LookupError: if \"name\" has not been registered.\n    \"\"\"\n    name = compat.as_str(name)\n    if name in self._registry:\n      return self._registry[name][_TYPE_TAG]\n    else:\n      raise LookupError(\n          \"%s registry has no entry for: %s\" % (self._name, name))\n"
},
{
    "Id": 225,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b1c9e600e02b93885dbebfa5dae92436c63d6c03",
    "Violation": "missing",
    "Bug report": "[XLA] Add range check for xla::Array<> indexing. ",
    "Number of deleted lines": 0,
    "Deleted lines": "      for (int64 i = sizes_.size() - 1; i >= 0; --i) {\n        if (index[i] != sizes_[i] - 1) {\n          break;\n        }\n        pieces.push_back(\"]\");\n        if (i != 0 && index[i - 1] != sizes_[i - 1] - 1) {\n          pieces.push_back(\",\\n\");\n        }\n      }\n    } while (next_index(&index));\n    return absl::StrJoin(pieces, \"\");\n  }\n\n private:\n  // Converts an initializer_list of type U to a vector of type int64. Used by\n  // the initializer list based constructors to convert the size type into int64\n  // to be passed to the size based constructor.\n  template <typename U>\n  static std::vector<int64> ToInt64Vector(\n      const std::initializer_list<U>& data) {\n    return std::vector<int64>(data.begin(), data.end());\n  }\n\n  // Returns the linear index from the list of per-dimension indexes. Function\n  // is templated so can be used with an std::array from operator() to avoid\n  // memory allocation.\n  template <typename U>\n  int64 calculate_index(const U& indexes) const {\n    CHECK_EQ(sizes_.size(), indexes.size());\n    int64 index = 0;\n    for (int64 i = 0; i < sizes_.size(); ++i) {\n      index *= sizes_[i];\n      index += indexes[i];\n    }\n    return index;\n  }\n\n  // Advances the specified set of indexes and returns true if we haven't\n  // wrapped around (i.e. result isn't {0, 0, ...}).\n  bool next_index(std::vector<int64>* index) const {\n    CHECK_EQ(index->size(), sizes_.size());\n    for (int64 i = sizes_.size() - 1; i >= 0; --i) {\n      (*index)[i]++;\n      if ((*index)[i] < sizes_[i]) {\n        return true;\n      }\n      (*index)[i] = 0;\n    }\n    return false;\n  }\n\n  std::vector<int64> sizes_;\n  std::unique_ptr<T[]> values_;\n};\n\n// Specialization of FillRandom() method for complex64 type. Uses real part of\n// the stddev parameter as the standard deviation value.\ntemplate <>\nvoid Array<complex64>::FillRandom(const complex64& stddev, const double mean,\n                                  const int seed);\n\n}  // namespace xla\n\n#endif  // TENSORFLOW_COMPILER_XLA_ARRAY_H_\n"
},
{
    "Id": 226,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1908d7ef706f0f3f8c7a300068355bf795fb3d17",
    "Violation": "improper",
    "Bug report": "Fix out-of-bounds StringPiece access in ForwardNUTF8CharPositions().  Even a simple invocation like 'int p = 0; ForwardNUTF8CharPositions(\"a\", 1, &p);' will cause an invalid access to in[1]. Checking for *pos < size before that access fixes this issue.",
    "Number of deleted lines": 1,
    "Deleted lines": "enum class UnicodeEncoding { UTF8, UTF16BE, UTF32BE };\n\n// Enumeration for character units.  Used by string such as\n// tf.strings.length and tf.substr.\n// TODO(edloper): Add support for: UTF32_CHAR, etc.\nenum class CharUnit { BYTE, UTF8_CHAR };\n\n// Whether or not the given byte is the trailing byte of a UTF-8/16/32 char.\ninline bool IsTrailByte(char x) { return static_cast<signed char>(x) < -0x40; }\n\n// Sets `encoding` based on `str`.\nStatus ParseUnicodeEncoding(const string& str, UnicodeEncoding* encoding);\n\n// Sets `unit` value based on `str`.\nStatus ParseCharUnit(const string& str, CharUnit* unit);\n\n// Returns the number of Unicode characters in a UTF-8 string.\n// Result may be incorrect if the input string is not valid UTF-8.\nint32 UTF8StrLen(const string& str);\n\n// Get the next UTF8 character position starting at the given position and\n// skipping the given number of characters. Position is a byte offset, and\n// should never be `null`. The function return true if successful. However, if\n// the end of the string is reached before the requested characters, then the\n// position will point to the end of string and this function will return false.\ntemplate <typename T>\nbool ForwardNUTF8CharPositions(const StringPiece in,\n                               const T num_utf8_chars_to_shift, T* pos) {\n  const size_t size = in.size();\n  T utf8_chars_counted = 0;\n  while (utf8_chars_counted < num_utf8_chars_to_shift && *pos < size) {\n    // move forward one utf-8 character\n    do {\n      ++*pos;\n    } while (IsTrailByte(in[*pos]) && *pos < size);\n    ++utf8_chars_counted;\n  }\n  return utf8_chars_counted == num_utf8_chars_to_shift;\n}\n\n// Get the previous UTF8 character position starting at the given position and\n// skipping the given number of characters. Position is a byte offset with a\n// positive value, relative to the beginning of the string, and should never be\n// `null`. The function return true if successful. However, if the beginning of\n// the string is reached before the requested character, then the position will\n// point to the beginning of the string and this function will return false.\ntemplate <typename T>\nbool BackNUTF8CharPositions(const StringPiece in,\n                            const T num_utf8_chars_to_shift, T* pos) {\n  const size_t start = 0;\n  T utf8_chars_counted = 0;\n  while (utf8_chars_counted < num_utf8_chars_to_shift && (*pos > start)) {\n    // move back one utf-8 character\n    do {\n      --*pos;\n    } while (IsTrailByte(in[*pos]) && *pos > start);\n    ++utf8_chars_counted;\n  }\n  return utf8_chars_counted == num_utf8_chars_to_shift;\n}\n\n}  // namespace tensorflow\n\n#endif  // TENSORFLOW_CORE_KERNELS_STRING_UTIL_H_\n"
},
{
    "Id": 227,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/aa54f547f04c3007b26df2379c6cf5f081948d0b",
    "Violation": "missing",
    "Bug report": " Updated the check_numerics function to also validate the gradient corresponding to the tensor it's validating",
    "Number of deleted lines": 1,
    "Deleted lines": "\n@ops.RegisterGradient(\"Fill\")\ndef _FillGrad(_, grad):\n  return None, math_ops.reduce_sum(grad)\n\n\nops.NoGradient(\"ZerosLike\")\n\n\n@ops.RegisterGradient(\"Gather\")\ndef _GatherGrad(op, grad):\n  \"\"\"Gradient for Gather op.\"\"\"\n  if op.inputs[0].get_shape().is_fully_defined():\n    dense_shape = constant_op.constant(op.inputs[0].get_shape().as_list())\n    values_shape = [-1] + op.inputs[0].get_shape()[1:].as_list()\n  else:\n    # op.inputs[0] can be large, so colocate the shape calculation with it.\n    with ops.colocate_with(op.inputs[0]):\n      dense_shape = array_ops.shape(op.inputs[0])\n      values_shape = array_ops.concat(0, [[-1], dense_shape[1:]])\n\n  values = array_ops.reshape(grad, values_shape)\n  indices = array_ops.reshape(op.inputs[1], [-1])\n  return [ops.IndexedSlices(values, indices, dense_shape), None]\n\n\n@ops.RegisterGradient(\"GatherNd\")\ndef _GatherNdGrad(unused_op, unused_grad):\n  raise NotImplementedError(\"Gradient for gather_nd is not implemented.\")\n\n\n@ops.RegisterGradient(\"CheckNumerics\")\ndef _CheckNumericsGrad(_, grad):\n  \"\"\"Gradient for check_numerics op.\"\"\"\n  return grad\n\n\n@ops.RegisterGradient(\"Identity\")\ndef _IdGrad(_, grad):\n  return grad\n\n\n@ops.RegisterGradient(\"RefIdentity\")\ndef _RefIdGrad(_, grad):\n  return grad\n\n\nops.NoGradient(\"StopGradient\")\n\n\n@ops.RegisterGradient(\"Reshape\")\ndef _ReshapeGrad(op, grad):\n  return [array_ops.reshape(grad, array_ops.shape(op.inputs[0])), None]\n\n\nops.NoGradient(\"InvertPermutation\")\n\n\ndef _ReshapeToInput(op, grad):\n  \"\"\"Reshapes the gradient to the shape of the original input.\"\"\"\n  return array_ops.reshape(grad, array_ops.shape(op.inputs[0]))\n\n\n@ops.RegisterGradient(\"ExpandDims\")\ndef _ExpandDimsGrad(op, grad):\n  return [_ReshapeToInput(op, grad), None]\n\n\n@ops.RegisterGradient(\"Squeeze\")\ndef _SqueezeGrad(op, grad):\n  return _ReshapeToInput(op, grad)"
},
{
    "Id": 228,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/236660d0fccff6f59f29a1936dc731d783722e28",
    "Violation": "missing",
    "Bug report": " [XLA:GPU] Fix host conv checker canonicalization for f16 and nans. The GPU-side checker is correct, but the host-side checker was canonicalizing nan to F16_MAX.  The effect of this is that you'd get a \"conv mismatch!\" error but no description of exactly what mismatched.",
    "Number of deleted lines": 0,
    "Deleted lines": "  TF_ASSIGN_OR_RETURN(auto shaped_rhs, to_shaped_buffer(rhs));\n\n  ExecutableRunOptions run_options;\n  run_options.set_device_ordinal(stream_exec->device_ordinal());\n  run_options.set_stream(stream);\n  run_options.set_allocator(allocator);\n  ServiceExecutableRunOptions service_run_options(run_options);\n\n  const ShapedBuffer* arg_buffers[] = {&shaped_lhs, &shaped_rhs};\n  TF_ASSIGN_OR_RETURN(auto result_buffer,\n                      comparator_exec_->ExecuteOnStream(&service_run_options,\n                                                        arg_buffers, nullptr));\n\n  double result;\n  CHECK(result_buffer.root_buffer().size() == sizeof(result));\n  stream->ThenMemcpy(&result, result_buffer.root_buffer(), sizeof(result));\n  TF_RETURN_IF_ERROR(stream->BlockHostUntilDone());\n  return result < kTolerance;\n}\n\n// Host side comparison code that does the same thing, but reports some of the\n// differences as well. It only print logs for debugging.\ntemplate <typename ElementType, typename ComparisonType>\nStatus HostCompare(se::Stream* stream, se::DeviceMemoryBase lhs,\n                   se::DeviceMemoryBase rhs) {\n  int64 n = lhs.size() / sizeof(ElementType);\n  std::vector<ElementType> host_lhs(n), host_rhs(n);\n  stream->ThenMemcpy(host_lhs.data(), lhs, lhs.size());\n  stream->ThenMemcpy(host_rhs.data(), rhs, rhs.size());\n  TF_RETURN_IF_ERROR(stream->BlockHostUntilDone());\n\n  const auto canonicalize = [](ComparisonType a) -> ComparisonType {\n    if (std::is_same<ElementType, Eigen::half>::value && a) {\n      constexpr float kMaxFp16Value = 65504.;\n      if (a < 0) {\n        return -(kMaxFp16Value + 1);\n      }\n      return kMaxFp16Value + 1;\n    }\n    return a;\n  };\n  int differences_seen = 0;\n  for (int64 i = 0; i < n && differences_seen < 10; i++) {\n    auto original_lhs = static_cast<ComparisonType>(host_lhs[i]);\n    auto original_rhs = static_cast<ComparisonType>(host_rhs[i]);\n    ComparisonType lhs = canonicalize(original_lhs);\n    ComparisonType rhs = canonicalize(original_rhs);\n    if (std::isnan(lhs) && std::isnan(rhs)) {\n      continue;\n    }\n    if (std::isinf(lhs) && std::isinf(rhs) && lhs == rhs) {\n      continue;\n    }\n    if (std::isfinite(lhs) != std::isfinite(rhs) ||\n        !(std::abs(lhs - rhs) / (std::max(std::abs(lhs), std::abs(rhs)) + 1) <\n          kTolerance)) {\n      differences_seen++;\n      LOG(ERROR) << \"Difference at \" << i << \": \" << original_lhs << \" vs \"\n                 << original_rhs;\n    }\n  }\n  return Status::OK();\n}\n\nStatusOr<bool> BufferComparator::CompareEqual(se::Stream* stream,\n                                              DeviceMemoryAllocator* allocator,\n                                              se::DeviceMemoryBase lhs,\n                                              se::DeviceMemoryBase rhs) {\n  TF_ASSIGN_OR_RETURN(auto result,\n                      CompareEqualImpl(stream, allocator, lhs, rhs));"
},
{
    "Id": 229,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8c6f391a2282684a25cbfec7687bd5d35261a209",
    "Violation": "missing",
    "Bug report": " [lite] Add check for bias_size is zero to avoid division by zero. This shouldn't happen for properly converted models. Just safety check",
    "Number of deleted lines": 0,
    "Deleted lines": "      *output_activation_min = 0.f;\n      *output_activation_max = std::numeric_limits<float>::max();\n      break;\n    case FusedActivationFunctionType::kRelu1:\n      *output_activation_min = -1.f;\n      *output_activation_max = 1.f;\n      break;\n    case FusedActivationFunctionType::kRelu6:\n      *output_activation_min = 0.f;\n      *output_activation_max = 6.f;\n      break;\n  }\n}\n\ntemplate <typename T>\ninline T ActivationFunctionWithMinMax(T x, T output_activation_min,\n                                      T output_activation_max) {\n  using std::max;\n  using std::min;\n  return min(max(x, output_activation_min), output_activation_max);\n}\n\n// Legacy function, left for compatibility only.\ntemplate <FusedActivationFunctionType Ac>\nfloat ActivationFunction(float x) {\n  float output_activation_min, output_activation_max;\n  GetActivationMinMax(Ac, &output_activation_min, &output_activation_max);\n  return ActivationFunctionWithMinMax(x, output_activation_min,\n                                      output_activation_max);\n}\n\ninline void BiasAndClamp(float clamp_min, float clamp_max, int bias_size,\n                         const float* bias_data, int array_size,\n                         float* array_data) {\n  // Note: see b/132215220: in May 2019 we thought it would be OK to replace\n  // this with the Eigen one-liner:\n  //   return (array.colwise() + bias).cwiseMin(clamp_max).cwiseMin(clamp_max).\n  // This turned out to severely regress performance: +4ms (i.e. 8%) on\n  // MobileNet v2 / 1.0 / 224. So we keep custom NEON code for now.\n  TFLITE_DCHECK_EQ((array_size % bias_size), 0);\n#ifdef USE_NEON\n  float* array_ptr = array_data;\n  float* array_end_ptr = array_ptr + array_size;\n  const auto clamp_min_vec = vdupq_n_f32(clamp_min);\n  const auto clamp_max_vec = vdupq_n_f32(clamp_max);\n  for (; array_ptr != array_end_ptr; array_ptr += bias_size) {\n    int i = 0;\n    for (; i <= bias_size - 16; i += 16) {\n      auto b0 = vld1q_f32(bias_data + i);\n      auto b1 = vld1q_f32(bias_data + i + 4);\n      auto b2 = vld1q_f32(bias_data + i + 8);\n      auto b3 = vld1q_f32(bias_data + i + 12);\n      auto a0 = vld1q_f32(array_ptr + i);\n      auto a1 = vld1q_f32(array_ptr + i + 4);\n      auto a2 = vld1q_f32(array_ptr + i + 8);\n      auto a3 = vld1q_f32(array_ptr + i + 12);\n      auto x0 = vaddq_f32(a0, b0);\n      auto x1 = vaddq_f32(a1, b1);\n      auto x2 = vaddq_f32(a2, b2);\n      auto x3 = vaddq_f32(a3, b3);\n      x0 = vmaxq_f32(clamp_min_vec, x0);\n      x1 = vmaxq_f32(clamp_min_vec, x1);\n      x2 = vmaxq_f32(clamp_min_vec, x2);\n      x3 = vmaxq_f32(clamp_min_vec, x3);\n      x0 = vminq_f32(clamp_max_vec, x0);\n      x1 = vminq_f32(clamp_max_vec, x1);\n      x2 = vminq_f32(clamp_max_vec, x2);\n      x3 = vminq_f32(clamp_max_vec, x3);\n      vst1q_f32(array_ptr + i, x0);\n      vst1q_f32(array_ptr + i + 4, x1);"
},
{
    "Id": 230,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7008e41f183ae9de3f4656067932b36afa822ef2",
    "Violation": "missing",
    "Bug report": " Fix the check for empty reduction indices. In the general case indices can be any rank.",
    "Number of deleted lines": 2,
    "Deleted lines": "      for (NodeDef* consumer : consumers) {\n        for (int i = 0; i < consumer->input_size(); ++i) {\n          const string& input = consumer->input(i);\n          if (input == false_port) {\n            consumer->set_input(i, false_node->name());\n            node_map_->UpdateInput(consumer->name(), false_port,\n                                   false_node->name());\n          } else if (input == true_port) {\n            consumer->set_input(i, true_node->name());\n            node_map_->UpdateInput(consumer->name(), true_port,\n                                   true_node->name());\n          }\n        }\n      }\n      return true;\n    }\n  }\n  return false;\n}\n\nbool ConstantFolding::IsReductionWithConstantIndices(\n    const NodeDef& node, bool* indices_is_empty) const {\n  // Ensure its an appropriate Reduce node.\n  if (!IsReduction(node) || node.input_size() < 2) {\n    return false;\n  }\n  // Ensure that the axes to reduce by are constant.\n  NodeDef* reductions_indices = node_map_->GetNode(node.input(1));\n  if (!IsReallyConstant(*reductions_indices) ||\n      !reductions_indices->attr().count(\"value\")) {\n    return false;\n  }\n  const TensorProto& reduction_indices_tensor =\n      reductions_indices->attr().at(\"value\").tensor();\n  *indices_is_empty =\n      reduction_indices_tensor.tensor_shape().dim(0).size() == 0;\n  return true;\n}\n\nbool ConstantFolding::IsReductionCandidateForSimplification(\n    const NodeDef& node, const GraphProperties& properties,\n    TensorShapeProto* input_tensor_shape, TensorShapeProto* output_tensor_shape,\n    bool* is_single_element_op) const {\n  // Get the properties of the input & output tensors and check if they both\n  // contain a single element.\n  if (!properties.HasInputProperties(node.name()) ||\n      !properties.HasOutputProperties(node.name())) {\n    return false;\n  }\n  const auto& input_props = properties.GetInputProperties(node.name())[0];\n  const auto& output_props = properties.GetOutputProperties(node.name())[0];\n  if (!input_props.has_shape() || input_props.shape().unknown_rank() ||\n      !output_props.has_shape() || output_props.shape().unknown_rank()) {\n    return false;\n  }\n  *input_tensor_shape = input_props.shape();\n  *output_tensor_shape = output_props.shape();\n  for (int i = 0; i < input_tensor_shape->dim_size(); ++i) {\n    if (input_tensor_shape->dim(i).size() < 0) {\n      return false;\n    }\n  }\n  for (int i = 0; i < output_tensor_shape->dim_size(); ++i) {\n    if (output_tensor_shape->dim(i).size() < 0) {\n      return false;\n    }\n  }\n  const int input_num_elements =\n      TensorShape(*input_tensor_shape).num_elements();\n  const int output_num_elements =\n      TensorShape(*output_tensor_shape).num_elements();\n  *is_single_element_op = input_num_elements == 1 && output_num_elements == 1;"
},
{
    "Id": 231,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/551a90f2e3d20420d68a2796d19f1c42b6636e0d",
    "Violation": "missing",
    "Bug report": " Add checks in ReduceWindowOpOnTensorsConversion. The pattern does not support ops with non-zero padding config. Add a check to prevent unexpected lowering. It is not easy to add tests because other patterns will convert body ops, and it causes issues like invalid IRs.",
    "Number of deleted lines": 0,
    "Deleted lines": "\nstruct ReduceWindowOpOnTensorsConversion\n    : public OpConversionPattern<mhlo::ReduceWindowOp> {\n  using OpConversionPattern<mhlo::ReduceWindowOp>::OpConversionPattern;\n\n  /// mhlo.reduce_window is mapped to a linalg.pooling operation. The type of\n  /// the pooling is determined based on the body of the reduce window\n  /// operation. This class enumerates the different variants.\n  enum class PoolingType {\n    kInvalid,\n    kMin,\n    kMax,\n    kAdd,\n  };\n\n  static PoolingType getPoolingType(mhlo::ReduceWindowOp reduce_op,\n                                    int result_index) {\n    if (Operation* op = reduce_op.getReductionOp(result_index)) {\n      if (isa<mhlo::MinOp>(*op)) return PoolingType::kMin;\n      if (isa<mhlo::MaxOp>(*op)) return PoolingType::kMax;\n      if (isa<mhlo::AddOp>(*op)) return PoolingType::kAdd;\n    }\n    return PoolingType::kInvalid;\n  }\n\n  LogicalResult matchAndRewrite(\n      mhlo::ReduceWindowOp op, ArrayRef<Value> args,\n      ConversionPatternRewriter& rewriter) const override {\n    auto loc = op.getLoc();\n    int rank = op.getResultTypes()[0].cast<ShapedType>().getRank();\n    if (rank != 4) {\n      return rewriter.notifyMatchFailure(op, \"expected NHWC pooling-based op\");\n    }\n\n    SmallVector<int64_t, 2> shapes;\n    shapes.push_back(op.window_dimensions().getValue<int64_t>(1));\n    shapes.push_back(op.window_dimensions().getValue<int64_t>(2));\n\n    if (op.window_strides() &&\n        (op.window_strides().getValue().getValue<int64_t>(0) != 1 ||\n         op.window_strides().getValue().getValue<int64_t>(3) != 1)) {\n      return rewriter.notifyMatchFailure(\n          op, \"expected window_strides to be [1,x,y,1]\");\n    }\n    if (op.window_dimensions() &&\n        (op.window_dimensions().getValue<int64_t>(0) != 1 ||\n         op.window_dimensions().getValue<int64_t>(3) != 1)) {\n      return rewriter.notifyMatchFailure(\n          op, \"expected window_dimensions to be [1,x,y,1]\");\n    }\n\n    Attribute strides;\n    if (op.window_stridesAttr()) {\n      strides = rewriter.getI64VectorAttr(\n          {op.window_strides().getValue().getValue<int64_t>(1),\n           op.window_strides().getValue().getValue<int64_t>(2)});\n    } else {\n      strides = rewriter.getI64VectorAttr({1, 1});\n    }\n    Attribute dilations;\n    if (op.window_dilations()) {\n      dilations = rewriter.getI64VectorAttr(\n          {op.window_dilations().getValue().getValue<int64_t>(1),\n           op.window_dilations().getValue().getValue<int64_t>(2)});\n    } else {\n      dilations = rewriter.getI64VectorAttr({1, 1});\n    }\n\n    SmallVector<Value> pooling_ops;\n"
},
{
    "Id": 232,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f6f62119587baf8ccb7378ceac86bacd2db2863d",
    "Violation": "missing",
    "Bug report": "Add missing validation in maxpooling_op.cc ",
    "Number of deleted lines": 0,
    "Deleted lines": "      auto value_stride = tensor_stride.flat<int32>();\n      stride.resize(tensor_stride.shape().num_elements());\n      std::copy_n(&value_stride(0), stride.size(), stride.begin());\n    }\n\n    OP_REQUIRES(context, ksize.size() == 4,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 4 dimensions\"));\n    OP_REQUIRES(context, stride.size() == 4,\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\"));\n    OP_REQUIRES(context, ksize[0] == 1 && stride[0] == 1,\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    OP_REQUIRES(\n        context, ksize[3] == 1 && stride[3] == 1,\n        errors::Unimplemented(\n            \"MaxPoolingGrad is not yet supported on the depth dimension.\"));\n\n    PoolParameters params{context,\n                          ksize,\n                          stride,\n                          padding_,\n                          explicit_paddings_,\n                          FORMAT_NHWC,\n                          tensor_in.shape()};\n    if (!context->status().ok()) {\n      return;\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, output_shape, &output));\n\n    SpatialMaxPoolWithArgMaxHelper<CPUDevice, T, int64>(\n        context, &tensor_out_dup, &tensor_out_arg_max, output, tensor_in,\n        out_backprop, params, true);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  std::vector<int64> explicit_paddings_;\n  TensorFormat data_format_;\n};\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <class T>\nclass MaxPoolingGradOp<Eigen::GpuDevice, T> : public OpKernel {\n public:\n  typedef Eigen::GpuDevice Device;\n\n  explicit MaxPoolingGradOp(OpKernelConstruction* context) : OpKernel(context) {\n    string data_format;\n    OP_REQUIRES_OK(context, context->GetAttr(\"data_format\", &data_format));\n    OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n                errors::InvalidArgument(\"Invalid data format\"));\n    if (context->num_inputs() == 3) {\n      OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n      OP_REQUIRES(context, ksize_.size() == 4,\n                  errors::InvalidArgument(\"Sliding window ksize field must \"\n                                          \"specify 4 dimensions\"));\n      OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n      OP_REQUIRES(context, stride_.size() == 4,\n                  errors::InvalidArgument(\"Sliding window strides field must \"\n                                          \"specify 4 dimensions\"));\n      const int32 ksize_n = GetTensorDim(ksize_, data_format_, 'N');\n      const int32 stride_n = GetTensorDim(stride_, data_format_, 'N');"
},
{
    "Id": 233,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b86513673b98ac6c4458033fcda718365539afae",
    "Violation": "missing",
    "Bug report": "added check for zero stride values to strided slice ",
    "Number of deleted lines": 0,
    "Deleted lines": "    Node* node = graph->NewNode();\n    node->operation.type = ToString(OperationType::SLICE);\n    RETURN_IF_ERROR(reader->AddOutputs(node));\n    Value<TensorRef<BHWC>>* input;\n    RETURN_IF_ERROR(reader->ReadValue(0, &input));\n    RETURN_IF_ERROR(graph->AddConsumer(node->id, input->id));\n\n    Tensor<Linear, DataType::INT32> tmp;\n    RETURN_IF_ERROR(reader->ReadTensor(1, &tmp));\n\n    bool read_without_batch = tmp.data.size() == 3;\n    bool read_with_batch = tmp.data.size() == 4;\n    if (!read_without_batch && !read_with_batch) {\n      return UnimplementedError(\n          \"Slicing is supported for 3 or 4 dimensional tensors only.\");\n    }\n\n    const auto* tf_options = reinterpret_cast<const TfLiteStridedSliceParams*>(\n        tflite_node->builtin_data);\n    auto out_shape = graph->FindOutputs(node->id)[0]->tensor.shape;\n    if (!tf_options) {\n      return InternalError(\"Missing tflite params\");\n    }\n    RETURN_IF_ERROR(CheckOptionsSupport(tf_options));\n\n    SliceAttributes attr;\n    if (read_without_batch) {\n      RETURN_IF_ERROR(ReadAttribsWithoutBatch(reader, tf_options,\n                                              input->tensor.shape, &attr));\n    }\n    if (read_with_batch) {\n      RETURN_IF_ERROR(\n          ReadAttribsWithBatch(reader, tf_options, input->tensor.shape, &attr));\n    }\n    if (attr.strides.h < 0 || attr.strides.w < 0 || attr.strides.c < 0) {\n      return UnimplementedError(\"Reverse slices are not supported.\");\n    }\n    if ((attr.ends.h - attr.starts.h + attr.strides.h - 1) / attr.strides.h !=\n        out_shape.h) {\n      return UnimplementedError(\"Output height doesn't match\");\n    }\n    if ((attr.ends.w - attr.starts.w + attr.strides.w - 1) / attr.strides.w !=\n        out_shape.w) {\n      return UnimplementedError(\"Output width doesn't match\");\n    }\n    if ((attr.ends.c - attr.starts.c + attr.strides.c - 1) / attr.strides.c !=\n        out_shape.c) {\n      return UnimplementedError(\"Output channels don't match\");\n    }\n    node->operation.attributes = attr;\n    return OkStatus();\n  }\n\n private:\n  Status UpdateWithMask(const TfLiteStridedSliceParams* tf_options,\n                        const BHWC& input_shape, int ignore_h, int ignore_w,\n                        int ignore_c, SliceAttributes* attr) {\n    if (tf_options->begin_mask & ignore_h) {\n      attr->starts.h = 0;\n    }\n    if (tf_options->begin_mask & ignore_w) {\n      attr->starts.w = 0;\n    }\n    if (tf_options->begin_mask & ignore_c) {\n      attr->starts.c = 0;\n    }\n\n    if (tf_options->end_mask & ignore_h) {\n      attr->ends.h = input_shape.h;\n    }"
},
{
    "Id": 234,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4377a561b7757ed83757f07532e6564c42c286ba",
    "Violation": "missing",
    "Bug report": " Add a check for group size when sorting grouped AllReduces within a block.",
    "Number of deleted lines": 0,
    "Deleted lines": "      std::vector<mlir::TF::DTensorAllReduceOp> ordered_all_reduces;\n      std::vector<mlir::Block*> ordered_blocks;\n      llvm::DenseSet<mlir::Block*> blocks;\n\n      cluster.GetBody().walk([&](mlir::TF::DTensorAllReduceOp all_reduce) {\n        if (!all_reduce.getDeviceType().contains(\"TPU\")) {\n          // Only combine all reduces for GPU and CPU\n          auto all_reduce_ranked_type =\n              all_reduce.getType().dyn_cast<mlir::RankedTensorType>();\n\n          if (all_reduce_ranked_type &&\n              all_reduce_ranked_type.hasStaticShape()) {\n            // Static known shape is required to merge all reduces. If shape is\n            // not known skip merging.\n            ordered_all_reduces.push_back(all_reduce);\n\n            blocks.insert(all_reduce->getBlock());\n          }\n        }\n      });\n\n      if (ordered_all_reduces.size() > 1) {\n        // Create dependency graph for all all_reduce operations, so that that\n        // independent ops can be merged\n        auto all_reduce_groups =\n            createIndependentReduceOpsGroups(ordered_all_reduces);\n\n        VLOG(2) << ordered_all_reduces.size() << \" all-reduce ops in \"\n                << all_reduce_groups.size() << \" groups\";\n\n        all_reduce_groups = createSubgroupsByElemType(all_reduce_groups);\n        all_reduce_groups = createSubgroupsByReductionAttr(all_reduce_groups);\n        all_reduce_groups = createSubgroupsByGroupAssignment(all_reduce_groups);\n\n        std::sort(all_reduce_groups.begin(), all_reduce_groups.end(),\n                  [](std::vector<mlir::TF::DTensorAllReduceOp> lhs,\n                     std::vector<mlir::TF::DTensorAllReduceOp> rhs) {\n                    if (lhs[0]->getBlock() == rhs[0]->getBlock())\n                      return lhs[0]->isBeforeInBlock(rhs[0]);\n                    return true;\n                  });\n        for (const auto& reduce_group : all_reduce_groups) {\n          if (reduce_group.size() > 1) {\n            VLOG(4) << \"Combining following reduce ops into one: ------------\";\n            for (auto reduce_op : reduce_group) {\n              VLOG(4) << mlir::GetNameFromLoc(reduce_op.getLoc());\n            }\n            VLOG(4) << \"-----------------------------------------------------\";\n          }\n          if (mlir::failed(CombineAllReduceOps(cluster, reduce_group))) {\n            return signalPassFailure();\n          }\n        }\n\n        for (auto* b : blocks) {\n          mlir::sortTopologically(b);\n        }\n      }\n    });\n  }\n};\n\n}  // namespace\n\nstd::unique_ptr<mlir::OperationPass<mlir::func::FuncOp>>\nCreateDTensorAllReduceCombineOptimization() {\n  return std::make_unique<DTensorAllReduceCombineOptimization>();\n}\n\n}  // namespace dtensor\n}  // namespace tensorflow\n"
},
{
    "Id": 235,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/31bd5026304677faa8a0b77602c6154171b9aec1",
    "Violation": "missing",
    "Bug report": " Prevent check fail in FFT ",
    "Number of deleted lines": 0,
    "Deleted lines": "          DCHECK_EQ(out->dtype(), DT_COMPLEX64);\n          DoRealForwardFFT<float, complex64>(ctx, fft_shape, in, out);\n        }\n      } else {\n        if (is_complex128) {\n          DCHECK_EQ(in.dtype(), DT_COMPLEX128);\n          DCHECK_EQ(out->dtype(), DT_DOUBLE);\n          DoRealBackwardFFT<complex128, double>(ctx, fft_shape, in, out);\n        } else {\n          DCHECK_EQ(in.dtype(), DT_COMPLEX64);\n          DCHECK_EQ(out->dtype(), DT_FLOAT);\n          DoRealBackwardFFT<complex64, float>(ctx, fft_shape, in, out);\n        }\n      }\n    }\n  }\n\n  template <typename RealT, typename ComplexT>\n  void DoRealForwardFFT(OpKernelContext* ctx, uint64* fft_shape,\n                        const Tensor& in, Tensor* out) {\n    // Create the axes (which are always trailing).\n    const auto axes = Eigen::ArrayXi::LinSpaced(FFTRank, 1, FFTRank);\n    auto device = ctx->eigen_device<CPUDevice>();\n    auto input = Tensor(in).flat_inner_dims<RealT, FFTRank + 1>();\n    const auto input_dims = input.dimensions();\n\n    // Slice input to fft_shape on its inner-most dimensions.\n    Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> input_slice_sizes;\n    input_slice_sizes[0] = input_dims[0];\n    TensorShape temp_shape{input_dims[0]};\n    for (int i = 1; i <= FFTRank; ++i) {\n      input_slice_sizes[i] = fft_shape[i - 1];\n      temp_shape.AddDim(fft_shape[i - 1]);\n    }\n\n    auto output = out->flat_inner_dims<ComplexT, FFTRank + 1>();\n    const Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> zero_start_indices;\n\n    // Compute the full FFT using a temporary tensor.\n    Tensor temp;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<ComplexT>::v(),\n                                           temp_shape, &temp));\n    auto full_fft = temp.flat_inner_dims<ComplexT, FFTRank + 1>();\n    full_fft.device(device) =\n        input.slice(zero_start_indices, input_slice_sizes)\n            .template fft<Eigen::BothParts, Eigen::FFT_FORWARD>(axes);\n\n    // Slice away the negative frequency components.\n    output.device(device) =\n        full_fft.slice(zero_start_indices, output.dimensions());\n  }\n\n  template <typename ComplexT, typename RealT>\n  void DoRealBackwardFFT(OpKernelContext* ctx, uint64* fft_shape,\n                         const Tensor& in, Tensor* out) {\n    auto device = ctx->eigen_device<CPUDevice>();\n    // Reconstruct the full FFT and take the inverse.\n    auto input = Tensor(in).flat_inner_dims<ComplexT, FFTRank + 1>();\n    auto output = out->flat_inner_dims<RealT, FFTRank + 1>();\n    const auto input_dims = input.dimensions();\n\n    // Calculate the shape of the temporary tensor for the full FFT and the\n    // region we will slice from input given fft_shape. We slice input to\n    // fft_shape on its inner-most dimensions, except the last (which we\n    // slice to fft_shape[-1] / 2 + 1).\n    Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> input_slice_sizes;\n    input_slice_sizes[0] = input_dims[0];\n    TensorShape full_fft_shape;\n    full_fft_shape.AddDim(input_dims[0]);\n    for (auto i = 1; i <= FFTRank; i++) {"
},
{
    "Id": 236,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/25bae42b3022b00788a29ae6c400922c31f88231",
    "Violation": "insufficient",
    "Bug report": " Add additional length check for inputs ",
    "Number of deleted lines": 1,
    "Deleted lines": "\n  Returns:\n    A `SparseTensor` with the concatenated output.\n\n  Raises:\n    TypeError: If `sp_inputs` is not a list of `SparseTensor`.\n  \"\"\"\n  axis = deprecation.deprecated_argument_lookup(\"axis\", axis, \"concat_dim\",\n                                                concat_dim)\n  sp_inputs = _convert_to_sparse_tensors(sp_inputs)\n\n  if len(sp_inputs) == 1:  # Degenerate case of one tensor.\n    return sp_inputs[0]\n\n  inds = [sp_input.indices for sp_input in sp_inputs]\n  vals = [sp_input.values for sp_input in sp_inputs]\n  shapes = [sp_input.dense_shape for sp_input in sp_inputs]\n\n  if expand_nonconcat_dim:\n    max_shape = math_ops.reduce_max(\n        array_ops.concat(\n            [array_ops.reshape(shape, [1, -1]) for shape in shapes], 0), 0)\n    shapes = [\n        array_ops.concat([\n            max_shape[:axis], shape[-1:]\n            if axis == -1 else shape[axis:axis + 1], []\n            if axis == -1 else max_shape[axis + 1:]\n        ], 0) for shape in shapes\n    ]\n\n  output_ind, output_val, output_shape = (\n      gen_sparse_ops.sparse_concat(inds, vals, shapes, axis, name=name))\n\n  shapes_value = [tensor_util.constant_value(shape) for shape in shapes]\n  if all(shape is not None for shape in shapes_value):\n    dim = sum(shape[axis] for shape in shapes_value)\n    output_shape = shapes_value[0]\n    output_shape[axis] = dim\n    output_shape = tensor_shape.as_shape(output_shape)\n  return sparse_tensor.SparseTensor(output_ind, output_val, output_shape)\n\n\n@tf_export(v1=[\"sparse.add\", \"sparse_add\"])\n@deprecation.deprecated_endpoints(\"sparse_add\")\ndef sparse_add(a, b, thresh=0):\n  \"\"\"Adds two tensors, at least one of each is a `SparseTensor`.\n\n  If one `SparseTensor` and one `Tensor` are passed in, returns a `Tensor`.  If\n  both arguments are `SparseTensor`s, this returns a `SparseTensor`.  The order\n  of arguments does not matter.  Use vanilla `tf.add()` for adding two dense\n  `Tensor`s.\n\n  The shapes of the two operands must match: broadcasting is not supported.\n\n  The indices of any input `SparseTensor` are assumed ordered in standard\n  lexicographic order.  If this is not the case, before this step run\n  `SparseReorder` to restore index ordering.\n\n  If both arguments are sparse, we perform \"clipping\" as follows.  By default,\n  if two values sum to zero at some index, the output `SparseTensor` would still\n  include that particular location in its index, storing a zero in the\n  corresponding value slot.  To override this, callers can specify `thresh`,\n  indicating that if the sum has a magnitude strictly smaller than `thresh`, its\n  corresponding value and index would then not be included.  In particular,\n  `thresh == 0.0` (default) means everything is kept and actual thresholding\n  happens only for a positive value.\n\n  For example, suppose the logical sum of two sparse operands is (densified):\n\n      [       2]\n      [.1     0]"
},
{
    "Id": 237,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e07e48b2e0908333a36f1c5726a9406a83b3ec90",
    "Violation": "missing",
    "Bug report": "Added a check on literal_.has_value() to avoid segfault. ",
    "Number of deleted lines": 0,
    "Deleted lines": "  // temporarily broken in LayoutAssignment::AssignLayouts.\n\n  if (!mutable_array_subshape->has_layout() ||\n      !LayoutUtil::Equal(mutable_array_subshape->layout(), new_layout)) {\n    *literal_ = literal_->Relayout(new_layout, shape_index);\n    *mutable_array_subshape->mutable_layout() = new_layout;\n  }\n}\n\nbool HloConstantInstruction::IdenticalSlowPath(\n    const HloInstruction& other,\n    const std::function<bool(const HloComputation*, const HloComputation*)>&\n        eq_computations) const {\n  const auto& other_slice = static_cast<const HloSliceInstruction&>(other);\n  return literal() == other_slice.literal();\n}\n\nstd::unique_ptr<HloInstruction>\nHloConstantInstruction::CloneWithNewOperandsImpl(\n    const Shape& shape, absl::Span<HloInstruction* const> new_operands,\n    HloCloneContext* context) const {\n  CHECK(literal_.has_value());\n  // Literal's shape may have no/different tiling info. Use this instruction's\n  // shape instead.\n  CHECK(Shape::Equal().MinorToMajorOnlyInLayout()(literal_->shape(),\n                                                  this->shape()));\n  return absl::make_unique<HloConstantInstruction>(literal_->Clone(),\n                                                   this->shape());\n}\n\nstring HloConstantInstruction::OperandsToStringWithCanonicalNameMap(\n    const HloPrintOptions& options,\n    CanonicalNameMap* canonical_name_map) const {\n  if (options.print_only_essential_constants()) {\n    if (literal().IsAll(0)) {\n      return \"0\";\n    }\n    if (literal().IsAll(1)) {\n      return \"1\";\n    }\n    if (shape().IsInteger()) {\n      return literal_->ToStringWithoutShapeOneline();\n    }\n    return \"{...}\";\n  }\n\n  // For constants, show the actual value in place of an empty operand list.\n  if (literal_.has_value() &&\n      ((shape().IsArray() && ShapeUtil::ElementsIn(shape()) <= 10) ||\n       options.print_large_constants())) {\n    // Literal::ToString emits multidimensional arrays over multiple\n    // lines. Compact this into one line by stripping out white space.\n    return literal_->ToStringWithoutShapeOneline();\n  } else {\n    // Do not show large constants or tuples.\n    return \"{...}\";\n  }\n}\n\nHloTraceInstruction::HloTraceInstruction(const string& tag,\n                                         HloInstruction* operand)\n    : HloInstruction(HloOpcode::kTrace, ShapeUtil::MakeNil()),\n      literal_(LiteralUtil::CreateR1U8(tag)) {\n  AppendOperand(operand);\n  operand->set_tracing(this);\n}\n\nHloInstructionProto HloTraceInstruction::ToProto() const {\n  HloInstructionProto proto = HloInstruction::ToProto();\n  *proto.mutable_literal() = literal_.ToProto();"
},
{
    "Id": 238,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cc560f64b6e3e6724517757e9789c52cde224ee9",
    "Violation": "missing",
    "Bug report": " Profiler: restore correct behavior of StartTracing with empty workers list. absl::StrSplit behaves differently from str_util::Split when the passed string is empty. Restore previous behavior by explicitly checking for an empty string.",
    "Number of deleted lines": 1,
    "Deleted lines": "  }\n  return Status::OK();\n}\n\n// Creates an empty event file if not already exists, which indicates that we\n// have a plugins/profile/ directory in the current logdir.\nStatus MaybeCreateEmptyEventFile(const tensorflow::string& logdir) {\n  // Suffix for an empty event file.  it should be kept in sync with\n  // _EVENT_FILE_SUFFIX in tensorflow/python/eager/profiler.py.\n  constexpr char kProfileEmptySuffix[] = \".profile-empty\";\n  std::vector<string> children;\n  TF_RETURN_IF_ERROR(Env::Default()->GetChildren(logdir, &children));\n  for (const string& child : children) {\n    if (absl::EndsWith(child, kProfileEmptySuffix)) {\n      return Status::OK();\n    }\n  }\n  EventsWriter event_writer(io::JoinPath(logdir, \"events\"));\n  return event_writer.InitWithSuffix(kProfileEmptySuffix);\n}\n\n// Starts tracing on a single or multiple TPU hosts and saves the result in the\n// given logdir. If no trace was collected, retries tracing for\n// num_tracing_attempts.\nStatus StartTracing(const tensorflow::string& service_addr,\n                    const tensorflow::string& logdir,\n                    const tensorflow::string& workers_list,\n                    bool include_dataset_ops, int duration_ms,\n                    int num_tracing_attempts) {\n  // Use the current timestamp as the run name.\n  tensorflow::string session_id = GetCurrentTimeStampAsString();\n  constexpr char kProfilePluginDirectory[] = \"plugins/profile/\";\n  tensorflow::string repository_root =\n      io::JoinPath(logdir, kProfilePluginDirectory);\n  std::vector<tensorflow::string> hostnames = absl::StrSplit(workers_list, ',');\n\n  TF_RETURN_IF_ERROR(MaybeCreateEmptyEventFile(logdir));\n\n  Status status = Status::OK();\n  int remaining_attempts = num_tracing_attempts;\n  tensorflow::ProfileOptions opts;\n  opts.set_include_dataset_ops(include_dataset_ops);\n  while (true) {\n    std::cout << \"Starting to profile TPU traces for \" << duration_ms << \" ms. \"\n              << \"Remaining attempt(s): \" << --remaining_attempts << std::endl;\n    if (hostnames.empty()) {\n      status = Profile(service_addr, logdir, duration_ms, repository_root,\n                       session_id, opts);\n    } else {\n      tensorflow::string tpu_master = service_addr;\n      status = NewSession(tpu_master, hostnames, duration_ms, repository_root,\n                          session_id, opts);\n    }\n    if (remaining_attempts <= 0 || status.ok() || !ShouldRetryTracing(status))\n      break;\n    std::cout << \"No trace event is collected. Automatically retrying.\"\n              << std::endl\n              << std::endl;\n  }\n\n  if (ShouldRetryTracing(status)) {\n    std::cout << \"No trace event is collected after \" << num_tracing_attempts\n              << \" attempt(s). \"\n              << \"Perhaps, you want to try again (with more attempts?).\"\n              << std::endl\n              << \"Tip: increase number of attempts with --num_tracing_attempts.\"\n              << std::endl;\n  }\n  return status;\n}\n"
},
{
    "Id": 239,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fe4f74018ec6a7dff2718ea59d0f317460c0b3ad",
    "Violation": "missing",
    "Bug report": " Temporarily check for empty proto fields to avoid a crash for old cached traces. We can remove this code once we land all the necessary changes and invalidate all the caches.",
    "Number of deleted lines": 0,
    "Deleted lines": "        CopySymbolDetailsToDeduplicatedNode(\n            deduplicated_node.mutable_children(0), &deduplicated_node);\n      }\n    }\n  }\n}\n\n// Recursively find computation size for HLOs -- applied only for convolutions.\n// This is only for convolutions, not other HLOs, categories or whole programs.\n// TODO(b/243596435) Find a permanent fix to this problem.\nint64_t GetComputationSize(Node node) {\n  int64_t computation_size = 0;\n  for (const auto& child : node.children()) {\n    if (GetComputationSize(child) != 0) {\n      computation_size = GetComputationSize(child);\n    }\n  }\n  if (node.has_xla()) {\n    if (node.xla().computation_primitive_size() > 0) {\n      return node.xla().computation_primitive_size();\n    } else {\n      return computation_size;\n    }\n  }\n  return 0;\n}\n\n// Fills op metrics into a node.\nvoid PopulateOpMetricsNode(\n    const OpMetrics& op_metrics, double peak_gigaflops_per_second_per_core,\n    std::vector<double> peak_mem_gibibytes_per_second_per_core,\n    uint64_t total_time_ps, Node* node) {\n  DCHECK_EQ(ChildrenTimePs(op_metrics), 0);\n\n  Metrics* metrics = node->mutable_metrics();\n  // The UI computes flops_rate = raw_flops / raw_time\n  // and memory_bandwidth = raw_bytes_accessed / raw_time. See:\n  // https://github.com/tensorflow/profiler/blob/master/frontend/app/common/utils/utils.ts\n  metrics->set_raw_time(op_metrics.time_ps());\n  metrics->set_raw_flops(op_metrics.flops());\n  metrics->set_raw_bytes_accessed(op_metrics.bytes_accessed());\n\n  // \"time\" is the op or category fraction of total time.\n  metrics->set_time(SafeDivide(op_metrics.time_ps(), total_time_ps));\n\n  // Hack to approximate utilization for INT8/4 convolution HLOs:\n  // Since MXU BW is 2x/4x for INT8/4, multiply peak BW by the factor detemrined\n  // by the computation size\n  if (GetComputationSize(*node) == 8) {\n    peak_gigaflops_per_second_per_core *= 2;\n  } else if (GetComputationSize(*node) == 4) {\n    peak_gigaflops_per_second_per_core *= 4;\n  }\n  double flops_utilization = SafeDivide(GigaFlopsPerSecondPerCore(op_metrics),\n                                        peak_gigaflops_per_second_per_core);\n  // The UI expects flops_utilization = flops / time. See:\n  // https://github.com/tensorflow/profiler/blob/master/frontend/app/common/utils/utils.ts\n  metrics->set_flops(flops_utilization * metrics->time());\n\n  // TODO(b/219984562): Use hierarchical roofline.\n  // For now, capture both overall and off-chip memory utilization.\n  const double mem_bw_utilization = SafeDivide(\n      GibiBytesPerSecondPerCore(op_metrics, -1,\n                                OpMetrics::MemoryAccessed::UNKNOWN),\n      peak_mem_gibibytes_per_second_per_core[MemBwType::MEM_BW_TYPE_ALL]);\n  metrics->set_memory_bandwidth_util(mem_bw_utilization);\n\n  const uint64 kHbm = 1;\n  const double hbm_bw_gibibytes_per_second = GibiBytesPerSecondPerCore(\n      op_metrics, kHbm, OpMetrics::MemoryAccessed::UNKNOWN);"
},
{
    "Id": 240,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c56d0cd8ce8239ee369fac1ae6b9cae67fd4c447",
    "Violation": "missing",
    "Bug report": " Avoid signed integer overflow when loading tensors with both 0 and large dims. `TensorShapeBase` ensures `num_elements` doesn't overflow when adding a new dimension. However, this check is insufficient to prevent other functions that use a different multiplication order from hitting an overflow _if any of the dimensions are 0_. For example, Eigen currently multiplies dimensions in reverse order, so dimensions of (0, 4294967296,4294967296) will trigger an overflow in Eigen code. To prevent overflow for all multiplication orders, we can that `num_elements` doesn't overflow if zero dimensions are skipped.",
    "Number of deleted lines": 0,
    "Deleted lines": "\ntemplate <class Shape>\nTensorShapeBase<Shape>::TensorShapeBase(const TensorShapeProto& proto) {\n  set_tag(REP16);\n  set_data_type(DT_INVALID);\n  // NOTE(irving): Unfortunately, TensorShape allows parsing protos with\n  // unknown_shape() set, and it seems hard to remove this without backwards\n  // compatibility issues.\n  if (kIsPartial && proto.unknown_rank()) {\n    set_ndims_byte(kUnknownRank);\n    set_num_elements(-1);\n  } else {\n    set_ndims_byte(0);\n    set_num_elements(1);\n    for (const auto& d : proto.dim()) {\n      AddDim(d.size());\n    }\n  }\n}\n\ntemplate <class Shape>\nStatus TensorShapeBase<Shape>::BuildTensorShapeBase(\n    const TensorShapeProto& proto, TensorShapeBase* out) {\n  out->set_tag(REP16);\n  out->set_data_type(DT_INVALID);\n  // NOTE(irving): Unfortunately, TensorShape allows parsing protos with\n  // unknown_shape() set, and it seems hard to remove this without backwards\n  // compatibility issues.\n  if (kIsPartial && proto.unknown_rank()) {\n    out->set_ndims_byte(kUnknownRank);\n    out->set_num_elements(-1);\n  } else {\n    out->set_ndims_byte(0);\n    out->set_num_elements(1);\n    Status s = OkStatus();\n    for (const auto& d : proto.dim()) {\n      s = out->AddDimWithStatus(d.size());\n      if (!s.ok()) {\n        return s;\n      }\n    }\n  }\n  return OkStatus();\n}\n\ntemplate <class Shape>\nTensorShapeBase<Shape>::TensorShapeBase(gtl::ArraySlice<int64_t> dim_sizes) {\n  set_tag(REP16);\n  set_data_type(DT_INVALID);\n  TF_CHECK_OK(InitDims(dim_sizes));\n}\n\ntemplate <class Shape>\nStatus TensorShapeBase<Shape>::BuildTensorShapeBase(\n    gtl::ArraySlice<int64_t> dim_sizes, TensorShapeBase* out) {\n  out->set_tag(REP16);\n  out->set_data_type(DT_INVALID);\n  return out->InitDims(dim_sizes);\n}\n\n// Returns true iff partial is true and val is < 0.\n// REQUIRES: val < kMaxRep16\n// REQUIRES: partial || val >= 0\nstatic inline bool Set16(bool partial, uint16* dst, int dim, int64_t val) {\n  if (partial) {\n    if (val < 0) {\n      dst[dim] = std::numeric_limits<uint16>::max();\n      return true;\n    }\n  }\n  dst[dim] = val;\n  return false;\n}\n\ntemplate <class Shape>\nStatus TensorShapeBase<Shape>::InitDims(gtl::ArraySlice<int64_t> dim_sizes) {"
},
{
    "Id": 241,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/80bb2f5511e7d2d386c79da52ff517691e19ac54",
    "Violation": "missing",
    "Bug report": " Add check condition for large values of range_max, which is causing session abort.",
    "Number of deleted lines": 0,
    "Deleted lines": "\n  In addition, this operation returns tensors `true_expected_count`\n  and `sampled_expected_count` representing the number of times each\n  of the target classes (`true_classes`) and the sampled\n  classes (`sampled_candidates`) is expected to occur in an average\n  tensor of sampled classes.  These values correspond to `Q(y|x)`\n  defined in [this\n  document](http://www.tensorflow.org/extras/candidate_sampling.pdf).\n  If `unique=True`, then these are post-rejection probabilities and we\n  compute them approximately.\n\n  Args:\n    true_classes: A `Tensor` of type `int64` and shape `[batch_size,\n      num_true]`. The target classes.\n    num_true: An `int`.  The number of target classes per training example.\n    num_sampled: An `int`.  The number of classes to randomly sample.\n    unique: A `bool`. Determines whether all sampled classes in a batch are\n      unique.\n    range_max: An `int`. The number of possible classes.\n    seed: An `int`. An operation-specific seed. Default is 0.\n    name: A name for the operation (optional).\n\n  Returns:\n    sampled_candidates: A tensor of type `int64` and shape `[num_sampled]`.\n      The sampled classes.\n    true_expected_count: A tensor of type `float`.  Same shape as\n      `true_classes`. The expected counts under the sampling distribution\n      of each of `true_classes`.\n    sampled_expected_count: A tensor of type `float`. Same shape as\n      `sampled_candidates`. The expected counts under the sampling distribution\n      of each of `sampled_candidates`.\n\n  \"\"\"\n  seed1, seed2 = random_seed.get_seed(seed)\n  return gen_candidate_sampling_ops.learned_unigram_candidate_sampler(\n      true_classes, num_true, num_sampled, unique, range_max, seed=seed1,\n      seed2=seed2, name=name)\n\n\n@tf_export('random.fixed_unigram_candidate_sampler',\n           'nn.fixed_unigram_candidate_sampler')\n@dispatch.add_dispatch_support\ndef fixed_unigram_candidate_sampler(true_classes,\n                                    num_true,\n                                    num_sampled,\n                                    unique,\n                                    range_max,\n                                    vocab_file='',\n                                    distortion=1.0,\n                                    num_reserved_ids=0,\n                                    num_shards=1,\n                                    shard=0,\n                                    unigrams=(),\n                                    seed=None,\n                                    name=None):\n  \"\"\"Samples a set of classes using the provided (fixed) base distribution.\n\n  This operation randomly samples a tensor of sampled classes\n  (`sampled_candidates`) from the range of integers `[0, range_max)`.\n\n  The elements of `sampled_candidates` are drawn without replacement\n  (if `unique=True`) or with replacement (if `unique=False`) from\n  the base distribution.\n\n  The base distribution is read from a file or passed in as an\n  in-memory array. There is also an option to skew the distribution by\n  applying a distortion power to the weights.\n\n  In addition, this operation returns tensors `true_expected_count`\n  and `sampled_expected_count` representing the number of times each"
},
{
    "Id": 242,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4ea68093eeaf4c4157368668afd7f809b806a504",
    "Violation": "missing",
    "Bug report": "Add negative parameter validation to convolution layers. ",
    "Number of deleted lines": 0,
    "Deleted lines": "        bias after being updated by an `Optimizer`.\n  \"\"\"\n\n  def __init__(self,\n               rank,\n               filters,\n               kernel_size,\n               strides=1,\n               padding='valid',\n               data_format=None,\n               dilation_rate=1,\n               groups=1,\n               activation=None,\n               use_bias=True,\n               kernel_initializer='glorot_uniform',\n               bias_initializer='zeros',\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               trainable=True,\n               name=None,\n               conv_op=None,\n               **kwargs):\n    super(Conv, self).__init__(\n        trainable=trainable,\n        name=name,\n        activity_regularizer=regularizers.get(activity_regularizer),\n        **kwargs)\n    self.rank = rank\n\n    if isinstance(filters, float):\n      filters = int(filters)\n    self.filters = filters\n    self.groups = groups or 1\n    self.kernel_size = conv_utils.normalize_tuple(\n        kernel_size, rank, 'kernel_size')\n    self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n    self.padding = conv_utils.normalize_padding(padding)\n    self.data_format = conv_utils.normalize_data_format(data_format)\n    self.dilation_rate = conv_utils.normalize_tuple(\n        dilation_rate, rank, 'dilation_rate')\n\n    self.activation = activations.get(activation)\n    self.use_bias = use_bias\n\n    self.kernel_initializer = initializers.get(kernel_initializer)\n    self.bias_initializer = initializers.get(bias_initializer)\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n    self.bias_regularizer = regularizers.get(bias_regularizer)\n    self.kernel_constraint = constraints.get(kernel_constraint)\n    self.bias_constraint = constraints.get(bias_constraint)\n    self.input_spec = InputSpec(min_ndim=self.rank + 2)\n\n    self._validate_init()\n    self._is_causal = self.padding == 'causal'\n    self._channels_first = self.data_format == 'channels_first'\n    self._tf_data_format = conv_utils.convert_data_format(\n        self.data_format, self.rank + 2)\n\n  def _validate_init(self):\n    if self.filters is not None and self.filters % self.groups != 0:\n      raise ValueError(\n          'The number of filters must be evenly divisible by the number of '\n          'groups. Received: groups={}, filters={}'.format(\n              self.groups, self.filters))\n\n    if not all(self.kernel_size):\n      raise ValueError('The argument `kernel_size` cannot contain 0(s). '"
},
{
    "Id": 243,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/199f1ff12a28d571100b323ec54a5eee47078d8b",
    "Violation": "missing",
    "Bug report": " Add necessary check in fft ops to fix crash. This PR tries to address the issue raised in 55263 where tf.single.rfft2d will crash when length contains negative value.",
    "Number of deleted lines": 0,
    "Deleted lines": "#include \"tensorflow/core/platform/stream_executor.h\"\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nnamespace tensorflow {\n\nclass FFTBase : public OpKernel {\n public:\n  explicit FFTBase(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& in = ctx->input(0);\n    const TensorShape& input_shape = in.shape();\n    const int fft_rank = Rank();\n    OP_REQUIRES(\n        ctx, input_shape.dims() >= fft_rank,\n        errors::InvalidArgument(\"Input must have rank of at least \", fft_rank,\n                                \" but got: \", input_shape.DebugString()));\n\n    Tensor* out;\n    TensorShape output_shape = input_shape;\n    uint64 fft_shape[3] = {0, 0, 0};\n\n    // In R2C or C2R mode, we use a second input to specify the FFT length\n    // instead of inferring it from the input shape.\n    if (IsReal()) {\n      const Tensor& fft_length = ctx->input(1);\n      OP_REQUIRES(ctx,\n                  fft_length.shape().dims() == 1 &&\n                      fft_length.shape().dim_size(0) == fft_rank,\n                  errors::InvalidArgument(\"fft_length must have shape [\",\n                                          fft_rank, \"]\"));\n\n      auto fft_length_as_vec = fft_length.vec<int32>();\n      for (int i = 0; i < fft_rank; ++i) {\n        fft_shape[i] = fft_length_as_vec(i);\n        // Each input dimension must have length of at least fft_shape[i]. For\n        // IRFFTs, the inner-most input dimension must have length of at least\n        // fft_shape[i] / 2 + 1.\n        bool inner_most = (i == fft_rank - 1);\n        uint64 min_input_dim_length =\n            !IsForward() && inner_most ? fft_shape[i] / 2 + 1 : fft_shape[i];\n        auto input_index = input_shape.dims() - fft_rank + i;\n        OP_REQUIRES(\n            ctx,\n            // We pass through empty tensors, so special case them here.\n            input_shape.dim_size(input_index) == 0 ||\n                input_shape.dim_size(input_index) >= min_input_dim_length,\n            errors::InvalidArgument(\n                \"Input dimension \", input_index,\n                \" must have length of at least \", min_input_dim_length,\n                \" but got: \", input_shape.dim_size(input_index)));\n        uint64 dim = IsForward() && inner_most && fft_shape[i] != 0\n                         ? fft_shape[i] / 2 + 1\n                         : fft_shape[i];\n        output_shape.set_dim(output_shape.dims() - fft_rank + i, dim);\n      }\n    } else {\n      for (int i = 0; i < fft_rank; ++i) {\n        fft_shape[i] =\n            output_shape.dim_size(output_shape.dims() - fft_rank + i);\n      }\n    }\n\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &out));\n\n    if (IsReal()) {\n      if (IsForward()) {\n        OP_REQUIRES(\n            ctx,\n            (in.dtype() == DT_FLOAT && out->dtype() == DT_COMPLEX64) ||"
},
{
    "Id": 244,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/076f909b70b251daea6c443c9b1929b9745aed20",
    "Violation": "improper",
    "Bug report": "fix boolean expression in length check ",
    "Number of deleted lines": 1,
    "Deleted lines": "\nclass TensorListSplitOp : public XlaOpKernel {\n public:\n  explicit TensorListSplitOp(OpKernelConstruction* ctx) : XlaOpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"element_dtype\", &dtype_));\n    // Only non-nested TensorList is supported for now.\n    OP_REQUIRES(\n        ctx, dtype_ != DT_VARIANT,\n        errors::Unimplemented(\n            \"Only non-nested TensorList is supported for TensorListReserve.\"));\n  }\n\n  void Compile(XlaOpKernelContext* ctx) override {\n    xla::XlaOp input_tensor = ctx->Input(0);\n\n    xla::XlaBuilder* b = input_tensor.builder();\n    auto shape_or = b->GetShape(input_tensor);\n    OP_REQUIRES_OK(ctx, shape_or.status());\n    xla::Shape element_shape = std::move(shape_or).value();\n    std::vector<int64_t> element_dims =\n        xla::SpanToVector(element_shape.dimensions());\n    OP_REQUIRES(\n        ctx, !element_dims.empty(),\n        errors::Unimplemented(\"Element dimensions have to be non-empty\"));\n\n    std::vector<int64_t> lengths;\n    OP_REQUIRES_OK(ctx, ctx->ConstantInputAsIntVector(2, &lengths));\n    OP_REQUIRES(ctx, !lengths.empty(),\n                errors::Unimplemented(\"Length has to be non-empty\"));\n    int64_t length = lengths[0];\n    for (int64_t len : lengths) {\n      OP_REQUIRES(ctx, len == length,\n                  errors::Unimplemented(\"All lengths have to be the same\"));\n    }\n    OP_REQUIRES(ctx, length,\n                errors::Unimplemented(\"All lengths must be positive\"));\n    OP_REQUIRES(\n        ctx, element_dims[0] % length == 0,\n        errors::Unimplemented(\"Buffer size has to be a multiple of length\"));\n    std::vector<int64_t> new_dims = {element_dims[0] / length, length};\n    for (int i = 1; i < element_dims.size(); i++) {\n      new_dims.push_back(element_dims[i]);\n    }\n\n    xla::XlaOp reshaped = xla::Reshape(input_tensor, new_dims);\n\n    xla::XlaOp result;\n    OP_REQUIRES_OK(ctx, ExecuteTensorListFromTensor(length, reshaped, &result));\n    ctx->SetTensorListOutput(0, result);\n  }\n\n private:\n  DataType dtype_;\n\n  TF_DISALLOW_COPY_AND_ASSIGN(TensorListSplitOp);\n};\n\nREGISTER_XLA_OP(Name(\"TensorListSplit\")\n                    .CompileTimeConstantInput(\"element_shape\")\n                    .CompileTimeConstantInput(\"lengths\"),\n                TensorListSplitOp);\n\nclass TensorListFromTensorOp : public XlaOpKernel {\n public:\n  explicit TensorListFromTensorOp(OpKernelConstruction* ctx)\n      : XlaOpKernel(ctx) {}\n\n  void Compile(XlaOpKernelContext* ctx) override {\n    const TensorShape& tensor_shape = ctx->InputShape(0);\n    int num_elements = tensor_shape.dim_size(0);\n    const xla::XlaOp tensor = ctx->Input(0);"
},
{
    "Id": 245,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3acc8eaf602b3e9a009f54e1e0164644dd793831",
    "Violation": "missing",
    "Bug report": "Add sanity check for resize-bilinear input shape. ",
    "Number of deleted lines": 1,
    "Deleted lines": "Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/c_api_internal.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n#include \"tensorflow/lite/kernels/op_macros.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace resize_bilinear {\n\n// This file has three implementation of RESIZE_BILINEAR.\nenum KernelType {\n  kReference,\n  kGenericOptimized,  // Neon-free\n  kNeonOptimized,\n};\n\nconstexpr int kInputTensor = 0;\nconstexpr int kSizeTensor = 1;\nconstexpr int kOutputTensor = 0;\n\nTfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n                                const TfLiteTensor* input,\n                                const TfLiteTensor* size,\n                                TfLiteTensor* output) {\n  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\n  output_size->data[0] = input->dims->data[0];\n  const int32* size_data = GetTensorData<int32>(size);\n  output_size->data[1] = size_data[0];\n  output_size->data[2] = size_data[1];\n  output_size->data[3] = input->dims->data[3];\n  return context->ResizeTensor(context, output, output_size);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n  const TfLiteTensor* size = GetInput(context, node, kSizeTensor);\n  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n\n  // TODO(ahentz): Our current implementations rely on the inputs being 4D.\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(size), 1);\n\n  TF_LITE_ENSURE_EQ(context, size->type, kTfLiteInt32);\n  // ResizeBilinear creates a float tensor even when the input is made of\n  // integers.\n  output->type = input->type;\n\n  if (!IsConstantTensor(size)) {\n    SetTensorToDynamic(output);\n    return kTfLiteOk;\n  }\n  return ResizeOutputTensor(context, input, size, output);\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteResizeBilinearParams*>(node->builtin_data);\n\n  const TfLiteTensor* input = GetInput(context, node, kInputTensor);"
},
{
    "Id": 246,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fffbe5a26da2d6fab5a3eb648cefef49db4d38de",
    "Violation": "missing",
    "Bug report": " Check if the session has been deleted before releasing a callable. In some versions of Python, the Session._session field may be cleared (in `Session.__del__()`) before a callable that has a reference to that Session is deleted. Add a defensive check in the `Session._Callable.__del__()` method.",
    "Number of deleted lines": 1,
    "Deleted lines": "            self._session, handle, feed_dict, fetch_list, status)\n\n  # pylint: disable=protected-access\n  class _Callable(object):\n    \"\"\"Experimental wrapper for the C++ `Session::MakeCallable()` API.\"\"\"\n\n    def __init__(self, session, callable_options):\n      self._session = session\n      self._handle = None\n      options_ptr = tf_session.TF_NewBufferFromString(\n          compat.as_bytes(callable_options.SerializeToString()))\n      try:\n        with errors.raise_exception_on_not_ok_status() as status:\n          if session._created_with_new_api:\n            self._handle = tf_session.TF_SessionMakeCallable(\n                session._session, options_ptr, status)\n          else:\n            self._handle = tf_session.TF_DeprecatedSessionMakeCallable(\n                session._session, options_ptr, status)\n      finally:\n        tf_session.TF_DeleteBuffer(options_ptr)\n\n    def __call__(self, *args):\n      # TODO(b/74355905): Support argument and return value nested structures,\n      # and tensor-like objects such as SparseTensors.\n      with errors.raise_exception_on_not_ok_status() as status:\n        if self._session._created_with_new_api:\n          return tf_session.TF_SessionRunCallable(\n              self._session._session, self._handle, args, status, None)\n        else:\n          return tf_session.TF_DeprecatedSessionRunCallable(\n              self._session._session, self._handle, args, status, None)\n\n    def __del__(self):\n      if self._handle is not None:\n        with errors.raise_exception_on_not_ok_status() as status:\n          if self._session._created_with_new_api:\n            tf_session.TF_SessionReleaseCallable(\n                self._session._session, self._handle, status)\n          else:\n            tf_session.TF_DeprecatedSessionReleaseCallable(\n                self._session._session, self._handle, status)\n  # pylint: enable=protected-access\n\n  # TODO(b/74355905): Reimplement `Session.make_callable()` using this method\n  # where possible.\n  def _make_callable_from_options(self, callable_options):\n    \"\"\"Returns a handle to a \"callable\" with the given options.\n\n    Args:\n      callable_options: A `CallableOptions` protocol buffer message describing\n        the computation that will be performed by the callable.\n\n    Returns:\n      A handle to the new callable.\n    \"\"\"\n    self._extend_graph()\n    return BaseSession._Callable(self, callable_options)\n\n\n@tf_export('Session')\nclass Session(BaseSession):\n  \"\"\"A class for running TensorFlow operations.\n\n  A `Session` object encapsulates the environment in which `Operation`\n  objects are executed, and `Tensor` objects are evaluated. For\n  example:\n\n  ```python\n  # Build a graph.\n  a = tf.constant(5.0)"
},
{
    "Id": 247,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ebeb598c2d1f341d6d641bf58c370cf7b43f6e37",
    "Violation": "missing",
    "Bug report": " Correctly check shape not None in Keras add_weight. When calling Keras add_weight with a np list, as written the `shape or ()` \"trick\" results in the following exception: \"\"\"ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\"\"\" This change fixes the problem by using an explicit `if`.",
    "Number of deleted lines": 1,
    "Deleted lines": "      regularizer: regularizer instance (callable).\n      trainable: whether the variable should be part of the layer's\n        \"trainable_variables\" (e.g. variables, biases)\n        or \"non_trainable_variables\" (e.g. BatchNorm mean, stddev).\n        Note, if the current variable scope is marked as non-trainable\n        then this parameter is ignored and any added variables are also\n        marked as non-trainable. `trainable` defaults to `True` unless\n        `synchronization` is set to `ON_READ`.\n      constraint: constraint instance (callable).\n      partitioner: Partitioner to be passed to the `Trackable` API.\n      use_resource: Whether to use `ResourceVariable`.\n      synchronization: Indicates when a distributed a variable will be\n        aggregated. Accepted values are constants defined in the class\n        `tf.VariableSynchronization`. By default the synchronization is set to\n        `AUTO` and the current `DistributionStrategy` chooses\n        when to synchronize. If `synchronization` is set to `ON_READ`,\n        `trainable` must not be set to `True`.\n      aggregation: Indicates how a distributed variable will be aggregated.\n        Accepted values are constants defined in the class\n        `tf.VariableAggregation`.\n      **kwargs: Additional keyword arguments. Accepted values are `getter` and\n        `collections`.\n\n    Returns:\n      The created variable.  Usually either a `Variable` or `ResourceVariable`\n      instance.  If `partitioner` is not `None`, a `PartitionedVariable`\n      instance is returned.\n\n    Raises:\n      RuntimeError: If called with partioned variable regularization and\n        eager execution is enabled.\n      ValueError: When giving unsupported dtype and no initializer or when\n        trainable has been set to True with synchronization set as `ON_READ`.\n    \"\"\"\n    shape = shape or ()\n    # Validate optional keyword arguments.\n    for kwarg in kwargs:\n      if kwarg not in ['getter', 'collections', 'experimental_autocast']:\n        raise TypeError('Unknown keyword argument:', kwarg)\n    getter = kwargs.pop('getter', None)\n    collections = kwargs.pop('collections', None)\n    # 'experimental_autocast' can be set to False by the caller to indicate an\n    # AutoCastVariable should never be created.\n    autocast = kwargs.pop('experimental_autocast', True)\n\n    if dtype is None:\n      dtype = self.dtype or backend.floatx()\n    dtype = dtypes.as_dtype(dtype)\n    if self._dtype is None:\n      self._dtype = dtype.base_dtype.name\n    initializer = initializers.get(initializer)\n    regularizer = regularizers.get(regularizer)\n    constraint = constraints.get(constraint)\n\n    if synchronization == tf_variables.VariableSynchronization.ON_READ:\n      if trainable:\n        raise ValueError(\n            'Synchronization value can be set to '\n            'VariableSynchronization.ON_READ only for non-trainable variables. '\n            'You have specified trainable=True and '\n            'synchronization=VariableSynchronization.ON_READ.')\n      else:\n        # Set trainable to be false when variable is to be synced on read.\n        trainable = False\n    elif trainable is None:\n      trainable = True\n\n    # Initialize variable when no initializer provided\n    if initializer is None:\n      # If dtype is DT_FLOAT, provide a uniform unit scaling initializer\n      if dtype.is_floating:"
},
{
    "Id": 248,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c7c4a42c4372ca560ea415fe3a798e18286cedec",
    "Violation": "improper",
    "Bug report": "Fix an error in keras input_layer.Input() dtype type checking. ",
    "Number of deleted lines": 2,
    "Deleted lines": "          instead of creating a placeholder.\n      sparse: Boolean, whether the placeholder created\n          is meant to be sparse.\n      name: Name of the layer (string).\n  \"\"\"\n\n  def __init__(self,\n               input_shape=None,\n               batch_size=None,\n               dtype=None,\n               input_tensor=None,\n               sparse=False,\n               name=None,\n               **kwargs):\n    if 'batch_input_shape' in kwargs:\n      batch_input_shape = kwargs.pop('batch_input_shape')\n      if input_shape and batch_input_shape:\n        raise ValueError('Only provide the input_shape OR '\n                         'batch_input_shape argument to '\n                         'InputLayer, not both at the same time.')\n      batch_size = batch_input_shape[0]\n      input_shape = batch_input_shape[1:]\n    if kwargs:\n      raise ValueError('Unrecognized keyword arguments:', kwargs.keys())\n\n    if not name:\n      prefix = 'input'\n      name = prefix + '_' + str(backend.get_uid(prefix))\n\n    if not dtype:\n      if input_tensor is None:\n        dtype = backend.floatx()\n      else:\n        dtype = backend.dtype(input_tensor)\n    elif input_tensor and input_tensor.dtype != dtype:\n      raise ValueError('`input_tensor.dtype` differs from `dtype`.')\n    super(InputLayer, self).__init__(dtype=dtype, name=name)\n    self.built = True\n    self.sparse = sparse\n    self.batch_size = batch_size\n    self.supports_masking = True\n\n    if isinstance(input_shape, tensor_shape.TensorShape):\n      input_shape = tuple(input_shape.as_list())\n\n    if input_tensor is None:\n      if input_shape is not None:\n        batch_input_shape = (batch_size,) + tuple(input_shape)\n      else:\n        batch_input_shape = None\n      graph = backend.get_graph()\n      with graph.as_default():\n        # In graph mode, create a graph placeholder to call the layer on.\n        if sparse:\n          input_tensor = backend.placeholder(\n              shape=batch_input_shape,\n              dtype=dtype,\n              name=self.name,\n              sparse=True)\n        else:\n          input_tensor = backend.placeholder(\n              shape=batch_input_shape,\n              dtype=dtype,\n              name=self.name)\n\n      self.is_placeholder = True\n      self._batch_input_shape = batch_input_shape\n    else:\n      if not tf_utils.is_symbolic_tensor(input_tensor):\n        raise ValueError('You should not pass an EagerTensor to `Input`. '\n                         'For example, instead of creating an '\n                         'InputLayer, you should instantiate your model and '"
},
{
    "Id": 249,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a5b8d6c4694e4cd3e3cc4a162053ab0dfa6e174f",
    "Violation": "insufficient",
    "Bug report": "Relax the check for whether the relevant aggregation dimensions are known ahead of time.",
    "Number of deleted lines": 1,
    "Deleted lines": "        input, depthwise_filter, strides, padding, name=\"depthwise\")\n    return nn_ops.conv2d(\n        depthwise, pointwise_filter, [1, 1, 1, 1], padding=\"VALID\", name=name)\n# pylint: enable=redefined-builtin,line-too-long\n\n\ndef sufficient_statistics(x, axes, shift=None, keep_dims=False, name=None):\n  \"\"\"Calculate the sufficient statistics for the mean and variance of `x`.\n\n  These sufficient statistics are computed using the one pass algorithm on\n  an input that's optionally shifted. See:\n  https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data\n\n  Args:\n    x: A `Tensor`.\n    axes: Array of ints. Axes along which to compute mean and variance.\n    shift: A `Tensor` containing the value by which to shift the data for\n      numerical stability, or `None` if no shift is to be performed. A shift\n      close to the true mean provides the most numerically stable results.\n    keep_dims: produce statistics with the same dimensionality as the input.\n    name: Name used to scope the operations that compute the sufficient stats.\n\n  Returns:\n    Four `Tensor` objects of the same type as `x`:\n\n    * the count (number of elements to average over).\n    * the (possibly shifted) sum of the elements in the array.\n    * the (possibly shifted) sum of squares of the elements in the array.\n    * the shift by which the mean must be corrected or None if `shift` is None.\n  \"\"\"\n  axes = list(set(axes))\n  with ops.name_scope(name, \"sufficient_statistics\", [x, shift]):\n    x = ops.convert_to_tensor(x, name=\"x\")\n    x_shape = x.get_shape()\n    if x_shape.is_fully_defined():\n      counts = 1\n      for d in axes:\n        counts *= x_shape[d].value\n      counts = constant_op.constant(counts, dtype=x.dtype)\n    else:  # shape needs to be inferred at runtime.\n      x_dims = array_ops.gather(\n          math_ops.cast(array_ops.shape(x), x.dtype), axes)\n      counts = math_ops.reduce_prod(x_dims, name=\"count\")\n    if shift is not None:\n      shift = ops.convert_to_tensor(shift, name=\"shift\")\n      m_ss = math_ops.sub(x, shift)\n      v_ss = math_ops.squared_difference(x, shift)\n    else:  # no shift.\n      m_ss = x\n      v_ss = math_ops.square(x)\n    m_ss = math_ops.reduce_sum(m_ss, axes, keep_dims=keep_dims, name=\"mean_ss\")\n    v_ss = math_ops.reduce_sum(v_ss, axes, keep_dims=keep_dims, name=\"var_ss\")\n  return counts, m_ss, v_ss, shift\n\n\ndef normalize_moments(counts, mean_ss, variance_ss, shift, name=None):\n  \"\"\"Calculate the mean and variance of based on the sufficient statistics.\n\n  Args:\n    counts: A `Tensor` containing a the total count of the data (one value).\n    mean_ss: A `Tensor` containing the mean sufficient statistics: the (possibly\n      shifted) sum of the elements to average over.\n    variance_ss: A `Tensor` containing the variance sufficient statistics: the\n      (possibly shifted) squared sum of the data to compute the variance over.\n    shift: A `Tensor` containing the value by which the data is shifted for\n      numerical stability, or `None` if no shift was performed.\n    name: Name used to scope the operations that compute the moments.\n\n  Returns:\n    Two `Tensor` objects: `mean` and `variance`.\n  \"\"\""
},
{
    "Id": 250,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0d65cfaab050295c311d9f2fb28388435359db27",
    "Violation": "insufficient",
    "Bug report": " Add an additional NoneType check when converting a traced tensor to a `KerasTensor`.",
    "Number of deleted lines": 1,
    "Deleted lines": "      generated by symbolic layer `__call__`s are not guaranteed to be unique,\n      and are subject to implementation details.\n  \"\"\"\n\n  def __init__(self, type_spec, inferred_value=None, name=None):\n    \"\"\"Constructs a KerasTensor.\"\"\"\n    if not isinstance(type_spec, type_spec_module.TypeSpec):\n      raise ValueError('KerasTensors must be constructed with a `tf.TypeSpec`.')\n\n    self._type_spec = type_spec\n    self._inferred_value = inferred_value\n    self._name = name\n\n  @property\n  def type_spec(self):\n    \"\"\"Returns the `tf.TypeSpec` symbolically inferred for this Keras output.\"\"\"\n    return self._type_spec\n\n  @property\n  def shape(self):\n    \"\"\"Returns the `TensorShape` symbolically inferred for this Keras output.\"\"\"\n    # TODO(kaftan): This is only valid for normal/sparse/ragged tensors.\n    # may need to raise an error when it's not valid for a type_spec,\n    # but some keras code (e.g. build-related stuff) will likely fail when\n    # it can't access shape or dtype\n    return self._type_spec._shape  # pylint: disable=protected-access\n\n  @classmethod\n  def from_tensor(cls, tensor):\n    \"\"\"Convert a traced (composite)tensor to a representative KerasTensor.\"\"\"\n    if isinstance(tensor, ops.Tensor):\n      name = getattr(tensor, 'name', None)\n      type_spec = type_spec_module.type_spec_from_value(tensor)\n      inferred_value = None\n      if (type_spec.dtype == dtypes.int32 and type_spec.shape.rank < 2):\n        # If this tensor might be representing shape information,\n        # (dtype=int32, rank of 0 or 1, not too large to represent a shape)\n        # we attempt to capture any value information tensorflow's\n        # shape handling can extract from the current scratch graph.\n        #\n        # Even though keras layers each trace in their own scratch\n        # graph, this shape value info extraction allows us to capture\n        # a sizable and useful subset of the C++ shape value inference TF can do\n        # if all tf ops appear in the same graph when using shape ops.\n        #\n        # Examples of things this cannot infer concrete dimensions for\n        # that the full single-graph C++ shape inference sometimes can are:\n        # * cases where the shape tensor is cast out of int32 before being\n        #   manipulated w/ floating point numbers then converted back\n        # * cases where int32 tensors w/ rank >= 2 are manipulated before being\n        #   used as a shape tensor\n        # * cases where int32 tensors too large to represent shapes are\n        #   manipulated to a smaller size before being used as a shape tensor\n        inferred_value = array_ops.ones(shape=tensor).shape\n        if inferred_value.dims:\n          inferred_value = inferred_value.as_list()\n          if len(inferred_value) > _MAX_TENSOR_RANK:\n            inferred_value = None\n        else:\n          inferred_value = None\n\n      return KerasTensor(type_spec, inferred_value=inferred_value, name=name)\n    else:\n      # Fallback to the generic arbitrary-typespec KerasTensor\n      name = getattr(tensor, 'name', None)\n      type_spec = type_spec_module.type_spec_from_value(tensor)\n      return cls(type_spec, name=name)\n\n  def _to_placeholder(self):\n    \"\"\"Convert this KerasTensor to a placeholder in a graph.\"\"\"\n    # If there is an inferred value for this tensor, inject the inferred value"
},
{
    "Id": 251,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/31849c61e0432009baabdfafc2ec1a1aed1a40e8",
    "Violation": "insufficient",
    "Bug report": " Small change in tf.nn.sufficient_statistics to guard against unknown shapes. Use is_fully_defined instead of checking shape.dims[d] as the dims variable may be None, if the rank is unknown.",
    "Number of deleted lines": 1,
    "Deleted lines": "def sufficient_statistics(x, axes, shift=None, keep_dims=None, name=None,\n                          keepdims=None):\n  \"\"\"Calculate the sufficient statistics for the mean and variance of `x`.\n\n  These sufficient statistics are computed using the one pass algorithm on\n  an input that's optionally shifted. See:\n  https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data\n\n  Args:\n    x: A `Tensor`.\n    axes: Array of ints. Axes along which to compute mean and variance.\n    shift: A `Tensor` containing the value by which to shift the data for\n      numerical stability, or `None` if no shift is to be performed. A shift\n      close to the true mean provides the most numerically stable results.\n    keep_dims: produce statistics with the same dimensionality as the input.\n    name: Name used to scope the operations that compute the sufficient stats.\n    keepdims: Alias for keep_dims.\n\n  Returns:\n    Four `Tensor` objects of the same type as `x`:\n\n    * the count (number of elements to average over).\n    * the (possibly shifted) sum of the elements in the array.\n    * the (possibly shifted) sum of squares of the elements in the array.\n    * the shift by which the mean must be corrected or None if `shift` is None.\n  \"\"\"\n  axes = list(set(axes))\n  keep_dims = deprecated_argument_lookup(\n      \"keepdims\", keepdims, \"keep_dims\", keep_dims)\n  if keep_dims is None:\n    keep_dims = False\n  with ops.name_scope(name, \"sufficient_statistics\", [x, shift]):\n    x = ops.convert_to_tensor(x, name=\"x\")\n    x_shape = x.get_shape()\n    if all(x_shape.dims[d].value is not None for d in axes):\n      counts = 1\n      for d in axes:\n        counts *= x_shape.dims[d].value\n      counts = constant_op.constant(counts, dtype=x.dtype)\n    else:  # shape needs to be inferred at runtime.\n      x_dims = array_ops.gather(\n          math_ops.cast(array_ops.shape(x), x.dtype), axes)\n      counts = math_ops.reduce_prod(x_dims, name=\"count\")\n    if shift is not None:\n      shift = ops.convert_to_tensor(shift, name=\"shift\")\n      m_ss = math_ops.subtract(x, shift)\n      v_ss = math_ops.squared_difference(x, shift)\n    else:  # no shift.\n      m_ss = x\n      v_ss = math_ops.square(x)\n    m_ss = math_ops.reduce_sum(m_ss, axes, keepdims=keep_dims, name=\"mean_ss\")\n    v_ss = math_ops.reduce_sum(v_ss, axes, keepdims=keep_dims, name=\"var_ss\")\n  return counts, m_ss, v_ss, shift\n\n\n@tf_export(\"nn.sufficient_statistics\", v1=[])\ndef sufficient_statistics_v2(x, axes, shift=None, keepdims=False, name=None):\n  \"\"\"Calculate the sufficient statistics for the mean and variance of `x`.\n\n  These sufficient statistics are computed using the one pass algorithm on\n  an input that's optionally shifted. See:\n  https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data\n\n  Args:\n    x: A `Tensor`.\n    axes: Array of ints. Axes along which to compute mean and variance.\n    shift: A `Tensor` containing the value by which to shift the data for\n      numerical stability, or `None` if no shift is to be performed. A shift\n      close to the true mean provides the most numerically stable results.\n    keepdims: produce statistics with the same dimensionality as the input.\n    name: Name used to scope the operations that compute the sufficient stats."
},
{
    "Id": 252,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/30bd9d5bcc64097d21872486a5726d756ed7067b",
    "Violation": "insufficient",
    "Bug report": " Explicitly handle Tensors in start & stop. The current check was doing a identity check in order to handle both tensors and integers. This becomes problematic when enabling tensor equality. Instead we explicitly check for Tensor type and only compare with sys.maxsize for non-Tensors.",
    "Number of deleted lines": 2,
    "Deleted lines": "\n  Args:\n    tensor: An ops.Tensor object.\n    slice_spec: The arguments to Tensor.__getitem__.\n    var: In the case of variable slice assignment, the Variable object to slice\n      (i.e. tensor is the read-only view of this variable).\n\n  Returns:\n    The appropriate slice of \"tensor\", based on \"slice_spec\".\n\n  Raises:\n    ValueError: If a slice range is negative size.\n    TypeError: If the slice indices aren't int, slice, ellipsis,\n      tf.newaxis or scalar int32/int64 tensors.\n  \"\"\"\n  if isinstance(slice_spec, bool) or \\\n  (isinstance(slice_spec, ops.Tensor) and slice_spec.dtype == dtypes.bool) or \\\n  (isinstance(slice_spec, np.ndarray) and slice_spec.dtype == bool):\n    return boolean_mask(tensor=tensor, mask=slice_spec)\n\n  if not isinstance(slice_spec, (list, tuple)):\n    slice_spec = [slice_spec]\n\n  begin, end, strides = [], [], []\n  index = 0\n\n  new_axis_mask, shrink_axis_mask = 0, 0\n  begin_mask, end_mask = 0, 0\n  ellipsis_mask = 0\n  for s in slice_spec:\n    if isinstance(s, _BaseSlice):\n      # python doesn't always use None when constructing ranges\n      # for example a[:] gives slice(None,sys.maxsize,None)\n      # whereas a[::1] gives slice(None,None,None)\n      if s.start is not None and s.start is not sys.maxsize:\n        _check_index(s.start)\n        begin.append(s.start)\n      else:\n        begin.append(0)\n        begin_mask |= (1 << index)\n      if s.stop is not None and s.stop != sys.maxsize:\n        _check_index(s.stop)\n        end.append(s.stop)\n      else:\n        end.append(0)\n        end_mask |= (1 << index)\n      if s.step is not None:\n        _check_index(s.step)\n        strides.append(s.step)\n      else:\n        strides.append(1)\n    elif s is Ellipsis:\n      begin.append(0)\n      end.append(0)\n      strides.append(1)\n      ellipsis_mask |= (1 << index)\n    elif s is newaxis:\n      begin.append(0)\n      end.append(0)\n      strides.append(1)\n      new_axis_mask |= (1 << index)\n    else:\n      _check_index(s)\n      begin.append(s)\n      end.append(s + 1)\n      strides.append(1)\n      shrink_axis_mask |= (1 << index)\n    index += 1\n\n  # stack possibly involves no tensors, so we must use op_scope correct graph.\n  with ops.name_scope(None, \"strided_slice\",\n                      [tensor] + begin + end + strides) as name:\n    if begin:\n      packed_begin, packed_end, packed_strides = (stack(begin), stack(end),\n                                                  stack(strides))\n      if (packed_begin.dtype == dtypes.int64 or\n          packed_end.dtype == dtypes.int64 or"
},
{
    "Id": 253,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fb1c4cd8283f262bca95ccd04df6f9eb4ae1da0c",
    "Violation": "missing",
    "Bug report": "Add None check for seq_len_mask before reshape.",
    "Number of deleted lines": 4,
    "Deleted lines": "\n  Args:\n    memory: `Tensor`, shaped `[batch_size, max_time, ...]`.\n    memory_sequence_length: `int32` `Tensor`, shaped `[batch_size]`.\n    check_inner_dims_defined: Python boolean.  If `True`, the `memory`\n      argument's shape is checked to ensure all but the two outermost\n      dimensions are fully defined.\n\n  Returns:\n    A (possibly masked), checked, new `memory`.\n\n  Raises:\n    ValueError: If `check_inner_dims_defined` is `True` and not\n      `memory.shape[2:].is_fully_defined()`.\n  \"\"\"\n  memory = nest.map_structure(\n      lambda m: ops.convert_to_tensor(m, name=\"memory\"), memory)\n  if check_inner_dims_defined:\n    def _check_dims(m):\n      if not m.get_shape()[2:].is_fully_defined():\n        raise ValueError(\"Expected memory %s to have fully defined inner dims, \"\n                         \"but saw shape: %s\" % (m.name, m.get_shape()))\n    nest.map_structure(_check_dims, memory)\n  if memory_sequence_length is None:\n    seq_len_mask = None\n  else:\n    seq_len_mask = array_ops.sequence_mask(\n        memory_sequence_length,\n        maxlen=array_ops.shape(nest.flatten(memory)[0])[1],\n        dtype=nest.flatten(memory)[0].dtype)\n  def _maybe_mask(m, seq_len_mask):\n    rank = m.get_shape().ndims\n    rank = rank if rank is not None else array_ops.rank(m)\n    extra_ones = array_ops.ones(rank - 2, dtype=dtypes.int32)\n    seq_len_mask = array_ops.reshape(\n        seq_len_mask,\n        array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), 0))\n    return m * seq_len_mask if memory_sequence_length is not None else m\n  return nest.map_structure(lambda m: _maybe_mask(m, seq_len_mask), memory)\n\n\nclass _BaseAttentionMechanism(AttentionMechanism):\n  \"\"\"A base AttentionMechanism class providing common functionality.\n\n  Common functionality includes:\n    1. Storing the query and memory layers.\n    2. Preprocessing and storing the memory.\n  \"\"\"\n\n  def __init__(self, query_layer, memory, memory_sequence_length=None,\n               memory_layer=None, check_inner_dims_defined=True, name=None):\n    \"\"\"Construct base AttentionMechanism class.\n\n    Args:\n      query_layer: Callable.  Instance of `tf.layers.Layer`.  The layer's depth\n        must match the depth of `memory_layer`.  If `query_layer` is not\n        provided, the shape of `query` must match that of `memory_layer`.\n      memory: The memory to query; usually the output of an RNN encoder.  This\n        tensor should be shaped `[batch_size, max_time, ...]`.\n      memory_sequence_length (optional): Sequence lengths for the batch entries\n        in memory.  If provided, the memory tensor rows are masked with zeros\n        for values past the respective sequence lengths.\n      memory_layer: Instance of `tf.layers.Layer` (may be None).  The layer's\n        depth must match the depth of `query_layer`.\n        If `memory_layer` is not provided, the shape of `memory` must match\n        that of `query_layer`.\n      check_inner_dims_defined: Python boolean.  If `True`, the `memory`\n        argument's shape is checked to ensure all but the two outermost\n        dimensions are fully defined.\n      name: Name to use when creating ops.\n    \"\"\"\n    if (query_layer is not None\n        and not isinstance(query_layer, layers_base._Layer)):  # pylint: disable=protected-access\n      raise TypeError("
},
{
    "Id": 254,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a0ca4bcb81dfd07fdb1c7872b5852f84cfc1a081",
    "Violation": "improper",
    "Bug report": "Fix separable convolution bias check",
    "Number of deleted lines": 1,
    "Deleted lines": "        'pointwise_kernel',\n        shape=pointwise_kernel_shape,\n        initializer=self.pointwise_initializer,\n        regularizer=self.pointwise_regularizer,\n        trainable=True,\n        dtype=self.dtype)\n    if self.use_bias:\n      self.bias = vs.get_variable('bias',\n                                  shape=(self.filters,),\n                                  initializer=self.bias_initializer,\n                                  regularizer=self.bias_regularizer,\n                                  trainable=True,\n                                  dtype=self.dtype)\n    else:\n      self.bias = None\n\n  def call(self, inputs):\n    if self.data_format == 'channels_first':\n      # Reshape to channels last\n      inputs = array_ops.transpose(inputs, (0, 2, 3, 1))\n\n    # Apply the actual ops.\n    outputs = nn.separable_conv2d(\n        inputs,\n        self.depthwise_kernel,\n        self.pointwise_kernel,\n        strides=(1,) + self.strides + (1,),\n        padding=self.padding.upper(),\n        rate=self.dilation_rate)\n\n    if self.data_format == 'channels_first':\n      # Reshape to channels first\n      outputs = array_ops.transpose(outputs, (0, 3, 1, 2))\n\n    if self.bias:\n      outputs = nn.bias_add(\n          outputs,\n          self.bias,\n          data_format=utils.convert_data_format(self.data_format, ndim=4))\n\n    if self.activation is not None:\n      return self.activation(outputs)\n    return outputs\n\n\ndef separable_conv2d(inputs,\n                     filters,\n                     kernel_size,\n                     strides=(1, 1),\n                     padding='valid',\n                     data_format='channels_last',\n                     dilation_rate=(1, 1),\n                     depth_multiplier=1,\n                     activation=None,\n                     use_bias=True,\n                     depthwise_initializer=None,\n                     pointwise_initializer=None,\n                     bias_initializer=init_ops.zeros_initializer(),\n                     depthwise_regularizer=None,\n                     pointwise_regularizer=None,\n                     bias_regularizer=None,\n                     activity_regularizer=None,\n                     trainable=True,\n                     name=None,\n                     reuse=None):\n  \"\"\"Functional interface for the depthwise separable 2D convolution layer.\n\n  This layer performs a depthwise convolution that acts separately on\n  channels, followed by a pointwise convolution that mixes channels.\n  If `use_bias` is True and a bias initializer is provided,\n  it adds a bias vector to the output."
},
{
    "Id": 255,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1ff493ed1a2059f82f7607a7f0a0aa2ce8d5a542",
    "Violation": "improper",
    "Bug report": "Replace a defensive check with TF_RET_CHECK",
    "Number of deleted lines": 4,
    "Deleted lines": "}  // namespace jit\n\nStatus DeviceNameToDeviceType(const string& device, DeviceType* device_type) {\n  DeviceNameUtils::ParsedName parsed;\n  if (!DeviceNameUtils::ParseFullName(device, &parsed)) {\n    return errors::Internal(\"Malformed assigned device '\", device, \"'\");\n  }\n  *device_type = DeviceType(parsed.type);\n  return Status::OK();\n}\n\nStatus PickDeviceForXlaImpl(absl::Span<const string> device_names,\n                            bool allow_mixing_unknown_and_cpu,\n                            bool* out_can_pick_device,\n                            string* out_device_picked) {\n  if (out_can_pick_device) {\n    *out_can_pick_device = true;\n  }\n\n#define FAILED_TO_PICK_DEVICE(failing_status) \\\n  do {                                        \\\n    if (out_can_pick_device) {                \\\n      *out_can_pick_device = false;           \\\n      return Status::OK();                    \\\n    } else {                                  \\\n      return failing_status;                  \\\n    }                                         \\\n  } while (false)\n\n  TF_RET_CHECK(!device_names.empty()) << \"No devices to choose from\";\n  DCHECK_NE(out_can_pick_device == nullptr, out_device_picked == nullptr);\n\n  absl::flat_hash_set<absl::string_view> device_names_set;\n  for (absl::string_view device_name : device_names) {\n    if (!device_name.empty()) {\n      // TODO(sanjoy): Figure out if this is necessary.\n      device_names_set.insert(device_name);\n    }\n  }\n\n  absl::optional<absl::string_view> maybe_gpu_device;\n  absl::optional<absl::string_view> maybe_cpu_device;\n  absl::optional<absl::string_view> maybe_unknown_device;\n\n  for (absl::string_view device_name : device_names_set) {\n    DeviceNameUtils::ParsedName parsed_name;\n    TF_RET_CHECK(DeviceNameUtils::ParseFullName(device_name, &parsed_name))\n        << device_name;\n    if (parsed_name.type == \"GPU\") {\n      if (maybe_gpu_device) {\n        FAILED_TO_PICK_DEVICE(errors::Internal(\n            \"Multiple GPU devices \", absl::StrJoin(device_names, \", \")));\n      }\n      maybe_gpu_device = device_name;\n    } else if (parsed_name.type == \"CPU\") {\n      if (maybe_cpu_device) {\n        FAILED_TO_PICK_DEVICE(errors::Internal(\n            \"Multiple CPU devices \", absl::StrJoin(device_names, \", \")));\n      }\n      maybe_cpu_device = device_name;\n    } else {\n      if (maybe_unknown_device) {\n        FAILED_TO_PICK_DEVICE(errors::Internal(\n            \"Multiple unknown devices \", absl::StrJoin(device_names, \", \")));\n      }\n      maybe_unknown_device = device_name;\n    }\n  }\n\n  if (maybe_unknown_device && maybe_gpu_device) {\n    FAILED_TO_PICK_DEVICE(errors::Internal(\n        \"Found both unknown and GPU devices: \", *maybe_unknown_device, \", \",\n        *maybe_gpu_device));\n  }"
},
{
    "Id": 256,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/201982013046116767545cda18137b38abb39468",
    "Violation": "missing",
    "Bug report": "toco: Fix missing check for buffer in ResizeBilinear.",
    "Number of deleted lines": 0,
    "Deleted lines": "  const auto& input_array = *model->arrays[input_name];\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  if (input_shape.dimensions_count() < 4) {\n    LOG(FATAL) << \"missing dimensions for \" << input_name;\n  }\n  const string& output_name = op->outputs[0];\n  const int output_depth = input_shape.dims(3);\n  ComputeConvSizes(input_shape, output_depth, op->kwidth, op->kheight,\n                   op->stride_width, op->stride_height, op->padding.type,\n                   model->GetArray(output_name).mutable_shape(),\n                   &op->padding.GetOrCreateFixedPadding());\n}\n\nvoid ProcessResizeBilinearOperator(Model* model, ResizeBilinearOperator* op) {\n  CHECK_EQ(op->inputs.size(), 2);\n  CHECK_EQ(op->outputs.size(), 1);\n\n  if (!model->arrays[op->inputs[0]]->has_shape() ||\n      !model->arrays[op->inputs[1]]->has_shape()) {\n    return;\n  }\n  const auto& input_data_shape = model->arrays[op->inputs[0]]->shape();\n\n  const string& output_size_name = op->inputs[1];\n  const auto& output_size_array = *model->arrays[output_size_name];\n  CHECK(output_size_array.data_type == ArrayDataType::kInt32);\n  CHECK(output_size_array.has_shape());\n  const auto& output_size_shape = output_size_array.shape();\n  CHECK_EQ(output_size_shape.dimensions_count(), 1);\n  CHECK_EQ(output_size_shape.dims(0), 2);\n  std::vector<int32> output_shape =\n      output_size_array.GetBuffer<ArrayDataType::kInt32>().data;\n  model->arrays[op->outputs[0]]->copy_shape(\n      Shape({input_data_shape.dims(0), output_shape[0], output_shape[1],\n             input_data_shape.dims(3)}));\n}\n\nvoid ProcessLstmCellOperator(Model* model, LstmCellOperator* op) {\n  // I/O arrays should be allocated on creation of op.\n  QCHECK_EQ(op->inputs.size(), LstmCellOperator::NUM_INPUTS);\n  QCHECK_EQ(op->outputs.size(), LstmCellOperator::NUM_OUTPUTS);\n\n  const auto& input_array =\n      *model->arrays[op->inputs[LstmCellOperator::DATA_INPUT]];\n  // Yield until all input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  CHECK_GE(input_shape.dimensions_count(), 2);\n\n  const auto& prev_activ_array =\n      *model->arrays[op->inputs[LstmCellOperator::PREV_ACTIV_INPUT]];\n  // Yield until all input dims have been resolved.\n  if (!prev_activ_array.has_shape()) {\n    return;\n  }\n  const auto& prev_activ_shape = prev_activ_array.shape();\n  CHECK_GE(prev_activ_shape.dimensions_count(), 2);\n\n  const auto& weights_array =\n      *model->arrays[op->inputs[LstmCellOperator::WEIGHTS_INPUT]];\n  // Yield until weights dims have been resolved.\n  if (!weights_array.has_shape()) {\n    return;\n  }"
},
{
    "Id": 257,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c676a2d7ce8884aad59ca9cd5f45e9b851574cac",
    "Violation": "missing",
    "Bug report": " [tensorflow] Add a check that strided slice op strides argument has reasonable size",
    "Number of deleted lines": 1,
    "Deleted lines": "  const int64_t dims;\n  int32 begin_mask;\n  int32 end_mask;\n  bool begin_valid;\n  bool end_valid;\n  gtl::InlinedVector<int64_t, 4>& begin;\n  gtl::InlinedVector<int64_t, 4>& end;\n  gtl::InlinedVector<int64_t, 4>& strides;\n  // This vector helps construct the final shape of the slice.\n  // The final tensor is reduced in rank whenever a single index e.g. foo[3]\n  // is called for. The final tensor increases in rank with tf.newaxis\n  // entries. If an index in this array is positive, the size of the dimension\n  // is obtained from canonical end-begin. Otherwise, if it is a kNewAxis,\n  // it will be 1. A shrunk dimension is skipped.\n  gtl::InlinedVector<int32, 4> final_shape_gather_indices;\n  // This vector has the same size as final_shape_gather_indices, but it\n  // remembers the sparse index that a dimension comes from, instead of dense\n  // index. A -1 in this vector means there the index is not from the sparse\n  // input.\n  gtl::InlinedVector<int32, 4> final_shape_gather_indices_sparse;\n  gtl::InlinedVector<int32, 4> input_shape_gather_indices_sparse;\n  // The dense indexed shrink mask is which processing dimensions\n  // should be shrunk. For example, if foo.shape = (10,10,10,10)\n  // foo[3, ..., 5] has sparse_shrink_axis_mask of 0x5 and\n  // dense_shrink_axis_mask of 0x9, yielding a final shape (10,10).\n  int32 shrink_axis_mask;\n};\n\n}  // namespace\n\ntemplate <class T>\nstatic Status TF_MUST_USE_RESULT BuildDenseSpec(\n    const StridedSliceSparseSpec& sparse, StridedSliceDenseSpec* dense) {\n  if (dense->dims < 0) {\n    return errors::InvalidArgument(\"Unexpected negative dense.dims\");\n  }\n\n  // Build expanded begin, end, strides, begin_mask, end_mask\n  // to remove any ellipsis\n  dense->begin.resize(dense->dims);\n  dense->end.resize(dense->dims);\n  dense->strides.resize(dense->dims);\n  dense->input_shape_gather_indices_sparse.resize(dense->dims);\n  // What indices to get the final shape from.\n  dense->begin_mask = 0;\n  dense->end_mask = 0;\n  dense->shrink_axis_mask = 0;\n  {\n    int full_index = 0;\n\n    const T* const strides_flat = sparse.strides_tensor.vec<T>().data();\n    dense->begin_valid = sparse.begin_tensor != nullptr;\n    dense->end_valid = sparse.end_tensor != nullptr;\n\n    const T* const begin_flat = sparse.begin_tensor != nullptr\n                                    ? sparse.begin_tensor->vec<T>().data()\n                                    : nullptr;\n    const T* const end_flat = sparse.end_tensor != nullptr\n                                  ? sparse.end_tensor->vec<T>().data()\n                                  : nullptr;\n\n    for (int i = 0; i < sparse.dims; i++) {\n      if ((1 << i) & sparse.ellipsis_mask) {\n        // Expand the ellipsis into the appropriate indices\n        // NOTE: this only works because we guaranteed one ellipsis\n        int32_t next_index = std::min(dense->dims - (sparse.dims - i) + 1 +\n                                          sparse.num_add_axis_after_ellipsis,\n                                      dense->dims);\n        for (; full_index < next_index; full_index++) {\n          // new_axis' aren't real axis so you have to skip\n          dense->begin[full_index] = dense->end[full_index] = 0;"
},
{
    "Id": 258,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f61175812426009a4c96e51befb2951612990903",
    "Violation": "missing",
    "Bug report": "To add a check of input_dims greater than zero in embedding layers. ",
    "Number of deleted lines": 0,
    "Deleted lines": "      `Flatten` then `Dense` layers upstream\n      (without it, the shape of the dense outputs cannot be computed).\n\n  Input shape:\n    2D tensor with shape: `(batch_size, input_length)`.\n\n  Output shape:\n    3D tensor with shape: `(batch_size, input_length, output_dim)`.\n  \"\"\"\n\n  def __init__(self,\n               input_dim,\n               output_dim,\n               embeddings_initializer='uniform',\n               embeddings_regularizer=None,\n               activity_regularizer=None,\n               embeddings_constraint=None,\n               mask_zero=False,\n               input_length=None,\n               **kwargs):\n    if 'input_shape' not in kwargs:\n      if input_length:\n        kwargs['input_shape'] = (input_length,)\n      else:\n        kwargs['input_shape'] = (None,)\n    dtype = kwargs.pop('dtype', K.floatx())\n    # We set autocast to False, as we do not want to cast floating- point inputs\n    # to self.dtype. In call(), we cast to int32, and casting to self.dtype\n    # before casting to int32 might cause the int32 values to be different due\n    # to a loss of precision.\n    kwargs['autocast'] = False\n    super(Embedding, self).__init__(dtype=dtype, **kwargs)\n\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.embeddings_initializer = initializers.get(embeddings_initializer)\n    self.embeddings_regularizer = regularizers.get(embeddings_regularizer)\n    self.activity_regularizer = regularizers.get(activity_regularizer)\n    self.embeddings_constraint = constraints.get(embeddings_constraint)\n    self.mask_zero = mask_zero\n    self.supports_masking = mask_zero\n    self.input_length = input_length\n    self._supports_ragged_inputs = True\n\n  @tf_utils.shape_type_conversion\n  def build(self, input_shape):\n    # Note: most sparse optimizers do not have GPU kernels defined. When\n    # building graphs, the placement algorithm is able to place variables on CPU\n    # since it knows all kernels using the variable only exist on CPU.\n    # When eager execution is enabled, the placement decision has to be made\n    # right now. Checking for the presence of GPUs to avoid complicating the\n    # TPU codepaths which can handle sparse optimizers.\n    if context.executing_eagerly() and context.context().num_gpus():\n      with ops.device('cpu:0'):\n        self.embeddings = self.add_weight(\n            shape=(self.input_dim, self.output_dim),\n            initializer=self.embeddings_initializer,\n            name='embeddings',\n            regularizer=self.embeddings_regularizer,\n            constraint=self.embeddings_constraint)\n    else:\n      self.embeddings = self.add_weight(\n          shape=(self.input_dim, self.output_dim),\n          initializer=self.embeddings_initializer,\n          name='embeddings',\n          regularizer=self.embeddings_regularizer,\n          constraint=self.embeddings_constraint)\n    self.built = True\n\n  def compute_mask(self, inputs, mask=None):"
},
{
    "Id": 259,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a0dc73569fc193c1ce26a7bd2d4a8776e7b813ac",
    "Violation": "missing",
    "Bug report": "add check for empty cs_prev_tensor",
    "Number of deleted lines": 0,
    "Deleted lines": "\n    Tensor* co_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"co\", TensorShape({batch_size, cell_size}),\n                                  &co_tensor));\n\n    Tensor* h_tensor = nullptr;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(\"h\", TensorShape({batch_size, cell_size}),\n                                  &h_tensor));\n\n    // Allocate our temp tensors.\n    Tensor xh_tensor;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n                            DataTypeToEnum<T>::v(),\n                            TensorShape({batch_size, input_size + cell_size}),\n                            &xh_tensor));\n\n    Tensor gates_tensor;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_temp(DataTypeToEnum<T>::v(),\n                                      TensorShape({batch_size, cell_size * 4}),\n                                      &gates_tensor));\n\n    const Device& device = ctx->eigen_device<Device>();\n\n    // Sanity check that each of the tensors have the required NDIMS.\n    OP_REQUIRES(ctx, x_tensor->dims() == 2,\n                errors::InvalidArgument(\"x_tensor must be rank 2 but is rank \",\n                                        x_tensor->dims(), \".\"));\n    OP_REQUIRES(\n        ctx, cs_prev_tensor->dims() == 2,\n        errors::InvalidArgument(\"cs_prev_tensor must be rank 2 but is rank \",\n                                cs_prev_tensor->dims(), \".\"));\n    OP_REQUIRES(\n        ctx, h_prev_tensor->dims() == 2,\n        errors::InvalidArgument(\"h_prev_tensor must be rank 2 but is rank \",\n                                h_prev_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, w_tensor->dims() == 2,\n                errors::InvalidArgument(\"w_tensor must be rank 2 but is rank \",\n                                        w_tensor->dims(), \".\"));\n    OP_REQUIRES(\n        ctx, wci_tensor->dims() == 1,\n        errors::InvalidArgument(\"wci_tensor must be rank 1 but is rank \",\n                                wci_tensor->dims(), \".\"));\n    OP_REQUIRES(\n        ctx, wcf_tensor->dims() == 1,\n        errors::InvalidArgument(\"wcf_tensor must be rank 1 but is rank \",\n                                wci_tensor->dims(), \".\"));\n    OP_REQUIRES(\n        ctx, wco_tensor->dims() == 1,\n        errors::InvalidArgument(\"wco_tensor must be rank 1 but is rank \",\n                                wco_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, b_tensor->dims() == 1,\n                errors::InvalidArgument(\"b_tensor must be rank 1 but is rank \",\n                                        b_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, xh_tensor.dims() == 2,\n                errors::InvalidArgument(\"xh_tensor must be rank 2 but is rank \",\n                                        xh_tensor.dims(), \".\"));\n    OP_REQUIRES(ctx, i_tensor->dims() == 2,\n                errors::InvalidArgument(\"i_tensor must be rank 2 but is rank \",\n                                        i_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, cs_tensor->dims() == 2,\n                errors::InvalidArgument(\"cs_tensor must be rank 2 but is rank \",\n                                        cs_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, f_tensor->dims() == 2,\n                errors::InvalidArgument(\"f_tensor must be rank 2 but is rank \",\n                                        f_tensor->dims(), \".\"));\n    OP_REQUIRES(ctx, o_tensor->dims() == 2,\n                errors::InvalidArgument(\"o_tensor must be rank 2 but is rank \","
},
{
    "Id": 260,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/258233804f2bc92b4bdb9714b396aed34b53ff0d",
    "Violation": "missing",
    "Bug report": " sanity check of empty tensor on avgpool3d_grad",
    "Number of deleted lines": 0,
    "Deleted lines": "        AllocateOutputSetMklShape(context, 2, &output_max, {},\n                                  output_max_mkl_shape, this->native_format_);\n        output_min->flat<float>()(0) = min_input;\n        output_max->flat<float>()(0) = max_input;\n      }\n    } catch (dnnl::error& e) {\n      string error_msg = \"Status: \" + std::to_string(e.status) +\n                         \", message: \" + string(e.message) + \", in file \" +\n                         string(__FILE__) + \":\" + std::to_string(__LINE__);\n      OP_REQUIRES_OK(\n          context,\n          errors::Aborted(\"Operation received an exception:\", error_msg));\n    }\n  }  // Compute\n\n private:\n  engine cpu_engine_ = engine(engine::kind::cpu, 0);\n};  // MklAvgPoolingOp\n\ntemplate <class Device, class T, bool native_format = false>\nclass MklAvgPoolingGradOp : public MklPoolingBackwardOpBase<T> {\n public:\n  explicit MklAvgPoolingGradOp(OpKernelConstruction* context)\n      : MklPoolingBackwardOpBase<T>(context) {\n    this->native_format_ = native_format;\n  }\n\n  void Compute(OpKernelContext* context) override {\n    try {\n      const Tensor& orig_input_tensor =\n          MklGetInput(context, kInputTensorIndexInputShape);\n      const Tensor& grad_tensor =\n          MklGetInput(context, kInputTensorIndexInputGradient);\n\n      MklDnnShape orig_input_mkl_shape, grad_mkl_shape;\n      GetMklShape(context, kInputTensorIndexInputShape, &orig_input_mkl_shape,\n                  this->native_format_);\n      GetMklShape(context, kInputTensorIndexInputGradient, &grad_mkl_shape,\n                  this->native_format_);\n      if (!context->status().ok()) return;\n\n      // Used to allocate output_diff_src/diff_src.\n      MklDnnData<T> grad_dnn_data(&cpu_engine_);\n      MklPoolParameters pool_params;\n      auto shape_vec = orig_input_tensor.vec<int32>();\n      TensorShape orig_input_shape;\n      for (int i = 0; i < orig_input_tensor.NumElements(); i++) {\n        (void)orig_input_shape.AddDimWithStatus(shape_vec(i));\n      }\n\n      bool is_pool2d = (this->ksize_.size() == 4);\n      this->InitMklPoolParameters(context, &pool_params, orig_input_mkl_shape,\n                                  orig_input_shape);\n\n      memory::dims filter_dims, strides, padding_left, padding_right;\n      this->PoolParamsToDims(&pool_params, &filter_dims, &strides,\n                             &padding_left, &padding_right, is_pool2d);\n\n      memory::dims orig_input_dims_mkl_order =\n          orig_input_mkl_shape.IsMklTensor()\n              ? orig_input_mkl_shape.GetSizesAsMklDnnDims()\n              : is_pool2d ? TFShapeToMklDnnDimsInNCHW(orig_input_shape,\n                                                      this->data_format_tf_)\n                          : TFShapeToMklDnnDimsInNCDHW(orig_input_shape,\n                                                       this->data_format_tf_);\n\n      memory::dims diff_dst_dims =\n          grad_mkl_shape.IsMklTensor()\n              ? grad_mkl_shape.GetSizesAsMklDnnDims()\n              : is_pool2d ? TFShapeToMklDnnDimsInNCHW(grad_tensor.shape(),"
},
{
    "Id": 261,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a3d9f9be9ac2296615644061b40cefcee341dcc4",
    "Violation": "missing",
    "Bug report": " Add missing validation to pooling_ops_3d ",
    "Number of deleted lines": 0,
    "Deleted lines": "                                        \"specify 5 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context, ksize_[0] == 1 && stride_[0] == 1,\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n    const int32 ksize_c = GetTensorDim(ksize_, data_format_, 'C');\n    const int32 stride_c = GetTensorDim(stride_, data_format_, 'C');\n    OP_REQUIRES(context, ksize_c == 1 && stride_c == 1,\n                errors::Unimplemented(\"MaxPooling3dGradGrad is not yet \"\n                                      \"supported on the depth dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_grad_backprop = context->input(2);\n\n    // For maxpooling3d, tensor_in should have 5 dimensions.\n    OP_REQUIRES(context, tensor_in.dims() == 5,\n                errors::InvalidArgument(\"tensor_in must be 5-dimensional\"));\n    OP_REQUIRES(context, tensor_out.dims() == 5,\n                errors::InvalidArgument(\"tensor_out must be 5-dimensional\"));\n    // For maxpooling3d, out_grad_backprop should have 5 dimensions.\n    OP_REQUIRES(\n        context, out_grad_backprop.dims() == 5,\n        errors::InvalidArgument(\"out_grad_backprop must be 5-dimensional\"));\n\n    Pool3dParameters params{context,  ksize_,       stride_,\n                            padding_, data_format_, tensor_in.shape()};\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {2}, 0, tensor_out.shape(), &output));\n\n    LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n        context, params, tensor_in, tensor_out, out_grad_backprop, output);\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n  TensorFormat data_format_;\n};\n\n#define REGISTER_KERNELS(D, T)                                             \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"MaxPool3D\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"),         \\\n      Pooling3DOp<D##Device, T, MAX>);                                     \\\n  REGISTER_KERNEL_BUILDER(Name(\"MaxPool3DGrad\")                            \\\n                              .Device(DEVICE_##D)                          \\\n                              .TypeConstraint<T>(\"T\")                      \\\n                              .TypeConstraint<T>(\"TInput\"),                \\\n                          MaxPooling3dGradOp<D##Device, T>);               \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"MaxPool3DGradGrad\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"), \\\n      MaxPooling3dGradGradOp<D##Device, T>);                               \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"AvgPool3D\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"),         \\\n      Pooling3DOp<D##Device, T, AVG>);                                     \\\n  REGISTER_KERNEL_BUILDER(Name(\"AvgPool3DGrad\")                            \\\n                              .Device(DEVICE_##D)                          \\\n                              .TypeConstraint<T>(\"T\")                      \\\n                              .HostMemory(\"orig_input_shape\"),             \\\n                          AvgPooling3dGradOp<D##Device, T>);\n\n#define REGISTER_CPU_KERNELS(T) REGISTER_KERNELS(CPU, T)\nTF_CALL_float(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n"
},
{
    "Id": 262,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/27bd8aaa7b58d2591fed43a6c245f3037664cfb1",
    "Violation": "missing",
    "Bug report": "Fix another Eigen missing validation ",
    "Number of deleted lines": 0,
    "Deleted lines": "        errors::Internal(\"Failed to reshape In[1] from \",\n                         in1.shape().DebugString()));\n    OP_REQUIRES(ctx, d1 == d2,\n                errors::InvalidArgument(\n                    \"In[0] mismatch In[1] shape: \", d1, \" vs. \", d2, \": \",\n                    in0.shape().DebugString(), \" \", in1.shape().DebugString(),\n                    \" \", lower_, \" \", adjoint_));\n    out_shape.AddDim(d1);\n    out_shape.AddDim(d3);\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));\n    if (out->NumElements() == 0) {\n      return;\n    }\n    Tensor out_reshaped;\n    OP_REQUIRES(ctx,\n                out_reshaped.CopyFrom(*out, TensorShape({batch_size, d1, d3})),\n                errors::Internal(\"Failed to reshape output from \",\n                                 out->shape().DebugString()));\n    LaunchBatchBandedTriangularSolve<Scalar>::Launch(\n        ctx, in0_reshaped, in1_reshaped, adjoint_, lower_, bcast,\n        &out_reshaped);\n  }\n\n private:\n  void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0,\n                            const Tensor& in1) {\n    OP_REQUIRES(\n        ctx, in0.dims() >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0.dims()));\n\n    OP_REQUIRES(\n        ctx, in1.dims() >= 2,\n        errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1.dims()));\n  }\n  bool lower_;\n  bool adjoint_;\n};\n\n#define REGISTER_BANDED_TRIANGULAR_SOLVE_CPU(TYPE)        \\\n  REGISTER_KERNEL_BUILDER(Name(\"BandedTriangularSolve\")   \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<TYPE>(\"T\"), \\\n                          BandedTriangularSolveOpCpu<TYPE>);\n\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(float);\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(double);\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(complex64);\nREGISTER_BANDED_TRIANGULAR_SOLVE_CPU(complex128);\n\n}  // namespace tensorflow\n"
},
{
    "Id": 263,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dedac5053f1ca2d6a7820e330714e50d2d724cee",
    "Violation": "missing",
    "Bug report": " Fix edge case bug in handling FP16 weights in XNNPACK delegate. Quasi-static tensors may become subgraph outputs after partitioning; we need to explicitly exclude them from outputs and treat as static tensors.",
    "Number of deleted lines": 3,
    "Deleted lines": "      DelegatePrepare,                // .Prepare\n      nullptr,                        // .CopyFromBufferHandle\n      nullptr,                        // .CopyToBufferHandle\n      nullptr,                        // .FreeBufferHandle\n      kTfLiteDelegateFlagsNone,       // .flags\n  };\n\n  // Unpacked data for quasi-static tensors, i.e. tensors produced by\n  // dequantizing or unpacking static buffers.\n  std::vector<char> static_unpacked_data_;\n  // Mapping from a tensor index for a quasi-static tensor to the offset to\n  // its unpacked data within static_unpacked_data_.\n  std::unordered_map<int, size_t> static_unpacked_data_map_;\n  // Set of indices of nodes which unpack static data, e.g. Dequantize\n  // operators which convert FP16 static weights to FP32. These nodes are simply\n  // ignored in the delegate implementation, because their outputs are\n  // pre-unpacked in DelegatePrepare.\n  std::unordered_set<int> static_unpack_nodes_;\n#if !defined(__EMSCRIPTEN__) || defined(__EMSCRIPTEN_PTHREADS__)\n  // Thread pool with smart-pointer for lifetime management.\n  std::unique_ptr<pthreadpool, decltype(&pthreadpool_destroy)> threadpool_{\n      nullptr, &pthreadpool_destroy};\n#endif\n};\n\nclass Subgraph {\n public:\n  static Subgraph* Create(TfLiteContext* context,\n                          const TfLiteDelegateParams* params,\n                          const Delegate* delegate) {\n    // Convert subgraph inputs and outputs to hash sets for faster lookup.\n    const std::unordered_set<int> inputs(\n        &params->input_tensors->data[0],\n        &params->input_tensors->data[params->input_tensors->size]);\n    const std::unordered_set<int> outputs(\n        &params->output_tensors->data[0],\n        &params->output_tensors->data[params->output_tensors->size]);\n    std::unordered_set<int> externals(outputs);\n\n    TfLiteIntArray* execution_plan;\n    if (context->GetExecutionPlan(context, &execution_plan) != kTfLiteOk) {\n      return nullptr;\n    }\n\n    xnn_subgraph_t subgraph_ptr = nullptr;\n    xnn_status status = xnn_create_subgraph(\n        /*external_value_ids=*/context->tensors_size, /*flags=*/0,\n        &subgraph_ptr);\n    if (status != xnn_status_success) {\n      TF_LITE_KERNEL_LOG(context, \"failed to create XNNPACK subgraph\");\n      return nullptr;\n    }\n\n    // Smart pointer to automatically release subgraph on exit.\n    std::unique_ptr<xnn_subgraph, decltype(&xnn_delete_subgraph)> subgraph(\n        subgraph_ptr, &xnn_delete_subgraph);\n\n    // Detect which tensors are used as inputs or outputs of any subgraph nodes.\n    // -1 denotes tensor not used in the subgraph. These indexes will be\n    // filtered out and removed later.\n    std::vector<int> tensors(context->tensors_size, -1);\n    for (int i = 0; i < params->nodes_to_replace->size; i++) {\n      const int node_index = params->nodes_to_replace->data[i];\n      if (delegate->static_unpack_nodes_.count(node_index)) {\n        // The node unpacks static input and can be skipped because its input\n        // was pre-unpacked in DelegatePrepare.\n        continue;\n      }\n\n      TfLiteNode* node = nullptr;\n      TfLiteRegistration* registration = nullptr;\n      if (context->GetNodeAndRegistration(context, node_index, &node,\n                                          &registration) != kTfLiteOk) {"
},
{
    "Id": 264,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ce589223a5fa78cb12efaf1efd1d8d0e5507bd08",
    "Violation": "missing",
    "Bug report": " Update nn_ops.py. Added check for pooling_ratio",
    "Number of deleted lines": 0,
    "Deleted lines": "      Pooling ratio for each dimension of `value`, currently only supports row\n      and col dimension and should be >= 1.0. For example, a valid pooling ratio\n      looks like [1.0, 1.44, 1.73, 1.0]. The first and last elements must be 1.0\n      because we don't allow pooling on batch and channels dimensions.  1.44 and\n      1.73 are pooling ratio on height and width dimensions respectively.\n    pseudo_random: An optional `bool`.  Defaults to `False`. When set to `True`,\n      generates the pooling sequence in a pseudorandom fashion, otherwise, in a\n      random fashion. Check paper (Graham, 2015) for difference between\n      pseudorandom and random.\n    overlapping: An optional `bool`.  Defaults to `False`.  When set to `True`,\n      it means when pooling, the values at the boundary of adjacent pooling\n      cells are used by both cells. For example:\n      `index  0  1  2  3  4`\n      `value  20 5  16 3  7`\n      If the pooling sequence is [0, 2, 4], then 16, at index 2 will be used\n      twice.  The result would be [20, 16] for fractional max pooling.\n    seed: An optional `int`.  Defaults to `0`.  If set to be non-zero, the\n      random number generator is seeded by the given seed.  Otherwise it is\n      seeded by a random seed.\n    name: A name for the operation (optional).\n\n  Returns:\n  A tuple of `Tensor` objects (`output`, `row_pooling_sequence`,\n  `col_pooling_sequence`).\n    output: Output `Tensor` after fractional max pooling.  Has the same type as\n      `value`.\n    row_pooling_sequence: A `Tensor` of type `int64`.\n    col_pooling_sequence: A `Tensor` of type `int64`.\n\n  References:\n    Fractional Max-Pooling:\n      [Graham, 2015](https://arxiv.org/abs/1412.6071)\n      ([pdf](https://arxiv.org/pdf/1412.6071.pdf))\n  \"\"\"\n  pooling_ratio = _get_sequence(pooling_ratio, 2, 3, \"pooling_ratio\")\n\n  if seed == 0:\n    return gen_nn_ops.fractional_max_pool(value, pooling_ratio, pseudo_random,\n                                          overlapping, deterministic=False,\n                                          seed=0, seed2=0, name=name)\n  else:\n    seed1, seed2 = random_seed.get_seed(seed)\n    return gen_nn_ops.fractional_max_pool(value, pooling_ratio, pseudo_random,\n                                          overlapping, deterministic=True,\n                                          seed=seed1, seed2=seed2, name=name)\n\n\n@tf_export(v1=[\"nn.fractional_avg_pool\"])\n@dispatch.add_dispatch_support\n@deprecation.deprecated(date=None, instructions=\"`seed2` and `deterministic` \"\n                        \"args are deprecated.  Use fractional_avg_pool_v2.\")\ndef fractional_avg_pool(value,\n                        pooling_ratio,\n                        pseudo_random=False,\n                        overlapping=False,\n                        deterministic=False,\n                        seed=0,\n                        seed2=0,\n                        name=None):  # pylint: disable=redefined-builtin\n  r\"\"\"Performs fractional average pooling on the input.\n\n  This is a deprecated version of `fractional_avg_pool`.\n\n  Fractional average pooling is similar to Fractional max pooling in the pooling\n  region generation step. The only difference is that after pooling regions are\n  generated, a mean operation is performed instead of a max operation in each\n  pooling region.\n\n  Args:\n    value: A `Tensor`. 4-D with shape `[batch, height, width, channels]`."
},
{
    "Id": 265,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/63feaf321165e1e2795f43e3834c007364921df6",
    "Violation": "missing",
    "Bug report": "Add check for raster bits.",
    "Number of deleted lines": 0,
    "Deleted lines": "  return 0;\n}\n\nstatic const char* GifErrorStringNonNull(int error_code) {\n  const char* error_string = GifErrorString(error_code);\n  if (error_string == nullptr) {\n    return \"Unknown error\";\n  }\n  return error_string;\n}\n\nuint8* Decode(const void* srcdata, int datasize,\n              const std::function<uint8*(int, int, int, int)>& allocate_output,\n              string* error_string, bool expand_animations) {\n  int error_code = D_GIF_SUCCEEDED;\n  InputBufferInfo info = {reinterpret_cast<const uint8*>(srcdata), datasize};\n  GifFileType* gif_file =\n      DGifOpen(static_cast<void*>(&info), &input_callback, &error_code);\n  const auto cleanup = gtl::MakeCleanup([gif_file]() {\n    int error_code = D_GIF_SUCCEEDED;\n    if (gif_file && DGifCloseFile(gif_file, &error_code) != GIF_OK) {\n      LOG(WARNING) << \"Fail to close gif file, reason: \"\n                   << GifErrorStringNonNull(error_code);\n    }\n  });\n  if (error_code != D_GIF_SUCCEEDED) {\n    *error_string = absl::StrCat(\"failed to open gif file: \",\n                                 GifErrorStringNonNull(error_code));\n    return nullptr;\n  }\n\n  if (DGifSlurp(gif_file) != GIF_OK) {\n    *error_string = absl::StrCat(\"failed to slurp gif file: \",\n                                 GifErrorStringNonNull(gif_file->Error));\n    LOG(ERROR) << *error_string;\n    return nullptr;\n  }\n\n  if (gif_file->ImageCount <= 0) {\n    *error_string = \"gif file does not contain any image\";\n    return nullptr;\n  }\n\n  int target_num_frames = gif_file->ImageCount;\n\n  // Don't request more memory than needed for each frame, preventing OOM\n  int max_frame_width = 0;\n  int max_frame_height = 0;\n  for (int k = 0; k < target_num_frames; k++) {\n    SavedImage* si = &gif_file->SavedImages[k];\n    if (max_frame_height < si->ImageDesc.Height)\n      max_frame_height = si->ImageDesc.Height;\n    if (max_frame_width < si->ImageDesc.Width)\n      max_frame_width = si->ImageDesc.Width;\n  }\n\n  const int width = max_frame_width;\n  const int height = max_frame_height;\n  const int channel = 3;\n  if (!expand_animations) target_num_frames = 1;\n\n  uint8* const dstdata =\n      allocate_output(target_num_frames, width, height, channel);\n  if (!dstdata) return nullptr;\n  for (int64_t k = 0; k < target_num_frames; k++) {\n    uint8* this_dst = dstdata + k * width * channel * height;\n\n    SavedImage* this_image = &gif_file->SavedImages[k];\n    GifImageDesc* img_desc = &this_image->ImageDesc;\n"
},
{
    "Id": 266,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e5b0eec199c2d03de54fd6a7fd9275692218e2bc",
    "Violation": "missing",
    "Bug report": " [lite] Add validation check for dilation height/width to be positive integers.",
    "Number of deleted lines": 0,
    "Deleted lines": "\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  // This is a builtin op, so we don't use the contents in 'buffer', if any.\n  // Instead, we allocate a new object to carry information from Prepare() to\n  // Eval().\n  return new OpData;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  bool has_bias = NumInputs(node) == 3;\n\n  TF_LITE_ENSURE(context, has_bias || NumInputs(node) == 2);\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* filter;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kFilterTensor, &filter));\n  const TfLiteTensor* bias = nullptr;\n\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);\n\n  const TfLiteType data_type = input->type;\n\n  const TfLiteType filter_type = filter->type;\n  const bool is_hybrid =\n      data_type == kTfLiteFloat32 && filter_type == kTfLiteInt8;\n  TF_LITE_ENSURE(context,\n                 data_type == kTfLiteFloat32 || data_type == kTfLiteUInt8 ||\n                     data_type == kTfLiteInt8 || data_type == kTfLiteInt16);\n  TF_LITE_ENSURE_TYPES_EQ(context, output->type, data_type);\n  if (!is_hybrid) {\n    TF_LITE_ENSURE(context,\n                   filter->type == data_type || data_type == kTfLiteInt16);\n  }\n\n  if (data_type == kTfLiteInt16) {\n    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);\n    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);\n  }\n\n  // Filter in DepthwiseConv is expected to be [1, H, W, O].\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 0), 1);\n\n  if (has_bias) {\n    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kBiasTensor, &bias));\n    if (data_type == kTfLiteUInt8 || data_type == kTfLiteInt8) {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n    } else if (data_type == kTfLiteInt16) {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt64);\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n    } else {\n      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, data_type);\n    }\n    TF_LITE_ENSURE_EQ(context, NumDimensions(bias), 1);\n    TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 3),"
},
{
    "Id": 267,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5cedb0427bd4db4117182da8bc0680dd555b4f49",
    "Violation": "missing",
    "Bug report": "Add checks for dilation_rate. ",
    "Number of deleted lines": 0,
    "Deleted lines": "      vst1q_s32(acc_buffer + 16 * i + 0, b0);\n      vst1q_s32(acc_buffer + 16 * i + 4, b1);\n      vst1q_s32(acc_buffer + 16 * i + 8, b2);\n      vst1q_s32(acc_buffer + 16 * i + 12, b3);\n    }\n  }\n#endif\n  for (; i < num_output_pixels; i++) {\n    memcpy(acc_buffer + i * output_depth, bias_data,\n           sizeof(acc_buffer[0]) * output_depth);\n  }\n}\n\ninline void DepthwiseConv(\n    const DepthwiseParams& params, const RuntimeShape& input_shape,\n    const uint8* input_data, const RuntimeShape& filter_shape,\n    const uint8* filter_data, const RuntimeShape& bias_shape,\n    const int32* bias_data, const RuntimeShape& output_shape,\n    uint8* output_data) {\n  gemmlowp::ScopedProfilingLabel label(\"DepthwiseConv/8bit\");\n  const int stride_width = params.stride_width;\n  const int stride_height = params.stride_height;\n  const int pad_width = params.padding_values.width;\n  const int pad_height = params.padding_values.height;\n  const int depth_multiplier = params.depth_multiplier;\n  const int32 output_activation_min = params.quantized_activation_min;\n  const int32 output_activation_max = params.quantized_activation_max;\n  const int32 input_offset = params.input_offset;\n  const int32 filter_offset = params.weights_offset;\n  const int32 output_offset = params.output_offset;\n  const int32 output_multiplier = params.output_multiplier;\n  const int output_shift = params.output_shift;\n  const int dilation_width_factor = params.dilation_width_factor;\n  const int dilation_height_factor = params.dilation_height_factor;\n  TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);\n  TFLITE_DCHECK_LE(output_activation_min, output_activation_max);\n  const int batches = MatchingDim(input_shape, 0, output_shape, 0);\n  const int output_depth = MatchingDim(filter_shape, 3, output_shape, 3);\n  const int input_height = input_shape.Dims(1);\n  const int input_width = input_shape.Dims(2);\n  const int input_depth = input_shape.Dims(3);\n  const int filter_height = filter_shape.Dims(1);\n  const int filter_width = filter_shape.Dims(2);\n  const int output_height = output_shape.Dims(1);\n  const int output_width = output_shape.Dims(2);\n#ifdef USE_NEON\n  const bool shift_left = (output_shift > 0);\n  const int32 multiplier_power_of_two = shift_left ? (1 << output_shift) : 1;\n#endif\n  TFLITE_DCHECK_EQ(output_depth, input_depth * depth_multiplier);\n  TFLITE_DCHECK_EQ(bias_shape.FlatSize(), output_depth);\n\n// Enable for arm64 except for the Nvidia Linux 4 Tegra (L4T) running on\n// Jetson TX-2. This compiler does not support the offsetof() macro.\n#if defined(__aarch64__) && !defined(GOOGLE_L4T)\n  // Call kernel optimized for depthwise convolutions using 3x3 filters if\n  // parameters are supported.\n  if (Fast3x3FilterKernelSupported(\n          input_shape, filter_shape, stride_width, stride_height,\n          dilation_width_factor, dilation_height_factor, pad_width, pad_height,\n          depth_multiplier, output_shape, output_shift)) {\n    DepthwiseConv3x3Filter(params, input_shape, input_data, filter_shape,\n                           filter_data, bias_shape, bias_data, output_shape,\n                           output_data);\n    return;\n  }\n#endif\n"
},
{
    "Id": 268,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/55aec0a33011773240f6696393952c984ca8de16",
    "Violation": "insufficient",
    "Bug report": " Add explicit not-None checks for the height and width in `resize_images()`. This was previously raising a `FutureWarning` when the height and/or width were dynamic.",
    "Number of deleted lines": 1,
    "Deleted lines": "  Returns:\n    If `images` was 4-D, a 4-D float Tensor of shape\n    `[batch, new_height, new_width, channels]`.\n    If `images` was 3-D, a 3-D float Tensor of shape\n    `[new_height, new_width, channels]`.\n  \"\"\"\n  images = ops.convert_to_tensor(images, name='images')\n  if images.get_shape().ndims is None:\n    raise ValueError('\\'images\\' contains no shape.')\n  # TODO(shlens): Migrate this functionality to the underlying Op's.\n  is_batch = True\n  if len(images.get_shape()) == 3:\n    is_batch = False\n    images = array_ops.expand_dims(images, 0)\n\n  _, height, width, depth = _ImageDimensions(images)\n\n  # Handle tensor-valued sizes as well as Python integers.\n  try:\n    new_width = ops.convert_to_tensor(new_width, dtypes.int32,\n                                      name='new_width')\n    new_width.get_shape().assert_has_rank(0)\n  except (TypeError, ValueError):\n    raise ValueError('new_width must be a scalar integer')\n  try:\n    new_height = ops.convert_to_tensor(new_height, dtypes.int32,\n                                       name='new_height')\n    new_height.get_shape().assert_has_rank(0)\n  except (TypeError, ValueError):\n    raise ValueError('new_height must be a scalar integer')\n\n  new_width_const = tensor_util.constant_value(new_width)\n  new_height_const = tensor_util.constant_value(new_height)\n\n  if width == new_width_const and height == new_height_const:\n    if not is_batch:\n      images = array_ops.squeeze(images, squeeze_dims=[0])\n    return images\n\n  new_size = array_ops.pack([new_height, new_width])\n\n  if method == ResizeMethod.BILINEAR:\n    images = gen_image_ops.resize_bilinear(images,\n                                           new_size,\n                                           align_corners=align_corners)\n  elif method == ResizeMethod.NEAREST_NEIGHBOR:\n    images = gen_image_ops.resize_nearest_neighbor(images,\n                                                   new_size,\n                                                   align_corners=align_corners)\n  elif method == ResizeMethod.BICUBIC:\n    images = gen_image_ops.resize_bicubic(images,\n                                          new_size,\n                                          align_corners=align_corners)\n  elif method == ResizeMethod.AREA:\n    images = gen_image_ops.resize_area(images,\n                                       new_size,\n                                       align_corners=align_corners)\n  else:\n    raise ValueError('Resize method is not implemented.')\n\n  # NOTE(mrry): The shape functions for the resize ops cannot unpack\n  # the packed values in `new_size`, so set the shape here.\n  images.set_shape([None, new_height_const, new_width_const, None])\n\n  if not is_batch:\n    images = array_ops.squeeze(images, squeeze_dims=[0])\n  return images\n\n\ndef per_image_whitening(image):\n  \"\"\"Linearly scales `image` to have zero mean and unit norm."
},
{
    "Id": 269,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c6899c721f3a4b4f2e71ae4e6d1767341112ff93",
    "Violation": "missing",
    "Bug report": "bug fix when iterators stops at multiple of batch_size ",
    "Number of deleted lines": 0,
    "Deleted lines": "  def get_feed_params(self):\n    \"\"\"Function returns a dict with data feed params while training.\n\n    Returns:\n      A dict with data feed params while training.\n    \"\"\"\n    return {'batch_size': self._batch_size}\n\n  def get_feed_dict_fn(self):\n    \"\"\"Returns a function, that will sample data and provide it to placeholders.\n\n    Returns:\n      A function that when called samples a random subset of batch size\n      from x and y.\n    \"\"\"\n    self.stopped = False\n\n    def _feed_dict_fn():\n      \"\"\"Samples data and provides it to placeholders.\n\n      Returns:\n        Dict of input and output tensors.\n      \"\"\"\n      if self.stopped:\n        raise StopIteration\n      inp = np.zeros(self.input_shape, dtype=self._input_dtype)\n      if self._y is not None:\n        out = np.zeros(self.output_shape, dtype=self._output_dtype)\n      for i in xrange(self._batch_size):\n        # Add handling when queue ends.\n        try:\n          inp[i, :] = six.next(self._x)\n        except StopIteration:\n          self.stopped = True\n          inp = inp[:i, :]\n          if self._y is not None:\n            out = out[:i]\n          break\n\n        if self._y is not None:\n          y = six.next(self._y)\n          if self.n_classes is not None and self.n_classes > 1:\n            if len(self.output_shape) == 2:\n              out.itemset((i, y), 1.0)\n            else:\n              for idx, value in enumerate(y):\n                out.itemset(tuple([i, idx, value]), 1.0)\n          else:\n            out[i] = y\n      if self._y is None:\n        return {self._input_placeholder.name: inp}\n      return {self._input_placeholder.name: inp,\n              self._output_placeholder.name: out}\n\n    return _feed_dict_fn\n\n\nclass DaskDataFeeder(object):\n  \"\"\"Data feeder for that reads data from dask.Series and dask.DataFrame.\n\n  Numpy arrays can be serialized to disk and it's possible to do random seeks\n  into them. DaskDataFeeder will remove requirement to have full dataset in the\n  memory and still do random seeks for sampling of batches.\n  \"\"\"\n\n  def __init__(self, x, y, n_classes, batch_size, shuffle=True,\n               random_state=None, epochs=None):\n    \"\"\"Initializes a DaskDataFeeder instance.\n\n    Args:"
},
{
    "Id": 270,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/66e0cb1d9afd251931f4f920c5d7bd638bc882b4",
    "Violation": "missing",
    "Bug report": " validate clip_norm argument in clip_by_norm API. The API clip_by_norm have argument clip_norm which accepts  0-D (scalar) `Tensor` > 0 . But if we pass -ve value for this argument then its not raising intended error and converting the input tensor into Negative which IMO is wrong. Hence I am adding validation code for -ve values to raise value error.",
    "Number of deleted lines": 0,
    "Deleted lines": "\n  ```\n  # Get your gradients after training\n  loss_value, grads = grad(model, features, labels)\n\n  # Apply some clipping\n  grads = [tf.clip_by_norm(g, norm)\n               for g in grads]\n\n  # Continue on with training\n  optimizer.apply_gradients(grads)\n  ```\n\n  Args:\n    t: A `Tensor` or `IndexedSlices`.  This must be a floating point type.\n    clip_norm: A 0-D (scalar) `Tensor` > 0. A maximum clipping value, also\n      floating point\n    axes: A 1-D (vector) `Tensor` of type int32 containing the dimensions\n      to use for computing the L2-norm. If `None` (the default), uses all\n      dimensions.\n    name: A name for the operation (optional).\n\n  Returns:\n    A clipped `Tensor` or `IndexedSlices`.\n\n  Raises:\n    ValueError: If the clip_norm tensor is not a 0-D scalar tensor.\n    TypeError: If dtype of the input is not a floating point or\n      complex type.\n  \"\"\"\n  with ops.name_scope(name, \"clip_by_norm\", [t, clip_norm]) as name:\n    values = ops.convert_to_tensor(\n        t.values if isinstance(t, indexed_slices.IndexedSlices) else t,\n        name=\"t\")\n\n    # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n    l2sum = math_ops.reduce_sum(values * values, axes, keepdims=True)\n    pred = l2sum > 0\n    # Two-tap tf.where trick to bypass NaN gradients\n    l2sum_safe = array_ops.where(pred, l2sum, array_ops.ones_like(l2sum))\n    l2norm = array_ops.where(pred, math_ops.sqrt(l2sum_safe), l2sum)\n    intermediate = values * clip_norm\n    # Assert that the shape is compatible with the initial shape,\n    # to prevent unintentional broadcasting.\n    values.shape.assert_is_compatible_with(intermediate.shape)\n    values_clip = array_ops.identity(\n        intermediate / math_ops.maximum(l2norm, clip_norm), name=name)\n\n    if isinstance(t, indexed_slices.IndexedSlices):\n      return indexed_slices.IndexedSlices(values_clip, t.indices, t.dense_shape)\n\n    return values_clip\n\n\n@tf_export(\"linalg.global_norm\", v1=[\"linalg.global_norm\", \"global_norm\"])\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints(\"global_norm\")\ndef global_norm(t_list, name=None):\n  \"\"\"Computes the global norm of multiple tensors.\n\n  Given a tuple or list of tensors `t_list`, this operation returns the\n  global norm of the elements in all tensors in `t_list`. The global norm is\n  computed as:\n\n  `global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))`\n\n  Any entries in `t_list` that are of type None are ignored.\n\n  Args:\n    t_list: A tuple or list of mixed `Tensors`, `IndexedSlices`, or None."
},
{
    "Id": 271,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/81ff894c113a5912ba52078ac27e36d06831112e",
    "Violation": "missing",
    "Bug report": "[XLA] Add bounds checks to xla::Array::Slice. To guard against specifying limits that are out of bounds, which ends up touching OOB data.",
    "Number of deleted lines": 0,
    "Deleted lines": "  int64_t num_elements() const { return values_.size; }\n\n  const T* begin() const { return values_.data.get(); }\n  T* begin() { return values_.data.get(); }\n  const T* end() const { return values_.data.get() + num_elements(); }\n  T* end() { return values_.data.get() + num_elements(); }\n\n  bool operator==(const Array<T>& other) const {\n    if (sizes_.size != other.sizes_.size) {\n      return false;\n    }\n    for (int64_t i = 0, end = sizes_.size; i < end; ++i) {\n      if (sizes_[i] != other.sizes_[i]) {\n        return false;\n      }\n    }\n    for (int64_t i = 0; i < num_elements(); ++i) {\n      if (values_[i] != other.values_[i]) {\n        return false;\n      }\n    }\n    return true;\n  }\n\n  bool operator!=(const Array<T>& other) const { return !(*this == other); }\n\n  // Performs the equivalent of a slice operation on this array.\n  Array<T> Slice(absl::Span<const int64_t> starts,\n                 absl::Span<const int64_t> limits) const {\n    CHECK_EQ(starts.size(), num_dimensions());\n    CHECK_EQ(limits.size(), num_dimensions());\n\n    OwnedBuffer<int64_t> sizes(starts.size());\n    for (int64_t i = 0; i < starts.size(); ++i) {\n      sizes[i] = limits[i] - starts[i];\n    }\n    Array<T> result(sizes.span());\n\n    OwnedBuffer<int64_t> index(sizes_.size, default_init_t{});\n    int64_t slice_i = 0;\n    for (int64_t i = 0; i < num_elements(); ++i, next_index(&index)) {\n      if (array_impl::all_inside_range(index.span(), starts, limits)) {\n        // Even though the bounds of result are different to our bounds, we're\n        // iterating in the same order. So we can simply write successive linear\n        // indices instead of recalculating a multi-dimensional index.\n        result.values_[slice_i++] = values_[i];\n      }\n    }\n    return result;\n  }\n\n  // Performs the equivalent of a DynamicUpdateSlice in-place on this array.\n  void UpdateSlice(const Array<T>& from,\n                   absl::Span<const int64_t> start_indices) {\n    CHECK_EQ(from.num_dimensions(), num_dimensions());\n    OwnedBuffer<int64_t> limit_indices(start_indices.size());\n    for (int64_t i = 0; i < start_indices.size(); ++i) {\n      limit_indices[i] = from.sizes_[i] + start_indices[i];\n    }\n    OwnedBuffer<int64_t> index(sizes_.size, default_init_t{});\n    int64_t from_i = 0;\n    for (int64_t i = 0; i < num_elements(); ++i, next_index(&index)) {\n      if (array_impl::all_inside_range(index.span(), start_indices,\n                                       limit_indices)) {\n        // Even though the bounds of from are different to our bounds, we're\n        // iterating in the same order. So we can simply write successive linear\n        // indices instead of recalculating a multi-dimensional index.\n        values_[i] = from.values_[from_i++];\n      }\n    }"
},
{
    "Id": 272,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b7e107eaa6dffb649d055d893a1fce734ee50d55",
    "Violation": "missing",
    "Bug report": " [XLA:GPU] Error out for ptxas version - Update ptxas version check. ",
    "Number of deleted lines": 5,
    "Deleted lines": "\n  static absl::Mutex* mutex = new absl::Mutex;\n  static AlreadyLoggedSetTy* already_logged = new AlreadyLoggedSetTy;\n\n  absl::MutexLock lock(mutex);\n\n  if (already_logged->insert(std::make_tuple(ptxas_path, cc_major, cc_minor))\n          .second) {\n    LOG(WARNING) << \"Falling back to the CUDA driver for PTX compilation; \"\n                    \"ptxas does not support CC \"\n                 << cc_major << \".\" << cc_minor;\n    LOG(WARNING) << \"Used ptxas at \" << ptxas_path;\n  }\n}\n\nstatic void AppendArgsFromOptions(GpuAsmOpts options,\n                                  std::vector<std::string>& args) {\n  if (options.disable_gpuasm_optimizations) {\n    args.push_back(\"-O0\");\n  }\n  args.insert(args.end(), options.extra_flags.begin(),\n              options.extra_flags.end());\n}\n\ntsl::StatusOr<std::array<int64_t, 3>> GetAsmCompilerVersion(\n    const std::string& preferred_cuda_dir) {\n  std::string ptxas_path = FindCudaExecutable(\"ptxas\", preferred_cuda_dir);\n  return GetToolVersion(ptxas_path);\n}\n\ntsl::StatusOr<std::vector<uint8_t>> CompileGpuAsm(int cc_major, int cc_minor,\n                                                  const char* ptx_contents,\n                                                  GpuAsmOpts options,\n                                                  bool cancel_if_reg_spill) {\n  auto ptxas_version_tuple = GetAsmCompilerVersion(options.preferred_cuda_dir);\n  if (ptxas_version_tuple.value() == std::array<int64_t, 3>{12, 3, 1}) {\n    return tsl::errors::Internal(\n        absl::StrFormat(\"ptxas 12.3.1 has a bug that we think can affect XLA. \"\n                        \"Please use a different version.\"));\n  }\n  std::string ptxas_path =\n      FindCudaExecutable(\"ptxas\", options.preferred_cuda_dir);\n\n  WarnIfBadPtxasVersion(ptxas_path);\n\n  // Write ptx into a temporary file.\n  std::string ptx_path;\n  auto env = tsl::Env::Default();\n  if (!env->LocalTempFilename(&ptx_path)) {\n    return tsl::errors::Internal(\"couldn't get temp PTX file name\");\n  }\n  TF_RETURN_IF_ERROR(tsl::WriteStringToFile(env, ptx_path, ptx_contents));\n  VLOG(2) << \"ptx written to: \" << ptx_path;\n\n  absl::Cleanup ptx_cleaner = [&ptx_path] {\n    TF_CHECK_OK(tsl::Env::Default()->DeleteFile(ptx_path));\n  };\n\n  // Invoke ptxas and collect its output.\n  std::string cubin_path;\n  if (!env->LocalTempFilename(&cubin_path)) {\n    return tsl::errors::Internal(\"couldn't get temp CUBIN file name\");\n  }\n  absl::Cleanup cubin_cleaner = [&cubin_path] {\n    // CUBIN file may never be created, so the failure to delete it should not\n    // produce TF error.\n    tsl::Env::Default()->DeleteFile(cubin_path).IgnoreError();\n  };\n  tsl::SubProcess ptxas_info_dumper;\n  // If the target is sm_90, hard code it to sm_90a so that all instructions\n  // can be used. We don't need the portability that sm_90 gives.\n  std::string extension = (cc_major == 9 && cc_minor == 0) ? \"a\" : \"\";\n  std::vector<std::string> ptxas_args = {\n      ptxas_path,\n      ptx_path,"
},
{
    "Id": 273,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cb164786dc891ea11d3a900e90367c339305dc7b",
    "Violation": "improper",
    "Bug report": " Properly handle the case where SpecializeType() returns an error `Status`. If the error case in `SpecializeType()` is reached, then we would get a crash when trying to access the value of an errorenous `StatusOr` object",
    "Number of deleted lines": 1,
    "Deleted lines": "\nStatus InferenceContext::input(StringPiece input_name,\n                               std::vector<ShapeHandle>* output) const {\n  const auto result = input_name_map_.find(input_name);\n  if (result == input_name_map_.end()) {\n    return errors::InvalidArgument(\"Unknown input name: \", input_name);\n  } else {\n    output->clear();\n    for (int i = result->second.first; i < result->second.second; ++i) {\n      output->push_back(inputs_[i]);\n    }\n  }\n  return Status::OK();\n}\n\nStatus InferenceContext::output(StringPiece output_name,\n                                std::vector<ShapeHandle>* output) const {\n  const auto result = output_name_map_.find(output_name);\n  if (result == output_name_map_.end()) {\n    return errors::InvalidArgument(\"Unknown output name: \", output_name);\n  } else {\n    output->clear();\n    for (int i = result->second.first; i < result->second.second; ++i) {\n      output->push_back(outputs_[i]);\n    }\n  }\n  return Status::OK();\n}\n\nvoid InferenceContext::PreInputInit(\n    const OpDef& op_def, const std::vector<const Tensor*>& input_tensors,\n    const std::vector<ShapeHandle>& input_tensors_as_shapes) {\n  // TODO(mdan): This is also done at graph construction. Run only here instead?\n  const auto ret = full_type::SpecializeType(attrs_, op_def);\n  DCHECK(ret.status().ok()) << \"while instantiating types: \" << ret.status();\n  ret_types_ = ret.ValueOrDie();\n\n  input_tensors_ = input_tensors;\n  input_tensors_as_shapes_ = input_tensors_as_shapes;\n\n  construction_status_ =\n      NameRangesForNode(attrs_, op_def, &input_name_map_, &output_name_map_);\n  if (!construction_status_.ok()) return;\n\n  int num_outputs = 0;\n  for (const auto& e : output_name_map_) {\n    num_outputs = std::max(num_outputs, e.second.second);\n  }\n  outputs_.assign(num_outputs, nullptr);\n  output_handle_shapes_and_types_.resize(num_outputs);\n}\n\nStatus InferenceContext::ExpandOutputs(int new_output_size) {\n  const int outputs_size = outputs_.size();\n  if (new_output_size < outputs_size) {\n    return errors::InvalidArgument(\"Trying to reduce number of outputs of op.\");\n  }\n  outputs_.resize(new_output_size, nullptr);\n  output_handle_shapes_and_types_.resize(new_output_size);\n  return Status::OK();\n}\n\nvoid InferenceContext::PostInputInit(\n    std::vector<std::unique_ptr<std::vector<ShapeAndType>>> input_handle_data) {\n  int num_inputs_from_node_def = 0;\n  for (const auto& e : input_name_map_) {\n    num_inputs_from_node_def =\n        std::max(num_inputs_from_node_def, e.second.second);\n  }\n\n  // Allow passing empty shapes/dtypes to avoid changing every single test."
},
{
    "Id": 274,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84d7bf6f64fd9c8677f7f26511ce3031fe8d35a6",
    "Violation": "missing",
    "Bug report": "Add is_numeric to dtypes.cc to check whether a data type is numeric ",
    "Number of deleted lines": 0,
    "Deleted lines": "          \"size\",\n          [](tensorflow::DataType self) {\n            return tensorflow::DataTypeSize(tensorflow::BaseType(self));\n          })\n\n      .def(\"__repr__\",\n           [](tensorflow::DataType self) {\n             return py::str(\"tf.{}\").format(DataTypeStringCompat(self));\n           })\n      .def(\"__str__\",\n           [](tensorflow::DataType self) {\n             return py::str(\"<dtype: {!r}>\")\n#if PY_MAJOR_VERSION < 3\n                 .format(py::bytes(DataTypeStringCompat(self)));\n#else\n                 .format(DataTypeStringCompat(self));\n#endif\n           })\n      .def(\"__hash__\", &DataTypeId)\n\n      .def_property_readonly(\n          \"is_numpy_compatible\",\n          [](tensorflow::DataType self) {\n            return tensorflow::DataTypeIsNumPyCompatible(\n                tensorflow::BaseType(self));\n          },\n          \"Returns whether this data type has a compatible NumPy data type.\")\n\n      .def_property_readonly(\n          \"is_bool\",\n          [](tensorflow::DataType self) {\n            return tensorflow::BaseType(self) == tensorflow::DT_BOOL;\n          },\n          \"Returns whether this is a boolean data type.\")\n      .def_property_readonly(\n          \"is_complex\",\n          [](tensorflow::DataType self) {\n            return tensorflow::DataTypeIsComplex(tensorflow::BaseType(self));\n          },\n          \"Returns whether this is a complex floating point type.\")\n      .def_property_readonly(\n          \"is_floating\",\n          [](tensorflow::DataType self) {\n            return tensorflow::DataTypeIsFloating(tensorflow::BaseType(self));\n          },\n          \"Returns whether this is a (non-quantized, real) floating point \"\n          \"type.\")\n      .def_property_readonly(\n          \"is_integer\",\n          [](tensorflow::DataType self) {\n            return tensorflow::DataTypeIsInteger(tensorflow::BaseType(self));\n          },\n          \"Returns whether this is a (non-quantized) integer type.\")\n      .def_property_readonly(\n          \"is_quantized\",\n          [](tensorflow::DataType self) {\n            return tensorflow::DataTypeIsQuantized(tensorflow::BaseType(self));\n          },\n          \"Returns whether this is a quantized data type.\")\n      .def_property_readonly(\n          \"is_unsigned\",\n          [](tensorflow::DataType self) {\n            return tensorflow::DataTypeIsUnsigned(tensorflow::BaseType(self));\n          },\n          R\"doc(Returns whether this type is unsigned.\n\nNon-numeric, unordered, and quantized types are not considered unsigned, and\nthis function returns `False`.)doc\");\n}\n"
},
{
    "Id": 275,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cd34289b744040974ebe81e1b1e88f1c752d68e0",
    "Violation": "missing",
    "Bug report": "Update types.h to check if a data type is numeric ",
    "Number of deleted lines": 0,
    "Deleted lines": "#undef MATCH_TYPE_AND_ENUM\n\n// All types not specialized are marked invalid.\ntemplate <class T>\nstruct IsValidDataType {\n  static constexpr bool value = false;\n};\n\n// Extra validity checking; not part of public API.\nstatic_assert(IsValidDataType<int64_t>::value, \"Incorrect impl for int64\");\nstatic_assert(IsValidDataType<int32>::value, \"Incorrect impl for int32\");\n\n// TODO(jeff): Maybe unify this with Tensor::CanUseDMA, or the underlying\n// is_simple<T> in tensor.cc (and possible choose a more general name?)\nconstexpr DataTypeSet kDataTypesCanUseMemcpy =\n    ToSet(DT_FLOAT) | ToSet(DT_DOUBLE) | ToSet(DT_INT32) | ToSet(DT_UINT32) |\n    ToSet(DT_UINT8) | ToSet(DT_UINT16) | ToSet(DT_INT16) | ToSet(DT_INT8) |\n    ToSet(DT_COMPLEX64) | ToSet(DT_COMPLEX128) | ToSet(DT_INT64) |\n    ToSet(DT_UINT64) | ToSet(DT_BOOL) | ToSet(DT_QINT8) | ToSet(DT_QUINT8) |\n    ToSet(DT_QINT16) | ToSet(DT_QUINT16) | ToSet(DT_QINT32) |\n    ToSet(DT_BFLOAT16) | ToSet(DT_HALF) | ToSet(DT_FLOAT8_E5M2) |\n    ToSet(DT_FLOAT8_E4M3FN);\ninline bool DataTypeCanUseMemcpy(DataType dt) {\n  return kDataTypesCanUseMemcpy.Contains(dt);\n}\n\n// Returns true iff 'dt' is a real, non-quantized floating point type.\nconstexpr DataTypeSet kDataTypeIsFloating =\n    ToSet(DT_HALF) | ToSet(DT_BFLOAT16) | ToSet(DT_FLOAT) | ToSet(DT_DOUBLE) |\n    ToSet(DT_FLOAT8_E4M3FN) | ToSet(DT_FLOAT8_E5M2);\ninline bool DataTypeIsFloating(DataType dt) {\n  return kDataTypeIsFloating.Contains(dt);\n}\n\n// Returns true iff 'dt' is a complex type.\nconstexpr DataTypeSet kDataTypeIsComplex =\n    ToSet(DT_COMPLEX64) | ToSet(DT_COMPLEX128);\ninline bool DataTypeIsComplex(DataType dt) {\n  return kDataTypeIsComplex.Contains(dt);\n}\n\ninline bool DataTypeIsQuantized(DataType dt) {\n  return kQuantizedTypes.Contains(dt);\n}\n\n// Is the dtype nonquantized integral?\nconstexpr DataTypeSet kDataTypeIsInteger =\n    ToSet(DT_INT8) | ToSet(DT_UINT8) | ToSet(DT_INT16) | ToSet(DT_UINT16) |\n    ToSet(DT_INT32) | ToSet(DT_UINT32) | ToSet(DT_INT64) | ToSet(DT_UINT64);\ninline bool DataTypeIsInteger(DataType dt) {\n  return kDataTypeIsInteger.Contains(dt);\n}\n\n// Is the dtype a signed integral type?\nconstexpr DataTypeSet kDataTypeIsSigned =\n    ToSet(DT_INT8) | ToSet(DT_INT16) | ToSet(DT_INT32) | ToSet(DT_INT64);\ninline bool DataTypeIsSigned(DataType dt) {\n  return kDataTypeIsSigned.Contains(dt);\n}\n\n// Is the dtype an unsigned integral type?\nconstexpr DataTypeSet kDataTypeIsUnsigned =\n    ToSet(DT_UINT8) | ToSet(DT_UINT16) | ToSet(DT_UINT32) | ToSet(DT_UINT64);\ninline bool DataTypeIsUnsigned(DataType dt) {\n  return kDataTypeIsUnsigned.Contains(dt);\n}\n\n// Returns a 0 on failure\nint DataTypeSize(DataType dt);\n"
},
{
    "Id": 276,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/43fd10302bcc8447e7a7205bae848a3a88624775",
    "Violation": "missing",
    "Bug report": "Return error on invalid input in tfl.atan2_custom",
    "Number of deleted lines": 1,
    "Deleted lines": "                              input_y->type == kTfLiteFloat64);\n\n  TfLiteIntArray* output_shape = TfLiteIntArrayCopy(input_y->dims);\n\n  return context->ResizeTensor(context, output, output_shape);\n}\n\ntemplate <typename Float>\nTfLiteStatus Atan2(const TfLiteTensor* input_y, const TfLiteTensor* input_x,\n                   TfLiteTensor* output) {\n  const Float* data_y = tflite::GetTensorData<Float>(input_y);\n  const Float* data_x = tflite::GetTensorData<Float>(input_x);\n  Float* data_output = tflite::GetTensorData<Float>(output);\n\n  const int64_t num_elements = NumElements(input_y);\n  for (int64_t i = 0; i < num_elements; ++i) {\n    data_output[i] = std::atan2(data_y[i], data_x[i]);\n  }\n\n  return TfLiteStatus::kTfLiteOk;\n}\n\nTfLiteStatus Atan2Eval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input_y = tflite::GetInput(context, node, 0);\n  const TfLiteTensor* input_x = tflite::GetInput(context, node, 1);\n  TfLiteTensor* output = tflite::GetOutput(context, node, 0);\n\n  switch (output->type) {\n    case kTfLiteFloat32:\n      TF_LITE_ENSURE_OK(context, Atan2<float>(input_y, input_x, output));\n      break;\n    case kTfLiteFloat64:\n      TF_LITE_ENSURE_OK(context, Atan2<double>(input_y, input_x, output));\n      break;\n    default:\n      TF_LITE_KERNEL_LOG(context, \"Unsupported datatype for atan2 output: %s\",\n                         TfLiteTypeGetName(output->type));\n  }\n\n  return TfLiteStatus::kTfLiteOk;\n}\n\n}  // namespace atan2\n\nTfLiteRegistration* Register_ATAN2() {\n  static TfLiteRegistration r = {nullptr, nullptr, atan2::Atan2Prepare,\n                                 atan2::Atan2Eval};\n  return &r;\n}\n\n}  // namespace custom\n}  // namespace ops\n}  // namespace tflite\n"
},
{
    "Id": 277,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/40c7fe94824100338ef0c495143b26501b1c367e",
    "Violation": "missing",
    "Bug report": "Return error on invalid input in tfl.topkv2 ",
    "Number of deleted lines": 0,
    "Deleted lines": "  TF_LITE_ENSURE_OK(\n      context, GetOutputSafe(context, node, kOutputIndexes, &output_indexes));\n  if (IsDynamicTensor(output_values)) {\n    TF_LITE_ENSURE_OK(context, ResizeOutput(context, node));\n  }\n  const TfLiteTensor* top_k;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTopK, &top_k));\n  int32 k;\n\n  switch (top_k->type) {\n    case kTfLiteInt32:\n      k = top_k->data.i32[0];\n      break;\n    case kTfLiteInt16:\n      k = top_k->data.i16[0];\n      break;\n    default:\n      TF_LITE_KERNEL_LOG(context,\n                         \"Type %s is currently not supported k Type by TopK.\",\n                         TfLiteTypeGetName(output_values->type));\n      return kTfLiteError;\n  }\n\n  switch (output_indexes->type) {\n    case kTfLiteInt32: {\n      return TopKImpl(context, node, k, GetTensorData<int32_t>(output_indexes));\n    } break;\n    case kTfLiteInt16: {\n      return TopKImpl(context, node, k, GetTensorData<int16_t>(output_indexes));\n    } break;\n    default:\n      TF_LITE_KERNEL_LOG(\n          context, \"Output index type %s is currently not supported by TopK.\",\n          TfLiteTypeGetName(output_values->type));\n  }\n\n  return kTfLiteOk;\n}\n\n}  // namespace topk_v2\nTfLiteRegistration* Register_TOPK_V2() {\n  static TfLiteRegistration r = {nullptr, nullptr, topk_v2::Prepare,\n                                 topk_v2::Eval};\n  return &r;\n}\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"
},
{
    "Id": 278,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ef049bdfc4f307c8b3a9dc480a90a5ff287f3d55",
    "Violation": "missing",
    "Bug report": "Add check for ResizeOutput return value in range.cc",
    "Number of deleted lines": 1,
    "Deleted lines": "  op_data->noop = false;\n\n  const TfLiteTensor* start;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kStartTensor, &start));\n  const TfLiteTensor* limit;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kLimitTensor, &limit));\n  const TfLiteTensor* delta;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kDeltaTensor, &delta));\n  // Make sure all the inputs are scalars.\n  TF_LITE_ENSURE_EQ(context, NumDimensions(start), 0);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(limit), 0);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(delta), 0);\n\n  // Currently only supports int32 and float.\n  // TODO(b/117912892): Support quantization as well.\n  const auto dtype = start->type;\n  if (dtype != kTfLiteFloat32 && dtype != kTfLiteInt32) {\n    TF_LITE_KERNEL_LOG(context, \"Unknown index output data type: %s\",\n                       TfLiteTypeGetName(dtype));\n    return kTfLiteError;\n  }\n\n  TF_LITE_ENSURE_TYPES_EQ(context, limit->type, dtype);\n  TF_LITE_ENSURE_TYPES_EQ(context, delta->type, dtype);\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n  output->type = dtype;\n\n  if (IsConstantOrPersistentTensor(start) &&\n      IsConstantOrPersistentTensor(limit) &&\n      IsConstantOrPersistentTensor(delta)) {\n    SetTensorToPersistentRo(output);\n    ResizeOutput(context, start, limit, delta, output);\n\n    op_data->noop = true;\n    return EvalImpl(context, start, delta, output);\n  }\n\n  SetTensorToDynamic(output);\n  return kTfLiteOk;\n}\n\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* start;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kStartTensor, &start));\n  const TfLiteTensor* limit;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kLimitTensor, &limit));\n  const TfLiteTensor* delta;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kDeltaTensor, &delta));\n\n  OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n  if (op_data->noop) {\n    return kTfLiteOk;\n  }\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  if (IsDynamicTensor(output)) {\n    TF_LITE_ENSURE_OK(context,\n                      ResizeOutput(context, start, limit, delta, output));\n  }\n  return EvalImpl(context, start, delta, output);\n}\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  return new OpData;\n}\n"
},
{
    "Id": 279,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1707ed9b9b0cc5cb02df22a06718c9c738825d39",
    "Violation": "missing",
    "Bug report": " Add a check to make sure that the allocation before an Evict() is not a prefetch.",
    "Number of deleted lines": 3,
    "Deleted lines": "    AddToPendingChunks(alternate_mem_interval, *chunk_candidate);\n\n    // If there was a previous allocation, the buffer location is the\n    // same as the previous. Otherwise, it is the operand.\n    if (prev_allocation != nullptr &&\n        (prev_allocation->is_copy_like_allocation() ||\n         prev_allocation->defining_position() == defining_position)) {\n      prev_allocation->set_end_time(request.end_time);\n    } else {\n      request.allocation_value->mutable_allocation_sequence()->push_back(\n          std::make_unique<MemorySpaceAssignment::Allocation>(\n              defining_position, MemorySpace::kAlternate, chunk_candidate,\n              request.inclusive_start_time, request.end_time,\n              /*is_scoped_allocation=*/false));\n      CreateOrAddToAliasedOffset(\n          *request.allocation_value->allocation_sequence()->back(),\n          preferred_offset);\n    }\n    request.allocation_value->allocation_sequence()->back()->AddUse(\n        request.use->hlo_use);\n    return Result::kSuccess;\n  }\n  if (request.prefer_no_copy_alternate_mem_allocation) {\n    LOG(WARNING) << \"Preferred no-copy allocation, but this was not possible: \"\n                 << request.use->hlo_use.ToString();\n  }\n  return Result::kFailOutOfMemory;\n}\n\nAlternateMemoryBestFitHeap::Result AlternateMemoryBestFitHeap::Evict(\n    const AllocationRequest& request) {\n  CHECK_GT(request.allocation_value->allocation_sequence()->size(), 0);\n  MemorySpaceAssignment::Allocation* prev_allocation =\n      request.allocation_value->allocation_sequence()->back().get();\n  // TODO(b/306478911): prev_allocation can never be a prefetch, or we would be\n  // using an incorrect start time (we would need to wait until the copies\n  // finish)\n\n  // The previous allocation's inclusive start time is the eviction's exclusive\n  // start time to ensure that the value is created before we start copying\n  // back to default memory.\n  int64_t eviction_exclusive_start_time = prev_allocation->start_time();\n  int64_t eviction_end_time = prev_allocation->end_time();\n  CHECK(eviction_exclusive_start_time <= eviction_end_time);\n\n  int64_t preferred_eviction_end_time =\n      std::max(options_.prefetch_interval_picker->PreferredEvictionEndTime(\n                   request.allocation_value->defining_position().shape(),\n                   eviction_exclusive_start_time, request.end_time),\n               eviction_end_time);\n  // Evictions must complete by the time of this use.\n  preferred_eviction_end_time =\n      std::min(preferred_eviction_end_time, request.latest_prefetch_time);\n\n  BufferInterval eviction_mem_interval;\n  eviction_mem_interval.buffer = request.allocation_value->value();\n  eviction_mem_interval.size = request.size;\n  // Try to reserve a buffer from the end of the previous allocation to the\n  // preferred eviction end time.\n  eviction_mem_interval.start = eviction_end_time + 1;\n  eviction_mem_interval.end = preferred_eviction_end_time;\n  int64_t preferred_offset = prev_allocation->chunk().offset;\n  VLOG(3) << \"Eviction (\" << eviction_exclusive_start_time << \", \"\n          << eviction_end_time\n          << \") preferred end time = \" << eviction_mem_interval.end;\n\n  for (; eviction_mem_interval.end > eviction_end_time;\n       --eviction_mem_interval.end) {\n    Chunk chunk_candidate =\n        FindChunkCandidate(eviction_mem_interval, preferred_offset);\n    if (chunk_candidate.offset == preferred_offset) {\n      AddToPendingChunks(eviction_mem_interval, chunk_candidate);\n      break;"
},
{
    "Id": 280,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f636be3bb1f556c15dba3028e61a8969d90dadd9",
    "Violation": "misleading",
    "Bug report": "Return error on invalid input in tfl.sign_custom",
    "Number of deleted lines": 5,
    "Deleted lines": "template <typename Op, typename T>\nTfLiteStatus PointwiseUnaryOpDoEval(\n    TfLiteContext* context,\n    const TfLiteTensor* input,\n    TfLiteTensor* output) {\n  const T* data = tflite::GetTensorData<T>(input);\n  T* data_output = tflite::GetTensorData<T>(output);\n\n  const int64_t num_elements = NumElements(input);\n  for (int64_t i = 0; i < num_elements; ++i) {\n    data_output[i] = Op::template Eval<T>(data[i]);\n  }\n\n  return TfLiteStatus::kTfLiteOk;\n}\n\n// A generic evaluation function where the actual data processing is handled\n// by the Op::Eval<T> function.\ntemplate <typename Op>\nTfLiteStatus PointwiseUnaryOpEval(TfLiteContext* context, TfLiteNode* node) {\n  const TfLiteTensor* input = tflite::GetInput(context, node, 0);\n  TfLiteTensor* output = tflite::GetOutput(context, node, 0);\n\n  switch (output->type) {\n    case kTfLiteFloat32:\n      TF_LITE_ENSURE_OK(\n          context,\n          (PointwiseUnaryOpDoEval<Op, float>(context, input, output)));\n      break;\n    case kTfLiteFloat64:\n      TF_LITE_ENSURE_OK(\n          context,\n          (PointwiseUnaryOpDoEval<Op, double>(context, input, output)));\n      break;\n    default:\n      TF_LITE_KERNEL_LOG(\n          context,\n          \"Unsupported datatype for atan2 output: %s\",\n          TfLiteTypeGetName(output->type));\n  }\n\n  return TfLiteStatus::kTfLiteOk;\n}\n\n// Operator that computes the sign function.\nstruct Sign {\n  template <typename T>\n  static T Eval(T x) {\n    if (x > 0) {\n      return 1;\n    }\n    if (x < 0) {\n      return -1;\n    }\n    return 0;\n  }\n};\n\n}  // namespace sign\n\nTfLiteRegistration* Register_SIGN() {\n  static TfLiteRegistration r = {nullptr, nullptr,\n                                 sign::PointwiseUnaryOpPrepare,\n                                 sign::PointwiseUnaryOpEval<sign::Sign>};\n  return &r;\n}\n\n}  // namespace custom\n}  // namespace ops\n}  // namespace tflite\n"
},
{
    "Id": 281,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e99e31597c1b5cc9f0cbc8a3dea71674d81c20b1",
    "Violation": "misleading",
    "Bug report": " Fix GRUCellBlockOp message for invalid rank of x. The validation checks that x is a matrix, so rank must be 2.",
    "Number of deleted lines": 2,
    "Deleted lines": "namespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\ntemplate <typename Device, typename T, bool USE_CUBLAS>\nclass GRUCellBlockOp : public OpKernel {\n public:\n  explicit GRUCellBlockOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n  // TODO(gitegaurav) Replace the input checks with some smarter function.\n  void Compute(OpKernelContext* ctx) override {\n    // Grab the input tensors.\n    const Tensor* x_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x_tensor));\n\n    const Tensor* h_prev_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"h_prev\", &h_prev_tensor));\n\n    const Tensor* w_ru_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"w_ru\", &w_ru_tensor));\n\n    const Tensor* w_c_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"w_c\", &w_c_tensor));\n\n    const Tensor* b_ru_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_ru\", &b_ru_tensor));\n\n    const Tensor* b_c_tensor = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_c\", &b_c_tensor));\n\n    // Sanity checks for input shapes.\n\n    // Shape of 'x' must be [batch_size, input_size]\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(x_tensor->shape()),\n                errors::InvalidArgument(\"Rank of x must be 2\", x_tensor->dims(),\n                                        \" vs. 2\"));\n    const int64_t batch_size = x_tensor->dim_size(0);\n    const int64_t input_size = x_tensor->dim_size(1);\n\n    // Shape of 'h' must be [batch_size, cell_size]\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(h_prev_tensor->shape()),\n                errors::InvalidArgument(\"Rank of h_prev must be 2, got \",\n                                        h_prev_tensor->dims()));\n    OP_REQUIRES(ctx, h_prev_tensor->dim_size(0) == batch_size,\n                errors::InvalidArgument(\"h_prev.dims(0) != batch_size: \",\n                                        h_prev_tensor->dim_size(0), \" vs. \",\n                                        batch_size));\n    const int64_t cell_size = h_prev_tensor->dim_size(1);\n\n    // Shape of 'w_ru' must be [input_size+cell_size, 2*cell_size]\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(w_ru_tensor->shape()),\n                errors::InvalidArgument(\"Rank of w_ru_ must be 2, got \",\n                                        w_ru_tensor->dims()));\n    OP_REQUIRES(ctx, w_ru_tensor->dim_size(0) == input_size + cell_size,\n                errors::InvalidArgument(\n                    \"w_ru.dim_size(0) != input_size + cell_size: \",\n                    w_ru_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n    OP_REQUIRES(ctx, w_ru_tensor->dim_size(1) == cell_size * 2,\n                errors::InvalidArgument(\"w_ru.dim_size(1) != cell_size * 2: \",\n                                        w_ru_tensor->dim_size(1), \" vs. \",\n                                        cell_size * 2));\n\n    // Shape of 'w_c' must be [input_size+cell_size, cell_size]\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(w_c_tensor->shape()),\n                errors::InvalidArgument(\"Rank of w_c must be 2, got \",\n                                        w_c_tensor->dims()));\n    OP_REQUIRES(ctx, w_c_tensor->dim_size(0) == input_size + cell_size,\n                errors::InvalidArgument(\n                    \"w_c.dim_size(0) != input_size + cell_size: \",\n                    w_c_tensor->dim_size(0), \" vs. \", input_size + cell_size));\n    OP_REQUIRES(ctx, w_c_tensor->dim_size(1) == cell_size,\n                errors::InvalidArgument("
},
{
    "Id": 282,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/18dd91ccd4b1817cd5c34e40f76823a162bea029",
    "Violation": "misleading",
    "Bug report": " [XLA] Report that real -> complex bitcast_convert is not allowed. The check as exists is bidirectional: it prevents conversions from complex to real and real to complex alike, but the reported error message was unidirectional.",
    "Number of deleted lines": 1,
    "Deleted lines": "    }\n  }\n  return result;\n}\n\n/* static */ StatusOr<Shape> ShapeInference::InferConvertShape(\n    const Shape& operand_shape, PrimitiveType new_element_type) {\n  auto old_element_type = operand_shape.element_type();\n  if (primitive_util::IsComplexType(old_element_type) &&\n      !primitive_util::IsComplexType(new_element_type)) {\n    return Unimplemented(\n        \"Conversion from complex to real type %s => %s is not implemented.\",\n        ShapeUtil::HumanString(operand_shape),\n        PrimitiveType_Name(new_element_type));\n  }\n  if (!operand_shape.IsArray() ||\n      !primitive_util::IsArrayType(new_element_type)) {\n    // Note: we may want to support tuple conversions via this operation in the\n    // future, by recursing into the tuple elements to check all sub-conversions\n    // are valid. For now we just reject them, though.\n    return InvalidArgument(\n        \"Convert does not allow non-arrays, so cannot convert from %s to %s.\",\n        ShapeUtil::HumanString(operand_shape),\n        PrimitiveType_Name(new_element_type));\n  }\n\n  return ShapeUtil::ChangeElementType(operand_shape, new_element_type);\n}\n\n/* static */ StatusOr<Shape> ShapeInference::InferBitcastConvertShape(\n    const Shape& operand_shape, PrimitiveType new_element_type) {\n  auto old_element_type = operand_shape.element_type();\n  if (primitive_util::IsComplexType(old_element_type) !=\n      primitive_util::IsComplexType(new_element_type)) {\n    return InvalidArgument(\"Conversion from complex to real type %s => %s.\",\n                           ShapeUtil::HumanString(operand_shape),\n                           PrimitiveType_Name(new_element_type));\n  }\n  if (!operand_shape.IsArray() ||\n      !primitive_util::IsArrayType(new_element_type)) {\n    // Note: we may want to support tuple conversions via this operation in the\n    // future, by recursing into the tuple elements to check all sub-conversions\n    // are valid. For now we just reject them, though.\n    return InvalidArgument(\n        \"Cannot convert from or to tuple type; requested conversion: %s => %s.\",\n        ShapeUtil::HumanString(operand_shape),\n        PrimitiveType_Name(new_element_type));\n  }\n\n  int input_bitwidth = primitive_util::BitWidth(old_element_type);\n  int output_bitwidth = primitive_util::BitWidth(new_element_type);\n  if (std::max(input_bitwidth, output_bitwidth) %\n          std::min(input_bitwidth, output_bitwidth) !=\n      0) {\n    return InvalidArgument(\n        \"Cannot bitcast types with undivisible bit-widths: %s => %s.\",\n        PrimitiveType_Name(old_element_type),\n        PrimitiveType_Name(new_element_type));\n  }\n  int ratio = std::max(output_bitwidth, input_bitwidth) /\n              std::min(output_bitwidth, input_bitwidth);\n\n  Shape new_shape = operand_shape;\n  new_shape.set_element_type(new_element_type);\n  if (input_bitwidth > output_bitwidth) {\n    ShapeUtil::AppendMinorDimension(ratio, &new_shape);\n  } else if (input_bitwidth < output_bitwidth) {\n    int last_dimension_idx = operand_shape.dimensions_size() - 1;\n    if (operand_shape.dimensions_size() < 1 ||\n        operand_shape.dimensions(last_dimension_idx) != ratio) {\n      return InvalidArgument("
},
{
    "Id": 283,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1e5c11676dce37bb7c8eb58b35fd298a655c6fd3",
    "Violation": "misleading",
    "Bug report": " [tf.data service] Include dispatcher address in version check error message. This is the error message that happens when the address was specified incorrectly, so it is useful to include the potentially-incorrect address in the error message.",
    "Number of deleted lines": 1,
    "Deleted lines": "  GetWorkersRequest req;\n  GetWorkersResponse resp;\n  grpc::ClientContext ctx;\n  grpc::Status s = stub_->GetWorkers(&ctx, req, &resp);\n  if (!s.ok()) {\n    return grpc_util::WrapError(\"Failed to get workers\", s);\n  }\n  workers.clear();\n  for (auto& worker : resp.workers()) {\n    workers.push_back(worker);\n  }\n  return Status::OK();\n}\n\nStatus DataServiceDispatcherClient::EnsureInitialized() {\n  mutex_lock l(mu_);\n  if (stub_) {\n    return Status::OK();\n  }\n  std::shared_ptr<grpc::ChannelCredentials> credentials;\n  TF_RETURN_IF_ERROR(\n      CredentialsFactory::CreateClientCredentials(protocol_, &credentials));\n  grpc::ChannelArguments args;\n  args.SetMaxReceiveMessageSize(std::numeric_limits<int32>::max());\n  args.SetInt(GRPC_ARG_USE_LOCAL_SUBCHANNEL_POOL, true);\n  auto channel = grpc::CreateCustomChannel(address_, credentials, args);\n  stub_ = DispatcherService::NewStub(channel);\n  GetVersionRequest req;\n  GetVersionResponse resp;\n  TF_RETURN_IF_ERROR(grpc_util::Retry(\n      [&] {\n        grpc::ClientContext ctx;\n        grpc::Status s = stub_->GetVersion(&ctx, req, &resp);\n        if (!s.ok()) {\n          return grpc_util::WrapError(\"Failed to get dispatcher version\", s);\n        }\n        return Status::OK();\n      },\n      \"check service version\",\n      /*deadline_micros=*/kint64max));\n  if (resp.version() != kDataServiceVersion) {\n    return errors::FailedPrecondition(\n        \"Version mismatch with tf.data service server. The server is running \"\n        \"version \",\n        resp.version(), \", while the client is running version \",\n        kDataServiceVersion,\n        \". Please ensure that the client and server side are running the \"\n        \"same version of TensorFlow.\");\n  }\n  return Status::OK();\n}\n\nclass GrpcDataTransferClient : public DataTransferClient {\n public:\n  GrpcDataTransferClient(std::shared_ptr<grpc::ChannelCredentials> credentials,\n                         std::string address) {\n    grpc::ChannelArguments args;\n    args.SetMaxReceiveMessageSize(-1);\n    auto channel = grpc::CreateCustomChannel(address, credentials, args);\n    stub_ = WorkerService::NewStub(channel);\n  }\n\n  Status GetElement(const GetElementRequest& req,\n                    GetElementResult& result) override {\n    {\n      mutex_lock l(mu_);\n      if (cancelled_) {\n        return errors::Cancelled(\"Client was cancelled.\");\n      }\n    }\n    grpc::ClientContext ctx;"
},
{
    "Id": 284,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/07898e752cf02518508f193a0be2e451450044bd",
    "Violation": "misleading",
    "Bug report": " Provide a more informative error message when the bazel version check fails. ",
    "Number of deleted lines": 2,
    "Deleted lines": "    print('    %s' % config['tensorrt_library_dir'])\n    print('    %s' % config['tensorrt_include_dir'])\n\n  if config.get('nccl_version', None):\n    print('Found NCCL %s in:' % config['nccl_version'])\n    print('    %s' % config['nccl_library_dir'])\n    print('    %s' % config['nccl_include_dir'])\n\n  print('\\n')\n\n  environ_cp['CUDA_TOOLKIT_PATH'] = config['cuda_toolkit_path']\n  return True\n\n\ndef main():\n  global _TF_WORKSPACE_ROOT\n  global _TF_BAZELRC\n  global _TF_CURRENT_BAZEL_VERSION\n\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '--workspace',\n      type=str,\n      default=os.path.abspath(os.path.dirname(__file__)),\n      help='The absolute path to your active Bazel workspace.')\n  args = parser.parse_args()\n\n  _TF_WORKSPACE_ROOT = args.workspace\n  _TF_BAZELRC = os.path.join(_TF_WORKSPACE_ROOT, _TF_BAZELRC_FILENAME)\n\n  # Make a copy of os.environ to be clear when functions and getting and setting\n  # environment variables.\n  environ_cp = dict(os.environ)\n\n  current_bazel_version = check_bazel_version(_TF_MIN_BAZEL_VERSION,\n                                              _TF_MAX_BAZEL_VERSION)\n  _TF_CURRENT_BAZEL_VERSION = convert_version_to_int(current_bazel_version)\n\n  reset_tf_configure_bazelrc()\n\n  cleanup_makefile()\n  setup_python(environ_cp)\n\n  if is_windows():\n    environ_cp['TF_NEED_OPENCL_SYCL'] = '0'\n    environ_cp['TF_NEED_COMPUTECPP'] = '0'\n    environ_cp['TF_NEED_OPENCL'] = '0'\n    environ_cp['TF_CUDA_CLANG'] = '0'\n    environ_cp['TF_NEED_TENSORRT'] = '0'\n    # TODO(ibiryukov): Investigate using clang as a cpu or cuda compiler on\n    # Windows.\n    environ_cp['TF_DOWNLOAD_CLANG'] = '0'\n    environ_cp['TF_NEED_MPI'] = '0'\n\n  if is_macos():\n    environ_cp['TF_NEED_TENSORRT'] = '0'\n  else:\n    environ_cp['TF_CONFIGURE_IOS'] = '0'\n\n  if environ_cp.get('TF_ENABLE_XLA', '1') == '1':\n    write_to_bazelrc('build --config=xla')\n\n  set_action_env_var(\n      environ_cp,\n      'TF_NEED_OPENCL_SYCL',\n      'OpenCL SYCL',\n      False,\n      bazel_config_name='sycl')\n  if environ_cp.get('TF_NEED_OPENCL_SYCL') == '1':\n    set_host_cxx_compiler(environ_cp)\n    set_host_c_compiler(environ_cp)\n    set_action_env_var(environ_cp, 'TF_NEED_COMPUTECPP', 'ComputeCPP', True)"
},
{
    "Id": 285,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/01e84d7cc214dbf5a7a21bc418ad43afb5694fbc",
    "Violation": "misleading",
    "Bug report": " Update error message for data_adapter with validation split. Remove the user provided value in the error string in case it contains large amount of data. Dump large input data to log might crash on user side.",
    "Number of deleted lines": 2,
    "Deleted lines": "  def _expand_single_1d_tensor(t):\n    # Leaves `CompositeTensor`s as-is.\n    if (isinstance(t, ops.Tensor) and\n        isinstance(t.shape, tensor_shape.TensorShape) and t.shape.rank == 1):\n      return array_ops.expand_dims_v2(t, axis=-1)\n    return t\n\n  return nest.map_structure(_expand_single_1d_tensor, data)\n\n\ndef train_validation_split(arrays, validation_split, shuffle=True):\n  \"\"\"Split arrays into random train and validation subsets.\n\n  Arguments:\n    arrays: Tensors to split. Allowed inputs are arbitrarily nested structures\n      of Tensors and NumPy arrays.\n    validation_split: Float between 0 and 1. The proportion of the dataset to\n      include in the validation split. The rest of the dataset will be included\n      in the training split.\n    shuffle: Bool. Whether to shuffle the data before performing a split. If\n      `False`, the last `validation_split` fraction of that training data will\n      become the validation split.\n\n  Returns:\n    `(train_arrays, validation_arrays)`\n  \"\"\"\n\n  def _can_split(t):\n    tensor_types = (ops.Tensor, np.ndarray)\n    if pd:\n      tensor_types = (ops.Tensor, np.ndarray, pd.Series, pd.DataFrame)\n    return isinstance(t, tensor_types) or t is None\n\n  flat_arrays = nest.flatten(arrays)\n  if not all(_can_split(t) for t in flat_arrays):\n    raise ValueError(\n        \"`validation_split` is only supported for Tensors or NumPy \"\n        \"arrays, found: {}\".format(arrays))\n\n  if all(t is None for t in flat_arrays):\n    return arrays, arrays\n\n  first_non_none = None\n  for t in flat_arrays:\n    if t is not None:\n      first_non_none = t\n      break\n\n  # Assumes all arrays have the same batch shape or are `None`.\n  batch_dim = int(first_non_none.shape[0])\n  indices = ops.convert_to_tensor_v2(range(batch_dim))\n  if shuffle:\n    indices = random_ops.random_shuffle(indices)\n  split_at = int(math.floor(batch_dim * (1. - validation_split)))\n  train_indices = indices[:split_at]\n  val_indices = indices[split_at:]\n\n  def _split(t, indices):\n    if t is None:\n      return t\n    t = ops.convert_to_tensor_v2(t)\n    return array_ops.gather_v2(t, indices)\n\n  train_arrays = nest.map_structure(\n      functools.partial(_split, indices=train_indices), arrays)\n  val_arrays = nest.map_structure(\n      functools.partial(_split, indices=val_indices), arrays)\n\n  return train_arrays, val_arrays\n\n\ndef unpack_x_y_sample_weight(data):\n  \"\"\"Unpacks user-provided data tuple.\"\"\"\n  if not isinstance(data, tuple):"
},
{
    "Id": 286,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4c75fb1cb917320acb386cf26adeb8e5151ca4f6",
    "Violation": "misleading",
    "Bug report": " Improve error message reporting for check_numerics gradient. At present the op message is only printed if the numeric check fails during the op's 'forward' computation. If the check fails during the gradient, there is no identifier on *which* op's gradient failed.",
    "Number of deleted lines": 2,
    "Deleted lines": "  # support an axis parameter, we transpose the gather dimension to the front,\n  # then use `unsorted_segment_sum` to build a\n  # [gather_axis, outer_axes, inner_axes] tensor with all the gradients\n  # affecting each index in `gather_axis` summed up.\n  transpose_dims = array_ops.concat(\n      [[outer_dims], outer_axes_indices, inner_axes_indices], 0)\n  values_transpose = array_ops.transpose(values, transpose_dims)\n  num_segments = params_shape[axis]\n\n  params_grad = math_ops.unsorted_segment_sum(values_transpose, indices,\n                                              num_segments)\n\n  # Inverts the above transpose by moving dimension 0 back to its original\n  # position.\n  invert_transpose_dims = array_ops.concat(\n      [outer_axes_indices + 1, [0], inner_axes_indices], 0)\n  params_grad = array_ops.transpose(params_grad, invert_transpose_dims)\n  return [params_grad, None, None]\n\n\n@ops.RegisterGradient(\"GatherNd\")\ndef _GatherNdGrad(op, grad):\n  ref = op.inputs[0]\n  indices = op.inputs[1]\n  ref_shape = array_ops.shape(ref, out_type=indices.dtype)\n  if indices.shape.ndims == 2 and indices.shape.dims[-1].value == 1:\n    ref_grad = ops.IndexedSlices(grad, array_ops.squeeze(indices, axis=-1),\n                                 ref_shape)\n  else:\n    ref_grad = array_ops.scatter_nd(indices, grad, ref_shape)\n  return [ref_grad, None]\n\n\n@ops.RegisterGradient(\"CheckNumerics\")\ndef _CheckNumericsGrad(_, grad):\n  \"\"\"Gradient for check_numerics op.\"\"\"\n  return array_ops.check_numerics(\n      grad, \"Not a number (NaN) or infinity (Inf) values detected in gradient.\")\n\n\n@ops.RegisterGradient(\"PlaceholderWithDefault\")\n@ops.RegisterGradient(\"Identity\")\ndef _IdGrad(_, grad):\n  return grad\n\n\n@ops.RegisterGradient(\"RefIdentity\")\ndef _RefIdGrad(_, grad):\n  return grad\n\n\n@ops.RegisterGradient(\"IdentityN\")\ndef _IdNGrad(_, *grad):\n  return grad\n\n\nops.NotDifferentiable(\"StopGradient\")\n\n\n@ops.RegisterGradient(\"Reshape\")\ndef _ReshapeGrad(op, grad):\n  return [array_ops.reshape(grad, array_ops.shape(op.inputs[0])), None]\n\n\nops.NotDifferentiable(\"InvertPermutation\")\n\n\ndef _ReshapeToInput(op, grad):\n  \"\"\"Reshapes the gradient to the shape of the original input.\"\"\"\n  return array_ops.reshape(grad, array_ops.shape(op.inputs[0]))\n\n\n@ops.RegisterGradient(\"ExpandDims\")\ndef _ExpandDimsGrad(op, grad):"
},
{
    "Id": 287,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/40918f36823973e816bd50766b1f447225b1bb9b",
    "Violation": "misleading",
    "Bug report": " Make the type check error message more informative for contrib.layers fully_connected.",
    "Number of deleted lines": 2,
    "Deleted lines": "  Note: that if `inputs` have a rank greater than 2, then `inputs` is flattened\n  prior to the initial matrix multiply by `weights`.\n\n  Args:\n    inputs: A tensor of at least rank 2 and static value for the last dimension;\n      i.e. `[batch_size, depth]`, `[None, None, None, channels]`.\n    num_outputs: Integer or long, the number of output units in the layer.\n    activation_fn: Activation function. The default value is a ReLU function.\n      Explicitly set it to None to skip it and maintain a linear activation.\n    normalizer_fn: Normalization function to use instead of `biases`. If\n      `normalizer_fn` is provided then `biases_initializer` and\n      `biases_regularizer` are ignored and `biases` are not created nor added.\n      default set to None for no normalizer function\n    normalizer_params: Normalization function parameters.\n    weights_initializer: An initializer for the weights.\n    weights_regularizer: Optional regularizer for the weights.\n    biases_initializer: An initializer for the biases. If None skip biases.\n    biases_regularizer: Optional regularizer for the biases.\n    reuse: Whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n    variables_collections: Optional list of collections for all the variables or\n      a dictionary containing a different list of collections per variable.\n    outputs_collections: Collection to add the outputs.\n    trainable: If `True` also add variables to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    scope: Optional scope for variable_scope.\n\n  Returns:\n     The tensor variable representing the result of the series of operations.\n\n  Raises:\n    ValueError: If x has rank less than 2 or if its last dimension is not set.\n  \"\"\"\n  if not isinstance(num_outputs, six.integer_types):\n    raise ValueError('num_outputs should be int or long, got %s.' %\n                     (num_outputs,))\n\n  layer_variable_getter = _build_variable_getter({\n      'bias': 'biases',\n      'kernel': 'weights'\n  })\n\n  with variable_scope.variable_scope(\n      scope,\n      'fully_connected', [inputs],\n      reuse=reuse,\n      custom_getter=layer_variable_getter) as sc:\n    inputs = ops.convert_to_tensor(inputs)\n    layer = core_layers.Dense(\n        units=num_outputs,\n        activation=None,\n        use_bias=not normalizer_fn and biases_initializer,\n        kernel_initializer=weights_initializer,\n        bias_initializer=biases_initializer,\n        kernel_regularizer=weights_regularizer,\n        bias_regularizer=biases_regularizer,\n        activity_regularizer=None,\n        trainable=trainable,\n        name=sc.name,\n        dtype=inputs.dtype.base_dtype,\n        _scope=sc,\n        _reuse=reuse)\n    outputs = layer.apply(inputs)\n\n    # Add variables to collections.\n    _add_variable_to_collections(layer.kernel, variables_collections, 'weights')\n    if layer.bias is not None:\n      _add_variable_to_collections(layer.bias, variables_collections, 'biases')\n\n    # Apply normalizer function / layer.\n    if normalizer_fn is not None:\n      if not normalizer_params:"
},
{
    "Id": 288,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9c1f14322484e44a93b77619ffd2e24b9b7a9b1d",
    "Violation": "misleading",
    "Bug report": " Fix error message in TF-keras dataset shape check. (Dimension and tensor # were transposed in the error message)",
    "Number of deleted lines": 1,
    "Deleted lines": "  def _verify_dataset_shape(self, dataset):\n    \"\"\"Verifies a dataset is of an appropriate shape for TPUs.\"\"\"\n    if not isinstance(dataset, dataset_ops.Dataset):\n      raise ValueError('The function passed as the `x` parameter did not '\n                       'return a `tf.data.Dataset`.')\n    if not isinstance(dataset.output_classes, tuple):\n      raise ValueError('The dataset must return a tuple of tf.Tensors, '\n                       'instead it returns: %s' % dataset.output_classes)\n    if len(dataset.output_classes) != 2:\n      raise ValueError(\n          'The dataset must return a 2-element tuple, got '\n          '%s output classes instead.' % (dataset.output_classes,))\n    for i, cls in enumerate(dataset.output_classes):\n      if cls != ops.Tensor:\n        raise ValueError('The dataset returned a non-Tensor type (%s) at '\n                         'index %d.' % (cls, i))\n    for i, shape in enumerate(dataset.output_shapes):\n      if not shape:\n        raise ValueError('The dataset returns a scalar tensor in '\n                         'tuple index %d. Did you forget to batch? '\n                         '(Output shapes: %s).' % (i,\n                                                   dataset.output_shapes))\n      for j, dim in enumerate(shape):\n        if dim.value is None:\n          if j == 0:\n            hint = (' Hint: did you use `ds.batch(BATCH_SIZE, '\n                    'drop_remainder=True)`?')\n          else:\n            hint = ''\n          raise ValueError(\n              'The Keras-TPU integration for `tf.data` '\n              'currently requires static shapes. The provided '\n              'dataset only has a partially defined shape. '\n              '(Dimension %d of output tensor %d is not statically known '\n              'for output shapes: %s.%s)' % (i, j, dataset.output_shapes, hint))\n\n  @property\n  def dummy_x(self):\n    return self._dummy_x\n\n  @property\n  def dummy_y(self):\n    return self._dummy_y\n\n  def make_infeed_instance(self, inputs):\n    # TODO(saeta): Verify inputs is as expected.\n    return self._infeed_instance\n\n  def build_infeed_from_input_specs(self, input_specs, execution_mode):\n    shard_infeed_tensors = self._get_next_ops\n    assert len(shard_infeed_tensors) == self._strategy.num_towers\n    infeed_ops = []\n    for shard_id in range(self._strategy.num_towers):\n      with ops.device('/device:CPU:0'):\n        infeed_ops.append(\n            tpu_ops.infeed_enqueue_tuple(\n                shard_infeed_tensors[shard_id],\n                [spec.shape for spec in input_specs],\n                name='infeed-enqueue-%s-%d' % (execution_mode, shard_id),\n                device_ordinal=shard_id))\n    return SizedInfeed(infeed_ops=infeed_ops,\n                       sharded_infeed_tensors=shard_infeed_tensors)\n\n\nclass TPUFunction(object):\n  \"\"\"K.function compatible interface for invoking a TPU compiled function.\n\n  Recompilation is triggered on-demand for each set of new inputs shapes: the\n  results are cached for future execution.  We expect most computations will\n  be dominated by a standard batch-size, followed by a straggler batch for\n  the end of training or evaluation."
},
{
    "Id": 289,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f0bf6c5191d224f229808f4b321158d890a481e0",
    "Violation": "misleading",
    "Bug report": "Minor change for better error msg in eager input type checking ",
    "Number of deleted lines": 1,
    "Deleted lines": "      node_stats->set_all_start_micros(pre_time);\n      node_stats->set_op_end_rel_micros(Env::Default()->NowMicros() - pre_time);\n    }\n    if (!status.ok()) {\n      if (result_handle != nullptr) result_handle->Unref();\n      return errors::Internal(\"Failed copying input tensor from \",\n                              actual_device->name(), \" to \",\n                              expected_device->name(), \" in order to run \",\n                              op->Name(), \": \", status.error_message());\n    }\n\n    (*handle)->Unref();\n    *handle = result_handle;\n  }\n  return Status::OK();\n}\n\nStatus ValidateInputTypeAndPlacement(EagerContext* ctx, Device* op_device,\n                                     EagerOperation* op, const OpKernel* kernel,\n                                     RunMetadata* run_metadata) {\n  Device* host_device = ctx->HostCPU();\n  const MemoryTypeVector& memtypes = kernel->input_memory_types();\n  if (memtypes.size() != op->Inputs().size()) {\n    return errors::InvalidArgument(\"expected \", memtypes.size(),\n                                   \" inputs, got \", op->Inputs().size());\n  }\n  for (int i = 0; i < op->Inputs().size(); ++i) {\n    const Device* expected_device =\n        memtypes[i] == HOST_MEMORY ? host_device : op_device;\n    TF_RETURN_IF_ERROR(MaybeCopyInputToExpectedDevice(\n        op, i, expected_device, run_metadata, &((*op->MutableInputs())[i])));\n    tensorflow::TensorHandle* handle = op->Inputs()[i];\n    if (handle->dtype != kernel->input_type(i)) {\n      return errors::InvalidArgument(\n          \"cannot compute \", op->Name(), \" as input #\", i,\n          \" was expected to be a \", DataTypeString(kernel->input_type(i)),\n          \" tensor but is a \", DataTypeString(handle->dtype), \" tensor\");\n    }\n  }\n  return Status::OK();\n}\n\nStatus SelectDevice(const NodeDef& ndef, EagerContext* ctx, Device** device) {\n  DeviceSet ds;\n  for (Device* d : *ctx->devices()) {\n    ds.AddDevice(d);\n  }\n  DeviceTypeVector final_devices;\n  auto status = SupportedDeviceTypesForNode(ds.PrioritizedDeviceTypeList(),\n                                            ndef, &final_devices);\n  if (!status.ok()) return status;\n  if (final_devices.empty()) {\n    return errors::Internal(\"Could not find valid device for node \",\n                            ndef.DebugString());\n  }\n  for (Device* d : *ctx->devices()) {\n    if (d->device_type() == final_devices[0].type_string()) {\n      *device = d;\n      return Status::OK();\n    }\n  }\n  return errors::Unknown(\"Could not find a device for node \",\n                         ndef.DebugString());\n}\n\n#ifdef TENSORFLOW_EAGER_USE_XLA\n// Synthesizes and returns a wrapper function over `op`, which must be a\n// primitive op (e.g. matmul).\n//\n// The wrapper function conforms to the function signature expected by\n// XlaLaunch, with input params ordered by <constants, (variable) args and"
},
{
    "Id": 290,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/178d62a63ea043a4b9969b4cd6f8983eb8eae523",
    "Violation": "misleading",
    "Bug report": " Update check failure to logging a warning for repeated computation placer registration. This is to bypass a duplicated registration issue seen in open-source build during TF/PJRT integration.",
    "Number of deleted lines": 1,
    "Deleted lines": "    }\n    StrAppend(&output, \"\\n\");\n  }\n  return output;\n}\n\nStatusOr<int> ComputationPlacer::DeviceId(int replica, int computation,\n                                          int replica_count,\n                                          int computation_count) {\n  TF_RET_CHECK(replica < replica_count);\n  TF_RET_CHECK(computation < computation_count);\n\n  return computation * replica_count + replica;\n}\n\nStatusOr<DeviceAssignment> ComputationPlacer::AssignDevices(\n    int replica_count, int computation_count) {\n  DeviceAssignment assignment(replica_count, computation_count);\n  for (int replica = 0; replica < replica_count; ++replica) {\n    for (int computation = 0; computation < computation_count; ++computation) {\n      TF_ASSIGN_OR_RETURN(\n          int device_id,\n          DeviceId(replica, computation, replica_count, computation_count));\n      assignment(replica, computation) = device_id;\n    }\n  }\n  return std::move(assignment);\n}\n\n/* static */ void ComputationPlacer::RegisterComputationPlacer(\n    se::Platform::Id platform_id,\n    ComputationPlacerCreationFunction creation_function) {\n  absl::MutexLock lock(&ComputationPlacer::platform_computation_placer_mutex_);\n  auto* computation_placers = GetPlatformComputationPlacers();\n  CHECK(computation_placers->find(platform_id) == computation_placers->end());\n  (*computation_placers)[platform_id].creation_function = creation_function;\n}\n\n/* static */ StatusOr<ComputationPlacer*> ComputationPlacer::GetForPlatform(\n    const se::Platform* platform) {\n  absl::MutexLock lock(&ComputationPlacer::platform_computation_placer_mutex_);\n  auto* computation_placers = GetPlatformComputationPlacers();\n\n  auto it = computation_placers->find(platform->id());\n  if (it == computation_placers->end()) {\n    return NotFound(\n        \"could not find registered computation placer for platform %s -- check \"\n        \"target linkage\",\n        platform->Name());\n  }\n\n  if (it->second.placer == nullptr) {\n    // Lazily create the computation placer the first time it is needed.\n    it->second.placer = (*it->second.creation_function)();\n  }\n\n  return it->second.placer.get();\n}\n\n/* static */ absl::Mutex ComputationPlacer::platform_computation_placer_mutex_(\n    absl::kConstInit);\n\n/* static */ std::map<se::Platform::Id, ComputationPlacer::State>*\nComputationPlacer::GetPlatformComputationPlacers() {\n  static auto* r = new std::map<se::Platform::Id, ComputationPlacer::State>;\n  return r;\n}\n\n}  // namespace xla\n\nstatic std::unique_ptr<xla::ComputationPlacer> CreateComputationPlacer() {"
},
{
    "Id": 291,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6aece71ebf756d32ea730576a7ff12d2cfc7b242",
    "Violation": "insufficient",
    "Bug report": "Places relatively cheap type checks for list, tuple, and dict before other more expensive checks. Specifically, this avoids calling expensive checks like isinstance(structure, collections.abc.Mapping) and nest._is_named_tuple in the most common cases (since these abc isinstance checks take ~10x as long as normal isinstance checks).",
    "Number of deleted lines": 1,
    "Deleted lines": "    # Pack a CompositeTensor's components according to a TypeSpec.\n    assert len(args) == 1\n    return instance._from_components(args[0])  # pylint: disable=protected-access\n  elif isinstance(instance, _six.moves.range):\n    return _sequence_like(list(instance), args)\n  elif isinstance(instance, _wrapt.ObjectProxy):\n    # For object proxies, first create the underlying type and then re-wrap it\n    # in the proxy type.\n    return type(instance)(_sequence_like(instance.__wrapped__, args))\n  else:\n    # Not a namedtuple\n    return type(instance)(args)\n\n\ndef _yield_value(iterable):\n  for _, v in _yield_sorted_items(iterable):\n    yield v\n\n\ndef _yield_sorted_items(iterable):\n  \"\"\"Yield (key, value) pairs for `iterable` in a deterministic order.\n\n  For Sequences, the key will be an int, the array index of a value.\n  For Mappings, the key will be the dictionary key.\n  For objects (e.g. namedtuples), the key will be the attribute name.\n\n  In all cases, the keys will be iterated in sorted order.\n\n  Args:\n    iterable: an iterable.\n\n  Yields:\n    The iterable's (key, value) pairs, in order of sorted keys.\n  \"\"\"\n  if isinstance(iterable, _collections_abc.Mapping):\n    # Iterate through dictionaries in a deterministic order by sorting the\n    # keys. Notice this means that we ignore the original order of `OrderedDict`\n    # instances. This is intentional, to avoid potential bugs caused by mixing\n    # ordered and plain dicts (e.g., flattening a dict but using a\n    # corresponding `OrderedDict` to pack it back).\n    for key in _sorted(iterable):\n      yield key, iterable[key]\n  elif _is_attrs(iterable):\n    for item in _get_attrs_items(iterable):\n      yield item\n  elif _is_namedtuple(iterable):\n    for field in iterable._fields:\n      yield field, getattr(iterable, field)\n  elif _is_composite_tensor(iterable):\n    type_spec = iterable._type_spec  # pylint: disable=protected-access\n    yield type_spec.value_type.__name__, type_spec._to_components(iterable)  # pylint: disable=protected-access\n  elif _is_type_spec(iterable):\n    # Note: to allow CompositeTensors and their TypeSpecs to have matching\n    # structures, we need to use the same key string here.\n    yield iterable.value_type.__name__, iterable._component_specs  # pylint: disable=protected-access\n  else:\n    for item in enumerate(iterable):\n      yield item\n\n\n# See the swig file (util.i) for documentation.\nis_sequence = _pywrap_utils.IsSequence\n\n\n# See the swig file (util.i) for documentation.\nis_sequence_or_composite = _pywrap_utils.IsSequenceOrComposite\n\n\n@tf_export(\"nest.is_nested\")\ndef is_nested(seq):\n  \"\"\"Returns true if its input is a collections.abc.Sequence (except strings)."
},
{
    "Id": 292,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9",
    "Violation": "missing",
    "Bug report": " Validate real and expected type of arguments to cwise ops. Without this validation, it is possible to trigger a `CHECK`-fail denial of service.",
    "Number of deleted lines": 0,
    "Deleted lines": "\n    const Tensor& in0;\n    const Tensor& in1;\n\n    BCast bcast;\n    Tensor* out = nullptr;\n    int64_t out_num_elements;\n\n    int64_t in0_num_elements;\n    int64_t in1_num_elements;\n\n    int ndims;\n    bool result;\n  };\n\n  void SetUnimplementedError(OpKernelContext* ctx);\n  void SetComputeError(OpKernelContext* ctx);\n};\n\n// Coefficient-wise binary operations:\n//   Device: E.g., CPUDevice, GPUDevice.\n//   Functor: defined in cwise_ops.h. E.g., functor::add.\ntemplate <typename Device, typename Functor>\nclass BinaryOp : public BinaryOpShared {\n public:\n  typedef typename Functor::in_type Tin;    // Input scalar data type.\n  typedef typename Functor::out_type Tout;  // Output scalar data type.\n\n  explicit BinaryOp(OpKernelConstruction* ctx)\n      : BinaryOpShared(ctx, DataTypeToEnum<Tout>::v(),\n                       DataTypeToEnum<Tin>::v()) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input_0 = ctx->input(0);\n    const Tensor& input_1 = ctx->input(1);\n    const Device& eigen_device = ctx->eigen_device<Device>();\n    bool error = false;\n    bool* const error_ptr = Functor::has_errors ? &error : nullptr;\n\n    // NOTE: Handle three simple cases before building the BinaryOpState, which\n    // is relatively expensive for small operations.\n    if (input_0.shape() == input_1.shape()) {\n      // tensor op tensor with no broadcasting.\n      Tensor* out;\n      OP_REQUIRES_OK(ctx, ctx->forward_input_or_allocate_output(\n                              {0, 1}, 0, input_0.shape(), &out));\n      functor::BinaryFunctor<Device, Functor, 1>()(\n          eigen_device, out->template flat<Tout>(),\n          input_0.template flat<Tin>(), input_1.template flat<Tin>(),\n          error_ptr);\n      if (Functor::has_errors && error) {\n        SetComputeError(ctx);\n      }\n      return;\n    } else if (input_0.shape().dims() == 0) {\n      // scalar op tensor.\n      Tensor* out;\n      OP_REQUIRES_OK(ctx, ctx->forward_input_or_allocate_output(\n                              {1}, 0, input_1.shape(), &out));\n\n      functor::BinaryFunctor<Device, Functor, 1>().Left(\n          eigen_device, out->template flat<Tout>(),\n          input_0.template scalar<Tin>(), input_1.template flat<Tin>(),\n          error_ptr);\n      if (Functor::has_errors && error) {\n        SetComputeError(ctx);\n      }\n      return;\n    } else if (input_1.shape().dims() == 0) {\n      // tensor op scalar.\n      Tensor* out;"
},
{
    "Id": 293,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/798b2ebda0cc6f12f1ca6460611f760149771a11",
    "Violation": "missing",
    "Bug report": " Ensure the allocation type is kTfLiteCustom when doing shallow copies in DeepOrShallowCopyTensorsShapeTypeData.  This code is correct only under the assumption that the caller has correctly prepared the tensors that get passed in for shallow copying, by setting their allocation types to kTfLiteCustom. This ensures that those tensors won't be double `free`'d later on. This check simply ensures that that assumption always holds, to ensure we fail early if ever a bug is introduced that breaks that assumption.",
    "Number of deleted lines": 0,
    "Deleted lines": "    const SrcVector& src_tensor_indices, Subgraph* dst_subgraph,\n    const DstVector& dst_tensor_indices) {\n  // Resize the destination subgraph inputs.\n  for (int i = 0; i < src_tensor_indices.size(); ++i) {\n    // Skip copying unused destination tensors.\n    if (dst_tensor_indices[i] == kTfLiteOptionalTensor) continue;\n    if (src_tensor_indices[i] == kTfLiteOptionalTensor) continue;\n\n    const TfLiteTensor* src_tensor =\n        src_subgraph->tensor(src_tensor_indices[i]);\n    TfLiteTensor* dst_tensor = dst_subgraph->tensor(dst_tensor_indices[i]);\n    std::vector<int> dims(src_tensor->dims->data,\n                          src_tensor->dims->data + src_tensor->dims->size);\n    dst_subgraph->ResizeInputTensor(dst_tensor_indices[i], dims);\n    dst_tensor->type = src_tensor->type;\n    if (!IsResourceOrVariant(src_tensor)) {\n      dst_tensor->bytes = 0;  // Don't allocate memory with AllocateTensors().\n      dst_tensor->data.raw = nullptr;\n    }\n  }\n  TF_LITE_ENSURE_OK(context, dst_subgraph->AllocateTensors());\n  // Deep or shallow copy the data from src subgraph to dst.\n  for (int i = 0; i < src_tensor_indices.size(); ++i) {\n    // Skip copying unused destination tensors.\n    if (dst_tensor_indices[i] == kTfLiteOptionalTensor) continue;\n    if (src_tensor_indices[i] == kTfLiteOptionalTensor) continue;\n\n    const TfLiteTensor* src_tensor =\n        src_subgraph->tensor(src_tensor_indices[i]);\n    TfLiteTensor* dst_tensor = dst_subgraph->tensor(dst_tensor_indices[i]);\n    if (IsResourceOrVariant(src_tensor)) {\n      TfLiteTensorRealloc(src_tensor->bytes, dst_tensor);\n      TF_LITE_ENSURE_OK(context, TfLiteTensorCopy(src_tensor, dst_tensor));\n    } else {\n      dst_tensor->bytes = src_tensor->bytes;\n      dst_tensor->data.raw = src_tensor->data.raw;\n    }\n  }\n  return kTfLiteOk;\n}\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n\n#endif  // TENSORFLOW_LITE_KERNELS_CONTROL_FLOW_COMMON_H_\n"
},
{
    "Id": 294,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b65d9ec2b78c7c23e368ed4eec7b4deb89dcd712",
    "Violation": "insufficient",
    "Bug report": "Fix value error generated on is_scalar check. Fix value error generated on is_scalar check. `is_scalar = shape is not None and not shape` raises a value error when shape is a scalar, \"ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\"",
    "Number of deleted lines": 1,
    "Deleted lines": "\n    Returns:\n      The created or existing `Variable` (or `PartitionedVariable`, if a\n      partitioner was used).\n\n    Raises:\n      ValueError: when creating a new variable and shape is not declared,\n        when reusing a variable and specifying a conflicting shape,\n        or when violating reuse during variable creation.\n    \"\"\"\n    if custom_getter is not None and not callable(custom_getter):\n      raise ValueError(\n          \"Passed a custom_getter which is not callable: %s\" % custom_getter)\n\n    # If a *_ref type is passed in an error would be triggered further down the\n    # stack. We prevent this using base_dtype to get a non-ref version of the\n    # type, before doing anything else. When _ref types are removed in favor of\n    # resources, this line can be removed.\n    try:\n      dtype = dtype.base_dtype\n    except AttributeError:\n      # .base_dtype not existing means that we will try and use the raw dtype\n      # which was passed in - this might be a NumPy type which is valid.\n      pass\n\n    # This is the main logic of get_variable.  However, custom_getter\n    # may override this logic.  So we save it as a callable and pass\n    # it to custom_getter.\n    # Note: the parameters of _true_getter, and their documentation, match\n    # *exactly* item-for-item with the docstring of this method.\n    def _true_getter(name, shape=None, dtype=dtypes.float32,  # pylint: disable=missing-docstring\n                     initializer=None, regularizer=None, reuse=None,\n                     trainable=True, collections=None, caching_device=None,\n                     partitioner=None, validate_shape=True, use_resource=None):\n      is_scalar = shape is not None and not shape\n      # Partitioned variable case\n      if partitioner is not None and not is_scalar:\n        if not callable(partitioner):\n          raise ValueError(\n              \"Partitioner must be callable, but received: %s\" % partitioner)\n        with ops.name_scope(None):\n          return self._get_partitioned_variable(name=name,\n                                                shape=shape,\n                                                dtype=dtype,\n                                                initializer=initializer,\n                                                regularizer=regularizer,\n                                                reuse=reuse,\n                                                trainable=trainable,\n                                                collections=collections,\n                                                caching_device=caching_device,\n                                                partitioner=partitioner,\n                                                validate_shape=validate_shape,\n                                                use_resource=use_resource)\n\n      # Special case for partitioned variable to allow reuse without having to\n      # specify partitioner.\n      if (reuse is True and partitioner is None\n          and name in self._partitioned_vars):\n        return self._get_partitioned_variable(name=name,\n                                              shape=shape,\n                                              dtype=dtype,\n                                              initializer=initializer,\n                                              regularizer=regularizer,\n                                              reuse=reuse,\n                                              trainable=trainable,\n                                              collections=collections,\n                                              caching_device=caching_device,\n                                              partitioner=None,\n                                              validate_shape=validate_shape,\n                                              use_resource=use_resource)\n"
},
{
    "Id": 295,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9baa064387b0a114c3fcec88abaa0568834e8e34",
    "Violation": "insufficient",
    "Bug report": " Only apply check for non-tensor case ",
    "Number of deleted lines": 3,
    "Deleted lines": "                                   #  [0, 0, 0, 0, 0, 0, 0]]\n\n  tf.pad(t, paddings, \"REFLECT\")  # [[6, 5, 4, 5, 6, 5, 4],\n                                  #  [3, 2, 1, 2, 3, 2, 1],\n                                  #  [6, 5, 4, 5, 6, 5, 4],\n                                  #  [3, 2, 1, 2, 3, 2, 1]]\n\n  tf.pad(t, paddings, \"SYMMETRIC\")  # [[2, 1, 1, 2, 3, 3, 2],\n                                    #  [2, 1, 1, 2, 3, 3, 2],\n                                    #  [5, 4, 4, 5, 6, 6, 5],\n                                    #  [5, 4, 4, 5, 6, 6, 5]]\n  ```\n\n  Args:\n    tensor: A `Tensor`.\n    paddings: A `Tensor` of type `int32`.\n    mode: One of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\" (case-insensitive)\n    name: A name for the operation (optional).\n    constant_values: In \"CONSTANT\" mode, the scalar pad value to use. Must be\n      same type as `tensor`.\n\n  Returns:\n    A `Tensor`. Has the same type as `tensor`.\n\n  Raises:\n    ValueError: When mode is not one of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\".\n  \"\"\"\n\n  # Convert lower/mixed case to upper for NumPy compatibility\n  # NumPy uses all lower-case modes.\n  mode = mode.upper()\n  if mode == \"CONSTANT\":\n    # TODO(rjryan): Once the forward compatibility period (3 weeks) have passed\n    # remove the \"Pad\" fallback here.\n    if constant_values != 0:\n      result = gen_array_ops.pad_v2(\n          tensor, paddings, constant_values, name=name)\n    else:\n      result = gen_array_ops.pad(tensor, paddings, name=name)\n  elif mode == \"REFLECT\":\n    result = gen_array_ops.mirror_pad(\n        tensor, paddings, mode=\"REFLECT\", name=name)\n  elif mode == \"SYMMETRIC\":\n    result = gen_array_ops.mirror_pad(\n        tensor, paddings, mode=\"SYMMETRIC\", name=name)\n  else:\n    raise ValueError(\"Unknown padding mode: %s\" % mode)\n\n  # Restore shape information where possible.\n  if not context.executing_eagerly():\n    paddings_constant = tensor_util.constant_value(\n        result.op.inputs[1], partial=True)\n    input_shape = result.op.inputs[0].shape\n    if (input_shape.ndims is not None and\n        not result.shape.is_fully_defined() and paddings_constant is not None):\n      new_shape = []\n      for padding, dim in zip(paddings_constant, input_shape.as_list()):\n        if padding is None or dim is None or any((x is None for x in padding)):\n          new_shape.append(None)\n        else:\n          new_shape.append(sum(padding) + dim)\n      result.set_shape(new_shape)\n\n  return result\n\n\n@tf_export(\"meshgrid\")\ndef meshgrid(*args, **kwargs):\n  \"\"\"Broadcasts parameters for evaluation on an N-D grid.\n\n  Given N one-dimensional coordinate arrays `*args`, returns a list `outputs`\n  of N-D coordinate arrays for evaluating expressions on an N-D grid.\n\n  Notes:\n"
},
{
    "Id": 296,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cb2828a844ccaf0394e602d15fd95e45073729a2",
    "Violation": "missing",
    "Bug report": " Check that the type of an implicitly dereferenced tensor matches the expected input type. The dtype of a tensor reference can change between the point when it is \"produced\" by an operation and consumed by the next operation. This evades checks in the executor that the type of tensor on each edge matches the type signatures of the producing and consuming operation, which could lead to undefined behavior. Although there is no existing operation that changes the type of a tensor reference, it is possible to use the OpKernelContext API to do so, so we add a further check in the runtime to defend against operations that might be added in the future.",
    "Number of deleted lines": 0,
    "Deleted lines": "    if (entry->ref == nullptr) {\n      if (expect_ref) {\n        return AttachDef(\n            errors::InvalidArgument(i, \"-th input expects a ref type\"),\n            item.kernel->def());\n      }\n      inp->tensor = entry->val.get();\n    } else {\n      {\n        mutex_lock ml(*entry->ref_mu);\n        if (!entry->ref->IsInitialized() && !IsInitializationOp(item.node)) {\n          return AttachDef(errors::FailedPrecondition(\n                               \"Attempting to use uninitialized value \",\n                               item.kernel->requested_input(i)),\n                           item.kernel->def());\n        }\n      }\n      if (expect_ref) {\n        inp->mutex_if_ref = entry->ref_mu;\n        inp->tensor = entry->ref;\n      } else {\n        // Automatically deref the tensor ref when the op expects a\n        // tensor but is given a ref to a tensor.  Need to deref it\n        // under the mutex.\n        {\n          mutex_lock l(*(entry->ref_mu));\n          DCHECK(!entry->val_field_is_set);\n          entry->val.Init(*entry->ref);\n          entry->val_field_is_set = true;\n        }\n        entry->ref = nullptr;\n        entry->ref_mu = nullptr;\n\n        inp->tensor = entry->val.get();\n      }\n    }\n  }\n  return Status::OK();\n}\n\nStatus ExecutorState::ProcessOutputs(const NodeItem& item, OpKernelContext* ctx,\n                                     EntryVector* outputs,\n                                     NodeExecStatsWrapper* stats) {\n  const Node* node = item.node;\n  DCHECK_EQ(0, outputs->size());\n  outputs->resize(item.num_outputs);\n\n  Status s = ctx->status();\n  if (!s.ok()) {\n    s = AttachDef(s, item.kernel->def());\n    // TODO(misard) Replace with a finer-grain enabling flag once we\n    // add better optional debugging support.\n    if (vlog_ && VLOG_IS_ON(1)) {\n      LOG(WARNING) << this << \" Compute status: \" << s;\n      DumpState();\n    }\n    if (s.code() == error::RESOURCE_EXHAUSTED) {\n      if (stats_collector_) {\n        string err = stats_collector_->ReportAllocsOnResourceExhausted(\n            s.error_message());\n        s = Status(s.code(), strings::StrCat(s.error_message(), err));\n      } else {\n        s = Status(\n            s.code(),\n            strings::StrCat(\n                s.error_message(),\n                \"\\nHint: If you want to see a list of allocated tensors when \"\n                \"OOM happens, add report_tensor_allocations_upon_oom \"\n                \"to RunOptions for current allocation info.\\n\"));\n      }"
},
{
    "Id": 297,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/924f80a4fdb34230965a7a8a4476901847463645",
    "Violation": "missing",
    "Bug report": "Add stricter type checking for tf.math.real. Fix for tf.math.real so that it only accepts tensors with numeric entries as input.",
    "Number of deleted lines": 1,
    "Deleted lines": "  return gen_math_ops.sign(x, name=name)\n\n\n@tf_export(\"math.real\", v1=[\"math.real\", \"real\"])\n@dispatch.register_unary_elementwise_api\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints(\"real\")\ndef real(input, name=None):\n  r\"\"\"Returns the real part of a complex (or real) tensor.\n\n  Given a tensor `input`, this operation returns a tensor of type `float` that\n  is the real part of each element in `input` considered as a complex number.\n\n  For example:\n\n  ```python\n  x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])\n  tf.math.real(x)  # [-2.25, 3.25]\n  ```\n\n  If `input` is already real, it is returned unchanged.\n\n  Args:\n    input: A `Tensor`. Must have numeric type.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `float32` or `float64`.\n  \"\"\"\n  with ops.name_scope(name, \"Real\", [input]) as name:\n    input = ops.convert_to_tensor(input, name=\"input\")\n    if input.dtype.is_complex:\n      real_dtype = input.dtype.real_dtype\n      return gen_math_ops.real(input, Tout=real_dtype, name=name)\n    else:\n      return input\n\n\n@tf_export(\"math.imag\", v1=[\"math.imag\", \"imag\"])\n@dispatch.register_unary_elementwise_api\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints(\"imag\")\ndef imag(input, name=None):\n  r\"\"\"Returns the imaginary part of a complex (or real) tensor.\n\n  Given a tensor `input`, this operation returns a tensor of type `float` that\n  is the imaginary part of each element in `input` considered as a complex\n  number. If `input` is real, a tensor of all zeros is returned.\n\n  For example:\n\n  ```python\n  x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])\n  tf.math.imag(x)  # [4.75, 5.75]\n  ```\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `float`, `double`,\n      `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `float32` or `float64`.\n  \"\"\"\n  with ops.name_scope(name, \"Imag\", [input]) as name:\n    input = ops.convert_to_tensor(input, name=\"input\")\n    if input.dtype.is_complex:\n      return gen_math_ops.imag(input, Tout=input.dtype.real_dtype, name=name)\n    else:\n      return array_ops.zeros_like(input)\n\n"
},
{
    "Id": 298,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e6df768b81e973f2123bc83a18a60773fc4da99e",
    "Violation": "insufficient",
    "Bug report": "[TFG] Fix IsAdd string type check in tf_op_names ",
    "Number of deleted lines": 1,
    "Deleted lines": ""
},
{
    "Id": 299,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4f4a0276a2cf9186c0541072964676159368286e",
    "Violation": "missing",
    "Bug report": " Add appropriate PyObject type check for bool. This PR fixes an issue where PyObject type in tf's C bindings does not check if an input is a boolean and will always cast to bool.",
    "Number of deleted lines": 2,
    "Deleted lines": "    return true;\n  }\n\n  return ParseInt64Value(key, dimension_value.get(), status, value);\n}\n\nbool ParseStringValue(const string& key, PyObject* py_value, TF_Status* status,\n                      tensorflow::StringPiece* value) {\n  if (PyBytes_Check(py_value)) {\n    Py_ssize_t size = 0;\n    char* buf = nullptr;\n    if (PyBytes_AsStringAndSize(py_value, &buf, &size) < 0) return false;\n    *value = tensorflow::StringPiece(buf, size);\n    return true;\n  }\n#if PY_MAJOR_VERSION >= 3\n  if (PyUnicode_Check(py_value)) {\n    Py_ssize_t size = 0;\n    const char* buf = PyUnicode_AsUTF8AndSize(py_value, &size);\n    if (buf == nullptr) return false;\n    *value = tensorflow::StringPiece(buf, size);\n    return true;\n  }\n#endif\n  TF_SetStatus(\n      status, TF_INVALID_ARGUMENT,\n      tensorflow::strings::StrCat(\"Expecting a string value for attr \", key,\n                                  \", got \", py_value->ob_type->tp_name)\n          .c_str());\n  return false;\n}\n\nbool ParseBoolValue(const string& key, PyObject* py_value, TF_Status* status,\n                    unsigned char* value) {\n  *value = PyObject_IsTrue(py_value);\n  return true;\n}\n\n// The passed in py_value is expected to be an object of the python type\n// dtypes.DType or an int.\nbool ParseTypeValue(const string& key, PyObject* py_value, TF_Status* status,\n                    int* value) {\n  if (IsInteger(py_value)) {\n    return ParseIntValue(key, py_value, status, value);\n  }\n\n  tensorflow::Safe_PyObjectPtr py_type_enum(\n      PyObject_GetAttrString(py_value, \"_type_enum\"));\n  if (py_type_enum == nullptr) {\n    PyErr_Clear();\n    TF_SetStatus(\n        status, TF_INVALID_ARGUMENT,\n        tensorflow::strings::StrCat(\"Expecting a DType.dtype for attr \", key,\n                                    \", got \", py_value->ob_type->tp_name)\n            .c_str());\n    return false;\n  }\n\n  return ParseIntValue(key, py_type_enum.get(), status, value);\n}\n\nbool SetOpAttrList(TFE_Context* ctx, TFE_Op* op, const char* key,\n                   PyObject* py_list, TF_AttrType type,\n                   tensorflow::gtl::FlatMap<string, int64_t>* attr_list_sizes,\n                   TF_Status* status) {\n  if (!PySequence_Check(py_list)) {\n    TF_SetStatus(\n        status, TF_INVALID_ARGUMENT,\n        tensorflow::strings::StrCat(\"Expecting sequence value for attr \", key,\n                                    \", got \", py_list->ob_type->tp_name)\n            .c_str());\n    return false;"
},
{
    "Id": 300,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/88609e2e22fa5c296de2e27e04d1cc4743b2dfcd",
    "Violation": "missing",
    "Bug report": " Add appropriate dtype check for tf.boolean_mask's mask. This PR tries to address the issue raised in 54412 where mask's dtype was checked in tf.boolean_mask and an invalid result has been returned instead.",
    "Number of deleted lines": 0,
    "Deleted lines": "  tensor = [0, 1, 2, 3]\n  mask = np.array([True, False, True, False])\n  tf.boolean_mask(tensor, mask)  # [0, 2]\n\n  # 2-D example\n  tensor = [[1, 2], [3, 4], [5, 6]]\n  mask = np.array([True, False, True])\n  tf.boolean_mask(tensor, mask)  # [[1, 2], [5, 6]]\n  ```\n\n  Args:\n    tensor:  N-D Tensor.\n    mask:  K-D boolean Tensor, K <= N and K must be known statically.\n    name:  A name for this operation (optional).\n    axis:  A 0-D int Tensor representing the axis in `tensor` to mask from. By\n      default, axis is 0 which will mask from the first dimension. Otherwise K +\n      axis <= N.\n\n  Returns:\n    (N-K+1)-dimensional tensor populated by entries in `tensor` corresponding\n    to `True` values in `mask`.\n\n  Raises:\n    ValueError:  If shapes do not conform.\n  \"\"\"\n\n  def _apply_mask_1d(reshaped_tensor, mask, axis=None):\n    \"\"\"Mask tensor along dimension 0 with a 1-D mask.\"\"\"\n    indices = squeeze(where_v2(mask), axis=[1])\n    return gather(reshaped_tensor, indices, axis=axis)\n\n  with ops.name_scope(name, values=[tensor, mask]):\n    tensor = ops.convert_to_tensor(tensor, name=\"tensor\")\n    mask = ops.convert_to_tensor(mask, name=\"mask\")\n\n    shape_mask = mask.get_shape()\n    ndims_mask = shape_mask.ndims\n    shape_tensor = tensor.get_shape()\n    if ndims_mask == 0:\n      raise ValueError(\"mask cannot be scalar.\")\n    if ndims_mask is None:\n      raise ValueError(\n          \"Number of mask dimensions must be specified, even if some dimensions\"\n          \" are None.  E.g. shape=[None] is ok, but shape=None is not.\")\n    axis = 0 if axis is None else axis\n    axis_value = tensor_util.constant_value(axis)\n    if axis_value is not None:\n      axis = axis_value\n      shape_tensor[axis:axis + ndims_mask].assert_is_compatible_with(shape_mask)\n\n    leading_size = gen_math_ops.prod(shape(tensor)[axis:axis + ndims_mask], [0])\n    tensor = reshape(\n        tensor,\n        concat([\n            shape(tensor)[:axis], [leading_size],\n            shape(tensor)[axis + ndims_mask:]\n        ], 0))\n    # TODO(yongtang): tf.reshape in C++ kernel might have set the shape\n    # correctly, so the following may not be needed? It still might be possible\n    # that there are some edge case where tensor_util.constant_value resolves\n    # more cases than ShapeInference of tf.reshape in C++ kernel.\n    if axis_value is not None:\n      first_dim = shape_tensor[axis:axis + ndims_mask].num_elements()\n      tensor.set_shape(\n          tensor_shape.as_shape(shape_tensor[:axis]).concatenate(\n              [first_dim]).concatenate(shape_tensor[axis + ndims_mask:]))\n\n    mask = reshape(mask, [-1])\n    return _apply_mask_1d(tensor, mask, axis)\n"
},
{
    "Id": 301,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a76646d4b4ad5d56b5e63c139985bbd1eb98dd90",
    "Violation": "misleading",
    "Bug report": " Add type checking at the beginning of tpu.shard(). Otherwise a message like \"TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.\" will be thrown, which is confusing.",
    "Number of deleted lines": 1,
    "Deleted lines": "      which must have size divisible by `num_shards`.\n    num_shards: The number of shards.\n    input_shard_axes: A list of dimensions along which to shard `inputs`, or\n      `None`. `None` means \"shard all inputs along dimension 0\". If not `None`,\n      there must be one dimension per input.\n    outputs_from_all_shards: Boolean or list of boolean. For each output, if\n      `True`, outputs from all shards are concatenated along the corresponding\n      `output_shard_axes` entry. Otherwise, each output is taken\n      from an arbitrary shard. If the argument is a boolean, the argument's\n      value is used for each output.\n    output_shard_axes: A list of dimensions along which to concatenate the\n      outputs of `computation`, or `None`. `None` means \"concatenate all outputs\n      along dimension 0\". If not `None`, there must be one dimension per output.\n      Ignored if `outputs_from_all_shards` is False.\n    infeed_queue: If not `None`, the `InfeedQueue` to use to augment the inputs\n      of `computation`.\n    device_assignment: If not `None`, a `DeviceAssignment` describing the\n      mapping between logical cores in the computation with physical cores in\n      the TPU topology. Uses a default device assignment if `None`. The\n      `DeviceAssignment` may be omitted if each shard of the computation uses\n      only one core, and there is either only one shard, or the number of shards\n      is equal to the number of cores in the TPU system.\n    name: (Deprecated) Does nothing.\n  Returns:\n    A list of output tensors.\n  Raises:\n    ValueError: If num_shards <= 0\n    ValueError: If len(input_shard_axes) != len(inputs)\n    ValueError: If len(output_shard_axes) != len(outputs from `computation`)\n  \"\"\"\n\n  if num_shards <= 0:\n    raise ValueError(\"num_shards must be a positive integer.\")\n\n  # Converts inputs to Tensors.\n  inputs = [] if inputs is None else [ops.convert_to_tensor(x) for x in inputs]\n\n  if input_shard_axes is None:\n    input_shard_axes = [0] * len(inputs)\n  if len(inputs) != len(input_shard_axes):\n    raise ValueError(\"Length of input_shard_axes must be equal to the number \"\n                     \"of inputs.\")\n\n  if inputs:\n    # Splits the `inputs` along the corresponding `input_shard_axes`, giving\n    # lists with layout [input][shard]\n    split_inputs = [\n        array_ops.split(x, num_shards, axis=axis)\n        for (axis, x) in zip(input_shard_axes, inputs)]\n\n    # Transposes the input lists to have layout [shard][input]\n    transposed_inputs = [list(i) for i in zip(*split_inputs)]\n  else:\n    transposed_inputs = [[]] * num_shards\n\n  outputs = replicate(\n      computation,\n      transposed_inputs,\n      infeed_queue=infeed_queue,\n      device_assignment=device_assignment,\n      name=name)\n\n  # There must be at least one shard since num_shards > 0.\n  # TODO(b/36647078) remove disable when pylint bug is fixed.\n  # pylint: disable=indexing-exception\n  if isinstance(outputs[0], ops.Operation):\n    # pylint: enable=indexing-exception\n    # There were no outputs from the computation and replicate returned a list\n    # of NoOps with control dependencies on the computation. Return the first\n    # one so it can be used as a control dependency or fetch node.\n    # TODO(b/36647078) remove disable when pylint bug is fixed.\n    # pylint: disable=indexing-exception"
},
{
    "Id": 302,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1c49c13ba59961cf7581e3e29b951db8faca94f5",
    "Violation": "missing",
    "Bug report": "Add type check for reduction axis in reducer operation. ",
    "Number of deleted lines": 0,
    "Deleted lines": "  resolved_axis->type = kTfLiteInt32;\n  // Creates a temp tensor to store temp sums when calculating mean.\n  node->temporaries->data[2] = op_data->scratch_tensor_index + 2;\n  TfLiteTensor* temp_sum = GetTemporary(context, node, /*index=*/2);\n  switch (op_context->input->type) {\n    case kTfLiteFloat32:\n      temp_sum->type = kTfLiteFloat32;\n      break;\n    case kTfLiteInt32:\n      temp_sum->type = kTfLiteInt64;\n      break;\n    case kTfLiteInt64:\n      temp_sum->type = kTfLiteInt64;\n      break;\n    case kTfLiteUInt8:\n      temp_sum->type = kTfLiteInt32;\n      break;\n    case kTfLiteInt8:\n      temp_sum->type = kTfLiteInt32;\n      break;\n    case kTfLiteBool:\n      temp_sum->type = kTfLiteBool;\n      break;\n    default:\n      return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\nTfLiteStatus PrepareSimple(TfLiteContext* context, TfLiteNode* node) {\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  OpContext op_context(context, node);\n  TF_LITE_ENSURE_OK(context, InitializeTemporaries(context, node, &op_context));\n\n  TfLiteTensor* resolved_axis = GetTemporary(context, node, /*index=*/1);\n  // Leaves work to Eval if axis is not constant; else resizes output.\n  if (!IsConstantTensor(op_context.axis)) {\n    SetTensorToDynamic(op_context.output);\n    SetTensorToDynamic(resolved_axis);\n    return kTfLiteOk;\n  }\n  resolved_axis->allocation_type = kTfLiteArenaRw;\n  TF_LITE_ENSURE_OK(context,\n                    ResizeTempAxis(context, &op_context, resolved_axis));\n  TF_LITE_ENSURE_OK(context, ResizeOutputTensor(context, &op_context));\n  return kTfLiteOk;\n}\n\nTfLiteStatus PrepareAny(TfLiteContext* context, TfLiteNode* node) {\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\n  const TfLiteTensor* input = GetInput(context, node, 0);\n  TF_LITE_ENSURE_EQ(context, input->type, kTfLiteBool);\n  return PrepareSimple(context, node);\n}\n\nTfLiteStatus PrepareMeanOrSum(TfLiteContext* context, TfLiteNode* node) {\n  TF_LITE_ENSURE_OK(context, PrepareSimple(context, node));\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  // reduce_mean requires a buffer to store intermediate sum result.\n  OpContext op_context(context, node);\n  if (op_context.input->type == kTfLiteInt8) {\n    const double real_multiplier =\n        static_cast<double>(op_context.input->params.scale) /\n        static_cast<double>(op_context.output->params.scale);\n    int exponent;\n    QuantizeMultiplier(real_multiplier, &data->multiplier, &exponent);\n    data->shift = exponent;"
},
{
    "Id": 303,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b6f3366a716ca9b5a1e6114a3bea050c80d8a475",
    "Violation": "missing",
    "Bug report": " Don't check for if null after already dereferenced. I'm not sure how it could be null at this point (and obviously it is nowhere else we'd have seen failures), but keeping the check as is and just moving it to where it would catch it before dereferencing.",
    "Number of deleted lines": 3,
    "Deleted lines": "#include \"tensorflow/core/graph/control_flow.h\"\n#include \"tensorflow/core/graph/graph.h\"\n\nnamespace tensorflow {\n\nStatus FunctionDefToBodyHelper(\n    const FunctionDef& fdef, const AttrSlice& attrs,\n    const FunctionLibraryDefinition* const lib_def,\n    const std::function<Status(const string&, const OpDef**)>& get_func_sig,\n    std::unique_ptr<FunctionBody>* fbody) {\n  // Instantiates the function template into a graph def.\n  InstantiationResult result;\n  TF_RETURN_IF_ERROR(InstantiateFunction(fdef, attrs, get_func_sig, &result));\n\n  auto graph = absl::make_unique<Graph>(lib_def);\n\n  auto construction_context_iter = fdef.attr().find(\"_construction_context\");\n  if (construction_context_iter != fdef.attr().end()) {\n    if (construction_context_iter->second.s() == \"kEagerRuntime\") {\n      graph->SetConstructionContext(ConstructionContext::kEagerRuntime);\n    } else {\n      DCHECK(false) << \"Unknown _construction_context attribute: \"\n                    << construction_context_iter->second.s();\n    }\n  }\n\n  GraphConstructorOptions opts;\n  opts.allow_internal_ops = true;\n  opts.expect_device_spec = false;\n  TF_RETURN_IF_ERROR(ConvertNodeDefsToGraph(opts, result.nodes, graph.get()));\n\n  const StackTracesMap& stack_traces =\n      lib_def->GetStackTraces(fdef.signature().name());\n  for (Node* n : graph->nodes()) {\n    auto it = stack_traces.find(n->name());\n    if (n && it != stack_traces.end()) {\n      n->SetStackTrace(it->second);\n    }\n  }\n\n  // Call BuildControlFlowInfo to validate that this function body has\n  // well-formed control flow.\n  std::vector<ControlFlowInfo> dummy;\n  TF_RETURN_IF_ERROR(BuildControlFlowInfo(graph.get(), &dummy));\n\n  *fbody = absl::make_unique<FunctionBody>(fdef, result.arg_types,\n                                           result.ret_types, graph.release());\n  return Status::OK();\n}\n\nStatus FunctionDefToBodyHelper(const FunctionDef& fdef, const AttrSlice& attrs,\n                               const FunctionLibraryDefinition* lib_def,\n                               std::unique_ptr<FunctionBody>* fbody) {\n  const auto get_func_sig = [&lib_def](const string& op, const OpDef** sig) {\n    return lib_def->LookUpOpDef(op, sig);\n  };\n  return FunctionDefToBodyHelper(fdef, attrs, lib_def, get_func_sig, fbody);\n}\n\n}  // end namespace tensorflow\n"
},
{
    "Id": 304,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/89fa1ae2cb34eab0e6137e72e6fab01f6c5bc164",
    "Violation": "improper",
    "Bug report": "Fix check for cloning FunctionLibraryRuntime",
    "Number of deleted lines": 1,
    "Deleted lines": "    run_opts.runner = &default_runner_;\n  }\n  DCHECK(run_opts.runner != nullptr);\n\n  Executor::Args exec_args;\n  ExecutorArgsFromOptions(run_opts, frame, &exec_args);\n  item->exec->RunAsync(exec_args, std::move(done));\n}\n\nbool FunctionLibraryRuntimeImpl::IsStateful(const string& func) const {\n  const OpDef* op_def;\n  const Status s = base_lib_def_->LookUpOpDef(func, &op_def);\n  return s.ok() && op_def->is_stateful();\n}\n\nstring FunctionLibraryRuntimeImpl::DebugString(Handle handle) {\n  Item* item = nullptr;\n  LocalHandle local_handle = parent_->GetHandleOnDevice(device_name_, handle);\n  Status s = GetOrCreateItem(local_handle, &item);\n  if (s.ok()) {\n    return tensorflow::DebugString(item->graph);\n  } else {\n    return s.ToString();\n  }\n}\n\nStatus FunctionLibraryRuntimeImpl::Clone(\n    std::unique_ptr<FunctionLibraryDefinition>* out_lib_def,\n    std::unique_ptr<ProcessFunctionLibraryRuntime>* out_pflr,\n    FunctionLibraryRuntime** out_flr, bool skip_flib_def) {\n  TF_RETURN_IF_ERROR(parent_->Clone(\n      env_, graph_def_version_, optimizer_.options(), custom_kernel_creator_,\n      out_lib_def, out_pflr, skip_flib_def));\n  *out_flr = (*out_pflr)->GetFLR(device_->name());\n  if (out_flr != nullptr) {\n    return Status::OK();\n  } else {\n    return errors::Internal(\"Cloning FunctionLibraryRuntime failed.\");\n  }\n}\n\nnamespace {\n\nstruct CustomCreatorSingleton {\n  mutex mu;\n  CustomKernelCreator* custom_creator = nullptr;\n\n  void Set(CustomKernelCreator* cb) {\n    mutex_lock l(mu);\n    custom_creator = cb;\n  }\n\n  CustomKernelCreator* Get() {\n    mutex_lock l(mu);\n    return custom_creator;\n  }\n};\n\nCustomCreatorSingleton* GetCustomCreatorSingleton() {\n  static CustomCreatorSingleton* ccs = new CustomCreatorSingleton;\n  return ccs;\n}\n\n}  // namespace\n\nconst CustomKernelCreator* GetDefaultCustomKernelCreator() {\n  return GetCustomCreatorSingleton()->Get();\n}\n\nvoid RegisterDefaultCustomKernelCreator(CustomKernelCreator* c) {\n  GetCustomCreatorSingleton()->Set(c);"
},
{
    "Id": 305,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3a7b36bca7f43ce4f0d0791ce0e0d84ece8683d9",
    "Violation": "improper",
    "Bug report": " [Grappler] Remove DCHECK from a MutableGraphView CanDedupControlWithRegularInput check.",
    "Number of deleted lines": 2,
    "Deleted lines": "\n// Determines if node is an Identity where it's first regular input is a Switch\n// node.\nbool IsIdentityConsumingSwitch(const MutableGraphView& graph,\n                               const NodeDef& node) {\n  if ((IsIdentity(node) || IsIdentityNSingleInput(node)) &&\n      node.input_size() > 0) {\n    TensorId tensor_id = ParseTensorName(node.input(0));\n    if (IsTensorIdControlling(tensor_id)) {\n      return false;\n    }\n\n    NodeDef* input_node = graph.GetNode(tensor_id.node());\n    return IsSwitch(*input_node);\n  }\n  return false;\n}\n\n// Determines if node input can be deduped by regular inputs when used as a\n// control dependency. Specifically, if a node is an Identity that leads to a\n// Switch node, when used as a control dependency, that control dependency\n// should not be deduped even though the same node is used as a regular input.\nbool CanDedupControlWithRegularInput(const MutableGraphView& graph,\n                                     const NodeDef& control_node) {\n  return !IsIdentityConsumingSwitch(graph, control_node);\n}\n\n// Determines if node input can be deduped by regular inputs when used as a\n// control dependency. Specifically, if a node is an Identity that leads to a\n// Switch node, when used as a control dependency, that control dependency\n// should not be deduped even though the same node is used as a regular input.\nbool CanDedupControlWithRegularInput(const MutableGraphView& graph,\n                                     absl::string_view control_node_name) {\n  NodeDef* control_node = graph.GetNode(control_node_name);\n  DCHECK(control_node != nullptr)\n      << \"Didn't find a node for control dependency: \" << control_node_name;\n  return CanDedupControlWithRegularInput(graph, *control_node);\n}\n\nbool HasRegularFaninNode(const MutableGraphView& graph, const NodeDef& node,\n                         absl::string_view fanin_node_name) {\n  const int num_regular_fanins =\n      graph.NumFanins(node, /*include_controlling_nodes=*/false);\n  for (int i = 0; i < num_regular_fanins; ++i) {\n    if (ParseTensorName(node.input(i)).node() == fanin_node_name) {\n      return true;\n    }\n  }\n  return false;\n}\n\nusing FanoutsMap =\n    absl::flat_hash_map<MutableGraphView::OutputPort,\n                        absl::flat_hash_set<MutableGraphView::InputPort>>;\n\nvoid SwapControlledFanoutInputs(const MutableGraphView& graph,\n                                const FanoutsMap::iterator& control_fanouts,\n                                absl::string_view to_node_name) {\n  absl::string_view from_node_name(control_fanouts->first.node->name());\n  string control = TensorIdToString({to_node_name, Graph::kControlSlot});\n  for (const auto& control_fanout : control_fanouts->second) {\n    const int start = graph.NumFanins(*control_fanout.node,\n                                      /*include_controlling_nodes=*/false);\n    for (int i = start; i < control_fanout.node->input_size(); ++i) {\n      TensorId tensor_id = ParseTensorName(control_fanout.node->input(i));\n      if (tensor_id.node() == from_node_name) {\n        control_fanout.node->set_input(i, control);\n        break;\n      }\n    }\n  }\n}"
},
{
    "Id": 306,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c5019e2156c749d35ec786ff7946a55006d9ba91",
    "Violation": "missing",
    "Bug report": "missing checking on null pointer dereference",
    "Number of deleted lines": 1,
    "Deleted lines": "\nbool GpuCudaMallocAsyncAllocator::TracksAllocationSizes() const {\n  return static_cast<bool>(stats_);\n}\n\nsize_t GpuCudaMallocAsyncAllocator::RequestedSize(const void* ptr) const {\n  if (!stats_ || !ptr) return 0;\n  mutex_lock l(lock_);\n  return size_map_.at(ptr);\n}\n\nsize_t GpuCudaMallocAsyncAllocator::AllocatedSize(const void* ptr) const {\n  if (!stats_ || !ptr) return 0;\n  mutex_lock l(lock_);\n  return size_map_.at(ptr);\n}\n\nabsl::optional<AllocatorStats> GpuCudaMallocAsyncAllocator::GetStats() {\n  if (!stats_) return absl::nullopt;\n  mutex_lock l(lock_);\n  return *stats_;\n}\n\nbool GpuCudaMallocAsyncAllocator::ClearStats() {\n  if (!stats_) return false;\n  mutex_lock l(lock_);\n  stats_->num_allocs = 0;\n  stats_->peak_bytes_in_use = stats_->bytes_in_use;\n  stats_->largest_alloc_size = 0;\n  return true;\n}\n\nvoid GpuCudaMallocAsyncAllocator::SetStream(void* stream) {\n#if TF_CUDA_MALLOC_ASYNC_SUPPORTED\n  uint64_t pool_size_64 = 0;\n  if (auto status = cuMemPoolGetAttribute(\n          pool_, CU_MEMPOOL_ATTR_RELEASE_THRESHOLD, &pool_size_64)) {\n    LOG(FATAL) <<  // Crash OK.\n        \"Failed to get CUDA pool attribute: \" << GetCudaErrorMessage(status);\n\n  }\n  cuda_stream_ = *(reinterpret_cast<CUstream*>(stream));\n  int64 prealloc_size = 0;\n  // TF_CUDA_MALLOC_ASYNC_SUPPORTED_PREALLOC=-1 is a special value that\n  // preallocates the total pool size.\n  TF_CHECK_OK(ReadInt64FromEnvVar(\"TF_CUDA_MALLOC_ASYNC_SUPPORTED_PREALLOC\", 0,\n                                  &prealloc_size));\n  if (prealloc_size == -1) {\n    prealloc_size = pool_size_64;\n  } else if (reserve_memory_) {\n    prealloc_size = pool_size_64;\n  }\n\n  if (prealloc_size != 0) {\n    void* ptr = AllocateRaw(0, prealloc_size);\n    DeallocateRaw(ptr);\n    VLOG(2) << Name() << \" GpuCudaMallocAsyncAllocator reserved the pool for \"\n            << prealloc_size << \" bytes\"\n            << \". First ptr: \" << ptr;\n    ClearStats();\n  }\n#endif\n}\n\n}  // namespace tensorflow\n"
},
{
    "Id": 307,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a7908e924af3105c3007988e219855174b26774f",
    "Violation": "missing",
    "Bug report": "Added check for output buffer ",
    "Number of deleted lines": 0,
    "Deleted lines": "\n  // Initialize libjpeg structures to have a memory source\n  // Modify the usual jpeg error manager to catch fatal errors.\n  struct jpeg_decompress_struct cinfo;\n  struct jpeg_error_mgr jerr;\n  jmp_buf jpeg_jmpbuf;\n  cinfo.err = jpeg_std_error(&jerr);\n  cinfo.client_data = &jpeg_jmpbuf;\n  jerr.error_exit = CatchError;\n  if (setjmp(jpeg_jmpbuf)) {\n    return false;\n  }\n\n  // set up, read header, set image parameters, save size\n  jpeg_create_decompress(&cinfo);\n  SetSrc(&cinfo, srcdata, datasize, false);\n\n  jpeg_read_header(&cinfo, TRUE);\n  jpeg_start_decompress(&cinfo);  // required to transfer image size to cinfo\n  if (width) *width = cinfo.output_width;\n  if (height) *height = cinfo.output_height;\n  if (components) *components = cinfo.output_components;\n\n  jpeg_destroy_decompress(&cinfo);\n\n  return true;\n}\n\n// -----------------------------------------------------------------------------\n// Compression\n\nnamespace {\nbool CompressInternal(const uint8* srcdata, int width, int height,\n                      const CompressFlags& flags, tstring* output) {\n  output->clear();\n  const int components = (static_cast<int>(flags.format) & 0xff);\n\n  int64 total_size = static_cast<int64>(width) * static_cast<int64>(height);\n  // Some of the internal routines do not gracefully handle ridiculously\n  // large images, so fail fast.\n  if (width <= 0 || height <= 0) {\n    LOG(ERROR) << \"Invalid image size: \" << width << \" x \" << height;\n    return false;\n  }\n  if (total_size >= (1LL << 29)) {\n    LOG(ERROR) << \"Image too large: \" << total_size;\n    return false;\n  }\n\n  int in_stride = flags.stride;\n  if (in_stride == 0) {\n    in_stride = width * (static_cast<int>(flags.format) & 0xff);\n  } else if (in_stride < width * components) {\n    LOG(ERROR) << \"Incompatible input stride\";\n    return false;\n  }\n\n  JOCTET* buffer = nullptr;\n\n  // NOTE: for broader use xmp_metadata should be made a unicode string\n  CHECK(srcdata != nullptr);\n  CHECK(output != nullptr);\n  // This struct contains the JPEG compression parameters and pointers to\n  // working space\n  struct jpeg_compress_struct cinfo;\n  // This struct represents a JPEG error handler.\n  struct jpeg_error_mgr jerr;\n  jmp_buf jpeg_jmpbuf;  // recovery point in case of error\n\n  // Step 1: allocate and initialize JPEG compression object"
},
{
    "Id": 308,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/04b97cde86550995da57d16d81084006456ccce5",
    "Violation": "missing",
    "Bug report": "Fix segmentation fault with tf.stack an keras's Input in TF2.0. This fix adds the `PySequence_Fast` and checks the return value to make sure it is not nullptr.",
    "Number of deleted lines": 2,
    "Deleted lines": "#if PY_MAJOR_VERSION >= 3\n  return PyFloat_Check(item) || PyLong_Check(item);\n#else\n  return PyFloat_Check(item) || PyInt_Check(item) || PyLong_Check(item);\n#endif\n}\n\nbool CheckOneInput(PyObject* item) {\n  if (EagerTensor_CheckExact(item) || CheckResourceVariable(item) ||\n      PyArray_Check(item) || IsNumberType(item)) {\n    return true;\n  }\n\n  // Sequences are not properly handled. Sequences with purely python numeric\n  // types work, but sequences with mixes of EagerTensors and python numeric\n  // types don't work.\n  // TODO(nareshmodi): fix\n  return false;\n}\n\nbool CheckInputsOk(PyObject* seq, int start_index,\n                   const tensorflow::OpDef& op_def) {\n  for (int i = 0; i < op_def.input_arg_size(); i++) {\n    PyObject* item = PyTuple_GET_ITEM(seq, i + start_index);\n    if (!op_def.input_arg(i).number_attr().empty() ||\n        !op_def.input_arg(i).type_list_attr().empty()) {\n      // This item should be a seq input.\n      if (!PySequence_Check(item)) {\n        VLOG(1) << \"Falling back to slow path for Op \\\"\" << op_def.name()\n                << \"\\\", Input \\\"\" << op_def.input_arg(i).name()\n                << \"\\\" since we expected a sequence, but got \"\n                << item->ob_type->tp_name;\n        return false;\n      }\n      for (Py_ssize_t j = 0; j < PySequence_Fast_GET_SIZE(item); j++) {\n        PyObject* inner_item = PySequence_Fast_GET_ITEM(item, j);\n        if (!CheckOneInput(inner_item)) {\n          VLOG(1) << \"Falling back to slow path for Op \\\"\" << op_def.name()\n                  << \"\\\", Input \\\"\" << op_def.input_arg(i).name()\n                  << \"\\\", Index \" << j\n                  << \" since we expected an EagerTensor/ResourceVariable, \"\n                     \"but got \"\n                  << inner_item->ob_type->tp_name;\n          return false;\n        }\n      }\n    } else if (!CheckOneInput(item)) {\n      VLOG(1)\n          << \"Falling back to slow path for Op \\\"\" << op_def.name()\n          << \"\\\", Input \\\"\" << op_def.input_arg(i).name()\n          << \"\\\" since we expected an EagerTensor/ResourceVariable, but got \"\n          << item->ob_type->tp_name;\n      return false;\n    }\n  }\n\n  return true;\n}\n\nPyObject* MaybeGetDType(PyObject* item) {\n  if (EagerTensor_CheckExact(item)) {\n    tensorflow::Safe_PyObjectPtr py_dtype(\n        PyObject_GetAttrString(item, \"dtype\"));\n    return PyObject_GetAttrString(py_dtype.get(), \"_type_enum\");\n  }\n\n  if (CheckResourceVariable(item)) {\n    tensorflow::Safe_PyObjectPtr py_dtype(\n        PyObject_GetAttrString(item, \"_dtype\"));\n    return PyObject_GetAttrString(py_dtype.get(), \"_type_enum\");\n  }\n"
},
{
    "Id": 309,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cd8d0bf58ad554588012898161c91fa453bbf7f0",
    "Violation": "missing",
    "Bug report": "Address edge case where runStats is null and the interface is closed.",
    "Number of deleted lines": 1,
    "Deleted lines": "\n    return 0;\n  }\n\n  /** Returns a reference to the Graph describing the computation run during inference. */\n  public Graph graph() {\n    return g;\n  }\n\n  /**\n   * Whether to collect stats during inference. This should only be enabled when needed, as it will\n   * add overhead.\n   */\n  public void enableStatLogging(boolean enabled) {\n    enableStats = enabled;\n    if (enableStats && runStats == null) {\n      runStats = new RunStats();\n    }\n  }\n\n  /** Returns the last stat summary string if logging is enabled. */\n  public String getStatString() {\n    return (runStats == null) ? \"\" : runStats.summary();\n  }\n\n  /**\n   * Cleans up the state associated with this Object. initializeTensorFlow() can then be called\n   * again to initialize a new session.\n   */\n  public void close() {\n    closeFeeds();\n    closeFetches();\n    sess.close();\n    g.close();\n    runStats.close();\n    runStats = null;\n    enableStats = false;\n  }\n\n  // Methods for taking a native Tensor and filling it with values from Java arrays.\n\n  /**\n   * Given a source array with shape {@link dims} and content {@link src}, copy the contents into\n   * the input Tensor with name {@link inputName}. The source array {@link src} must have at least\n   * as many elements as that of the destination Tensor. If {@link src} has more elements than the\n   * destination has capacity, the copy is truncated.\n   */\n  public void fillNodeFloat(String inputName, int[] dims, float[] src) {\n    addFeed(inputName, Tensor.create(mkDims(dims), FloatBuffer.wrap(src)));\n  }\n\n  /**\n   * Given a source array with shape {@link dims} and content {@link src}, copy the contents into\n   * the input Tensor with name {@link inputName}. The source array {@link src} must have at least\n   * as many elements as that of the destination Tensor. If {@link src} has more elements than the\n   * destination has capacity, the copy is truncated.\n   */\n  public void fillNodeInt(String inputName, int[] dims, int[] src) {\n    addFeed(inputName, Tensor.create(mkDims(dims), IntBuffer.wrap(src)));\n  }\n\n  /**\n   * Given a source array with shape {@link dims} and content {@link src}, copy the contents into\n   * the input Tensor with name {@link inputName}. The source array {@link src} must have at least\n   * as many elements as that of the destination Tensor. If {@link src} has more elements than the\n   * destination has capacity, the copy is truncated.\n   */\n  public void fillNodeDouble(String inputName, int[] dims, double[] src) {\n    addFeed(inputName, Tensor.create(mkDims(dims), DoubleBuffer.wrap(src)));\n  }\n"
},
{
    "Id": 310,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1a1a381b5be7701843c3f1e34aa1846ae2a1d0ce",
    "Violation": "improper",
    "Bug report": "Fix a SIGSEGV bug in InferShapeForXlaGatherOp. Since `ComputeOutputComponent` may return nullptr, we need to check for null attributes explicitly to be safe.",
    "Number of deleted lines": 1,
    "Deleted lines": "\n    if (!window_result_shape) {\n      op->emitOpError(\"failed to infer window result shape\");\n    }\n\n    if (window_result_shape.value() != source_shape) {\n      op->emitOpError(\n          \"Source shape does not match the shape of window-reduced operand.\");\n    }\n  }\n\n  return RefineResultType(op.getOperation(), op.getResult(),\n                          op.getOperand().getType());\n}\n\nbool ShapeInference::InferShapeForXlaGatherOp(XlaGatherOp op) {\n  xla::Shape input_shape = xla::TypeToShape(op.getOperand().getType());\n  if (input_shape == xla::Shape() || input_shape.is_unbounded_dynamic())\n    return false;\n\n  xla::Shape start_indices_shape =\n      xla::TypeToShape(op.getStartIndices().getType());\n  if (start_indices_shape == xla::Shape()) return false;\n\n  xla::GatherDimensionNumbers gather_dim_numbers;\n  if (!gather_dim_numbers.ParseFromString(op.getDimensionNumbers().str()))\n    return false;\n\n  DenseIntElementsAttr slice_sizes_attr;\n  if (DenseIntElementsAttr attr;\n      matchPattern(op.getSliceSizes(), m_Constant(&attr))) {\n    slice_sizes_attr = attr;\n  } else if (const auto it = results_.find(ValuePort(op.getSliceSizes()));\n             it != results_.end() &&\n             llvm::isa<DenseIntElementsAttr>(it->second)) {\n    slice_sizes_attr = llvm::cast<DenseIntElementsAttr>(it->second);\n  } else {\n    return false;\n  }\n\n  llvm::SmallVector<int64_t> slice_sizes;\n  for (const auto& attr : slice_sizes_attr.getValues<APInt>()) {\n    slice_sizes.push_back(attr.getSExtValue());\n  }\n\n  auto output_shape = xla::ShapeInference::InferGatherShape(\n      input_shape, start_indices_shape, gather_dim_numbers, slice_sizes);\n  if (!output_shape.ok()) {\n    op->emitError() << output_shape.status().message();\n    return false;\n  }\n\n  auto refined_type = xla::ConvertShapeToType<RankedTensorType>(\n      *output_shape, mlir::Builder(op));\n  if (!refined_type.ok()) {\n    op->emitError() << refined_type.status().message();\n    return false;\n  }\n\n  return RefineResultType(op, op.getOutput(), *refined_type);\n}\n\nstd::optional<RankedTensorType> InferXlaConvOutputShape(\n    llvm::SmallVector<int64_t> input_tensor_dims,\n    llvm::SmallVector<int64_t> kernel_tensor_dims,\n    llvm::SmallVector<int64_t> window_strides,\n    llvm::SmallVector<std::pair<int64_t, int64_t>> paddings,\n    llvm::SmallVector<int64_t> lhs_dilations,\n    llvm::SmallVector<int64_t> rhs_dilations, int64_t batch_group_count,\n    xla::ConvolutionDimensionNumbers dnums, Type element_type) {\n  auto num_spatial_dims = input_tensor_dims.size() - 2;"
},
{
    "Id": 311,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9720b405905dee209a3f7d003de21d388e1aaef4",
    "Violation": "improper",
    "Bug report": " Avoid nullptr as row offsets to cusparseCreateCsr. As of CUDA 12.2 additional input validation allows NULL for the row offsets only when rows=0.",
    "Number of deleted lines": 1,
    "Deleted lines": "    values_vec.reserve(batch_size);\n\n    // Temporary buffers reused across batch\n    Tensor buffer1_t, buffer2_t;\n\n    // Compute intermediate results\n    for (int i = 0; i < batch_size; ++i) {\n      int a_batch = broadcast_batch_a ? 0 : i;\n      int b_batch = broadcast_batch_b ? 0 : i;\n      GpuSparseSpGEMMDescr gemmDesc;\n      GpuSparseConstSpMatDescr matA;\n      GpuSparseConstSpMatDescr matB;\n      GpuSparseSpMatDescr matC;\n      OP_REQUIRES_OK(ctx, gemmDesc.Initialize());\n      OP_REQUIRES_OK(ctx,\n                     matA.InitializeCsr(\n                         a_input_dense_shape(a_input_dense_shape.size() - 2),\n                         a_input_dense_shape(a_input_dense_shape.size() - 1),\n                         a_input_matrix->col_indices_vec(a_batch).size(),\n                         a_input_matrix->row_pointers_vec(a_batch).data(),\n                         a_input_matrix->col_indices_vec(a_batch).data(),\n                         a_input_matrix->values_vec<T>(a_batch).data()));\n      OP_REQUIRES_OK(ctx,\n                     matB.InitializeCsr(\n                         b_input_dense_shape(b_input_dense_shape.size() - 2),\n                         b_input_dense_shape(b_input_dense_shape.size() - 1),\n                         b_input_matrix->col_indices_vec(b_batch).size(),\n                         b_input_matrix->row_pointers_vec(b_batch).data(),\n                         b_input_matrix->col_indices_vec(b_batch).data(),\n                         b_input_matrix->values_vec<T>(b_batch).data()));\n      OP_REQUIRES_OK(ctx,\n                     matC.InitializeCsr<int, T>(\n                         a_input_dense_shape(a_input_dense_shape.size() - 2),\n                         b_input_dense_shape(b_input_dense_shape.size() - 1), 0,\n                         nullptr, nullptr, nullptr));\n\n      // Check required size for buffer1 and possibly re-allocate\n      size_t bufferSize1;\n      OP_REQUIRES_OK(\n          ctx, cudaSparse.SpGEMM_workEstimation<T>(matA, matB, matC, gemmDesc,\n                                                   &bufferSize1, nullptr));\n      if (bufferSize1 > buffer1_t.NumElements()) {\n        OP_REQUIRES_OK(\n            ctx, ctx->allocate_temp(\n                     DT_INT8, TensorShape({static_cast<int64_t>(bufferSize1)}),\n                     &buffer1_t));\n      }\n      void* buffer1 = buffer1_t.flat<int8>().data();\n\n      // Do workEstimation using buffer1.\n      // buffer1 implicitly captured in gemmDesc for use in the compute call.\n      OP_REQUIRES_OK(\n          ctx, cudaSparse.SpGEMM_workEstimation<T>(matA, matB, matC, gemmDesc,\n                                                   &bufferSize1, buffer1));\n\n      // Compute size for buffer2 and possibly re-allocate\n      size_t bufferSize2;\n      OP_REQUIRES_OK(ctx,\n                     cudaSparse.SpGEMM_compute<T>(matA, matB, matC, gemmDesc,\n                                                  &bufferSize2, nullptr));\n      if (bufferSize2 > buffer2_t.NumElements()) {\n        OP_REQUIRES_OK(\n            ctx, ctx->allocate_temp(\n                     DT_INT8, TensorShape({static_cast<int64_t>(bufferSize2)}),\n                     &buffer2_t));\n      }\n      void* buffer2 = buffer2_t.flat<int8>().data();\n\n      // Compute the gemm.\n      // Note that buffer1 is implicitly consumed here and buffer2 is implicitly\n      // captured for use by by the copy call."
},
{
    "Id": 312,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/47eaa828a1dd4bf50ec4203ef4bbb348b3ef0dd0",
    "Violation": "missing",
    "Bug report": "Add nullptr check",
    "Number of deleted lines": 0,
    "Deleted lines": "    status->status = tensorflow::errors::FailedPrecondition(\n        \"Accessing device stream is not supported for a CPU device.\");\n    return nullptr;\n  } else if (!cc_ctx->op_device_context()->IsPluggableDevice()) {\n    status->status = tensorflow::errors::FailedPrecondition(\n        \"Accessing device stream is only supported for pluggable devices.\");\n    return nullptr;\n  } else {  // Is a PluggableDevice\n    TF_SetStatus(status, TF_OK, \"\");\n    auto c_stream = static_cast<stream_executor::CStream*>(\n        cc_ctx->op_device_context()->stream()->implementation());\n    return c_stream->Handle();\n  }\n#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)\n}\n\nint TF_NumInputs(TF_OpKernelContext* ctx) {\n  auto* cc_ctx = reinterpret_cast<::tensorflow::OpKernelContext*>(ctx);\n  return cc_ctx->num_inputs();\n}\n\nint TF_NumOutputs(TF_OpKernelContext* ctx) {\n  auto* cc_ctx = reinterpret_cast<::tensorflow::OpKernelContext*>(ctx);\n  return cc_ctx->num_outputs();\n}\n\nvoid TF_GetInput(TF_OpKernelContext* ctx, int i, TF_Tensor** tensor,\n                 TF_Status* status) {\n  auto* cc_ctx = reinterpret_cast<::tensorflow::OpKernelContext*>(ctx);\n  if (i < 0 || i >= cc_ctx->num_inputs()) {\n    TF_SetStatus(status, TF_OUT_OF_RANGE, \"input index out of range\");\n    return;\n  }\n  const ::tensorflow::Tensor& cc_tensor(cc_ctx->input(i));\n  TF_Tensor* result =\n      ::tensorflow::TF_TensorFromTensor(cc_tensor, &status->status);\n  if (TF_GetCode(status) == TF_OK) {\n    *tensor = result;\n  }\n}\n\nvoid TF_InputRange(TF_OpKernelContext* ctx, const char* name,\n                   TF_InputRange_Args* args) {\n  auto* cc_ctx = reinterpret_cast<::tensorflow::OpKernelContext*>(ctx);\n  int start = -1, stop = -1;\n  auto status = cc_ctx->op_kernel().InputRange(name, &start, &stop);\n  args->start = start;\n  args->stop = stop;\n  tensorflow::Set_TF_Status_from_Status(args->status, status);\n}\n\nTF_DataType TF_InputDatatype(TF_OpKernelContext* ctx, int index) {\n  auto* cc_ctx = reinterpret_cast<::tensorflow::OpKernelContext*>(ctx);\n  CHECK_GE(index, 0);                     // Crash OK\n  CHECK_LT(index, cc_ctx->num_inputs());  // Crash OK\n  return static_cast<TF_DataType>(cc_ctx->input_dtype(index));\n}\n\nvoid TF_SetOutput(TF_OpKernelContext* ctx, int i, const TF_Tensor* tensor,\n                  TF_Status* status) {\n  auto* cc_ctx = reinterpret_cast<::tensorflow::OpKernelContext*>(ctx);\n  if (i < 0 || i >= cc_ctx->num_outputs()) {\n    TF_SetStatus(status, TF_OUT_OF_RANGE, \"output index out of range\");\n    return;\n  }\n  ::tensorflow::Tensor cc_tensor;\n  ::tensorflow::Status s = ::tensorflow::TF_TensorToTensor(tensor, &cc_tensor);\n  TF_SetStatus(status, TF_OK, \"\");\n  ::tensorflow::Set_TF_Status_from_Status(status, s);\n  if (s.ok()) {"
},
{
    "Id": 313,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2fc1f2b5a8b8152c43b81cf31394f3e0a2cb837",
    "Violation": "missing",
    "Bug report": "Add null pointer check",
    "Number of deleted lines": 0,
    "Deleted lines": "  // TODO(cheshire): Return an error instead.\n  // TODO(cheshire): Why are these checked only for `half` and `float`?\n  if (dtype == blas::DataType::kHalf || dtype == blas::DataType::kFloat) {\n    if (transa == blas::Transpose::kNoTranspose) {\n      if (lda < static_cast<int64_t>(m)) {\n        LOG(WARNING) << \"GEMM lda was smaller than m (no transpose case); \"\n                        \"precondition violation\";\n      }\n    } else {\n      if (lda < static_cast<int64_t>(k)) {\n        LOG(WARNING) << \"GEMM lda (\" << lda << \") was smaller than k (\" << k\n                     << \") (transpose case); precondition violation\";\n      }\n    }\n    if (transb == blas::Transpose::kNoTranspose) {\n      if (ldb < static_cast<int64_t>(k)) {\n        LOG(WARNING) << \"GEMM ldb (\" << ldb << \") was smaller than k (\" << k\n                     << \") (no transpose case); precondition violation\";\n      }\n    } else {\n      if (ldb < static_cast<int64_t>(n)) {\n        LOG(WARNING) << \"GEMM ldb was smaller than n (transpose case); \"\n                        \"precondition violation\";\n      }\n    }\n  }\n\n  VLOG(1) << absl::StrFormat(\n      \"doing cuBLAS SGEMM: at=%d bt=%d m=%u n=%u \"\n      \"k=%u alpha=%p a=%p lda=%d b=%p ldb=%d beta=%p \"\n      \"c=%p ldc=%d\",\n      static_cast<int>(transa), static_cast<int>(transb), m, n, k, alpha,\n      a.opaque(), lda, b.opaque(), ldb, beta, c->opaque(), ldc);\n\n  switch (dtype) {\n    case blas::DataType::kHalf: {\n#if CUDA_VERSION < 7050\n      return tsl::errors::Internal(\n          \"fp16 sgemm is not implemented in this cuBLAS version \"\n          \"(need at least CUDA 7.5)\");\n#endif\n\n      return DoBlasInternalImpl(\n          cublasSgemmEx, stream, true /* = pointer_mode_host */, math_type,\n          AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n          static_cast<const float *>(alpha), a.opaque(), SE_CUDA_DATA_HALF, lda,\n          b.opaque(), SE_CUDA_DATA_HALF, ldb, static_cast<const float *>(beta),\n          c->opaque(), SE_CUDA_DATA_HALF, ldc);\n    }\n#if CUDA_VERSION > 11000\n    case blas::DataType::kBF16: {\n      return DoBlasInternalImpl(\n          cublasSgemmEx, stream, true /* = pointer_mode_host */, math_type,\n          AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n          static_cast<const float *>(alpha), a.opaque(), CUDA_R_16BF, lda,\n          b.opaque(), CUDA_R_16BF, ldb, static_cast<const float *>(beta),\n          c->opaque(), CUDA_R_16BF, ldc);\n    }\n#endif\n    case dnn::kFloat:\n      return DoBlasInternalImpl(\n          cublasSgemm, stream, true /* = pointer_mode_host */, math_type,\n          AsCublasOperation(transa), AsCublasOperation(transb), m, n, k,\n          static_cast<const float *>(alpha),\n          static_cast<const float *>(a.opaque()), lda,\n          static_cast<const float *>(b.opaque()), ldb,\n          static_cast<const float *>(beta), static_cast<float *>(c->opaque()),\n          ldc);\n    case dnn::kDouble:\n      return DoBlasInternalImpl("
},
{
    "Id": 314,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b677392e4af8095dbde8068b0ceb60bca815e94b",
    "Violation": "missing",
    "Bug report": " Reject non-PjRt devices in PjRtArray::Reshard(). PjRt buffers traditionally support some degree of interoperability between PjRt clients (e.g., CPU and TPU). However, this is not universally true between arbitrary IFRT clients that may use a non-PjRt-compatible runtime. This change adds extra checks to make sure that non-PjRt devices are not accidentally used in PjRtArray's destination devices.",
    "Number of deleted lines": 0,
    "Deleted lines": "  return future;\n}\n\nStatusOr<tsl::RCReference<Array>> PjRtArray::Reshard(\n    std::shared_ptr<const Sharding> new_sharding,\n    ArrayCopySemantics semantics) {\n  DCHECK(this);\n  if (new_sharding->devices().size() != sharding_->devices().size()) {\n    return InvalidArgument(\n        \"Resharding to a different number of devices: %d; expected %d\",\n        new_sharding->devices().size(), sharding_->devices().size());\n  }\n  // TODO(hyeontaek): We should have an equivalence test for sharding that\n  // permits device changes and nothing else.\n  PjRtBuffers buffers;\n  buffers.reserve(pjrt_buffers_.size());\n  for (int i = 0; i < pjrt_buffers_.size(); ++i) {\n    if (pjrt_buffers_[i]->device() == new_sharding->devices()[i]) {\n      switch (semantics) {\n        case ArrayCopySemantics::kAlwaysCopy:\n          // TODO(hyeontaek): kAlwaysCopy should clone the buffer, but the PjRt\n          // API does not have efficient buffer cloning on the same device.\n          buffers.push_back(pjrt_buffers_[i]);\n          break;\n        case ArrayCopySemantics::kReuseInput:\n          buffers.push_back(pjrt_buffers_[i]);\n          break;\n        case ArrayCopySemantics::kDonateInput:\n          // TODO(hyeontaek): We may try std::move(pjrt_buffers_[i]), but this\n          // would be unsafe if there is a subsequent access to the buffer.\n          buffers.push_back(pjrt_buffers_[i]);\n          break;\n      }\n    } else {\n      TF_ASSIGN_OR_RETURN(\n          std::unique_ptr<xla::PjRtBuffer> copied_buffer,\n          pjrt_buffers_[i]->CopyToDevice(new_sharding->devices()[i]));\n      if (semantics == ArrayCopySemantics::kDonateInput) {\n        pjrt_buffers_[i] = nullptr;\n      }\n      buffers.push_back(std::shared_ptr<PjRtBuffer>(copied_buffer.release()));\n    }\n  }\n  return PjRtArray::Create(client_, dtype_, shape_, std::move(new_sharding),\n                           std::move(buffers));\n}\n\nFuture<Status> PjRtArray::GetReadyFuture() const {\n  DCHECK(this);\n  if (pjrt_buffers_.size() == 1) {\n    return pjrt_buffers_.front()->GetReadyFuture();\n  }\n  std::vector<Future<Status>> futures;\n  futures.reserve(pjrt_buffers_.size());\n  for (auto& buf : pjrt_buffers_) {\n    futures.push_back(buf->GetReadyFuture());\n  }\n  return JoinFutures(absl::MakeSpan(futures));\n}\n\nFuture<Status> PjRtArray::Delete() {\n  DCHECK(this);\n  for (auto& buffer : pjrt_buffers_) {\n    buffer->Delete();\n  }\n  // TODO(hyeontaek): Return a correct future.\n  return Future<Status>(OkStatus());\n}\n\nbool PjRtArray::IsDeleted() const {"
},
{
    "Id": 315,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/aaa3fb49374d59c89115730c8e2f672e70b9e3fa",
    "Violation": "missing",
    "Bug report": " [TFLite] Bucketize op: Fix processing of bucket boundary array. The param value may be a nullptr, which is an error; we should catch this and avoid dereferencing it.",
    "Number of deleted lines": 0,
    "Deleted lines": "    }\n    case BuiltinOperator_HASHTABLE: {\n      auto params = safe_allocator.Allocate<TfLiteHashtableParams>();\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      if (const auto* hashtable_params =\n              op->builtin_options_as_HashtableOptions()) {\n        params->table_id = hashtable_params->table_id();\n        TF_LITE_ENSURE_STATUS(ConvertTensorType(\n            hashtable_params->key_dtype(), &params->key_dtype, error_reporter));\n        TF_LITE_ENSURE_STATUS(ConvertTensorType(hashtable_params->value_dtype(),\n                                                &params->value_dtype,\n                                                error_reporter));\n      }\n      *builtin_data = params.release();\n      return kTfLiteOk;\n    }\n    case BuiltinOperator_RANDOM_STANDARD_NORMAL: {\n      auto params = safe_allocator.Allocate<TfLiteRandomParams>();\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      if (const auto* random_std_normal_params =\n              op->builtin_options_as_RandomOptions()) {\n        params->seed = random_std_normal_params->seed();\n        params->seed2 = random_std_normal_params->seed2();\n      }\n      *builtin_data = params.release();\n      return kTfLiteOk;\n    }\n    case BuiltinOperator_BUCKETIZE: {\n      auto params = safe_allocator.Allocate<TfLiteBucketizeParams>();\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      if (const auto* bucketize_params =\n              op->builtin_options_as_BucketizeOptions()) {\n        const flatbuffers::Vector<float>* boundaries =\n            bucketize_params->boundaries();\n        params->num_boundaries = boundaries->size();\n        params->boundaries = boundaries->data();\n      }\n      *builtin_data = params.release();\n      return kTfLiteOk;\n    }\n    case BuiltinOperator_RANDOM_UNIFORM: {\n      auto params = safe_allocator.Allocate<TfLiteRandomParams>();\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      if (const auto* random_uniform_params =\n              op->builtin_options_as_RandomOptions()) {\n        params->seed = random_uniform_params->seed();\n        params->seed2 = random_uniform_params->seed2();\n      }\n      *builtin_data = params.release();\n      return kTfLiteOk;\n    }\n    // Below are the ops with no builtin_data structure.\n    // TODO(aselle): Implement call in BuiltinOptions, but nullptrs are\n    // ok for now, since there is no call implementation either.\n    case BuiltinOperator_CALL:\n    case BuiltinOperator_CONCAT_EMBEDDINGS:\n    case BuiltinOperator_COS:\n    case BuiltinOperator_CUSTOM:\n    case BuiltinOperator_EMBEDDING_LOOKUP:\n    case BuiltinOperator_EQUAL:\n    case BuiltinOperator_MATRIX_DIAG:\n    case BuiltinOperator_MATRIX_SET_DIAG:\n    case BuiltinOperator_RELU_N1_TO_1:\n    case BuiltinOperator_SELECT:\n    case BuiltinOperator_SELECT_V2:\n    case BuiltinOperator_SLICE:\n    case BuiltinOperator_TILE:\n    case BuiltinOperator_TOPK_V2:\n    case BuiltinOperator_TRANSPOSE:\n    case BuiltinOperator_RANGE:\n    case BuiltinOperator_SQUARED_DIFFERENCE:"
},
{
    "Id": 316,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1835465ac5a9c823f7187cb0dd5786da9c360838",
    "Violation": "missing",
    "Bug report": " Add error_reporter DCHECK back into SimpleMemoryAllocator. This check was removed due to an internal build problem.",
    "Number of deleted lines": 0,
    "Deleted lines": "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/lite/micro/simple_memory_allocator.h\"\n\n#include <cstddef>\n#include <cstdint>\n#include <new>\n\n#include \"tensorflow/lite/core/api/error_reporter.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/micro/memory_helpers.h\"\n\nnamespace tflite {\n\nSimpleMemoryAllocator::SimpleMemoryAllocator(ErrorReporter* error_reporter,\n                                             uint8_t* buffer_head,\n                                             uint8_t* buffer_tail)\n    : error_reporter_(error_reporter),\n      buffer_head_(buffer_head),\n      buffer_tail_(buffer_tail),\n      head_(buffer_head),\n      tail_(buffer_tail) {}\n\nSimpleMemoryAllocator::SimpleMemoryAllocator(ErrorReporter* error_reporter,\n                                             uint8_t* buffer,\n                                             size_t buffer_size)\n    : SimpleMemoryAllocator(error_reporter, buffer, buffer + buffer_size) {}\n\n/* static */\nSimpleMemoryAllocator* SimpleMemoryAllocator::Create(\n    ErrorReporter* error_reporter, uint8_t* buffer_head, size_t buffer_size) {\n  TFLITE_DCHECK(buffer_head != nullptr);\n  SimpleMemoryAllocator tmp =\n      SimpleMemoryAllocator(error_reporter, buffer_head, buffer_size);\n\n  // Allocate enough bytes from the buffer to create a SimpleMemoryAllocator.\n  // The new instance will use the current adjusted tail buffer from the tmp\n  // allocator instance.\n  uint8_t* allocator_buffer = tmp.AllocateFromTail(\n      sizeof(SimpleMemoryAllocator), alignof(SimpleMemoryAllocator));\n  // Use the default copy constructor to populate internal states.\n  return new (allocator_buffer) SimpleMemoryAllocator(tmp);\n}\n\nSimpleMemoryAllocator::~SimpleMemoryAllocator() {}\n\nuint8_t* SimpleMemoryAllocator::AllocateFromHead(size_t size,\n                                                 size_t alignment) {\n  uint8_t* const aligned_result = AlignPointerUp(head_, alignment);\n  const size_t available_memory = tail_ - aligned_result;\n  if (available_memory < size) {\n    TF_LITE_REPORT_ERROR(\n        error_reporter_,\n        \"Failed to allocate memory. Requested: %u, available %u, missing: %u\",\n        size, available_memory, size - available_memory);\n    return nullptr;\n  }\n  head_ = aligned_result + size;\n  return aligned_result;\n}\n\nuint8_t* SimpleMemoryAllocator::AllocateFromTail(size_t size,\n                                                 size_t alignment) {\n  uint8_t* const aligned_result = AlignPointerDown(tail_ - size, alignment);\n  if (aligned_result < head_) {\n    const size_t missing_memory = head_ - aligned_result;\n    TF_LITE_REPORT_ERROR("
},
{
    "Id": 317,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7578e120de2a3a5282ced8d41881f19363f83466",
    "Violation": "missing",
    "Bug report": " Fix crash on closing the app when classifier failed to initialize. When testing on an API 21 emulator, the classifier fails to initialize. The fix is to check for null before calling `.close()`.",
    "Number of deleted lines": 1,
    "Deleted lines": "  }\n\n  /** Load the model and labels. */\n  @Override\n  public void onActivityCreated(Bundle savedInstanceState) {\n    super.onActivityCreated(savedInstanceState);\n    startBackgroundThread();\n  }\n\n  @Override\n  public void onResume() {\n    super.onResume();\n    startBackgroundThread();\n\n    // When the screen is turned off and turned back on, the SurfaceTexture is already\n    // available, and \"onSurfaceTextureAvailable\" will not be called. In that case, we can open\n    // a camera and start preview from here (otherwise, we wait until the surface is ready in\n    // the SurfaceTextureListener).\n    if (textureView.isAvailable()) {\n      openCamera(textureView.getWidth(), textureView.getHeight());\n    } else {\n      textureView.setSurfaceTextureListener(surfaceTextureListener);\n    }\n  }\n\n  @Override\n  public void onPause() {\n    closeCamera();\n    stopBackgroundThread();\n    super.onPause();\n  }\n\n  @Override\n  public void onDestroy() {\n    classifier.close();\n    super.onDestroy();\n  }\n\n  /**\n   * Sets up member variables related to camera.\n   *\n   * @param width The width of available size for camera preview\n   * @param height The height of available size for camera preview\n   */\n  private void setUpCameraOutputs(int width, int height) {\n    Activity activity = getActivity();\n    CameraManager manager = (CameraManager) activity.getSystemService(Context.CAMERA_SERVICE);\n    try {\n      for (String cameraId : manager.getCameraIdList()) {\n        CameraCharacteristics characteristics = manager.getCameraCharacteristics(cameraId);\n\n        // We don't use a front facing camera in this sample.\n        Integer facing = characteristics.get(CameraCharacteristics.LENS_FACING);\n        if (facing != null && facing == CameraCharacteristics.LENS_FACING_FRONT) {\n          continue;\n        }\n\n        StreamConfigurationMap map =\n            characteristics.get(CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP);\n        if (map == null) {\n          continue;\n        }\n\n        // // For still image captures, we use the largest available size.\n        Size largest =\n            Collections.max(\n                Arrays.asList(map.getOutputSizes(ImageFormat.JPEG)), new CompareSizesByArea());\n        imageReader =\n            ImageReader.newInstance(\n                largest.getWidth(), largest.getHeight(), ImageFormat.JPEG, /*maxImages*/ 2);\n"
},
{
    "Id": 318,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c1b9ac9f215a3a83f7f0b6233bf4cef0b3e74598",
    "Violation": "missing",
    "Bug report": "Error checking in c/python code.",
    "Number of deleted lines": 0,
    "Deleted lines": "  }\n\n  return PyObject_TypeCheck(o, SparseTensorValueType) == 1;\n}\n\nint IsSequenceForDataHelper(PyObject* o) {\n  return IsSequenceHelper(o) == 1 && !PyList_Check(o) &&\n         !IsSparseTensorValueType(o);\n}\n\nbool GetNextValuesForDict(PyObject* nested,\n                          std::vector<Safe_PyObjectPtr>* next_values) {\n  std::vector<PyObject*> result;\n\n  PyObject* keys = PyDict_Keys(nested);\n  if (PyList_Sort(keys) == -1) return false;\n  Py_ssize_t size = PyList_Size(keys);\n  for (Py_ssize_t i = 0; i < size; ++i) {\n    // We know that key and item will not be deleted because nested owns\n    // a reference to them and callers of flatten must not modify nested\n    // while the method is running.\n    PyObject* key = PyList_GET_ITEM(keys, i);\n    PyObject* item = PyDict_GetItem(nested, key);\n    Py_INCREF(item);\n    next_values->emplace_back(item);\n  }\n  Py_DECREF(keys);\n  return true;\n}\n\nbool GetNextValuesForIterable(PyObject* nested,\n                              std::vector<Safe_PyObjectPtr>* next_values) {\n  PyObject* item;\n  PyObject* iterator = PyObject_GetIter(nested);\n  while ((item = PyIter_Next(iterator)) != nullptr) {\n    next_values->emplace_back(item);\n  }\n  Py_DECREF(iterator);\n  return true;\n}\n\n// GetNextValues returns the values that the FlattenHelper function will recurse\n// over next.\nbool GetNextValues(PyObject* nested,\n                   std::vector<Safe_PyObjectPtr>* next_values) {\n  if (PyDict_Check(nested)) {\n    // if nested is dictionary, sort it by key and recurse on each value\n    return GetNextValuesForDict(nested, next_values);\n  }\n  // iterate and recurse\n  return GetNextValuesForIterable(nested, next_values);\n}\n\n// Similar to above, just specialized for the functions in the data pacakage.\nbool GetNextValuesForData(PyObject* nested,\n                          std::vector<Safe_PyObjectPtr>* next_values) {\n  if (PyDict_Check(nested)) {\n    // if nested is dictionary, sort it by key and recurse on each value\n    return GetNextValuesForDict(nested, next_values);\n  } else if (IsSparseTensorValueType(nested)) {\n    // if nested is a SparseTensorValue, just return itself as a single item\n    Py_INCREF(nested);\n    next_values->emplace_back(nested);\n    return true;\n  }\n  // iterate and recurse\n  return GetNextValuesForIterable(nested, next_values);\n}\n\nbool FlattenHelper("
},
{
    "Id": 319,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ed043aec4962dfdc3c58e2ad90dacb557dafcf4e",
    "Violation": "missing",
    "Bug report": " Lite: ResizeTensor Dim size check added to avoid reallocation if no change",
    "Number of deleted lines": 0,
    "Deleted lines": "      TfLiteTensor* tensor = &tensors_[tensor_index];\n      if (tensor->delegate && tensor->delegate != node.delegate &&\n          tensor->data_is_stale) {\n        TF_LITE_ENSURE_STATUS(EnsureTensorDataIsReadable(tensor_index));\n      }\n    }\n\n    if (check_cancelled_func_ != nullptr &&\n        check_cancelled_func_(cancellation_data_)) {\n      ReportError(\"Client requested cancel during Invoke()\");\n      return kTfLiteError;\n    }\n\n    EnsureTensorsVectorCapacity();\n    tensor_resized_since_op_invoke_ = false;\n    if (OpInvoke(registration, &node) == kTfLiteError) {\n      return ReportOpError(context_, node, registration, node_index,\n                           \"failed to invoke\");\n    }\n\n    // Force execution prep for downstream ops if the latest op triggered the\n    // resize of a dynamic tensor.\n    if (tensor_resized_since_op_invoke_ &&\n        HasDynamicTensor(*context_, node.outputs)) {\n      next_execution_plan_index_to_prepare_ = execution_plan_index + 1;\n    }\n  }\n\n  return status;\n}\n\nTfLiteStatus Subgraph::ResizeTensor(TfLiteContext* context,\n                                    TfLiteTensor* tensor,\n                                    TfLiteIntArray* new_size) {\n  // Note here that context->impl_ is recovering the this pointer for an\n  // instance of Interpreter to call into the member function ResizeTensorImpl\n  // (this function is static).\n  return static_cast<Subgraph*>(context->impl_)\n      ->ResizeTensorImpl(tensor, new_size);\n}\n\nvoid Subgraph::ReportErrorImpl(const char* format, va_list args) {\n  error_reporter_->Report(format, args);\n}\n\nvoid Subgraph::ReportErrorC(TfLiteContext* context, const char* format, ...) {\n  va_list args;\n  va_start(args, format);\n  auto* f = static_cast<Subgraph*>(context->impl_);\n  // Note here that context->impl_ is recovering the this pointer for an\n  // instance of Subgraph to call into the member function ReportErrorImpl\n  // (this function is static).\n  f->ReportErrorImpl(format, args);\n  va_end(args);\n}\n\n// Entry point for C node plugin API to report an error.\nvoid Subgraph::ReportError(const char* format, ...) {\n  va_list args;\n  va_start(args, format);\n  auto* f = static_cast<Subgraph*>(context_->impl_);\n  // Note here that context->impl_ is recovering the this pointer for an\n  // instance of Subgraph to call into the member function ReportErrorImpl\n  // (this function is static).\n  f->ReportErrorImpl(format, args);\n  va_end(args);\n}\n\nTfLiteStatus Subgraph::AddTensors(int tensors_to_add,\n                                  int* first_new_tensor_index) {"
},
{
    "Id": 320,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/20d54796563631c23c27548b321487e8b0c982a9",
    "Violation": "insufficient",
    "Bug report": " Add a nil check before init the device_name string, and also assign an empty string as a placeholder.",
    "Number of deleted lines": 1,
    "Deleted lines": "        return absl::NotFoundError(\"Input tensor is not found in the graph.\");\n      }\n\n      inputs_.push_back(input->id);\n      input_tensor_ids_.push_back(tensor_index);\n      tensor->buffer_handle = input->id;\n      tensor->delegate = &delegate_;\n    }\n\n    // Prepare graph outputs.\n    //\n    // Note that graph.outputs() cannot be used directly, as the notion of graph output has a\n    // different meaning in public API and GPU-internal API.\n    for (int tensor_index : TfLiteIntArrayView(delegate_params->output_tensors)) {\n      auto* tensor = &context->tensors[tensor_index];\n      if (IsConstantTensor(tensor)) continue;\n      // For quantized models, actual outputs of GPU graph are float tensors, so they should be\n      // quantized to be the 8-bit outputs of delegate.\n      if (options_.enable_quantization &&\n          quant_conversion_map_.find(tensor_index) != quant_conversion_map_.end()) {\n        tensor_index = quant_conversion_map_[tensor_index];\n        tensor = &context->tensors[tensor_index];\n      }\n      const auto* output = find_value(tensor_index);\n      if (!output || tensor->type != TfLiteType::kTfLiteFloat32) {\n        return absl::NotFoundError(\"Output tensor is not found in the graph.\");\n      }\n\n      outputs_.push_back(output->id);\n      output_tensor_ids_.push_back(tensor_index);\n      tensor->buffer_handle = output->id;\n      tensor->delegate = &delegate_;\n    }\n\n    std::string device_name = std::string([[metal_device_ name] UTF8String]);\n    GpuInfo gpu_info;\n    GetGpuInfoFromDeviceDescription(device_name, GpuApi::kMetal, &gpu_info);\n    size_t storage_type_size;\n    CalculationsPrecision precision;\n    if (options_.allow_precision_loss) {\n      storage_type_size = sizeof(HalfBits);\n      if (gpu_info.IsRoundToNearestSupported()) {\n        precision = CalculationsPrecision::F16;\n      } else {\n        precision = CalculationsPrecision::F32_F16;\n      }\n    } else {\n      storage_type_size = sizeof(float);\n      precision = CalculationsPrecision::F32;\n    }\n\n    CreateGpuModelInfo create_info;\n    create_info.precision = precision;\n    create_info.storage_type = GetFastestStorageType(gpu_info);\n    create_info.hints.Add(ModelHints::kAllowSpecialKernels);\n    const DataType external_data_type = DeduceDataTypeFromPrecision(create_info.precision);\n    const TensorStorageType external_storage_type = TensorStorageType::BUFFER;\n    for (auto& value : graph.inputs()) {\n      Layout layout = value->tensor.shape.b == 1 ? Layout::HWC : Layout::BHWC;\n      create_info.external_mutable_tensors[value->id] =\n          TensorDescriptor{external_data_type, external_storage_type, layout};\n    }\n    for (auto& value : graph.outputs()) {\n      Layout layout = value->tensor.shape.b == 1 ? Layout::HWC : Layout::BHWC;\n      create_info.external_mutable_tensors[value->id] =\n          TensorDescriptor{external_data_type, external_storage_type, layout};\n    }\n\n    // TODO(impjdi): Merge logic with above.\n    // Pre-allocate input and output metal buffers\n    std::vector<::tflite::gpu::ValueId> input_ids;"
},
{
    "Id": 321,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/db10718b38b2884cb5ed46d33c135c079f649d16",
    "Violation": "missing",
    "Bug report": "With some memory allocators, attempting to allocate 0 bytes will return a null pointer. This specifically happens when building tensorflow with mkl support. If TF_TensorData returns null, the go code to create a slice from the data leads to a null pointer exception. This fixes the issue by checking for the nil return and returning a slice zero value to (nil) to the caller. ",
    "Number of deleted lines": 0,
    "Deleted lines": "\tval := reflect.New(typ)\n\traw := tensorData(t.c)\n\tif t.DataType() != String {\n\t\tif err := decodeTensor(bytes.NewReader(raw), t.Shape(), typ, val); err != nil {\n\t\t\tpanic(bug(\"unable to decode Tensor of type %v and shape %v - %v\", t.DataType(), t.Shape(), err))\n\t\t}\n\t} else {\n\t\tnflattened := numElements(t.Shape())\n\t\td := stringDecoder{offsets: bytes.NewReader(raw[0 : 8*nflattened]), data: raw[8*nflattened:], status: newStatus()}\n\t\tif err := d.decode(val, t.Shape()); err != nil {\n\t\t\tpanic(bug(\"unable to decode String tensor with shape %v - %v\", t.Shape(), err))\n\t\t}\n\t}\n\treturn reflect.Indirect(val).Interface()\n}\n\n// WriteContentsTo writes the serialized contents of t to w.\n//\n// Returns the number of bytes written. See ReadTensor for\n// reconstructing a Tensor from the serialized form.\n//\n// WARNING: WriteContentsTo is not comprehensive and will fail\n// if t.DataType() is non-numeric (e.g., String). See\n// https://github.com/tensorflow/tensorflow/issues/6003.\nfunc (t *Tensor) WriteContentsTo(w io.Writer) (int64, error) {\n\tif err := isTensorSerializable(t.DataType()); err != nil {\n\t\treturn 0, err\n\t}\n\treturn io.Copy(w, bytes.NewReader(tensorData(t.c)))\n}\n\nfunc tensorData(c *C.TF_Tensor) []byte {\n\t// See: https://github.com/golang/go/wiki/cgo#turning-c-arrays-into-go-slices\n\tcbytes := C.TF_TensorData(c)\n\tlength := int(C.TF_TensorByteSize(c))\n\tslice := (*[1 << 30]byte)(unsafe.Pointer(cbytes))[:length:length]\n\treturn slice\n}\n\nvar types = []struct {\n\ttyp      reflect.Type\n\tdataType C.TF_DataType\n}{\n\t{reflect.TypeOf(float32(0)), C.TF_FLOAT},\n\t{reflect.TypeOf(float64(0)), C.TF_DOUBLE},\n\t{reflect.TypeOf(int32(0)), C.TF_INT32},\n\t{reflect.TypeOf(uint32(0)), C.TF_UINT32},\n\t{reflect.TypeOf(uint8(0)), C.TF_UINT8},\n\t{reflect.TypeOf(int16(0)), C.TF_INT16},\n\t{reflect.TypeOf(int8(0)), C.TF_INT8},\n\t{reflect.TypeOf(\"\"), C.TF_STRING},\n\t{reflect.TypeOf(complex(float32(0), float32(0))), C.TF_COMPLEX64},\n\t{reflect.TypeOf(int64(0)), C.TF_INT64},\n\t{reflect.TypeOf(uint64(0)), C.TF_UINT64},\n\t{reflect.TypeOf(false), C.TF_BOOL},\n\t{reflect.TypeOf(uint16(0)), C.TF_UINT16},\n\t{reflect.TypeOf(complex(float64(0), float64(0))), C.TF_COMPLEX128},\n\t// TODO(apassos): support DT_RESOURCE representation in go.\n\t// TODO(keveman): support DT_VARIANT representation in go.\n}\n\n// shapeAndDataTypeOf returns the data type and shape of the Tensor\n// corresponding to a Go type.\nfunc shapeAndDataTypeOf(val reflect.Value) (shape []int64, dt DataType, err error) {\n\ttyp := val.Type()\n\tfor typ.Kind() == reflect.Array || typ.Kind() == reflect.Slice {\n\t\tshape = append(shape, int64(val.Len()))\n\t\tif val.Len() > 0 {\n\t\t\t// In order to check tensor structure properly in general case we need to iterate over all slices of the tensor to check sizes match\n\t\t\t// Since we already going to iterate over all elements in encodeTensor() let's"
},
{
    "Id": 322,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2465d4e77654f0d4f7799bc46d5fd5812590acc6",
    "Violation": "missing",
    "Bug report": " Add a check in auto-sharding setup and die if the input mesh shape contains more than two shardable dimensions, which is currently not supported.",
    "Number of deleted lines": 0,
    "Deleted lines": "                                 absl::StrJoin(device_mesh_shape, \",\"), \"]\"));\n    lines.push_back(absl::StrCat(\"device_mesh_alpha: [\",\n                                 absl::StrJoin(device_mesh_alpha, \",\"), \"]\"));\n    lines.push_back(absl::StrCat(\"device_mesh_beta: [\",\n                                 absl::StrJoin(device_mesh_beta, \",\"), \"]\"));\n\n    lines.push_back(absl::StrCat(\"load_strategy: \", load_strategy));\n    if (load_strategy) {\n      lines.push_back(absl::StrCat(\"strategy_vector: [\",\n                                   absl::StrJoin(strategy_vector, \",\"), \"]\"));\n    }\n\n    return absl::StrJoin(lines, \"\\n\");\n  }\n\n  Status CheckAndSetup() {\n    if (device_mesh_shape.empty()) {\n      return tsl::errors::OutOfRange(\n          \"device_mesh_shape is empty and it needs to be specified.\");\n    }\n    if (device_mesh_shape.size() > 3) {\n      return tsl::errors::OutOfRange(\n          absl::StrCat(\"Not supported: the length of device_mesh_shape is \"\n                       \"greater than 3, actual length: \",\n                       device_mesh_shape.size()));\n    }\n    // All values in device_mesh_shape must be greater than 0.\n    if (absl::c_any_of(device_mesh_shape,\n                       [](const int64_t i) { return i <= 0; })) {\n      return tsl::errors::OutOfRange(\n          absl::StrCat(\"device_mesh_shape values need to be larger than 0: \"\n                       \"device_mesh_shape=\",\n                       absl::StrJoin(device_mesh_shape, \",\")));\n    }\n    if (device_mesh_alpha.empty()) {\n      // Generates simple device_mesh_alpha based on the size of\n      // device_mesh_shape.\n      device_mesh_alpha =\n          std::vector(device_mesh_shape.size(), kDeviceMeshAlpha);\n      VLOG(0) << \"Using default values for device_mesh_alpha: \"\n              << absl::StrJoin(device_mesh_alpha, \",\");\n    }\n    if (device_mesh_beta.empty()) {\n      // Generates simple device_mesh_beta based on the size of\n      // device_mesh_shape.\n      device_mesh_beta = std::vector(device_mesh_shape.size(), kDeviceMeshBeta);\n      VLOG(0) << \"Using default values for device_mesh_beta: \"\n              << absl::StrJoin(device_mesh_beta, \",\");\n    }\n\n    // If device_mesh_shape has only one value, append 1 to it\n    if (device_mesh_shape.size() == 1) {\n      device_mesh_shape.push_back(1);\n      device_mesh_alpha.push_back(1.0);\n      device_mesh_beta.push_back(1.0);\n    }\n\n    if (device_mesh_shape.size() != device_mesh_alpha.size() ||\n        device_mesh_shape.size() != device_mesh_beta.size()) {\n      return tsl::errors::OutOfRange(absl::StrCat(\n          \"Sizes do not match: length of device_mesh_shape is \",\n          device_mesh_shape.size(), \", length of device_mesh_alpha is \",\n          device_mesh_alpha.size(), \", length of device_mesh_beta is \",\n          device_mesh_beta.size(),\n          \". If not sure how to set device_mesh_alpha and \"\n          \"device_mesh_beta, \"\n          \"please leave them empty and default values will be used.\"));\n    }\n    int64_t total_devices = 1;\n    for (auto i : device_mesh_shape) {"
},
{
    "Id": 323,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3e0152a8b4aad03dd06274e0dd3b94bd5f8bf5d3",
    "Violation": "missing",
    "Bug report": "Fix invalid syntax error when import carla is present. The issue is that, when `import carla` is invoked, I/O operation for `std::ostringstream s` might fail, which caused the conversion of AttrValue to string as empty. This PR check `s.good()` to make sure the I/O operation is OK, and, fallback to normal conversion if locale-neutral I/O operation fails.",
    "Number of deleted lines": 1,
    "Deleted lines": "      strings::StrAppend(&ret, ShapeToPython(value.list().shape(i)));\n    }\n  } else if (value.list().tensor_size() > 0) {\n    for (int i = 0; i < value.list().tensor_size(); ++i) {\n      if (i > 0) strings::StrAppend(&ret, \", \");\n      strings::StrAppend(&ret, TensorToPython(value.list().tensor(i)));\n    }\n  } else if (value.list().func_size() > 0) {\n    for (int i = 0; i < value.list().func_size(); ++i) {\n      if (i > 0) strings::StrAppend(&ret, \", \");\n      strings::StrAppend(&ret, StringToPython(value.list().func(i).name()));\n    }\n  }\n  return ret;\n}\n\n// NOTE: The return value may contain spaces (for example, it could be\n// a string \"foo bar\" with an embedded space) and is not safe to pass\n// to WordWrap().\nstring AttrValueToPython(const string& type, const AttrValue& value,\n                         const string& dtype_module) {\n  if (type == \"string\") {\n    return StringToPython(value.s());\n  } else if (type == \"int\") {\n    return strings::StrCat(value.i());\n  } else if (type == \"float\") {\n    if (std::isnan(value.f()) || std::isinf(value.f())) {\n      return strings::StrCat(\"float('\", value.f(), \"')\");\n    } else {\n      // Use locale-independent conversion.\n      static_assert(FLT_DIG < 10, \"FLT_DIG is too big\");\n      std::ostringstream s;\n      s.imbue(std::locale::classic());\n      s << std::setprecision(FLT_DIG) << value.f();\n      return s.str();\n    }\n  } else if (type == \"bool\") {\n    return value.b() ? \"True\" : \"False\";\n  } else if (type == \"type\") {\n    return DataTypeToPython(value.type(), dtype_module);\n  } else if (type == \"shape\") {\n    return ShapeToPython(value.shape());\n  } else if (type == \"tensor\") {\n    return TensorToPython(value.tensor());\n  } else if (type == \"func\") {\n    return StringToPython(value.func().name());\n  } else if (absl::StartsWith(type, \"list(\")) {\n    return strings::StrCat(\"[\", AttrListToPython(value, dtype_module), \"]\");\n  } else {\n    return \"?\";\n  }\n}\n\nvoid GenerateLowerCaseOpName(const string& str, string* result) {\n  const char joiner = '_';\n  const char namespace_separator = '>';\n  const int last_index = str.size() - 1;\n  for (int i = 0; i <= last_index; ++i) {\n    const char c = str[i];\n    // Convert namespace separators ('>' characters) to joiners\n    if (c == namespace_separator) {\n      result->push_back(joiner);\n      continue;\n    }\n\n    // Emit a joiner only if a previous-lower-to-now-upper or a\n    // now-upper-to-next-lower transition happens.\n    // (But don't emit an extra joiner if we just saw a namespace separator\n    if (isupper(c) && (i > 0)) {\n      if (islower(str[i - 1]) || ((i < last_index) && islower(str[i + 1]))) {\n        if (!(str[i - 1] == namespace_separator)) {"
},
{
    "Id": 324,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0317f64491ba42376d96b157983a02d8b31b679e",
    "Violation": "improper",
    "Bug report": " Update RNNCell._rnn_get_variable to use Variable._trainable in TF2 mode. When using a legacy RNNCell in TF2 mode within a tf.function the \"var in trainable_variables()\" check led to treating a tf.bool tensor as a Python bool. This change makes use within a tf.function use the same logic that is used in Eager mode.",
    "Number of deleted lines": 2,
    "Deleted lines": "  def __call__(self, inputs, state, scope=None):\n    \"\"\"Run this RNN cell on inputs, starting from the given state.\n\n    Args:\n      inputs: `2-D` tensor with shape `[batch_size, input_size]`.\n      state: if `self.state_size` is an integer, this should be a `2-D Tensor`\n        with shape `[batch_size, self.state_size]`.  Otherwise, if\n        `self.state_size` is a tuple of integers, this should be a tuple with\n        shapes `[batch_size, s] for s in self.state_size`.\n      scope: VariableScope for the created subgraph; defaults to class name.\n\n    Returns:\n      A pair containing:\n\n      - Output: A `2-D` tensor with shape `[batch_size, self.output_size]`.\n      - New state: Either a single `2-D` tensor, or a tuple of tensors matching\n        the arity and shapes of `state`.\n    \"\"\"\n    if scope is not None:\n      with vs.variable_scope(\n          scope, custom_getter=self._rnn_get_variable) as scope:\n        return super(RNNCell, self).__call__(inputs, state, scope=scope)\n    else:\n      scope_attrname = \"rnncell_scope\"\n      scope = getattr(self, scope_attrname, None)\n      if scope is None:\n        scope = vs.variable_scope(\n            vs.get_variable_scope(), custom_getter=self._rnn_get_variable)\n        setattr(self, scope_attrname, scope)\n      with scope:\n        return super(RNNCell, self).__call__(inputs, state)\n\n  def _rnn_get_variable(self, getter, *args, **kwargs):\n    variable = getter(*args, **kwargs)\n    if context.executing_eagerly():\n      trainable = variable._trainable  # pylint: disable=protected-access\n    else:\n      trainable = (\n          variable in tf_variables.trainable_variables() or\n          (isinstance(variable, tf_variables.PartitionedVariable) and\n           list(variable)[0] in tf_variables.trainable_variables()))\n    if trainable and all(variable is not v for v in self._trainable_weights):\n      self._trainable_weights.append(variable)\n    elif not trainable and all(\n        variable is not v for v in self._non_trainable_weights):\n      self._non_trainable_weights.append(variable)\n    return variable\n\n  @property\n  def state_size(self):\n    \"\"\"size(s) of state(s) used by this cell.\n\n    It can be represented by an Integer, a TensorShape or a tuple of Integers\n    or TensorShapes.\n    \"\"\"\n    raise NotImplementedError(\"Abstract method\")\n\n  @property\n  def output_size(self):\n    \"\"\"Integer or TensorShape: size of outputs produced by this cell.\"\"\"\n    raise NotImplementedError(\"Abstract method\")\n\n  def build(self, _):\n    # This tells the parent Layer object that it's OK to call\n    # self.add_variable() inside the call() method.\n    pass\n\n  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n    if inputs is not None:\n      # Validate the given batch_size and dtype against inputs if provided.\n      inputs = ops.convert_to_tensor(inputs, name=\"inputs\")\n      if batch_size is not None:"
},
{
    "Id": 325,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b8c517ab4ef0bd851ef2f8187935fd3a90261af5",
    "Violation": "missing",
    "Bug report": "Reinstate eager check inside _GradientsHelper",
    "Number of deleted lines": 0,
    "Deleted lines": "  else:\n    return op.inputs\n\n\ndef _Consumers(t, func_graphs):\n  \"\"\"Returns the consumers of t, crossing closure boundaries where necessary.\n\n  Args:\n    t: Tensor\n    func_graphs: a list of FuncGraphs that may have captured t.\n\n  Returns:\n    A list of tensors. The tensors will be from the current graph and/or\n    func_graphs.\n  \"\"\"\n  consumers = t.consumers()\n  for func in func_graphs:\n    for input_t, placeholder in _Captures(func).items():\n      if input_t == t:\n        consumers.extend(_Consumers(placeholder, func_graphs))\n  return consumers\n\n\ndef _GradientsHelper(ys,\n                     xs,\n                     grad_ys=None,\n                     name=\"gradients\",\n                     colocate_gradients_with_ops=False,\n                     gate_gradients=False,\n                     aggregation_method=None,\n                     stop_gradients=None,\n                     unconnected_gradients=UnconnectedGradients.NONE,\n                     src_graph=None):\n  \"\"\"Implementation of gradients().\"\"\"\n  if src_graph is None:\n    src_graph = ops.get_default_graph()\n  try:\n    unconnected_gradients = UnconnectedGradients(unconnected_gradients)\n  except ValueError:\n    raise ValueError(\n        \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\n  # If src_graph is a _FuncGraph (i.e. a function body), gather it and all\n  # ancestor graphs. This is necessary for correctly handling captured values.\n  func_graphs = []\n  curr_graph = src_graph\n  while _IsFunction(curr_graph):\n    func_graphs.append(curr_graph)\n    if isinstance(curr_graph, FuncGraph):\n      curr_graph = curr_graph.outer_graph\n    else:\n      assert isinstance(curr_graph, framework_function._FuncGraph)  # pylint: disable=protected-access\n      curr_graph = curr_graph._outer_graph  # pylint: disable=protected-access\n\n  ys = _AsList(ys)\n  xs = _AsList(xs)\n  stop_gradients = [] if stop_gradients is None else _AsList(stop_gradients)\n  if grad_ys is None:\n    grad_ys = [None] * len(ys)\n  else:\n    grad_ys = _AsList(grad_ys)\n\n  with ops.name_scope(\n      name, \"gradients\",\n      list(ys) + list(xs) + list(stop_gradients) + list(grad_ys)) as grad_scope:\n    # Get a uid for this call to gradients that can be used to help\n    # cluster ops for compilation.\n    gradient_uid = ops.get_default_graph().unique_name(\"uid\")\n    ys = ops.convert_n_to_tensor_or_indexed_slices(ys, name=\"y\")\n    xs = ["
},
{
    "Id": 326,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c9b4689bc4d4024aa16b7d6cfc1c65fa1ed8486e",
    "Violation": "improper",
    "Bug report": "Removed no longer supported call to in_eager_execution. Swapped context.in_eager_execution() to the currently supported context.executing_eagerly(). Added negation to eager check. In all likelihood, the negation was always supposed to be there since getting default graph in eager mode does not make sense",
    "Number of deleted lines": 1,
    "Deleted lines": "  elif cmd == 'graph' or cmd == 'scope':\n    tfprof_node = tfprof_output_pb2.GraphNodeProto()\n    ret = print_mdl.PrintModelAnalysis(graph_str, run_meta_str,\n                                       op_log.SerializeToString(),\n                                       cmd.encode('utf-8'),\n                                       opts.SerializeToString())\n    try:\n      tfprof_node.ParseFromString(ret)\n    except message.DecodeError as e:\n      sys.stderr.write('Cannot parse returned proto: %s.\\n' % e)\n  else:\n    raise errors.InvalidArgumentError(\n        None, None, 'unknown cmd: %s\\n' % cmd)\n\n  return tfprof_node\n\n\n@tf_export('profiler.advise')\ndef advise(graph=None, run_meta=None, options=_DEFAULT_ADVISE_OPTIONS):\n  \"\"\"Auto profile and advise.\n\n    Builds profiles and automatically check anomalies of various\n    aspects. For more details:\n    https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/README.md\n\n  Args:\n    graph: tf.Graph. If None and eager execution is not enabled, use\n        default graph.\n    run_meta: optional tensorflow.RunMetadata proto. It is necessary to\n        to support run time information profiling, such as time and memory.\n    options: see ALL_ADVICE example above. Default checks everything.\n  Returns:\n    Returns AdviceProto proto\n  \"\"\"\n  if not graph and context.in_eager_execution():\n    graph = ops.get_default_graph()\n\n  if options == _DEFAULT_ADVISE_OPTIONS:\n    options = ALL_ADVICE.copy()\n\n  # pylint: disable=protected-access\n  op_log = tfprof_logger.merge_default_with_oplog(\n      graph, None, run_meta, add_trace=True)\n  # pylint: enable=protected-access\n\n  run_meta_str = run_meta.SerializeToString() if run_meta else b''\n\n  opts = _build_advisor_options(options)\n  ret = tfprof_output_pb2.AdviceProto()\n  ret.ParseFromString(\n      print_mdl.PrintModelAnalysis(\n          _graph_string(graph), run_meta_str, op_log.SerializeToString(),\n          'advise'.encode('utf-8'), opts.SerializeToString()))\n  return ret\n"
},
{
    "Id": 327,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e5496b556734bb1d8de85311092804e0150b3009",
    "Violation": "unnecessary",
    "Bug report": " Remove extraneous check for Eager mode.The check is already made once at the start of the method",
    "Number of deleted lines": 2,
    "Deleted lines": "    assert_op: An `Operation` that, when executed, raises a\n    `tf.errors.InvalidArgumentError` if `condition` is not true.\n    @compatibility{eager} returns None.\n\n  Raises:\n    @compatibility{eager} `tf.errors.InvalidArgumentError` if `condition`\n    is not true\n  \"\"\"\n  if context.in_eager_mode():\n    if not condition:\n      xs = ops.convert_n_to_tensor(data)\n      data_str = [_summarize_eager(x, summarize) for x in xs]\n      raise errors.InvalidArgumentError(\n          node_def=None,\n          op=None,\n          message=\"Expected '%s' to be true. Summarized data: %s\" %\n          (condition, \"\\n\".join(data_str)))\n    return\n\n  with ops.name_scope(name, \"Assert\", [condition, data]) as name:\n    xs = ops.convert_n_to_tensor(data)\n    if all([x.dtype in {dtypes.string, dtypes.int32} for x in xs]):\n      # As a simple heuristic, we assume that string and int32 are\n      # on host to avoid the need to use cond. If it is not case,\n      # we will pay the price copying the tensor to host memory.\n      return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\n    else:\n      condition = ops.convert_to_tensor(condition, name=\"Condition\")\n\n      def true_assert():\n        return gen_logging_ops._assert(\n            condition, data, summarize, name=\"Assert\")\n\n      guarded_assert = cond(condition, no_op, true_assert, name=\"AssertGuard\")\n      if context.in_eager_mode():\n        return\n      return guarded_assert.op\n\n\ndef _Identity(data, name=None):\n  \"\"\"Return a tensor with the same shape and contents as the input tensor.\n\n  Args:\n    data: A Tensor.\n    name: A name for this operation (optional).\n\n  Returns:\n    A Tensor with the same type and value as the input Tensor.\n  \"\"\"\n  data = ops.internal_convert_to_tensor_or_indexed_slices(data, as_ref=True)\n  if isinstance(data, ops.Tensor):\n    if data.dtype._is_ref_dtype:  # pylint: disable=protected-access\n      return gen_array_ops._ref_identity(data, name=name)\n    else:\n      return array_ops.identity(data, name=name)\n  else:\n    if not isinstance(data, (ops.IndexedSlices, sparse_tensor.SparseTensor)):\n      raise TypeError(\"Type %s not supported\" % type(data))\n    values = _Identity(data.values, name=name)\n    indices = array_ops.identity(data.indices, name=\"indices\")\n    if isinstance(data, ops.IndexedSlices):\n      dense_shape = data.dense_shape\n      if dense_shape is not None:\n        dense_shape = array_ops.identity(dense_shape, name=\"dense_shape\")\n      return ops.IndexedSlices(values, indices, dense_shape)\n    else:\n      dense_shape = array_ops.identity(data.dense_shape, name=\"dense_shape\")\n      return sparse_tensor.SparseTensor(indices, values, dense_shape)\n\n\ndef _NextIteration(data, name=None):\n  data = ops.internal_convert_to_tensor_or_indexed_slices(data, as_ref=True)"
},
{
    "Id": 328,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/be5116dd131a92da298dbb68d26e0d47f66f2fe5",
    "Violation": "improper",
    "Bug report": " Correct graph check in broadcast_to gradient. ",
    "Number of deleted lines": 1,
    "Deleted lines": "  tensor_grad = array_ops.tensor_scatter_update(\n      array_ops.identity(grad), indices,\n      array_ops.zeros_like(op.inputs[2], dtype=grad.dtype))\n  return [tensor_grad, None, updates_grad]\n\n\n@ops.RegisterGradient(\"TensorScatterAdd\")\ndef _TensorScatterAddGrad(op, grad):\n  indices = op.inputs[1]\n  updates_grad = array_ops.gather_nd(grad, indices)\n  tensor_grad = array_ops.identity(grad)\n  return [tensor_grad, None, updates_grad]\n\n\n@ops.RegisterGradient(\"TensorScatterSub\")\ndef _TensorScatterSubGrad(op, grad):\n  indices = op.inputs[1]\n  updates_grad = array_ops.gather_nd(grad, indices)\n  tensor_grad = array_ops.identity(grad)\n  return [tensor_grad, None, -updates_grad]\n\n\n@ops.RegisterGradient(\"ScatterNdNonAliasingAdd\")\ndef _ScatterNdNonAliasingAddGrad(op, grad):\n  indices = op.inputs[1]\n  updates_grad = array_ops.gather_nd(grad, indices)\n  return [grad, None, updates_grad]\n\n\n@ops.RegisterGradient(\"BroadcastTo\")\ndef _BroadcastToGrad(op, grad):\n  input_value = op.inputs[0]\n  broadcast_shape = op.inputs[1]\n  input_value_shape = array_ops.shape(input_value)\n  if not context.executing_eagerly():\n    broadcast_shape_static = tensor_shape.TensorShape(\n        pywrap_tf_session.TF_TryEvaluateConstant_wrapper(\n            broadcast_shape.graph._c_graph, broadcast_shape._as_tf_output()))  # pylint: disable=protected-access\n    if broadcast_shape_static.is_fully_defined():\n      broadcast_shape = constant_op.constant(\n          broadcast_shape_static.as_list(), dtype=dtypes.int32)\n  _, reduction_axes = gen_array_ops.broadcast_gradient_args(\n      broadcast_shape, input_value_shape)\n  updates_grad_reshaped = math_ops.reduce_sum(\n      grad, axis=reduction_axes, keepdims=True)\n  updates_grad = array_ops.reshape(updates_grad_reshaped, input_value_shape)\n  return [updates_grad, None]\n"
},
{
    "Id": 329,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1d6dae88efef68dd7fbeeb5c39ea0f69c1c721c1",
    "Violation": "missing",
    "Bug report": "Add check to tf.device when called with a function in eager mode. ",
    "Number of deleted lines": 0,
    "Deleted lines": "    self._unfeedable_tensors.add(tensor)\n\n  def is_feedable(self, tensor):\n    \"\"\"Returns `True` if and only if `tensor` is feedable.\"\"\"\n    return tensor not in self._unfeedable_tensors\n\n  def prevent_fetching(self, op):\n    \"\"\"Marks the given `op` as unfetchable in this graph.\"\"\"\n    self._unfetchable_ops.add(op)\n\n  def is_fetchable(self, tensor_or_op):\n    \"\"\"Returns `True` if and only if `tensor_or_op` is fetchable.\"\"\"\n    if isinstance(tensor_or_op, Tensor):\n      return tensor_or_op.op not in self._unfetchable_ops\n    else:\n      return tensor_or_op not in self._unfetchable_ops\n\n\n# TODO(agarwal): currently device directives in an outer eager scope will not\n# apply to inner graph mode code. Fix that.\ndef device(device_name_or_function):\n  \"\"\"Wrapper for `Graph.device()` using the default graph.\n\n  See\n  @{tf.Graph.device}\n  for more details.\n\n  Args:\n    device_name_or_function: The device name or function to use in\n      the context.\n\n  Returns:\n    A context manager that specifies the default device to use for newly\n    created ops.\n  \"\"\"\n  if context.in_graph_mode():\n    return get_default_graph().device(device_name_or_function)\n  else:\n    # TODO(agarwal): support device functions in EAGER mode.\n    return context.device(device_name_or_function)\n\n\ndef container(container_name):\n  \"\"\"Wrapper for `Graph.container()` using the default graph.\n\n  Args:\n    container_name: The container string to use in the context.\n\n  Returns:\n    A context manager that specifies the default container to use for newly\n    created stateful ops.\n  \"\"\"\n  return get_default_graph().container(container_name)\n\n\ndef colocate_with(op, ignore_existing=False):\n  if context.in_graph_mode():\n    return get_default_graph().colocate_with(op, ignore_existing)\n  else:\n    if op is not None:\n      return device(op.device)\n    else:\n      return _NullContextmanager()\n\n\ndef control_dependencies(control_inputs):\n  \"\"\"Wrapper for `Graph.control_dependencies()` using the default graph.\n\n  See @{tf.Graph.control_dependencies}\n  for more details.\n\n  Args:\n    control_inputs: A list of `Operation` or `Tensor` objects which\n      must be executed or computed before running the operations\n      defined in the context.  Can also be `None` to clear the control"
},
{
    "Id": 330,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8c3822edbb31cf71cedaf49f2167e45c1e2d0b83",
    "Violation": "missing",
    "Bug report": "Update the is_dtensor check to only run in eager mode.",
    "Number of deleted lines": 0,
    "Deleted lines": "\n    Args:\n      dtensor: The DTensor whose layout is to be fetched.\n\n    Returns:\n      The `Layout` of this DTensor.\n\n    Raises:\n      RuntimeError: When not called eagerly.\n    \"\"\"\n    if not context.executing_eagerly():\n      raise RuntimeError(\"FetchLayout must be called eagerly.\")\n    if issubclass(type(dtensor), resource_variable_ops.BaseResourceVariable):\n      dtensor = dtensor.read_value()\n    try:\n      layout_string = _pywrap_dtensor_device.FetchLayout(\n          context.context()._handle,  # pylint: disable=protected-access\n          dtensor,\n          self._device_info)\n    except core._NotOkStatusException as e:  # pylint: disable=protected-access\n      raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n    return layout_lib.Layout.from_string(layout_string)\n\n  def is_dtensor(self, tensor: Any) -> bool:\n    \"\"\"Check whether the input tensor is a DTensor.\n\n    In Python, a DTensor has the same type as a `tf.Tensor`. This method will\n    let you check and handle the tensor differently if a tf.Tensor is a DTensor.\n\n    Args:\n      tensor: an object to be checked.\n\n    Returns:\n      bool, True if the given tensor is a DTensor.\n    \"\"\"\n    if not tensor_util.is_tensor(tensor):\n      return False\n    if isinstance(tensor, variables.Variable):\n      # Get the resource handle for tf.Variable\n      tensor = tensor._handle   # pylint: disable=protected-access\n    return _pywrap_dtensor_device.IsDTensor(\n        context.context()._handle,  # pylint: disable=protected-access\n        tensor,\n        self._device_info,\n    )\n\n  def set_same_shape_policy(self, enabled):\n    \"\"\"Guess layouts using the layouts of other tensors with the same shape.\n\n    This is the default behavior, and is quite safe. The `default_layout` scope\n    overrides shape-based guesses.\n\n    Args:\n      enabled: A boolean indicating whether to use the policy.\n    \"\"\"\n    _pywrap_dtensor_device.SetSameShapePolicy(self._device_info, enabled)\n\n  def set_tpu_core_ids(self, mesh_name, tpu_core_ids):\n    \"\"\"Sets the singleton global device ID-to-physical core ID map.\n\n    Args:\n      mesh_name: The name of a mesh. If empty, set the default mapping.\n      tpu_core_ids: TPU core IDs sorted by TF task/device ordinal.\n    \"\"\"\n    _pywrap_dtensor_device.SetTPUCoreIDs(self._device_info, mesh_name,\n                                         tpu_core_ids)\n\n  def clear_tpu_core_ids(self):\n    _pywrap_dtensor_device.ClearTPUCoreIDs(self._device_info)\n\n  def tpu_core_ids_to_locations(self, tpu_core_ids):"
},
{
    "Id": 331,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a63f3006f703428ff980748cdbe24d6a13f761e2",
    "Violation": "missing",
    "Bug report": "Skip checking for graph_key in V1 optimizer when running in eager mode.",
    "Number of deleted lines": 1,
    "Deleted lines": "    v = self._non_slot_dict.get(key, None)\n    if v is None:\n      self._maybe_initialize_trackable()\n      distribution_strategy = distribute_ctx.get_strategy()\n      with distribution_strategy.extended.colocate_vars_with(colocate_with):\n        if eager:\n          restored_initial_value = self._preload_simple_restoration(\n              name=name)\n          if restored_initial_value is not None:\n            initial_value = restored_initial_value\n        v = variable_scope.variable(\n            initial_value, name=name, trainable=False,\n            use_resource=resource_variable_ops.is_resource_variable(\n                colocate_with))\n      # Restore this variable by name if necessary, but don't add a\n      # Trackable dependency. Optimizers return the current graph's\n      # non-slot variables from _checkpoint_dependencies explicitly rather\n      # than unconditionally adding dependencies (since there may be multiple\n      # non-slot variables with the same name in different graphs, trying to\n      # save all of them would result in errors).\n      self._handle_deferred_dependencies(name=name, trackable=v)\n      self._non_slot_dict[key] = v\n\n    return v\n\n  def _trackable_children(self,\n                          save_type=trackable.SaveType.CHECKPOINT,\n                          **kwargs):\n    \"\"\"From Trackable. Gather graph-specific non-slot variables to save.\"\"\"\n    current_graph_non_slot_variables = {}\n    current_graph_key = ops.get_default_graph()._graph_key  # pylint: disable=protected-access\n    for (name, _), variable_object in sorted(self._non_slot_dict.items(),\n                                             # Avoid comparing graphs\n                                             key=lambda item: item[0][0]):\n      if variable_object._graph_key == current_graph_key:  # pylint: disable=protected-access\n        current_graph_non_slot_variables[name] = variable_object\n    current_graph_non_slot_variables.update(\n        super(Optimizer, self)._trackable_children(save_type, **kwargs))\n    return current_graph_non_slot_variables\n\n  def _lookup_dependency(self, name):\n    \"\"\"From Trackable. Find a non-slot variable in the current graph.\"\"\"\n    unconditional = super(Optimizer, self)._lookup_dependency(name)\n    if unconditional is not None:\n      return unconditional\n    graph = None if context.executing_eagerly() else ops.get_default_graph()\n    return self._get_non_slot_variable(name, graph=graph)\n\n  def _get_non_slot_variable(self, name, graph=None):\n    non_slot = self._non_slot_dict.get((name, graph), None)\n    if distribute_utils.value_container(non_slot) is not non_slot:\n      # This is a mirrored non-slot.  In order to enable code like `_finish`\n      # to assign to a non-slot, return the current context replica.\n      return non_slot.get()\n    else:\n      return non_slot\n\n  def _non_slot_variables(self):\n    \"\"\"Additional variables created by the `Optimizer`.\n\n    Returns:\n      A list or tuple of variables.\n    \"\"\"\n    return self._non_slot_dict.values()\n\n  def _assert_valid_dtypes(self, tensors):\n    \"\"\"Asserts tensors are all valid types (see `_valid_dtypes`).\n\n    Args:\n      tensors: Tensors to check.\n"
},
{
    "Id": 332,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dd7d791e02396346d98b7b2c58137d7e51756c0c",
    "Violation": "missing",
    "Bug report": "Add isinstance check for eager execution.",
    "Number of deleted lines": 2,
    "Deleted lines": "def get_from_proto_function(collection_name):\n  \"\"\"Returns the from_proto function for collection_name.\"\"\"\n  try:\n    return _proto_function_registry.lookup(collection_name)[2]\n  except LookupError:\n    return None\n\n\ndef _op_to_colocate_with(v, graph):\n  \"\"\"Operation object corresponding to v to use for colocation constraints.\"\"\"\n  if v is None:\n    return None, None\n  if isinstance(v, Operation):\n    return v, None\n\n  # We always want to colocate with the reference op.\n  # When 'v' is a ResourceVariable, the reference op is the handle creating op.\n  #\n  # What this should be is:\n  # if isinstance(v, ResourceVariable):\n  #   return v.handle.op, v\n  # However, that would require a circular import dependency.\n  # As of October 2018, there were attempts underway to remove\n  # colocation constraints altogether. Assuming that will\n  # happen soon, perhaps this hack to work around the circular\n  # import dependency is acceptable.\n  if hasattr(v, \"handle\") and isinstance(v.handle, Tensor):\n    device_only_candidate = lambda: None\n    device_only_candidate.device = v.device\n    device_only_candidate.name = v.name\n    if graph.building_function:\n      return graph.capture(v.handle).op, device_only_candidate\n    else:\n      return v.handle.op, device_only_candidate\n\n  if isinstance(v, internal.NativeObject):\n    return v.op, None\n  else:\n    return convert_to_tensor(v, as_ref=True).op, None\n\n\ndef _is_keras_symbolic_tensor(x):\n  return hasattr(x, \"graph\") and getattr(x.graph, \"name\", None) == \"keras_graph\"\n\n\n# Helper functions for op wrapper modules generated by `python_op_gen`.\n\n\ndef to_raw_op(f):\n  \"\"\"Make a given op wrapper function `f` raw.\n\n  Raw op wrappers can only be called with keyword arguments.\n\n  Args:\n    f: An op wrapper function to make raw.\n\n  Returns:\n    Raw `f`.\n  \"\"\"\n  # Copy `f` to get a new `__dict__`, otherwise `tf_export` will fail\n  # due to double-registration.\n  f = types.FunctionType(f.__code__, f.__globals__, f.__name__, f.__defaults__,\n                         f.__closure__)\n  return kwarg_only(f)\n\n\ndef raise_from_not_ok_status(e, name):\n  e.message += (\" name: \" + name if name is not None else \"\")\n  raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n\n\ndef add_exit_callback_to_default_func_graph(fn):"
},
{
    "Id": 333,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0a9b39caefd437fec742ae48b25061abd6e2699b",
    "Violation": "missing",
    "Bug report": " When allocating GPU constants, check to see if the destination. tensor is intialized early (because we ran out of memory) and report it as such.",
    "Number of deleted lines": 0,
    "Deleted lines": "  const auto stream_id = gpu_device_context->stream_id();\n\n  VLOG(1) << \"GpuDevice::ComputeAsync \" << op_kernel->name() << \" op \"\n          << op_kernel->def().op() << \" on GPU\" << gpu_id_ << \" stream[\"\n          << stream_id << \"]\";\n\n  // When TraceMe profiling is off (which is the default), the\n  // following TraceMe constructor is simply a conditional test of\n  // false value. Measurements show that its overhead is negligible.\n  port::Tracing::TraceMe activity(op_kernel->name(), op_kernel->type_string());\n  op_kernel->ComputeAsync(context, done);\n}\n\nStatus BaseGPUDevice::MakeTensorFromProto(const TensorProto& tensor_proto,\n                                          const AllocatorAttributes alloc_attrs,\n                                          Tensor* tensor) {\n  AllocatorAttributes attr;\n  attr.set_on_host(true);\n  attr.set_gpu_compatible(true);\n  Allocator* host_alloc = GetAllocator(attr);\n  Tensor parsed(tensor_proto.dtype());\n  if (!parsed.FromProto(host_alloc, tensor_proto)) {\n    return errors::InvalidArgument(\"Cannot parse tensor from proto: \",\n                                   tensor_proto.DebugString());\n  }\n  Status status;\n  if (alloc_attrs.on_host()) {\n    *tensor = parsed;\n  } else {\n    if (!DMAHelper::CanUseDMA(&parsed)) {\n      return errors::Internal(\"GPU copy from non-DMA \",\n                              DataTypeString(parsed.dtype()), \" tensor\");\n    }\n    Tensor copy(GetAllocator(alloc_attrs), parsed.dtype(), parsed.shape());\n    port::Tracing::ScopedAnnotation annotation(\"MakeTensorFromProto\");\n    Notification n;\n    device_contexts_[0]->CopyCPUTensorToDevice(&parsed, this, &copy,\n                                               [&n, &status](const Status& s) {\n                                                 status = s;\n                                                 n.Notify();\n                                               });\n    n.WaitForNotification();\n    *tensor = copy;\n  }\n  return status;\n}\n\nnamespace {\nclass ConcretePerOpGpuDevice : public PerOpGpuDevice {\n public:\n  ConcretePerOpGpuDevice() : device_(&stream_device_) {}\n\n  void Reinitialize(OpKernelContext* context, const cudaStream_t* cuda_stream,\n                    int gpu_id, Allocator* base_allocator, char* scratch) {\n    stream_device_.Reinitialize(context, cuda_stream, gpu_id, base_allocator,\n                                scratch);\n  }\n\n  const Eigen::GpuDevice& device() const override { return device_; }\n\n private:\n  EigenCudaStreamDevice stream_device_;\n  Eigen::GpuDevice device_;\n};\n}  // namespace\n\nvoid BaseGPUDevice::ReinitializeDevice(OpKernelContext* context,\n                                       PerOpGpuDevice* device, int stream_id,\n                                       Allocator* allocator) {\n  ConcretePerOpGpuDevice* concrete_device ="
},
{
    "Id": 334,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d8df06a9403b1434aea8b82a193fc30a4ab29bbb",
    "Violation": "missing",
    "Bug report": " Add assert in Operation->printAssembly to check improperly created Op's. We allow the name of an operation to be different from the name of the 'ConcreteType' op it was instantiated with. This can happen when you sub-class an existing op and provide a getOperationName for it. Such a situation leads to an assertion too deep and at a place seeminly unrelated, and typically when the module is printed with the trace: printOperation, printAssembly, Op::print, getOperand, dyn_cast<OperationStmt>, isa. 'isa' will complain about being called on a null pointer, and the null pointer actually comes from the getAs<> in printAssembly. This should have been caught in printAssembly.",
    "Number of deleted lines": 1,
    "Deleted lines": "/// This provides public APIs that all operations should have.  The template\n/// argument 'ConcreteType' should be the concrete type by CRTP and the others\n/// are base classes by the policy pattern.\ntemplate <typename ConcreteType, template <typename T> class... Traits>\nclass Op : public OpState,\n           public Traits<ConcreteType>...,\n           public ConstFoldingHook<\n               ConcreteType,\n               typelist_contains<OpTrait::OneResult<ConcreteType>, OpState,\n                                 Traits<ConcreteType>...>::value> {\npublic:\n  /// Return the operation that this refers to.\n  const Operation *getOperation() const { return OpState::getOperation(); }\n  Operation *getOperation() { return OpState::getOperation(); }\n\n  /// Return true if this \"op class\" can match against the specified operation.\n  /// This hook can be overridden with a more specific implementation in\n  /// the subclass of Base.\n  ///\n  static bool isClassFor(const Operation *op) {\n    return op->getName().getStringRef() == ConcreteType::getOperationName();\n  }\n\n  /// This is the hook used by the AsmParser to parse the custom form of this\n  /// op from an .mlir file.  Op implementations should provide a parse method,\n  /// which returns boolean true on failure.  On success, they should return\n  /// false and fill in result with the fields to use.\n  static bool parseAssembly(OpAsmParser *parser, OperationState *result) {\n    return ConcreteType::parse(parser, result);\n  }\n\n  /// This is the hook used by the AsmPrinter to emit this to the .mlir file.\n  /// Op implementations should provide a print method.\n  static void printAssembly(const Operation *op, OpAsmPrinter *p) {\n    op->getAs<ConcreteType>()->print(p);\n  }\n\n  /// This is the hook that checks whether or not this instruction is well\n  /// formed according to the invariants of its opcode.  It delegates to the\n  /// Traits for their policy implementations, and allows the user to specify\n  /// their own verify() method.\n  ///\n  /// On success this returns false; on failure it emits an error to the\n  /// diagnostic subsystem and returns true.\n  static bool verifyInvariants(const Operation *op) {\n    return BaseVerifier<Traits<ConcreteType>...>::verifyTrait(op) ||\n           op->getAs<ConcreteType>()->verify();\n  }\n\n  // TODO: Provide a dump() method.\n\nprotected:\n  explicit Op(const Operation *state) : OpState(state) {}\n\nprivate:\n  template <typename... Types> struct BaseVerifier;\n\n  template <typename First, typename... Rest>\n  struct BaseVerifier<First, Rest...> {\n    static bool verifyTrait(const Operation *op) {\n      return First::verifyTrait(op) || BaseVerifier<Rest...>::verifyTrait(op);\n    }\n  };\n\n  template <typename First> struct BaseVerifier<First> {\n    static bool verifyTrait(const Operation *op) {\n      return First::verifyTrait(op);\n    }\n  };\n\n  template <> struct BaseVerifier<> {"
},
{
    "Id": 335,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4f38b1ac8e42727e18a2f0bde06d3bee8e77b250",
    "Violation": "missing",
    "Bug report": " Prevent null dereference read in GetInitOp. We have a map of maps. We test that the key exists in the first map but then we don't have any validation that this also means the second map has the needed key. In the scenariosc where this is not the case, we'll dereference a nullptr, if we don't have this chec",
    "Number of deleted lines": 3,
    "Deleted lines": "Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/cc/saved_model/loader_util.h\"\n\n#include <vector>\n\n#include \"tensorflow/cc/saved_model/constants.h\"\n#include \"tensorflow/core/lib/strings/strcat.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/protobuf_internal.h\"\n\nnamespace tensorflow {\nnamespace internal {\n\n// A SavedModel may store the name of the initialization op to run in the\n// in the SignatureDef (v2) or a collection (v1). If an init_op collection\n// exists, then the collection must contain exactly one op.\nStatus GetInitOp(const string& export_dir, const MetaGraphDef& meta_graph_def,\n                 string* init_op_name) {\n  const auto& sig_def_map = meta_graph_def.signature_def();\n  const auto& init_op_sig_it =\n      meta_graph_def.signature_def().find(kSavedModelInitOpSignatureKey);\n  if (init_op_sig_it != sig_def_map.end()) {\n    *init_op_name = init_op_sig_it->second.outputs()\n                        .find(kSavedModelInitOpSignatureKey)\n                        ->second.name();\n    return Status::OK();\n  }\n\n  const auto& collection_def_map = meta_graph_def.collection_def();\n  string init_op_collection_key;\n  if (collection_def_map.find(kSavedModelMainOpKey) !=\n      collection_def_map.end()) {\n    init_op_collection_key = kSavedModelMainOpKey;\n  } else {\n    init_op_collection_key = kSavedModelLegacyInitOpKey;\n  }\n\n  const auto init_op_it = collection_def_map.find(init_op_collection_key);\n  if (init_op_it != collection_def_map.end()) {\n    if (init_op_it->second.node_list().value_size() != 1) {\n      return errors::FailedPrecondition(\n          strings::StrCat(\"Expected exactly one main op in : \", export_dir));\n    }\n    *init_op_name = init_op_it->second.node_list().value(0);\n  }\n  return Status::OK();\n}\n\nStatus GetAssetFileDefs(const MetaGraphDef& meta_graph_def,\n                        std::vector<AssetFileDef>* asset_file_defs) {\n  // With SavedModel v2, we write asset file def into metagraph instead of\n  // collection, so read from metagraph first.\n  if (meta_graph_def.asset_file_def_size() > 0) {\n    for (const auto& asset : meta_graph_def.asset_file_def()) {\n      asset_file_defs->push_back(asset);\n    }\n    return Status::OK();\n  }\n  // Fall back to read from collection to be backward compatible with v1.\n  const auto& collection_def_map = meta_graph_def.collection_def();\n  const auto assets_it = collection_def_map.find(kSavedModelAssetsKey);"
},
{
    "Id": 336,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e7de472681079932b2547024f31c876da54f61a0",
    "Violation": "insufficient",
    "Bug report": " Fix a bug in flatbuffer importer that use tensor quantization before checking.",
    "Number of deleted lines": 1,
    "Deleted lines": "  if (element_ty.isSignlessInteger())\n    return DenseElementsAttr::get(\n        type, builder.getIntegerAttr(element_ty, unique_index));\n\n  if (element_ty.isa<mlir::FloatType>())\n    return DenseElementsAttr::get(\n        type, builder.getFloatAttr(element_ty, unique_index));\n\n  if (auto qtype = element_ty.dyn_cast<QuantizedType>()) {\n    mlir::RankedTensorType new_type =\n        RankedTensorType::get(type.getShape(), qtype.getStorageType());\n    return DenseElementsAttr::get(\n        new_type, builder.getIntegerAttr(qtype.getStorageType(), unique_index));\n  }\n  llvm_unreachable(\"unhandled element type\");\n}\n\n// TODO(b/172664358): Creates a new op instead of reusing constant op.\n// Creates a constant op to represent stateful variable. The function static\n// variable `stateful_variable_idx` is used as a unique value for each constant\n// to avoid CSEed. `tensor` is the data structure of flatbuffer. `shaped_type`\n// is the ShapedType for the const op.\nOperation* BuildVariableOp(const tflite::TensorT& tensor,\n                           mlir::RankedTensorType shaped_type,\n                           OpBuilder builder, Location loc) {\n  static int stateful_variable_idx = 0;\n  mlir::ElementsAttr value =\n      GetSplat(shaped_type, stateful_variable_idx++, builder);\n  if (IsQuantized(tensor)) {\n    auto op = builder.create<tfl::QConstOp>(\n        loc, mlir::TypeAttr::get(shaped_type), value);\n    return op.getOperation();\n  }\n  auto op = builder.create<tfl::ConstOp>(loc, value);\n  if (!tensor.quantization->min.empty()) {\n    if (auto stats_op =\n            ConvertMinMaxToStatsOp(tensor, builder, op.getResult())) {\n      return stats_op;\n    }\n  }\n  return op.getOperation();\n}\n\nStatusOr<Operation*> BuildConstOp(const tflite::TensorT& tensor,\n                                  const std::vector<uint8_t>& buffer,\n                                  bool is_variable, OpBuilder builder,\n                                  Location loc) {\n  TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n                                               /*shapeless_are_scalars=*/true,\n                                               /*is_constant=*/true));\n  auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n  if (!shaped_type) {\n    return errors::Internal(\"Constant doesn't have a shape\");\n  }\n\n  auto elem_type = shaped_type.getElementType();\n\n  mlir::ElementsAttr value;\n  if (is_variable) {\n    return BuildVariableOp(tensor, shaped_type, builder, loc);\n  } else if (auto float_type = elem_type.dyn_cast<mlir::FloatType>()) {\n    TF_ASSIGN_OR_RETURN(value,\n                        ConvertFloatBuffer(shaped_type, float_type, buffer));\n  } else if (elem_type.isa<mlir::IntegerType, QuantizedType>()) {\n    TF_ASSIGN_OR_RETURN(value,\n                        ConvertIntBuffer(shaped_type, elem_type, buffer));\n  } else if (elem_type.isa<mlir::TF::StringType>()) {\n    tensorflow::TensorProto repr = ConvertTfliteConstTensor(tensor, buffer);\n    std::vector<llvm::StringRef> refs;\n    refs.reserve(repr.string_val_size());\n"
},
{
    "Id": 337,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/19b2e1b5868a044df4622ef7e26fa5570ca52e5e",
    "Violation": "insufficient",
    "Bug report": "Only perform scalar check for a tensor shape if it's not empty.",
    "Number of deleted lines": 1,
    "Deleted lines": "\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include \"tensorflow/compiler/tf2tensorrt/convert/weights.h\"\n\n#include <functional>\n#include <numeric>\n\n#include \"absl/strings/str_cat.h\"\n#include \"tensorflow/compiler/tf2tensorrt/convert/utils.h\"\n\n#if GOOGLE_CUDA && GOOGLE_TENSORRT\n\nnamespace tensorflow {\nnamespace tensorrt {\n\nnamespace convert {\n\nTRT_ShapedWeights::TRT_ShapedWeights(nvinfer1::DataType type)\n    : shape_(0, DimsAdapter::StorageType{}), type_(type), volume_(0) {}\n\nStatusOr<TRT_ShapedWeights> TRT_ShapedWeights::CreateWithTensor(\n    nvinfer1::DataType type, DimsAdapter dims, Tensor tensor) {\n  TRT_ShapedWeights weights(type);\n  weights.shape_ = dims;\n  weights.tensor_ = std::forward<Tensor>(tensor);\n  weights.volume_ = weights.shape_.Volume();\n  if (weights.shape_.NumDims() == 0) {\n    DCHECK(weights.shape_.IsScalar());\n  }\n  return weights;\n}\n\nnvinfer1::Weights TRT_ShapedWeights::GetTrtWeights() const {\n  return nvinfer1::Weights{type_, GetPointer<int8>(), volume_};\n}\n\nStatus TRT_ShapedWeights::SetShape(DimsAdapter dims) {\n  if (volume_ != dims.Volume()) {\n    VLOG(2) << \"Changing shape from \" << shape_.DebugString() << \", to \"\n            << dims.DebugString();\n    return errors::Internal(\"SetShape would change number of elements\");\n  }\n  shape_ = std::move(dims);\n  return Status::OK();\n}\n\nsize_t TRT_ShapedWeights::size_bytes() const {\n  size_t data_type_size = -1;\n  switch (type_) {\n    case nvinfer1::DataType::kFLOAT:\n    case nvinfer1::DataType::kINT32:\n      data_type_size = 4;\n      break;\n    case nvinfer1::DataType::kHALF:\n      data_type_size = 2;\n      break;\n    case nvinfer1::DataType::kINT8:\n    case nvinfer1::DataType::kBOOL:\n      data_type_size = 1;\n      break;\n  }\n  return volume_ * data_type_size;\n}\n"
},
{
    "Id": 338,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9c92b50fc4b95985a0749101976d04896bf19bfe",
    "Violation": "improper",
    "Bug report": " [conv3d_transpose] Fix dim check for bias. Per discussion with @thaink, the previous way to do the dim check for bias is not correct. So we need this change.",
    "Number of deleted lines": 1,
    "Deleted lines": "      reinterpret_cast<TfLiteConv3DTransposeParams*>(node->builtin_data);\n  OpData* opdata = reinterpret_cast<OpData*>(node->user_data);\n  // Check number of inputs/outputs.\n  TF_LITE_ENSURE(context, node->inputs->size == 3 || node->inputs->size == 4);\n  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));\n  const TfLiteTensor* output_shape;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &output_shape));\n  const TfLiteTensor* filter;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &input));\n\n  // Check dimensionality of inputs/outputs.\n  TF_LITE_ENSURE_EQ(context, output_shape->dims->size, 1);\n  TF_LITE_ENSURE_EQ(context, NumElements(output_shape), 5);\n  TF_LITE_ENSURE_EQ(context, input->dims->size, 5);\n  TF_LITE_ENSURE_EQ(context, filter->dims->size, 5);\n\n  // Input and filter must have the same number of channels.\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(input, 4),\n                    SizeOfDimension(filter, 4));\n\n  // Check types.\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, kTfLiteFloat32);\n  TF_LITE_ENSURE_TYPES_EQ(context, filter->type, kTfLiteFloat32);\n  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input->type);\n  TF_LITE_ENSURE_TYPES_EQ(context, output_shape->type, kTfLiteInt32);\n\n  // Check bias.\n  const TfLiteTensor* bias = GetInput(context, node, 3);\n  if (bias) {\n    TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input->type);\n    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 4));\n  }\n\n  // GenericOptimized kernel currently doesn't support dilation.\n  if (params->dilation_depth_factor > 1 || params->dilation_height_factor > 1 ||\n      params->dilation_width_factor > 1) {\n    kernel_type = kReference;\n  }\n\n  // Allocate temporary tensors.\n  TF_LITE_ENSURE_STATUS(\n      AllocateTemporaryTensorsIfRequired(context, node, kernel_type));\n\n  // Check temporary tensors.\n  TfLiteTensor* col2im = nullptr;\n  if (opdata->need_col2im) {\n    node->temporaries->data[opdata->col2im_index] = opdata->col2im_id;\n    TF_LITE_ENSURE_OK(context, GetTemporarySafe(context, node,\n                                                opdata->col2im_index, &col2im));\n  }\n\n  // Resize the output tensor.\n  if (!IsConstantTensor(output_shape)) {\n    SetTensorToDynamic(output);\n    if (opdata->need_col2im) {\n      SetTensorToDynamic(col2im);\n    }\n  } else {\n    TF_LITE_ENSURE_STATUS(ResizeOutputAndTemporaryTensors(\n        context, opdata, params, output_shape, filter, input, col2im, output));\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  return Prepare(kernel_type, context, node);"
},
{
    "Id": 339,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5bc536f1afbaff5d3d5a14a9185cd1e3cc31b302",
    "Violation": "improper",
    "Bug report": "[Fix] bug fix during check static shape.",
    "Number of deleted lines": 1,
    "Deleted lines": "#include \"tensorflow/compiler/mlir/lite/transforms/passes.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/attribute_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/validators.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/utils/mangling_util.h\"\n#include \"tensorflow/compiler/xla/status.h\"\n#include \"tensorflow/compiler/xla/statusor.h\"\n#include \"tensorflow/core/framework/tensor.pb.h\"\n#include \"tensorflow/core/framework/tensor_shape.pb.h\"\n#include \"tensorflow/core/framework/types.pb.h\"\n#include \"tensorflow/core/protobuf/error_codes.pb.h\"\n\nnamespace mlir {\nnamespace TFL {\n\n//===----------------------------------------------------------------------===//\n// The actual LegalizeTF Pass.\nnamespace {\n\nusing xla::Status;\nusing xla::StatusOr;\n\n// Legalize operations in functions.\nstruct LegalizeTF : public FunctionPass<LegalizeTF> {\n  void runOnFunction() override;\n};\n\n// Returns true if all tensor value in `values` has static shape and same shape.\nbool HasSameStaticShapes(Operation* op) {\n  auto values = op->getOperands();\n  int index = 0;\n  ArrayRef<int64_t> shape;\n  for (Value value : values) {\n    auto shaped_type = value.getType().dyn_cast<ShapedType>();\n    if (!shaped_type && !shaped_type.hasStaticShape()) {\n      return false;\n    }\n    if (index == 0) {\n      shape = shaped_type.getShape();\n    } else {\n      if (shape != shaped_type.getShape()) {\n        return false;\n      }\n    }\n    ++index;\n  }\n  return true;\n}\n\n#include \"tensorflow/compiler/mlir/lite/transforms/generated_legalize_tf.inc\"\n\n#define DECL_CONVERT_OP(tf_op)                                             \\\n  struct ConvertTF##tf_op##Op : public RewritePattern {                    \\\n    explicit ConvertTF##tf_op##Op(MLIRContext* context)                    \\\n        : RewritePattern(TF::tf_op##Op::getOperationName(), 1, context) {} \\\n    PatternMatchResult matchAndRewrite(                                    \\\n        Operation* op, PatternRewriter& rewriter) const override;          \\\n  }\n\n// TODO(antiagainst): Define this pattern in a table-driven manner once variadic\n// operands are properly supported in declarative rewrite rule specification.\n\nDECL_CONVERT_OP(Assert);\nDECL_CONVERT_OP(Concat);\nDECL_CONVERT_OP(ConcatV2);\nDECL_CONVERT_OP(MatMul);\nDECL_CONVERT_OP(MatrixDiagV2);\nDECL_CONVERT_OP(MatrixDiagV3);\nDECL_CONVERT_OP(Pack);\nDECL_CONVERT_OP(Reshape);\nDECL_CONVERT_OP(Split);"
},
{
    "Id": 340,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/264eb6ed1dbfb5e078c7dd977da8d7e633106fc5",
    "Violation": "missing",
    "Bug report": " Fixed add bias transformation. Added check for convolution with dynamic weights.",
    "Number of deleted lines": 0,
    "Deleted lines": "\n#include \"absl/memory/memory.h\"\n#include \"absl/strings/str_cat.h\"\n#include \"absl/types/any.h\"\n#include \"tensorflow/lite/delegates/gpu/common/data_type.h\"\n#include \"tensorflow/lite/delegates/gpu/common/model.h\"\n#include \"tensorflow/lite/delegates/gpu/common/operations.h\"\n#include \"tensorflow/lite/delegates/gpu/common/status.h\"\n\nnamespace tflite {\nnamespace gpu {\nnamespace {\n\nTransformResult FillBias(\n    int output_channels,\n    tflite::gpu::Tensor<Linear, DataType::FLOAT32>* biases) {\n  if (biases->data.empty()) {\n    *biases =\n        MakeZeroTensor<Linear, DataType::FLOAT32>(Linear(output_channels));\n    return {TransformStatus::APPLIED, \"Added bias\"};\n  }\n  if (biases->shape.v != output_channels) {\n    float last_value = biases->data.back();\n    biases->shape.v = output_channels;\n    biases->data.resize(output_channels, last_value);\n    return {TransformStatus::APPLIED, \"Bias extended\"};\n  }\n  return {TransformStatus::SKIPPED, \"\"};\n}\n\nclass AddBias : public NodeTransformation {\n public:\n  TransformResult ApplyToNode(Node* node, GraphFloat32* graph) final {\n    if (node->operation.type == ToString(OperationType::CONVOLUTION_2D)) {\n      auto& attr =\n          absl::any_cast<Convolution2DAttributes&>(node->operation.attributes);\n      return FillBias(attr.weights.shape.o, &attr.bias);\n    }\n    if (node->operation.type ==\n        ToString(OperationType::CONVOLUTION_TRANSPOSED)) {\n      auto& attr = absl::any_cast<ConvolutionTransposedAttributes&>(\n          node->operation.attributes);\n      return FillBias(attr.weights.shape.o, &attr.bias);\n    }\n    if (node->operation.type ==\n        ToString(OperationType::DEPTHWISE_CONVOLUTION)) {\n      auto& attr = absl::any_cast<DepthwiseConvolution2DAttributes&>(\n          node->operation.attributes);\n      return FillBias(attr.weights.shape.o * attr.weights.shape.i, &attr.bias);\n    }\n    if (node->operation.type == ToString(OperationType::FULLY_CONNECTED)) {\n      auto& attr =\n          absl::any_cast<FullyConnectedAttributes&>(node->operation.attributes);\n      return FillBias(attr.weights.shape.o, &attr.bias);\n    }\n    return {TransformStatus::SKIPPED, \"\"};\n  }\n};\n\n}  // namespace\n\nstd::unique_ptr<NodeTransformation> NewAddBias() {\n  return absl::make_unique<AddBias>();\n}\n\n}  // namespace gpu\n}  // namespace tflite\n"
},
{
    "Id": 341,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8cef4cda26e08256b6698e942820d9a3ac1bcc94",
    "Violation": "missing",
    "Bug report": "Add minor checks for data_format and padding value ",
    "Number of deleted lines": 2,
    "Deleted lines": "  if (perm.empty()) return failure();\n\n  // Update data_format attribute and result types.\n  if (failed(::mlir::TF::UpdateDataFormat(data_format, this))) return failure();\n\n  // Update convolution attributes.\n  setAttr(\"dilations\", ShuffleArrayAttr(dilations(), perm));\n  setAttr(\"strides\", ShuffleArrayAttr(strides(), perm));\n  setAttr(\"explicit_paddings\", ShuffleArrayAttr(explicit_paddings(), perm, 2));\n\n  return success();\n}\n\n// Verifies the inferred return type of the given operation.\ntemplate <typename OpT,\n          typename std::enable_if<llvm::is_one_of<\n              OpT, Conv2DOpAdaptor, Conv3DOpAdaptor>::value>::type* = nullptr>\nstatic LogicalResult inferConvReturnTypes(\n    OpT op, llvm::SmallVectorImpl<mlir::Type>& inferredReturnTypes,\n    llvm::Optional<mlir::Location> location,\n    ArrayRef<Attribute> explicit_padding) {\n  const int64_t num_spatial_dims = std::is_same<OpT, Conv2DOpAdaptor>() ? 2 : 3;\n  const int64_t num_dims = 2 + num_spatial_dims;\n  const Value input = op.input();\n  const Value filter = op.filter();\n  const TensorType input_ty = input.getType().template cast<TensorType>();\n  const TensorType filter_ty = filter.getType().template cast<TensorType>();\n  const StringRef paddings = op.padding().getValue();\n\n  ArrayRef<Attribute> strides = op.strides().getValue();\n  StringRef data_format = op.data_format().getValue();\n  ArrayRef<Attribute> dilations = op.dilations().getValue();\n\n  tensorflow::TensorFormat format;\n  FormatFromString(data_format.str(), &format);\n  tensorflow::Padding padding;\n  GetPaddingFromString(paddings.str(), &padding);\n  auto get_int = [](Attribute attr) {\n    return attr.template cast<IntegerAttr>().getInt();\n  };\n\n  // Necessary sanity checks.\n  // Verifies that,\n  // * Ranks of operands and result are valid\n  // * Length of explicit_paddings attribute is valid and has non negative\n  //   elements\n  // * strides and dilations attributes have positive elements\n  if (!IsOfRankOrUnranked(input, num_dims) ||\n      !IsOfRankOrUnranked(filter, num_dims))\n    return emitOptionalError(location, \"requires operands to be \", num_dims,\n                             \"D tensor\");\n\n  if (padding == tensorflow::Padding::EXPLICIT) {\n    if (explicit_padding.size() == 0) {\n      return emitOptionalError(location,\n                               \"requires attribute 'explicit_paddings' with \"\n                               \"'EXPLICIT' padding mode\");\n    }\n    if (explicit_padding.size() != num_dims * 2) {\n      return emitOptionalError(\n          location, \"requires explicit_paddings attribute length to be \",\n          num_dims * 2);\n    }\n    auto is_negative = [](Attribute val) {\n      return val.cast<IntegerAttr>().getValue().getSExtValue() < 0;\n    };\n    if (llvm::any_of(explicit_padding, is_negative))\n      return emitOptionalError(location,\n                               \"requires non negative explicit paddings\");\n  }\n\n  if (failed(VerifyConvOpAttributes(num_dims, strides, dilations, location))) {\n    return failure();"
},
{
    "Id": 342,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0d5668cbdc6b46d099bd3abd93374c09b2e8121f",
    "Violation": "improper",
    "Bug report": " [XLA:SHAPE_UTIL] Return nullopt instead of a check failure if the input dimensions are not sorted.",
    "Number of deleted lines": 1,
    "Deleted lines": "      return nil;\n    }\n  }\n\n  return std::make_tuple(true, deleted_indices, inserted_indices);\n}\n\n/* static */ std::vector<std::pair<int64_t, int64_t>>\nShapeUtil::DimensionsUnmodifiedByReshape(const Shape& input_shape,\n                                         const Shape& output_shape) {\n  CHECK(input_shape.IsArray());\n  CHECK(output_shape.IsArray());\n\n  // Unmodified dimensions are merely common factors of rank 1.\n  auto common_factors =\n      CommonFactors(input_shape.dimensions(), output_shape.dimensions());\n  for (size_t i = 0; i < common_factors.size() - 1;) {\n    if (1 != common_factors[i + 1].first - common_factors[i].first ||\n        1 != common_factors[i + 1].second - common_factors[i].second) {\n      common_factors.erase(common_factors.begin() + i);\n    } else {\n      ++i;\n    }\n  }\n  // `CommonFactors(a, b).back() == (a.rank, b.rank)` so we must pop it.\n  common_factors.pop_back();\n  return std::vector<std::pair<int64_t, int64_t>>(common_factors.begin(),\n                                                  common_factors.end());\n}\n\n/* static */ absl::optional<std::vector<int64_t>>\nShapeUtil::ReshapeLeavesDimensionsUnmodified(\n    const Shape& from_shape, const Shape& to_shape,\n    absl::Span<const int64_t> input_dim_indices) {\n  CHECK(std::is_sorted(input_dim_indices.begin(), input_dim_indices.end()));\n\n  std::vector<int64_t> output_dim_indices;\n  std::vector<std::pair<int64_t, int64_t>> unmodified_dims =\n      ShapeUtil::DimensionsUnmodifiedByReshape(from_shape, to_shape);\n  size_t i = 0;  // index to unmodified_dims\n  for (int64_t input_dim_index : input_dim_indices) {\n    // Search unmodified_dims for input_dim_index. We can search from the last\n    // matching position because input_dim_indices is guaranteed to be sorted.\n    while (i < unmodified_dims.size() &&\n           unmodified_dims[i].first < input_dim_index) {\n      ++i;\n    }\n    if (i >= unmodified_dims.size() ||\n        unmodified_dims[i].first != input_dim_index) {\n      return absl::nullopt;\n    }\n    output_dim_indices.push_back(unmodified_dims[i].second);\n  }\n  return output_dim_indices;\n}\n\n/* static */ bool ShapeUtil::TransposeIsBitcast(\n    const Shape& input_shape, const Shape& output_shape,\n    absl::Span<const int64_t> dimension_mapping) {\n  CHECK(LayoutUtil::HasLayout(input_shape) &&\n        LayoutUtil::HasLayout(output_shape));\n\n  if (!SameElementType(input_shape, output_shape)) {\n    return false;\n  }\n\n  // Check the reshape permutes the positions of each dimension in the\n  // minor-to-major order. positions[i]=k means dimension `i` is k-th minor.\n  //   input_positions = apply(dimension_mapping, output_positions)\n  //\n  // Because the positions of each dimension are the inverse permutation of the"
},
{
    "Id": 343,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/824af2acfa0cdf897c08d91224aea0958c1afc02",
    "Violation": "missing",
    "Bug report": " Add ndmin check. Added ndmin check to allow maximum 32 ndmin to make same behavior as numpy. Currently it is crashing when very large ndmin is passed.",
    "Number of deleted lines": 0,
    "Deleted lines": "    raise ValueError('Overriding the shape is not supported.')\n\n  a = asarray(a)\n  dtype = dtype or np_utils.result_type(a)\n  fill_value = asarray(fill_value, dtype=dtype)\n  return array_ops.broadcast_to(fill_value, array_ops.shape(a))\n\n\ndef _array_internal(val, dtype=None, copy=True, ndmin=0):  # pylint: disable=redefined-outer-name\n  \"\"\"Main implementation of np.array().\"\"\"\n  result_t = val\n\n  if not isinstance(result_t, ops.Tensor):\n    dtype = np_utils.result_type_unary(result_t, dtype)\n    # We can't call `convert_to_tensor(result_t, dtype=dtype)` here because\n    # convert_to_tensor doesn't allow incompatible arguments such as (5.5, int)\n    # while np.array allows them. We need to convert-then-cast.\n\n    # EagerTensor conversion complains about \"mixed types\" when converting\n    # tensors with no dtype information. This is because it infers types based\n    # on one selected item in the list. So e.g. when converting [2., 2j]\n    # to a tensor, it will select float32 as the inferred type and not be able\n    # to convert the list to a float 32 tensor.\n    # Since we have some information about the final dtype we care about, we\n    # supply that information so that convert_to_tensor will do best-effort\n    # conversion to that dtype first.\n    result_t = np_arrays.convert_to_tensor(result_t, dtype_hint=dtype)\n    result_t = math_ops.cast(result_t, dtype=dtype)\n  elif dtype:\n    result_t = math_ops.cast(result_t, dtype)\n\n  if copy:\n    result_t = array_ops.identity(result_t)\n\n  if ndmin == 0:\n    return result_t\n\n  ndims = array_ops.rank(result_t)\n\n  def true_fn():\n    old_shape = array_ops.shape(result_t)\n    new_shape = array_ops.concat(\n        [array_ops.ones(ndmin - ndims, dtypes.int32), old_shape], axis=0)\n    return array_ops.reshape(result_t, new_shape)\n\n  result_t = np_utils.cond(\n      np_utils.greater(ndmin, ndims), true_fn, lambda: result_t)\n  return result_t\n\n\n# TODO(wangpeng): investigate whether we can make `copy` default to False.\n# pylint: disable=g-short-docstring-punctuation,g-no-space-after-docstring-summary,g-doc-return-or-yield,g-doc-args\n@np_utils.np_doc_only('array')\ndef array(val, dtype=None, copy=True, ndmin=0):  # pylint: disable=redefined-outer-name\n  \"\"\"Since Tensors are immutable, a copy is made only if val is placed on a\n\n  different device than the current one. Even if `copy` is False, a new Tensor\n  may need to be built to satisfy `dtype` and `ndim`. This is used only if `val`\n  is an ndarray or a Tensor.\n  \"\"\"  # pylint:disable=g-docstring-missing-newline\n  if dtype:\n    dtype = np_utils.result_type(dtype)\n  return _array_internal(val, dtype, copy, ndmin)\n\n\n# pylint: enable=g-short-docstring-punctuation,g-no-space-after-docstring-summary,g-doc-return-or-yield,g-doc-args\n\n\n@np_utils.np_doc('asarray')\ndef asarray(a, dtype=None):"
},
{
    "Id": 344,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b73a3c21a224f479af8d3b8af320c220a091906c",
    "Violation": "missing",
    "Bug report": "[XLA] Add check for potential out-of-bound access.",
    "Number of deleted lines": 0,
    "Deleted lines": "    }\n    default:\n      return InvalidArgument(\"Unsupported type for Sort: %s\",\n                             PrimitiveType_Name(type));\n  }\n}\n}  // namespace\n\nStatus HloEvaluator::HandleSort(HloInstruction* sort) {\n  TF_RET_CHECK(sort->operand_count() >= 1)\n      << \"Expected at least 1 operand for sort\";\n  for (int64_t i = 1; i < sort->operand_count(); ++i) {\n    TF_RET_CHECK(ShapeUtil::SameDimensions(sort->operand(0)->shape(),\n                                           sort->operand(i)->shape()))\n        << \"All Sort operands must have the same dimensions\";\n  }\n\n  if (VLOG_IS_ON(3)) {\n    for (int64_t i = 0; i < sort->operand_count(); ++i) {\n      VLOG(3) << \"HandleSort operand \" << i << \" literal: \"\n              << GetEvaluatedLiteralFor(sort->operand(i)).ToString();\n    }\n  }\n  Shape key_shape = sort->operand(0)->shape();\n  auto rank = key_shape.rank();\n  std::vector<Literal> result_literals;\n  result_literals.reserve(sort->operand_count());\n  for (int64_t i = 0; i < sort->operand_count(); ++i) {\n    result_literals.emplace_back(sort->operand(i)->shape());\n  }\n  std::vector<int64_t> zero_base(rank, 0);\n  std::vector<int64_t> increment(rank, 1);\n  int64_t sort_dim = sort->dimensions(0);\n  int64_t sort_dim_elements = key_shape.dimensions(sort_dim);\n  increment[sort_dim] = sort_dim_elements;\n  HloEvaluator embedded_evaluator(max_loop_iterations_);\n  // Iterate through each dimension except 'sort_dim'.\n  TF_RETURN_IF_ERROR(ShapeUtil::ForEachIndexWithStatus(\n      key_shape, zero_base, AsInt64Slice(key_shape.dimensions()), increment,\n      [&](absl::Span<const int64_t> indices) -> StatusOr<bool> {\n        // Extract a slice from each operand literal that corresponds to\n        // exactly the row in dimension 'sort_dim'.\n        std::vector<int64_t> limit_indices(indices.begin(), indices.end());\n        absl::c_for_each(limit_indices, [](int64_t& index) { ++index; });\n        limit_indices[sort_dim] = sort_dim_elements;\n        std::vector<Literal> literals_to_sort;\n        literals_to_sort.reserve(sort->operand_count());\n        for (int64_t i = 0; i < sort->operand_count(); ++i) {\n          TF_ASSIGN_OR_RETURN(auto literal_to_sort,\n                              GetEvaluatedLiteralFor(sort->operand(i))\n                                  .Slice(indices, limit_indices)\n                                  .Reshape({sort_dim_elements}));\n          literals_to_sort.push_back(std::move(literal_to_sort));\n        }\n        std::vector<int64_t> indices_to_sort(sort_dim_elements);\n        std::iota(indices_to_sort.begin(), indices_to_sort.end(), 0);\n        Status compare_status = Status::OK();\n        auto comparator = [sort, &compare_status, &embedded_evaluator,\n                           &literals_to_sort](int64_t a, int64_t b) {\n          std::vector<Literal> literals;\n          literals.reserve(2 * sort->operand_count());\n          for (int64_t i = 0; i < sort->operand_count(); ++i) {\n            auto lhs = ExtractFromIndexPositions(literals_to_sort[i], {a},\n                                                 /*extract_as_scalar=*/true);\n            if (!lhs.ok()) {\n              compare_status = lhs.status();\n              return false;\n            }\n            literals.push_back(std::move(lhs.ValueOrDie()));\n            auto rhs = ExtractFromIndexPositions(literals_to_sort[i], {b},"
},
{
    "Id": 345,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/63753d5f1531b17cf8cbbf1d8b77c16edcfb9711",
    "Violation": "improper",
    "Bug report": " Change DCHECK_LE to DCHECK_LT when checking invariant on original indices for sorted items Indices of items should be strictly smaller than the size of the vector.",
    "Number of deleted lines": 1,
    "Deleted lines": "        tensorflow::kImportModelDefaultGraphFuncName);\n    resource_context = loading_result.resource_context.get();\n  } else {\n    func = bef_file_->GetFunction({name.data(), name.size()});\n    resource_context = resource_context_.get();\n  }\n  DCHECK(func);\n\n  return RunInternal(run_options, name, *func, inputs, captures, outputs,\n                     resource_context);\n}\n\nnamespace {\n\n// Sort the strings in `names` and store the results in `sorted_names`. In\n// addition, the original index in `names` for the item `sorted_names[i]` is\n// stored in `original_indices[i]`.\nvoid CreateSortedNamesAndOriginalIndices(absl::Span<const std::string> names,\n                                         std::vector<std::string>& sorted_names,\n                                         std::vector<int>& original_indices) {\n  DCHECK(sorted_names.empty());\n  DCHECK(original_indices.empty());\n\n  // Generate indices.\n  original_indices.resize(names.size());\n  std::iota(original_indices.begin(), original_indices.end(), 0);\n\n  // Sort indices by comparing the corresponding names.\n  std::sort(original_indices.begin(), original_indices.end(),\n            [&](int x, int y) { return names[x] < names[y]; });\n\n  // Use sorted indices to generate sorted names.\n  sorted_names.reserve(names.size());\n  for (int original_index : original_indices) {\n    DCHECK_LE(original_index, names.size());\n    sorted_names.push_back(names[original_index]);\n  }\n}\n\n}  // namespace\n\nstruct SavedModelImpl::JoinedSignature {\n  // A unique name for the joined signature.\n  std::string name;\n  // The feed nodes for the corresponding inputs, but they might not be in the\n  // original order and if there are more than one original inputs mapped to the\n  // same feed node, only one is picked here.\n  tensorflow::GraphImportConfig::InputArrays input_nodes;\n  // The fetch nodes for the outputs, which should be in the original order.\n  std::vector<std::string> output_nodes;\n  // The target nodes that should be run but not returned as outputs.\n  std::vector<std::string> target_nodes;\n};\n\ntensorflow::Status SavedModelImpl::RunMultipleSignatures(\n    const RunOptions& run_options, absl::Span<const std::string> names,\n    absl::Span<const std::vector<tensorflow::Tensor>> multi_inputs,\n    std::vector<std::vector<tensorflow::Tensor>>* multi_outputs) {\n  TF_RET_CHECK(names.size() == multi_inputs.size())\n      << \"the sizes of names and inputs should be the same\";\n  TF_RET_CHECK(multi_outputs) << \"outputs must be provided\";\n  multi_outputs->clear();\n\n  // Sort `names` into determinisitic order to share loading result disregarding\n  // the order in `names`.\n  std::vector<std::string> sorted_signature_names;\n  std::vector<int> original_indices;\n  CreateSortedNamesAndOriginalIndices(names, sorted_signature_names,\n                                      original_indices);\n\n  // Due to possible overlapping of feed nodes among user-specified inputs,"
},
{
    "Id": 346,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7f9929732ced22fe8ef42a695dae39c1caf44608",
    "Violation": "missing",
    "Bug report": " For gather op, if params.shape[:batch_dims] is not the same as indice s.shape[:batch_dims], return an error instead of check fail ",
    "Number of deleted lines": 0,
    "Deleted lines": "      }\n    }\n\n    OP_REQUIRES(\n        c, axis >= -params.dims() && axis < params.dims(),\n        errors::InvalidArgument(\"Expected axis in the range [\", -params.dims(),\n                                \", \", params.dims(), \"), but got \", axis));\n\n    if (axis < 0) {\n      axis = params.dims() + axis;\n    }\n\n    if (batch_dims_ != 0) {\n      if (batch_dims_ < 0) {\n        batch_dims_ = indices.dims() + batch_dims_;\n      }\n\n      if (!axis_is_set) axis = batch_dims_;\n\n      OP_REQUIRES(\n          c, batch_dims_ >= -indices.dims() && batch_dims_ < indices.dims(),\n          errors::InvalidArgument(\"Expected batch_dims in the range [\",\n                                  -indices.dims(), \", \", indices.dims(),\n                                  \"), but got \", batch_dims_));\n\n      OP_REQUIRES(c, batch_dims_ < params.dims(),\n                  errors::InvalidArgument(\"batch_dims (\", batch_dims_,\n                                          \") must be less than rank(params) (\",\n                                          params.dims(), \").\"));\n\n      OP_REQUIRES(c, axis >= batch_dims_,\n                  errors::InvalidArgument(\"batch_dims (\", batch_dims_,\n                                          \") must be less than or equal to \",\n                                          \"axis (\", axis, \").\"));\n    }\n\n    // Check that we have enough index space\n    int64 gather_dim_size = params.dim_size(axis);\n    const int64 N = indices.NumElements();\n    OP_REQUIRES(\n        c, gather_dim_size <= std::numeric_limits<Index>::max(),\n        errors::InvalidArgument(\"params.shape[\", axis, \"] too large for \",\n                                DataTypeString(DataTypeToEnum<Index>::v()),\n                                \" indexing: \", gather_dim_size, \" > \",\n                                std::numeric_limits<Index>::max()));\n\n    // The result shape is params.shape[:axis] + indices.shape[batch_dims:] +\n    // params.shape[axis + 1:].\n    TensorShape result_shape;\n    int64 batch_size = 1;\n    int64 outer_size = 1;\n    int64 inner_size = 1;\n\n    for (int i = 0; i < batch_dims_; ++i) {\n      result_shape.AddDim(params.dim_size(i));\n      batch_size *= params.dim_size(i);\n    }\n    for (int i = batch_dims_; i < axis; ++i) {\n      result_shape.AddDim(params.dim_size(i));\n      outer_size *= params.dim_size(i);\n    }\n    for (int i = batch_dims_; i < indices.dims(); ++i) {\n      result_shape.AddDim(indices.dim_size(i));\n    }\n    for (int i = axis + 1; i < params.dims(); ++i) {\n      result_shape.AddDim(params.dim_size(i));\n      inner_size *= params.dim_size(i);\n    }\n\n    Tensor* out = nullptr;"
},
{
    "Id": 347,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9e62869465573cb2d9b5053f1fa02a81fce21d69",
    "Violation": "missing",
    "Bug report": "Add more validation to RequantizationRangePerChannel. ",
    "Number of deleted lines": 0,
    "Deleted lines": "#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/type_traits.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/meta_support.h\"\n#include \"tensorflow/core/kernels/no_op.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/util/mkl_threadpool.h\"\n#include \"tensorflow/core/util/mkl_util.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\nclass MklRequantizationRangePerChannelOp : public OpKernel {\n public:\n  explicit MklRequantizationRangePerChannelOp(OpKernelConstruction* ctx)\n      : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"clip_value_max\", &clip_value_max_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(kInputTensorIndex);\n    const Tensor& input_min = ctx->input(kInputMinIndex);\n    const Tensor& input_max = ctx->input(kInputMaxIndex);\n\n    const size_t depth = input_max.NumElements();\n    OP_REQUIRES(\n        ctx, input_min.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                depth, \" was \", input_min.dim_size(0)));\n    OP_REQUIRES(\n        ctx, input_max.dim_size(0) == depth,\n        errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                depth, \" was \", input_max.dim_size(0)));\n\n    const float* input_min_data = input_min.flat<float>().data();\n    const float* input_max_data = input_max.flat<float>().data();\n    std::vector<float> ranges(depth);\n    bool is_non_negative = true;\n    Eigen::array<int, 2> shuffling({1, 0});\n    auto input_matrix = input.flat_inner_dims<qint32>();\n\n    // TODO: verify performance of not transposing and finding the min max\n    // directly from input_matrix vs the one presented below of transposing and\n    // using the transposed matrix as the transposing operation in itself might\n    // be more costly.\n    // Note that this operation is a calibration step for quantization and will\n    // cease to exist in the final inference graph(will exist as a const node).\n    auto transposed_input = input_matrix.shuffle(shuffling);\n\n    // Find the ranges of each channel in parallel.\n    float out_min_max = std::numeric_limits<float>::min();\n\n#ifdef ENABLE_ONEDNN_OPENMP\n#ifdef _MSC_VER\n#pragma omp parallel for\n#else\n#pragma omp parallel for reduction(max : out_min_max)\n#endif\n#endif  // ENABLE_ONEDNN_OPENMP\n    // TODO: Add eigen parallel_for\n    for (int64_t i = 0; i < depth; ++i) {\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> min =\n          transposed_input.chip<0>(i).minimum();\n      Eigen::Tensor<qint32, 0, Eigen::RowMajor> max =\n          transposed_input.chip<0>(i).maximum();\n      const int32_t min_per_channel = min();\n      const int32_t max_per_channel = max();\n      const int32_t abs_max =\n          std::max(std::abs(min_per_channel), std::abs(max_per_channel));"
},
{
    "Id": 348,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ba91c04e001f417641e757a6417e5325c1c4e15e",
    "Violation": "insufficient",
    "Bug report": "Add more check to sparsity parameter verifier.",
    "Number of deleted lines": 1,
    "Deleted lines": "\n      if (num_elements != array_segments->size() - 1) {\n        return absl::nullopt;\n      }\n\n      if (array_indices->size() !=\n          array_segments->Get(array_segments->size() - 1)) {\n        return absl::nullopt;\n      }\n\n      for (int j = 0; j < array_indices->size(); j++) {\n        if (array_indices->Get(j) < 0 ||\n            array_indices->Get(j) >= dim_sizes[original_dim]) {\n          return absl::nullopt;\n        }\n      }\n\n      // Need to reset num_elements when seeing a sparse dimension.\n      num_elements = array_indices->size();\n    }\n  }\n\n  return num_elements;\n}\n\nabsl::optional<uint64_t> VerifyAndCountSparseElements(const Tensor& tensor) {\n  const auto* sparsity = tensor.sparsity();\n  if (sparsity->traversal_order() == nullptr ||\n      sparsity->dim_metadata() == nullptr) {\n    return absl::nullopt;\n  }\n\n  const int total_dims = sparsity->traversal_order()->size();\n\n  if (sparsity->dim_metadata()->size() != total_dims) {\n    return absl::nullopt;\n  }\n\n  const int block_rank = total_dims - tensor.shape()->size();\n  if (block_rank > 0 && (sparsity->block_map() == nullptr ||\n                         sparsity->block_map()->size() != block_rank)) {\n    return absl::nullopt;\n  }\n\n  // For a n-dimensional tensor (d0, ..., dn-1) with k-dimensional block (dn,\n  // ..., dn+k-1), the expanded_dim_sizes holds the size of each dimension in\n  // the order of (d0, ..., dn-1, dn, ..., dn+k-1), not the traversal order.\n  // For example, a 4x4 tensor with 2x2 block has expanded_dim_sizes = {2, 2, 2,\n  // 2}.\n  std::vector<int> expanded_dim_sizes;\n  expanded_dim_sizes.resize(total_dims);\n  const int original_rank = tensor.shape()->size();\n  // First go through the original tensor dimensions, populate their sizes.\n  for (int i = 0; i < original_rank; i++) {\n    expanded_dim_sizes[i] = tensor.shape()->Get(i);\n  }\n  // Then go through the block dimensions, and\n  //   1. populate block dimension size.\n  //   2. block_map[i] has the original dimension that block dimension i maps\n  //   to. Divide the size of the original dimension by the size of the ith\n  //   block dimension.\n  for (int i = 0; i < block_rank; i++) {\n    int original_block_dim =\n        sparsity->traversal_order()->Get(i + original_rank);\n    int block_dim_size =\n        sparsity->dim_metadata()->Get(i + original_rank)->dense_size();\n    if (block_dim_size == 0) {\n      return absl::nullopt;\n    }\n\n    expanded_dim_sizes[original_block_dim] = block_dim_size;"
},
{
    "Id": 349,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1610f391833738972b538e4ee97f90dbd30fc745",
    "Violation": "improper",
    "Bug report": " Replace DCHECK with actual validation in AddRangeStats",
    "Number of deleted lines": 2,
    "Deleted lines": "// Add statistics to StatsPartitionMap for (instance, feature dim, bucket id).\nstatic void AddInstanceStatsToMap(\n    const int32_t instance, const int32_t feature_dim, const int32_t bucket_id,\n    const int32_t logits_dims, const int32_t stats_dims,\n    StatsPartitionMap* stats_map, const TTypes<float>::ConstMatrix& gradients,\n    const TTypes<float>::ConstMatrix& hessians,\n    const TTypes<int32>::ConstVec& node_ids) {\n  const int32_t node_id = node_ids(instance);\n  const auto key = StatsPartitionKey(node_id, feature_dim, bucket_id);\n  std::pair<StatsPartitionIterator, bool> const& insert_result =\n      stats_map->insert(StatsPartitionIterator::value_type(\n          key, std::vector<float>(stats_dims, 0.0f)));\n  auto& stats = insert_result.first->second;\n  for (int stat_dim = 0; stat_dim < logits_dims; ++stat_dim) {\n    stats[stat_dim] += gradients(instance, stat_dim);\n  }\n  for (int stat_dim = logits_dims; stat_dim < stats_dims; ++stat_dim) {\n    stats[stat_dim] += hessians(instance, stat_dim - logits_dims);\n  }\n}\n\n// Add statistics to StatsPartitionMap for bucket_id ranging from\n// (start_instance, start_feature_dim) to (end_instance, end_feature_dim),\n// inclusive on start and end instances, exclusive on end feature dim.\nstatic void AddRangeStats(OpKernelContext* const context,\n                          const int start_instance, const int end_instance,\n                          const int start_feature_dim,\n                          const int end_feature_dim,\n                          StatsPartitionMap* stats_map,\n                          const TTypes<float>::ConstMatrix& gradients,\n                          const TTypes<float>::ConstMatrix& hessians,\n                          const TTypes<int32>::ConstVec& node_ids,\n                          const int32_t feature_dims, const int32_t bucket_id,\n                          const int32_t logits_dims, const int32_t stats_dims) {\n  DCHECK_LE(start_instance, end_instance);\n  if (start_instance == end_instance) {\n    DCHECK_LT(start_feature_dim, end_feature_dim);\n  }\n  for (int32_t instance = start_instance; instance <= end_instance;\n       ++instance) {\n    const int32_t start_f_dim =\n        (instance == start_instance) ? start_feature_dim + 1 : 0;\n    const int32_t end_f_dim =\n        (instance == end_instance) ? end_feature_dim : feature_dims;\n    for (int32_t f_dim = start_f_dim; f_dim < end_f_dim; ++f_dim) {\n      AddInstanceStatsToMap(instance, f_dim, bucket_id, logits_dims, stats_dims,\n                            stats_map, gradients, hessians, node_ids);\n    }\n  }\n}\n\nclass BoostedTreesSparseAggregateStatsOp : public OpKernel {\n public:\n  explicit BoostedTreesSparseAggregateStatsOp(\n      OpKernelConstruction* const context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"max_splits\", &max_splits_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_buckets\", &num_buckets_));\n  }\n\n  void Compute(OpKernelContext* const context) override {\n    // node_ids.\n    const Tensor* node_ids_t;\n    OP_REQUIRES_OK(context, context->input(\"node_ids\", &node_ids_t));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(node_ids_t->shape()),\n        errors::InvalidArgument(\"node_ids must be a vector, received shape \",\n                                node_ids_t->shape().DebugString()));\n    const auto node_ids = node_ids_t->vec<int32>();\n    const auto num_nodes = node_ids_t->NumElements();\n    for (int i = 0; i < num_nodes; ++i) {\n      OP_REQUIRES(\n          context, node_ids(i) <= max_splits_,"
},
{
    "Id": 350,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/150a6c06b281246cb5a075a704fceeb257bb63af",
    "Violation": "missing",
    "Bug report": "Add a check on the 0th dimension of filter for DepthwiseConv.",
    "Number of deleted lines": 0,
    "Deleted lines": "\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteDepthwiseConvParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  // TODO(ahentz): use could use GetOptionalInputTensor() here, but we need to\n  // decide whether we are OK with optional tensors being completely absent, as\n  // opposed to having -1 as their index.\n  bool hasBias = NumInputs(node) == 3;\n\n  TF_LITE_ENSURE(context, hasBias || NumInputs(node) == 2);\n  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n  const TfLiteTensor* filter = GetInput(context, node, kFilterTensor);\n  const TfLiteTensor* bias = nullptr;\n\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);\n\n  // The parameter 'depth_multiplier' is redundant, so we check here to make\n  // sure it is consistent with the given dimensions.\n  TF_LITE_ENSURE_EQ(context,\n                    params->depth_multiplier * SizeOfDimension(input, 3),\n                    SizeOfDimension(filter, 3));\n\n  const TfLiteType data_type = input->type;\n  TF_LITE_ENSURE(context, data_type == kTfLiteFloat32 ||\n                              data_type == kTfLiteUInt8 ||\n                              data_type == kTfLiteInt8);\n  TF_LITE_ENSURE_EQ(context, output->type, data_type);\n  TF_LITE_ENSURE_EQ(context, filter->type, data_type);\n\n  if (hasBias) {\n    bias = GetInput(context, node, kBiasTensor);\n    if (data_type == kTfLiteUInt8 || data_type == kTfLiteInt8) {\n      TF_LITE_ENSURE_EQ(context, bias->type, kTfLiteInt32);\n      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);\n    } else {\n      TF_LITE_ENSURE_EQ(context, bias->type, data_type);\n    }\n    TF_LITE_ENSURE_EQ(context, NumDimensions(bias), 1);\n    TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 3),\n                      SizeOfDimension(bias, 0));\n  }\n\n  int channels_out = SizeOfDimension(filter, 3);\n  int width = SizeOfDimension(input, 2);\n  int height = SizeOfDimension(input, 1);\n  int filter_width = SizeOfDimension(filter, 2);\n  int filter_height = SizeOfDimension(filter, 1);\n  int batches = SizeOfDimension(input, 0);\n\n  // Matching GetWindowedOutputSize in TensorFlow.\n  auto padding = params->padding;\n  int out_width, out_height;\n\n  data->padding = ComputePaddingHeightWidth(\n      params->stride_height, params->stride_width,\n      params->dilation_height_factor, params->dilation_width_factor, height,\n      width, filter_height, filter_width, padding, &out_height, &out_width);\n\n  // Note that quantized inference requires that all tensors have their\n  // parameters set. This is usually done during quantized training or\n  // calibration.\n  if (data_type != kTfLiteFloat32) {\n    TF_LITE_ENSURE_EQ(context, filter->quantization.type,\n                      kTfLiteAffineQuantization);"
},
{
    "Id": 351,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bf686faeddcca97be6ad7b6421cb26ab1c3cea2c",
    "Violation": "missing",
    "Bug report": "TFLite: Enhance input check for ResizeNearestNeghbor",
    "Number of deleted lines": 2,
    "Deleted lines": "namespace resize_nearest_neighbor {\n\n// This file has three implementations of RESIZE_NEAREST_NEIGHBOR.\nenum KernelType {\n  kReference,\n  kGenericOptimized,\n  kNeonOptimized,\n};\n\nconstexpr int kInputTensor = 0;\nconstexpr int kSizeTensor = 1;\nconstexpr int kOutputTensor = 0;\n\nTfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n                                const TfLiteTensor* input,\n                                const TfLiteTensor* size,\n                                TfLiteTensor* output) {\n  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\n  output_size->data[0] = input->dims->data[0];\n  const int32* size_data = GetTensorData<int32>(size);\n  output_size->data[1] = size_data[0];\n  output_size->data[2] = size_data[1];\n  output_size->data[3] = input->dims->data[3];\n  return context->ResizeTensor(context, output, output_size);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n  const TfLiteTensor* size = GetInput(context, node, kSizeTensor);\n  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n\n  // TODO(ahentz): Our current implementations rely on the inputs being 4D.\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(size), 1);\n\n  TF_LITE_ENSURE_EQ(context, size->type, kTfLiteInt32);\n  output->type = input->type;\n\n  if (!IsConstantTensor(size)) {\n    SetTensorToDynamic(output);\n    return kTfLiteOk;\n  }\n  return ResizeOutputTensor(context, input, size, output);\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params =\n      reinterpret_cast<TfLiteResizeNearestNeighborParams*>(node->builtin_data);\n\n  const TfLiteTensor* input = GetInput(context, node, kInputTensor);\n  TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n  const TfLiteTensor* size = GetInput(context, node, kSizeTensor);\n\n  if (IsDynamicTensor(output)) {\n    TF_LITE_ENSURE_OK(context,\n                      ResizeOutputTensor(context, input, size, output));\n  }\n\n  tflite::ResizeNearestNeighborParams op_params;\n  op_params.align_corners = params->align_corners;\n\n  if (output->type == kTfLiteFloat32) {\n    reference_ops::ResizeNearestNeighbor(\n        op_params, GetTensorShape(input), GetTensorData<int32>(input),\n        GetTensorShape(size), GetTensorData<int32>(size),\n        GetTensorShape(output), GetTensorData<int32>(output));\n  } else if (output->type == kTfLiteUInt8) {\n    if (kernel_type == kReference) {\n      reference_ops::ResizeNearestNeighbor(\n          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),\n          GetTensorShape(size), GetTensorData<int32>(size),"
},
{
    "Id": 352,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c040db5e9003cc20016586df9f2964db83b98c4f",
    "Violation": "missing",
    "Bug report": " [XLA] Add a defensive check in dynamic dimension inference to prevent scalar reshape with dynamic dimension. In theory we can just ignore a [1] -> [] reshape, but adding a check here for now.",
    "Number of deleted lines": 3,
    "Deleted lines": "  // The returned value is a scalar, which doesn't have any dynamic dimension in\n  // the shape (although the value contains the real size of the dynamic\n  // dimension of the input).\n  return Status::OK();\n}\n\nStatus DynamicDimensionInferenceVisitor::PassThroughDynamicDimension(\n    HloInstruction* hlo) {\n  return ForEachOperandDynamicDimension(\n      hlo, [&](HloInstruction* operand, ShapeIndex index, int64 dimension,\n               int64 operand_index, HloInstruction* dynamic_size,\n               DimensionConstraint constraint) {\n        parent_->SetDynamicSize(hlo, index, dimension, dynamic_size,\n                                constraint);\n        return Status::OK();\n      });\n}\n\nStatus DynamicDimensionInferenceVisitor::HandleElementwiseUnary(\n    HloInstruction* hlo) {\n  return PassThroughDynamicDimension(hlo);\n}\n\nStatus DynamicDimensionInferenceVisitor::HandleSelect(HloInstruction* hlo) {\n  return PassThroughDynamicDimension(hlo);\n}\n\nStatus DynamicDimensionInferenceVisitor::HandleElementwiseBinary(\n    HloInstruction* hlo) {\n  return PassThroughDynamicDimension(hlo);\n}\n\nStatus DynamicDimensionInferenceVisitor::HandleReshape(HloInstruction* hlo) {\n  return ForEachOperandDynamicDimension(\n      hlo, [&](HloInstruction* operand, ShapeIndex index, int64 dimension,\n               int64 operand_index, HloInstruction* dynamic_size,\n               DimensionConstraint constraint) {\n        HloInstruction* reshape = hlo;\n        // Reshape is supported as long as it is the most\n        // major one and it is combining with other non-dynamic dimensions.\n        const int64 output_most_major = reshape->shape().dimensions(0);\n        const int64 input_most_major = operand->shape().dimensions(0);\n        if (dimension == 0) {\n          if (output_most_major > input_most_major) {\n            const int64 multiplier =\n                reshape->shape().dimensions(0) / operand->shape().dimensions(0);\n            HloInstruction* multiplier_hlo =\n                hlo->parent()->AddInstruction(HloInstruction::CreateConstant(\n                    LiteralUtil::CreateR0<int32>(multiplier)));\n\n            HloInstruction* new_dynamic_size =\n                hlo->parent()->AddInstruction(HloInstruction::CreateBinary(\n                    dynamic_size->shape(), HloOpcode::kMultiply, dynamic_size,\n                    multiplier_hlo));\n            parent_->SetDynamicSize(reshape, {}, 0, new_dynamic_size,\n                                    {.stride = 1, .multiple_of = multiplier});\n            return Status::OK();\n          } else if (output_most_major < input_most_major) {\n            // Output dimension is decomposed from input most major dimension.\n            // In this case, we don't know which one is dynamic, e.g., when we\n            // have:\n            //\n            //           [<=a/c, c, b]\n            //              | Reshape\n            //           [<=a, b] // a is dynamic, has to be multiple of c.\n            //             |  Reshape\n            // [1, 1, ... , a/c, c, b]\n            //\n            // Any dimension from the first '1' to 'a/c' can be dynamic.\n            //\n            // We use the following logics to disambiguate://\n            // 1. If the user sets \"inferred_dimension\", then use that as\n            // dynamic dimension.\n            //"
},
{
    "Id": 353,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/48393637f8154be16088d84742485a0e153ecbb2",
    "Violation": "improper",
    "Bug report": "Change check to allow tensors with up to 6 dims.",
    "Number of deleted lines": 2,
    "Deleted lines": "\n  const string& output_name = op->outputs[0];\n  const int block_size = op->block_size;\n  CHECK_NE(block_size, 0) << \"Invalid block_size in \" << output_name;\n  const int batch = input_shape.dims(0);\n  const int height = input_shape.dims(1);\n  const int width = input_shape.dims(2);\n  const int depth = input_shape.dims(3);\n  QCHECK_EQ(width % block_size, 0);\n  QCHECK_EQ(height % block_size, 0);\n\n  model->GetArray(output_name)\n      .copy_shape(Shape({batch, height / block_size, width / block_size,\n                         depth * block_size * block_size}));\n}\n\nvoid ProcessOpWithShapeInput(Model* model, Operator* op) {\n  CHECK_EQ(op->outputs.size(), 1);\n  auto& output_array = model->GetArray(op->outputs[0]);\n  if (output_array.has_shape()) {\n    // We have already run\n    return;\n  }\n\n  auto& dims_array = model->GetArray(op->inputs[0]);\n  if (!dims_array.has_shape()) {\n    // Yield until dims shape been resolved.\n    return;\n  }\n  if (!dims_array.buffer) {\n    // Yield until the dims are constant\n    return;\n  }\n  CHECK(dims_array.data_type == ArrayDataType::kInt32) << \"dims must be int32\";\n  CHECK_LE(RequiredBufferSizeForShape(dims_array.shape()), 4)\n      << \"dims vector can be no larger than 4 values\";\n\n  std::vector<int32> const& dims =\n      dims_array.GetBuffer<ArrayDataType::kInt32>().data;\n  *(output_array.mutable_shape()->mutable_dims()) = dims;\n}\n\nvoid ProcessFullyConnectedOperator(Model* model, FullyConnectedOperator* op) {\n  const auto& input_array = model->GetArray(op->inputs[0]);\n  // Yield until input dims have been resolved.\n  if (!input_array.has_shape()) {\n    return;\n  }\n  const auto& input_shape = input_array.shape();\n  CHECK_GE(input_shape.dimensions_count(), 1);\n\n  const auto& weights_array = model->GetArray(op->inputs[1]);\n  // Yield until weights dims have been resolved.\n  if (!weights_array.has_shape()) {\n    return;\n  }\n  const auto& weights_shape = weights_array.shape();\n\n  const int weights_output_depth = weights_shape.dims(0);\n  CHECK_EQ(weights_shape.dimensions_count(), 2);\n\n  const int input_overall_size = RequiredBufferSizeForShape(input_shape);\n  const int matmul_repeats = input_overall_size / weights_shape.dims(1);\n  CHECK_EQ(matmul_repeats * weights_shape.dims(1), input_overall_size);\n\n  auto& output_array = model->GetArray(op->outputs[0]);\n  output_array.copy_shape(Shape({matmul_repeats, weights_output_depth}));\n}\n\nvoid ProcessTensorFlowReshapeOperator(Model* model,\n                                      TensorFlowReshapeOperator* op) {\n  auto& output_array = model->GetArray(op->outputs[0]);"
},
{
    "Id": 354,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/662128e8ca3411286b234553a7efc1356353d0f5",
    "Violation": "missing",
    "Bug report": " add rank checking for MEAN op. The MEAN op of NNAPI only supports a tensor with rank <= 4. Check the rank of the input tensor before delegating the op.",
    "Number of deleted lines": 0,
    "Deleted lines": "          ExpectMinAndroidSdkVersion(android_sdk_version,\n                                     kMinSdkVersionForNNAPI12, &val_ctx);\n        }\n\n        if (android_sdk_version >= kMinSdkVersionForNNAPI13) {\n          Expect(weight_type == kTfLiteFloat32 || weight_type == kTfLiteUInt8 ||\n                     weight_type == kTfLiteInt8,\n                 NNAPIValidationFailureType::kUnsupportedInputType,\n                 \"Weight has to be Float32 or UINT8 or INT8\", &val_ctx);\n        } else {\n          Expect(weight_type == kTfLiteFloat32 || weight_type == kTfLiteUInt8,\n                 NNAPIValidationFailureType::kUnsupportedInputType,\n                 \"Weight has to be Float32 or UINT8\", &val_ctx);\n        }\n      }\n    } break;\n    case kTfLiteBuiltinMean: {\n      ExpectMaxOpVersion(version, 2, &val_ctx);\n      ExpectMinAndroidSdkVersion(android_sdk_version, kMinSdkVersionForNNAPI11,\n                                 &val_ctx);\n      if (android_sdk_version >= kMinSdkVersionForNNAPI12) {\n        Expect(context->tensors[node->inputs->data[0]].type == kTfLiteFloat32 ||\n                   IsQuantized(context->tensors[node->inputs->data[0]].type),\n               NNAPIValidationFailureType::kUnsupportedInputType,\n               \"Expected Float32 or Quantized input\", &val_ctx);\n      } else {\n        Expect(context->tensors[node->inputs->data[0]].type == kTfLiteFloat32,\n               NNAPIValidationFailureType::kUnsupportedInputType,\n               \"Expected Float32 input\", &val_ctx);\n      }\n      Expect(context->tensors[node->outputs->data[0]].dims->size > 0,\n             NNAPIValidationFailureType::kUnsupportedOutputType,\n             \"NNAPI does not support generating a scalar as output for MEAN.\",\n             &val_ctx);\n    } break;\n    case kTfLiteBuiltinEmbeddingLookup: {\n      ExpectOpVersion(version, 1, &val_ctx);\n      Expect(context->tensors[node->inputs->data[1]].type == kTfLiteFloat32,\n             NNAPIValidationFailureType::kUnsupportedInputType,\n             \"NNAPI only support float32 values.\", &val_ctx);\n    } break;\n    case kTfLiteBuiltinHashtableLookup: {\n      ExpectOpVersion(version, 1, &val_ctx);\n      Expect(context->tensors[node->outputs->data[0]].type == kTfLiteFloat32,\n             NNAPIValidationFailureType::kUnsupportedOutputType,\n             \"NNAPI only support float32 output.\", &val_ctx);\n    } break;\n    case kTfLiteBuiltinMaximum:\n    case kTfLiteBuiltinMinimum: {\n      ExpectMaxOpVersion(version, 3, &val_ctx);\n      ExpectMinAndroidSdkVersion(android_sdk_version, kMinSdkVersionForNNAPI12,\n                                 &val_ctx);\n      const auto input_type = context->tensors[node->inputs->data[0]].type;\n      EXPECT_INPUT_TYPE_IN(input_type, kTfLiteFloat32, kTfLiteUInt8,\n                           kTfLiteInt8, kTfLiteInt32);\n      const TfLiteTensor& operand0 = context->tensors[node->inputs->data[0]];\n      if (operand0.dims->size == 0) {\n        Expect(operand0.allocation_type == kTfLiteMmapRo,\n               NNAPIValidationFailureType::kUnsupportedInputType,\n               \"Scalar operand should be constant\", &val_ctx);\n      }\n      const TfLiteTensor& operand1 = context->tensors[node->inputs->data[1]];\n      if (operand1.dims->size == 0) {\n        Expect(operand1.allocation_type == kTfLiteMmapRo,\n               NNAPIValidationFailureType::kUnsupportedInputType,\n               \"Scalar operand should be constant\", &val_ctx);\n      }\n    } break;\n    case kTfLiteBuiltinCast: {\n      ExpectOpVersion(version, 1, &val_ctx);"
},
{
    "Id": 355,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9b947dd6377c022091c8aa005cdcff52c53ff5f0",
    "Violation": "insufficient",
    "Bug report": "Also check dst_format",
    "Number of deleted lines": 1,
    "Deleted lines": "  auto* axis_node = regular_fanin_1.node_view();\n  if (!IsConstant(*axis_node->node())) {\n    return false;\n  }\n  const auto* value_attr = axis_node->GetAttr(kAttrValue);\n  if (value_attr == nullptr) {\n    return false;\n  }\n  Tensor tensor;\n  if (!tensor.FromProto(value_attr->tensor())) {\n    LOG(ERROR) << \"Failed to parse TensorProto.\";\n    return false;\n  }\n  auto indices = [&context](absl::Span<const char> labels) {\n    return GetDimensionIndicesFromLabel(context.src_dim_indices, labels);\n  };\n  return IsAlongAxis(tensor, indices({'N', 'H', 'W', 'C'}), kRank) ||\n         IsAlongAxis(tensor, indices({'H', 'W', 'C'}), kRank) ||\n         IsAlongAxis(tensor, indices({'N', 'H', 'W'}), kRank) ||\n         IsAlongAxis(tensor, indices({'H', 'W'}), kRank) ||\n         IsAlongAxis(tensor, indices({'C'}), kRank);\n}\n\nStatus ReduceTransposer::TransposeNode(TransposeContext* context,\n                                       utils::MutableNodeView* node) {\n  DCHECK(IsReduceOp(*node->node()));\n  const auto& regular_fanin = node->GetRegularFanin(0);\n  const auto* output_shape_attr =\n      regular_fanin.node_view()->GetAttr(kAttrOutputShape);\n  const auto& shape = output_shape_attr->list().shape(0);\n  const int rank = shape.dim_size();\n  std::string src_format = context->src_format;\n  std::string dst_format = context->dst_format;\n  // Update the format from 4D to 5D layout if necessary.\n  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");\n  if (allow_5d) {\n    std::string src_format_3d = src_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";\n    std::string dst_format_3d = dst_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";\n    context->AssignDeviceAndDataFormats(context->target_device, src_format_3d,\n                                        dst_format_3d);\n  }\n  if (!ShouldProcess(*context, *node) || !IsFaninPortRankN(*node, 0, rank) ||\n      !IsReduceAxisSupported(*context, *node) ||\n      !IsAfterDstToSrcTransform(*context, *node)) {\n    // Change back to the original layout due to early exit.\n    if (allow_5d) {\n      context->AssignDeviceAndDataFormats(context->target_device, src_format,\n                                          dst_format);\n    }\n    return Status::OK();\n  }\n  VLOG(3) << \"GenericLayoutOptimizer: transforming node '\" << node->GetName()\n          << \"' with op '\" << node->GetOp() << \"' from data format '\"\n          << context->src_format << \"' to '\" << context->dst_format << \"'\";\n  TF_RETURN_IF_ERROR(UpdateFaninEdgesWithOp(context, {0}, node, kOpTranspose));\n  TF_RETURN_IF_ERROR(\n      UpdateFaninEdgesWithOp(context, {1}, node, kOpDataFormatDimMap));\n  if (KeepDims(*node)) {\n    TF_RETURN_IF_ERROR(\n        UpdateFanoutEdgesWithOp(context, {0}, node, kOpTranspose));\n  }\n  // Change back the format from 5D to 4D layout.\n  if (allow_5d) {\n    context->AssignDeviceAndDataFormats(context->target_device, src_format,\n                                        dst_format);\n  }\n  return context->graph_view->GetMutationBuilder()->Apply();\n}\n\nStatus ReverseV2Transposer::TransposeNode(TransposeContext* context,\n                                          utils::MutableNodeView* node) {"
},
{
    "Id": 356,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/450dec35448a73b3fcb5d4f82108d5fdcb3f59b4",
    "Violation": "missing",
    "Bug report": "Internal change, add some checks on the sparseTensor format checking.",
    "Number of deleted lines": 1,
    "Deleted lines": "          \"Ragged tensor input should have row_splits elements number the same \"\n          \"as the sample count + 1. But got \",\n          indices_or_row_splits.NumElements(),\n          \" elements for row_splits and sample count as \", sample_count, \".\"));\n    }\n  } else {\n    return absl::InvalidArgumentError(\n        absl::StrCat(\"Invalid indices_or_row_splits input, Got dimension of \",\n                     indices_or_row_splits.dims(), \" and size of \",\n                     indices_or_row_splits.NumElements(), \".\"));\n  }\n  return OkStatus();\n}\n\nStatus ComputeRowIdsBeforePadding(const Tensor& indices_or_row_splits,\n                                  const int32 total_id_count,\n                                  int32* row_ids_before_padding) {\n  // The only difference between dense tensor, sparse tensor and ragged tensor\n  // is the row ids output.\n  if (indices_or_row_splits.NumElements() == 0) {\n    // Dense tensor to COO format.\n    // Row ids are just the index ids.\n    for (int32 i = 0; i < total_id_count; ++i) {\n      *(row_ids_before_padding + i) = i;\n    }\n  } else if (indices_or_row_splits.dims() == 2 &&\n             indices_or_row_splits.NumElements() > 0) {\n    // Sparse tensor to COO format.\n    // TODO(pineapplejuice233): should we support arbitrary rank of sparse tensor and\n    // convert it to 2D?\n    // For 2D sparse tensor, as we always combine on the last dimension.\n    // The row ids are just the sample ids which is the first dim of the\n    // indices.\n    auto indices_matrix = indices_or_row_splits.matrix<int32>();\n    for (int32 i = 0; i < total_id_count; ++i) {\n      *(row_ids_before_padding + i) = indices_matrix(i, 0);\n    }\n  } else if (indices_or_row_splits.dims() == 1 &&\n             indices_or_row_splits.NumElements() > 0) {\n    // Ragged tensor to COO format.\n    const int32* indices_or_row_splits_ptr =\n        indices_or_row_splits.flat<int32>().data();\n    int32 current_row_id = -1;\n    for (int32 i = 0; i < total_id_count; ++i) {\n      while (i == *(indices_or_row_splits_ptr + 1 + current_row_id)) {\n        current_row_id += 1;\n      }\n      *(row_ids_before_padding + i) = current_row_id;\n    }\n  } else {\n    return absl::InvalidArgumentError(\n        absl::StrCat(\"Invalid indices_or_row_splits input, Got dimension of \",\n                     indices_or_row_splits.dims(), \" and size of \",\n                     indices_or_row_splits.NumElements(), \".\"));\n  }\n  return OkStatus();\n}\n\n// Convert the input sparse/dense/ragged tensor into COO format and normalize\n// the combiner. Note the COO tensor it produces only contains three 1D tensors\n// and no partitioning is performed on these tensors.\nclass ConvertToCooTensorOp : public OpKernel {\n public:\n  explicit ConvertToCooTensorOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"sample_count\", &sample_count_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"combiner\", &combiner_));\n    OP_REQUIRES_OK(ctx, ValidateInputCombiner(combiner_));\n  }\n  ~ConvertToCooTensorOp() override = default;\n  ConvertToCooTensorOp(const ConvertToCooTensorOp&) = delete;\n  ConvertToCooTensorOp& operator=(const ConvertToCooTensorOp&) = delete;\n"
},
{
    "Id": 357,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/356f360e8772a2697ec0d30036237342549803f5",
    "Violation": "missing",
    "Bug report": " Add additional shape validation to compute_accidental_hits. In `compute_accidental_hits`, the `sampled_candidates` must be a vector, as is shown in the kernel implementation in `tensorflow/core/kernels candidate_sampler_ops.cc`. This fix adds shape validation of `sampled_candidates` in the shape function whenever possible.",
    "Number of deleted lines": 1,
    "Deleted lines": "    .Attr(\"num_shards: int >= 1 = 1\")\n    .Attr(\"shard: int >= 0 = 0\")\n    .Attr(\"unigrams: list(float) = []\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .SetShapeFn(CandidateSamplerShapeFn)\n    .SetIsStateful();\n\nREGISTER_OP(\"AllCandidateSampler\")\n    .Input(\"true_classes: int64\")\n    .Output(\"sampled_candidates: int64\")\n    .Output(\"true_expected_count: float\")\n    .Output(\"sampled_expected_count: float\")\n    .Attr(\"num_true: int >= 1\")\n    .Attr(\"num_sampled: int >= 1\")\n    .Attr(\"unique: bool\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .SetShapeFn(CandidateSamplerShapeFn)\n    .SetIsStateful();\n\nREGISTER_OP(\"ComputeAccidentalHits\")\n    .Input(\"true_classes: int64\")\n    .Input(\"sampled_candidates: int64\")\n    .Output(\"indices: int32\")\n    .Output(\"ids: int64\")\n    .Output(\"weights: float\")\n    .Attr(\"num_true: int\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      int64 num_true;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"num_true\", &num_true));\n\n      // Validate true_classes.\n      ShapeHandle true_classes;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &true_classes));\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(\n          c->WithValue(c->Dim(true_classes, 1), num_true, &unused));\n\n      // All three outputs are the same shape.\n      ShapeHandle v = c->Vector(InferenceContext::kUnknownDim);\n      c->set_output(0, v);\n      c->set_output(1, v);\n      c->set_output(2, v);\n      return Status::OK();\n    });\n\n}  // namespace tensorflow\n"
},
{
    "Id": 358,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7c88788e63f3a747d2794175076db551d768734e",
    "Violation": "missing",
    "Bug report": " Shape validation of max_features in QuantizedReluX. In shape function of QuantizedReluX, `max_value` and `min_features` have shape validation but not `max_features`. This fix add restriction to `max_features` as well.",
    "Number of deleted lines": 0,
    "Deleted lines": "REGISTER_OP(\"QuantizedRelu6\")\n    .Input(\"features: Tinput\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"QuantizedReluX\")\n    .Input(\"features: Tinput\")\n    .Input(\"max_value: float\")\n    .Input(\"min_features: float\")\n    .Input(\"max_features: float\")\n    .Output(\"activations: out_type\")\n    .Output(\"min_activations: float\")\n    .Output(\"max_activations: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"QuantizedBatchNormWithGlobalNormalization\")\n    .Input(\"t: Tinput\")\n    .Input(\"t_min: float\")\n    .Input(\"t_max: float\")\n    .Input(\"m: Tinput\")\n    .Input(\"m_min: float\")\n    .Input(\"m_max: float\")\n    .Input(\"v: Tinput\")\n    .Input(\"v_min: float\")\n    .Input(\"v_max: float\")\n    .Input(\"beta: Tinput\")\n    .Input(\"beta_min: float\")\n    .Input(\"beta_max: float\")\n    .Input(\"gamma: Tinput\")\n    .Input(\"gamma_min: float\")\n    .Input(\"gamma_max: float\")\n    .Output(\"result: out_type\")\n    .Output(\"result_min: float\")\n    .Output(\"result_max: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .Attr(\"variance_epsilon: float\")\n    .Attr(\"scale_after_normalization: bool\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &input));\n\n      DimensionHandle last_dim = c->Dim(input, 3);\n      for (int i = 1; i < 5; ++i) {  // covers m, v, beta, gamma\n        ShapeHandle vec;\n        TF_RETURN_IF_ERROR(c->WithRank(c->input(i * 3), 1, &vec));"
},
{
    "Id": 359,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ff6be80a1ec3c353ebd0d17e2f0b46d9097310db",
    "Violation": "missing",
    "Bug report": " Improve the shape function for ParameterizedTruncatedNormal.  The parameters of ParameterizedTruncatedNormal should be 0-D or 1-D, which is checked in ther kernel functions. There is no check in the shape function of the ops. This fix improves the shape function and checks the parameters of ParameterizedTruncatedNormal whever possible.",
    "Number of deleted lines": 1,
    "Deleted lines": "    .Output(\"output: Tout\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"Tout: {int32, int64}\")\n    .Attr(\"T: {int32, int64}\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      return shape_inference::RandomShape(c);\n    });\n\nREGISTER_OP(\"RandomStandardNormal\")\n    .Input(\"shape: T\")\n    .SetIsStateful()\n    .Output(\"output: dtype\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"dtype: {half,bfloat16,float,double}\")\n    .Attr(\"T: {int32, int64}\")\n    .SetShapeFn(shape_inference::RandomShape);\n\nREGISTER_OP(\"ParameterizedTruncatedNormal\")\n    .Input(\"shape: T\")\n    .Input(\"means: dtype\")\n    .Input(\"stdevs: dtype\")\n    .Input(\"minvals: dtype\")\n    .Input(\"maxvals: dtype\")\n    .SetIsStateful()\n    .Output(\"output: dtype\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"dtype: {half,bfloat16,float,double}\")\n    .Attr(\"T: {int32, int64}\")\n    .SetShapeFn(shape_inference::RandomShape);\n\nREGISTER_OP(\"TruncatedNormal\")\n    .Input(\"shape: T\")\n    .SetIsStateful()\n    .Output(\"output: dtype\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"dtype: {half,bfloat16,float,double}\")\n    .Attr(\"T: {int32, int64}\")\n    .SetShapeFn(shape_inference::RandomShape);\n\nREGISTER_OP(\"RandomShuffle\")\n    .Input(\"value: T\")\n    .SetIsStateful()\n    .Output(\"output: T\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"T: type\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Multinomial\")\n    .SetIsStateful()\n    .Input(\"logits: T\")\n    .Input(\"num_samples: int32\")\n    .Output(\"output: output_dtype\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"output_dtype: {int32, int64} = DT_INT64\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle logits_shape;\n      ShapeHandle unused;\n      DimensionHandle num_samples;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &logits_shape));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->MakeDimForScalarInput(1, &num_samples));"
},
{
    "Id": 360,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c59c37e7b2d563967da813fa50fe20b21f4da683",
    "Violation": "missing",
    "Bug report": " Prevent array write out-of-bounds. If user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.",
    "Number of deleted lines": 0,
    "Deleted lines": "#include <stdint.h>\n\n#include <functional>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace arg_min_max {\n\nconstexpr int kInputTensor = 0;\nconstexpr int kAxis = 1;\nconstexpr int kOutputTensor = 0;\n\nTfLiteStatus ResizeOutput(TfLiteContext* context, const TfLiteTensor* input,\n                          const TfLiteTensor* axis, TfLiteTensor* output) {\n  int axis_value;\n  // Retrive all 8 bytes when axis type is kTfLiteInt64 to avoid data loss.\n  if (axis->type == kTfLiteInt64) {\n    axis_value = static_cast<int>(*GetTensorData<int64_t>(axis));\n  } else {\n    axis_value = *GetTensorData<int>(axis);\n  }\n  if (axis_value < 0) {\n    axis_value += NumDimensions(input);\n  }\n\n  // Copy the input dimensions to output except the axis dimension.\n  TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\n  int j = 0;\n  for (int i = 0; i < NumDimensions(input); ++i) {\n    if (i != axis_value) {\n      output_dims->data[j] = SizeOfDimension(input, i);\n      ++j;\n    }\n  }\n  return context->ResizeTensor(context, output, output_dims);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kInputTensor, &input));\n  const TfLiteTensor* axis;\n  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, kAxis, &axis));\n  // Make sure the axis is only 1 dimension.\n  TF_LITE_ENSURE_EQ(context, NumElements(axis), 1);\n  // Make sure the axis is only either int32 or int64.\n  TF_LITE_ENSURE(context,\n                 axis->type == kTfLiteInt32 || axis->type == kTfLiteInt64);\n\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  auto* params = reinterpret_cast<TfLiteArgMaxParams*>(node->builtin_data);\n  switch (params->output_type) {\n    case kTfLiteInt32:\n      output->type = kTfLiteInt32;\n      break;\n    case kTfLiteInt64:"
},
{
    "Id": 361,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e82a377de614fed51da8a7c5242a90a7967169f2",
    "Violation": "missing",
    "Bug report": "Correct axis check",
    "Number of deleted lines": 3,
    "Deleted lines": "}\n\n//===----------------------------------------------------------------------===//\n// DivOp\n//===----------------------------------------------------------------------===//\n\nOpFoldResult DivOp::fold(ArrayRef<Attribute> operands) {\n  // TODO(b/142478136): Handle fused ops.\n  if (fused_activation_function() != \"NONE\") return {};\n  return ConstFoldBinaryOp(\n      getType(), operands, [](APFloat a, APFloat b) { return a / b; },\n      [](APInt a, APInt b) { return a.sdiv(b); },\n      getOperation()->isCommutative());\n}\n\n//===----------------------------------------------------------------------===//\n// PackOp\n//===----------------------------------------------------------------------===//\n\n// TODO(b/133486129): Implement shape inference for pack\n\nstatic LogicalResult Verify(PackOp op) {\n  // TODO(antiagainst): Implement other checks as in\n  // tensorflow/lite/kernels/pack.cc\n\n  if (op.getOperation()->getNumOperands() != op.values_count())\n    return op.emitOpError(\"input count should match 'values_count' attribute\");\n\n  Value operand0 = op.getOperand(0);\n  auto input_type = operand0.getType().cast<ShapedType>();\n\n  // Check axis bounds.\n  if (input_type.hasRank()) {\n    int64_t axis_value = op.axis().getSExtValue();\n    if (abs(axis_value) > input_type.getRank())\n      return op.emitOpError(\"op attribute 'axis' is out of bounds, got \")\n             << axis_value;\n  }\n\n  // Make sure all inputs have the same shape and element type.\n  // TODO(b/135032063): Simplify once fixed.\n  for (Type operand_type : op.getOperandTypes()) {\n    if (failed(mlir::verifyCompatibleShape(input_type, operand_type)))\n      return op.emitOpError(\"operands should be of the same type. got \")\n             << input_type << \", \" << operand_type;\n  }\n\n  return success();\n}\n\n//===----------------------------------------------------------------------===//\n// PReluOp\n//===----------------------------------------------------------------------===//\n\nstatic LogicalResult Verify(PReluOp op) {\n  auto input_type = op.input().getType().cast<ShapedType>();\n  auto alpha_type = op.alpha().getType().cast<ShapedType>();\n  auto output_type = op.output().getType().cast<ShapedType>();\n\n  if (input_type.hasStaticShape() && alpha_type.hasStaticShape()) {\n    if (input_type.getRank() != alpha_type.getRank() + 1) {\n      return op.emitOpError(\"'alpha' should have one less rank than 'input'.\");\n    }\n\n    // Check if alpha is broadcastable\n    for (int i = 0; i < alpha_type.getRank(); i++) {\n      if (alpha_type.getDimSize(i) != input_type.getDimSize(i + 1) &&\n          alpha_type.getDimSize(i) != 1) {\n        return op.emitOpError(\n            llvm::formatv(\"'alpha' is not broadcastable at dimension {0}.\", i));\n      }\n    }\n  }"
},
{
    "Id": 362,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/27de8e717c1bec91398f5a6be6c7287b657fc960",
    "Violation": "missing",
    "Bug report": " Improve shape function for CudnnRNNParamsSize. In cudnn_rnn_ops.cc, the CudnnRNNParamsSize does not have restrictions on num_layers, num_units, and input_size, though they all should be scalars. This fix adds the shape check of num_layers, num_units, and input_size for CudnnRNNParamsSize.",
    "Number of deleted lines": 0,
    "Deleted lines": "namespace tensorflow {\nnamespace {\n\nconstexpr auto kRNNModeAttrs =\n    \"rnn_mode: {'rnn_relu', 'rnn_tanh', 'lstm', 'gru'} = 'lstm'\";\n\nconstexpr auto kRNNInputModeAttrs =\n    \"input_mode: {'linear_input', 'skip_input', 'auto_select'} = \"\n    \"'linear_input'\";\n\nconstexpr auto kRNNDirectionAttrs =\n    \"direction: {'unidirectional', 'bidirectional'} = 'unidirectional'\";\n\n}  // namespace\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\n\n\nREGISTER_OP(\"CudnnRNNParamsSize\")\n    .Input(\"num_layers: int32\")\n    .Input(\"num_units: int32\")\n    .Input(\"input_size: int32\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"S: {int32, int64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Output(\"params_size: S\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->Vector(1));\n      return Status::OK();\n    });\n\n\nREGISTER_OP(\"CudnnRNN\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .SetIsStateful()\n    .Output(\"output: T\")\n    .Output(\"output_h: T\")\n    .Output(\"output_c: T\")\n    .Output(\"reserve_space: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      auto seq_length = c->Dim(input_shape, 0);\n      auto batch_size = c->Dim(input_shape, 1);\n      auto num_units = c->Dim(input_h_shape, 2);\n      string direction;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n      string rnn_mode;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n      int dir_count = (direction == \"bidirectional\") ? 2 : 1;\n      DimensionHandle output_size;\n      TF_RETURN_IF_ERROR(c->Multiply(num_units, dir_count, &output_size));"
},
{
    "Id": 363,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4a1d1c8413a3752af7dc91a7128e202660b0f05c",
    "Violation": "improper",
    "Bug report": " Fix mismatch of shape restriction in DrawBoundingBoxes. In the kernel of DrawBoundingBoxes, the shape of the input images should be 4-D. Though in the shape function, at the end `UnchangedShapeWithRankAtLeast(c, 3)` was used instead (at the beginning of the shape function the validation is `WithRank(c->input(0), 4, &images)` which is correct). This fix address the discrepancy by changing to `UnchangedShape`.",
    "Number of deleted lines": 1,
    "Deleted lines": "\n// --------------------------------------------------------------------------\nREGISTER_OP(\"HSVToRGB\")\n    .Input(\"images: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, double} = DT_FLOAT\")\n    .SetShapeFn(ColorspaceShapeFn);\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"DrawBoundingBoxes\")\n    .Input(\"images: T\")\n    .Input(\"boxes: float\")\n    .Output(\"output: T\")\n    .Attr(\"T: {float, half} = DT_FLOAT\")\n    .SetShapeFn([](InferenceContext* c) {\n      // The rank of images should be 4.\n      ShapeHandle images;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 4, &images));\n      // Channel depth should be either 1 (GRY), 3 (RGB), or 4 (RGBA).\n      if (c->ValueKnown(c->Dim(images, 3))) {\n        int64 depth = c->Value(c->Dim(images, 3));\n        if (!(depth == 1 || depth == 3 || depth == 4)) {\n          return errors::InvalidArgument(\"Channel depth should be either 1 (GRY), \"\n                                         \"3 (RGB), or 4 (RGBA)\");\n        }\n      }\n\n      // The rank of boxes is 3: [batch, num_bounding_boxes, 4].\n      ShapeHandle boxes;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 3, &boxes));\n      // The last value of boxes shape is 4.\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(boxes, 2), 4, &unused));\n\n      return shape_inference::UnchangedShapeWithRankAtLeast(c, 3);\n    });\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"SampleDistortedBoundingBox\")\n    .Input(\"image_size: T\")\n    .Input(\"bounding_boxes: float\")\n    .Output(\"begin: T\")\n    .Output(\"size: T\")\n    .Output(\"bboxes: float\")\n    .Attr(\"T: {uint8, int8, int16, int32, int64}\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"min_object_covered: float = 0.1\")\n    .Attr(\"aspect_ratio_range: list(float) = [0.75, 1.33]\")\n    .Attr(\"area_range: list(float) = [0.05, 1.0]\")\n    .Attr(\"max_attempts: int = 100\")\n    .Attr(\"use_image_if_no_bounding_boxes: bool = false\")\n    .SetIsStateful()\n    .SetShapeFn([](InferenceContext* c) {\n      // Get inputs and validate ranks.\n      ShapeHandle image_size;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 1, &image_size));\n      ShapeHandle bounding_boxes;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 3, &bounding_boxes));\n      // image_size: 1-D with [height, width, channels]\n      // bounding_boxes: 3-D with shape [batch, N, 4]\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(image_size, 0), 3, &unused));\n      TF_RETURN_IF_ERROR(c->WithValue(c->Dim(bounding_boxes, 2), 4, &unused));\n\n      c->set_output(0, c->Vector(3));\n      c->set_output(1, c->Vector(3));\n      c->set_output(2, c->MakeShape({1, 1, 4}));\n      return Status::OK();\n    });\n"
},
{
    "Id": 364,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/181ca305a7954ce86a453a39db0b4f6d10b82720",
    "Violation": "missing",
    "Bug report": " Add shape validation in shape function of MapAndBatchDataset. In MapAndBatchDataset, batch_size, num_parallel_batches, and drop_remainder are 0-D scalars. This fix adds the shape check to those Inputs. Note since the Input of `other_arguments` is a list and is before `batch_size`, the shape of the `batch_size` and others could not be obtained through index like `c->input(2)` etc directly. It is still possible to obtain the ShapeHandle with names `c >input(\"batch_size\", &batch_size)`, though.",
    "Number of deleted lines": 1,
    "Deleted lines": "    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"MapDataset\")\n    .Input(\"input_dataset: variant\")\n    .Input(\"other_arguments: Targuments\")\n    .Output(\"handle: variant\")\n    .Attr(\"f: func\")\n    .Attr(\"Targuments: list(type) >= 0\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"ParallelMapDataset\")\n    .Input(\"input_dataset: variant\")\n    .Input(\"other_arguments: Targuments\")\n    .Input(\"num_parallel_calls: int32\")\n    .Output(\"handle: variant\")\n    .Attr(\"f: func\")\n    .Attr(\"Targuments: list(type) >= 0\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"MapAndBatchDataset\")\n    .Input(\"input_dataset: variant\")\n    .Input(\"other_arguments: Targuments\")\n    .Input(\"batch_size: int64\")\n    .Input(\"num_parallel_batches: int64\")\n    .Input(\"drop_remainder: bool\")\n    .Output(\"handle: variant\")\n    .Attr(\"f: func\")\n    .Attr(\"Targuments: list(type) >= 0\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"MapAndBatchDatasetV2\")\n    .Input(\"input_dataset: variant\")\n    .Input(\"other_arguments: Targuments\")\n    .Input(\"batch_size: int64\")\n    .Input(\"num_parallel_calls: int64\")\n    .Input(\"drop_remainder: bool\")\n    .Output(\"handle: variant\")\n    .Attr(\"f: func\")\n    .Attr(\"Targuments: list(type) >= 0\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"PrefetchDataset\")\n    .Input(\"input_dataset: variant\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // buffer_size should be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n\nREGISTER_OP(\"ScanDataset\")\n    .Input(\"input_dataset: variant\")\n    .Input(\"initial_state: Tstate\")\n    .Input(\"other_arguments: Targuments\")\n    .Output(\"handle: variant\")\n    .Attr(\"f: func\")\n    .Attr(\"Tstate: list(type) >= 1\")\n    .Attr(\"Targuments: list(type) >= 0\")\n    .Attr(\"output_types: list(type) >= 1\")"
},
{
    "Id": 365,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6e153325b66330dafea4e4e8b67b5d56b1a37852",
    "Violation": "missing",
    "Bug report": " [XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast produces a scalar. This avoids crashing within last_dimension when attempting to match.",
    "Number of deleted lines": 0,
    "Deleted lines": "namespace {\n\nbool HasDefaultLayout(const Shape& shape) {\n  return shape.has_layout() &&\n         LayoutUtil::IsMonotonicWithDim0Major(shape.layout());\n}\n\nbool IsTritonSupportedInstruction(const HloInstruction* instr) {\n  // TODO(bchetioui): expand with non-trivial instructions.\n  if (instr->IsElementwise()) {\n    return IsTritonSupportedElementwise(instr->opcode(),\n                                        instr->shape().element_type());\n  }\n\n  switch (instr->opcode()) {\n    case HloOpcode::kBitcast:\n    case HloOpcode::kConvert:\n    case HloOpcode::kParameter:\n      return true;\n    default:\n      return false;\n  }\n}\n\n// Returns true if a trivially connected producer of 'consumer' with opcode\n// 'opcode' exists. If such an instruction is found, the value of 'producer' is\n// set to it. The definition of \"trivial\" operations is as given in\n// 'IsTriviallyFusible'.\nbool TrivialEdge(HloInstruction** producer, HloInstruction* consumer,\n                 HloOpcode opcode);\n\nbool BitcastIsTilingNoop(HloInstruction* bitcast) {\n  CHECK_EQ(bitcast->opcode(), HloOpcode::kBitcast);\n\n  // In the Softmax rewriter for now, tiling is derived from a hero reduction\n  // operation, which should be reducing its input on the last axis. Therefore,\n  // a bitcast is always a no-op with regards to a tile if\n  //   (1) it does not change the size of the reduction dimension of its input\n  //       (the last one); if its input is already reduced, then (1) is true\n  //       by default\n  //   (2) the layout of its output is ordered in the same way as the layout of\n  //       its input. This is a fuzzy definition, but since we assume fusible\n  //       ops to always have a default layout, we can just check if both the\n  //       bitcast and its input have a default layout\n  auto last_dimension = [](const HloInstruction* instr) {\n    return instr->shape().dimensions().back();\n  };\n\n  HloInstruction* reduce = nullptr;\n  TrivialEdge(&reduce, bitcast->mutable_operand(0), HloOpcode::kReduce);\n\n  return (HasDefaultLayout(bitcast->shape()) &&\n          HasDefaultLayout(bitcast->operand(0)->shape()) &&\n          (reduce != nullptr ||\n           last_dimension(bitcast->operand(0)) == last_dimension(bitcast)));\n}\n\nbool IsTriviallyFusible(HloInstruction* instr, int num_allowed_users = 1) {\n  // Checks whether an op is trivially fusible. An op is said to be trivially\n  // fusible if it does not increase the amount of memory read/written by the\n  // resulting fusion, is compatible with any chosen tiling, and can be\n  // codegen'd using Triton. The op is allowed to have up to num_allowed_users\n  // users.\n  if (instr->user_count() > num_allowed_users ||\n      !HasDefaultLayout(instr->shape())) {\n    return false;\n  }\n\n  if (instr->opcode() == HloOpcode::kBitcast && BitcastIsTilingNoop(instr)) {\n    return true;"
},
{
    "Id": 366,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d94ffe08a65400f898241c0374e9edc6fa8ed257",
    "Violation": "missing",
    "Bug report": " Prevent an OOB read in expand_dims.cc. The for loop that follows this check assumes that `axis` is between `0` and `input_dims.size`. If user supplied `axis` is negative, the if code before this check is supposed to bring it back to positive (similar to how in Python one can do `l[-3]` to mean `l[-3 + len(l)]`).",
    "Number of deleted lines": 0,
    "Deleted lines": "\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <stdint.h>\n#include <string.h>\n\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace expand_dims {\n\n// Input indices\nenum { kInput = 0, kAxis };\n\nnamespace {\nTfLiteStatus ExpandTensorDim(TfLiteContext* context, const TfLiteTensor& input,\n                             int axis, TfLiteTensor* output) {\n  const TfLiteIntArray& input_dims = *input.dims;\n  if (axis < 0) {\n    axis = input_dims.size + 1 + axis;\n  }\n  TF_LITE_ENSURE(context, axis <= input_dims.size);\n\n  TfLiteIntArray* output_dims = TfLiteIntArrayCreate(input_dims.size + 1);\n  for (int i = 0; i < output_dims->size; ++i) {\n    if (i < axis) {\n      output_dims->data[i] = input_dims.data[i];\n    } else if (i == axis) {\n      output_dims->data[i] = 1;\n    } else {\n      output_dims->data[i] = input_dims.data[i - 1];\n    }\n  }\n\n  return context->ResizeTensor(context, output, output_dims);\n}\n\nTfLiteStatus GetAxisValueFromTensor(TfLiteContext* context,\n                                    const TfLiteTensor& axis, int* axis_value) {\n  TF_LITE_ENSURE_EQ(context, NumElements(&axis), 1);\n  switch (axis.type) {\n    case kTfLiteInt32:\n      *axis_value = *GetTensorData<int32_t>(&axis);\n      return kTfLiteOk;\n    case kTfLiteInt64:\n      *axis_value = *GetTensorData<int64_t>(&axis);\n      return kTfLiteOk;\n    default:\n      return kTfLiteError;\n  }\n}\n\n}  // namespace\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n  const TfLiteTensor* input;"
},
{
    "Id": 367,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a680ed0bf03d5ca3b2c4a70c0d95eeebc20da6d6",
    "Violation": "missing",
    "Bug report": " For Substr check pos and len rank equality only when their rank is known. This fixes a bug where len has unknown rank, while pos has known shape. The WithRank(...) check returned error in such a case. Here we compare their ranks only when both pos and len have known rank.",
    "Number of deleted lines": 2,
    "Deleted lines": "\nREGISTER_OP(\"StringStrip\")\n    .Input(\"input: string\")\n    .Output(\"output: string\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"StringLength\")\n    .Input(\"input: string\")\n    .Output(\"output: int32\")\n    .Attr(\"unit: {'BYTE', 'UTF8_CHAR'} = 'BYTE'\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"EncodeBase64\")\n    .Input(\"input: string\")\n    .Output(\"output: string\")\n    .Attr(\"pad: bool = false\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"DecodeBase64\")\n    .Input(\"input: string\")\n    .Output(\"output: string\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Substr\")\n    .Input(\"input: string\")\n    .Input(\"pos: T\")\n    .Input(\"len: T\")\n    .Output(\"output: string\")\n    .Attr(\"T: {int32, int64}\")\n    .Attr(\"unit: {'BYTE', 'UTF8_CHAR'} = 'BYTE'\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle pos_shape = c->input(1);\n      ShapeHandle len_shape = c->input(2);\n      ShapeHandle unused;\n      // Check that pos/len have same rank\n      TF_RETURN_IF_ERROR(c->WithRank(pos_shape, c->Rank(len_shape), &unused));\n      // Check that dimensions are equal\n      for (int32 i = 0; i < c->Rank(pos_shape); ++i) {\n        DimensionHandle pos_dim = c->Dim(pos_shape, i);\n        DimensionHandle len_dim = c->Dim(len_shape, i);\n        if (c->Value(pos_dim) != c->Value(len_dim)) {\n          return errors::InvalidArgument(\n              \"pos and len shapes must match: \", c->DebugString(pos_shape),\n              \" vs. \", c->DebugString(len_shape));\n        }\n      }\n      // c->input(0) is the ShapeHandle to input strings\n      // BroadcastBinaryOpShapeFn infers shape from c->input(0) and c->input(1).\n      return shape_inference::BroadcastBinaryOpShapeFn(c);\n    });\n\nREGISTER_OP(\"UnicodeScript\")\n    .Input(\"input: int32\")\n    .Output(\"output: int32\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"UnicodeEncode\")\n    .Input(\"input_values: int32\")\n    .Input(\"input_splits: Tsplits\")\n    .Attr(\"errors: {'ignore', 'replace', 'strict'} = 'replace'\")\n    .Attr(\"output_encoding: {'UTF-8', 'UTF-16-BE', 'UTF-32-BE'}\")\n    .Attr(\"replacement_char: int = 65533\")  // 0xFFFD unicode replacement char\n    .Attr(\"Tsplits: {int32, int64} = DT_INT64\")\n    .Output(\"output: string\")\n    .SetShapeFn([](InferenceContext* c) {\n      // Check rank of inner values\n      ShapeHandle input_inner_values_shape = c->input(0);\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(input_inner_values_shape, 1, &unused));\n\n      // Check rank of input_splits\n      ShapeHandle splits_shape = c->input(1);"
},
{
    "Id": 368,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9187be7adff07be82856add498aa3ff4b5f95998",
    "Violation": "missing",
    "Bug report": "add checks for compression_type and buffer_size also",
    "Number of deleted lines": 0,
    "Deleted lines": "    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"ShuffleAndRepeatDataset\")\n    .Input(\"input_dataset: variant\")\n    .Input(\"buffer_size: int64\")\n    .Input(\"seed: int64\")\n    .Input(\"seed2: int64\")\n    .Input(\"count: int64\")\n    .Output(\"handle: variant\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"CacheDataset\")\n    .Input(\"input_dataset: variant\")\n    .Input(\"filename: string\")\n    .Output(\"handle: variant\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"TextLineDataset\")\n    .Input(\"filenames: string\")\n    .Input(\"compression_type: string\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // `filenames` must be a scalar or a vector.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n\nREGISTER_OP(\"SqlDataset\")\n    .Input(\"driver_name: string\")\n    .Input(\"data_source_name: string\")\n    .Input(\"query: string\")\n    .Output(\"handle: variant\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"FixedLengthRecordDataset\")\n    .Input(\"filenames: string\")\n    .Input(\"header_bytes: int64\")\n    .Input(\"record_bytes: int64\")\n    .Input(\"footer_bytes: int64\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"TFRecordDataset\")\n    .Input(\"filenames: string\")\n    .Input(\"compression_type: string\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // `filenames` must be a scalar or a vector.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n      // `compression_type` could only be a scalar."
},
{
    "Id": 369,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/779664494d43b18a812361197dcbea2f25912c02",
    "Violation": "missing",
    "Bug report": "Add shape check to TextLineDataset op",
    "Number of deleted lines": 4,
    "Deleted lines": "    .Input(\"seed: int64\")\n    .Input(\"seed2: int64\")\n    .Output(\"handle: variant\")\n    .Attr(\"reshuffle_each_iteration: bool = true\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"ShuffleAndRepeatDataset\")\n    .Input(\"input_dataset: variant\")\n    .Input(\"buffer_size: int64\")\n    .Input(\"seed: int64\")\n    .Input(\"seed2: int64\")\n    .Input(\"count: int64\")\n    .Output(\"handle: variant\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"CacheDataset\")\n    .Input(\"input_dataset: variant\")\n    .Input(\"filename: string\")\n    .Output(\"handle: variant\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"TextLineDataset\")\n    .Input(\"filenames: string\")\n    .Input(\"compression_type: string\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn(shape_inference::ScalarShape);  // TODO(mrry): validate\n                                                // that `filenames` is\n                                                // a scalar or a\n                                                // vector.\n\nREGISTER_OP(\"SqlDataset\")\n    .Input(\"driver_name: string\")\n    .Input(\"data_source_name: string\")\n    .Input(\"query: string\")\n    .Output(\"handle: variant\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"FixedLengthRecordDataset\")\n    .Input(\"filenames: string\")\n    .Input(\"header_bytes: int64\")\n    .Input(\"record_bytes: int64\")\n    .Input(\"footer_bytes: int64\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"TFRecordDataset\")\n    .Input(\"filenames: string\")\n    .Input(\"compression_type: string\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // `filenames` must be a scalar or a vector.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n      // `compression_type` could only be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));"
},
{
    "Id": 370,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c4dea2255c71037c9cade9cbd1d7820b3429b3fa",
    "Violation": "missing",
    "Bug report": "Add shape check for buffer_size with TFRecordDataset",
    "Number of deleted lines": 0,
    "Deleted lines": "    .Input(\"driver_name: string\")\n    .Input(\"data_source_name: string\")\n    .Input(\"query: string\")\n    .Output(\"handle: variant\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"FixedLengthRecordDataset\")\n    .Input(\"filenames: string\")\n    .Input(\"header_bytes: int64\")\n    .Input(\"record_bytes: int64\")\n    .Input(\"footer_bytes: int64\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"TFRecordDataset\")\n    .Input(\"filenames: string\")\n    .Input(\"compression_type: string\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // `filenames` must be a scalar or a vector.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n      // `compression_type` could only be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused) );\n      return shape_inference::ScalarShape(c);\n    });\n\nREGISTER_OP(\"Iterator\")\n    .Output(\"handle: resource\")\n    .Attr(\"shared_name: string\")\n    .Attr(\"container: string\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"MakeIterator\")\n    .Input(\"dataset: variant\")\n    .Input(\"iterator: resource\")\n    .SetShapeFn(shape_inference::NoOutputs);\n\nREGISTER_OP(\"OneShotIterator\")\n    .Output(\"handle: resource\")\n    .Attr(\"dataset_factory: func\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .SetIsStateful()\n    .SetShapeFn(shape_inference::ScalarShape);\n\nnamespace {\n\nStatus IteratorGetNextShapeFn(shape_inference::InferenceContext* c) {\n  shape_inference::ShapeHandle unused;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n  std::vector<PartialTensorShape> output_shapes;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"output_shapes\", &output_shapes));\n  if (output_shapes.size() != c->num_outputs()) {\n    return errors::InvalidArgument(\n        \"`output_shapes` must be the same length as `output_types` (\","
},
{
    "Id": 371,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d97ffbdf362fa7d06ef8d946c8620ff7a3a50a08",
    "Violation": "missing",
    "Bug report": "Add shape check for compression_type in TFrecordDataset",
    "Number of deleted lines": 0,
    "Deleted lines": "\nREGISTER_OP(\"SqlDataset\")\n    .Input(\"driver_name: string\")\n    .Input(\"data_source_name: string\")\n    .Input(\"query: string\")\n    .Output(\"handle: variant\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"FixedLengthRecordDataset\")\n    .Input(\"filenames: string\")\n    .Input(\"header_bytes: int64\")\n    .Input(\"record_bytes: int64\")\n    .Input(\"footer_bytes: int64\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"TFRecordDataset\")\n    .Input(\"filenames: string\")\n    .Input(\"compression_type: string\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // `filenames` must be a scalar or a vector.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n\nREGISTER_OP(\"Iterator\")\n    .Output(\"handle: resource\")\n    .Attr(\"shared_name: string\")\n    .Attr(\"container: string\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"MakeIterator\")\n    .Input(\"dataset: variant\")\n    .Input(\"iterator: resource\")\n    .SetShapeFn(shape_inference::NoOutputs);\n\nREGISTER_OP(\"OneShotIterator\")\n    .Output(\"handle: resource\")\n    .Attr(\"dataset_factory: func\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .SetIsStateful()\n    .SetShapeFn(shape_inference::ScalarShape);\n\nnamespace {\n\nStatus IteratorGetNextShapeFn(shape_inference::InferenceContext* c) {\n  shape_inference::ShapeHandle unused;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n  std::vector<PartialTensorShape> output_shapes;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"output_shapes\", &output_shapes));\n  if (output_shapes.size() != c->num_outputs()) {\n    return errors::InvalidArgument(\n        \"`output_shapes` must be the same length as `output_types` (\","
},
{
    "Id": 372,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7586dee9aa8b4b63143ab658ca59658aaed0df97",
    "Violation": "missing",
    "Bug report": " Add shape check to TFRecordDataset. The inputs of TFRecordDataset have the requirements for shapes. However, the check was not done in the shape function. This fix adds shape checks whenever possible.",
    "Number of deleted lines": 1,
    "Deleted lines": "    .SetShapeFn(shape_inference::ScalarShape);  // TODO(mrry): validate\n                                                // that `filenames` is\n                                                // a scalar or a\n                                                // vector.\n\nREGISTER_OP(\"SqlDataset\")\n    .Input(\"driver_name: string\")\n    .Input(\"data_source_name: string\")\n    .Input(\"query: string\")\n    .Output(\"handle: variant\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"FixedLengthRecordDataset\")\n    .Input(\"filenames: string\")\n    .Input(\"header_bytes: int64\")\n    .Input(\"record_bytes: int64\")\n    .Input(\"footer_bytes: int64\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"TFRecordDataset\")\n    .Input(\"filenames: string\")\n    .Input(\"compression_type: string\")\n    .Input(\"buffer_size: int64\")\n    .Output(\"handle: variant\")\n    .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                      // stateful to inhibit constant folding.\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"Iterator\")\n    .Output(\"handle: resource\")\n    .Attr(\"shared_name: string\")\n    .Attr(\"container: string\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .SetShapeFn(shape_inference::ScalarShape);\n\nREGISTER_OP(\"MakeIterator\")\n    .Input(\"dataset: variant\")\n    .Input(\"iterator: resource\")\n    .SetShapeFn(shape_inference::NoOutputs);\n\nREGISTER_OP(\"OneShotIterator\")\n    .Output(\"handle: resource\")\n    .Attr(\"dataset_factory: func\")\n    .Attr(\"output_types: list(type) >= 1\")\n    .Attr(\"output_shapes: list(shape) >= 1\")\n    .Attr(\"container: string = ''\")\n    .Attr(\"shared_name: string = ''\")\n    .SetIsStateful()\n    .SetShapeFn(shape_inference::ScalarShape);\n\nnamespace {\n\nStatus IteratorGetNextShapeFn(shape_inference::InferenceContext* c) {\n  shape_inference::ShapeHandle unused;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n  std::vector<PartialTensorShape> output_shapes;\n  TF_RETURN_IF_ERROR(c->GetAttr(\"output_shapes\", &output_shapes));\n  if (output_shapes.size() != c->num_outputs()) {\n    return errors::InvalidArgument(\n        \"`output_shapes` must be the same length as `output_types` (\",\n        output_shapes.size(), \" vs. \", c->num_outputs());\n  }"
},
{
    "Id": 373,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/851177fee860211e2fabcb019d644e75b7f701b0",
    "Violation": "missing",
    "Bug report": "Add shape check for shift of tf.roll",
    "Number of deleted lines": 0,
    "Deleted lines": "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n\nnamespace tensorflow {\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Roll\")\n    .Input(\"input: T\")\n    .Input(\"shift: Tshift\")\n    .Input(\"axis: Taxis\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tshift: {int32,int64}\")\n    .Attr(\"Taxis: {int32,int64}\")\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // The `input` must be 1-D or higher\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\n      // The `axis` must be scalar or 1-D.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));\n\n      return shape_inference::UnchangedShape(c);\n    });\n\n}  // namespace tensorflow\n"
},
{
    "Id": 374,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3f796ff8c9e6d7ff88f99c056b78e88fb0b31114",
    "Violation": "missing",
    "Bug report": "Add axis shape check for tf.roll",
    "Number of deleted lines": 0,
    "Deleted lines": "/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n\nnamespace tensorflow {\n\n// --------------------------------------------------------------------------\nREGISTER_OP(\"Roll\")\n    .Input(\"input: T\")\n    .Input(\"shift: Tshift\")\n    .Input(\"axis: Taxis\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .Attr(\"Tshift: {int32,int64}\")\n    .Attr(\"Taxis: {int32,int64}\")\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // The `input` must be 1-D or higher\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\n\n      return shape_inference::UnchangedShape(c);\n    });\n\n}  // namespace tensorflow\n"
},
{
    "Id": 375,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/10467d29e05d9957a6e3cb2335f8eeba1fd8896e",
    "Violation": "missing",
    "Bug report": " Improve shape function check for tf.roll. The `tf.roll` op has requirements for the shape of inputs. However, the shape of the inputs are only done at the runtime inside the kernel. This fix improve the shape function so that the check could be done early if shape is already known in the shape function.",
    "Number of deleted lines": 1,
    "Deleted lines": "    .SetShapeFn(shape_inference::UnchangedShape);\n\n}  // namespace tensorflow\n"
},
{
    "Id": 376,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/41deb95a7bde735d3c8b9adedd8b1fe8c1ef2732",
    "Violation": "missing",
    "Bug report": "support unknown rank, check rank>=0",
    "Number of deleted lines": 0,
    "Deleted lines": "  }\n  if (!FastBoundsCheck(dim_index, s->dims_.size())) {\n    *out = nullptr;\n    return errors::InvalidArgument(\"Out of range dim_index \", dim_index_in,\n                                   \" for shape with \", s->dims_.size(),\n                                   \" dimensions\");\n  }\n  std::vector<DimensionHandle> dims(s->dims_);\n  dims[dim_index] = new_dim;\n  return ReturnCreatedShape(dims, out);\n}\n\nShapeHandle InferenceContext::MakeShape(\n    const std::vector<DimensionHandle>& dims) {\n  return shape_manager_.MakeShape(dims);\n}\n\nShapeHandle InferenceContext::MakeShape(\n    std::initializer_list<DimensionOrConstant> dims) {\n  std::vector<DimensionHandle> dims_actual;\n  dims_actual.reserve(dims.size());\n  for (const DimensionOrConstant& d : dims) {\n    dims_actual.push_back(MakeDim(d));\n  }\n\n  return shape_manager_.MakeShape(dims_actual);\n}\n\nShapeHandle InferenceContext::UnknownShape() {\n  return shape_manager_.UnknownShape();\n}\n\nShapeHandle InferenceContext::UnknownShapeOfRank(int64 rank) {\n  CHECK_LE(rank, kint32max) << \"rank must be less than kint32max\";\n  std::vector<DimensionHandle> dims(rank);\n  for (int32 i = 0; i < rank; ++i) {\n    dims[i] = UnknownDim();\n  }\n  return MakeShape(dims);\n}\n\nShapeHandle InferenceContext::Scalar() { return MakeShape({}); }\n\nShapeHandle InferenceContext::Vector(DimensionOrConstant dim) {\n  return MakeShape({dim});\n}\n\nShapeHandle InferenceContext::Matrix(DimensionOrConstant dim1,\n                                     DimensionOrConstant dim2) {\n  return MakeShape({dim1, dim2});\n}\n\nStatus InferenceContext::MakeShapeFromShapeTensor(int input_idx,\n                                                  ShapeHandle* out) {\n  ShapeHandle input_shape;\n  TF_RETURN_IF_ERROR(WithRank(input(input_idx), 1, &input_shape));\n\n  requested_input_tensor_as_partial_shape_[input_idx] = true;\n  if (input_idx < input_tensors_as_shapes_.size() &&\n      input_tensors_as_shapes_[input_idx].IsSet() &&\n      RankKnown(input_tensors_as_shapes_[input_idx])) {\n    *out = input_tensors_as_shapes_[input_idx];\n    return Status::OK();\n  }\n\n  return MakeShapeFromTensor(input_tensor(input_idx), input_shape, out);\n}\n\nStatus InferenceContext::MakeShapeFromTensor(const Tensor* t,\n                                             ShapeHandle tensor_shape,"
},
{
    "Id": 377,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8b742f8559e88474735d0a2c03e00da65e40b412",
    "Violation": "missing",
    "Bug report": "Fix check error on shape overflow.",
    "Number of deleted lines": 2,
    "Deleted lines": "    TensorShapes* input_matrix_shapes, TensorShape* batch_shape) {\n  int input_rank = -1;\n  for (int i = 0; i < NumMatrixInputs(context); ++i) {\n    const Tensor& in = context->input(i);\n    if (i == 0) {\n      input_rank = in.dims();\n      OP_REQUIRES(\n          context, input_rank >= 2,\n          errors::InvalidArgument(\"Input tensor \", i,\n                                  \" must have rank >= 2, got \", input_rank));\n      // If the tensor rank is greater than 2, we consider the inner-most\n      // dimensions as matrices, and loop over all the other outer (\"batch\")\n      // dimensions to compute the results.\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        OP_REQUIRES_OK(context,\n                       batch_shape->AddDimWithStatus(in.dim_size(dim)));\n      }\n    } else {\n      // Make sure that all inputs have the same rank and outer dimensions.\n      OP_REQUIRES(context, input_rank == in.dims(),\n                  errors::InvalidArgument(\n                      \"All input tensors must have the same rank.\"));\n      for (int dim = 0; dim < input_rank - 2; ++dim) {\n        OP_REQUIRES(\n            context, in.dim_size(dim) == batch_shape->dim_size(dim),\n            errors::InvalidArgument(\n                \"All input tensors must have the same outer dimensions.\"));\n      }\n    }\n\n    const int row_dimension = input_rank - 2;\n    const int col_dimension = input_rank - 1;\n    const int64_t num_rows = in.dim_size(row_dimension);\n    const int64_t num_cols = in.dim_size(col_dimension);\n    input_matrix_shapes->emplace_back(\n        std::initializer_list<int64_t>({num_rows, num_cols}));\n    inputs->emplace_back(&in);\n    OP_REQUIRES(\n        context, in.dtype() == DataTypeToEnum<InputScalar>::v(),\n        errors::InvalidArgument(\"Invalid input dtype \", in.dtype(), \" vs \",\n                                DataTypeToEnum<InputScalar>::v()));\n  }\n  // Have the derived class validate that the inputs are as expected.\n  ValidateInputMatrixShapes(context, *input_matrix_shapes);\n}\n\ntemplate <class InputScalar, class OutputScalar>\nvoid LinearAlgebraOp<InputScalar, OutputScalar>::PrepareOutputs(\n    OpKernelContext* context, const TensorShapes& input_matrix_shapes,\n    const TensorShape& batch_shape, TensorOutputs* outputs,\n    TensorShapes* output_matrix_shapes) {\n  // Get shape for each of the matrix outputs produced by the derived class.\n  *output_matrix_shapes = GetOutputMatrixShapes(input_matrix_shapes);\n  const int num_outputs = output_matrix_shapes->size();\n\n  // Make sure the number of op outputs is what the derived class expects.\n  OP_REQUIRES(\n      context, num_outputs <= context->num_outputs(),\n      errors::Internal(\n          \"Derived class expected more outputs (%d) that the op has (%d).\",\n          num_outputs, context->num_outputs()));\n\n  // Allocate outputs.\n  std::set<int> unused_inputs;\n  for (int input_idx = 0; input_idx < context->num_inputs(); ++input_idx) {\n    unused_inputs.insert(input_idx);\n  }\n  for (int output_idx = 0; output_idx < context->num_outputs(); ++output_idx) {\n    TensorShape output_tensor_shape({});\n    if (output_idx < num_outputs) {\n      // This output is used, set up output shape and allocate it.\n      const TensorShape& output_matrix_shape ="
},
{
    "Id": 378,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1595906c2192b7f402f746652042a592ad290378",
    "Violation": "missing",
    "Bug report": " Prevent CHECK-fail DOS in BoostedTreesSparseAggregateStatsOp. Calling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.",
    "Number of deleted lines": 0,
    "Deleted lines": "      OpKernelConstruction* const context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"max_splits\", &max_splits_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_buckets\", &num_buckets_));\n  }\n\n  void Compute(OpKernelContext* const context) override {\n    // node_ids.\n    const Tensor* node_ids_t;\n    OP_REQUIRES_OK(context, context->input(\"node_ids\", &node_ids_t));\n    const auto node_ids = node_ids_t->vec<int32>();\n\n    // gradients.\n    const Tensor* gradients_t;\n    OP_REQUIRES_OK(context, context->input(\"gradients\", &gradients_t));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsMatrix(gradients_t->shape()),\n        errors::InvalidArgument(\"gradients must be a matrix, received shape \",\n                                gradients_t->shape().DebugString()));\n    const auto gradients = gradients_t->matrix<float>();\n\n    // hessians.\n    const Tensor* hessians_t;\n    OP_REQUIRES_OK(context, context->input(\"hessians\", &hessians_t));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsMatrix(hessians_t->shape()),\n        errors::InvalidArgument(\"hessians must be a matrix, received shape \",\n                                hessians_t->shape().DebugString()));\n    const auto hessians = hessians_t->matrix<float>();\n\n    // feature indices.\n    const Tensor* feature_indices_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"feature_indices\", &feature_indices_t));\n    const auto feature_indices = feature_indices_t->matrix<int32>();\n\n    // feature values.\n    const Tensor* feature_values_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"feature_values\", &feature_values_t));\n    const auto feature_values = feature_values_t->vec<int32>();\n\n    // feature shape.\n    const Tensor* feature_shape_t;\n    OP_REQUIRES_OK(context, context->input(\"feature_shape\", &feature_shape_t));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(feature_shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    feature_shape_t->shape().DebugString()));\n    const auto feature_shape = feature_shape_t->vec<int32>();\n\n    const int64_t batch_size = gradients_t->dim_size(0);\n    const int64_t logits_dims = gradients_t->dim_size(1);\n    const int64_t hessians_dims = hessians_t->dim_size(1);\n    const int64_t stats_dims = logits_dims + hessians_dims;\n    const int64_t num_sparse_entries = feature_indices_t->dim_size(0);\n    const int32_t feature_dims = feature_shape(1);\n    OP_REQUIRES(context, num_sparse_entries <= batch_size * feature_dims,\n                errors::InvalidArgument(\n                    \"feature_indices dim0 should be <= gradients dim0 * \"\n                    \"feature_shape[1]. features_indices dim0: \",\n                    num_sparse_entries, \" gradients dim0: \", batch_size,\n                    \", feature_shape[1]: \", feature_dims));\n\n    // Aggregate statistics info to map.\n    StatsPartitionMap stats_map;\n\n    int prev_instance = 0;\n    int prev_f_dim = -1;\n"
},
{
    "Id": 379,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/54c94431e5dd17fc46d99da1a3f132c76414c161",
    "Violation": "missing",
    "Bug report": " Prevent CHECK-fail DOS in BoostedTreesSparseAggregateStatsOp. Calling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.",
    "Number of deleted lines": 0,
    "Deleted lines": "    for (int32_t f_dim = start_f_dim; f_dim < end_f_dim; ++f_dim) {\n      AddInstanceStatsToMap(instance, f_dim, bucket_id, logits_dims, stats_dims,\n                            stats_map, gradients, hessians, node_ids);\n    }\n  }\n}\n\nclass BoostedTreesSparseAggregateStatsOp : public OpKernel {\n public:\n  explicit BoostedTreesSparseAggregateStatsOp(\n      OpKernelConstruction* const context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"max_splits\", &max_splits_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_buckets\", &num_buckets_));\n  }\n\n  void Compute(OpKernelContext* const context) override {\n    // node_ids.\n    const Tensor* node_ids_t;\n    OP_REQUIRES_OK(context, context->input(\"node_ids\", &node_ids_t));\n    const auto node_ids = node_ids_t->vec<int32>();\n\n    // gradients.\n    const Tensor* gradients_t;\n    OP_REQUIRES_OK(context, context->input(\"gradients\", &gradients_t));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsMatrix(gradients_t->shape()),\n        errors::InvalidArgument(\"gradients must be a matrix, received shape \",\n                                gradients_t->shape().DebugString()));\n    const auto gradients = gradients_t->matrix<float>();\n\n    // hessians.\n    const Tensor* hessians_t;\n    OP_REQUIRES_OK(context, context->input(\"hessians\", &hessians_t));\n    const auto hessians = hessians_t->matrix<float>();\n\n    // feature indices.\n    const Tensor* feature_indices_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"feature_indices\", &feature_indices_t));\n    const auto feature_indices = feature_indices_t->matrix<int32>();\n\n    // feature values.\n    const Tensor* feature_values_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"feature_values\", &feature_values_t));\n    const auto feature_values = feature_values_t->vec<int32>();\n\n    // feature shape.\n    const Tensor* feature_shape_t;\n    OP_REQUIRES_OK(context, context->input(\"feature_shape\", &feature_shape_t));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(feature_shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    feature_shape_t->shape().DebugString()));\n    const auto feature_shape = feature_shape_t->vec<int32>();\n\n    const int64_t batch_size = gradients_t->dim_size(0);\n    const int64_t logits_dims = gradients_t->dim_size(1);\n    const int64_t hessians_dims = hessians_t->dim_size(1);\n    const int64_t stats_dims = logits_dims + hessians_dims;\n    const int64_t num_sparse_entries = feature_indices_t->dim_size(0);\n    const int32_t feature_dims = feature_shape(1);\n    OP_REQUIRES(context, num_sparse_entries <= batch_size * feature_dims,\n                errors::InvalidArgument(\n                    \"feature_indices dim0 should be <= gradients dim0 * \"\n                    \"feature_shape[1]. features_indices dim0: \",\n                    num_sparse_entries, \" gradients dim0: \", batch_size,\n                    \", feature_shape[1]: \", feature_dims));\n"
},
{
    "Id": 380,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5d96267d907ac2119cbccf1416b749195e8fd8de",
    "Violation": "missing",
    "Bug report": " Prevent CHECK-fail DOS in BoostedTreesSparseAggregateStatsOp. Calling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.",
    "Number of deleted lines": 0,
    "Deleted lines": "  if (start_instance == end_instance) {\n    DCHECK_LT(start_feature_dim, end_feature_dim);\n  }\n  for (int32_t instance = start_instance; instance <= end_instance;\n       ++instance) {\n    const int32_t start_f_dim =\n        (instance == start_instance) ? start_feature_dim + 1 : 0;\n    const int32_t end_f_dim =\n        (instance == end_instance) ? end_feature_dim : feature_dims;\n    for (int32_t f_dim = start_f_dim; f_dim < end_f_dim; ++f_dim) {\n      AddInstanceStatsToMap(instance, f_dim, bucket_id, logits_dims, stats_dims,\n                            stats_map, gradients, hessians, node_ids);\n    }\n  }\n}\n\nclass BoostedTreesSparseAggregateStatsOp : public OpKernel {\n public:\n  explicit BoostedTreesSparseAggregateStatsOp(\n      OpKernelConstruction* const context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"max_splits\", &max_splits_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_buckets\", &num_buckets_));\n  }\n\n  void Compute(OpKernelContext* const context) override {\n    // node_ids.\n    const Tensor* node_ids_t;\n    OP_REQUIRES_OK(context, context->input(\"node_ids\", &node_ids_t));\n    const auto node_ids = node_ids_t->vec<int32>();\n\n    // gradients.\n    const Tensor* gradients_t;\n    OP_REQUIRES_OK(context, context->input(\"gradients\", &gradients_t));\n    const auto gradients = gradients_t->matrix<float>();\n\n    // hessians.\n    const Tensor* hessians_t;\n    OP_REQUIRES_OK(context, context->input(\"hessians\", &hessians_t));\n    const auto hessians = hessians_t->matrix<float>();\n\n    // feature indices.\n    const Tensor* feature_indices_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"feature_indices\", &feature_indices_t));\n    const auto feature_indices = feature_indices_t->matrix<int32>();\n\n    // feature values.\n    const Tensor* feature_values_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"feature_values\", &feature_values_t));\n    const auto feature_values = feature_values_t->vec<int32>();\n\n    // feature shape.\n    const Tensor* feature_shape_t;\n    OP_REQUIRES_OK(context, context->input(\"feature_shape\", &feature_shape_t));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(feature_shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    feature_shape_t->shape().DebugString()));\n    const auto feature_shape = feature_shape_t->vec<int32>();\n\n    const int64_t batch_size = gradients_t->dim_size(0);\n    const int64_t logits_dims = gradients_t->dim_size(1);\n    const int64_t hessians_dims = hessians_t->dim_size(1);\n    const int64_t stats_dims = logits_dims + hessians_dims;\n    const int64_t num_sparse_entries = feature_indices_t->dim_size(0);\n    const int32_t feature_dims = feature_shape(1);\n    OP_REQUIRES(context, num_sparse_entries <= batch_size * feature_dims,\n                errors::InvalidArgument("
},
{
    "Id": 381,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/41ab69692ede0db3422fa70bc5889d470741e69c",
    "Violation": "missing",
    "Bug report": " Check for tensors to be vectors in BoostedTreesSparseAggregateStatsOp. Calling `tensor->vec` should only happen after checking that the tensor shape implies a vector. Otherwise, we can get denial of service via `CHECK`-fails",
    "Number of deleted lines": 0,
    "Deleted lines": "    const auto node_ids = node_ids_t->vec<int32>();\n\n    // gradients.\n    const Tensor* gradients_t;\n    OP_REQUIRES_OK(context, context->input(\"gradients\", &gradients_t));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsMatrix(gradients_t->shape()),\n        errors::InvalidArgument(\"gradients must be a matrix, received shape \",\n                                gradients_t->shape().DebugString()));\n    const auto gradients = gradients_t->matrix<float>();\n\n    // hessians.\n    const Tensor* hessians_t;\n    OP_REQUIRES_OK(context, context->input(\"hessians\", &hessians_t));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsMatrix(hessians_t->shape()),\n        errors::InvalidArgument(\"hessians must be a matrix, received shape \",\n                                hessians_t->shape().DebugString()));\n    const auto hessians = hessians_t->matrix<float>();\n\n    // feature indices.\n    const Tensor* feature_indices_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"feature_indices\", &feature_indices_t));\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(feature_indices_t->shape()),\n                errors::InvalidArgument(\n                    \"feature_indices must be a matrix, received shape \",\n                    feature_indices_t->shape().DebugString()));\n    const auto feature_indices = feature_indices_t->matrix<int32>();\n\n    // feature values.\n    const Tensor* feature_values_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"feature_values\", &feature_values_t));\n    const auto feature_values = feature_values_t->vec<int32>();\n\n    // feature shape.\n    const Tensor* feature_shape_t;\n    OP_REQUIRES_OK(context, context->input(\"feature_shape\", &feature_shape_t));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(feature_shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    feature_shape_t->shape().DebugString()));\n    const auto feature_shape = feature_shape_t->vec<int32>();\n\n    const int64_t batch_size = gradients_t->dim_size(0);\n    const int64_t logits_dims = gradients_t->dim_size(1);\n    const int64_t hessians_dims = hessians_t->dim_size(1);\n    const int64_t stats_dims = logits_dims + hessians_dims;\n    const int64_t num_sparse_entries = feature_indices_t->dim_size(0);\n    const int32_t feature_dims = feature_shape(1);\n    OP_REQUIRES(context, num_sparse_entries <= batch_size * feature_dims,\n                errors::InvalidArgument(\n                    \"feature_indices dim0 should be <= gradients dim0 * \"\n                    \"feature_shape[1]. features_indices dim0: \",\n                    num_sparse_entries, \" gradients dim0: \", batch_size,\n                    \", feature_shape[1]: \", feature_dims));\n\n    // Aggregate statistics info to map.\n    StatsPartitionMap stats_map;\n\n    int prev_instance = 0;\n    int prev_f_dim = -1;\n\n    for (int i = 0; i < num_sparse_entries; ++i) {\n      // the instance number within a batch\n      const int32_t instance = feature_indices(i, 0);\n      DCHECK_LE(instance, batch_size);\n      DCHECK_GE(instance, prev_instance);\n      // the node id within a tree."
},
{
    "Id": 382,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8d733ecdb270dd90b2b5f53fd220d5ce17a5e20f",
    "Violation": "missing",
    "Bug report": " Check for tensors to be vectors in BoostedTreesSparseAggregateStatsOp. Calling `tensor->vec` should only happen after checking that the tensor shape implies a vector. Otherwise, we can get denial of service via `CHECK`-fails",
    "Number of deleted lines": 0,
    "Deleted lines": "                          const TTypes<float>::ConstMatrix& hessians,\n                          const TTypes<int32>::ConstVec& node_ids,\n                          const int32_t feature_dims, const int32_t bucket_id,\n                          const int32_t logits_dims, const int32_t stats_dims) {\n  DCHECK_LE(start_instance, end_instance);\n  if (start_instance == end_instance) {\n    DCHECK_LT(start_feature_dim, end_feature_dim);\n  }\n  for (int32_t instance = start_instance; instance <= end_instance;\n       ++instance) {\n    const int32_t start_f_dim =\n        (instance == start_instance) ? start_feature_dim + 1 : 0;\n    const int32_t end_f_dim =\n        (instance == end_instance) ? end_feature_dim : feature_dims;\n    for (int32_t f_dim = start_f_dim; f_dim < end_f_dim; ++f_dim) {\n      AddInstanceStatsToMap(instance, f_dim, bucket_id, logits_dims, stats_dims,\n                            stats_map, gradients, hessians, node_ids);\n    }\n  }\n}\n\nclass BoostedTreesSparseAggregateStatsOp : public OpKernel {\n public:\n  explicit BoostedTreesSparseAggregateStatsOp(\n      OpKernelConstruction* const context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"max_splits\", &max_splits_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"num_buckets\", &num_buckets_));\n  }\n\n  void Compute(OpKernelContext* const context) override {\n    // node_ids.\n    const Tensor* node_ids_t;\n    OP_REQUIRES_OK(context, context->input(\"node_ids\", &node_ids_t));\n    const auto node_ids = node_ids_t->vec<int32>();\n\n    // gradients.\n    const Tensor* gradients_t;\n    OP_REQUIRES_OK(context, context->input(\"gradients\", &gradients_t));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsMatrix(gradients_t->shape()),\n        errors::InvalidArgument(\"gradients must be a matrix, received shape \",\n                                gradients_t->shape().DebugString()));\n    const auto gradients = gradients_t->matrix<float>();\n\n    // hessians.\n    const Tensor* hessians_t;\n    OP_REQUIRES_OK(context, context->input(\"hessians\", &hessians_t));\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsMatrix(hessians_t->shape()),\n        errors::InvalidArgument(\"hessians must be a matrix, received shape \",\n                                hessians_t->shape().DebugString()));\n    const auto hessians = hessians_t->matrix<float>();\n\n    // feature indices.\n    const Tensor* feature_indices_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"feature_indices\", &feature_indices_t));\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(feature_indices_t->shape()),\n                errors::InvalidArgument(\n                    \"feature_indices must be a matrix, received shape \",\n                    feature_indices_t->shape().DebugString()));\n    const auto feature_indices = feature_indices_t->matrix<int32>();\n\n    // feature values.\n    const Tensor* feature_values_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"feature_values\", &feature_values_t));\n    const auto feature_values = feature_values_t->vec<int32>();\n"
},
{
    "Id": 383,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f482488b481a799ca07e7e2d153cf47b8e91a60c",
    "Violation": "missing",
    "Bug report": " TFLite OpenGL ES delegate: out of boundary writes fixed for bhwc->phwc4 conversion.",
    "Number of deleted lines": 1,
    "Deleted lines": "      }\n      vec4 v = vec4(0);\n      int dst_channel = gid.z * 4;\n      int index = (gid.y * sizes_.x + gid.x) * sizes_.w + dst_channel;\n      for (int i = 0; i < 4; ++i, ++index, ++dst_channel) {\n        if (dst_channel >= sizes_.w) break;\n        v[i] = input_data.elements[index];\n      }\n      output_data.elements[(gid.z * sizes_.y + gid.y) * sizes_.x + gid.x] = v;\n    })\";\n\n  GlShader shader;\n  RETURN_IF_ERROR(\n      GlShader::CompileShader(GL_COMPUTE_SHADER, shader_source, &shader));\n  GlProgram program;\n  RETURN_IF_ERROR(GlProgram::CreateWithShader(shader, &program));\n  *converter = ConverterBhwcToPhwc4(std::move(program), workgroup_size);\n  return OkStatus();\n}\n\nStatus ConverterBhwcToPhwc4::Convert(const BHWC& shape, const GlBuffer& source,\n                                     CommandQueue* command_queue,\n                                     GlBuffer* destination) {\n  if (source.bytes_size() < BytesForBHWC(shape)) {\n    return InvalidArgumentError(\n        \"BhwcToPhwc4: Input data size does not match expected size.\");\n  }\n  if (destination->bytes_size() < BytesForPHWC4(shape)) {\n    return InvalidArgumentError(\n        \"BhwcToPhwc4: output data size does not match expected size.\");\n  }\n  if (shape.b != 1) {\n    return UnimplementedError(\"BhwcToPhwc4: Batch size is not equal to 1.\");\n  }\n  uint3 workload = uint3(shape.w, shape.h, shape.c);\n  uint3 num_workgroups = IntegralDivideRoundUp(workload, workgroup_size_);\n\n  RETURN_IF_ERROR(program_.SetParameter(\n      {\"sizes_\",\n       int4(static_cast<int32_t>(workload.x), static_cast<int32_t>(workload.y),\n            static_cast<int32_t>(workload.z), static_cast<int32_t>(shape.c))}));\n  RETURN_IF_ERROR(source.BindToIndex(0));\n  RETURN_IF_ERROR(destination->BindToIndex(1));\n  if (command_queue) {\n    return command_queue->Dispatch(program_, num_workgroups);\n  }\n  return program_.Dispatch(num_workgroups);\n}\n\n}  // namespace gl\n}  // namespace gpu\n}  // namespace tflite\n"
},
{
    "Id": 384,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/58759659ee547a957c5d36e72f2274ab34fdb6cb",
    "Violation": "improper",
    "Bug report": "Fix OOB check for result_index in header generation",
    "Number of deleted lines": 1,
    "Deleted lines": "    code += \"\\\"\";\n    code += entries[i].name();\n    code += \"\\\"\";\n  }\n  if (end > 0) {\n    code += \", \";\n  }\n  code += \"nullptr};\\n    return kNames;\\n  }\";\n  return code;\n}\n\nStatus ValidateFeedFetchCppNames(const tf2xla::Config& config) {\n  for (const tf2xla::Feed& feed : config.feed()) {\n    if (!feed.name().empty()) {\n      TF_RETURN_IF_ERROR(ValidateCppIdent(feed.name(), \"feed name\"));\n    }\n  }\n  for (const tf2xla::Fetch& fetch : config.fetch()) {\n    if (!fetch.name().empty()) {\n      TF_RETURN_IF_ERROR(ValidateCppIdent(fetch.name(), \"fetch name\"));\n    }\n  }\n  return Status::OK();\n}\n\n}  // namespace\n\nStatus GenerateHeader(const CodegenOpts& opts, const tf2xla::Config& config,\n                      const CompileResult& compile_result,\n                      const MetadataResult& metadata_result, string* header) {\n  TF_RETURN_IF_ERROR(ValidateConfig(config));\n  TF_RETURN_IF_ERROR(ValidateFeedFetchCppNames(config));\n  const int64 result_index = compile_result.aot->result_buffer_index();\n  const xla::BufferSizes& temp_sizes = compile_result.aot->buffer_sizes();\n  if (result_index < 0 || result_index > temp_sizes.size()) {\n    return errors::InvalidArgument(\"result index: \", result_index,\n                                   \" is outside the range of temp sizes: [0,\",\n                                   temp_sizes.size(), \")\");\n  }\n\n  // Compute sizes and generate methods.\n  std::vector<int64> arg_sizes;\n  TF_RETURN_IF_ERROR(ComputeArgSizes(compile_result, &arg_sizes));\n  const xla::ProgramShape& ps = compile_result.program_shape;\n  string methods_arg, methods_result;\n  TF_RETURN_IF_ERROR(GenArgMethods(config, ps, compile_result, &methods_arg));\n  TF_RETURN_IF_ERROR(GenResultMethods(config, ps, &methods_result));\n  const std::vector<intptr_t> iarg(arg_sizes.begin(), arg_sizes.end());\n  const std::vector<intptr_t> itemp(temp_sizes.begin(), temp_sizes.end());\n  const size_t arg_bytes_aligned =\n      runtime::aligned_buffer_bytes(iarg.data(), iarg.size());\n  const size_t arg_bytes_total = total_buffer_bytes(iarg.data(), iarg.size());\n  const size_t temp_bytes_aligned =\n      runtime::aligned_buffer_bytes(itemp.data(), itemp.size());\n  const size_t temp_bytes_total =\n      total_buffer_bytes(itemp.data(), itemp.size());\n\n  // Create rewrite strings for namespace start and end.\n  string ns_start;\n  for (const string& n : opts.namespaces) {\n    ns_start += strings::StrCat(\"namespace \", n, \" {\\n\");\n  }\n  ns_start += \"\\n\";\n  string ns_end(\"\\n\");\n  for (int i = opts.namespaces.size() - 1; i >= 0; --i) {\n    const string& n = opts.namespaces[i];\n    ns_end += strings::StrCat(\"}  // end namespace \", n, \"\\n\");\n  }\n\n  // Generate metadata.\n  const string arg_names_code ="
},
{
    "Id": 385,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/80b65ab79bf8dd6ec03c570b59a1208bb27fec24",
    "Violation": "improper",
    "Bug report": " Small fix to axis check for tfl.pack to tosa. There was an off-by-one error when checking the axis value based on the input rank.",
    "Number of deleted lines": 1,
    "Deleted lines": "    if (next_tensor_shape.size() != input_tensor_rank) {\n      (void)rewriter.notifyMatchFailure(op, \"input tensor rank mismatch\");\n      return std::nullopt;\n    }\n    for (int d = 0; d < input0_tensor_shape.size(); d++) {\n      if (input0_tensor_shape[d] != next_tensor_shape[d]) {\n        (void)rewriter.notifyMatchFailure(op, \"input tensor shape mismatch\");\n        return std::nullopt;\n      }\n    }\n  }\n\n  // If input tensors are rank 0, should reshape them to rank 1 size 1 before\n  // performing concat.\n  if (input_tensor_rank == 0) {\n    SmallVector<int64_t, 1> reshape_rank1_size1_shape({1});\n    RankedTensorType reshape_rank1_size1_type =\n        tensorflow::GetTypeFromTFTensorShape(reshape_rank1_size1_shape,\n                                             result_type.getElementType());\n    DenseI64ArrayAttr shape_rank1_size1_attr = rewriter.getDenseI64ArrayAttr(\n        tensorflow::ConvertMlirShapeToTF(reshape_rank1_size1_shape));\n    for (int i = 0; i < inputs.size(); i++) {\n      auto a0_reshape_op = CreateOpAndInfer<tosa::ReshapeOp>(\n          rewriter, op->getLoc(), reshape_rank1_size1_type, inputs[i],\n          shape_rank1_size1_attr);\n      inputs[i] = a0_reshape_op.getResult();\n    }\n  }\n\n  // Sanity check 2: axis can be from [0, rank(input)+1]\n  // Where rank(input)+1 means create a new dimension\n  // Negative values are also allowed up to -(rank(input)+1)\n  // where the axis \"wraps around\".\n  if (axis < 0) axis += input_tensor_rank;\n  if ((axis < 0) || (axis > (input_tensor_rank + 1))) {\n    (void)rewriter.notifyMatchFailure(op, \"axis out of valid range\");\n    return std::nullopt;\n  }\n\n  // Sanity check 2: if input shape is [A, B, C], output shape should be [N,\n  // A, B, C]\n  // 2.a check output is rank(input) + 1\n  SmallVector<int64_t> output_shape_vals(result_type.getShape().begin(),\n                                         result_type.getShape().end());\n  if (output_shape_vals.size() != (input_tensor_rank + 1)) {\n    (void)rewriter.notifyMatchFailure(op, \"output tensor rank mismatch\");\n    return std::nullopt;\n  }\n  // 2.b check output rank 0 is N\n  if (output_shape_vals[axis] != inputs.size()) {\n    (void)rewriter.notifyMatchFailure(op, \"output tensor shape mismatch\");\n    return std::nullopt;\n  }\n  // Most of the cases when PackOp.axis() is within [0, rank(input) - 1].\n  // We can directly concatenate along that axis and perform the reshape.\n  // For example, stack N [A, B, C] input tensor ranks along axis = 1\n  // after concatenation, output will be [A, N * B, C]\n  // and then reshape it into [A, N, B, C]\n  // a special case would be PackOp.axis() equal to rank(input), in which case\n  // we can't directly concatenate along the PackOp.axis(), instead\n  // we concat along axis=0, and reshape into [N, A, B, C]\n  // and then we need an extra transpose to [A, B, C, N].\n  int64_t concat_axis;\n  SmallVector<int32_t> perm;\n  SmallVector<int64_t> reshape_output_shape;\n  if (axis == 0 && input_tensor_rank == 0) {\n    concat_axis = 0;\n  } else if (axis == input_tensor_rank) {\n    concat_axis = 0;\n\n    // A special case when stack axis is equal to input tensor rank:"
},
{
    "Id": 386,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2ff14318050e26302785a49a1719d29ddcc91b4",
    "Violation": "improper",
    "Bug report": " [XNNPACK] Fix incorrect check in slice node. begin+size == input dimension is valid, e.g. input size is 3, begin is 2, size is 1.",
    "Number of deleted lines": 2,
    "Deleted lines": "      }\n      if (begin[i] >= input_shape->data[i]) {\n        TF_LITE_MAYBE_KERNEL_LOG(\n            logging_context,\n            \"begin %\" PRId64\n            \" must be less than input dimension %d in SLICE node #%d\",\n            begin[i], input_shape->data[i], node_index);\n      }\n      if (size[i] <= 0) {\n        if (size[i] != -1) {\n          TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n                                   \"size %\" PRId64\n                                   \" must be positive or -1 in SLICE node #%d\",\n                                   size[i], node_index);\n          return kTfLiteError;\n        }\n        size[i] = input_shape->data[i] - begin[i];\n      }\n      if (size[i] > input_shape->data[i]) {\n        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n                                 \"size %\" PRId64\n                                 \" must be less than or equals to input \"\n                                 \"dimension %d in SLICE node #%d\",\n                                 size[i], input_shape->data[i], node_index);\n        return kTfLiteError;\n      }\n      if (size[i] != output_shape->data[i]) {\n        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n                                 \"size %\" PRId64\n                                 \" does not match output shape %d at \"\n                                 \"dimension %d in SLICE node #%d\",\n                                 size[i], output_shape->data[i], i, node_index);\n        return kTfLiteError;\n      }\n      if (begin[i] + size[i] >= input_shape->data[i]) {\n        TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n                                 \"begin + size (%\" PRId64 \" + %\" PRId64\n                                 \") must be less input \"\n                                 \"dimension %d in SLICE node #%d\",\n                                 begin[i], size[i], input_shape->data[i],\n                                 node_index);\n        return kTfLiteError;\n      }\n    }\n\n    if (subgraph != nullptr) {\n      // Convert to size_t.\n      std::array<size_t, XNN_MAX_TENSOR_DIMS> offsets;\n      std::copy(begin.begin(), begin.end(), offsets.begin());\n      std::array<size_t, XNN_MAX_TENSOR_DIMS> sizes;\n      std::copy(size.begin(), size.end(), sizes.begin());\n\n      const xnn_status status = xnn_define_static_slice(\n          subgraph, num_dims, offsets.data(), sizes.data(),\n          xnnpack_tensors[node->inputs->data[0]],\n          xnnpack_tensors[node->outputs->data[0]], /*flags=*/0);\n      if (status != xnn_status_success) {\n        TF_LITE_KERNEL_LOG(logging_context, \"failed to delegate %s node #%d\",\n                           EnumNameBuiltinOperator(BuiltinOperator_SLICE),\n                           node_index);\n        return kTfLiteError;\n      }\n    }\n    return kTfLiteOk;\n  }\n\n  static TfLiteStatus VisitSoftmaxNode(\n      xnn_subgraph_t subgraph, const Delegate& delegate,\n      TfLiteContext* logging_context, int node_index, TfLiteNode* node,\n      const TfLiteTensor* tensors, const TfLiteSoftmaxParams* params,\n      const std::vector<uint32_t>& xnnpack_tensors) {\n    if (params->beta != 1.0f) {\n      if (logging_context != nullptr) {\n        TF_LITE_KERNEL_LOG(logging_context,"
},
{
    "Id": 387,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d23458fdd2655c83ff9d54725062ded31b644ba4",
    "Violation": "improper",
    "Bug report": " [XLA:CPU] Do not check that the size of the XLA parameter buffer is exactly equal to the size of the underlying given buffer Instead, check that the underlying allocation is \"large enough\". This is also more consistent with XLA:GPU behavior. The mismatch can happen when the input comes from tf.where, which is backed by an allocation larger than is actually required.",
    "Number of deleted lines": 1,
    "Deleted lines": "  XlaDebugInfoManager::Get()->RegisterModule(\n      ModuleUniqueName(module_name_, shared_module().get()), shared_module(),\n      buffer_assignment_);\n\n  // Resolve symbols in the constructor rather than at execution time to avoid\n  // races because FindSymbol is not thread safe.\n  llvm::Expected<llvm::JITEvaluatedSymbol> sym =\n      jit_->FindCompiledSymbol(entry_function_name);\n  // We expect to find the symbol provided with entry_function_name; otherwise\n  // this is an internal error.\n  CHECK(*sym) << \"Symbol \" << entry_function_name << \" not found.\";\n  // getAddress can do work under the hood in the jit, so it needs to be\n  // guarded by the mutex.\n  compute_function_ = reinterpret_cast<ComputeFunctionType>(sym->getAddress());\n  VLOG(1) << \"compute_function_ at address \"\n          << reinterpret_cast<void*>(compute_function_);\n  jit_->DoneCompiling();\n}\n\nCpuExecutable::~CpuExecutable() {\n  XlaDebugInfoManager::Get()->UnregisterModule(\n      ModuleUniqueName(module_name_, shared_module().get()), shared_module(),\n      buffer_assignment_);\n}\n\nstatic StatusOr<MaybeOwningDeviceMemory> MemoryForAllocation(\n    const BufferAllocation& allocation,\n    absl::Span<ExecutionInput const> arguments,\n    se::DeviceMemoryAllocator* memory_allocator, int device_ordinal) {\n  VLOG(3) << allocation.ToString();\n  if (allocation.is_entry_computation_parameter()) {\n    se::DeviceMemoryBase out = arguments[allocation.parameter_number()]\n                                   .Buffer(allocation.param_shape_index())\n                                   .AsDeviceMemoryBase();\n    CHECK_EQ(allocation.size(), out.size())\n        << \"Size mismatch on param \" << allocation.parameter_number()\n        << \" at shape index \" << allocation.param_shape_index().ToString();\n    VLOG(3) << \"allocation is a parameter\";\n    return MaybeOwningDeviceMemory{out};\n  } else if (allocation.is_constant()) {\n    VLOG(3) << \"allocation is a constant\";\n    return MaybeOwningDeviceMemory{se::DeviceMemoryBase{}};\n  } else if (allocation.is_thread_local()) {\n    VLOG(3) << \"buffer is thread-local\";\n    return MaybeOwningDeviceMemory{se::DeviceMemoryBase{}};\n  }\n\n  int64_t buffer_size = allocation.size();\n  TF_ASSIGN_OR_RETURN(se::OwningDeviceMemory out,\n                      memory_allocator->Allocate(device_ordinal, buffer_size));\n  VLOG(3) << \"buffer allocated \" << buffer_size << \" bytes [\" << out->opaque()\n          << \"]\";\n\n  // Since the output buffer and all the temporary buffers were written into\n  // by the JITed code, msan has no way of knowing their memory was\n  // initialized. Mark them initialized so that msan doesn't flag loads from\n  // these buffers.\n  TF_ANNOTATE_MEMORY_IS_INITIALIZED(out->opaque(), buffer_size);\n  return MaybeOwningDeviceMemory{std::move(out)};\n}\n\nStatusOr<std::vector<MaybeOwningDeviceMemory>> CpuExecutable::CreateBufferTable(\n    se::DeviceMemoryAllocator* memory_allocator, int device_ordinal,\n    absl::Span<ExecutionInput const> arguments) {\n  std::vector<MaybeOwningDeviceMemory> buffers(\n      assignment_->Allocations().size());\n  VLOG(3) << \"Allocating \" << assignment_->Allocations().size()\n          << \" allocations for module \" << module().name();\n  for (BufferAllocation::Index i = 0; i < assignment_->Allocations().size();\n       ++i) {\n    const BufferAllocation& allocation = assignment_->GetAllocation(i);"
},
{
    "Id": 388,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4863013a3ec5b97c042a38ab567bcc4a62ccde5c",
    "Violation": "insufficient",
    "Bug report": " Add checking for number of inputs in GetOptionalInputTensor to avoid indexing out of array bounds.",
    "Number of deleted lines": 1,
    "Deleted lines": "                               int index) {\n  return &context\n              ->tensors[flatbuffers::EndianScalar(node->outputs->data[index])];\n}\ninline TfLiteTensor* GetTemporary(TfLiteContext* context,\n                                  const TfLiteNode* node, int index) {\n  return &context->tensors[flatbuffers::EndianScalar(\n      node->temporaries->data[index])];\n}\ninline const TfLiteTensor* GetIntermediates(TfLiteContext* context,\n                                            const TfLiteNode* node, int index) {\n  return &context->tensors[node->intermediates->data[index]];\n}\ninline int NumInputs(const TfLiteNode* node) { return node->inputs->size; }\ninline int NumOutputs(const TfLiteNode* node) { return node->outputs->size; }\ninline int NumIntermediates(const TfLiteNode* node) {\n  return node->intermediates->size;\n}\n\ninline int64_t NumElements(const TfLiteIntArray* dims) {\n  int64_t count = 1;\n  for (int i = 0; i < dims->size; ++i) {\n    count *= dims->data[i];\n  }\n  return count;\n}\n\ninline int64_t NumElements(const TfLiteTensor* t) {\n  return NumElements(t->dims);\n}\n\ninline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,\n                                                  const TfLiteNode* node,\n                                                  int index) {\n  const bool use_tensor = node->inputs->data[index] != kTfLiteOptionalTensor;\n  if (use_tensor) {\n    return &context\n                ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];\n  }\n  return nullptr;\n}\n\n// Determines whether tensor is constant.\ninline bool IsConstantTensor(const TfLiteTensor* tensor) {\n  return tensor->allocation_type == kTfLiteMmapRo;\n}\n\n// Determines whether tensor is dynamic. Note that a tensor can be non-const and\n// not dynamic. This function specifically checks for a dynamic tensor.\ninline bool IsDynamicTensor(const TfLiteTensor* tensor) {\n  return tensor->allocation_type == kTfLiteDynamic;\n}\n\n// Sets tensor to dynamic.\ninline void SetTensorToDynamic(TfLiteTensor* tensor) {\n  if (tensor->allocation_type != kTfLiteDynamic) {\n    tensor->allocation_type = kTfLiteDynamic;\n    tensor->data.raw = nullptr;\n  }\n}\n\n// Determines whether it is a hybrid op - one that has float inputs and\n// quantized weights.\ninline bool IsHybridOp(const TfLiteTensor* input, const TfLiteTensor* weight) {\n  return ((weight->type == kTfLiteUInt8 || weight->type == kTfLiteInt8) &&\n          input->type == kTfLiteFloat32);\n}\n\n// Check dimensionality match and populate OpData for Conv and DepthwiseConv.\nTfLiteStatus PopulateConvolutionQuantizationParams(\n    TfLiteContext* context, const TfLiteTensor* input,"
},
{
    "Id": 389,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1e38a0025c9a983bf3229299109b5b1781215c7e",
    "Violation": "missing",
    "Bug report": " [XLA] CHECK that sparse indices are in range in MutableLiteralBase::AppendSparseElement. Previously there was no range-checking on sparse elements' indices.",
    "Number of deleted lines": 1,
    "Deleted lines": "inline NativeT LiteralBase::Get(absl::Span<const int64> multi_index) const {\n  return root_piece().Get<NativeT>(multi_index);\n}\n\ntemplate <typename NativeT>\ninline void MutableLiteralBase::Set(absl::Span<const int64> multi_index,\n                                    const ShapeIndex& shape_index,\n                                    NativeT value) {\n  return piece(shape_index).Set<NativeT>(multi_index, value);\n}\n\ntemplate <typename NativeT>\ninline void MutableLiteralBase::Set(absl::Span<const int64> multi_index,\n                                    NativeT value) {\n  return root_piece().Set<NativeT>(multi_index, value);\n}\n\ntemplate <typename NativeT>\nNativeT LiteralBase::GetFirstElement() const {\n  return data<NativeT>().at(0);\n}\n\ntemplate <typename NativeT>\nNativeT LiteralBase::GetSparseElement(int64 sparse_element_number,\n                                      const ShapeIndex& shape_index) const {\n  CHECK(\n      LayoutUtil::IsSparseArray(ShapeUtil::GetSubshape(shape(), shape_index)));\n  return data<NativeT>(shape_index)[sparse_element_number];\n}\n\ntemplate <typename NativeT>\nvoid MutableLiteralBase::AppendSparseElement(\n    absl::Span<const int64> multi_index, NativeT value,\n    const ShapeIndex& shape_index) {\n  // TODO(jlebar): CHECK that multi_index is in range?\n  Piece& p = piece(shape_index);\n  const Shape& subshape = p.subshape();\n  CHECK(LayoutUtil::IsSparseArray(subshape));\n  int64 rank = subshape.rank();\n  CHECK_EQ(multi_index.size(), rank);\n  int64 last_element = p.sparse_indices()->index_count();\n  CHECK_LT(last_element, LayoutUtil::MaxSparseElements(subshape.layout()));\n  p.sparse_indices()->Append(multi_index);\n  CHECK_LT(last_element, p.data<NativeT>().size());\n  p.data<NativeT>()[last_element] = value;\n}\n\ntemplate <typename NativeT>\nvoid LiteralBase::EachCell(\n    std::function<void(absl::Span<const int64> indices, NativeT value)>\n        per_cell) const {\n  if (ShapeUtil::IsZeroElementArray(shape())) {\n    return;\n  }\n  std::vector<int64> indices(shape().rank(), 0);\n  do {\n    per_cell(indices, Get<NativeT>(indices));\n  } while (IndexUtil::BumpIndices(shape(), absl::MakeSpan(indices)));\n}\n\ntemplate <typename NativeT>\ninline void MutableLiteralBase::PopulateR1(absl::Span<const NativeT> values) {\n  CHECK(shape().IsArray());\n  CHECK_EQ(shape().rank(), 1);\n  CHECK_EQ(ShapeUtil::ElementsIn(shape()), values.size());\n  CHECK_EQ(shape().element_type(),\n           primitive_util::NativeToPrimitiveType<NativeT>());\n  auto data_span = data<NativeT>();\n  std::copy(values.begin(), values.end(), data_span.begin());\n}\n\ntemplate <typename NativeT>\nvoid MutableLiteralBase::PopulateR2(\n    std::initializer_list<std::initializer_list<NativeT>> values) {\n  CHECK(shape().IsArray());\n  CHECK_EQ(shape().rank(), 2);"
},
{
    "Id": 390,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1610da3f992487bd9a8181d1e83cae99fe1e34d9",
    "Violation": "missing",
    "Bug report": "add more sanity check on AvgPoolGrad op",
    "Number of deleted lines": 0,
    "Deleted lines": "        return;\n      }\n      MklDnnShape orig_input_mkl_shape, grad_mkl_shape;\n      GetMklShape(context, kInputTensorIndexInputShape, &orig_input_mkl_shape,\n                  this->native_format_);\n      GetMklShape(context, kInputTensorIndexInputGradient, &grad_mkl_shape,\n                  this->native_format_);\n      if (!context->status().ok()) return;\n\n      // Used to allocate output_diff_src/diff_src.\n      MklDnnData<T> grad_dnn_data(&cpu_engine_);\n      MklPoolParameters pool_params;\n      this->InitMklPoolParameters(context, &pool_params, orig_input_mkl_shape,\n                                  output_shape);\n\n      memory::dims filter_dims, strides, padding_left, padding_right;\n      this->PoolParamsToDims(&pool_params, &filter_dims, &strides,\n                             &padding_left, &padding_right, is_pool2d);\n\n      memory::dims orig_input_dims_mkl_order =\n          orig_input_mkl_shape.IsMklTensor()\n              ? orig_input_mkl_shape.GetSizesAsMklDnnDims()\n              : is_pool2d ? TFShapeToMklDnnDimsInNCHW(output_shape,\n                                                      this->data_format_tf_)\n                          : TFShapeToMklDnnDimsInNCDHW(output_shape,\n                                                       this->data_format_tf_);\n\n      memory::dims diff_dst_dims =\n          grad_mkl_shape.IsMklTensor()\n              ? grad_mkl_shape.GetSizesAsMklDnnDims()\n              : is_pool2d ? TFShapeToMklDnnDimsInNCHW(grad_tensor.shape(),\n                                                      this->data_format_tf_)\n                          : TFShapeToMklDnnDimsInNCDHW(grad_tensor.shape(),\n                                                       this->data_format_tf_);\n      memory::dims output_dims_mkl_order;\n      this->GetOutputDims(pool_params, &output_dims_mkl_order);\n\n      // get src memory::desc\n      memory::desc src_md =\n          orig_input_mkl_shape.IsMklTensor()\n              ? orig_input_mkl_shape.GetMklLayout()\n              : memory::desc(orig_input_dims_mkl_order, MklDnnType<T>(),\n                             this->data_format_mkldnn_);\n\n      // Get diff_dst memory::desc.\n      memory::desc diff_dst_md =\n          grad_mkl_shape.IsMklTensor()\n              ? grad_mkl_shape.GetMklLayout()\n              : memory::desc(diff_dst_dims, MklDnnType<T>(),\n                             this->data_format_mkldnn_);\n\n      // Pass prop_kind::forward_training to create a forward primitive\n      // that is used in the backward pass.\n      MklPoolingParams bwdParams(\n          orig_input_dims_mkl_order, output_dims_mkl_order, filter_dims,\n          strides, padding_left, padding_right,\n          dnnl::algorithm::pooling_avg_exclude_padding,\n          prop_kind::forward_training,\n          static_cast<memory::format_tag>(this->data_format_mkldnn_), src_md,\n          this->native_format_);\n      MklPoolingBwdPrimitive<T>* pooling_bwd =\n          MklPoolingBwdPrimitiveFactory<T>::Get(bwdParams);\n\n      std::shared_ptr<stream> bwd_cpu_stream;\n      MklDnnThreadPool eigen_tp(context);\n      bwd_cpu_stream.reset(CreateStream(&eigen_tp, pooling_bwd->GetEngine()));\n      // TODO(intel-tf): Refactor (lines 249-262) common code for\n      // max & avg pooling into superclass or common utils function.\n      // Check whether we need to reorder diff_dst.\n      std::shared_ptr<PoolingBwdPd> pooling_bwd_pd ="
},
{
    "Id": 391,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a68f57a24203fd49c4a5c4a8f51098d4415a93f8",
    "Violation": "missing",
    "Bug report": " [XNNPACK] Add missing return when output channels do not match in TransposeConvolution Add a check that input channels in the filter and tensor match.",
    "Number of deleted lines": 0,
    "Deleted lines": "      }\n    }\n\n    const int output_tensor_index = node->outputs->data[0];\n    const TfLiteTensor& output_tensor = tensors[output_tensor_index];\n    TF_LITE_ENSURE_STATUS(\n        CheckTensorFloat32OrQUInt8Type(delegate, logging_context, output_tensor,\n                                       output_tensor_index, node_index));\n    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor, 4,\n                                           output_tensor_index));\n    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(\n        logging_context, output_tensor, output_tensor_index, node_index));\n\n    const int* input_tensor_dims = input_tensor.dims->data;\n    const int input_height = input_tensor_dims[1];\n    const int input_width = input_tensor_dims[2];\n\n    const int* filter_tensor_dims = filter_tensor.dims->data;\n    const int output_channels = filter_tensor_dims[0];\n    const int kernel_height = filter_tensor_dims[1];\n    const int kernel_width = filter_tensor_dims[2];\n    const int input_channels = filter_tensor_dims[3];\n\n    const int32_t* output_shape = GetTensorData<int32_t>(&output_shape_tensor);\n    const int output_height = output_shape[1];\n    const int output_width = output_shape[2];\n    const int output_tensor_channels = output_shape[3];\n    if (output_channels != output_tensor_channels) {\n      TF_LITE_MAYBE_KERNEL_LOG(\n          logging_context,\n          \"transpose convolution kernel output channel dimension (%d) \"\n          \"doesn't match output shape channel dimension (%d) in node #%d: \"\n          \"4 dimensions expected\",\n          output_channels, output_tensor_channels, node_index);\n    }\n\n    int padding_top = 0;\n    int padding_bottom = 0;\n    int padding_left = 0;\n    int padding_right = 0;\n    int adjustment_height = 0;\n    int adjustment_width = 0;\n    TF_LITE_ENSURE_STATUS(CalculateTransposeConvPaddings(\n        logging_context, deconv_params->padding, input_height, input_width,\n        kernel_height, kernel_width, /*dilation_height=*/1,\n        /*dilation_width=*/1, deconv_params->stride_height,\n        deconv_params->stride_width, node_index, output_height, output_width,\n        &padding_top, &padding_bottom, &padding_left, &padding_right,\n        &adjustment_height, &adjustment_width));\n\n    if (subgraph != nullptr) {\n      const xnn_status status = xnn_define_deconvolution_2d(\n          subgraph,\n          /*padding_top=*/padding_top,\n          /*padding_right=*/padding_right,\n          /*padding_bottom=*/padding_bottom,\n          /*padding_left=*/padding_left,\n          /*adjustment_height=*/adjustment_height,\n          /*adjustment_width=*/adjustment_width,\n          static_cast<uint32_t>(kernel_height),\n          static_cast<uint32_t>(kernel_width),\n          static_cast<uint32_t>(deconv_params->stride_height),\n          static_cast<uint32_t>(deconv_params->stride_width),\n          /*dilation_height=*/1,\n          /*dilation_width=*/1,\n          /*groups=*/1,\n          /*group_input_channels=*/input_channels,\n          /*group_output_channels=*/output_channels,\n          /*output_min=*/-std::numeric_limits<float>::infinity(),\n          /*output_max=*/+std::numeric_limits<float>::infinity(),"
},
{
    "Id": 392,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1b54cadd19391b60b6fcccd8d076426f7221d5e8",
    "Violation": "missing",
    "Bug report": "Add missing validation to sparse dense cwise ops.",
    "Number of deleted lines": 0,
    "Deleted lines": "\nusing Eigen::TensorRef;\nusing tensorflow::gtl::ArraySlice;\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename Device, typename T, typename Functor>\nclass SparseDenseBinaryOpShared : public OpKernel {\n public:\n  explicit SparseDenseBinaryOpShared(OpKernelConstruction *ctx)\n      : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *dense_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"sp_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"sp_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"sp_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"dense\", &dense_t));\n\n    // Validations.\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(indices_t->shape()),\n                errors::InvalidArgument(\n                    \"Input sp_indices should be a matrix but received shape: \",\n                    indices_t->shape().DebugString()));\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsVector(values_t->shape()) &&\n                    TensorShapeUtils::IsVector(shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Inputs sp_values and sp_shape should be vectors \"\n                    \"but received shapes: \",\n                    values_t->shape().DebugString(), \" and \",\n                    shape_t->shape().DebugString()));\n    OP_REQUIRES(\n        ctx, values_t->dim_size(0) == indices_t->dim_size(0),\n        errors::InvalidArgument(\n            \"The first dimension of values and indices should match. (\",\n            values_t->dim_size(0), \" vs. \", indices_t->dim_size(0), \")\"));\n\n    const auto indices_mat = indices_t->matrix<int64_t>();\n    const auto shape_vec = shape_t->vec<int64_t>();\n    const auto lhs_dims = BCast::FromShape(TensorShape(shape_vec));\n    const auto rhs_dims = BCast::FromShape(dense_t->shape());\n    BCast b(lhs_dims, rhs_dims, false);  // false for keeping the same num dims.\n\n    // True iff (size(lhs) >= size(rhs)) and all dims in lhs is greater or equal\n    // to dims in rhs (from right to left).\n    auto VecGreaterEq = [](ArraySlice<int64_t> lhs, ArraySlice<int64_t> rhs) {\n      if (lhs.size() < rhs.size()) return false;\n      for (size_t i = 0; i < rhs.size(); ++i) {\n        if (lhs[lhs.size() - 1 - i] < rhs[rhs.size() - 1 - i]) return false;\n      }\n      return true;\n    };\n    OP_REQUIRES(ctx, VecGreaterEq(lhs_dims, rhs_dims) && b.IsValid(),\n                errors::InvalidArgument(\n                    \"SparseDenseBinaryOpShared broadcasts dense to sparse \"\n                    \"only; got incompatible shapes: [\",\n                    absl::StrJoin(lhs_dims, \",\"), \"] vs. [\",\n                    absl::StrJoin(rhs_dims, \",\"), \"]\"));\n\n    Tensor *output_values = nullptr;\n    Tensor dense_gathered;\n    const int64_t nnz = indices_t->dim_size(0);\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({nnz}), &output_values));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(DataTypeToEnum<T>::value, TensorShape({nnz}),\n                                &dense_gathered));\n    bool op_is_div = false;\n    if (absl::StrContains(ctx->op_kernel().type_string_view(), \"Div\")) {\n      op_is_div = true;\n    }\n    // Pulls relevant entries from the dense side, with reshape and broadcasting"
},
{
    "Id": 393,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b51b82fe65ebace4475e3c54eb089c18a4403f1c",
    "Violation": "missing",
    "Bug report": " Add missing validation to AddManySparseToTensorsMap. Sparse tensors have a set of requirements for the 3 components and not all of them were checked.",
    "Number of deleted lines": 2,
    "Deleted lines": "\n    Tensor sparse_handle(DT_INT64, TensorShape({}));\n    auto sparse_handle_t = sparse_handle.scalar<int64_t>();\n\n    sparse_handle_t() = handle;\n\n    context->set_output(0, sparse_handle);\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"AddSparseToTensorsMap\").Device(DEVICE_CPU),\n                        AddSparseToTensorsMapOp);\n\ntemplate <typename T>\nclass AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\n public:\n  explicit AddManySparseToTensorsMapOp(OpKernelConstruction* context)\n      : SparseTensorAccessingOp(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor* input_indices;\n    const Tensor* input_values;\n    const Tensor* input_shape;\n    SparseTensorsMap* map;\n\n    OP_REQUIRES_OK(context, context->input(\"sparse_indices\", &input_indices));\n    OP_REQUIRES_OK(context, context->input(\"sparse_values\", &input_values));\n    OP_REQUIRES_OK(context, context->input(\"sparse_shape\", &input_shape));\n    OP_REQUIRES_OK(context, GetMap(context, true /* is_writing */, &map));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(input_indices->shape()),\n                errors::InvalidArgument(\n                    \"Input indices should be a matrix but received shape \",\n                    input_indices->shape().DebugString()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_values->shape()),\n                errors::InvalidArgument(\n                    \"Input values should be a vector but received shape \",\n                    input_values->shape().DebugString()));\n\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(input_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shape should be a vector but received shape \",\n                    input_shape->shape().DebugString()));\n\n    int rank = input_shape->NumElements();\n\n    OP_REQUIRES(\n        context, rank > 1,\n        errors::InvalidArgument(\n            \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\n\n    auto input_shape_vec = input_shape->vec<int64_t>();\n    int new_num_elements = 1;\n    bool overflow_ocurred = false;\n    for (int i = 0; i < input_shape_vec.size(); i++) {\n      new_num_elements =\n          MultiplyWithoutOverflow(new_num_elements, input_shape_vec(i));\n      if (new_num_elements < 0) {\n        overflow_ocurred = true;\n        break;\n      }\n    }\n\n    OP_REQUIRES(\n        context, !overflow_ocurred,\n        errors::Internal(\"Encountered overflow from large input shape.\"));\n\n    TensorShape tensor_input_shape(input_shape_vec);\n    gtl::InlinedVector<int64_t, 8> std_order(rank);\n    std::iota(std_order.begin(), std_order.end(), 0);\n    SparseTensor input_st;\n    OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\n                                                 tensor_input_shape, std_order,\n                                                 &input_st));\n\n    const int64_t N = input_shape_vec(0);\n\n    Tensor sparse_handles(DT_INT64, TensorShape({N}));\n    auto sparse_handles_t = sparse_handles.vec<int64_t>();"
},
{
    "Id": 394,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/23968a8bf65b009120c43b5ebcceaf52dbc9e943",
    "Violation": "missing",
    "Bug report": " Fix out of bound access in DequantizeOp by adding check for axis < input dimension",
    "Number of deleted lines": 0,
    "Deleted lines": "\n    need_cast_ = true;\n    if (ctx->output_type(0) == DT_FLOAT) {\n      need_cast_ = false;\n      OP_REQUIRES(ctx,\n                  (mode_string == \"MIN_COMBINED\" ||\n                   mode_string == \"MIN_FIRST\" || mode_string == \"SCALED\"),\n                  errors::InvalidArgument(\"Mode string must be 'MIN_COMBINED',\"\n                                          \" 'MIN_FIRST', or 'SCALED', is '\" +\n                                          mode_string + \"'\"));\n    } else {\n      OP_REQUIRES(\n          ctx, (mode_string == \"MIN_COMBINED\"),\n          errors::InvalidArgument(\"When output type is bfloat16, Mode\"\n                                  \" string must be 'MIN_COMBINED', is '\" +\n                                  mode_string + \"'\"));\n    }\n\n    if (mode_string == \"MIN_COMBINED\") {\n      mode_ = QUANTIZE_MODE_MIN_COMBINED;\n    } else if (mode_string == \"MIN_FIRST\") {\n      mode_ = QUANTIZE_MODE_MIN_FIRST;\n    } else if (mode_string == \"SCALED\") {\n      mode_ = QUANTIZE_MODE_SCALED;\n    }\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"narrow_range\", &narrow_range_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"axis\", &axis_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const Tensor& input_min_tensor = ctx->input(1);\n    const Tensor& input_max_tensor = ctx->input(2);\n\n    int num_slices = 1;\n    if (axis_ > -1) {\n      num_slices = input.dim_size(axis_);\n    }\n    OP_REQUIRES(ctx, input_min_tensor.NumElements() == num_slices,\n                errors::InvalidArgument(\n                    \"input_min_tensor must have as many elements as input on \"\n                    \"the dequantization axis (\",\n                    axis_, \"), got \", input_min_tensor.NumElements(),\n                    \", expected \", num_slices));\n    OP_REQUIRES(ctx, input_max_tensor.NumElements() == num_slices,\n                errors::InvalidArgument(\n                    \"input_max_tensor must have as many elements as input on \"\n                    \"the dequantization axis (\",\n                    axis_, \"), got \", input_max_tensor.NumElements(),\n                    \", expected \", num_slices));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor float_output =\n        need_cast_ ? tensorflow::Tensor(DT_FLOAT, input.shape()) : *output;\n    if (num_slices == 1) {\n      const float min_range = input_min_tensor.flat<float>()(0);\n      const float max_range = input_max_tensor.flat<float>()(0);\n      DequantizeTensor(ctx, input, min_range, max_range, &float_output);\n    } else {\n      OP_REQUIRES(ctx, mode_ != QUANTIZE_MODE_MIN_FIRST,\n                  errors::Unimplemented(\"MIN_FIRST mode is not implemented for \"\n                                        \"Dequantize with axis != -1.\"));\n\n      int64_t pre_dim = 1, post_dim = 1;\n      for (int i = 0; i < axis_; ++i) {\n        pre_dim *= float_output.dim_size(i);\n      }\n      for (int i = axis_ + 1; i < float_output.dims(); ++i) {\n        post_dim *= float_output.dim_size(i);"
},
{
    "Id": 395,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4923de56ec94fff7770df259ab7f2288a74feb41",
    "Violation": "missing",
    "Bug report": " Don't do any work when reshaping 0 elements sparse tensor. If reshaping to 0 elements tensor, check that input has no elements. If reshaping no elements input, check that output has no elements.",
    "Number of deleted lines": 0,
    "Deleted lines": "            \" output_shape=\", output_shape.DebugString()));\n    output_shape.set_dim(unknown_index, missing);\n  }\n\n  OP_REQUIRES(\n      context, output_shape.num_elements() == dense_size,\n      errors::InvalidArgument(\"Input to reshape is a tensor with \", dense_size,\n                              \" dense values, but the requested shape has \",\n                              output_shape.num_elements(),\n                              \". input_shape=\", input_shape.DebugString(),\n                              \" output_shape=\", output_shape.DebugString()));\n\n  // Optimize for reshaping to the same shape.\n  if (input_shape == output_shape) {\n    context->set_output(output_indices_idx, input_indices_in);\n    context->set_output(output_shape_idx, input_shape_in);\n    return;\n  }\n\n  Tensor *result_shape = nullptr;\n  OP_REQUIRES_OK(context, context->allocate_output(output_shape_idx,\n                                                   TensorShape({output_rank}),\n                                                   &result_shape));\n  auto output_shape_vec = result_shape->vec<int64>();\n  for (int j = 0; j < output_shape.dims(); ++j) {\n    output_shape_vec(j) = output_shape.dim_size(j);\n  }\n\n  Tensor *result_indices = nullptr;\n  OP_REQUIRES_OK(context,\n                 context->allocate_output(output_indices_idx,\n                                          TensorShape({nnz, output_rank}),\n                                          &result_indices));\n  if (nnz > 0) {\n    OP_REQUIRES_OK(context, functor::ReshapeSparseTensorFunctor<Device>()(\n                                context, input_shape, output_shape,\n                                input_indices_in.matrix<int64>(),\n                                result_indices->matrix<int64>()));\n  }\n}\n\n#define EXPLICITLY_INSTANTIATE_FUNCTION(Device)                    \\\n  template void ReshapeSparseTensor<Device>(                       \\\n      OpKernelContext * context, const Tensor &input_indices_in,   \\\n      const Tensor &input_shape_in, const Tensor &target_shape_in, \\\n      int output_indices_idx, int output_shape_idx)\nEXPLICITLY_INSTANTIATE_FUNCTION(CPUDevice);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nEXPLICITLY_INSTANTIATE_FUNCTION(GPUDevice);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#undef EXPLICITLY_INSTANTIATE_FUNCTION\n\n}  // namespace tensorflow\n"
},
{
    "Id": 396,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/467730fe90282a75f15f67d701b278e86cfad65e",
    "Violation": "missing",
    "Bug report": "Fix dimension check for tf.keras.losses.BinaryCrossentropy. The reason was that broadcasting was applied directly. This fix adds dimension check to throw an error if there is a mismatch.",
    "Number of deleted lines": 0,
    "Deleted lines": "    output = array_ops.reshape(output, [-1, output_shape[-1]])\n\n  if py_any([_is_symbolic_tensor(v) for v in [target, output]]):\n    with get_graph().as_default():\n      res = nn.sparse_softmax_cross_entropy_with_logits_v2(\n          labels=target, logits=output)\n  else:\n    res = nn.sparse_softmax_cross_entropy_with_logits_v2(\n        labels=target, logits=output)\n\n  if update_shape and output_rank >= 3:\n    # If our output includes timesteps or spatial dimensions we need to reshape\n    return array_ops.reshape(res, output_shape[:-1])\n  else:\n    return res\n\n\n@keras_export('keras.backend.binary_crossentropy')\ndef binary_crossentropy(target, output, from_logits=False):\n  \"\"\"Binary crossentropy between an output tensor and a target tensor.\n\n  Arguments:\n      target: A tensor with the same shape as `output`.\n      output: A tensor.\n      from_logits: Whether `output` is expected to be a logits tensor.\n          By default, we consider that `output`\n          encodes a probability distribution.\n\n  Returns:\n      A tensor.\n  \"\"\"\n  if not from_logits:\n    if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or\n        output.op.type != 'Sigmoid'):\n      epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n      output = clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)\n\n      # Compute cross entropy from probabilities.\n      bce = target * math_ops.log(output + epsilon())\n      bce += (1 - target) * math_ops.log(1 - output + epsilon())\n      return -bce\n    else:\n      # When sigmoid activation function is used for output operation, we\n      # use logits from the sigmoid function directly to compute loss in order\n      # to prevent collapsing zero when training.\n      assert len(output.op.inputs) == 1\n      output = output.op.inputs[0]\n  return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n\n@keras_export('keras.backend.sigmoid')\ndef sigmoid(x):\n  \"\"\"Element-wise sigmoid.\n\n  Arguments:\n      x: A tensor or variable.\n\n  Returns:\n      A tensor.\n  \"\"\"\n  return nn.sigmoid(x)\n\n\n@keras_export('keras.backend.hard_sigmoid')\ndef hard_sigmoid(x):\n  \"\"\"Segment-wise linear approximation of sigmoid.\n\n  Faster than sigmoid.\n  Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\n  In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`."
},
{
    "Id": 397,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7e2d53c1c371f38c7f0ef13c1c06336b22a195c0",
    "Violation": "missing",
    "Bug report": "[tf.data] Adds the expected check for better debugging.",
    "Number of deleted lines": 0,
    "Deleted lines": "          : DatasetIterator<Dataset>(params),\n            state_(params.dataset->initial_state_) {}\n\n      Status Initialize(IteratorContext* ctx) override {\n        TF_RETURN_IF_ERROR(\n            dataset()->input_->MakeIterator(ctx, prefix(), &input_impl_));\n        return dataset()->captured_func_->Instantiate(\n            ctx, &instantiated_captured_func_);\n      }\n\n      Status GetNextInternal(IteratorContext* ctx,\n                             std::vector<Tensor>* out_tensors,\n                             bool* end_of_sequence) override {\n        mutex_lock l(mu_);\n\n        std::vector<Tensor> next_element;\n        TF_RETURN_IF_ERROR(\n            input_impl_->GetNext(ctx, &next_element, end_of_sequence));\n        if (*end_of_sequence) {\n          return Status::OK();\n        }\n\n        std::vector<Tensor> args;\n        args.reserve(state_.size() + next_element.size());\n        std::copy(state_.begin(), state_.end(), std::back_inserter(args));\n        std::copy(next_element.begin(), next_element.end(),\n                  std::back_inserter(args));\n\n        std::vector<Tensor> state_and_output;\n        state_and_output.reserve(dataset()->state_types_.size() +\n                                 output_dtypes().size());\n\n        Status s = instantiated_captured_func_->Run(ctx, std::move(args),\n                                                    &state_and_output);\n        if (s.ok()) {\n          state_.clear();\n          size_t i = 0;\n          for (; i < dataset()->state_types_.size(); ++i) {\n            if (state_and_output[i].dtype() != dataset()->state_types_[i]) {\n              return errors::InvalidArgument(\n                  \"Got wrong type for scan_func return value \", i,\n                  \" (expected \", DataTypeString(dataset()->state_types_[i]),\n                  \", got \", DataTypeString(state_and_output[i].dtype()), \").\");\n            }\n            state_.push_back(std::move(state_and_output[i]));\n          }\n          for (; i < state_and_output.size(); ++i) {\n            const size_t output_index = i - dataset()->state_types_.size();\n            if (state_and_output[i].dtype() != output_dtypes()[output_index]) {\n              return errors::InvalidArgument(\n                  \"Got wrong type for scan_func return value \", i,\n                  \" (expected \",\n                  DataTypeString(dataset()->state_types_[output_index]),\n                  \", got \", DataTypeString(state_and_output[i].dtype()), \").\");\n            }\n            if (!output_shapes()[output_index].IsCompatibleWith(\n                    state_and_output[i].shape())) {\n              return errors::InvalidArgument(\n                  \"Got wrong shape for scan_func return value \", i,\n                  \" (expected \", output_shapes()[output_index].DebugString(),\n                  \", got \", state_and_output[i].shape().DebugString(), \").\");\n            }\n\n            out_tensors->push_back(std::move(state_and_output[i]));\n          }\n        } else if (errors::IsOutOfRange(s)) {\n          if (dataset()->preserve_cardinality_) {\n            // To guarantee that the transformation preserves the cardinality of\n            // the dataset, we convert `OutOfRange` to `InvalidArgument` as the\n            // former may be interpreted by a caller as the end of sequence."
},
{
    "Id": 398,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a12b8c4afdca3ac2945d62b3b83ca2599ab360f9",
    "Violation": "insufficient",
    "Bug report": " [xla] Improve validation of Broadcast shape. If one misreads the semantics of this instruction, it's easy to cause an out of bounds access into the dimensions here. Add an extra check to return a proper error to the user rather than crashing in that case.",
    "Number of deleted lines": 2,
    "Deleted lines": "\nStatus ShapeVerifier::HandleGetTupleElement(HloInstruction* get_tuple_element) {\n  return CheckShape(get_tuple_element,\n                    ShapeInference::InferGetTupleElementShape(\n                        get_tuple_element->operand(0)->shape(),\n                        get_tuple_element->tuple_index()));\n}\n\nStatus ShapeVerifier::HandleReduce(HloInstruction* reduce) {\n  std::vector<const Shape*> operand_shapes;\n  for (const HloInstruction* operand : reduce->operands()) {\n    operand_shapes.push_back(&operand->shape());\n  }\n  return CheckShape(reduce, ShapeInference::InferReduceShape(\n                                operand_shapes, reduce->dimensions(),\n                                reduce->to_apply()->ComputeProgramShape()));\n}\n\nStatus ShapeVerifier::HandleBitcast(HloInstruction* bitcast) {\n  return Status::OK();\n}\n\nStatus ShapeVerifier::HandleBroadcast(HloInstruction* broadcast) {\n  // HLO broadcast has no exact analog at the proto level so there is no\n  // ShapeInference method. Check the output shape explicitly.\n  const Shape& operand_shape = broadcast->operand(0)->shape();\n  // Check for mixed precision.\n  TF_RETURN_IF_ERROR(CheckShape(broadcast, broadcast->shape()));\n  TF_RET_CHECK(ShapeUtil::Rank(operand_shape) ==\n               broadcast->dimensions().size());\n  for (int64 operand_dimension = 0;\n       operand_dimension < ShapeUtil::Rank(operand_shape);\n       ++operand_dimension) {\n    int64 output_dimension = broadcast->dimensions()[operand_dimension];\n    TF_RET_CHECK(broadcast->shape().dimensions(output_dimension) ==\n                 operand_shape.dimensions(operand_dimension))\n        << broadcast->ToString() << \" operand shape \" << operand_shape;\n  }\n  return Status::OK();\n}\n\nStatus ShapeVerifier::HandleReshape(HloInstruction* reshape) {\n  // Check for mixed precision.\n  TF_RETURN_IF_ERROR(CheckShape(reshape, reshape->shape()));\n  TF_RET_CHECK(ShapeUtil::ElementsIn(reshape->shape()) ==\n               ShapeUtil::ElementsIn(reshape->operand(0)->shape()));\n  return Status::OK();\n}\n\nStatus ShapeVerifier::HandleTranspose(HloInstruction* transpose) {\n  return CheckShape(\n      transpose, ShapeInference::InferTransposeShape(\n                     transpose->operand(0)->shape(), transpose->dimensions()));\n}\n\nStatus ShapeVerifier::HandleParameter(HloInstruction* hlo) {\n  return Status::OK();\n}\n\nStatus ShapeVerifier::HandleFusion(HloInstruction* fusion) {\n  for (HloInstruction* fused_param : fusion->fused_parameters()) {\n    int64 param_no = fused_param->parameter_number();\n    if (!ShapesSame(fused_param->shape(), fusion->operand(param_no)->shape())) {\n      return InternalError(\n          \"Shape mismatch between parameter number %d and its operand in \"\n          \"%s.\",\n          param_no, fusion->ToString().c_str());\n    }\n  }\n  return Status::OK();\n}\n"
},
{
    "Id": 399,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/32dc203f55a7462ddf780c68d619af574daedd46",
    "Violation": "insufficient",
    "Bug report": "Improve gradient shape validation errors.",
    "Number of deleted lines": 2,
    "Deleted lines": "            if (not isinstance(out_grad, ops.Tensor) and\n                not out_grad) and _IsTrainable(op.outputs[i]):\n              # Only floating-point outputs get a zero gradient. Gradient\n              # functions should ignore the gradient for other outputs.\n              # TODO(apassos) gradients of resource handles might be an\n              # issue here because of zeros.\n              if loop_state:\n                out_grads[i] = loop_state.ZerosLike(op, i)\n              else:\n                out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)\n          with ops.name_scope(op.name + \"_grad\"):\n            # pylint: disable=protected-access\n            with ops.get_default_graph()._original_op(op):\n              # pylint: enable=protected-access\n              if grad_fn:\n                # If grad_fn was found, do not use SymbolicGradient even for\n                # functions.\n                in_grads = _MaybeCompile(\n                    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n              else:\n                # For function call ops, we add a 'SymbolicGradient'\n                # node to the graph to compute gradients.\n                in_grads = _MaybeCompile(\n                    grad_scope, op, func_call, lambda: _SymGrad(op, out_grads))\n              in_grads = _AsList(in_grads)\n              _VerifyGeneratedGradients(in_grads, op)\n              if gate_gradients and len(\n                  [x for x in in_grads if x is not None]) > 1:\n                in_grads = control_flow_ops.tuple(in_grads)\n          _LogOpGradients(op, out_grads, in_grads)\n        else:\n          # If no grad_fn is defined or none of out_grads is available,\n          # just propagate a list of None backwards.\n          in_grads = [None] * len(op.inputs)\n        for t_in, in_grad in zip(op.inputs, in_grads):\n          if in_grad is not None:\n            if (isinstance(in_grad, ops.Tensor) and\n                t_in.dtype != dtypes.resource):\n              in_grad.set_shape(t_in.get_shape())\n            _SetGrad(grads, t_in, in_grad)\n        if loop_state:\n          loop_state.ExitGradWhileContext(op, before=False)\n\n      # Update pending count for the inputs of op and enqueue ready ops.\n      _UpdatePendingAndEnqueueReady(grads, op, queue, pending_count, loop_state)\n\n  if loop_state:\n    loop_state.PostProcessing()\n  return [_GetGrad(grads, x) for x in xs]\n\n\ndef _HasAnyNotNoneGrads(grads, op):\n  \"\"\"Return true iff op has real gradient.\"\"\"\n  out_grads = _GetGrads(grads, op)\n  for out_grad in out_grads:\n    if isinstance(out_grad, (ops.Tensor, ops.IndexedSlices)):\n      return True\n    if out_grad and isinstance(out_grad, collections.Sequence):\n      if any([g is not None for g in out_grad]):\n        return True\n  return False\n\n\ndef _UpdatePendingAndEnqueueReady(grads, op, queue, pending_count, loop_state):\n  \"\"\"Update pending count for the inputs of op and enqueue ready ops.\"\"\"\n  for x in op.inputs:\n    # pylint: disable=protected-access\n    pending_count[x.op._id] -= 1\n    ready = (pending_count[x.op._id] == 0)\n    if loop_state and not ready:\n      ready = (pending_count[x.op._id] > 0 and\n               control_flow_ops.IsLoopSwitch(x.op))\n    # pylint: enable=protected-access\n    if ready:\n      if control_flow_ops.IsLoopExit(x.op):"
},
{
    "Id": 400,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/25821f0d91623d654bb1bdd62423e644bae9f7f8",
    "Violation": "improper",
    "Bug report": "TensorFlow: Fix OP_REQUIRES check for depthwise pooling.",
    "Number of deleted lines": 2,
    "Deleted lines": "          context, data_format_ == FORMAT_NHWC,\n          errors::InvalidArgument(\"Default MaxPoolingOp only supports NHWC.\"));\n    } else {\n      data_format_ = FORMAT_NHWC;\n    }\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 4,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 4 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 4,\n                errors::InvalidArgument(\"Sliding window stride field must \"\n                                        \"specify 4 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context, ksize_[0] == 1 && stride_[0] == 1,\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    PoolParameters params{context,  ksize_,      stride_,\n                          padding_, FORMAT_NHWC, tensor_in.shape()};\n    if (!context->status().ok()) {\n      return;\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0, params.forward_output_shape(), &output));\n\n    if (params.depth_window > 1) {\n      // Validate spec against the current implementation.  A\n      // relaxation of these requirements would be ideal.\n      OP_REQUIRES(context, params.out_depth % params.depth_window > 0,\n                  errors::Unimplemented(\n                      \"Depthwise max pooling requires \"\n                      \"the depth window to evenly divide the input depth.\"));\n      OP_REQUIRES(\n          context, params.out_depth == params.depth_stride,\n          errors::Unimplemented(\"Depthwise max pooling requires \"\n                                \"the depth window to equal the depth stride.\"));\n\n      DepthwiseMaxPool(context, output, tensor_in, params);\n    } else {\n      SpatialMaxPool(context, output, tensor_in, params, padding_);\n    }\n  }\n\n private:\n  // Single-threaded implementation of DepthwiseMaxPool which\n  // does not handle all of the same options as SpatialMaxPool\n  // (strict assumptions on no padding, stride).\n  //\n  // TODO(vrv): implement a more general depthwise-max pool that works\n  // on GPU as well.\n  void DepthwiseMaxPool(OpKernelContext* context, Tensor* output,\n                        const Tensor& tensor_in, const PoolParameters& params) {\n    Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        in_by_pool(tensor_in.flat<T>().data(), params.depth_window,\n                   tensor_in.NumElements() / params.depth_window);\n    Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>> out_by_pool(\n        output->flat<T>().data(), 1, output->NumElements());\n    out_by_pool = in_by_pool.colwise().maxCoeff();\n  }\n\n  void SpatialMaxPool(OpKernelContext* context, Tensor* output,\n                      const Tensor& tensor_in, const PoolParameters& params,\n                      const Padding& padding) {\n    // On GPU, use Eigen's Spatial Max Pooling.  On CPU, use an\n    // EigenMatrix version that is currently faster than Eigen's\n    // Spatial MaxPooling implementation.\n    //\n    // TODO(vrv): Remove this once we no longer need it.\n    if (std::is_same<Device, GPUDevice>::value) {\n      Eigen::PaddingType pt = BrainPadding2EigenPadding(padding);"
},
{
    "Id": 401,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/05ec322172958f6e67e4bcaef4681e6aa54fabeb",
    "Violation": "missing",
    "Bug report": " Return error message with illegal input rather than check-failing in op_kernel.",
    "Number of deleted lines": 0,
    "Deleted lines": "            ctx->allocate_output(i, const_tensor.shape(), &output_tensor));\n\n        Device* device = dynamic_cast<Device*>(ctx->device());\n        if (device == nullptr) {\n          return errors::Internal(\"DeviceBase was not a Device.\");\n        }\n        ctx->op_device_context()->CopyCPUTensorToDevice(\n            &const_tensor, device, output_tensor,\n            [&](Status status) { TF_CHECK_OK(status); });\n\n        if (device->device_type() == DEVICE_GPU) {\n          // The GPUDeviceContext enqueues the host->device transfer in a\n          // separate stream from the main compute stream. We must ensure the\n          // compute stream is synchronized with the host->device transfer\n          // stream now otherwise we will create a race condition.\n          auto* gpu_device_context =\n              static_cast<GPUDeviceContext*>(ctx->op_device_context());\n          gpu_device_context->stream()->ThenWaitFor(\n              gpu_device_context->host_to_device_stream());\n        }\n      } else {\n        // No copy required.\n        ctx->set_output(i, const_tensor);\n        output_tensor = ctx->mutable_output(i);\n      }\n      if (XlaTensor* xla_tensor = XlaTensor::FromTensor(output_tensor)) {\n        xla_tensor->set_host_tensor(const_tensor);\n      }\n    } else {\n      const TensorShape& shape = kernel->outputs[i].shape;\n      const DataType& type = kernel->outputs[i].type;\n      VLOG(2) << \"Retval \" << i << \" shape \" << shape.DebugString() << \" type \"\n              << DataTypeString(type);\n      if (type == DT_RESOURCE) {\n        ctx->set_output(i, ctx->input(kernel->outputs[i].input_index));\n      } else {\n        se::DeviceMemoryBase buffer = output.buffer({output_num});\n        if (allocate_xla_tensors_) {\n          Tensor* output_tensor;\n          TF_RETURN_IF_ERROR(ctx->allocate_output(i, shape, &output_tensor));\n          XlaTensor* xla_tensor = XlaTensor::FromTensor(output_tensor);\n          if (xla_tensor) {\n            xla_tensor->set_shaped_buffer(ScopedShapedBuffer(\n                ExtractSubShapedBuffer(&output, output_num, xla_allocator_)));\n            if (use_multiple_streams_) {\n              xla_tensor->SetDefinedOn(stream, definition_event);\n            }\n          } else {\n            // xla_tensor wasn't valid, which must mean this is a zero-element\n            // tensor.\n            CHECK_EQ(output_tensor->TotalBytes(), 0);\n          }\n        } else {\n          Tensor output_tensor = XlaTensorBuffer::MakeTensor(\n              ctx->expected_output_dtype(i), shape, buffer, allocator);\n          output.set_buffer(xla::OwningDeviceMemory(), {output_num});\n          ctx->set_output(i, output_tensor);\n        }\n        ++output_num;\n      }\n    }\n\n    if (VLOG_IS_ON(3)) {\n      VLOG(3) << ctx->mutable_output(i)->DebugString();\n    }\n  }\n\n  // Apply variable updates, if any.\n  VLOG(2) << \"Applying variable updates\";\n  for (int i = 0; i < kernel->resource_updates.size(); ++i) {"
},
{
    "Id": 402,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/62cb54f2caf48480dc6b3c1ce9629eaac4688f83",
    "Violation": "missing",
    "Bug report": " Set 2nd output shape for SparseSegmentReduceGradV2 Fixes a debug check failure.",
    "Number of deleted lines": 0,
    "Deleted lines": "\n  ShapeHandle indices_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &indices_shape));\n\n  // indices and segment_ids should merge cleanly.\n  ShapeHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(c->input(2), indices_shape, &unused));\n\n  // output_dim0 should be a scalar\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n\n  ShapeHandle subshape;\n  TF_RETURN_IF_ERROR(c->Subshape(data_shape, 1, &subshape));\n\n  // For the V2 version of the op, dim0 is the number of unique indices, which\n  // is always unknown at inference time.\n  const Tensor* dim0 = outputs_unique_indices ? nullptr : c->input_tensor(3);\n  ShapeHandle dim0_shape;\n  if (dim0 == nullptr) {\n    // We don't have the value at inference time, so the output\n    // shape is unknown.\n    dim0_shape = c->Vector(InferenceContext::kUnknownDim);\n  } else {\n    auto dim0_value = dim0->scalar<int32>()();\n    if (dim0_value < 0) {\n      return errors::InvalidArgument(\n          \"Cannot specify a negative value for output_dim0\");\n    }\n    dim0_shape = c->Vector(dim0_value);\n  }\n\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(c->Concatenate(dim0_shape, subshape, &out));\n  c->set_output(0, out);\n  return OkStatus();\n}\n\nStatus SparseSegmentReductionGradShapeFn(InferenceContext* c) {\n  return SparseSegmentReductionGradShapeFnImpl(\n      c,\n      /*outputs_unique_indices=*/false);\n}\n\nStatus SparseSegmentReductionGradV2ShapeFn(InferenceContext* c) {\n  return SparseSegmentReductionGradShapeFnImpl(c,\n                                               /*outputs_unique_indices=*/true);\n}\n\nStatus SparseSegmentReductionWithNumSegmentsShapeFn(InferenceContext* c) {\n  ShapeHandle data_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &data_shape));\n\n  ShapeHandle indices_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &indices_shape));\n\n  ShapeHandle segment_ids_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &segment_ids_shape));\n\n  ShapeHandle num_segments_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &num_segments_shape));\n\n  // indices and segment_ids should merge cleanly.\n  ShapeHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(indices_shape, segment_ids_shape, &unused));\n\n  ShapeHandle subshape;\n  TF_RETURN_IF_ERROR(c->Subshape(data_shape, 1, &subshape));\n\n  ShapeHandle out;\n  const Tensor* dim0 = c->input_tensor(3);"
},
{
    "Id": 403,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9b0f99ddd27e7738732a154be5469391ee8fc977",
    "Violation": "missing",
    "Bug report": "Add check to ensure element sizes are the same",
    "Number of deleted lines": 1,
    "Deleted lines": "\n  auto div_func = [&](int indexes[N]) {\n    const int32 input1_val =\n        params.input1_offset + input1_data[SubscriptToIndex(desc1, indexes)];\n    const int32 input2_val =\n        params.input2_offset + input2_data[SubscriptToIndex(desc2, indexes)];\n    TFLITE_DCHECK_NE(input2_val, 0);\n    int recip_shift;\n    const int32 input2_inv =\n        (input2_val > 0) ? GetReciprocal(input2_val, 31, &recip_shift)\n                         : -GetReciprocal(-input2_val, 31, &recip_shift);\n    const int headroom = CountLeadingSignBits(input1_val);\n    const int32 unscaled_quotient = MultiplyByQuantizedMultiplierGreaterThanOne(\n        input1_val, input2_inv, headroom);\n    const int total_shift = params.output_shift - recip_shift - headroom;\n    const int32 unclamped_result =\n        params.output_offset +\n        MultiplyByQuantizedMultiplierSmallerThanOneExp(\n            unscaled_quotient, params.output_multiplier, total_shift);\n    const int32 clamped_output =\n        std::min(params.quantized_activation_max,\n                 std::max(params.quantized_activation_min, unclamped_result));\n    output_data[SubscriptToIndex(output_desc, indexes)] =\n        static_cast<uint8>(clamped_output);\n  };\n  NDOpsHelper<N>(output_desc, div_func);\n}\n\ntemplate <typename T>\ninline void SubWithActivation(\n    const ArithmeticParams& params, const RuntimeShape& input1_shape,\n    const T* input1_data, const RuntimeShape& input2_shape,\n    const T* input2_data, const RuntimeShape& output_shape, T* output_data) {\n  ruy::profiler::ScopeLabel label(\"SubWithActivation_optimized\");\n\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto input2_map = MapAsVector(input2_data, input2_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  T activation_min, activation_max;\n  GetActivationParams(params, &activation_min, &activation_max);\n  output_map.array() = (input1_map.array() - input2_map.array())\n                           .cwiseMin(activation_max)\n                           .cwiseMax(activation_min);\n}\n\ninline void SubNonBroadcast(const ArithmeticParams& params,\n                            const RuntimeShape& input1_shape,\n                            const float* input1_data,\n                            const RuntimeShape& input2_shape,\n                            const float* input2_data,\n                            const RuntimeShape& output_shape,\n                            float* output_data) {\n  ruy::profiler::ScopeLabel label(\"SubNonBroadcast\");\n  SubWithActivation<float>(params, input1_shape, input1_data, input2_shape,\n                           input2_data, output_shape, output_data);\n}\n\ntemplate <typename T>\nvoid Sub(const ArithmeticParams& params, const RuntimeShape& input1_shape,\n         const T* input1_data, const RuntimeShape& input2_shape,\n         const T* input2_data, const RuntimeShape& output_shape,\n         T* output_data) {\n  ruy::profiler::ScopeLabel label(\"Sub\");\n\n  auto input1_map = MapAsVector(input1_data, input1_shape);\n  auto input2_map = MapAsVector(input2_data, input2_shape);\n  auto output_map = MapAsVector(output_data, output_shape);\n  if (input1_shape == input2_shape) {\n    output_map.array() = input1_map.array() - input2_map.array();\n  } else if (input1_shape.FlatSize() == 1) {\n    auto scalar = input1_data[0];"
},
{
    "Id": 404,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f8ec0f101bac066faa2e917ac714ca9eea310eac",
    "Violation": "missing",
    "Bug report": "adding checks that pad fusion works only Conv2D",
    "Number of deleted lines": 1,
    "Deleted lines": "      MklDnnConvUtil conv_utl(context, strides_, padding_, data_format_,\n                             dilations_);\n      auto src_tf_shape = GetTfShape(context, kInputIndex_Src);\n      auto filter_tf_shape = GetTfShape(context, kInputIndex_Filter);\n      conv_utl.GetConvFwdSizesInMklOrder(\n          src_tf_shape, filter_tf_shape, &src_dims, &filter_dims,\n          &strides, &dilations, &dst_dims_tf_order, &dst_dims_mkl_order,\n          &padding_left, &padding_right, padEnabled);\n      if (!context->status().ok()) return;\n\n      // Check for corner case - if there is nothing to compute, return.\n      TensorShape dst_tf_shape = MklDnnDimsToTFShape(dst_dims_tf_order);\n\n      // Corner cases: output with 0 elements and 0 batch size.\n      Tensor* dst_tensor = nullptr;\n      if (dst_tf_shape.num_elements() == 0 ||\n          dst_dims_tf_order[0] == 0) {\n        MklDnnShape dst_mkl_shape;\n        dst_mkl_shape.SetMklTensor(false);\n        AllocateOutputSetMklShape(context, kOutputIndex_Dst,\n                    &dst_tensor, src_tf_shape, dst_mkl_shape);\n\n        // MklConv2D/3D also outputs converted filter\n        // as 2nd output of Conv2D/3D.\n        filter_mkl_shape.SetMklTensor(false);\n        Tensor* output_filter_tensor = nullptr;\n        AllocateOutputSetMklShape(context, kOutputIndex_Filter,\n                                  &output_filter_tensor,\n                                  filter_tf_shape, filter_mkl_shape);\n        return;\n      }\n\n      bool isConv2D = (strides_.size() == 4);\n      // TODO(Intel-tf) Add check to make sure padEnabled is true only for 2D\n\n      // Create memory for user data.\n      // Describe how the inputs and outputs of Convolution look like. Also\n      // specify buffers containing actual input and output data.\n      auto tf_fmt = isConv2D ? TFDataFormatToMklDnnDataFormat(data_format_)\n                             : TFDataFormatToMklDnn3DDataFormat(data_format_);\n\n      // If input is in MKL layout, then simply grab input layout; otherwise,\n      // construct input Tf layout. For TF layout, although input shape\n      // (src_dims) required is in MKL-DNN order, the layout is Tensorflow's\n      // layout depending on data format:\n      //     Conv2D: NHWC or NCHW\n      //     Conv3D: NDHWC or NCDHW\n      auto src_md = src_mkl_shape.IsMklTensor()\n                        ? src_mkl_shape.GetMklLayout()\n                        : memory::desc(src_dims, MklDnnType<T>(), tf_fmt);\n\n      // Although filter shape (filter_dims) required is in MKL-DNN order,\n      // the layout is Tensorflow's layout (HWIO).\n      auto filter_md = filter_mkl_shape.IsMklTensor()  // Should NEVER be true\n                           ? filter_mkl_shape.GetMklLayout()\n                           : memory::desc(filter_dims, MklDnnType<T>(),\n                                          isConv2D ? memory::format::hwio\n                                                   : memory::format::dhwio);\n      // MKLDNN dilation starts from 0.\n      for (int i = 0; i < dilations.size(); i++) dilations[i] -= 1;\n\n      // In some cases, primitve descriptor includes potentialy large buffers,\n      // we don't cache those primitves if the env variable\n      // TF_MKL_OPTIMIZE_PRIMITVE_MEMUSE is true. MKL DNN allocates buffers\n      // in the following cases\n      //   1. Legacy CPU without AVX512/AVX2, or\n      //   2. 1x1 convolution with stride != 1\n      bool do_not_cache = MklPrimitiveFactory<T>::IsPrimitiveMemOptEnabled() &&\n                    (src_dims[MklDnnDims::Dim_N] > kSmallBatchSize) &&\n                    (MklPrimitiveFactory<T>::IsLegacyPlatform() ||\n                     IsConv1x1StrideNot1(filter_dims, strides));"
},
{
    "Id": 405,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9718fed7b9aba244359b3d38c2a1dc20e50428bd",
    "Violation": "missing",
    "Bug report": " Added size check to avoid memory corruption in GraphDefImporter::ConvertNodeDef.",
    "Number of deleted lines": 0,
    "Deleted lines": "                        ConvertAttribute(full_type_def, b_));\n    state.addAttribute(dialect_->getFullTypeAttrIdentifier(), full_type);\n    return ::tensorflow::OkStatus();\n  };\n  if (node.has_experimental_type()) {\n    TF_RETURN_IF_ERROR(add_full_type(node.experimental_type()));\n  } else if (op_reg_data && op_reg_data->type_ctor) {\n    FullTypeDef full_type_def;\n    TF_RETURN_IF_ERROR(\n        tensorflow::full_type::SpecializeType(node, *op_def, full_type_def));\n    TF_RETURN_IF_ERROR(add_full_type(full_type_def));\n  }\n\n  for (auto &name_attr : node.attr()) {\n    if (name_attr.first.empty())\n      return InvalidArgument(\"Node \", node.name(), \" has an empty attr name\");\n    TF_ASSIGN_OR_RETURN(Attribute attr,\n                        ConvertAttributeValue(name_attr.second, b_));\n    state.addAttribute(name_attr.first, attr);\n  }\n\n  // Add missing default attributes.\n  for (const auto &attr_def : op_def->attr()) {\n    if (attr_def.has_default_value() &&\n        !state.attributes.get(attr_def.name())) {\n      TF_ASSIGN_OR_RETURN(Attribute attr,\n                          ConvertAttributeValue(attr_def.default_value(), b_));\n      state.addAttribute(attr_def.name(), attr);\n    }\n  }\n\n  // Get the result types. Ops can have multiple named results. Track the\n  // segment sizes.\n  SmallVector<std::pair<unsigned, unsigned>> result_segments;\n  result_segments.reserve(op_def->output_arg_size());\n  state.types.reserve(op_def->output_arg_size() + 1);\n  for (const OpDef::ArgDef &def : op_def->output_arg()) {\n    unsigned index = state.types.size();\n    TF_ASSIGN_OR_RETURN(unsigned size,\n                        ArgNumType(state.attributes, def, state.types));\n    result_segments.emplace_back(index, size);\n  }\n  state.types.push_back(dialect_->getControlType());\n\n  // Collect the operands. Set backedges to a placeholder and resolve them\n  // later.\n  state.operands.reserve(node.input_size());\n  SmallVector<Value> control_operands;\n  struct BackedgeResolution {\n    ResultInfo *info;\n    size_t operand_index;\n    ResultId id;\n  };\n  SmallVector<BackedgeResolution> unresolved_data_operands,\n      unresolved_control_operands;\n  for (const std::string &input : node.input()) {\n    TF_ASSIGN_OR_RETURN(Result result, GetResult(s, input));\n    if (result.control) {\n      if (result.info) {\n        unresolved_control_operands.push_back(BackedgeResolution{\n            result.info, control_operands.size(), result.id});\n      }\n      control_operands.push_back(result.control);\n    } else {\n      if (result.info) {\n        unresolved_data_operands.push_back(\n            BackedgeResolution{result.info, state.operands.size(), result.id});\n      }\n      state.operands.push_back(result.data);\n    }"
},
{
    "Id": 406,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/edd9fb416e04b8ca4398c4eea65f14dc6704a44a",
    "Violation": "unnecessary",
    "Bug report": " TfLiteTensorCopy returns an error status when src and dest bytes are not equal. So we don't need to check them specifically if we ensure the status of the call to copy (which we should do anyways).",
    "Number of deleted lines": 2,
    "Deleted lines": "    TfLiteTensor* dst_tensor = dst_subgraph->tensor(dst_tensor_indices[i]);\n    if (resize_subgraph_inputs) {\n      std::vector<int> dims(src_tensor->dims->data,\n                            src_tensor->dims->data + src_tensor->dims->size);\n      dst_subgraph->ResizeInputTensor(dst_tensor_indices[i], dims);\n    } else {\n      TF_LITE_ENSURE_OK(\n          context, context->ResizeTensor(context, dst_tensor,\n                                         TfLiteIntArrayCopy(src_tensor->dims)));\n    }\n    dst_tensor->type = src_tensor->type;\n  }\n  return kTfLiteOk;\n}\n\n// Copy the tensors data from tensors `src_tensor_indices` in `src_subgraph`\n// to `dst_tensor_indices` in `dst_subgraph`.\ntemplate <typename SrcVector, typename DstVector>\nTfLiteStatus CopyTensorsData(TfLiteContext* context, Subgraph* src_subgraph,\n                             const SrcVector& src_tensor_indices,\n                             Subgraph* dst_subgraph,\n                             const DstVector& dst_tensor_indices) {\n  TF_LITE_ENSURE_EQ(context, src_tensor_indices.size(),\n                    dst_tensor_indices.size());\n  for (int i = 0; i < src_tensor_indices.size(); ++i) {\n    // Skip copying unused destination tensors.\n    if (dst_tensor_indices[i] == kTfLiteOptionalTensor) continue;\n\n    const TfLiteTensor* src_tensor =\n        src_subgraph->tensor(src_tensor_indices[i]);\n    TfLiteTensor* dst_tensor = dst_subgraph->tensor(dst_tensor_indices[i]);\n    if (IsDynamicTensor(dst_tensor)) {\n      TfLiteTensorRealloc(src_tensor->bytes, dst_tensor);\n    }\n    TF_LITE_ENSURE_EQ(context, src_tensor->bytes, dst_tensor->bytes);\n    TfLiteTensorCopy(src_tensor, dst_tensor);\n  }\n  return kTfLiteOk;\n}\n\n// Propagate tensor shapes and types from `src_tensor_indices` in `src_subgraph`\n// to `dst_tensor_indices` in `dst_subgraph` and copy data deeply.\ntemplate <typename SrcVector, typename DstVector>\nTfLiteStatus DeepCopyTensorsShapeTypeData(TfLiteContext* context,\n                                          TfLiteNode* node,\n                                          Subgraph* src_subgraph,\n                                          const SrcVector& src_tensor_indices,\n                                          Subgraph* dst_subgraph,\n                                          const DstVector& dst_tensor_indices) {\n  const OpData* op_data = reinterpret_cast<OpData*>(node->user_data);\n\n  if (op_data->body_has_dynamic_output_tensors) {\n    Subgraph* this_subgraph = reinterpret_cast<Subgraph*>(context->impl_);\n    bool resize_subgraph_inputs = (dst_subgraph != this_subgraph);\n    TF_LITE_ENSURE_OK(\n        context, CopyTensorsShapeAndType(\n                     context, src_subgraph, src_tensor_indices, dst_subgraph,\n                     dst_tensor_indices, resize_subgraph_inputs));\n    if (resize_subgraph_inputs) {\n      TF_LITE_ENSURE_OK(context, dst_subgraph->AllocateTensors());\n    }\n  }\n  TF_LITE_ENSURE_OK(context,\n                    CopyTensorsData(context, src_subgraph, src_tensor_indices,\n                                    dst_subgraph, dst_tensor_indices));\n  return kTfLiteOk;\n}\n\n// Propagate tensor shapes and types from `src_tensor_indices` in `src_subgraph`\n// to `dst_tensor_indices` in `dst_subgraph` and copy data shallowly.\ntemplate <typename SrcVector, typename DstVector>\nTfLiteStatus ShallowCopyTensorsShapeTypeData("
},
{
    "Id": 407,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8a2e7deb21f02e4072d6b62cf7f447b9264afe01",
    "Violation": "improper",
    "Bug report": " Adjust checks for type(Tensor) to isinstance or is_eager/is_symbolic_tensor.",
    "Number of deleted lines": 1,
    "Deleted lines": "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Subscribe function.\"\"\"\n\nimport contextlib\nimport re\n\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import tf_logging as logging\n\n\ndef _recursive_apply(tensors, apply_fn):\n  \"\"\"Helper method to recursively apply a function to structure of tensors.\n\n  The structure of the tensors should take the form similar to fetches in\n  `tf.compat.v1.Session` and includes single `Tensor`, `list`, nested `list`,\n  `tuple`,\n  `namedtuple`, or `dict`.\n\n  Args:\n    tensors: Single `Tensor`, `list`, nested `list, `tuple`, `namedtuple`, or\n      `dict`.\n    apply_fn: Function to apply to each `Tensor` and should return a `Tensor`.\n\n  Returns:\n    Returns the modified tensors with the same structure.\n  Raises:\n    `TypeError` if undefined type in the tensors structure.\n  \"\"\"\n  tensors_type = type(tensors)\n  if tensors_type is ops.Tensor:\n    return apply_fn(tensors)\n  elif isinstance(tensors, variables.Variable):\n    return apply_fn(tensors.value())\n  elif isinstance(tensors, (list, tuple)):\n    tensors = [_recursive_apply(t, apply_fn) for t in tensors]\n    if tensors_type is list:\n      return list(tensors)\n    elif tensors_type is tuple:\n      return tuple(tensors)\n    return tensors_type(*tensors)  # collections.namedtuple\n  elif tensors_type is dict:\n    return dict((k, _recursive_apply(v, apply_fn)) for k, v in tensors.items())\n  else:\n    raise TypeError(f'_recursive_apply argument {tensors!r} has invalid type '\n                    f'{tensors_type!r}')\n\n\nclass _ControlOutputCache(object):\n  \"\"\"Helper class to manage calculating and caching control_outputs in graph.\"\"\"\n\n  __slots__ = ['cache']\n\n  def __init__(self):\n    self.cache = {}\n\n  def calc_control_outputs(self, graph):\n    \"\"\"Returns the map of control_outputs for a given graph.\n\n    Args:\n      graph: The graph to parse.\n\n    Returns:\n      A map of the control outputs.\n    \"\"\"\n    control_outputs = {}\n    for op in graph.get_operations():"
},
{
    "Id": 408,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b68b869e75916e6de37c2ca23a93643faf333011",
    "Violation": "improper",
    "Bug report": "Fix invalid keras tensor isinstance check",
    "Number of deleted lines": 1,
    "Deleted lines": "    elif input_tensor is not None and input_tensor.dtype != dtype:\n      raise ValueError('`input_tensor.dtype` differs from `dtype`: %s vs. %s' %\n                       (input_tensor.dtype, dtype))\n    super(InputLayer, self).__init__(dtype=dtype, name=name)\n    self.built = True\n    self.sparse = sparse\n    self.ragged = ragged\n    self.batch_size = batch_size\n    self.supports_masking = True\n\n    if isinstance(input_shape, tensor_shape.TensorShape):\n      input_shape = tuple(input_shape.as_list())\n    elif isinstance(input_shape, int):\n      input_shape = (input_shape,)\n\n    if input_tensor is None:\n      if input_shape is not None:\n        batch_input_shape = (batch_size,) + tuple(input_shape)\n      else:\n        batch_input_shape = None\n      graph = backend.get_graph()\n      with graph.as_default():\n        input_tensor = backend.placeholder(\n            shape=batch_input_shape,\n            dtype=dtype,\n            name=self.name,\n            sparse=sparse,\n            ragged=ragged)\n\n      self.is_placeholder = True\n      self._batch_input_shape = batch_input_shape\n    else:\n      raise_eager_tensor_error = False\n      if keras_tensor.keras_tensors_enabled():\n        if not isinstance(input_tensor, keras_tensor.keras_tensors_enabled()):\n          raise_eager_tensor_error = True\n      else:\n        if not tf_utils.is_symbolic_tensor(input_tensor):\n          raise_eager_tensor_error = True\n      if raise_eager_tensor_error:\n        raise ValueError('You should not pass an EagerTensor to `Input`. '\n                         'For example, instead of creating an '\n                         'InputLayer, you should instantiate your model and '\n                         'directly call it on your input.')\n      self.is_placeholder = False\n      try:\n        self._batch_input_shape = tuple(input_tensor.shape.as_list())\n      except ValueError:\n        # If the shape cannot be represented as a tuple (e.g. unknown rank)\n        self._batch_input_shape = None\n    # Create an input node.\n    input_tensor._keras_mask = None\n    node_module.Node(layer=self, outputs=input_tensor)\n\n    # Store type spec\n    if isinstance(input_tensor, (\n        composite_tensor.CompositeTensor, keras_tensor.KerasTensor)):\n      self._type_spec = input_tensor._type_spec  # pylint: disable=protected-access\n    else:\n      self._type_spec = tensor_spec.TensorSpec(\n          shape=input_tensor.shape, dtype=input_tensor.dtype, name=self.name)\n\n  def get_config(self):\n    config = {\n        'batch_input_shape': self._batch_input_shape,\n        'dtype': self.dtype,\n        'sparse': self.sparse,\n        'ragged': self.ragged,\n        'name': self.name\n    }\n    return config"
},
{
    "Id": 409,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/040aaf39aebda57921991d05d29be5123e908d7c",
    "Violation": "missing",
    "Bug report": "Don't check that bool arrays are quantized.",
    "Number of deleted lines": 4,
    "Deleted lines": "}\n\nbool ReshapeIsEquivalentToTranspose(const Model& model,\n                                    const TensorFlowReshapeOperator* op,\n                                    bool allow_extra_unary_dims) {\n  CHECK(!op->shape.empty());\n  CHECK(model.HasArray(op->inputs[0]));\n  CHECK(model.HasArray(op->outputs[0]));\n\n  const auto& input_array = model.GetArray(op->inputs[0]);\n  const auto& output_array = model.GetArray(op->outputs[0]);\n\n  CHECK(input_array.has_shape());\n  CHECK(output_array.has_shape());\n\n  std::vector<int> in_shape = input_array.shape().dims();\n  std::vector<int> out_shape = output_array.shape().dims();\n\n  // If the reshape changes the number of dimensions so it cannot be interpreted\n  // as a transpose.\n  if (!allow_extra_unary_dims && in_shape.size() != out_shape.size()) {\n    return false;\n  }\n\n  in_shape.erase(std::remove(in_shape.begin(), in_shape.end(), 1),\n                 in_shape.end());\n  out_shape.erase(std::remove(out_shape.begin(), out_shape.end(), 1),\n                  out_shape.end());\n  return in_shape == out_shape;\n}\n\nvoid CheckFinalDataTypesSatisfied(const Model& model) {\n  for (const auto& array_entry : model.GetArrayMap()) {\n    const auto& array = *array_entry.second;\n    // If the final data type is int16, the data type may be float, for example\n    // after dequantization.\n    if (array.final_data_type != ArrayDataType::kNone &&\n        array.final_data_type != ArrayDataType::kInt16) {\n      CHECK(array.final_data_type == array.data_type)\n          << \"Array \\\"\" << array_entry.first\n          << \"\\\" has mis-matching actual and final data types (\"\n          << ArrayDataTypeName(array.data_type) << \",\"\n          << ArrayDataTypeName(array.final_data_type) << \").\";\n    }\n  }\n}\n\nArrayDataType ConvertIODataTypeToArrayDataType(IODataType type) {\n  switch (type) {\n    case FLOAT:\n      return ArrayDataType::kFloat;\n    case QUANTIZED_UINT8:\n      return ArrayDataType::kUint8;\n    case QUANTIZED_INT16:\n      return ArrayDataType::kInt16;\n    case INT32:\n      return ArrayDataType::kInt32;\n    case INT64:\n      return ArrayDataType::kInt64;\n    case BOOL:\n      return ArrayDataType::kBool;\n    default:\n      return ArrayDataType::kNone;\n  }\n}\n\nvoid FinishBuildingRNNStates(Model* model) {\n  for (const auto& rnn_state : model->flags.rnn_states()) {\n    if (!model->HasArray(rnn_state.back_edge_source_array()) ||\n        !model->HasArray(rnn_state.state_array())) {\n      CHECK(model->HasArray(rnn_state.back_edge_source_array()));\n      CHECK(model->HasArray(rnn_state.state_array()));\n      continue;\n    }\n    const auto& src_array = model->GetArray(rnn_state.back_edge_source_array());\n    auto& dst_array = model->GetArray(rnn_state.state_array());\n    if (src_array.data_type == ArrayDataType::kNone &&\n        dst_array.data_type == ArrayDataType::kNone) {\n      dst_array.data_type = ArrayDataType::kFloat;"
},
{
    "Id": 410,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a0de0ca6a39f3037e1be6ec740829863bcda3e8",
    "Violation": "improper",
    "Bug report": "[XLA:GPU] Fix type check in IsMatrixMultiplication",
    "Number of deleted lines": 1,
    "Deleted lines": "\nShape GetShapeFromTensorType(mlir::Value value) {\n  constexpr char kDefaultLayoutAttrName[] = \"xla_shape\";\n\n  mlir::Operation* op = value.getDefiningOp();\n  CHECK(op);\n  CHECK(value.getType().isa<mlir::TensorType>());\n  Shape shape;\n  if (auto attr = op->getAttrOfType<mlir::StringAttr>(kDefaultLayoutAttrName)) {\n    shape = *xla::ParseShape(\n        absl::string_view(attr.getValue().data(), attr.getValue().size()));\n  } else {\n    shape = TypeToShape(value.getType());\n  }\n  return shape;\n}\n\n}  // namespace\n\nbool IsMatrixMultiplication(const HloInstruction& dot) {\n  if (dot.opcode() != HloOpcode::kDot) {\n    return false;\n  }\n  const Shape& lhs_shape = dot.operand(0)->shape();\n  const Shape& rhs_shape = dot.operand(1)->shape();\n  const DotDimensionNumbers& dim_numbers = dot.dot_dimension_numbers();\n\n  PrimitiveType output_primitive_type = dot.shape().element_type();\n  bool type_is_allowed =\n      (output_primitive_type == F8E4M3FN || output_primitive_type == F8E5M2 ||\n       output_primitive_type == F16 || output_primitive_type == BF16 ||\n       output_primitive_type == F32 || output_primitive_type == F64 ||\n       output_primitive_type == C64 || output_primitive_type == C128) ||\n      (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&\n       lhs_shape.element_type() == S8);\n  bool shapes_are_valid =\n      type_is_allowed &&\n      IsRank2(lhs_shape, dim_numbers.lhs_batch_dimensions_size()) &&\n      IsRank2(rhs_shape, dim_numbers.lhs_batch_dimensions_size()) &&\n      IsRank2(dot.shape(), dim_numbers.lhs_batch_dimensions_size()) &&\n      !ShapeUtil::IsZeroElementArray(lhs_shape) &&\n      !ShapeUtil::IsZeroElementArray(rhs_shape);\n\n  if (!shapes_are_valid) {\n    return false;\n  }\n\n  // The size of the reduction dimension should match. The shape inference\n  // guarantees this invariant, so the check here is for programming\n  // errors.\n  CHECK_EQ(lhs_shape.dimensions(dim_numbers.lhs_contracting_dimensions(0)),\n           rhs_shape.dimensions(dim_numbers.rhs_contracting_dimensions(0)));\n\n  return true;\n}\n\nVector3 GetReductionTiling(const ReductionDimensions& reduction_dimensions) {\n  if (reduction_dimensions.is_row_reduction) {\n    int64_t tile_z = std::min(reduction_dimensions.dimensions[0],\n                              BatchedReductionRaceFreeBound());\n    return {tile_z, 1, 16};\n  }\n\n  // Column reduction.\n  return {1, 128, 1};\n}\n\nconst char* const kCusolverCholeskyCallTarget = \"__cusolver$cholesky\";\n\nbool IsCustomCallToCusolver(const HloInstruction& hlo) {\n  if (hlo.opcode() != HloOpcode::kCustomCall) {"
},
{
    "Id": 411,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/db9b247cd1f3ff046359f7b64ca60c2d697fe2e1",
    "Violation": "insufficient",
    "Bug report": " Fix the functional model loading with nested sequential model. The nested sequential model is created with _is_graph_network = False, the current instance check is not strong enough.",
    "Number of deleted lines": 1,
    "Deleted lines": "  if layer not in layer_indices:\n    layer_indices[layer] = len(layer_indices)\n\n  # Propagate to all previous tensors connected to this node.\n  nodes_in_progress.add(node)\n  if not node.is_input:\n    for tensor in node.keras_inputs:\n      _build_map_helper(tensor, finished_nodes, nodes_in_progress,\n                        nodes_in_decreasing_depth, layer_indices)\n\n  finished_nodes.add(node)\n  nodes_in_progress.remove(node)\n  nodes_in_decreasing_depth.append(node)\n\n\ndef _map_subgraph_network(inputs, outputs):\n  \"\"\"Returns the nodes and layers in the topology from `inputs` to `outputs`.\n\n  Args:\n    inputs: List of input tensors.\n    outputs: List of output tensors.\n\n  Returns:\n    A tuple of List{Node] and List[Layer].\n  \"\"\"\n  base_layer_utils.create_keras_history(outputs)\n  # Keep only nodes and layers in the topology between inputs and outputs.\n  _, nodes_by_depth, layers, _ = _map_graph_network(inputs, outputs)\n  return nest.flatten([nodes for nodes in nodes_by_depth.values()]), layers\n\n\ndef _should_skip_first_node(layer):\n  \"\"\"Returns True if the first layer node should not be saved or loaded.\"\"\"\n  # Networks start with a pre-existing node linking their input to output.\n  return isinstance(layer, Functional)\n\n\ndef _deserialize_keras_tensors(kwargs, layer_map):\n  \"\"\"Deserializes Keras Tensors passed to `call`..\"\"\"\n\n  def _deserialize_keras_tensor(t):\n    \"\"\"Deserializes a single Keras Tensor passed to `call`.\"\"\"\n    if isinstance(t, tf_utils.ListWrapper):\n      t = t.as_list()\n      layer_name = t[0]\n      node_index = t[1]\n      tensor_index = t[2]\n\n      layer = layer_map[layer_name]\n      node = layer._inbound_nodes[node_index]\n      return nest.flatten(node.outputs)[tensor_index]\n    return t\n\n  kwargs = tf_utils.convert_inner_node_data(kwargs, wrap=True)\n  return nest.map_structure(_deserialize_keras_tensor, kwargs)\n\n\ndef connect_ancillary_layers(model, created_layers):\n  \"\"\"Adds layers that are not connected to the outputs to the model.\"\"\"\n  # Layers not connected to outputs, such as those added in `add_loss`.\n  ancillary_layers = [\n      layer for layer in created_layers.values() if layer not in model.layers\n  ]\n  if ancillary_layers:\n    relevant_nodes = nest.flatten([\n        layer.inbound_nodes[1:]\n        if _should_skip_first_node(layer) else layer.inbound_nodes\n        for layer in created_layers.values()\n    ])\n    model._insert_layers(ancillary_layers, relevant_nodes)\n  return model"
},
{
    "Id": 412,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a4b6b6bcc7a813162bf0378727950e321aca19c",
    "Violation": "improper",
    "Bug report": "Add stricter type checking for tf.math.real (using is_numeric)",
    "Number of deleted lines": 1,
    "Deleted lines": "  return gen_math_ops.sign(x, name=name)\n\n\n@tf_export(\"math.real\", v1=[\"math.real\", \"real\"])\n@dispatch.register_unary_elementwise_api\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints(\"real\")\ndef real(input, name=None):\n  r\"\"\"Returns the real part of a complex (or real) tensor.\n\n  Given a tensor `input`, this operation returns a tensor of type `float` that\n  is the real part of each element in `input` considered as a complex number.\n\n  For example:\n\n  ```python\n  x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])\n  tf.math.real(x)  # [-2.25, 3.25]\n  ```\n\n  If `input` is already real, it is returned unchanged.\n\n  Args:\n    input: A `Tensor`. Must have numeric type.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `float32` or `float64`.\n  \"\"\"\n  with ops.name_scope(name, \"Real\", [input]) as name:\n    input = ops.convert_to_tensor(input, name=\"input\")\n    if input.dtype.is_complex:\n      real_dtype = input.dtype.real_dtype\n      return gen_math_ops.real(input, Tout=real_dtype, name=name)\n    elif tf.debugging.is_numeric_tensor(input):\n      return input\n    else:\n      raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))\n\n\n@tf_export(\"math.imag\", v1=[\"math.imag\", \"imag\"])\n@dispatch.register_unary_elementwise_api\n@dispatch.add_dispatch_support\n@deprecation.deprecated_endpoints(\"imag\")\ndef imag(input, name=None):\n  r\"\"\"Returns the imaginary part of a complex (or real) tensor.\n\n  Given a tensor `input`, this operation returns a tensor of type `float` that\n  is the imaginary part of each element in `input` considered as a complex\n  number. If `input` is real, a tensor of all zeros is returned.\n\n  For example:\n\n  ```python\n  x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])\n  tf.math.imag(x)  # [4.75, 5.75]\n  ```\n\n  Args:\n    input: A `Tensor`. Must be one of the following types: `float`, `double`,\n      `complex64`, `complex128`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of type `float32` or `float64`.\n  \"\"\"\n  with ops.name_scope(name, \"Imag\", [input]) as name:\n    input = ops.convert_to_tensor(input, name=\"input\")\n    if input.dtype.is_complex:\n      return gen_math_ops.imag(input, Tout=input.dtype.real_dtype, name=name)\n    else:"
},
{
    "Id": 413,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8a0fee00855a0e806bd5c9cc1ad6c0175a985922",
    "Violation": "unnecessary",
    "Bug report": " [XLA] Don't use isnan on values which can't have NaN. While we are here, don't upcast to double just to check if something is NaN.",
    "Number of deleted lines": 5,
    "Deleted lines": "  if (!ShapeUtil::EqualStructure(shape(), other.shape())) {\n    return false;\n  }\n\n  return root_piece().ForEachSubpieceWithBool(\n      [&](const ShapeIndex& index, const Piece& piece) {\n        const Piece& other_piece = other.piece(index);\n        const Shape& subshape = piece.subshape();\n        const Shape& other_subshape = other_piece.subshape();\n        if (subshape.element_type() != other_subshape.element_type()) {\n          return false;\n        }\n        if (!piece.subshape().IsArray()) {\n          return true;\n        }\n        if (subshape.rank() != other_subshape.rank()) {\n          return false;\n        }\n\n        for (int64_t i = 0; i < subshape.rank(); ++i) {\n          if (piece.GetDynamicSize(i) != other_piece.GetDynamicSize(i)) {\n            return false;\n          }\n        }\n\n        if (!piece.EqualElements(other_piece)) {\n          return false;\n        }\n        return true;\n      });\n}\n\ntemplate <typename NativeT>\nstatic bool EqualIncludingNan(NativeT a, NativeT b) {\n  // msvc can't compile std::isnan(a) where `a` is uint8_t.  This is a bug\n  // according to https://en.cppreference.com/w/cpp/numeric/math/isnan, but it's\n  // easy to work around.\n  return a == b || (std::isnan(static_cast<double>(a)) &&\n                    std::isnan(static_cast<double>(b)));\n}\n\ntemplate <typename T>\nstatic bool EqualIncludingNan(std::complex<T> a, std::complex<T> b) {\n  return EqualIncludingNan(a.real(), b.real()) &&\n         EqualIncludingNan(a.imag(), b.imag());\n}\n\ntemplate <typename NativeT>\nstatic bool AllElementsEqualValue(absl::Span<const NativeT> data,\n                                  NativeT value) {\n  for (int64_t i = 0; i < data.size(); ++i) {\n    if (!EqualIncludingNan(data[i], value)) {\n      return false;\n    }\n  }\n  return true;\n}\n\nbool Literal::Piece::IsAll(const Literal& scalar) const {\n  CHECK(ShapeUtil::IsScalar(scalar.shape())) << scalar.shape().ToString();\n  if (!subshape().IsArray()) {\n    return false;\n  }\n\n  CHECK(LayoutUtil::IsDenseArray(subshape()))\n      << __func__ << \" is only supported for dense arrays: \" << subshape();\n  CHECK_EQ(subshape().element_type(), scalar.shape().element_type());\n  return primitive_util::PrimitiveTypeSwitch<bool>(\n      [&](auto primitive_type_constant) -> bool {\n        if constexpr (primitive_util::IsArrayType(primitive_type_constant)) {\n          using NativeT = NativeTypeOf<primitive_type_constant>;\n          return AllElementsEqualValue(this->data<NativeT>(),\n                                       scalar.GetFirstElement<NativeT>());\n        }\n        return false;"
},
{
    "Id": 414,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/580140611a47413dcf6373deb1250c0ed605e873",
    "Violation": "missing",
    "Bug report": " [XLA] Do not check fail in proto copy from if the backend config proto and desired proto type do not match.",
    "Number of deleted lines": 2,
    "Deleted lines": "    return InvalidArgument(\"Unknown API version\");\n  }\n  return found->second;\n}\n\nstd::ostream& operator<<(std::ostream& os, HloInstruction::FusionKind kind) {\n  return os << ToString(kind);\n}\n\nbool HloPtrComparator::operator()(const HloInstruction* const& lhs,\n                                  const HloInstruction* const& rhs) const {\n  if (rhs == nullptr) {\n    // Nothing compares less than nullptr.\n    return false;\n  }\n  if (lhs == nullptr) {\n    return true;\n  }\n  auto lhs_module = lhs->GetModule();\n  auto rhs_module = rhs->GetModule();\n  CHECK((lhs_module == nullptr && rhs_module == nullptr) ||\n        (lhs_module != nullptr && rhs_module != nullptr));\n  if (lhs_module != nullptr &&\n      lhs_module->unique_id() != rhs_module->unique_id()) {\n    return lhs_module->unique_id() < rhs_module->unique_id();\n  }\n  return lhs->unique_id() < rhs->unique_id();\n}\n\nStatus HloInstruction::GetBackendConfigInternal(\n    tensorflow::protobuf::Message* proto) const {\n  proto->Clear();\n\n  if (auto* proto_ptr = backend_config_.GetProtoPtr()) {\n    proto->CopyFrom(*proto_ptr);\n    return Status::OK();\n  }\n\n  auto& raw_string = raw_backend_config_string();\n  // Empty string does not parse as valid JSON, but it's a valid backend config,\n  // corresponding to the empty proto.\n  if (raw_string.empty()) {\n    return Status::OK();\n  }\n  TF_RETURN_IF_ERROR(tensorflow::HumanReadableJsonToProto(raw_string, proto));\n  backend_config_.SetProto(*proto);\n  return Status::OK();\n}\n\nconst std::string& HloInstruction::BackendConfigRep::GetRawString() const {\n  if (proto_ && raw_string_.empty()) {\n    raw_string_ = BackendConfigToRawString(*proto_).ValueOrDie();\n  }\n  return raw_string_;\n}\n\nHloInstruction::BackendConfigRep HloInstruction::BackendConfigRep::Clone()\n    const {\n  // Prefer cloning protobuf, raw_string_ will be lazily generated if accessed.\n  BackendConfigRep cloned;\n  if (auto* proto = GetProtoPtr()) {\n    cloned.SetProto(*proto);\n  } else {\n    cloned.raw_string_ = raw_string_;\n  }\n  return cloned;\n}\n\nHloInstruction::BackendConfigRep& HloInstruction::BackendConfigRep::operator=(\n    std::string raw_string) {\n  raw_string_ = std::move(raw_string);\n  proto_.reset();"
},
{
    "Id": 415,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d3d1cd3ad2becac5c31387f7fc483af65c7c8c84",
    "Violation": "unnecessary",
    "Bug report": " Fixes the crashes caused by the refcount checks for non-copyable types. For async value refs, refcounts don't equal to number of waiters as waiters can make copies of the async value refs.",
    "Number of deleted lines": 1,
    "Deleted lines": "  bool IsKnownReady() {\n    CHECK(IsValid());\n    return promise_ref_.IsAvailable();\n  }\n\n  // Blocks the calling thread until the promise is ready, then returns the\n  // final value.\n  T Await() {\n    CHECK(IsValid());\n    if (!promise_ref_.IsAvailable()) {\n      const auto keys = on_block_start_();\n      BlockUntilReady(promise_ref_.GetAsyncValue());\n      on_block_end_(keys);\n    }\n    DCHECK(promise_ref_.IsConcrete());\n    return *promise_ref_;\n  }\n\n  // Registers callback to be called once the promise is ready, with the final\n  // value.\n  //\n  // callback may be called on an internal system thread or the calling thread.\n  // The client should avoid any potentially re-entrant API calls within the\n  // callback, for example by using the callback to enqueue work on a\n  // client-owned threadpool.\n  void OnReady(absl::AnyInvocable<void(T) &&> callback) {\n    CHECK(IsValid());\n    promise_ref_.AndThen([promise = promise_ref_.AsPtr(),\n                          callback = std::move(callback)]() mutable {\n      DCHECK(promise.IsConcrete());\n      if constexpr (std::is_copy_constructible_v<T>) {\n        std::move(callback)(*promise);\n        return;\n      }\n      DCHECK_EQ(promise.value()->NumRef(), 1);\n      std::move(callback)(std::move(*promise));\n    });\n  }\n\n  // Indicates that event will not complete until after this becomes ready.\n  //\n  // May safely be called with event==nullptr in which case AssertHappensBefore\n  // has no effect.\n  void AssertHappensBefore(ScopedAsyncTrackingEvent* event) {\n    CHECK(IsValid());\n    if (event) {\n      event->AddDependency(promise_ref_.CopyRCRef());\n    }\n  }\n\n private:\n  // Wrapped object to wait on.\n  tfrt::AsyncValueRef<T> promise_ref_;\n  // Function that is called before a thread starts blocking on the promise.\n  PjRtFutureHelpers::OnBlockStartFn on_block_start_;\n  // Function that is called after a thread finishes blocking on the promise.\n  PjRtFutureHelpers::OnBlockEndFn on_block_end_;\n};\n\n}  // namespace xla\n\n#endif  // TENSORFLOW_COMPILER_XLA_PJRT_PJRT_FUTURE_H_\n"
},
{
    "Id": 416,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/02907e867c74651a9eb74971f56559d5db2efa1c",
    "Violation": "missing",
    "Bug report": " Use Nano seconds in Timestamp check as Pico seconds can lead to overflow.",
    "Number of deleted lines": 0,
    "Deleted lines": " public:\n  // REQUIRED: plane, line and event cannot be nullptr.\n  XEventVisitor(const XPlaneVisitor* plane, const XLine* line,\n                const XEvent* event);\n\n  int64_t Id() const { return event_->metadata_id(); }\n\n  absl::string_view Name() const { return metadata_->name(); }\n\n  absl::optional<int64_t> Type() const { return type_; }\n\n  bool HasDisplayName() const { return !metadata_->display_name().empty(); }\n\n  absl::string_view DisplayName() const { return metadata_->display_name(); }\n\n  double OffsetNs() const { return PicoToNano(event_->offset_ps()); }\n\n  int64_t OffsetPs() const { return event_->offset_ps(); }\n\n  int64_t LineTimestampNs() const { return line_->timestamp_ns(); }\n\n  int64_t TimestampNs() const { return line_->timestamp_ns() + OffsetNs(); }\n\n  int64_t TimestampPs() const {\n    return NanoToPico(line_->timestamp_ns()) + event_->offset_ps();\n  }\n\n  double DurationNs() const { return PicoToNano(event_->duration_ps()); }\n\n  int64_t DurationPs() const { return event_->duration_ps(); }\n\n  int64_t EndOffsetPs() const {\n    return event_->offset_ps() + event_->duration_ps();\n  }\n  int64_t EndTimestampPs() const { return TimestampPs() + DurationPs(); }\n\n  int64_t NumOccurrences() const { return event_->num_occurrences(); }\n\n  bool operator<(const XEventVisitor& other) const {\n    return GetTimespan() < other.GetTimespan();\n  }\n\n  const XEventMetadata* metadata() const { return metadata_; }\n\n  XEventMetadataVisitor Metadata() const {\n    return XEventMetadataVisitor(plane_, metadata_);\n  }\n\n  Timespan GetTimespan() const { return Timespan(TimestampPs(), DurationPs()); }\n\n private:\n  const XPlaneVisitor* plane_;\n  const XLine* line_;\n  const XEvent* event_;\n  const XEventMetadata* metadata_;\n  absl::optional<int64_t> type_;\n};\n\nclass XLineVisitor {\n public:\n  // REQUIRED: plane and line cannot be nullptr.\n  XLineVisitor(const XPlaneVisitor* plane, const XLine* line)\n      : plane_(plane), line_(line) {}\n\n  int64_t Id() const { return line_->id(); }\n\n  int64_t DisplayId() const {\n    return line_->display_id() ? line_->display_id() : line_->id();\n  }\n"
}]