{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/603097be18824a33069addec7b8f14ba5c3bc67a",
    "Commit message": "OneDNN MaxPooling: reduce memory use for inference path (#52728)\n\nSummary:\nFor OneDNN MaxPooling training, it will save indices as a workspace for backward, but for inference, indices are not necessary, this PR will make check to avoid saving indices to reduce memory use for inference path.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52728\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27062435\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 9e70268a8ba491a7914b980079c0945d753cd4f3",
    "Deleted lines": 1,
    "Added lines": 11,
    "Changed lines": 12,
    "Deleted code": "      ideep::prop_kind::forward);",
    "Added code": "#include <ATen/core/grad_mode.h>\n  auto aprop_kind = ideep::prop_kind::forward;\n  // for max_pool, prop_kind::forward will save indices as workspace for backward use,\n  // for inference, don't need the indices, set aprop_kind to prop_kind::forward_inference\n  // can reduce the memory use.\n  if (ideep::algorithm::pooling_max == algo\n      && !(input.requires_grad() && at::GradMode::is_enabled())) {\n    aprop_kind = ideep::prop_kind::forward_inference;\n  }\n\n      aprop_kind);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0a7eef9bcf6d1f8b5531102342ffc21f24beb58d",
    "Commit message": "[BE] Remove stale CUDA version check from cpp_extension.py (#113447)\n\nAs at least CUDA-11.x is needed to build PyTorch on latest trunk.\nBut still skip `--generate-dependencies-with-compile` if running on ROCm\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113447\nApproved by: https://github.com/Skylion007, https://github.com/atalman, https://github.com/PaliC, https://github.com/huydhn",
    "Deleted lines": 5,
    "Added lines": 2,
    "Changed lines": 7,
    "Deleted code": "        # --generate-dependencies-with-compile was added in CUDA 10.2.\n        # Compilation will work on earlier CUDA versions but header file\n        # dependencies are not correctly computed.\n        required_cuda_version = '11.0'\n        if torch.version.cuda is not None and TorchVersion(torch.version.cuda) >= required_cuda_version:",
    "Added code": "        # --generate-dependencies-with-compile is not supported by ROCm\n        if torch.version.cuda is not None:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ae2c219de2fd032036aa1d2a04101f1c23fd5bbe",
    "Commit message": "Revert \"[BE] Remove stale CUDA version check from cpp_extension.py (#113447)\"\n\nThis reverts commit 7ccca60927cdccde63d6a1d40480950f24e9877a.\n\nReverted https://github.com/pytorch/pytorch/pull/113447 on behalf of https://github.com/malfet due to Broke ROCM ([comment](https://github.com/pytorch/pytorch/pull/113447#issuecomment-1806407892))",
    "Deleted lines": 6,
    "Added lines": 11,
    "Changed lines": 17,
    "Deleted code": "        cuda_compile_rule.append('  depfile = $out.d')\n        cuda_compile_rule.append('  deps = gcc')\n        # Note: non-system deps with nvcc are only supported\n        # on Linux so use --generate-dependencies-with-compile\n        # to make this work on Windows too.\n        nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'",
    "Added code": "        # --generate-dependencies-with-compile was added in CUDA 10.2.\n        # Compilation will work on earlier CUDA versions but header file\n        # dependencies are not correctly computed.\n        required_cuda_version = '11.0'\n        if torch.version.cuda is not None and TorchVersion(torch.version.cuda) >= required_cuda_version:\n            cuda_compile_rule.append('  depfile = $out.d')\n            cuda_compile_rule.append('  deps = gcc')\n            # Note: non-system deps with nvcc are only supported\n            # on Linux so use --generate-dependencies-with-compile\n            # to make this work on Windows too.\n            nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7ccca60927cdccde63d6a1d40480950f24e9877a",
    "Commit message": "[BE] Remove stale CUDA version check from cpp_extension.py (#113447)\n\nAs at least CUDA-11.x is needed to build PyTorch on latest trunk\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113447\nApproved by: https://github.com/Skylion007, https://github.com/atalman, https://github.com/PaliC, https://github.com/huydhn",
    "Deleted lines": 11,
    "Added lines": 6,
    "Changed lines": 17,
    "Deleted code": "        # --generate-dependencies-with-compile was added in CUDA 10.2.\n        # Compilation will work on earlier CUDA versions but header file\n        # dependencies are not correctly computed.\n        required_cuda_version = '11.0'\n        if torch.version.cuda is not None and TorchVersion(torch.version.cuda) >= required_cuda_version:\n            cuda_compile_rule.append('  depfile = $out.d')\n            cuda_compile_rule.append('  deps = gcc')\n            # Note: non-system deps with nvcc are only supported\n            # on Linux so use --generate-dependencies-with-compile\n            # to make this work on Windows too.\n            nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'",
    "Added code": "        cuda_compile_rule.append('  depfile = $out.d')\n        cuda_compile_rule.append('  deps = gcc')\n        # Note: non-system deps with nvcc are only supported\n        # on Linux so use --generate-dependencies-with-compile\n        # to make this work on Windows too.\n        nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/93cea394dee1315c7a85ead7bb7af21363157c4f",
    "Commit message": "CMake: Loosen CUDA consistency check (#113174)\n\nCloses #108931, closes #108932, see also conda-forge/pytorch-cpu-feedstock#203\n\nCurrently we compare `CUDA_INCLUDE_DIRS` and expect exact equality\nwith `CUDAToolkit_INCLUDE_DIR` however this fails in the presense of\nsymbolic links or for split installs where there are multiple include paths.\nGiven that, it makes sense to loosen the requirement to just version\nequality under the assumption that two installs of the same version\nshould still be compatible.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113174\nApproved by: https://github.com/malfet",
    "Deleted lines": 4,
    "Added lines": 3,
    "Changed lines": 7,
    "Deleted code": "if(NOT CMAKE_CUDA_COMPILER_VERSION STREQUAL CUDAToolkit_VERSION OR\n    NOT CUDA_INCLUDE_DIRS STREQUAL CUDAToolkit_INCLUDE_DIR)\n  message(FATAL_ERROR \"Found two conflicting CUDA installs:\\n\"\n                      \"V${CUDAToolkit_VERSION} in '${CUDAToolkit_INCLUDE_DIR}'\")",
    "Added code": "if(NOT CMAKE_CUDA_COMPILER_VERSION VERSION_EQUAL CUDAToolkit_VERSION)\n  message(FATAL_ERROR \"Found two conflicting CUDA versions:\\n\"\n                      \"V${CUDAToolkit_VERSION} in '${CUDAToolkit_INCLUDE_DIRS}'\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6b4c686b9a33a1503a4a4133f9067dd31e0822f7",
    "Commit message": "[aotindutor] Forward fix a performance regression (#110800)\n\nSummary: Forward fix a performance regression caused by https://github.com/pytorch/pytorch/pull/110510. When a model is run once, all those kernel pointers are initialized and removing the if-nullptr check will cause those loadKernel be unnecessarily executed again when we rerun the foward function. Another way to do this is to codegen loadKernel in the initializer, which I may do in a later PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110800\nApproved by: https://github.com/jansel",
    "Deleted lines": 2,
    "Added lines": 6,
    "Changed lines": 8,
    "Deleted code": "                f\"\"\"kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n                f\"\"\"{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"",
    "Added code": "            self.writeline(f\"if (kernels.{name} == nullptr) {{\")\n                f\"\"\"    kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n            self.writeline(\"}\")\n            self.writeline(f\"if ({name} == nullptr) {{\")\n                f\"\"\"    {name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n            self.writeline(\"}\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/45296f87ec865a7a500a6fd98353035c040d0cb7",
    "Commit message": "Fix for verify_dynamo on ROCm (#97013)\n\nPrior to this change ROCm was not exiting check_cuda, causing an exception at packaging.version.parse(torch.version.cuda), this change exits check_cuda if torch.version.cuda is None\n\n```\npython verify_dynamo.py\n\nPython version: 3.9.16\n`torch` version: 2.1.0a0+git2b2f10c\nCUDA version: None\nROCM version: 5.4\n\nAll required checks passed\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97013\nApproved by: https://github.com/jithunnair-amd, https://github.com/malfet, https://github.com/kit1980",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if not torch.cuda.is_available():",
    "Added code": "    if not torch.cuda.is_available() or torch.version.hip is not None:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bede7d999523d02e636a8981c0dff233b67f1a62",
    "Commit message": "Fixed check for the buffer overflow in assert (#36476)\n\nSummary:\nThis code looks like a mistake\n```C++\nAT_ASSERT(size_t(kind) < sizeof(names) / sizeof(AttributeKind));\n```\nIt does not check if `kind` variable fits in array of pointer called `names`\n\nEven if we write something like this: that assert won't fail\n```C++\nAttributeKind kind = AttributeKind::ival;\n*((unsigned int*)&kind2) += 1;\n```\nSo I fixed it\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36476\n\nDifferential Revision: D21018748\n\nPulled By: colesbury\n\nfbshipit-source-id: f4d3b8faf64cf07232d595075f831805084f5d00",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  AT_ASSERT(size_t(kind) < sizeof(names) / sizeof(AttributeKind));",
    "Added code": "  AT_ASSERT(size_t(kind) < sizeof(names) / sizeof(*names));"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f6639359357452de8bfc691430396ded98ea399c",
    "Commit message": "[MPS] Fix boundary checks in generateKernelOffsets (#116915)\n\n`TORCH_CHECK(i<UINT32_MAX)` is always false, it should be `TORCH_CHECK(iterShape[i] < UINT32_MAX)`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116915\nApproved by: https://github.com/Skylion007, https://github.com/kulinseth\nghstack dependencies: #116903, #116904",
    "Deleted lines": 3,
    "Added lines": 3,
    "Changed lines": 6,
    "Deleted code": "    TORCH_CHECK(i <= UINT32_MAX);\n    iterShapeData[i] = (uint32_t)(iterShape[i]);\n      strides[i][offset] = iter.strides(offset)[i];",
    "Added code": "  TORCH_CHECK(iter.can_use_32bit_indexing(), \"Can't be indexed using 32-bit iterator\");\n    iterShapeData[i] = static_cast<uint32_t>(iterShape[i]);\n      strides[i][offset] = static_cast<uint32_t>(iter.strides(offset)[i]);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cf732053e4f6b93b0a93006613552cd97f415b80",
    "Commit message": "nn.EmbeddingBag bound check (#96022)\n\nSummary: Today if we're accessing out of bound embedding rows, it'll either go through or throw IMA. This is not ideal - adding bound checks. This will probably slow things down - need to benchmark it.\n\nTest Plan:\nTODO: add some tests\n\nTried a simple example and it's showing this:\n```\naten/src/ATen/native/cuda/EmbeddingBag.cu:143: EmbeddingBag_updateOutputKernel_sum_mean: block: [0,0,0], thread: [0,1,0] Assertion `input[emb] < numRows` failed.\n```\n\nDifferential Revision: D43810777\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/96022\nApproved by: https://github.com/cpuhrsch, https://github.com/ngimel",
    "Deleted lines": 4,
    "Added lines": 6,
    "Changed lines": 10,
    "Deleted code": "    index_t padding_idx) {\n    index_t padding_idx) {\n            padding_idx);\n            padding_idx);",
    "Added code": "    index_t padding_idx, int64_t numRows) {\n        CUDA_KERNEL_ASSERT(input[emb] < numRows);\n    index_t padding_idx, int64_t numRows) {\n        CUDA_KERNEL_ASSERT(input[emb] < numRows);\n            padding_idx, weight.size(0));\n            padding_idx, weight.size(0));"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/175ccfc4c8443bcc65c87d9c942272d3ebf16b0b",
    "Commit message": "Verify flatbuffer module fields are initialized (#109794)\n\nFixes #109793\n\nAdd validation on flatbuffer module field to prevent segfault\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109794\nApproved by: https://github.com/malfet",
    "Deleted lines": 2,
    "Added lines": 4,
    "Changed lines": 6,
    "Deleted code": "  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")\n      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\")",
    "Added code": "      ivalues && module->object_types(),\n      \"Parsing flatbuffer module: Corrupted ivalues/object_types field\");\n  TORCH_CHECK(\n      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bde7b81f34925491fbcbb9e355697eb594e36923",
    "Commit message": "[S337714] Back out \"[PyTorch] Don't do extra numel() check in TensorImpl::data() (#98090)\" (#100893)\n\nSummary:\nOriginal commit changeset: 9875964c3b32\n\nOriginal Phabricator Diff: D44586464\n\nReviewed By: drdarshan\n\nDifferential Revision: D45664329\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/100893\nApproved by: https://github.com/xush6528",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if (data == nullptr) {",
    "Added code": "    if (is_empty()) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2e224d62b6afecc78d885d0a4e160354950f6424",
    "Commit message": "Add USE_CUDNN check to AT_CUDNN_ENABLED definition (#25037)\n\nSummary:\nWe have environment variable USE_CUDNN with self-explanatory name. However cpp code is compiled based on cpp macro definition AT_CUDNN_ENABLED, which is defined as:\n\n```\n  IF (NOT AT_CUDA_ENABLED OR NOT CUDNN_FOUND)\n    MESSAGE(STATUS \"CuDNN not found. Compiling without CuDNN support\")\n    set(AT_CUDNN_ENABLED 0)\n  ELSE()\n    include_directories(SYSTEM ${CUDNN_INCLUDE_DIRS})\n    set(AT_CUDNN_ENABLED 1)\n  ENDIF()\n```\n\nSo, even if USE_CUDNN is set to 0, cpp is compiled with cuDNN if cmake finds cuDNN in the system. I actually tested it and was very surprised when I was debugging cuDNN code which I built with USE_CUDNN=0. I believe that cmake code above should look like this:\n\n`IF (NOT AT_CUDA_ENABLED OR NOT CUDNN_FOUND OR NOT USE_CUDNN) ...`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25037\n\nDifferential Revision: D17048683\n\nPulled By: pbelevich\n\nfbshipit-source-id: 48afa19eaae0bba2ffd49c1f68db0b4efd5cf85e",
    "Deleted lines": 2,
    "Added lines": 5,
    "Changed lines": 7,
    "Deleted code": "  IF (NOT AT_CUDA_ENABLED OR NOT CUDNN_FOUND)\n    MESSAGE(STATUS \"CuDNN not found. Compiling without CuDNN support\")",
    "Added code": "  IF (NOT USE_CUDNN)\n    MESSAGE(STATUS \"USE_CUDNN is set to 0. Compiling without cuDNN support\")\n    set(AT_CUDNN_ENABLED 0)\n  ELSEIF (NOT CUDNN_FOUND)\n    MESSAGE(WARNING \"CuDNN not found. Compiling without CuDNN support\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/30e1c74dc19ae2b622b46ebcdb7972c42775ac80",
    "Commit message": "Update cuda amp to also check xla device (#63413)\n\nSummary:\nFixes https://github.com/pytorch/xla/issues/3086. Pytorch/XLA:GPU also use cuda amp. I verified the pt/xla `test_autocast` with this fix and all test passed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63413\n\nReviewed By: ngimel\n\nDifferential Revision: D30380785\n\nPulled By: bdhirsh\n\nfbshipit-source-id: fd1a1de7d224c616fc3fa90b80a688a21f6b1ecc",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "        if not torch.cuda.is_available() and self.device == 'cuda':",
    "Added code": "        if torch.cuda.amp.common.amp_definitely_not_available() and self.device == 'cuda':"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5a63c452e638dad8e077887ad8d2c94ff0e23917",
    "Commit message": "Disable cuDNN persistent RNN on sm_86 devices (#49534)\n\nSummary:\nExcludes sm_86 GPU devices from using cuDNN persistent RNN.\n\nThis is because there are some hard-to-detect edge cases that will throw exceptions with cudnn 8.0.5 on Nvidia A40 GPU.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49534\n\nReviewed By: mruberry\n\nDifferential Revision: D25632378\n\nPulled By: mrshenli\n\nfbshipit-source-id: cbe78236d85d4d0c2e4ca63a3fc2c4e2de662d9e",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "      if (prop->minor == 6) {\n        // Excludes sm_86 GPU devices from using persistent rnn.\n        // This is because there are some edge cases that will throw exceptions with cudnn 8.0.5 on Nvidia A40 GPU.\n        return false;\n      }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/611080a118fff166c85f3200d860f3b059abac6f",
    "Commit message": "[hot fix] cuda 11.0.x doesn't support sm86. (#47408)\n\nSummary:\nBump condition check from >11.0 to >11.0.3\n\nCMAKE 3.5 doesn't support VERSION_GREATER_EQUAL see [here](https://github.com/Dav1dde/glad/issues/134), so we might need to bump this again iv 11.0.4+ releases.\n\nshould fix https://github.com/pytorch/pytorch/issues/47352\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47408\n\nReviewed By: glaringlee\n\nDifferential Revision: D24759949\n\nPulled By: walterddr\n\nfbshipit-source-id: de384c7b150babaf799cce53ed198e5e931899da",
    "Deleted lines": 2,
    "Added lines": 3,
    "Changed lines": 5,
    "Deleted code": "    set(CUDA_LIMIT_GPU_ARCHITECTURE \"8.6\")\nif(CUDA_VERSION VERSION_GREATER \"11.0\")",
    "Added code": "    set(CUDA_LIMIT_GPU_ARCHITECTURE \"8.0\")\nif(NOT CUDA_VERSION VERSION_LESS \"11.1\")\n  set(CUDA_LIMIT_GPU_ARCHITECUTRE \"8.6\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/563bbeb8905f4cea0bc5353dc12518c61113128e",
    "Commit message": "fix undef CUDA_VERSION warning (#37866)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37866\n\nmake sure not to check `CUDA_VERSION` if it is not defined\n\nTest Plan: CI gree\n\nReviewed By: anjali411\n\nDifferential Revision: D21408844\n\nfbshipit-source-id: 5a9afe372b3f1fbaf08a7c43fa3e0e654a569d5f",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "#if CUDA_VERSION < 10000",
    "Added code": "#if defined(CUDA_VERSION) && (CUDA_VERSION < 10000)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5b51ca6808191e9f3dcea1d43fa731488cc688bb",
    "Commit message": "Update CUDA compiler matrix (#86360)\n\nSwitch GCC/Clang max versions to be exclusive as the `include/crt/host_config.h` checks the major version only for the upper bound. This allows to be less restrictive and match the checks in the aforementioned header.\nAlso update the versions using that header in the CUDA SDKs.\n\nFollow up to #82860\n\nI noticed this as PyTorch 1.12.1 with CUDA 11.3.1 and GCC 10.3 was failing in the `test_cpp_extensions*` tests.\n\nExample for CUDA 11.3.1 from the SDK header:\n\n```\n#if __GNUC__ > 11\n// Error out\n...\n#if (__clang_major__ >= 12) || (__clang_major__ < 3) || ((__clang_major__ == 3) &&  (__clang_minor__ < 3))\n// Error out\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86360\nApproved by: https://github.com/ezyang",
    "Deleted lines": 28,
    "Added lines": 33,
    "Changed lines": 61,
    "Deleted code": "from typing import List, Optional, Union, Tuple\nCUDA_GCC_VERSIONS = {\n    '10.2': (MINIMUM_GCC_VERSION, (8, 0, 0)),\n    '11.1': (MINIMUM_GCC_VERSION, (10, 0, 0)),\n    '11.2': (MINIMUM_GCC_VERSION, (10, 2, 1)),\n    '11.3': (MINIMUM_GCC_VERSION, (10, 2, 1)),\n    '11.4': ((6, 0, 0), (11, 5, 0)),\n    '11.5': ((6, 0, 0), (11, 5, 0)),\n    '11.6': ((6, 0, 0), (11, 5, 0)),\n    '11.7': ((6, 0, 0), (11, 5, 0)),\nCUDA_CLANG_VERSIONS = {\n    '10.2': ((3, 3, 0), (8, 0, 0)),\n    '11.1': ((6, 0, 0), (9, 0, 0)),\n    '11.2': ((6, 0, 0), (9, 0, 0)),\n    '11.3': ((6, 0, 0), (11, 0, 0)),\n    '11.4': ((6, 0, 0), (11, 0, 0)),\n    '11.5': ((6, 0, 0), (12, 0, 0)),\n    '11.6': ((6, 0, 0), (12, 0, 0)),\n    '11.7': ((6, 0, 0), (13, 0, 0)),\n    cuda_compiler_bounds = CUDA_CLANG_VERSIONS if compiler_name.startswith('clang') else CUDA_GCC_VERSIONS\n        min_compiler_version, max_compiler_version = cuda_compiler_bounds[cuda_str_version]\n        # Special case for 11.4.0, which has lower compiler bounds that 11.4.1\n            max_compiler_version = (10, 0, 0)\n        max_compiler_version_str = '.'.join(map(str, max_compiler_version))\n        version_bound_str = f'>={min_compiler_version_str}'\n        version_bound_str = f'{version_bound_str}, <={max_compiler_version_str}'\n        if compiler_version > TorchVersion(max_compiler_version_str):\n                f'than the maximum required version by CUDA {cuda_str_version} ({max_compiler_version_str}). '",
    "Added code": "from typing import Dict, List, Optional, Union, Tuple\nVersionRange = Tuple[Tuple[int, ...], Tuple[int, ...]]\nVersionMap = Dict[str, VersionRange]\n# Or from include/crt/host_config.h in the CUDA SDK\n# The second value is the exclusive(!) upper bound, i.e. min <= version < max\nCUDA_GCC_VERSIONS: VersionMap = {\n    '10.2': (MINIMUM_GCC_VERSION, (9, 0)),\n    '11.0': (MINIMUM_GCC_VERSION, (10, 0)),\n    '11.1': (MINIMUM_GCC_VERSION, (11, 0)),\n    '11.2': (MINIMUM_GCC_VERSION, (11, 0)),\n    '11.3': (MINIMUM_GCC_VERSION, (11, 0)),\n    '11.4': ((6, 0, 0), (12, 0)),\n    '11.5': ((6, 0, 0), (12, 0)),\n    '11.6': ((6, 0, 0), (12, 0)),\n    '11.7': ((6, 0, 0), (12, 0)),\nMINIMUM_CLANG_VERSION = (3, 3, 0)\nCUDA_CLANG_VERSIONS: VersionMap = {\n    '10.2': (MINIMUM_CLANG_VERSION, (9, 0)),\n    '11.1': (MINIMUM_CLANG_VERSION, (11, 0)),\n    '11.2': (MINIMUM_CLANG_VERSION, (12, 0)),\n    '11.3': (MINIMUM_CLANG_VERSION, (12, 0)),\n    '11.4': (MINIMUM_CLANG_VERSION, (13, 0)),\n    '11.5': (MINIMUM_CLANG_VERSION, (13, 0)),\n    '11.6': (MINIMUM_CLANG_VERSION, (14, 0)),\n    '11.7': (MINIMUM_CLANG_VERSION, (14, 0)),\n    cuda_compiler_bounds: VersionMap = CUDA_CLANG_VERSIONS if compiler_name.startswith('clang') else CUDA_GCC_VERSIONS\n        min_compiler_version, max_excl_compiler_version = cuda_compiler_bounds[cuda_str_version]\n        # Special case for 11.4.0, which has lower compiler bounds than 11.4.1\n            max_excl_compiler_version = (11, 0)\n        max_excl_compiler_version_str = '.'.join(map(str, max_excl_compiler_version))\n        version_bound_str = f'>={min_compiler_version_str}, <{max_excl_compiler_version_str}'\n        if compiler_version >= TorchVersion(max_excl_compiler_version_str):\n                f'than the maximum required version by CUDA {cuda_str_version}. '"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cafd0f33042f5344a27ccde33b352eab676a0bdd",
    "Commit message": "[jit][edge] Fix array index checking in mobile interpreter. (#73241)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73241\n\nStop using non-portable out-of-range indexing in mobile interpreter, also change code types indexing to use vector.at() to catch out-of-range bugs earlier.\n\nTest Plan: buck test mode/dbg mode/no-gpu -c fbcode.platform=platform010 //caffe2/test/cpp/jit:jit -- BackendTest.TestCompiler\n\nReviewed By: dhruvbird, r-barnes\n\nDifferential Revision: D34370237\n\nfbshipit-source-id: 1827f75ed00ecc10bbcece48329b0ac87189b079\n(cherry picked from commit ab943ef414c8d109bd766f672def63be28af2571)",
    "Deleted lines": 6,
    "Added lines": 5,
    "Changed lines": 11,
    "Deleted code": "          listConstruct(stack, *code.types_[inst.X], inst.N);\n          dictConstruct(stack, *code.types_[inst.X], inst.N);\n          namedTupleConstruct(stack, code.types_[inst.X], inst.N);\n          auto type = code.types_[inst.X]->expect<c10::ClassType>();\n          at::ArrayRef<TypePtr> types(\n              &(code.types_[inst.X]), &(code.types_[inst.X + inst.N]));",
    "Added code": "          listConstruct(stack, *code.types_.at(inst.X), inst.N);\n          dictConstruct(stack, *code.types_.at(inst.X), inst.N);\n          namedTupleConstruct(stack, code.types_.at(inst.X), inst.N);\n          auto type = code.types_.at(inst.X)->expect<c10::ClassType>();\n          at::ArrayRef<TypePtr> types(&code.types_.at(inst.X), inst.N);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8198474eb763c8d526ede3418211479c2f4cbd30",
    "Commit message": "Fix scope name when parent scope is empty for torch.onnx.export (#112654)\n\nPrevious to this PR, we only checked TorchScript nodes for scope compatibility, skipping their parent's scope reference check.\nThis PR fixes adds a check not only for the node being traversed, but its parents as well\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112654\nApproved by: https://github.com/BowenBao",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  while (!parent->isRoot()) {",
    "Added code": "  while (isCompatibleScope(parent)) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/37dea0454dd310cfe443859f717862657df6b753",
    "Commit message": "[quant] add checking number of args when checking observer in same graph (#75460)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75460\n\nadd checking for number of args checking observer in same graph\n\nTest Plan:\npython3 test/test_quantization.py TestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D35479504\n\nfbshipit-source-id: d7dc38a27fdf8e0b236b6976d484b0701c61184c\n(cherry picked from commit 45542f796f5e6f6259f3ec647dbd2a9fa69ababc)",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if isinstance(node.args[0], Node):",
    "Added code": "    if len(node.args) > 0 and isinstance(node.args[0], Node):"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7fbdc86aecd6fddb3d309a1d655efbf51e840139",
    "Commit message": "[jit] Removed a local function to check for dominators and used the one added to Node class (#60909)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/60909\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D29441864\n\nPulled By: navahgar\n\nfbshipit-source-id: 362bd462fa70256dd1f8b05756a76da0cb3d4b76",
    "Deleted lines": 14,
    "Added lines": 2,
    "Changed lines": 16,
    "Deleted code": "// TODO: Use the function `isDominatedBy` in Node class once\n// https://github.com/pytorch/pytorch/pull/56437 lands.\nbool isDominatedBy(Node* node, Node* dominator) {\n  while (node) {\n    if (node->owningBlock() == dominator->owningBlock()) {\n      return dominator->isBefore(node);\n    }\n    node = node->owningBlock()->owningNode();\n  }\n  return false;\n}\n\n        if (!isDominatedBy(node, prev)) {\n        if (!isDominatedBy(node, prev)) {",
    "Added code": "        if (!node->isDominatedBy(prev)) {\n        if (!node->isDominatedBy(prev)) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bdbd3ed312e0fc81e75302239ea78b3445fe95e7",
    "Commit message": "When nopython=True, Dynamo can't allow graph breaks. (#90970)\n\nI count the number of sub-graphs (for tiny-GPT2 in huggingface) by\n```\n    class GraphCaptureCompiler:\n        def __init__(self):\n            self.captured_graphs = []\n        def compile(self, gm, example_inputs):\n            self.captured_graphs.append(gm)\n            return gm\n    compiler = GraphCaptureCompiler()\n    torch._dynamo.optimize(compiler, nopython=True)(Wrapper(fn))(*args)\n```\n\nAlthough `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90970\nApproved by: https://github.com/ezyang, https://github.com/jansel, https://github.com/thiagocrepaldi",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "                if self.has_backedge():",
    "Added code": "                if self.has_backedge() and self.should_compile_partial_graph():"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7e9bf2ed860b8b60d252eead4cc457c3fe5f1667",
    "Commit message": "When nopython=True, Dynamo can't allow graph breaks. (#90970)\n\nI count the number of sub-graphs (for tiny-GPT2 in huggingface) by\n```\n    class GraphCaptureCompiler:\n        def __init__(self):\n            self.captured_graphs = []\n        def compile(self, gm, example_inputs):\n            self.captured_graphs.append(gm)\n            return gm\n    compiler = GraphCaptureCompiler()\n    torch._dynamo.optimize(compiler, nopython=True)(Wrapper(fn))(*args)\n```\n\nAlthough `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90970\nApproved by: https://github.com/ezyang, https://github.com/jansel",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "                if self.has_backedge():",
    "Added code": "                if self.has_backedge() and self.should_compile_partial_graph():"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9234f5026dbaf09a41b82bb6cf5f10ad4eeb03f2",
    "Commit message": "Make WorkNCCL use CUDAEvent::query() rather than re-implement it (#49343)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49343\n\nat::cuda::CUDAEvent is \"lazy\" and only creates an event when it's first recorded. Until then, at::cuda::CUDAEvent is empty. If we use at::cuda::CUDAEvent::query() this is taken into account (an empty event is always ready), but WorkNCCL extracts the raw cudaEvent_t value from at::cuda::CUDAEvent and calls cudaEventQuery manually and doesn't check this. This could cause a failure.\n\nIt's unclear if this is ever supposed to happen, but we're seeing that failure, and we want to sort it out in order to see if there's something \"deeper\" going on.\nghstack-source-id: 118532806\n\nTest Plan: Unit tests\n\nReviewed By: SciPioneer\n\nDifferential Revision: D25537844\n\nfbshipit-source-id: 506319f4742e1c0a02aa75ecc01112ea3be42d8f",
    "Deleted lines": 5,
    "Added lines": 1,
    "Changed lines": 6,
    "Deleted code": "    auto ret = cudaEventQuery((*cudaEvents_)[i]);\n    if (ret != cudaSuccess && ret != cudaErrorNotReady) {\n      AT_CUDA_CHECK(ret);\n    }\n    if (ret == cudaErrorNotReady) {",
    "Added code": "    if (!(*cudaEvents_)[i].query()) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/04db1b874ff4fe902af3dd7540159645309490a7",
    "Commit message": "prevent overriding shuffle settings in DataLoader for datapipes\n\nFixes https://github.com/pytorch/data/issues/295\n\nFollow-up to https://github.com/pytorch/pytorch/pull/75014#issuecomment-1091921305. We only need to update locations where we actually check `shuffle` for identity with a boolean value, i.e. `shuffle is False`. For bool-ish checks like `if shuffle:`, `None` behaves just like `False`.\n\n`IterDataPipe`'s are currently not mentioned in the docstring. Since this change only applies to them, I didn't update it. LMK, if I should do that.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75505\nApproved by: https://github.com/ejguan",
    "Deleted lines": 3,
    "Added lines": 6,
    "Changed lines": 9,
    "Deleted code": "                 shuffle: bool = False, sampler: Union[Sampler, Iterable, None] = None,\n                dataset = torch.utils.data.graph_settings.apply_shuffle_settings(dataset, shuffle=shuffle)\n            elif shuffle is not False:",
    "Added code": "                 shuffle: Optional[bool] = None, sampler: Union[Sampler, Iterable, None] = None,\n                if shuffle is not None:\n                    dataset = torch.utils.data.graph_settings.apply_shuffle_settings(dataset, shuffle=shuffle)\n            # We cannot check `shuffle is not None` here, since previously `shuffle=False` was the default.\n            elif shuffle not in {False, None}:\n            shuffle = bool(shuffle)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e9c1ccee2247a7746fde202067a7d47b72809968",
    "Commit message": "Bug fix: allow std 0 in the meta definition of `normal_` (#70085)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70085\n\nAll other `normal` variants allow 0.  Looks like a mistake made while\ncopying the check.  Even the `normal_` implementation disagrees:\n\n```\n>>> t = torch.rand(2, 3, device='meta')\n>>> t.normal_(mean=4, std=0)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: normal_ expects std > 0.0, but found std=0\n>>>\n>>>\n>>> t = torch.rand(2, 3)\n>>> t.normal_(mean=4, std=0)\ntensor([[4., 4., 4.],\n        [4., 4., 4.]])\n```\n\nFixes #69523.\n\nTest Plan: Imported from OSS\n\nReviewed By: davidberard98\n\nDifferential Revision: D34089967\n\nPulled By: bdhirsh\n\nfbshipit-source-id: c57963e55f06c9513c4f0839f8f7a21eca86b584\n(cherry picked from commit d6ffe43ddddd24daa5d9eb8befc852dd2108fc89)",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  TORCH_CHECK(std > 0.0, \"normal_ expects std > 0.0, but found std=\", std);  // TODO: dedupe",
    "Added code": "  TORCH_CHECK(std >= 0.0, \"normal_ expects std >= 0.0, but found std=\", std);  // TODO: dedupe"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c99277e177cf16736262251c7e92ea5e9ba2c5c2",
    "Commit message": "handle the case in acc_ops.sum when dim == 0, differentiating it from the case when dim is None (#64869)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64869\n\nhandle the case in acc_ops.sum when dim == 0, differentiating it from the case when dim is None\n\nReviewed By: 842974287\n\nDifferential Revision: D30872739\n\nfbshipit-source-id: 2755d3230804a16ef1c9289f804138c6dd7766b3",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if dim:",
    "Added code": "    if dim is not None:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d72db37c4a6513c0f67f6f69870c9c45bf4880e6",
    "Commit message": "Remove a redundant check from code. (#93025)\n\nIn file: combinatorics.py, the comparison of Collection length creates a logical short circuit.\n\n   if isinstance(self.sampler, Sized) and len(self.sampler) >= 0:\n\nHere, the right side of the comparison will always return true.\n\nI suggested that the Collection length check should be removed since this is redundant.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/93025\nApproved by: https://github.com/albanD",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "        if isinstance(self.sampler, Sized) and len(self.sampler) >= 0:",
    "Added code": "        if isinstance(self.sampler, Sized):"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/37008940999a41aedf3d7e77c86167c8e4852d7a",
    "Commit message": "Fix FSDP `summon_full_params(..., with_grads=True)` when grad precision is not `fp32` (#112746)\n\nFixes #112717\n\nI moved the `torch.empty` call after the conditional so that we don't need to check whether `flat_param.grad` is None\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112746\nApproved by: https://github.com/awgu",
    "Deleted lines": 4,
    "Added lines": 6,
    "Changed lines": 10,
    "Deleted code": "        padded_unsharded_grad = torch.empty(\n            flat_param._padded_unsharded_size,  # type: ignore[attr-defined]\n            device=self.device,\n        )",
    "Added code": "\n        padded_unsharded_grad = torch.empty(\n            flat_param._padded_unsharded_size,  # type: ignore[attr-defined]\n            device=self.device,\n            dtype=sharded_grad.dtype,\n        )"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c170d395de8ca441d3bedb20c9e45beb666f216c",
    "Commit message": "utils: Only check for xnnpack if torch installed (#74342)\n\nSummary:\nFixes a bug where collect_env.py was not able to be run without having\ntorch installed\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/74342\n\nReviewed By: malfet, janeyx99\n\nDifferential Revision: D34943464\n\nPulled By: seemethere\n\nfbshipit-source-id: dbaa0004b88cb643a9c6426c9ea7c5be3d3c9ef5\n(cherry picked from commit 4f39ebb823f88df0c3902db15deaffc6ba481cb3)",
    "Deleted lines": 2,
    "Added lines": 5,
    "Changed lines": 7,
    "Deleted code": "    import torch.backends.xnnpack\n    return str(torch.backends.xnnpack.enabled)  # type: ignore[attr-defined]",
    "Added code": "    if TORCH_AVAILABLE:\n        import torch.backends.xnnpack\n        return str(torch.backends.xnnpack.enabled)  # type: ignore[attr-defined]\n    else:\n        return \"N/A\""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/65faf1a7eb07129d8b1f017fac341e178620dabd",
    "Commit message": "[fx2trt] Add version check for ProfilingVerbosity bulider config (#70286)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70286\n\natt\n\nTest Plan:\npython test/fx2trt/test_quant_trt.py\n\nImported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D33274058\n\nfbshipit-source-id: c7657f9ba8b578d40d6fc1793b8b363898700eee",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "        builder_config.profiling_verbosity = profiling_verbosity if profiling_verbosity else trt.ProfilingVerbosity.LAYER_NAMES_ONLY",
    "Added code": "        if trt.__version__ >= \"8.2\":\n            builder_config.profiling_verbosity = profiling_verbosity \\\n                if profiling_verbosity else \\\n                trt.ProfilingVerbosity.LAYER_NAMES_ONLY"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6a1147d0596d49e7c2c8f4894484ffcd322f612d",
    "Commit message": "[package] fix orderedimporter dummy package check (#72533)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72533\n\nThe current check that we have for dummy packages is too expansive; it\nwill skip anything without a `__file__`, including extension modules in\nthe standard library.\n\nSo first check if a module was created by torch.package before skipping\nit, which should rule out anything accidentally getting skipped (as the\nonly time torch.package creates something without a `__file__` is the\ndummy case).\n\nTest Plan: Imported from OSS\n\nReviewed By: PaliC\n\nDifferential Revision: D34082792\n\nPulled By: suo\n\nfbshipit-source-id: 18b17eb0f693927697657b20843ec5cd8bcccb47\n(cherry picked from commit f571370078f7e12afa46ba4b2f6be2d96efce93b)",
    "Deleted lines": 2,
    "Added lines": 13,
    "Changed lines": 15,
    "Deleted code": "    def _check_if_fileless_package(self, module):\n                if self._check_if_fileless_package(module):",
    "Added code": "    def _is_torchpackage_dummy(self, module):\n        \"\"\"Returns true iff this module is an empty PackageNode in a torch.package.\n\n        If you intern `a.b` but never use `a` in your code, then `a` will be an\n        empty module with no source. This can break cases where we are trying to\n        re-package an object after adding a real dependency on `a`, since\n        OrderedImportere will resolve `a` to the dummy package and stop there.\n\n        See: https://github.com/pytorch/pytorch/pull/71520#issuecomment-1029603769\n        \"\"\"\n        if not getattr(module, \"__torch_package__\", False):\n            return False\n                if self._is_torchpackage_dummy(module):"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6c98d904c09b69f1e7748cf3d80e2193df5fff63",
    "Commit message": "handle the case of -0.0 on tanh quantization (#44406)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44406\n\nthis fix makes fakelowp identical to hw\n\n- mask out the floating point number with 0x7fff so we are always dealing\nwith positive numbers\n- dsp implementation is correct, ice-ref suffers from this same problem\n\nTest Plan: - tested with test_fusions.py, can't enable the test until the fix in ice-ref appears\n\nReviewed By: venkatacrc\n\nDifferential Revision: D23603878\n\nfbshipit-source-id: a72d93a4bc811f98d1b5e82ddb204be028addfeb",
    "Deleted lines": 5,
    "Added lines": 5,
    "Changed lines": 10,
    "Deleted code": "        float val = X_data[i];\n        short shortAbsInput = _cvtss_sh(abs(val), 0);\n        // Clamp the input in the range of\n        //  (short)tanhLUTMinOffset to (short)(tanhLUTMaxOffset - 1)\n        if (val < 0.0) {",
    "Added code": "        short val = _cvtss_sh(X_data[i], 0);\n        unsigned short max16BitPositive = 0x7FFF;\n        unsigned short input16Bit = (*(unsigned short*)& val);\n        short shortAbsInput = input16Bit & max16BitPositive; // mask out negative bit\n        if (input16Bit > max16BitPositive) {  // negative value"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5fdddbbfe8ef17b2c81ed34a48f3b963944aa4c3",
    "Commit message": "Fix checking of current mode in PyOperator dispatch (#92357)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92357\nApproved by: https://github.com/voznesenskym",
    "Deleted lines": 3,
    "Added lines": 3,
    "Changed lines": 6,
    "Deleted code": "            curr_mode = type(_get_current_dispatch_mode())\n                curr_mode in self.python_key_mode_table\n            return self.python_key_mode_table[curr_mode](*args, **kwargs)",
    "Added code": "            curr_mode = _get_current_dispatch_mode()\n                type(curr_mode) in self.python_key_mode_table\n            return self.python_key_mode_table[type(curr_mode)](*args, **kwargs)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c7c711bfb88fcb0ef573125a5a8655c49156055b",
    "Commit message": "Add optional tensor arguments to (#63967)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/63435\n\nAdds optional tensor arguments to check handling torch function checks. The only one I didn't do this for in the functional file was `multi_head_attention_forward` since that already took care of some optional tensor arguments but not others so it seemed like arguments were specifically chosen\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63967\n\nReviewed By: albanD\n\nDifferential Revision: D30640441\n\nPulled By: ezyang\n\nfbshipit-source-id: 5ef9554d2fb6c14779f8f45542ab435fb49e5d0f",
    "Deleted lines": 36,
    "Added lines": 36,
    "Changed lines": 72,
    "Deleted code": "    if has_torch_function_unary(input):\n            (input,),\n    if has_torch_function_unary(input):\n            (input,),\n    if has_torch_function_unary(input):\n            (input,),\n    if has_torch_function_unary(input):\n            (input,),\n    if has_torch_function_variadic(input, weight):\n        return handle_torch_function(linear, (input, weight), input, weight, bias=bias)\n    if has_torch_function_variadic(input1, input2, weight):\n            (input1, input2, weight),\n    if has_torch_function_variadic(input, weight):\n            (input, weight),\n    if has_torch_function_unary(input):\n            (input,),\n    if has_torch_function_unary(input):\n            (input,),\n    if has_torch_function_unary(input):\n            layer_norm, (input,), input, normalized_shape, weight=weight, bias=bias, eps=eps\n    if has_torch_function_unary(input):\n        return handle_torch_function(group_norm, (input,), input, num_groups, weight=weight, bias=bias, eps=eps)\n    if has_torch_function_variadic(input, target):\n            (input, target),\n    if has_torch_function_variadic(input, target):\n            (input, target),\n    if has_torch_function_variadic(input, target):\n            (input, target),\n    if has_torch_function_variadic(input, target):\n            (input, target),\n    if has_torch_function_variadic(input, target):\n            (input, target),\n    if has_torch_function_variadic(input, target):\n            (input, target),\n    if has_torch_function_unary(input):\n        return handle_torch_function(normalize, (input,), input, p=p, dim=dim, eps=eps, out=out)",
    "Added code": "    if has_torch_function_variadic(input, _random_samples):\n            (input, _random_samples),\n    if has_torch_function_variadic(input, _random_samples):\n            (input, _random_samples),\n    if has_torch_function_variadic(input, _random_samples):\n            (input, _random_samples),\n    if has_torch_function_variadic(input, _random_samples):\n            (input, _random_samples),\n    if has_torch_function_variadic(input, weight, bias):\n        return handle_torch_function(linear, (input, weight, bias), input, weight, bias=bias)\n    if has_torch_function_variadic(input1, input2, weight, bias):\n            (input1, input2, weight, bias),\n    if has_torch_function_variadic(input, weight, offsets, per_sample_weights):\n            (input, weight, offsets, per_sample_weights),\n    if has_torch_function_variadic(input, running_mean, running_var, weight, bias):\n            (input, running_mean, running_var, weight, bias),\n    if has_torch_function_variadic(input, running_mean, running_var, weight, bias):\n            (input, running_mean, running_var, weight, bias),\n    if has_torch_function_variadic(input, weight, bias):\n            layer_norm, (input, weight, bias), input, normalized_shape, weight=weight, bias=bias, eps=eps\n    if has_torch_function_variadic(input, weight, bias):\n        return handle_torch_function(group_norm, (input, weight, bias,), input, num_groups, weight=weight, bias=bias, eps=eps)\n    if has_torch_function_variadic(input, target, weight):\n            (input, target, weight),\n    if has_torch_function_variadic(input, target, weight):\n            (input, target, weight),\n    if has_torch_function_variadic(input, target, weight):\n            (input, target, weight),\n    if has_torch_function_variadic(input, target, weight, pos_weight):\n            (input, target, weight, pos_weight),\n    if has_torch_function_variadic(input, target, weight):\n            (input, target, weight),\n    if has_torch_function_variadic(input, target, weight):\n            (input, target, weight),\n    if has_torch_function_variadic(input, out):\n        return handle_torch_function(normalize, (input, out), input, p=p, dim=dim, eps=eps, out=out)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/db6e0c7c0eb56ac45af2b9c1e70d081b6bee83ee",
    "Commit message": "Replace a platform.system() check with sys.platform (#51766)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51766\n\nCheck if we are on Windows using `sys.platform` rather than\n`platform.system()`.  Even though `platform.system()` is more modern, it\nhas a few downsides: this performs a runtime check of the platform type,\nwhich has non-zero overhead.  On Linux it actually executes the separate\n`/bin/uname` process.  On the other hand `sys.platform` is determined\nwhen the Python interpreter is compiled, so this is a simple hard-coded\nstring.\n\nBecause it is a runtime check, `platform.system()` checks also cannot be\nanalyzed by static type checkers like Pyre and Mypy.  These type\ncheckers do understand `sys.platform` checks, and can correctly avoid\ncomplaining about code paths that use platform-specific modules and\nfunctions.  e.g., they can avoid complaining about `ctypes.WinDLL` not\nexisting on Linux if its use is guarded by a `sys.platform` check.\nghstack-source-id: 121107705\n\nTest Plan: Ran tests on Linux, and will check CI test results.\n\nReviewed By: mraway\n\nDifferential Revision: D26271724\n\nPulled By: simpkins\n\nfbshipit-source-id: b86e427e4ceec0324464ba4bc88b95d5813172d0",
    "Deleted lines": 2,
    "Added lines": 1,
    "Changed lines": 3,
    "Deleted code": "import platform\nif platform.system() == 'Windows':",
    "Added code": "if sys.platform == \"win32\":"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6fde0cb507d59d2a9168f3051feba6865e9d1048",
    "Commit message": "Fix memory leak in THTensor_(addmm) (#3536)\n\nTHTensor_(newContiguous) always increments the refcount. It may return\r\nthe same pointer if the tensor is always contiguous. Since we added the\r\ncheck for zero strides, it may be called when the tensor is already\r\ncontiguous. We need to make sure that THTensor_(free) is always called\r\nin this case.\r\n\r\nFixes #3498",
    "Deleted lines": 2,
    "Added lines": 6,
    "Changed lines": 8,
    "Deleted code": "  if(m1_ != m1)\n  if(m2_ != m2)",
    "Added code": "  int free_m1 = 0;\n  int free_m2 = 0;\n    free_m1 = 1;\n    free_m2 = 1;\n  if(free_m1)\n  if(free_m2)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0c0c9e743e82b398435ed07719e998aa15ac1ce1",
    "Commit message": "Fix dimensions check\n\nSummary:\nTo match CPU implementation [here](https://github.com/caffe2/caffe2/blob/master/caffe2/operators/segment_reduction_op.h#L323)\nCloses https://github.com/caffe2/caffe2/pull/1360\n\nDifferential Revision: D6111071\n\nPulled By: Maratyszcza\n\nfbshipit-source-id: ba0019ff483ff28f4aa452103c3bad5d9294af96",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    CHECK_LT(num_reduce_dims_, input.dims().size());",
    "Added code": "    CHECK_LE(num_reduce_dims_, input.dims().size());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4d0fbb0e6f578bea14f3f52b0a927bcc20f8b109",
    "Commit message": "ConcatOp: fix axis check with add_axis.\n\nSummary: when adding a new axis to concatenate along, allow it to be the last axis. For example, concated 1D columns into a 2D matrix with axis=1, add_axis=1.\n\nReviewed By: hoangmit\n\nDifferential Revision: D5622495\n\nfbshipit-source-id: 8d7c8650c198450ccd4f9e1c98e4ea9f40162be0",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "  CAFFE_ENFORCE_LT(axis_, input_zero.ndim(), \"Axis not in input ndim range.\");",
    "Added code": "  CAFFE_ENFORCE_LT(\n      axis_,\n      input_zero.ndim() + (add_axis_ ? 1 : 0),\n      \"Axis not in input ndim range.\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b2d110447190abe5d66b0b59a775cc4881f3e30e",
    "Commit message": "Fixed numpy bool check (#77857)\n\nFixes #75704\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/77857\nApproved by: https://github.com/jbschlosser",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "        if self._qkv_same_embed_dim is False:",
    "Added code": "        if not self._qkv_same_embed_dim:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/85cbe0d8258ab06897e2f049e61f74d8aa935240",
    "Commit message": "Fix Concat Dimension Bug (#22088)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/22088\n\nThis diff is similar to D14163001. We need to handle the edge case when add_axis=1.\n\nReviewed By: jspark1105\n\nDifferential Revision: D15949003\n\nfbshipit-source-id: 328d1e07b78b69bde81eee78c9ff5a8fb81f629b",
    "Deleted lines": 1,
    "Added lines": 3,
    "Changed lines": 4,
    "Deleted code": "  const int canonical_axis = canonical_axis_index_(axis, in[0].dims_size());",
    "Added code": "  int adj_size = in[0].dims_size() + (add_axis ? 1 : 0);\n  const int canonical_axis = canonical_axis_index_(axis, adj_size);\n  CAFFE_ENFORCE_LT(canonical_axis, adj_size, \"Axis not in input ndim range.\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4b45f08f8765549915417997c30ae8981f2ad125",
    "Commit message": "Added dim check for index_copy_ (#21617)\n\nSummary:\nFixing reported [bug](https://github.com/pytorch/pytorch/issues/20322)\nThe issue was related to not checking the dimensions of source vs destination tensors.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/21617\n\nDifferential Revision: D15749963\n\nPulled By: izdeby\n\nfbshipit-source-id: acff114c729fd9c0a9a51325e0ebd8b42e1f2fc1",
    "Deleted lines": 1,
    "Added lines": 5,
    "Changed lines": 6,
    "Deleted code": "  }",
    "Added code": "  }\n  } else if ((source.dim() != self.dim()) && (source.dim() != 0 && self.dim() != 0)) {\n    AT_INDEX_ERROR(\"index_copy_(): When source and destination are not scalars, their dimensionality must match. Source dimensionality (\",\n                   source.dim(), \"), destination dimensionality (\", self.dim(), \")\");\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4f63f348aef3da8b4d53f61098f4e32bd916c221",
    "Commit message": "Fix condition in inferUnsqueezeGeometry (#4909)\n\nThe bounds check was too conservative by an extra one.",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  int64_t new_stride = dim >= tensor.dim() - 1 ? 1 : sizes[dim] * strides[dim];",
    "Added code": "  int64_t new_stride = dim >= tensor.dim() ? 1 : sizes[dim] * strides[dim];"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bf32ea80942ce720b105efcd517fd11182edeb08",
    "Commit message": "Fix dimension check in 1D instance norm, allowing 2D tensors alongside 3D. (#9924)\n\nSummary:\nFixes #9776.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/9924\n\nDifferential Revision: D9028328\n\nPulled By: soumith\n\nfbshipit-source-id: d5f22abb2be83b34aee95ebe144c97519a6854f8",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "        if input.dim() != 3:\n            raise ValueError('expected 3D input (got {}D input)'",
    "Added code": "        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError('expected 2D or 3D input (got {}D input)'"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/036a159fb9c064877e32949b72c8f605fb358d5b",
    "Commit message": "Audit AT_ASSERT sites in TensorImpl.h; doc improvements (#20649)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20649\n\nI went through every occurrence of AT_ASSERT in this file and\nthought about whether or not it should be TORCH_INTERNAL_ASSERT\nor TORCH_CHECK.  I think I did a good job at it.  Some thoughts:\n\n- In order to decide if a check is \"internal\" or not, we must\n  think about where the separation between userspace and our internals\n  are.  I think any code that utilizes the PyTorch or Caffe2 C++ frontends\n  count as userspace.  An important collorary is that the majority of operator\n  code \"counts\" as userspace, even though it lives in our repository.  This\n  is inline with TCB (trusted computing base) thinking: you want the TCB to\n  be as small as possible, and because we have a *lot* of operator\n  implementations, they should not count as TCB.\n\n- The primary test I applied when considering an AT_ASSERT was whether or\n  not I could trigger this error by just making method calls on caffe2::Tensor\n  or at::Tensor.  If I could, that made it a TORCH_CHECK.  This covers most\n  of the misapplications of TORCH_INTERNAL_ASSERT.  One place I didn't\n  do this was the \"is variable\" checks; I think you have to work a bit\n  harder to trigger this case, and userspace code is not mixing up\n  Variables and Tensros.\n\n- I updated the docs for device_opt_, explaining when it could be nullopt.\n  (The nullopt checks here are TORCH_CHECK, because you can trigger them\n  by taking an undefined tensor and poking the methods.)\n\nDifferential Revision: D15395576\n\nfbshipit-source-id: 1c51b396012e7d949fbb4258092cf80e5e6f851b",
    "Deleted lines": 59,
    "Added lines": 58,
    "Changed lines": 117,
    "Deleted code": "  AT_ASSERT((unsigned)k <= dims.size());\n  AT_ASSERT((unsigned)l < dims.size());\n  AT_ASSERT(axis_index >= -ndims);\n  AT_ASSERT(axis_index < ndims);\n    AT_ASSERT(compute_numel() == numel_);\n    if (device_opt_.has_value()) {\n      // See NOTE [c10::optional operator usage in CUDA]\n      return (*device_opt_).index();\n    }\n\n    AT_ERROR(\n    if (device_opt_.has_value()) {\n      // See NOTE [c10::optional operator usage in CUDA]\n      return *device_opt_;\n    }\n\n    AT_ERROR(\n    AT_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    AT_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    AT_ASSERT(dim() == 0);\n    if (autograd_meta()) {\n      autograd_meta()->set_requires_grad(requires_grad, this);\n    } else {\n      AT_ERROR(\"set_requires_grad is not implemented for Tensor\");\n    }\n    if (autograd_meta()) {\n      return autograd_meta()->requires_grad();\n    } else {\n      AT_ERROR(\"requires_grad is not implemented for Tensor\");\n    }\n    AT_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    AT_ASSERTM(\n    AT_ASSERTM(\n    AT_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    AT_ASSERT(dtype_initialized());\n    AT_ASSERT(dtype_initialized());\n    AT_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    AT_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    AT_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    AT_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    AT_ASSERT(device_opt_.has_value());\n    AT_ASSERT(sizes_.size() >= 1u);\n    AT_ASSERTM(num >= 0, \"`num` must be non-negative for Extend\");\n    AT_ASSERTM(\n      AT_ASSERTM(\n    AT_ASSERTM(\n    AT_ASSERTM(\n    AT_ASSERTM(\n      AT_ASSERT(d >= 0);\n    AT_ASSERTM(\n    AT_ASSERTM(\n    // AT_ASSERTM(src.dtype_initialized(),\n    AT_ASSERTM(\n    AT_ASSERTM(\n        AT_ASSERT(storage_offset_ == 0); // because we just reallocated\n      AT_ASSERT(storage_offset_ == 0); // because we just reallocated\n    AT_ASSERT(has_storage());\n    AT_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    AT_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged",
    "Added code": "  TORCH_CHECK((unsigned)k <= dims.size());\n  TORCH_CHECK((unsigned)l < dims.size());\n  TORCH_CHECK(axis_index >= -ndims);\n  TORCH_CHECK(axis_index < ndims);\n    TORCH_INTERNAL_ASSERT(compute_numel() == numel_);\n    TORCH_CHECK(\n        device_opt_.has_value(),\n    // See NOTE [c10::optional operator usage in CUDA]\n    return (*device_opt_).index();\n    TORCH_CHECK(\n        device_opt_.has_value(),\n    // See NOTE [c10::optional operator usage in CUDA]\n    return *device_opt_;\n    TORCH_INTERNAL_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    TORCH_INTERNAL_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    TORCH_INTERNAL_ASSERT(dim() == 0);\n    TORCH_INTERNAL_ASSERT(autograd_meta(), \"set_requires_grad is not implemented for Tensor\");\n    autograd_meta()->set_requires_grad(requires_grad, this);\n    TORCH_INTERNAL_ASSERT(autograd_meta(), \"requires_grad is not implemented for Tensor\");\n    return autograd_meta()->requires_grad();\n    TORCH_INTERNAL_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    TORCH_CHECK(\n    TORCH_CHECK(\n    TORCH_INTERNAL_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    TORCH_CHECK(dtype_initialized(),\n        \"Cannot access data pointer of Tensor that doesn't have initialized dtype \"\n        \"(e.g., caffe2::Tensor x(CPU), prior to calling mutable_data<T>() on x)\");\n    TORCH_CHECK(dtype_initialized(),\n        \"Cannot report itemsize of Tensor that doesn't have initialized dtype \"\n        \"(e.g., caffe2::Tensor x(CPU), prior to calling mutable_data<T>() on x)\");\n    TORCH_INTERNAL_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    TORCH_INTERNAL_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    TORCH_INTERNAL_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    TORCH_INTERNAL_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    // TODO: A useful internal assert would be to show that device_opt_ is null\n    // only if you are an undefined tensor\n    TORCH_CHECK(device_opt_.has_value(), \"device_type cannot be run on undefined Tensor\");\n    TORCH_CHECK(sizes_.size() >= 1u);\n    TORCH_CHECK(num >= 0, \"`num` must be non-negative for Extend\");\n    TORCH_CHECK(\n      TORCH_CHECK(\n    TORCH_CHECK(\n    TORCH_CHECK(\n    TORCH_CHECK(\n      TORCH_CHECK(d >= 0);\n    TORCH_CHECK(\n    TORCH_CHECK(\n    // TORCH_CHECK(src.dtype_initialized(),\n    TORCH_CHECK(\n    TORCH_CHECK(\n        TORCH_INTERNAL_ASSERT(storage_offset_ == 0); // because we just reallocated\n      TORCH_INTERNAL_ASSERT(storage_offset_ == 0); // because we just reallocated\n    TORCH_CHECK(has_storage(), \"cannot call storage_initialized on tensor that does not have storage\");\n    TORCH_INTERNAL_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n    TORCH_INTERNAL_ASSERT(!is_variable());  // TODO: remove this when Variable and Tensor are merged\n  //\n  // INVARIANT: device_opt_ is only nullopt for undefined tensors\n  // (which do not have a device.)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a9deda5469a6ef73692a9dd796cc4eeba4436d6c",
    "Commit message": "Fix issue in sparce_coo_tensor only supporting CUDA device.\n\n## Motivation\nThe at::native::_validate_sparse_coo_tensor_args only supports checking the indices on CUDA device and CPU device. To extend the function to support more device type.\n\n## Solution\nCopy  the indices to the CPU to validate the correctness.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76293\nApproved by: https://github.com/mrshenli",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if (indices.is_cuda()) {",
    "Added code": "    if (!indices.is_cpu()) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d9870d70c12dc59b0f8bce288910422bcb60b044",
    "Commit message": "Exempt `_foreach_norm` from autograd_not_implemented_fallback check (#93995)\n\nFixes #93940\nPull Request resolved: https://github.com/pytorch/pytorch/pull/93995\nApproved by: https://github.com/ngimel, https://github.com/albanD",
    "Deleted lines": 1,
    "Added lines": 9,
    "Changed lines": 10,
    "Deleted code": "        if (!is_aliased_output[idx_ret] && t.has_storage())",
    "Added code": "        // note(crcrpar): `_foreach_norm` returns a list of scalar Tensors and\n        // each Tensor shares a storage of a hidden, intermediate 1D Tensor\n        // created inside the CUDA implemenetation. This is because the\n        // reference implementation of nvidia/apex repo returns this 1D Tensor\n        // where each element represents the norm of corresponding input Tensor,\n        // here I want to return the same number of Tensors as the input\n        // TensorList, see https://github.com/pytorch/pytorch/issues/93940\n        if (!is_aliased_output[idx_ret] && t.has_storage() &&\n            op_name != \"aten::_foreach_norm\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/98f9ff90268ae62ab6d794cce0786121bf17edc9",
    "Commit message": "[ONNX] Fix an assertion failure involving Slice (#71965)\n\nBefore this change, exporting a model to ONNX involving Slice crashes at `axes[i]` in line 153 if C++ assertions are enabled:\n```\n/usr/include/c++/11.1.0/bits/stl_vector.h:1045: std::vector<_Tp, _Alloc>::reference std::vector<_Tp, _Alloc>::operator[](std::vector<_Tp, _Alloc>::size_type) [with _Tp = long int; _Alloc = std::allocator<long int>; std::vector<_Tp, _Alloc>::reference = long int&; std::vector<_Tp, _Alloc>::size_type = long unsigned int]: Assertion '__n < this->size()' failed.\n```\nThe relevant check is https://github.com/gcc-mirror/gcc/blob/releases/gcc-11.1.0/libstdc++-v3/include/bits/stl_vector.h#L1045, which checks the vector index.\n\nThe issue can be reproduced by exporting Mask R-CNN or similar ones. For example,\n```Python\nimport io\nimport torch\nimport torchvision as tv\n\nmodel = tv.models.detection.maskrcnn_resnet50_fpn(pretrained=False)\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\nwith io.BytesIO() as f:\n    torch.onnx.export(model, x, f, opset_version=11)\n```\n(extracted from [onnxoptimizer tests](https://github.com/onnx/optimizer/blob/master/onnxoptimizer/test/optimizer_test.py))\n\nTested environment: Arch Linux x86_64 with pytorch and torchvisoin installed from [the official repo](https://github.com/archlinux/svntogit-community/blob/packages/python-pytorch/trunk/PKGBUILD) and [AUR](https://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=python-torchvision), respectively.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72989",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    axes.reserve(inputTensorValues[3].sizes()[0]);",
    "Added code": "    axes.resize(inputTensorValues[3].sizes()[0]);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b05c34259b5e3ff9b55fba5a24d7c5f2f2036471",
    "Commit message": "relax size check in flatten_for_scatter_gather (#40573)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40573\n\nPer title, to workaround apex sbn bug.\n\nTest Plan: Covered by existing tests\n\nReviewed By: blefaudeux\n\nDifferential Revision: D22236942\n\nfbshipit-source-id: ddb164ee347a7d472a206087e4dbd16aa9d72387",
    "Deleted lines": 6,
    "Added lines": 2,
    "Changed lines": 8,
    "Deleted code": "      if (t.sizes() != other[i].sizes()) {\n            \"All tensor operands to scatter/gather must have the same size\");\n      }\n      if (t.strides() != other[i].strides()) {\n        throw std::runtime_error(\n            \"All tensor operands to scatter/gather must have the same layout (strides)\");",
    "Added code": "      if (t.numel() != other[i].numel()) {\n            \"All tensor operands to scatter/gather must have the same number of elements\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/71af538e31547e5b1bc814c9e00323a21905baf3",
    "Commit message": "Updated assert to remove check on 3rd dim for MHA (#39402)\n\nSummary:\n## Description\n* Updated assert statement to remove check on 3rd dimension (features) for keys and values in MultiheadAttention / Transform\n* The feature dimension for keys and values can now be of different sizes\n* Refer to https://github.com/pytorch/pytorch/issues/27623\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39402\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D21841678\n\nPulled By: Nayef211\n\nfbshipit-source-id: f0c9e5e0f33259ae2abb6bf9e7fb14e3aa9008eb",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "    assert key.size() == value.size()",
    "Added code": "    # allow MHA to have different sizes for the feature dimension\n    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b8ab3080b1043a610ba2825a2be406a1833b1d70",
    "Commit message": "Fix InferShapesAndTypes() for convolutions\n\nSummary:\nIf kernel sizes were specified via \"kernel_w\" and \"kernel_h\", tensor size\ninference was incorrect in InferShapesAndTypes(): it was checking for\n\"helper_w\" instead of \"kernel_w\".\n\nReviewed By: akyrola\n\nDifferential Revision: D5884280\n\nfbshipit-source-id: 430cbedcedadbe3570384e706198a4ddc499504e",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "        helper.HasArgument(\"kernel_h\") && helper.HasArgument(\"helper_w\")) {",
    "Added code": "        helper.HasArgument(\"kernel_h\") && helper.HasArgument(\"kernel_w\")) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/be253b8ee8a104997773d11ed28928a48193217d",
    "Commit message": "Fix overflow check in `geometry_is_contiguous` (#73162)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73162\n\nThe existing check isn't safe for 32-bit `size_t` because the max\n64-bit int will overflow.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D34524229\n\nPulled By: malfet\n\nfbshipit-source-id: 58b1bb2fa605e9972644723ba8b37643ba024f38\n(cherry picked from commit b15aa0fbd49153bcdf61bc7676449f3ee4a5bb51)",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  assert(sizes.size() < static_cast<std::size_t>(std::numeric_limits<std::int64_t>::max()));",
    "Added code": "  assert(!overflows<std::int64_t>(sizes.size()));"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/128dd6b1502ad3687ffb79484b72d4cfafec496e",
    "Commit message": "[pytorch] Relax the check that makes sure number of outs equals number of returns (#76049)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76049\n\n## Context\nWe are trying to add an out variant for an existing operator, e.g.,:\n```\nchunk.out(Tensor self, int chunks, int dim=0, *, Tensor(a!)[] out) -> Tensor(a!)[]\n```\nNotice the out argument is a mutable list of tensors. The existing guideline defined in [model.py](https://fburl.com/nn299ifx) requires the same argument type to be returned from this operator. Given the fact that we don't support mutable tensor list as a return type and it seems not useful to add such a return type.\n\nThe solution I'm proposing is to relax the constraint that the number of outs needs to be the same as the number of returns, so we can return a `void`.\n```\nchunk.out(Tensor self, int chunks, int dim=0, *, Tensor(a!)[] out) -> ()\n```\n\nTest Plan: Rely on existing CI\n\nReviewed By: ezyang, iseeyuan\n\nDifferential Revision: D35737310\n\nfbshipit-source-id: 66b5738cc1dcd13d532a6c97fea979bd58f381df\n(cherry picked from commit 9aac5493285cd4f49a07053edfa5916c449a930c)",
    "Deleted lines": 3,
    "Added lines": 2,
    "Changed lines": 5,
    "Deleted code": "            assert len(self.arguments.out) == len(\n                self.returns\n            ), \"Must return as many arguments as there are out arguments\"",
    "Added code": "            assert len(self.arguments.out) == len(self.returns) or len(self.returns) == 0, \\\n                \"Must return as many arguments as there are out arguments, or no return at all\""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/993d8bb77e1ce79940705b1c7667dc9276f449df",
    "Commit message": "Use size to check same tensor sizes in reduce_scatter and allgather (#84099)\n\nSummary:\nPrevious code uses tensor.numel() to check if all tensors have the same size in order to switch between reduce_scatter_v v.s. reduce_scatter, same applies to allgather. However, if the user input tensor is zero in the last dimension (e.g., [648632,0]), then numel() returns zero and check_same_numel is always true.\n\nThis patch fixes the check to use size rather than numel, to cover the above case.\n\nDifferential Revision: D39044439\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/84099\nApproved by: https://github.com/kwen2501",
    "Deleted lines": 7,
    "Added lines": 6,
    "Changed lines": 13,
    "Deleted code": "bool check_same_numel(const std::vector<at::Tensor>& input_tensors) {\n  int64_t numel = input_tensors[0].numel();\n    if (numel != input_tensor.numel()) {\n  bool same_numel = check_same_numel(outputTensors.back());\n  if (same_numel) {\n  bool same_numel = check_same_numel(inputTensors.back());\n  if (same_numel) {",
    "Added code": "bool check_same_size(const std::vector<at::Tensor>& input_tensors) {\n    if (!input_tensors[0].is_same_size(input_tensor)) {\n  bool same_size = check_same_size(outputTensors.back());\n  if (same_size) {\n  bool same_size = check_same_size(inputTensors.back());\n  if (same_size) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/363d5300356e17e7e9eb92381a4cf8a932fca3e0",
    "Commit message": "Fix decision logic for `should_cast_forward_inputs` in `_root_pre_forward()` and `_pre_forward()` (#99546)\n\nFixes #99545\n\nThere is currently no topological constraint dictating FSDP instances own ``FlatParamHandle`` s directly. If all parameters are managed by descendant FSDP instances leaving an FSDP instance with no direct ``state._handles``, the  ``should_cast_forward_inputs`` decisions below in both ``_root_pre_forward()`` and ``_pre_forward()`` respectively can return incorrect decisions [^1].\n\nFor [``_root_pre_forward()``](https://github.com/pytorch/pytorch/blob/436edc5ac3de4c4e677ed136473bafe72002cc93/torch/distributed/fsdp/_runtime_utils.py#L514):\n\nhttps://github.com/pytorch/pytorch/blob/436edc5ac3de4c4e677ed136473bafe72002cc93/torch/distributed/fsdp/_runtime_utils.py#L602-L604\n\nFor [``_pre_forward``](https://github.com/pytorch/pytorch/blob/436edc5ac3de4c4e677ed136473bafe72002cc93/torch/distributed/fsdp/_runtime_utils.py#L384):\n\nhttps://github.com/pytorch/pytorch/blob/436edc5ac3de4c4e677ed136473bafe72002cc93/torch/distributed/fsdp/_runtime_utils.py#L420-L422\n\nSee the [related issue](https://github.com/pytorch/pytorch/issues/99545) for reproduction.\n\n### Remediation\n\nIn this PR, I amend the two decision statements referenced above (in both `_root_pre_forward()` and `_pre_forward()`) to account for FSDP instances without direct handles:\n```python\nshould_cast_forward_inputs = len(state._handles) > 0 and all(\n    not handle._force_full_precision for handle in state._handles\n)\n```\n\nIf one configures ``MixedPrecision`` in the example above with ``cast_forward_inputs=True`` and the ``should_cast_forward_inputs`` adjustment above, FSDP returns to the expected behavior and produces no error.\n\nThough the check is the same in both ``_root_pre_forward()`` and ``_pre_forward()`` and hence could be refactored into a separate function, I figured it may make sense to retain separate statements to preserve the ability for root-specific behavior in the future. Whichever approach the team prefers I can update this PR with.\n\n### Implementation considerations and questions:\n\n1. Rather than write a test that would arguably have a poor utility/resource usage profile, I have not added any tests associated with this PR. The new decision logic is exercised by all existing tests (which continue to pass after this PR of course) so I think the utility of new tests is fairly modest. Let me know if you think new tests should be added and I'm happy to do so.\n2. As discussed above, the decision statement shared among ``_pre_forward()`` and ``_root_pre_forward()`` could be factored out into a separate function. Given the simplicity of the statement and to retain current flexibility for root-specific decisions it might not be worth the refactor so I haven't done it yet. Let me know if you'd like me to do so.\n3. The note below could be updated to indicate the utility of setting ``cast_forward_inputs=True`` for the situations addressed with this PR but I haven't done so since I'm not sure it's worth complicating the current usage guidance. I'd be happy to add verbiage describing the use case if the team wants it.\nhttps://github.com/pytorch/pytorch/blob/cde35b406902d421ea5ae5f10a114da95e7171f1/torch/distributed/fsdp/api.py#L175-L181\n\nThanks again to the PyTorch distributed team for your immensely valuable contributions to the open-source ML community!\n\n[^1]: Though one could keep the existing decision logic and impose a new topological constraint requiring all FSDP instances have direct `_handles`, I think retaining the current wrapping flexibility is both convenient and useful enough (e.g. programmatic wrapping of modules that may or may not already have all parameters handled by descendant FSDP instances) to update the decision logic as discussed here instead.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/99546\nApproved by: https://github.com/awgu",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "        should_cast_forward_inputs = all(\n        should_cast_forward_inputs = all(",
    "Added code": "        should_cast_forward_inputs = len(state._handles) > 0 and all(\n        should_cast_forward_inputs = len(state._handles) > 0 and all("
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9c3cb6e652e6033e355cef92229fde5b0baf271b",
    "Commit message": "Fix stride checks in gemm dispatch (#3548)\n\nFrom https://software.intel.com/en-us/mkl-developer-reference-fortran-gemm:\r\n\r\n lda: \"When transa = 'N' or 'n', then lda must be at least max(1, m),\r\n       otherwise lda must be at least max(1, k).\"\r\n\r\n ldb: \"When transb = 'N' or 'n', then ldb must be at least max(1, k),\r\n       otherwise ldb must be at least max(1, n).\"\r\n\r\nPartly addresses #3525",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "      (lda >= THMax(1, (transa_ ? m : k))) && (lda <= INT_MAX) &&\n      (ldb >= THMax(1, (transb_ ? k : n))) && (ldb <= INT_MAX) &&",
    "Added code": "      (lda >= THMax(1, (transa_ ? k : m))) && (lda <= INT_MAX) &&\n      (ldb >= THMax(1, (transb_ ? n : k))) && (ldb <= INT_MAX) &&"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7f125bca1cd42ebd8e07c97f1bd1682dff5cf387",
    "Commit message": "[Metal] Add pin_memory check in empty_strided (#47228)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47228\n\nAdd the false checking if pin_memory has been specified to `False`\nghstack-source-id: 115715087\n\nTest Plan:\n- CircleCI\n- Sandcastle\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D24690472\n\nfbshipit-source-id: c65fc494fcd7b0b409a80c86e108a029ca7fd71e",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "      !pin_memory.has_value(),",
    "Added code": "      !pin_memory.has_value() || !pin_memory.value(),"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f02b7a9c36dd6182da694bc47a5c345285dfd951",
    "Commit message": "Pad: don't error when unused fill value is zero\n\nFixes pytorch/vision#5873\n\nIn the python version of `F.pad`, checking that the fill value was\nleft as default was done by comparing against zero:\nhttps://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/nn/functional.py#L4366\n\nSo if someone does explicitly pass in a zero-value, then this\n`TORCH_CHECK` was an accidental BC-break.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76307\n\nApproved by: https://github.com/albanD, https://github.com/jbschlosser, https://github.com/datumbox",
    "Deleted lines": 4,
    "Added lines": 3,
    "Changed lines": 7,
    "Deleted code": "  TORCH_CHECK(\n      !value.has_value(), \"Padding mode \\\"\",\n      padding_mode_string(mode),\n      \"\\\" doesn't take in value argument\");",
    "Added code": "  TORCH_CHECK(!value.has_value() || *value == 0,\n              \"Padding mode \\\"\", padding_mode_string(mode),\n              \"\\\" doesn't take in value argument\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4839f73f329b38819e6f69a8662d61dc36558e52",
    "Commit message": "Fix incorrect tensor storage check  (#86845)\n\nFix incorrect tensor storage check\n\nThis change contains an incorrect check for storage: https://github.com/pytorch/pytorch/pull/86557\n**self.storage is not None**\nshould have been:\n**not torch._C._has_storage(self)**\n\nThese fixes were run through the DirectML test suite, and confirm the check is now working correctly.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86845\nApproved by: https://github.com/martinb35, https://github.com/bdhirsh",
    "Deleted lines": 3,
    "Added lines": 5,
    "Changed lines": 8,
    "Deleted code": "\n                or (self.storage is None and self.device.type == \"privateuseone\")\n            self.storage is None and self.device.type == \"privateuseone\"",
    "Added code": "                or (\n                    not torch._C._has_storage(self)\n                    and self.device.type == \"privateuseone\"\n                )\n            not torch._C._has_storage(self) and self.device.type == \"privateuseone\""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/75be4f9cdb503d6eff189b2bc5c05d96bff66653",
    "Commit message": "check tensor has storage before refer to tensor data ptr\n\nIn the exporter dedupe initializers passes, check the tensor has storage before reference to tensor's data_ptr, otherwise it will result in a crash.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76342\nApproved by: https://github.com/BowenBao",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "      (t1.data_ptr() == t2.data_ptr());",
    "Added code": "      (t1.has_storage() && t2.has_storage() && t1.data_ptr() == t2.data_ptr());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a3701b674046bcefb5927a6643364b186f77dbcf",
    "Commit message": "fix backward bug for custom device (#98586)\n\nFixes #ISSUE_NUMBER\nIn the backward on some device , it may get an error to get device index because of exchange a new thread.\nSo just set_device and check the device index in `setDevice`  func may be better for some many kinds of devices.\nFor CUDA, the device index check is also included in `setDevice`  func.https://github.com/pytorch/pytorch/blob/master/c10/cuda/impl/CUDAGuardImpl.h#:~:text=%7D-,void%20setDevice(Device%20d)%20const%20override%20%7B,%7D,-void%20uncheckedSetDevice(Device\n```\nvoid setDevice(Device d) const override {\n    TORCH_INTERNAL_ASSERT(d.is_cuda());\n    Device current_device = getDevice();\n    if (current_device != d) {\n      C10_CUDA_CHECK(cudaSetDevice(d.index()));\n    }\n  }\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98586\nApproved by: https://github.com/albanD",
    "Deleted lines": 2,
    "Added lines": 1,
    "Changed lines": 3,
    "Deleted code": "      if (impl && device < impl->deviceCount() &&\n          impl->getDevice().index() != device) {",
    "Added code": "      if (impl && device < impl->deviceCount()) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6f5945e4bb1258d39a2878a08a910fcc8f659d5e",
    "Commit message": "triton supports devices < 7.0, not 6.0 (#90020)\n\ntriton is still buggy with Pascal devices, so make the error checker reflect that.\n\nAlso, this < 6.0 never worked, as the `has_triton` definition in utils.py was checking >= 7.0.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90020\nApproved by: https://github.com/yanboliang, https://github.com/anijain2305",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "                if device_props.major < 6:\n                        f\"Found {device_props.name} which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 6.0, but your device is of CUDA capability {device_props.major}.{device_props.minor}\"  # noqa: B950",
    "Added code": "                if device_props.major < 7:\n                        f\"Found {device_props.name} which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability {device_props.major}.{device_props.minor}\"  # noqa: B950"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7c44823a4e0506b2eb7f1aa667ba939ba9d56969",
    "Commit message": "Fix layout/device checks in sparse-dense addmm (#94843)\n\nResolves #94684\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94843\nApproved by: https://github.com/cpuhrsch",
    "Deleted lines": 9,
    "Added lines": 19,
    "Changed lines": 28,
    "Deleted code": "    const Scalar& alpha\n) {\n  AT_ASSERT(r.layout() == kStrided, \"addmm_sparse_dense: expected strided result tensor, got tensor with layout \", r.layout());\n  AT_ASSERT(sparse_.layout() == kSparse, \"addmm_sparse_dense: expected sparse tensor, got tensor with layout \", sparse_.layout());\n\n      t.device());\n      t.device());\n      t.device());\n",
    "Added code": "    const Scalar& alpha) {\n      r.device());\n      sparse_.device());\n      dense.device());\n\n  TORCH_CHECK(\n      r.layout() == kStrided,\n      \"addmm_sparse_dense: expected strided result tensor, got tensor with layout \",\n      r.layout());\n  TORCH_CHECK(\n      t.layout() == kStrided,\n      \"addmm_sparse_dense: expected 't' to have strided layout, got tensor with layout \",\n      t.layout());\n  TORCH_CHECK(\n      sparse_.layout() == kSparse && dense.layout() == kStrided,\n      \"addmm_sparse_dense: expected either 'mat1' to have sparse layout and 'mat2' to have strided layout, got 'mat1' with layout \",\n      sparse_.layout(),\n      \" and 'mat2' with layout \",\n      dense.layout());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/871b5caae76185cff141c522b3133e7543c8dabf",
    "Commit message": "Fix hpu deserialization bug (#109499)\n\n# Motivation\nfix hpu deserialization bug. It should check hpu model if and only if location start with hpu. Otherwise, it always raise an AssertError if hpu is not imported. This break the serialization/desirialization functionality abourt other third-party like IPEX.\n\n# Solution\nonly assert hpu model when start with hpu\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109499\nApproved by: https://github.com/ezyang",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "    hpu = getattr(torch, \"hpu\", None)\n    assert hpu is not None, \"HPU device module is not loaded\"",
    "Added code": "        hpu = getattr(torch, \"hpu\", None)\n        assert hpu is not None, \"HPU device module is not loaded\""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6592259ea52f45e1fc9a633ccb5b154ba5099334",
    "Commit message": "[HPU] Enable torch.jit.load for HPU (#81759)\n\nAs per torch.jit.load documentation, all previously saved modules,\nirrespective of their device, are first loaded onto CPU, and then\nare moved to the devices they were saved from. So far, supported\ndevices included CPU and CUDA only. To enable torch.jit.load for\nHPU, additional check for HPU is introduced.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/81759\nApproved by: https://github.com/eellison",
    "Deleted lines": 2,
    "Added lines": 3,
    "Changed lines": 5,
    "Deleted code": "      if (device.is_cuda() || device.is_xpu() || device.is_meta()) {\n            \"supported devices include CPU and CUDA, however got \",",
    "Added code": "      if (device.is_cuda() || device.is_xpu() || device.is_meta() ||\n          device.is_hpu()) {\n            \"supported devices include CPU, CUDA and HPU, however got \","
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1becd2c314f45bded8d3fbec91d785e7190b4afe",
    "Commit message": "Align checks in `_use_cudnn_ctc_loss` with those in `_cudnn_ctc_loss` (#115617)\n\nThis PR is intended to fix the following problem:\n\nWhen using `CTCLoss`, there is a cudnn path gated by a call to [`_use_cudnn_ctc_loss`](\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/cudnn/LossCTC.cpp#L73-L101) which checks some conditions\n\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/LossCTC.cpp#L486-L496\n\nHowever, there are more checks in `_cudnn_ctc_loss`\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/cudnn/LossCTC.cpp#L122-L130\n\nsome of which are not present in `_use_cudnn_ctc_loss` (e.g. the check that `targets` is on CPU which will cause a RuntimeError after dispatching to `_cudnn_ctc_loss`). Instead, these checks should be in `_use_cudnn_ctc_loss` so that the normal `_ctc_loss` path will be used if the checks are not met)\n\ne.g. Before this PR\n\n```python\n>>> import torch\n>>> ctcloss = torch.nn.CTCLoss()\n>>> log_probs = torch.randn((50, 3, 15), device='cuda').log_softmax(2)\n>>> target = torch.randint(1, 15, (30 + 25 + 20,), dtype = torch.int)\n>>> input_lengths = torch.tensor((50, 50, 50), device='cuda')\n>>> target_lengths = torch.tensor((30, 25, 20), device='cuda')\n>>> ctcloss(log_probs, target, input_lengths, target_lengths)\ntensor(4.1172, device='cuda:0')\n>>> target = target.to('cuda')\n>>> ctcloss(log_probs, target, input_lengths, target_lengths)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/loss.py\", line 1779, in forward\n    return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n  File \"/data/users/mg1998/pytorch/torch/nn/functional.py\", line 2660, in ctc_loss\n    return torch.ctc_loss(\nRuntimeError: Expected tensor to have CPU Backend, but got tensor with CUDA Backend (while checking arguments for cudnn_ctc_loss)\n```\n\nAfter this PR the above snippet runs without error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115617\nApproved by: https://github.com/janeyx99",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "      (log_probs.device().type() == at::kCUDA);",
    "Added code": "      (log_probs.device().type() == at::kCUDA) &&\n      (targets.device().type() == at::kCPU) &&\n      (targets.is_contiguous()) &&\n      (log_probs.dim() == 3);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/490f2d75700a806bdc6110e881e78493cde163e3",
    "Commit message": "Skip privateuse1's checkZeroPoints (#114117)\n\nWe want to use ``quantize_per_channel`` to create a quantized tensor, but we found that ``checkZeroPoints`` for ``privateuse1`` backend failed.\n\n``quantize_tensor_per_channel_affine`` will ``checkZeroPoints`` for all backends expect ``CUDA``:\nhttps://github.com/pytorch/pytorch/blob/140c54e6ccc5e97f1b7f1e0fcd3d8c6af7dd2ab2/aten/src/ATen/native/quantized/AffineQuantizer.cpp#L162-L164\n\nHowever, our ``privateuse1`` backend will get a segmentation error if we try to cast our data to int64_t in ``checkZeroPoints``:\nhttps://github.com/pytorch/pytorch/blob/140c54e6ccc5e97f1b7f1e0fcd3d8c6af7dd2ab2/aten/src/ATen/native/quantized/AffineQuantizer.cpp#L82-L88\n\nSo if we can skip ``privateuse1``'s ``checkZeroPoints`` and check this item in the actual device function? What do you think?\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114117\nApproved by: https://github.com/jerryzh168",
    "Deleted lines": 4,
    "Added lines": 6,
    "Changed lines": 10,
    "Deleted code": "    if(qtensor.device().type() != c10::DeviceType::CUDA){\n    }  // for cuda, this check will occur in the actual cuda function\n    if(qtensor.device().type() != c10::DeviceType::CUDA){\n    }  // for cuda, this check will occur in the actual cuda function",
    "Added code": "    if (qtensor.device().type() != c10::DeviceType::CUDA &&\n        qtensor.device().type() != c10::DeviceType::PrivateUse1) {\n    }  // for cuda and privateuse1, this check will occur in the actual device function\n    if(qtensor.device().type() != c10::DeviceType::CUDA &&\n       qtensor.device().type() != c10::DeviceType::PrivateUse1){\n    }  // for cuda and privateuse1, this check will occur in the actual device function"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a076a74f1118da171cf70d00d1de4abbe27cf85d",
    "Commit message": "[Nested Tensor] Add xpu device in assertion for nested tensor creation (#114664)\n\nAdd xpu device checking in nested tensor creation\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114664\nApproved by: https://github.com/jgong5, https://github.com/xunnanxu",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "      storage_device.is_cpu() || storage_device.is_cuda() || storage_device.is_privateuseone(),\n      \"NestedTensorImpl storage must be either CUDA, CPU or \", get_privateuse1_backend(), \" but got \",",
    "Added code": "      storage_device.is_cpu() || storage_device.is_cuda() || storage_device.is_xpu() || storage_device.is_privateuseone(),\n      \"NestedTensorImpl storage must be either CUDA, CPU, XPU or \", get_privateuse1_backend(), \" but got \","
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/097defb1608827d82b18b27adeec0a98b72a9281",
    "Commit message": "[device mesh] only check when world size > num_devices per host (#111091)\n\nas titled\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111091\nApproved by: https://github.com/awgu, https://github.com/wz337\nghstack dependencies: #110898, #110900",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "            if world_size % num_devices_per_host != 0:",
    "Added code": "            if (\n                world_size > num_devices_per_host\n                and world_size % num_devices_per_host != 0\n            ):"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/57af1ec14594a73c8f2b73bf70c04ba7efeb6eab",
    "Commit message": "observers: use torch.all to check for valid min and max values (#43151)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43151\n\nUsing `torch.all` instead of `torch.sum` and length check.\nIt's unclear whether the increase in perf (~5% for small inputs) is\nreal, but should be a net benefit, especially for larger channel inputs.\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23170426\n\nfbshipit-source-id: ee5c25eb93cee1430661128ac9458a9c525df8e5",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "            assert torch.sum(min_val <= max_val) == len(min_val), \"min {} should be less than max {}\".format(",
    "Added code": "            assert torch.all(min_val <= max_val), \"min {} should be less than max {}\".format("
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/07f0f383fa23e63eca164036ab58ab983e9437eb",
    "Commit message": "update tensor-like to check instance for torch function impl (#111087)\n\ntensor like should check the instance for a torch function impl, not the type\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111087\nApproved by: https://github.com/ezyang",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    return type(inp) is torch.Tensor or hasattr(type(inp), \"__torch_function__\")",
    "Added code": "    return type(inp) is torch.Tensor or hasattr(inp, \"__torch_function__\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a33d8133a52c7453958b70facc40bd7448d5d88d",
    "Commit message": "Slight cleanup of VariableBuilder giant if condition (#95471)\n\nSome of these changes are semantics preserving, some are not. Please review carefully.\n\n* Use `istype(x, y)` over `type(x) is y`\n* Use istype over isinstance in frozenset. If the user subclassed the type in question, we must treat it as a user defined class as it may have custom behavior\n* The `isinstance(value, (int, float))` condition for `wrap_unspecialized_primitive` is dead-ish; direct int/float values are caught earlier istype check. Technically however, if you subclassed int/float it would pass through, however this is almost assuredly not intended behavior\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95471\nApproved by: https://github.com/Skylion007",
    "Deleted lines": 7,
    "Added lines": 6,
    "Changed lines": 13,
    "Deleted code": "        elif isinstance(value, frozenset) and (\n        elif type(value) is torch.autograd.function.FunctionMeta:\n            and type(getattr(value, \"__self__\", None))\n            is torch.autograd.function.FunctionMeta\n        elif isinstance(value, (int, float)) or (\n            HAS_NUMPY and (isinstance(value, np.number))\n        ):",
    "Added code": "        elif istype(value, frozenset) and (\n        elif istype(value, torch.autograd.function.FunctionMeta):\n            and istype(\n                getattr(value, \"__self__\", None), torch.autograd.function.FunctionMeta\n            )\n        elif HAS_NUMPY and isinstance(value, np.number):"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4e1bd4abe7691f460cb021e5b314168caa42ef92",
    "Commit message": "Fix scalar type resolution for optional tensor (#94427)\n\nWhen TorchScript Value has an optional tensor, `dtype()` or `scalarType()` is not available and raise (by design).\n\nThe symbolic `_op_with_optional_float_cast` must check whether the tensor is otpional or not before calling the scalar type resolution API. This PR fixes that\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94427\nApproved by: https://github.com/abock, https://github.com/shubhambhokare1",
    "Deleted lines": 7,
    "Added lines": 10,
    "Changed lines": 17,
    "Deleted code": "            input_scalar_type = _type_utils.JitScalarType.from_value(input)\n            if input.isCompleteTensor() and input_scalar_type != dtype_0:\n                raise errors.SymbolicValueError(\n                    f\"Inputs of {op_name} must have same dtype. Got {dtype_0.scalar_name()} and {input_scalar_type.scalar_name()}\",\n                    input,\n                )\n            t = g.op(\"Cast\", t, to_i=dtype.onnx_type())",
    "Added code": "\n            if input.isCompleteTensor():\n                input_scalar_type = _type_utils.JitScalarType.from_value(input)\n                if input_scalar_type != dtype_0:\n                    raise errors.SymbolicValueError(\n                        f\"Inputs of {op_name} must have same dtype.\"\n                        f\"Got {dtype_0.scalar_name()} and {input_scalar_type.scalar_name()}\",\n                        input,\n                    )\n            t = g.op(\"Cast\", t, to_i=_type_utils.JitScalarType(dtype).onnx_type())"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b90db4a78f8d760377a81a5a64d03ab4b67599de",
    "Commit message": "[DataPipe] Fix type checking to accept both Iter and Map DataPipe (#87285)\n\nFixes https://github.com/pytorch/data/issues/841\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87285\nApproved by: https://github.com/NivekT",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    assert isinstance(datapipe, IterDataPipe)",
    "Added code": "    assert isinstance(datapipe, (IterDataPipe, MapDataPipe))"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/63cbdc92a750a667ffdcfbdac563d02db6fd9559",
    "Commit message": "switching the exact check to isinstance check (#84023)\n\nSimplifying a type check if an object is a SymIntNode in `is_symint_node`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/84023\nApproved by: https://github.com/ezyang",
    "Deleted lines": 2,
    "Added lines": 1,
    "Changed lines": 3,
    "Deleted code": "  // TODO: switch this to `isinstance`\n  if (obj.get_type().equal(tp_symn)) {",
    "Added code": "  if (py::isinstance(obj, tp_symn)) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b7d58745c882a66f5c20d2ba05b99ddce7491d38",
    "Commit message": "dbr quant overhead[6/x]: remove unneeded isinstance checks in `op_convert_before_hook` (#68371)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68371\n\n`isinstance` has some overhead, changing the code in `op_convert_before_hook`\nto use the information calculate during tracing instead which is cheaper.\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeDBR\n```\n\nfunction level benchmarking\n```\n// MobileNetV2, 1x3x224x224 input, % of time spent by function during DBR convert\n\n// before\nop_convert_before_hook = 3.55%\nisinstance = 1.62%\n\n// after\nop_convert_before_hook = 2.89%\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D32463757\n\nPulled By: vkuzo\n\nfbshipit-source-id: 129efe9c279a41f55b8bfd09132e21c0066298a6",
    "Deleted lines": 4,
    "Added lines": 1,
    "Changed lines": 5,
    "Deleted code": "        if isinstance(args[0], (tuple, list)):  # torch.cat variants\n                if not isinstance(arg, torch.Tensor):\n                    new_args.append(arg)\n                    continue",
    "Added code": "        if orig_op is torch.cat:  # torch.cat variants"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6420071b43dc9f2679c22952b5051b0c28f42da2",
    "Commit message": "Disable complex dispatch on min/max functions (#50347)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/50064\n\n**PROBLEM:**\nIn issue https://github.com/pytorch/pytorch/issues/36377, min/max functions were disabled for complex inputs (via dtype checks).\nHowever, min/max kernels are still being compiled and dispatched for complex.\n\n**FIX:**\nThe aforementioned dispatch has been disabled & we now rely on errors produced\nby dispatch macro to not run those ops on complex, instead of doing redundant dtype checks.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50347\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D25870385\n\nPulled By: anjali411\n\nfbshipit-source-id: 921541d421c509b7a945ac75f53718cd44e77df1",
    "Deleted lines": 3,
    "Added lines": 3,
    "Changed lines": 6,
    "Deleted code": "    AT_DISPATCH_ALL_TYPES_AND_COMPLEX(input.scalar_type(), \"min_all\", [&] {\n    AT_DISPATCH_ALL_TYPES_AND_COMPLEX(input.scalar_type(), \"max_all\", [&] {\n    AT_DISPATCH_ALL_TYPES_AND_COMPLEX(input.scalar_type(), \"_aminmax_all_all\", [&] {",
    "Added code": "    AT_DISPATCH_ALL_TYPES(input.scalar_type(), \"min_all\", [&] {\n    AT_DISPATCH_ALL_TYPES(input.scalar_type(), \"max_all\", [&] {\n    AT_DISPATCH_ALL_TYPES(input.scalar_type(), \"_aminmax_all_all\", [&] {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/73431f087b48560ab4b65f8814809fc019061223",
    "Commit message": "Allow torch.load and torch.save to take pathlib.Path (#3589)\n\n* Allow torch.load to take pathlib.Path\r\n\r\npathlib has been python standard library for filesystem path since python 3.4\r\nBut `torch.load` currently cannot take `pathlib.Path` as its filename of state dictionary.\r\nI changed `torch.load` and `_with_file_like` to check so that they can accept `pathlib.Path` typed filepath.\r\n\r\n* Fix flake8: too long line & indentation",
    "Deleted lines": 2,
    "Added lines": 7,
    "Changed lines": 9,
    "Deleted code": "    if isinstance(f, str) or (sys.version_info[0] == 2 and isinstance(f, unicode)):\n    if isinstance(f, str) or (sys.version_info[0] == 2 and isinstance(f, unicode)):",
    "Added code": "    import pathlib\n    if isinstance(f, str) or \\\n            (sys.version_info[0] == 2 and isinstance(f, unicode)) or \\\n            (sys.version_info[0] == 3 and isinstance(f, pathlib.Path)):\n    if isinstance(f, str) or \\\n            (sys.version_info[0] == 2 and isinstance(f, unicode)) or \\\n            (sys.version_info[0] == 3 and isinstance(f, pathlib.Path)):"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/24ca6aab024f4e45e462e387ffd710a1d25b59da",
    "Commit message": "Improves type-checking guards. (#43339)\n\nSummary:\nPR https://github.com/pytorch/pytorch/issues/38157 fixed type checking for mypy by including `if False` guards on some type-checker-only imports. However other typecheckers - [like pyright](https://github.com/microsoft/pylance-release/issues/262#issuecomment-677758245) - will respect this logic and ignore the imports. Using [`if TYPE_CHECKING`](https://docs.python.org/3/library/typing.html#typing.TYPE_CHECKING) instead means both mypy and pyright will work correctly.\n\n[For background, an example of where the current code fails](https://github.com/microsoft/pylance-release/issues/262) is if you make a file `tmp.py` with the contents\n```python\nimport torch\ntorch.ones((1,))\n```\nThen [`pyright tmp.py --lib`](https://github.com/microsoft/pyright#command-line) will fail with a `\"ones\" is not a known member of module` error. This is because it can't find the `_VariableFunctions.pyi` stub file, as pyright respects the `if False` logic. After adding the `TYPE_CHECKING` guard, all works correctly.\n\nCredit to erictraut for suggesting the fix.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43339\n\nReviewed By: agolynski\n\nDifferential Revision: D23348142\n\nPulled By: ezyang\n\nfbshipit-source-id: c8a58122a7b0016845c311da39a1cc48748ba03f",
    "Deleted lines": 4,
    "Added lines": 7,
    "Changed lines": 11,
    "Deleted code": "from typing import Set, Type\nif False:\nif False:\n    from torch._C._VariableFunctions import *",
    "Added code": "from typing import Set, Type, TYPE_CHECKING\nif TYPE_CHECKING:\nif TYPE_CHECKING:\n    # Some type signatures pulled in from _VariableFunctions here clash with\n    # signatures already imported. For now these clashes are ignored; see\n    # PR #43339 for details.\n    from torch._C._VariableFunctions import *  # type: ignore"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/92ebb04f9206882e6d312a8b91318545f43a53c2",
    "Commit message": "added check for NumberType (#44375)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/44107\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44375\n\nReviewed By: mrshenli\n\nDifferential Revision: D23906728\n\nPulled By: eellison\n\nfbshipit-source-id: 3b534e5dd3af1f5e43a7314953e64117cbe8ffe4",
    "Deleted lines": 3,
    "Added lines": 3,
    "Changed lines": 6,
    "Deleted code": "    // Add implicit conversion of int/float/bool types to tensors\n    if (kind == c10::TypeKind::IntType || kind == c10::TypeKind::BoolType ||\n        kind == c10::TypeKind::FloatType) {",
    "Added code": "    // Add implicit conversion of int/float/bool/number types to tensors\n    if (kind == c10::TypeKind::NumberType || kind == c10::TypeKind::IntType ||\n        kind == c10::TypeKind::BoolType || kind == c10::TypeKind::FloatType) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/40d6f2a02027023216607adb892d3b9c7493904c",
    "Commit message": "Update sdp_utils to check gradmode and subclassed tensors (#92323)\n\n# Summary\nFix up the grad check test to check for subclassed tensors and gradmode\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92323\nApproved by: https://github.com/soulitzer",
    "Deleted lines": 1,
    "Added lines": 7,
    "Changed lines": 8,
    "Deleted code": "  if (params.query.requires_grad() || params.key.requires_grad() || params.value.requires_grad()) {",
    "Added code": "#include <ATen/TensorSubclassLikeUtils.h>\n  bool any_tensors_are_subclass =\n      at::areAnyTensorSubclassLike({params.query, params.key, params.value});\n  const bool any_inputs_require_grad = params.query.requires_grad() ||\n      params.key.requires_grad() || params.value.requires_grad();\n  const bool gradmode_enabled = at::GradMode::is_enabled();\n  if ((any_inputs_require_grad && gradmode_enabled) || any_tensors_are_subclass) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fe6aa0844466e5dd2669092eac5edde153108b28",
    "Commit message": "[PyTorch] IValue(const c10::Scalar&) improvements\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72551\n\nDon't delegate to `operator=` for construction. Catch hypothetical addition of a new Scalar type via debug assertion rather than checking in prod.\n\nDifferential Revision: [D34090560](https://our.internmc.facebook.com/intern/diff/D34090560/)\n\nApproved by: https://github.com/malfet",
    "Deleted lines": 5,
    "Added lines": 7,
    "Changed lines": 12,
    "Deleted code": "      *this = s.toDouble();\n      *this = s.toBool();\n    } else if (s.isIntegral(false)) {\n      *this = s.toLong();\n      TORCH_CHECK(false, \"Unknown type in Scalar\");",
    "Added code": "      tag = Tag::Double;\n      payload.u.as_double = s.toDouble();\n      tag = Tag::Bool;\n      payload.u.as_bool = s.toBool();\n      TORCH_INTERNAL_ASSERT_DEBUG_ONLY(s.isIntegral(false), \"Unknown type in Scalar\");\n      tag  = Tag::Int;\n      payload.u.as_int = s.toLong();"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1c5a8125798392f8d7c57e88735f43a14ae0beca",
    "Commit message": "Better type checking in disable_torch_function/dispatch\n\nA follow up fix for PR #74509\nOriginal issue: #73933\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/74720\nApproved by: https://github.com/gchanan",
    "Deleted lines": 4,
    "Added lines": 8,
    "Changed lines": 12,
    "Deleted code": "  } else if (PyList_CheckExact(args)) {\n  } else {\n  } else if (PyList_CheckExact(args)) {\n  } else {",
    "Added code": "  } else if (PyList_Check(args)) {\n  } else if (PyTuple_Check(args)) {\n  } else {\n    throw torch::TypeError(\"expected List or Tuple (got %s)\", Py_TYPE(args)->tp_name);\n  } else if (PyList_Check(args)) {\n  } else if (PyTuple_Check(args)) {\n  } else {\n    throw torch::TypeError(\"expected List or Tuple (got %s)\", Py_TYPE(args)->tp_name);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/35e7ac3fa17bc20d982ea69440d30cd9658dff25",
    "Commit message": "Fix bug in singleCheckErrors (#71706)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71706\n\nThis fixes a bug in singleCheckErrors introduced by #69437 (thanks\nLezcano for the catch). Checking existence of a substring in a larger\nstring is done with (name.find(text) != name.npos) but we omitted the\nsecond half of the check.\n\nTest Plan: - Code reading; I guess there are no tests for this in CI\n\nReviewed By: mikaylagawarecki\n\nDifferential Revision: D33742822\n\nPulled By: zou3519\n\nfbshipit-source-id: a12017bb12b941282704bd2110e8632f02c24b04\n(cherry picked from commit afb5a04a44232671961d554139e5e19ee711fcab)",
    "Deleted lines": 3,
    "Added lines": 3,
    "Changed lines": 6,
    "Deleted code": "    } else if (name.find(\"eig\") || name.find(\"syevd\")) {\n    } else if (name.find(\"lstsq\")) {\n    } else if (name.find(\"lu_factor\")) {",
    "Added code": "    } else if (name.find(\"eig\") != name.npos || name.find(\"syevd\") != name.npos) {\n    } else if (name.find(\"lstsq\") != name.npos) {\n    } else if (name.find(\"lu_factor\") != name.npos) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0f0829d88e839be1e150e917aca5b1edb64752ee",
    "Commit message": "Strict bound check for SequenceFunctor\n\nSummary:\nThis exhibits the problem in NMT training where some out of bound data seems to\nhave silently written over bound, and causing random segfaults elsewhere in the\ncode. This itself does not solve the problem, but will trigger us to then fix the out\nof bound issues.\n\nDifferential Revision: D5832646\n\nfbshipit-source-id: 5eb259e4584e5341ef3f19362f98f0a9554e9aec",
    "Deleted lines": 4,
    "Added lines": 6,
    "Changed lines": 10,
    "Deleted code": "  explicit SequenceFunctor(const int* sl) : sl(sl) {}\n    return j >= sl[i];\n  const int* sl;\n        SequenceFunctor(sequence_lengths->data<int>()),",
    "Added code": "  explicit SequenceFunctor(const int* sl, const size_t len) : sl_(sl), len_(len) {}\n    CAFFE_ENFORCE(i < len_, \"Out of bound.\");\n    return j >= sl_[i];\n  const int* sl_;\n  const size_t len_;\n        SequenceFunctor(sequence_lengths->data<int>(), sequence_lengths->size()),"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a7cc6531399300f999a404718827e2a94c115aaf",
    "Commit message": "cmake: handle CUDA 9.1 in GCC version check\n\nSummary:\nGCC version check is currently being skipped when using the\nnewly released CUDA 9.1.\n\nThis will also handle other CUDA 9.x minor releases if any,\nreducing our work if there are such releases like 9.2. This\nassumes that the next major CUDA version will be 10.0,\nneeding adjustment only after such major version is\nreleased.\nCloses https://github.com/caffe2/caffe2/pull/1658\n\nDifferential Revision: D6659000\n\nPulled By: pietern\n\nfbshipit-source-id: 79291b5da9d4e8b4f2c7ac82fe2b1e7939438bc9",
    "Deleted lines": 3,
    "Added lines": 4,
    "Changed lines": 7,
    "Deleted code": "    # CUDA 9.0 requires GCC version <= 6\n    if (CUDA_VERSION VERSION_EQUAL 9.0)\n          \"CUDA 9.0 is not compatible with GCC version >= 7. \"",
    "Added code": "    # CUDA 9.x requires GCC version <= 6\n    if ((CUDA_VERSION VERSION_EQUAL   9.0) OR\n        (CUDA_VERSION VERSION_GREATER 9.0  AND CUDA_VERSION VERSION_LESS 10.0))\n          \"CUDA ${CUDA_VERSION} is not compatible with GCC version >= 7. \""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/218f4506fdcde69e3f8f2f2b2b51fefd996c577b",
    "Commit message": "Fix CUDA check for gcc > 5.\n\nSummary:\nIn response to https://github.com/caffe2/caffe2/pull/504 , this PR modifies the gcc compiler check for CUDA slightly. All ABI since [gcc-3](https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html) are compatible with eachother. The check from https://github.com/caffe2/caffe2/pull/504 forced the 'regular' CXX / CC compiler to be set to gcc < 6 but this is not required.\n\nAccording to the documentation for [FindCUDA](https://cmake.org/cmake/help/v3.0/module/FindCUDA.html), `CUDA_HOST_COMPILER` is set to `CMAKE_C_COMPILER` by default. This PR checks if `CMAKE_C_COMPILER` is too new for CUDA 8 and whether `CUDA_HOST_COMPILER` is set to `CMAKE_C_COMPILER`. It also modifies the message slightly.\nCloses https://github.com/caffe2/caffe2/pull/525\n\nDifferential Revision: D5590749\n\nPulled By: Yangqing\n\nfbshipit-source-id: 89f9ea7aecc787d6b74bf794da8aea82fc547ec1",
    "Deleted lines": 6,
    "Added lines": 5,
    "Changed lines": 11,
    "Deleted code": "    if (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND\n        NOT CMAKE_CXX_COMPILER_VERSION VERSION_LESS 6.0)\n        \"Use the following options to use another version (for example): \\n\"\n        \"  -DCMAKE_CXX_COMPILER=/usr/bin/g++-5\\n\"\n        \"  -DCMAKE_C_COMPILER=/usr/bin/gcc-5\\n\"\n        \"  -DCUDA_HOST_COMPILER:FILEPATH=/usr/bin/gcc-5\\n\")",
    "Added code": "    if (CMAKE_C_COMPILER_ID STREQUAL \"GNU\" AND\n        NOT CMAKE_C_COMPILER_VERSION VERSION_LESS 6.0 AND\n        CUDA_HOST_COMPILER STREQUAL CMAKE_C_COMPILER)\n        \"Use the following option to use another version (for example): \\n\"\n        \"  -DCUDA_HOST_COMPILER=/usr/bin/gcc-5\\n\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3f5dc95b57496c4ea938be381efcdc2ea92bb4cc",
    "Commit message": "fix device check in op bench (#29918)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29918\n\nSome of the tests don't specify `device` in the input configs so filter by device won't work for them. This diff fixes that issue.\n\nTest Plan:\n```\nbuck run mode/opt //caffe2/benchmarks/operator_benchmark/pt:qpool_test -- --iterations 1 --device cpu\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: QAdaptiveAvgPool2dBenchmark\n# Mode: Eager\n# Name: QAdaptiveAvgPool2dBenchmark_N4_C3_input_size(224,224)_output_size(112,112)_contigTrue_dtypetorch.qint32\n# Input: N: 4, C: 3, input_size: (224, 224), output_size: (112, 112), contig: True, dtype: torch.qint32\nForward Execution Time (us) : 2891.172\n\nReviewed By: hl475\n\nDifferential Revision: D18535766\n\nfbshipit-source-id: 09d89cf23b3caab6c0bc3b8a9ae55cc439b98e0f",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "                (self.args.device == 'None' or self.args.device in op_test_config.test_name)):",
    "Added code": "                (self.args.device == 'None' or 'device' not in op_test_config.test_name or\n                    self.args.device in op_test_config.test_name)):"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1c02be1b6a0f6d02d3a0ae19c13d51a3e59a55ae",
    "Commit message": "Fix AttributeError in _get_device_attr (#48406)\n\nSummary:\nIn PyTorch 1.5, when running `torch.cuda.reset_peak_memory_stats()` on a machine where `torch.cuda.is_available() is False`, I would get:\n```\nAssertionError:\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx\n```\n\nIn PyTorch 1.7, the same gets me a worse error (and a user warning about missing NVIDIA drivers if you look for it):\n```\n...\n  File \"/opt/conda/lib/python3.7/site-packages/torch/_utils.py\", line 440, in _get_device_attr\n    if device_type.lower() == \"cuda\":\nAttributeError: 'NoneType' object has no attribute 'lower'\n```\n\nThe formerly raised AssertionError is depended on by libraries like pytorch_memlab: https://github.com/Stonesjtu/pytorch_memlab/blob/ec9a72fc302981ddc3ee56d6e16694610d646c36/pytorch_memlab/line_profiler/line_profiler.py#L90\nIt would be pretty gross if pytorch_memlab had to change that to catch an AttributeError.\n\nWith this patch, we get a more sensible:\n```\n...\n  File \"/opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py\", line 209, in reset_peak_memory_stats\n    return torch._C._cuda_resetPeakMemoryStats(device)\nRuntimeError: invalid argument to reset_peak_memory_stats\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48406\n\nReviewed By: mrshenli\n\nDifferential Revision: D25205630\n\nPulled By: ngimel\n\nfbshipit-source-id: 7c505a6500d730f3a2da348020e2a7a5e1306dcb",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if device_type.lower() == \"cuda\":",
    "Added code": "    if device_type and device_type.lower() == \"cuda\":"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/29b702144bf5bb96dfd8fcbd04b6562a27ca5385",
    "Commit message": "Fix issue in s_addmm_out_sparse_dense_cpu only supporting CUDA device checking (#77018)\n\n## Motivation\nThe at::native::s_addmm_out_sparse_dense_cpu only supports the CPU tensors. But it only checks whether the tensor is on CUDA device which is not enough.\n\n## Solution\nChange the tensor device type checkging from is_cuda to !is_cpu to protect other backends than the CUDA.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/77018\nApproved by: https://github.com/IvanYashchuk",
    "Deleted lines": 4,
    "Added lines": 16,
    "Changed lines": 20,
    "Deleted code": "  TORCH_CHECK(!t.is_cuda(),  \"Expected all tensors to be on the same device. addmm expected 't' to be CPU tensor, but got CUDA tensor\");\n  TORCH_CHECK(!r.is_cuda(), \"Expected all tensors to be on the same device. addmm: expected 'out' to be CPU tensor, but got CUDA tensor\");\n  TORCH_CHECK(!sparse_.is_cuda(), \"Expected all tensors to be on the same device. addmm: expected 'mat1' to be a CPU tensor, but got a CUDA tensor\");\n  TORCH_CHECK(!dense.is_cuda(), \"Expected all tensors to be on the same device. addmm: expected 'mat2' to be a CPU tensor, but got a CUDA tensor\");",
    "Added code": "  TORCH_CHECK(\n      t.is_cpu(),\n      \"Expected all tensors to be on the same device. addmm expected 't' to be CPU tensor, but got tensor on \",\n      t.device());\n  TORCH_CHECK(\n      r.is_cpu(),\n      \"Expected all tensors to be on the same device. addmm: expected 'out' to be CPU tensor, but got tensor on \",\n      t.device());\n  TORCH_CHECK(\n      sparse_.is_cpu(),\n      \"Expected all tensors to be on the same device. addmm: expected 'mat1' to be a CPU tensor, but got tensor on \",\n      t.device());\n  TORCH_CHECK(\n      dense.is_cpu(),\n      \"Expected all tensors to be on the same device. addmm: expected 'mat2' to be a CPU tensor, but got tensor on \",\n      t.device());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bbc7c79b20e67da450dd9b7de70cc6b68e656714",
    "Commit message": "add device checks for sparse csr (#97520)\n\nFixes #95373\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97520\nApproved by: https://github.com/cpuhrsch",
    "Deleted lines": 0,
    "Added lines": 12,
    "Changed lines": 12,
    "Deleted code": "",
    "Added code": "    TORCH_CHECK(\n      self.is_cuda(),\n      \"add: expected 'self' to be CUDA tensor, but got tensor on device: \",\n      self.device());\n    TORCH_CHECK(\n      other.is_cuda(),\n      \"add: expected 'other' to be CUDA tensor, but got tensor on device: \",\n      other.device());\n    TORCH_CHECK(\n      out.is_cuda(),\n      \"add: expected 'out' to be CUDA tensor, but got tensor on device: \",\n      out.device());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/faa7eb81c634492b70fcc0327622bb0aa812cacd",
    "Commit message": "change error_message for XPU Autocast data type check (#102073)\n\nXPU autocast supports bf16 and fp16 data types, we are going to change the error_message for that.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/102073\nApproved by: https://github.com/jgong5",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "                error_message += 'XPU Autocast only supports dtype of torch.bfloat16 currently.'",
    "Added code": "                error_message += 'XPU Autocast only supports dtypes of torch.bfloat16 and torch.float16 currently.'"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/48a49b2683ffa21eb1b472e503c129c043c18f87",
    "Commit message": "use more informative error message for ConstandPad2d/3d (#104762)\n\nFixes #104508\n\nAs discussed in #104508, the current error message for `torch.nn.ConstantPad2d` and `torch.nn.ConstantPad3d` is misleading, this PR fixes the problem.\nThe fixed error message is shown below:\nFor `torch.nn.ConstantPad2d`:\n<img width=\"619\" alt=\"image\" src=\"https://github.com/pytorch/pytorch/assets/6964699/dd15f42a-b6ad-4c6d-aa41-f26d08144189\">\nFor `torch.nn.ConstantPad3d`:\n<img width=\"630\" alt=\"image\" src=\"https://github.com/pytorch/pytorch/assets/6964699/ac99b80f-73c1-4d7f-b9a1-74bf45ee4c21\">\n\ncc:\n@mikaylagawarecki Please help me check this PR, thanks!\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104762\nApproved by: https://github.com/mikaylagawarecki",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "  TORCH_CHECK(static_cast<int64_t>(pad.size()) <= input_dim * 2, \"Padding length too large\");",
    "Added code": "  TORCH_CHECK(static_cast<int64_t>(pad.size()) <= input_dim * 2,\n              \"Padding length should be less than or equal to two times the input dimension but got padding length \", pad.size(), \" and input of dimension \", input_dim);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bdb3fb49bc7a73cbcc5c865dda8be32888deb584",
    "Commit message": "[c10d] Fix the check message of unsupported collectives ops. (#101775)\n\n1. Fix the check message of unsupported collectives ops.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/101775\nApproved by: https://github.com/H-Huang",
    "Deleted lines": 20,
    "Added lines": 20,
    "Changed lines": 40,
    "Deleted code": "        c10::str(\"Backend \", getBackendName(), \"does not implement startCoalescing\"));\n        c10::str(\"Backend \", getBackendName(), \"does not implement endCoalescing\"));\n        c10::str(\"Backend \", getBackendName(), \"does not support broadcast\"));\n        c10::str(\"Backend \", getBackendName(), \"does not support allreduce\"));\n            \"does not support allreduce_coalesced\"));\n        c10::str(\"Backend \", getBackendName(), \"does not support reduce\"));\n        c10::str(\"Backend \", getBackendName(), \"does not support allgather\"));\n            \"Backend \", getBackendName(), \"does not support _allgather_base\"));\n            \"does not support allgather_coalesced\"));\n            \"does not support allgather_into_tensor_coalesced\"));\n        c10::str(\"Backend \", getBackendName(), \"does not support gather\"));\n        c10::str(\"Backend \", getBackendName(), \"does not support scatter\"));\n            \"Backend \", getBackendName(), \"does not support reduce_scatter\"));\n            \"does not support _reduce_scatter_base\"));\n            \"Backend \", getBackendName(), \"does not support alltoall_base\"));\n        c10::str(\"Backend \", getBackendName(), \"does not support alltoall\"));\n        false, c10::str(\"Backend \", getBackendName(), \"does not support send\"));\n        false, c10::str(\"Backend \", getBackendName(), \"does not support recv\"));\n            \"Backend \", getBackendName(), \"does not support recvAnysource\"));\n        c10::str(\"Backend \", getBackendName(), \"does not support barrier\"));",
    "Added code": "        c10::str(\"Backend \", getBackendName(), \" does not implement startCoalescing\"));\n        c10::str(\"Backend \", getBackendName(), \" does not implement endCoalescing\"));\n        c10::str(\"Backend \", getBackendName(), \" does not support broadcast\"));\n        c10::str(\"Backend \", getBackendName(), \" does not support allreduce\"));\n            \" does not support allreduce_coalesced\"));\n        c10::str(\"Backend \", getBackendName(), \" does not support reduce\"));\n        c10::str(\"Backend \", getBackendName(), \" does not support allgather\"));\n            \"Backend \", getBackendName(), \" does not support _allgather_base\"));\n            \" does not support allgather_coalesced\"));\n            \" does not support allgather_into_tensor_coalesced\"));\n        c10::str(\"Backend \", getBackendName(), \" does not support gather\"));\n        c10::str(\"Backend \", getBackendName(), \" does not support scatter\"));\n            \"Backend \", getBackendName(), \" does not support reduce_scatter\"));\n            \" does not support _reduce_scatter_base\"));\n            \"Backend \", getBackendName(), \" does not support alltoall_base\"));\n        c10::str(\"Backend \", getBackendName(), \" does not support alltoall\"));\n        false, c10::str(\"Backend \", getBackendName(), \" does not support send\"));\n        false, c10::str(\"Backend \", getBackendName(), \" does not support recv\"));\n            \"Backend \", getBackendName(), \" does not support recvAnysource\"));\n        c10::str(\"Backend \", getBackendName(), \" does not support barrier\"));"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4ab1588d9919bc1a62219a5c2393e0784ddaae70",
    "Commit message": "Enhance error message for dependency check (#96642)\n\nIf python development library is missing when building pytorch from source, cmake will raise the error like:\n```\nCMake Error at cmake/Dependencies.cmake:1079 (if):\n  if given arguments:\n\n    \"VERSION_LESS\" \"3\"\n\n  Unknown arguments specified\n```\n\nit's quite a misleading information that user would consider it's a syntax error or cmake version problem.\n\nThis PR add a check to ensure `PYTHONLIBS_VERSION_STRING` exist before using.\n\nRelated  #87993\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/96642\nApproved by: https://github.com/kit1980",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "  if(NOT PYTHONLIBS_VERSION_STRING)\n    message(FATAL_ERROR\n      \"Python development libraries could not be found.\")\n  endif()\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/577e90ae9bf257040acb68da3626d9a64d07bf7a",
    "Commit message": "Improve error message for missing ops (#80005)\n\nThe current error message is ill formed. Example\n\nerror: Following ops cannot be found. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/aten::to.prim_dtype ()\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/80005\nApproved by: https://github.com/cccclai",
    "Deleted lines": 2,
    "Added lines": 3,
    "Changed lines": 5,
    "Deleted code": "        \"Following ops cannot be found. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/\",\n        c10::Join(\", \", unsupported_op_names));",
    "Added code": "        \"Following ops cannot be found: [\",\n        c10::Join(\", \", unsupported_op_names),\n        \"]. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/c/mobile/\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/22044c6f7cbdafdd340714bbe220b621e1927826",
    "Commit message": "Use TORCH_CHECK instead of AT_ASSERT in torch::cuda::gather() (#27456)\n\nSummary:\nThe error message produced by AT_ASSERT() in gather() encouraged users to file a bug report (\"please report a bug to PyTorch...\"). The assertion should be a regular argument check since it can be triggered by passing tensors with different dimensionality, e.g. `torch.cuda.comm.gather([torch.rand(1, device='cuda'), torch.rand(1, 1, device='cuda')])`.\n\nSee: https://github.com/pytorch/pytorch/issues/26400\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27456\n\nDifferential Revision: D19300270\n\nPulled By: ezyang\n\nfbshipit-source-id: ec87d225e23445020b377521e0daccceb4748215",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "    AT_ASSERT(tensor.ndimension() == static_cast<int64_t>(expected_size.size()));",
    "Added code": "    TORCH_CHECK(\n        tensor.ndimension() == static_cast<int64_t>(expected_size.size()),\n        \"Gather input tensors must have the same number of dimensions: got \",\n        tensor.ndimension(), \", but expected \", expected_size.size());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a5aceba61fc290236af955e2c4fff4513476c9ff",
    "Commit message": "[static-runtime] a pass through checks throwing exceptions (#95983)\n\nSummary: increasing verbosity where possible\n\nTest Plan: CI\n\nDifferential Revision: D43761268\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95983\nApproved by: https://github.com/davidberard98",
    "Deleted lines": 7,
    "Added lines": 49,
    "Changed lines": 56,
    "Deleted code": "      TORCH_CHECK(v->type()->kind() != FunctionType::Kind);\n        kwargs.empty(), \"Schema is not available, but BlockRunner got kwargs.\");\n    TORCH_CHECK(total_num_inputs == block_info_.num_inputs());\n      \"Static runtime got too many arguments\");\n        false, \"Static runtime is missing required kwarg \", schema_arg.name());\n  TORCH_CHECK(consumed_kwargs == kwargs.size());\n  TORCH_CHECK(num_outputs() == node->outputs().size());",
    "Added code": "      TORCH_CHECK(\n          v->type()->kind() != FunctionType::Kind,\n          \"got \",\n          typeKindToString(v->type()->kind()),\n          \" instead of \",\n          typeKindToString(FunctionType::Kind));\n        kwargs.empty(),\n        \"BlockRunner got kwargs; is_root_block: \",\n        std::to_string(is_root_block_),\n        \"schema: \",\n        schema ? schema->name() : \"(not available)\");\n    TORCH_CHECK(\n        total_num_inputs == block_info_.num_inputs(),\n        \"Block runner got \",\n        std::to_string(total_num_inputs),\n        \" inputs; \",\n        \" first_input_is_self: \",\n        std::to_string(first_input_is_self_),\n        \"; SR block expects \",\n        std::to_string(block_info_.num_inputs()),\n        \" inputs for schema \",\n        schema ? schema->name() : \"(not available)\");\n      \"Static runtime got \",\n      std::to_string(args.size()),\n      \" arguments, expects \",\n      std::to_string(schema_args.size() - 1),\n      \" for schema \",\n      schema->name());\n        false,\n        \"Static runtime is missing required kwarg \",\n        schema_arg.name(),\n        \" for schema \",\n        schema->name());\n  TORCH_CHECK(\n      consumed_kwargs == kwargs.size(),\n      \"kwargs size mismatch (consumed \",\n      std::to_string(consumed_kwargs),\n      \", expected \",\n      std::to_string(kwargs.size()),\n      \" for schema \",\n      schema->name());\n  TORCH_CHECK(\n      num_outputs() == node->outputs().size(),\n      \"Node \",\n      node->kind().toQualString(),\n      \" has \",\n      std::to_string(num_outputs()),\n      \" outputs, expected \",\n      std::to_string(node->outputs().size()));"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc0d68a1ee3800ed4024762d018f85256e80f5ad",
    "Commit message": "[JIT] Print out interface mismatch for prim::ModuleDictIndex (#47300)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47300\n\n**Summary**\nThis commit augments the module interface subtyping check that is done\nbefore the emission of the `prim::ModuleDictIndex` operator so that the\nerror message that is printed if the subtyping check fails provides more\ninformation on which methods do not match.\n\n**Test Plan**\nExisting unit tests for `prim::ModuleDictIndex`. Compilation of `ModWithWrongAnnotation` now produces this error:\n```\nAttribute module is not of annotated type __torch__.jit.test_module_containers.ModuleInterface: Method on class '__torch__.jit.test_module_containers.DoesNotImplementInterface' (1) is not compatible with interface '__torch__.jit.test_module_containers.ModuleInterface' (2)\n  (1) forward(__torch__.jit.test_module_containers.DoesNotImplementInterface self, Tensor inp) -> ((Tensor, Tensor))\n  (2) forward(InterfaceType<ModuleInterface> self, Any inp) -> (Any)\n:\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D24709538\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 6b6cb75e4b2b12b08576a5530b4b90cbcad9b6e5",
    "Deleted lines": 2,
    "Added lines": 4,
    "Changed lines": 6,
    "Deleted code": "          if (!attr_type->isSubtypeOf(type_hint)) {\n                << \" is not of annotated type \" << type_hint->annotation_str();",
    "Added code": "          std::stringstream ss;\n          if (!attr_type->isSubtypeOfExt(type_hint, &ss)) {\n                << \" is not of annotated type \" << type_hint->annotation_str()\n                << \": \" << ss.str();"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/871e240e6367f94966a3e2f9deefbfa98e314d6d",
    "Commit message": "Improved error message for interpolation (#72066)\n\nSummary:\nDescription:\n- Improved error message for CUDA interpolation with antialiasing\n\njbschlosser could you please check this PR and the wording if the error message is more clear now ? Thank.\nI'm skipping all the tests now and once we are agreed on the wording if any updates are required, I update and restart the tests to ensure nothing is broken.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72066\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D33892729\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 6249c7a1c51aa2e242f4bb8bfbe3f2abab17a8e8\n(cherry picked from commit 44eb5391cf4fed54b379e96dfa9f23ef6ab1ecfa)",
    "Deleted lines": 2,
    "Added lines": 6,
    "Changed lines": 8,
    "Deleted code": "            \"Too much shared memory required: \", shmem_size, \" vs \", sharedMemPerBlock);\n            \"Too much shared memory required: \", shmem_size, \" vs \", sharedMemPerBlock);",
    "Added code": "            \"Provided interpolation parameters can not be handled with current algorithm implementation. \",\n            \"Please reduce the scale factor. Too much shared memory required: \",\n            shmem_size, \" vs \", sharedMemPerBlock);\n            \"Provided interpolation parameters can not be handled with current algorithm implementation. \",\n            \"Please reduce the scale factor. Too much shared memory required: \",\n            shmem_size, \" vs \", sharedMemPerBlock);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c9548176965557a76526ba0db23ff5c9facd3e97",
    "Commit message": "print matrix dims in torch cuda matrix multiply error (#52780)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52780\n\ntrying to improve the error message for torch matrix multiply dimension mismatch\n\nTest Plan: check if code compiles\n\nReviewed By: akyrola\n\nDifferential Revision: D26617036\n\nfbshipit-source-id: de23e551af985a00384fb1cccd04120b9d2728b3",
    "Deleted lines": 1,
    "Added lines": 7,
    "Changed lines": 8,
    "Deleted code": "  TORCH_CHECK(mat1_sizes[1] == mat2_sizes[0], \"mat1 dim 1 must match mat2 dim 0\");",
    "Added code": "  TORCH_CHECK(\n      mat1_sizes[1] == mat2_sizes[0],\n      \"mat1 dim 1 must match mat2 dim 0\",\n      \" mat1 dim1:\",\n      mat1_sizes[1],\n      \" mat2 dim0: \",\n      mat2_sizes[0]);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7caceea6e8c352a934f8fecf21f55454007d63b2",
    "Commit message": "better error messages for Conv*d input shape checking",
    "Deleted lines": 0,
    "Added lines": 26,
    "Changed lines": 26,
    "Deleted code": "",
    "Added code": "static void check_input_shape_forward(const at::Tensor& input,\n\t\t\t\t      const at::Tensor& weight,\n\t\t\t\t      int64_t groups, bool transposed) {\n  if (!transposed) {\n    if (input.size(1) != (weight.size(1) * groups)) {\n      std::stringstream ss;\n      ss << \"Given groups=\" << groups << \", weight\" << weight.sizes()\n\t << \", so expected input\" << input.sizes() << \"  to have \"\n\t << (weight.size(1) * groups) << \" channels, but got \" << input.size(1)\n\t << \" channels instead\";\n      throw std::runtime_error(ss.str());\n    }\n  } else { // transposed\n    if (input.size(1) != weight.size(0)) {\n      std::stringstream ss;\n      ss << \"Given transposed=\" << transposed << \", weight\" << weight.sizes()\n\t << \", so expected input\" << input.sizes() << \"  to have \"\n\t << weight.size(0) << \" channels, but got \" << input.size(1)\n\t << \" channels instead\";\n      throw std::runtime_error(ss.str());\n    }\n  }\n}\n\n  check_input_shape_forward(input, weight, groups, transposed);\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/93256617c8622760181dacf03c41cc0577ac0ea6",
    "Commit message": "C++ Adam optimizer - corrected messages for check of default options (#36161)\n\nSummary:\nModified messages in the check of default options for the Adam optimizer.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36161\n\nDifferential Revision: D20920140\n\nPulled By: yf225\n\nfbshipit-source-id: e697ef1741d4dd86f7f18dc0be2c3b4bd3894d8f",
    "Deleted lines": 3,
    "Added lines": 3,
    "Changed lines": 6,
    "Deleted code": "     TORCH_CHECK(std::get<0>(betas) >= 0, \"Invalid learning rate: \", std::get<0>(betas));\n     TORCH_CHECK(std::get<1>(betas) >= 0, \"Invalid learning rate: \", std::get<1>(betas));\n     TORCH_CHECK(defaults.weight_decay() >= 0, \"Invalid learning rate: \", defaults.weight_decay());",
    "Added code": "     TORCH_CHECK(0 <= std::get<0>(betas) && std::get<0>(betas) < 1.0, \"Invalid beta parameter at index 0: \", std::get<0>(betas));\n     TORCH_CHECK(0 <= std::get<1>(betas) && std::get<1>(betas) < 1.0, \"Invalid beta parameter at index 1: \", std::get<1>(betas));\n     TORCH_CHECK(defaults.weight_decay() >= 0, \"Invalid weight_decay value: \", defaults.weight_decay());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/99f06c0cc2a907d8fbf613768356838548f1f8c0",
    "Commit message": "[BE] update errors to be more descriptive (#115443)\n\nwe call `_check_single_tensor` and `_check_tensor_list` as validation but don't print out the param types that were invalid\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115443\nApproved by: https://github.com/XilunWu",
    "Deleted lines": 5,
    "Added lines": 11,
    "Changed lines": 16,
    "Deleted code": "            f\"Invalid function argument. Expected parameter `{param_name}` to be of type torch.Tensor.\"\n    if not isinstance(param, list) or not all(\n        isinstance(p, torch.Tensor) for p in param\n    ):\n            f\"Invalid function argument. Expected parameter `{param_name}` to be of type List[torch.Tensor].\"",
    "Added code": "            f\"\"\"Invalid function argument. Expected parameter `{param_name}` of type torch.Tensor\n             but got {type(param)} instead.\"\"\"\n    if not isinstance(param, list):\n        raise TypeError(\n            f\"\"\"Invalid function argument. Expected parameter `{param_name}` of type List[torch.Tensor]\n             but got {type(param)} instead.\"\"\"\n        )\n    elif not all(isinstance(p, torch.Tensor) for p in param):\n            f\"\"\"Invalid function argument. Expected parameter `{param_name}` of type List[torch.Tensor]\n             but got {type(param)} with elements of type {[type(p) for p in param]}.\"\"\"\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9a9eadacc6ac3b734a6d607ae6f63ec1a0d1438d",
    "Commit message": "explicitly check device for grid_sampler (fixes: #8599) (#8646)",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "        if input.device != grid.device:\n            raise RuntimeError((\"input (device {}) and grid (device {}) must be on the same device\" +\n                                \"for grid_sampler\").format(input.device, grid.device))"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/86fb522acdd11f8caf5f4b0e0f3269ca515745c5",
    "Commit message": "Remove cudaMemcpy on full memory overlap (#34548)\n\nSummary:\nTensorIterator is already checking partial overlap, so there is no trivial UB, but TensorITerator allows full overlap, and it is not a bad idea to skip the memcpy in such case.\n\nfixes: https://github.com/pytorch/pytorch/issues/34525\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34548\n\nDifferential Revision: D20371643\n\nPulled By: ngimel\n\nfbshipit-source-id: ff9e2e872537010afe040204e008b2499af963ad",
    "Deleted lines": 7,
    "Added lines": 10,
    "Changed lines": 17,
    "Deleted code": "    // Perform the copy\n    AT_CUDA_CHECK(cudaMemcpyAsync(\n        iter.data_ptr(0),\n        iter.data_ptr(1),\n        numel * iter.element_size(0),\n        cudaMemcpyDeviceToDevice,\n        copy_stream));",
    "Added code": "    void *dst = iter.data_ptr(0);\n    void *src = iter.data_ptr(1);\n    size_t size = numel * iter.element_size(0);\n    if (src != dst || src_device != dst_device) {\n      // Perform the copy\n      AT_CUDA_CHECK(cudaMemcpyAsync(\n          dst, src, size,\n          cudaMemcpyDeviceToDevice,\n          copy_stream));\n    }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/157d478a30f27fd9d866c1235841721a559c8d0b",
    "Commit message": "Fix omission of shape in size check in index.\n\nSigned-off-by: Edward Z. Yang <ezyangfb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78897\n\nApproved by: https://github.com/Lezcano, https://github.com/anjali411",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "                        index[j] <= self.shape[k + j],",
    "Added code": "                        index.shape[j] == self.shape[k + j],"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c1384ef99e7a0d8a439df8972532fe4f155a5683",
    "Commit message": "Fix NDPooling gradient non-symmetric padding check.\n\nReviewed By: dutran\n\nDifferential Revision: D5436817\n\nfbshipit-source-id: 7fc325589bcd92b7964067493f3342430476126b",
    "Deleted lines": 6,
    "Added lines": 8,
    "Changed lines": 14,
    "Deleted code": "      if (pad_t() != pad_l() || pad_l() != pad_r()) {\n        CAFFE_ENFORCE(\n            legacy_pad_ == LegacyPadding::CAFFE_LEGACY_POOLING,\n            \"Cudnn pooling only supports even padding on both sides, with \"\n            \"the only exception of the caffe legacy pooling case where we \"\n            \"try to preserve backward compatibility with Caffe.\");",
    "Added code": "      for (int i = 0; i < kernel_.size(); ++i) {\n        if (pads_[i] != pads_[kernel_.size() + i]) {\n          CAFFE_ENFORCE(\n              legacy_pad_ == LegacyPadding::CAFFE_LEGACY_POOLING,\n              \"Cudnn pooling only supports even padding on both sides, with \"\n              \"the only exception of the caffe legacy pooling case where we \"\n              \"try to preserve backward compatibility with Caffe.\");\n        }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f14887a63f83b931df9fc5d841c7d3829141ff58",
    "Commit message": "check for exact shape match before loading (#8619)\n\n* check for exact shape match before loading\r\n\r\n* Use RuntimeError instead of ValueError to keep it consistent with other errors\r\n\r\n* fix lint",
    "Deleted lines": 0,
    "Added lines": 7,
    "Changed lines": 7,
    "Deleted code": "",
    "Added code": "\n                if input_param.shape != param.shape:\n                    # local shape should match the one in checkpoint\n                    error_msgs.append('Size mismatch: copying a param of {} from checkpoint, '\n                                      'where the shape is {} in current model.'\n                                      .format(param.shape, input_param.shape))\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/02e2158e754bafda46e663052c838aeb6ab6b560",
    "Commit message": "Fix for out of bounds read in mobile interpreter INTERFACE_CALL opcode handler (#110301)\n\nSummary:\nThe INTERFACE_CALL opcode for the mobile TorchScript interpreter contained an out of bounds read issue leading to memory corruption.\n\nThis change adds an explicit check that the number of inputs passed to the format method called when handling the INTERFACE_CALL opcode is a valid and within bounds of the stack.\n\nTest Plan: contbuild + OSS signals\n\nDifferential Revision: D49739450\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110301\nApproved by: https://github.com/dbort",
    "Deleted lines": 0,
    "Added lines": 9,
    "Changed lines": 9,
    "Deleted code": "",
    "Added code": "          if (inst.N == 0 || inst.N > stack.size()) {\n            TORCH_CHECK(\n                false,\n                \"INTERFACE_CALL N=\",\n                inst.N,\n                \" not in range [1, \",\n                stack.size(),\n                \"]\");\n          }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4d07428edee863e7f5920f0672957a9711a9f0b5",
    "Commit message": "Fix for out of bounds read in mobile interpreter FORMAT opcode handler (#110303)\n\nSummary:\nThe FORMAT opcode for the mobile TorchScript interpreter contained an out of bounds read issue leading to memory corruption.\n\nThis change adds an explicit check that the number of inputs passed to the format method called when handling the FORMAT opcode is a valid and within bounds of the stack.\n\nTest Plan: contbuild + OSS signals\n\nDifferential Revision: D49739095\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110303\nApproved by: https://github.com/malfet",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "  if (num_inputs == 0 || num_inputs > stack.size()) {\n    AT_ERROR(\"Invalid number of inputs for format string: \", num_inputs);\n  }\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b3ace213f240dc0f0f2a738f825f46e0d0dffca4",
    "Commit message": "Heap buffer overflow at `source_range_serialization.cpp:73` (#103969)\n\nHi! We've been fuzzing torchvision project with [sydr-fuzz](https://github.com/ispras/oss-sydr-fuzz).\nWe've found a heap buffer overflow error at `source_range_serialization.cpp:73` in pytorch project.\n\nThe error occurs because there is not check in `deserialize_source` that `text_table_` size can be less than `fnameIndex`. To prevent the error the corresponding check must be located.\n\ntorchvision version: 9d0a93eee90bf7c401b74ebf9c8be80346254f15\npytorch version: 0f1621df1a0a73956c7ce4e2f72f069e610e0137\n\nOS: Ubuntu 20.04\n\nHow to reproduce\n\n1. Build docker from [here](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/torchvision) and run the container:\n\n        sudo docker build -t oss-sydr-fuzz-torchvision .\n        sudo docker run --privileged --rm -v `pwd`:/fuzz -it oss-sydr-fuzz-torchvision /bin/bash\n\n2. Run the target on this input:  [serialization-crash.txt](https://github.com/pytorch/pytorch/files/11819901/serialization-crash.txt)\n\n        /encode_png_fuzz serialization-crash.txt\n\n3. You will see the following output:\n\n        =================================================================\n        ==13==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60200055a630 at pc 0x0000010197b7 bp 0x7ffd4cfb15f0 sp 0x7ffd4cfb15e8\n        READ of size 8 at 0x60200055a630 thread T0\n            #0 0x10197b6 in std::__shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, (__gnu_cxx::_Lock_policy)2>::get() const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1325:16\n            #1 0x10197b6 in std::__shared_ptr_access<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, (__gnu_cxx::_Lock_policy)2, false, false>::_M_get() const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1024:66\n            #2 0x10197b6 in std::__shared_ptr_access<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, (__gnu_cxx::_Lock_policy)2, false, false>::operator*() const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1011:10\n            #3 0xde888c2 in torch::jit::SourceRangeDeserializer::deserialize_source(c10::IValue const&) /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:73:16\n            #4 0xde8802b in torch::jit::SourceRangeDeserializer::deserialize(c10::IValue const&) /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:51:37\n            #5 0xde8e9c7 in torch::jit::ConcreteSourceRangeUnpickler::unpickle() /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:224:39\n            #6 0xde8fb19 in torch::jit::ConcreteSourceRangeUnpickler::findSourceRangeThatGenerated(torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:231:3\n            #7 0x10798e7 in torch::jit::Source::findSourceRangeThatGenerated(torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/frontend/source_range.cpp:144:23\n            #8 0x1079d9a in torch::jit::SourceRange::findSourceRangeThatGenerated() const /pytorch/torch/csrc/jit/frontend/source_range.h:384:26\n            #9 0x1079acd in torch::jit::SourceRange::highlight(std::ostream&) const /pytorch/torch/csrc/jit/frontend/source_range.cpp:149:32\n            #10 0x1026fe2 in torch::jit::Lexer::expected(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::Token const&) /pytorch/torch/csrc/jit/frontend/lexer.h:461:13\n            #11 0x10417d9 in torch::jit::Lexer::expected(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/frontend/lexer.h:465:5\n            #12 0x102e52c in torch::jit::Lexer::expect(int) /pytorch/torch/csrc/jit/frontend/lexer.h:471:7\n            #13 0xcee774c in torch::jit::ParserImpl::parseIdent() /pytorch/torch/csrc/jit/frontend/parser.cpp:52:16\n            #14 0xcef4ea8 in torch::jit::ParserImpl::parseBaseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:195:22\n            #15 0xcef2c1b in torch::jit::ParserImpl::parseExp(int) /pytorch/torch/csrc/jit/frontend/parser.cpp:284:16\n            #16 0xcefac6a in torch::jit::ParserImpl::parseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:262:12\n            #17 0xcefac6a in torch::jit::ParserImpl::parseSubscriptExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:403:15\n            #18 0xceff39f in torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()::operator()() const /pytorch/torch/csrc/jit/frontend/parser.cpp:354:54\n            #19 0xceff39f in torch::jit::Expr std::__invoke_impl<void, torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()&>(std::__invoke_other, torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:60:14\n            #20 0xceea935 in torch::jit::ParserImpl::parseSequence(int, int, int, std::function<void ()> const&) /pytorch/torch/csrc/jit/frontend/parser.cpp:339:7\n            #21 0xceefd69 in torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)()) /pytorch/torch/csrc/jit/frontend/parser.cpp:353:5\n            #22 0xcef895a in torch::jit::ParserImpl::parseSubscript(c10::intrusive_ptr<torch::jit::Tree, c10::detail::intrusive_target_default_null_type<torch::jit::Tree> > const&) /pytorch/torch/csrc/jit/frontend/parser.cpp:430:9\n            #23 0xcef5e5c in torch::jit::ParserImpl::parseBaseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:206:18\n            #24 0xcef2c1b in torch::jit::ParserImpl::parseExp(int) /pytorch/torch/csrc/jit/frontend/parser.cpp:284:16\n            #25 0xceeeb9d in torch::jit::ParserImpl::parseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:262:12\n            #26 0xceeeb9d in torch::jit::ParserImpl::parseExpOrExpTuple() /pytorch/torch/csrc/jit/frontend/parser.cpp:94:19\n            #27 0xcee8a36 in torch::jit::ParserImpl::parseStmt(bool) /pytorch/torch/csrc/jit/frontend/parser.cpp:612:20\n            #28 0xcee7e72 in torch::jit::ParserImpl::parseStatements(bool, bool) /pytorch/torch/csrc/jit/frontend/parser.cpp:697:23\n            #29 0xcee56f5 in torch::jit::ParserImpl::parseClass() /pytorch/torch/csrc/jit/frontend/parser.cpp:747:9\n            #30 0xcee544a in torch::jit::Parser::parseClass() /pytorch/torch/csrc/jit/frontend/parser.cpp:812:17\n            #31 0xdddbea9 in torch::jit::SourceImporterImpl::parseSourceIfNeeded(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:182:42\n            #32 0xdddadbc in torch::jit::SourceImporterImpl::findNamedType(c10::QualifiedName const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:135:3\n            #33 0xdde1d88 in torch::jit::SourceImporterImpl::resolveType(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:261:10\n            #34 0xcf2ba5f in torch::jit::ScriptTypeParser::parseTypeFromExpr(torch::jit::Expr const&) const /pytorch/torch/csrc/jit/frontend/script_type_parser.cpp:238:24\n            #35 0xcf2bec7 in torch::jit::ScriptTypeParser::parseType(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/frontend/script_type_parser.cpp:312:10\n            #36 0xddf4284 in torch::jit::SourceImporter::loadType(c10::QualifiedName const&) const /pytorch/torch/csrc/jit/serialization/import_source.cpp:786:27\n            #37 0xdd739f7 in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0::operator()(c10::QualifiedName const&) const /pytorch/torch/csrc/jit/serialization/import.cpp:146:33\n            #38 0xdd739f7 in c10::StrongTypePtr std::__invoke_impl<c10::StrongTypePtr, torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&>(std::__invoke_other, torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:60:14\n            #39 0xdd73880 in std::enable_if<is_invocable_r_v<c10::StrongTypePtr, torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&>, c10::StrongTypePtr>::type std::__invoke_r<c10::StrongTypePtr, torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&>(torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:113:9\n            #40 0xdd736d6 in std::_Function_handler<c10::StrongTypePtr (c10::QualifiedName const&), torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0>::_M_invoke(std::_Any_data const&, c10::QualifiedName const&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/std_function.h:291:9\n            #41 0xdd76349 in std::function<c10::StrongTypePtr (c10::QualifiedName const&)>::operator()(c10::QualifiedName const&) const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/std_function.h:622:14\n            #42 0xdeb9f48 in torch::jit::Unpickler::readGlobal(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/unpickler.cpp:835:9\n            #43 0xdeb012d in torch::jit::Unpickler::readInstruction() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:511:7\n            #44 0xdeae437 in torch::jit::Unpickler::run() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:251:27\n            #45 0xdeae0d2 in torch::jit::Unpickler::parse_ivalue() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:204:3\n            #46 0xddd6de3 in torch::jit::readArchiveAndTensors(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<std::function<c10::StrongTypePtr (c10::QualifiedName const&)> >, c10::optional<std::function<c10::intrusive_ptr<c10::ivalue::Object, c10::detail::intrusive_target_default_null_type<c10::ivalue::Object> > (c10::StrongTypePtr, c10::IValue)> >, c10::optional<c10::Device>, caffe2::serialize::PyTorchStreamReader&, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&), std::shared_ptr<torch::jit::DeserializationStorageContext>) /pytorch/torch/csrc/jit/serialization/import_read.cpp:53:20\n            #47 0xdd732dd in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/import.cpp:184:10\n            #48 0xdd69885 in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::deserialize(c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:287:19\n            #49 0xdd6c855 in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:438:25\n            #50 0xdd6c1c7 in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:421:10\n            #51 0xdd6dce4 in torch::jit::load(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:503:10\n            #52 0xf2d3f75 in torch::serialize::InputArchive::load_from(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>) /pytorch/torch/csrc/api/src/serialize/input-archive.cpp:97:13\n            #53 0x60509c in void torch::load<at::Tensor, char*&>(at::Tensor&, char*&) /pytorch/torch/include/torch/csrc/api/include/torch/serialize.h:107:11\n            #54 0x6036be in LLVMFuzzerTestOneInput /vision/encode_png.cc:38:5\n            #55 0x66b041 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n            #56 0x6544cc in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n            #57 0x65a61b in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n            #58 0x654222 in main /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n            #59 0x7f3d12cc7082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082) (BuildId: 1878e6b475720c7c51969e69ab2d276fae6d1dee)\n            #60 0x542cdd in _start (/encode_png_fuzz+0x542cdd)\n\n        0x60200055a630 is located 16 bytes to the right of 16-byte region [0x60200055a610,0x60200055a620)\n        allocated by thread T0 here:\n            #0 0x60057d in operator new(unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/asan/asan_new_delete.cpp:95:3\n            #1 0xde9185d in std::_Vector_base<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_M_allocate(unsigned long) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:346:20\n            #2 0xde9185d in void std::vector<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_M_realloc_insert<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >(__gnu_cxx::__normal_iterator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >*, std::vector<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > >, std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >&&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:440:33\n            #3 0xde916a1 in std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >& std::vector<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::emplace_back<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >(std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >&&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:121:4\n            #4 0xde8f445 in torch::jit::SourceRangeDeserializer::SourceRangeDeserializer(c10::IValue) /pytorch/torch/csrc/jit/serialization/source_range_serialization.h:42:19\n            #5 0xde8e141 in torch::jit::ConcreteSourceRangeUnpickler::unpickle() /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:215:28\n            #6 0xde8fb19 in torch::jit::ConcreteSourceRangeUnpickler::findSourceRangeThatGenerated(torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:231:3\n            #7 0x10798e7 in torch::jit::Source::findSourceRangeThatGenerated(torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/frontend/source_range.cpp:144:23\n            #8 0x1079d9a in torch::jit::SourceRange::findSourceRangeThatGenerated() const /pytorch/torch/csrc/jit/frontend/source_range.h:384:26\n            #9 0x1079acd in torch::jit::SourceRange::highlight(std::ostream&) const /pytorch/torch/csrc/jit/frontend/source_range.cpp:149:32\n            #10 0x1026fe2 in torch::jit::Lexer::expected(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::Token const&) /pytorch/torch/csrc/jit/frontend/lexer.h:461:13\n            #11 0x10417d9 in torch::jit::Lexer::expected(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/frontend/lexer.h:465:5\n            #12 0xcee774c in torch::jit::ParserImpl::parseIdent() /pytorch/torch/csrc/jit/frontend/parser.cpp:52:16\n            #13 0xcef4ea8 in torch::jit::ParserImpl::parseBaseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:195:22\n            #14 0xcef2c1b in torch::jit::ParserImpl::parseExp(int) /pytorch/torch/csrc/jit/frontend/parser.cpp:284:16\n            #15 0xcefac6a in torch::jit::ParserImpl::parseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:262:12\n            #16 0xcefac6a in torch::jit::ParserImpl::parseSubscriptExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:403:15\n            #17 0xceff39f in torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()::operator()() const /pytorch/torch/csrc/jit/frontend/parser.cpp:354:54\n            #18 0xceff39f in torch::jit::Expr std::__invoke_impl<void, torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()&>(std::__invoke_other, torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:60:14\n            #19 0xceea935 in torch::jit::ParserImpl::parseSequence(int, int, int, std::function<void ()> const&) /pytorch/torch/csrc/jit/frontend/parser.cpp:339:7\n            #20 0xceefd69 in torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)()) /pytorch/torch/csrc/jit/frontend/parser.cpp:353:5\n            #21 0xcef895a in torch::jit::ParserImpl::parseSubscript(c10::intrusive_ptr<torch::jit::Tree, c10::detail::intrusive_target_default_null_type<torch::jit::Tree> > const&) /pytorch/torch/csrc/jit/frontend/parser.cpp:430:9\n            #22 0xcef5e5c in torch::jit::ParserImpl::parseBaseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:206:18\n            #23 0xcef2c1b in torch::jit::ParserImpl::parseExp(int) /pytorch/torch/csrc/jit/frontend/parser.cpp:284:16\n            #24 0xceeeb9d in torch::jit::ParserImpl::parseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:262:12\n            #25 0xceeeb9d in torch::jit::ParserImpl::parseExpOrExpTuple() /pytorch/torch/csrc/jit/frontend/parser.cpp:94:19\n            #26 0xcee8a36 in torch::jit::ParserImpl::parseStmt(bool) /pytorch/torch/csrc/jit/frontend/parser.cpp:612:20\n            #27 0xcee7e72 in torch::jit::ParserImpl::parseStatements(bool, bool) /pytorch/torch/csrc/jit/frontend/parser.cpp:697:23\n            #28 0xcee56f5 in torch::jit::ParserImpl::parseClass() /pytorch/torch/csrc/jit/frontend/parser.cpp:747:9\n            #29 0xcee544a in torch::jit::Parser::parseClass() /pytorch/torch/csrc/jit/frontend/parser.cpp:812:17\n            #30 0xdddbea9 in torch::jit::SourceImporterImpl::parseSourceIfNeeded(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:182:42\n            #31 0xdddadbc in torch::jit::SourceImporterImpl::findNamedType(c10::QualifiedName const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:135:3\n            #32 0xdde1d88 in torch::jit::SourceImporterImpl::resolveType(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:261:10\n            #33 0xcf2ba5f in torch::jit::ScriptTypeParser::parseTypeFromExpr(torch::jit::Expr const&) const /pytorch/torch/csrc/jit/frontend/script_type_parser.cpp:238:24\n\n        SUMMARY: AddressSanitizer: heap-buffer-overflow /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1325:16 in std::__shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, (__gnu_cxx::_Lock_policy)2>::get() const\n        Shadow bytes around the buggy address:\n          0x0c04800a3470: fa fa 00 00 fa fa 00 00 fa fa fd fa fa fa 00 00\n          0x0c04800a3480: fa fa fd fa fa fa fd fd fa fa fd fd fa fa fd fa\n          0x0c04800a3490: fa fa fd fd fa fa 00 00 fa fa 00 00 fa fa 00 00\n          0x0c04800a34a0: fa fa fd fa fa fa fd fd fa fa fd fa fa fa 00 fa\n          0x0c04800a34b0: fa fa fd fd fa fa fd fd fa fa fd fa fa fa fd fd\n        =>0x0c04800a34c0: fa fa 00 00 fa fa[fa]fa fa fa fa fa fa fa fa fa\n          0x0c04800a34d0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n          0x0c04800a34e0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n          0x0c04800a34f0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n          0x0c04800a3500: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n          0x0c04800a3510: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n        Shadow byte legend (one shadow byte represents 8 application bytes):\n          Addressable:           00\n          Partially addressable: 01 02 03 04 05 06 07\n          Heap left redzone:       fa\n          Freed heap region:       fd\n          Stack left redzone:      f1\n          Stack mid redzone:       f2\n          Stack right redzone:     f3\n          Stack after return:      f5\n          Stack use after scope:   f8\n          Global redzone:          f9\n          Global init order:       f6\n          Poisoned by user:        f7\n          Container overflow:      fc\n          Array cookie:            ac\n          Intra object redzone:    bb\n          ASan internal:           fe\n          Left alloca redzone:     ca\n          Right alloca redzone:    cb\n        ==13==ABORTING\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103969\nApproved by: https://github.com/davidberard98",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    TORCH_CHECK(\n        (uint64_t)fnameIndex < text_table_.size(),\n        \"Text table index is out of range\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d8466964b348b6172317f70b8e52de02402bad54",
    "Commit message": "Add range check to multi margin loss target (#89008)\n\nFixes https://github.com/pytorch/pytorch/issues/88724\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89008\nApproved by: https://github.com/ngimel",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "  CUDA_KERNEL_ASSERT(target_k >= 0 && target_k < dim && \"target index is out of bounds\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c22ac14969a863a00b5ebb04a3453610c7a27713",
    "Commit message": "[Error-reporting] Set upper boundary on border element (#59311)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/59311\n\nThe diff sets the upper boundary on border element when presenting the error message. This is required in order to avoid unnecessary log contamination\n\nTest Plan: Example of log contamination: https://www.internalfb.com/fblearner/details/276849996/operator/2942475685?tab=try_27021599785797968\n\nReviewed By: d4l3k\n\nDifferential Revision: D28812745\n\nfbshipit-source-id: 4f491b9acc8cc9831d763f185022879bbbfb4c8a",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "        # upper boundary on width\n        width = max(width, 250)\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/43f810fa96a0d2c40387c8c84f710926d9ede3c1",
    "Commit message": "Add streams boundary check to `torch::cuda::scatter`` (#53057)\n\nSummary:\nAccessing elements of `std::vector` outside of its boundaries can lead to crashes/memory corruptions\n\nFixes https://github.com/pytorch/pytorch/issues/52526\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53057\n\nReviewed By: janeyx99\n\nDifferential Revision: D26736829\n\nPulled By: malfet\n\nfbshipit-source-id: 7aa13c53c8d062adfef082153809a7a724a74ee5",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "    if (streams && (*streams)[i]) {\n      if (streams && (*streams)[i]) {",
    "Added code": "    if (i < (streams ? streams->size() : 0U) && (*streams)[i]) {\n      if (i < (streams ? streams->size() : 0U) && (*streams)[i]) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f548946363ae8d2875b345b58c71b057f628cd1e",
    "Commit message": "Fix out-of-boundary access in `caffe2::StartsWith` (#36672)\n\nSummary:\n`std::mismatch( InputIt1 first1, InputIt1 last1, InputIt2 first2 )` assumes that container for `first2` iterator contains at least `last1 - first` elements, which is not the case if `prefix` is longer than `str`\nFound while running unit tests on Windows\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36672\n\nDifferential Revision: D21049407\n\nPulled By: malfet\n\nfbshipit-source-id: ad45779d47a0c6898900e0247c920829a2179f62",
    "Deleted lines": 12,
    "Added lines": 18,
    "Changed lines": 30,
    "Deleted code": "  const std::string& s1, const std::string& s2, size_t max_distance = 0);\n\nCAFFE2_API inline bool StartsWith(const std::string& str, const std::string& prefix) {\n  return std::mismatch(prefix.begin(), prefix.end(), str.begin()).first ==\nCAFFE2_API int32_t editDistanceHelper(const char* s1,\n  size_t s1_len,\n  const char* s2,\n  size_t s2_len,\n  std::vector<size_t> &current,\n  std::vector<size_t> &previous,\n  std::vector<size_t> &previous1,\n  size_t max_distance);",
    "Added code": "    const std::string& s1,\n    const std::string& s2,\n    size_t max_distance = 0);\n\nCAFFE2_API inline bool StartsWith(\n    const std::string& str,\n    const std::string& prefix) {\n  return str.length() >= prefix.length() &&\n      std::mismatch(prefix.begin(), prefix.end(), str.begin()).first ==\nCAFFE2_API int32_t editDistanceHelper(\n    const char* s1,\n    size_t s1_len,\n    const char* s2,\n    size_t s2_len,\n    std::vector<size_t>& current,\n    std::vector<size_t>& previous,\n    std::vector<size_t>& previous1,\n    size_t max_distance);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b7bb34d7625d95e5088638721dcc07c2bc5e2ade",
    "Commit message": "[MPS] Add version check (#78192)\n\nUse `instancesRespondToSelector:` to test the presence of\n`optimizationLevel` in `MPSGraphCompilationDescriptor`, which according\nto\nhttps://developer.apple.com/documentation/metalperformanceshadersgraph/mpsgraphcompilationdescriptor/3922624-optimizationlevel\nis only available on 12.3 or newer\n\nThis works around a limitations of `@available(macOS 12.3, *)` macro in\nshared libraries dynamically loaded by apps targeting older runtime.\nAnd deployment target for macos Python conda binaries is 10.14:\n```\n% otool -l `which python3`\n...\nLoad command 9\n      cmd LC_BUILD_VERSION\n  cmdsize 32\n platform 1\n    minos 10.14\n      sdk 10.14\n...\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78192\nApproved by: https://github.com/atalman, https://github.com/seemethere",
    "Deleted lines": 0,
    "Added lines": 7,
    "Changed lines": 7,
    "Deleted code": "",
    "Added code": "  // Check that MacOS 12.3+ version of MPS framework is available\n  id mpsCD = NSClassFromString(@\"MPSGraphCompilationDescriptor\");\n  if (![mpsCD instancesRespondToSelector:@selector(optimizationLevel)]) {\n    // According to https://developer.apple.com/documentation/metalperformanceshadersgraph/mpsgraphcompilationdescriptor/3922624-optimizationlevel\n    // this means we are running on older MacOS\n    return;\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b1f08e7426a56a323e6928365918093b65aa4fb6",
    "Commit message": "Call uncheckedSetDevice in ~InlineDeviceGuard only when device index are different (#35438)\n\nSummary:\nSetting device could be expensive, especially when a debugger is present. We should check the device are different before we set.\n\ncc: ptrblck\nPull Request resolved: https://github.com/pytorch/pytorch/pull/35438\n\nDifferential Revision: D20664084\n\nPulled By: ngimel\n\nfbshipit-source-id: 2440b4c9d96c41b4a19d5b1e8e1756fa40f090f0",
    "Deleted lines": 2,
    "Added lines": 17,
    "Changed lines": 19,
    "Deleted code": "    C10_CUDA_CHECK(cudaSetDevice(d.index()));\n    C10_CUDA_CHECK_WARN(cudaSetDevice(d.index()));",
    "Added code": "  c10::optional<Device> uncheckedGetDevice() const noexcept {\n    int device;\n    auto err = cudaGetDevice(&device);\n    C10_CUDA_CHECK_WARN(err);\n    if (err != cudaSuccess) {\n      return c10::nullopt;\n    }\n    return Device(DeviceType::CUDA, device);\n  }\n    Device current_device = getDevice();\n    if (current_device != d) {\n      C10_CUDA_CHECK(cudaSetDevice(d.index()));\n    }\n    auto current_device = uncheckedGetDevice();\n    if (!current_device.has_value() || current_device.value() != d) {\n      C10_CUDA_CHECK_WARN(cudaSetDevice(d.index()));\n    }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/53953316444485c8ee250022988ef87778ae1352",
    "Commit message": "Avoid GIL during exit (#116709)\n\nStacks recorded when tensors are being freed during exit could\ntry to acquire the GIL. Py_IsInitialized can be used to check if we\nare post Python exit and should not attempt to acquire the GIL.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116709\nApproved by: https://github.com/aaronenyeshi",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    if (!Py_IsInitialized()) {\n      return {};\n    }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8269c4f3d30ad950a873d900f7de0880cdd38878",
    "Commit message": "Added nullptr check for pthradpool_get_threads_count (#34087)\n\nSummary:\nWe get seg fault without this in using XNNPACK.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34087\n\nDifferential Revision: D20199787\n\nPulled By: kimishpatel\n\nfbshipit-source-id: d3d274e7bb197461632b21688820cd4c10dcd819",
    "Deleted lines": 0,
    "Added lines": 12,
    "Changed lines": 12,
    "Deleted code": "",
    "Added code": "  // The current fix only useful when XNNPACK calls pthreadpool_get_threads_count with nullptr.\n  if (threadpool == nullptr) {\n    return 1;\n  }\n  // TODO: Future fix: If we keep maintaining two different threadpools.\n  // Old C2 and new one for XNNPACK, then the we have two different pthreadpool pointer\n  // types. One is caffe2::Thredpool*, the other is pthreadpool* (pthreadpool_new_if_impl.c)\n  // XNNPACK calls pthreadpool_get_threads_count during op setup using pthreadpool*, and\n  // uses _parallelize_ interface for for actual work.\n  // While NNPACK uses caffe2::Threadpool*.\n  // Thus if pthreadpool_get_threads_count is getting called from XNNPACK we cannot\n  // reinterpret_cast it to ThreadPool. It will seg fault or worse will have unedfined behavior."
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/13121598efc7d86cb7ae6e05322bb95c1d0f16bc",
    "Commit message": "[Pytorch, sparsity] Bug fix to update requantization and zp parameters of input (#52797)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52797\n\nAlso sneaking in change to check for realloc failure for packed activation buffer\n\nFB:\nIn dynamic quantization input's quantization scale and zero point can be\ndifferent on every iterations. Thus requantization scale needs to be\nrecomputed.\n\nEarlier bug that calculated those only at op creation time results in wrong\nresults on subsequent runs.\n\nThis diff fixes that.\n\nTest Plan:\nFB:\nbuck test caffe2/torch/fb/model_optimization:sparsity_test\n\nReviewed By: z-a-f, jiatongzhou\n\nDifferential Revision: D26651968\n\nfbshipit-source-id: e5b9acef03fc45f31c43d88a175f3a64f7dbf4bd",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "      if (op->prepacked_a == NULL) {\n        pytorch_qnnp_log_error(\n            \"failed to allocate %zu bytes for packed activation buffer\",\n            (k_stride * m_stride));\n        return pytorch_qnnp_status_out_of_memory;\n      }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e8e29690ef0306da25b5f191623476001d29a18b",
    "Commit message": "Add has_debug_def() check to net's debug_def()\n\nSummary: same as title\n\nReviewed By: salexspb\n\nDifferential Revision: D6264232\n\nfbshipit-source-id: e9f499e0c8758bcb52f079521fa95973fcba441f",
    "Deleted lines": 2,
    "Added lines": 7,
    "Changed lines": 9,
    "Deleted code": "  inline const std::shared_ptr<const NetDef> debug_def() const {\n    return net_def_;",
    "Added code": "  inline const NetDef& debug_def() const {\n    CAFFE_ENFORCE(has_debug_def(), \"net_def was null!\");\n    return *net_def_;\n  }\n\n  inline bool has_debug_def() const {\n    return net_def_ != nullptr;"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/eed22921237eb4c1f4399af177ce912147a885c3",
    "Commit message": "check for null commonworld in DestroyCommonWorld\n\nSummary: Check for nullptr before closing a common world.\n\nReviewed By: pietern\n\nDifferential Revision: D5746256\n\nfbshipit-source-id: d395bf60d3b7f2c2629761d2b6fd46085683390c",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    if (OperatorBase::InputBlob(0).GetRaw() == nullptr) {\n      return true;\n    }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1f83d8eec25c7339bd3e2862baf9b389e6a738a4",
    "Commit message": "[Static Runtime] Return nullptr if the number of input args doesn't match (#58018)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58018\n\n- Add checks for the number of input args and return nullptr if it doesn't match. This is intended to make Static Runtime more robust so that op schema change is less likely to break things. Imagine that a new arg is added to an op or a new overload is added that has this added arg, SR would simply ignore this added arg. If this arg has a default value, SR would run the model with the default value and give you wrong results, which can be hard to track down.\n\nReviewed By: ajyu\n\nDifferential Revision: D28047955\n\nfbshipit-source-id: 01067059edd5cfea80c4ee121829f7733b11f601",
    "Deleted lines": 7,
    "Added lines": 104,
    "Changed lines": 111,
    "Deleted code": "      DCHECK(p_node->inputs().size() == 3);\n        // TODO: Support only 9 args once the old signature has been removed.\n        TORCH_CHECK(\n            p_node->inputs().size() == 8 || p_node->inputs().size() == 9,\n            \"Expected number of inputs is 8 or 9, but got \" +\n                std::to_string(p_node->inputs().size()));\n",
    "Added code": "  if (n->inputs().size() != 2) {\n    return nullptr;\n  }\n  if (n->inputs().size() != 5) {\n    return nullptr;\n  }\n// TODO: support\n// clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor\n  if (n->inputs().size() != 3) {\n    return nullptr;\n  }\n  if (n->inputs().size() != 2) {\n    return nullptr;\n  }\n      if (n->inputs().size() != 4) {\n        return nullptr;\n      }\n  if (n->inputs().size() != 2) {\n    return nullptr;\n  }\n  if (n->inputs().size() != 2) {\n    return nullptr;\n  }\n      if (n->inputs().size() != 2) {\n        return nullptr;\n      }\n  if (n->inputs().size() != 1) {\n    return nullptr;\n  }\n  if (n->inputs().size() != 1) {\n    return nullptr;\n  }\n      if (n->inputs().size() != 1) {\n        return nullptr;\n      }\n// TODO: fix clone\n// clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor\n  if (n->inputs().size() != 2) {\n    return nullptr;\n  }\n      if (n->inputs().size() != 9) {\n        return nullptr;\n      }\n      if (n->inputs().size() != 9) {\n        return nullptr;\n      }\n      if (n->inputs().size() != 4) {\n        return nullptr;\n      }\n  if (n->inputs().size() != 2) {\n    return nullptr;\n  }\n  if (n->inputs().size() != 2) {\n    return nullptr;\n  }\n      // Keep TORCH_CHECK here because there is no alternative for fallback\n      TORCH_CHECK(n->inputs().size() == 2);\n  if (n->inputs().size() != 2 && n->inputs().size() != 4) {\n    return nullptr;\n  }\n    if (n->inputs().size() != 3) {\n      return nullptr;\n    }\n    if (n->inputs().size() != 3) {\n      return nullptr;\n    }\n    if (n->inputs().size() != 2) {\n      return nullptr;\n    }\n    if (n->inputs().size() != 2) {\n      return nullptr;\n    }\n    if (n->inputs().size() != 2) {\n      return nullptr;\n    }\n    if (n->inputs().size() != 5) {\n      return nullptr;\n    }\n    if (n->inputs().size() != 4) {\n      return nullptr;\n    }\n      // TODO: Support only 9 args once the old signature has been removed.\n      if (n->inputs().size() != 8 && n->inputs().size() != 9) {\n        return nullptr;\n      }\n  if (n->inputs().size() != 2 && n->inputs().size() != 3) {\n    return nullptr;\n  }\n  if (n->inputs().size() != 3) {\n    return nullptr;\n  }\n      if (n->inputs().size() != 2) {\n        return nullptr;\n      }\n  if (n->inputs().size() != 3) {\n    return nullptr;\n  }\n      if (n->inputs().size() != 6) {\n        return nullptr;\n      }\n        // ignore Input(5): `bool cudnn_enable=True`\n  if (n->inputs().size() != 2) {\n    return nullptr;\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d471eaeb1d2fbc7efcde6408d7d1e513b969af25",
    "Commit message": "fix inline_container.cc inplace loading (#108573)\n\nSummary:\nbypass-github-pytorch-ci-checks\nbypass-github-export-checks\nforce-merge-on-github\n\nDifferential Revision: D48971847\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108573\nApproved by: https://github.com/wqfish",
    "Deleted lines": 1,
    "Added lines": 5,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "  std::vector<uint8_t> buffer;\n  if (buf == nullptr) {\n    buffer.resize(chunk_size);\n    buf = buffer.data();\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a5ca445f7953711bc90c111c3cad2ec87f02e74a",
    "Commit message": "Check for corrupted ivalues. (#104243)\n\nHi! We've been fuzzing torchvision project with [sydr-fuzz](https://github.com/ispras/oss-sydr-fuzz).\nWe've found a SEGV error at address 0x0 at `vector.h:163` in pytorch third-party project flatbuffers.\n\nThe error occurs because the `ivalues` field of flatbuffer module can be null, so the corresponding check must be inserted.\n\ntorchvision version: 9d0a93eee90bf7c401b74ebf9c8be80346254f15\n\npytorch version: 0f1621df1a0a73956c7ce4e2f72f069e610e0137\n\nOS: Ubuntu 20.04\n\nHow to reproduce\n\n1. Build docker from [here](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/torchvision) and run the container:\n\n        sudo docker build -t oss-sydr-fuzz-torchvision .\n        sudo docker run --privileged --rm -v `pwd`:/fuzz -it oss-sydr-fuzz-torchvision /bin/bash\n\n2. Run the target on this input:\n[malformed-module.txt](https://github.com/pytorch/pytorch/files/11879653/malformed-module.txt)\n\n        /encode_png_fuzz malformed-module.txt\n\n3. You will see the following output:\n\n        AddressSanitizer:DEADLYSIGNAL\n        =================================================================\n        ==1154==ERROR: AddressSanitizer: SEGV on unknown address 0x000000000000 (pc 0x00000d17cc61 bp 0x7ffcbe8637f0 sp 0x7ffcbe863660 T0)\n        ==1154==The signal is caused by a READ memory access.\n        ==1154==Hint: address points to the zero page.\n            #0 0xd17cc61 in flatbuffers::Vector<flatbuffers::Offset<torch::jit::mobile::serialization::IValue> >::size() const /pytorch/third_party/flatbuffers/include/flatbuffers/vector.h:163:48\n            #1 0xd17cc61 in torch::jit::(anonymous namespace)::FlatbufferLoader::parseModule(torch::jit::mobile::serialization::Module*) /pytorch/torch/csrc/jit/mobile/flatbuffer_loader.cpp:293:32\n            #2 0xd17dd23 in torch::jit::parse_and_initialize_mobile_module_for_jit(void*, unsigned long, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, std::vector<c10::IValue, std::allocator<c10::IValue> >&, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >*) /pytorch/torch/csrc/jit/mobile/flatbuffer_loader.cpp:809:29\n            #3 0xdd661b4 in torch::jit::parse_and_initialize_jit_module(std::shared_ptr<char>, unsigned long, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, c10::optional<c10::Device>) /pytorch/torch/csrc/jit/serialization/import.cpp:345:28\n            #4 0xdd6b24a in torch::jit::_load_jit_module_from_bytes(std::shared_ptr<char>, unsigned long, std::shared_ptr<torch::jit::CompilationUnit>, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:547:14\n            #5 0xdd6c6df in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:443:10\n            #6 0xdd6c1c7 in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:421:10\n            #7 0xdd6dce4 in torch::jit::load(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:503:10\n            #8 0xf2d3f75 in torch::serialize::InputArchive::load_from(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>) /pytorch/torch/csrc/api/src/serialize/input-archive.cpp:97:13\n            #9 0x60509c in void torch::load<at::Tensor, char*&>(at::Tensor&, char*&) /pytorch/torch/include/torch/csrc/api/include/torch/serialize.h:107:11\n            #10 0x6036be in LLVMFuzzerTestOneInput /vision/encode_png.cc:38:5\n            #11 0x66b041 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n            #12 0x6544cc in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n            #13 0x65a61b in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n            #14 0x654222 in main /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n            #15 0x7f0c87b9c082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082) (BuildId: 1878e6b475720c7c51969e69ab2d276fae6d1dee)\n            #16 0x542cdd in _start (/encode_png_fuzz+0x542cdd)\n\n        AddressSanitizer can not provide additional info.\n        SUMMARY: AddressSanitizer: SEGV /pytorch/third_party/flatbuffers/include/flatbuffers/vector.h:163:48 in flatbuffers::Vector<flatbuffers::Offset<torch::jit::mobile::serialization::IValue> >::size() const\n        ==1154==ABORTING\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104243\nApproved by: https://github.com/kit1980",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6cc0f1c20c2f87a6c7b0e4abd5419e5007920999",
    "Commit message": "Checking for nullptr in get_model_bytecode_version (#97149)\n\nOne-liner commit to check that the ptr is not null. Just had `test_jit` that had a segfault there.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97149\nApproved by: https://github.com/kit1980",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "  TORCH_CHECK(data != nullptr, \"Pointer to bytes is null.\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/61b9d8fccd3361f21e1f3548c2a9538b62cc7525",
    "Commit message": "[Profiler][Trivial] Add null handling to `AppendOnlyList::copy` memcpy path. (#83963)\n\nIt is apparently undefined behavior to do pointer arithmetic on nullptr. In the case of AppendOnlyList, `next_` will only be null if `end_` is also null and thus the `memcpy` path will only be triggered if `n == 0`. Nonetheless, it is UB to `memcpy(0, 0, 0)`\n\nThe extra null check is in a `C10_LIKELY` block so the extra cost should be negligible, and indeed after dusting off the component microbenchmarks there's no observable difference.\n\nDifferential Revision: [D38969443](https://our.internmc.facebook.com/intern/diff/D38969443/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/83963\nApproved by: https://github.com/slgong-fb",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "    int n = src.size();\n    if (C10_LIKELY(next_ + n <= end_)) {",
    "Added code": "    size_t n = src.size();\n    if (C10_LIKELY(next_ && (next_ + n <= end_))) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4ddf27ba48ba3313a20d3316a8929cd42436ddbc",
    "Commit message": "[op-bench] check device attribute in user inputs\n\nSummary: The device attribute in the op benchmark can only include 'cpu' or 'cuda'. So adding a check in this diff.\n\nTest Plan: buck run caffe2/benchmarks/operator_benchmark:benchmark_all_test -- --warmup_iterations 1 --iterations 1\n\nReviewed By: ngimel\n\nDifferential Revision: D22538252\n\nfbshipit-source-id: 3e5af72221fc056b8d867321ad22e35a2557b8c3",
    "Deleted lines": 4,
    "Added lines": 15,
    "Changed lines": 19,
    "Deleted code": "    if all(item not in chars_range for item in [',', '-']):\n        raise ValueError(\"The correct format for operator_range is \"\n    for item in ranges:\n        if len(item) == 1:",
    "Added code": "_supported_devices = {\"cpu\", \"cuda\"}\n    _validate(configs)\ndef _validate(configs):\n    \"\"\" Validate inputs from users.\"\"\"\n    if 'device' in configs:\n        for v in configs['device']:\n            assert(v in _supported_devices), \"Device needs to be a string.\"\n\n\n    _validate(configs)\n\n    if all(item not in chars_range for item in [',', '-']):\n        raise ValueError(\"The correct format for operator_range is \"\n    for item in ranges:\n        if len(item) == 1:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f441bb1c2088e9ce6684765a75869a72817be39d",
    "Commit message": "check error status of CUDA launch after Magma kernels (#29003)\n\nSummary:\nas part of https://github.com/pytorch/hub/issues/62 I found that the stack-trace of a failed kernel launch was being recorded elsewhere, even with CUDA_LAUNCH_BLOCKING=1.\n\nSo, I started debugging, and found that magma launches don't do error checking.\n\nI eventually found the issue to be that I didn't compile-in sm37 SASS into the magma binary and the failure was on `x.inverse()`, and that's somehow a problem for magma 2.5.1 (but not 2.5.0).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29003\n\nDifferential Revision: D18397358\n\nPulled By: soumith\n\nfbshipit-source-id: 04baca68eac209d7af773daddd0193697d4ab0d9",
    "Deleted lines": 0,
    "Added lines": 40,
    "Changed lines": 40,
    "Deleted code": "",
    "Added code": "  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n  AT_CUDA_CHECK(cudaGetLastError());\n AT_CUDA_CHECK(cudaGetLastError());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e24dee00d40d01bd83b7a08fbcf9cdd51a05b04b",
    "Commit message": "add kernel launch checks after each kernel launch to silence the check (#58432)\n\nSummary:\nT90898552\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58432\n\nReviewed By: r-barnes\n\nDifferential Revision: D28487446\n\nPulled By: ngimel\n\nfbshipit-source-id: 3a756ffa3cd68720e132af27cd5ae36f7fd4a2d8",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "    C10_CUDA_KERNEL_LAUNCH_CHECK();",
    "Added code": "          C10_CUDA_KERNEL_LAUNCH_CHECK();\n          C10_CUDA_KERNEL_LAUNCH_CHECK();"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c06dfd7c26102ac2436ca25609c92fa794e972ca",
    "Commit message": "[fx2trt] Check input device in TRTModule (#63893)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63893\n\nAdd a check to ensure all the inputs are on cuda device.\n\nTest Plan: CI\n\nReviewed By: kflu, houseroad\n\nDifferential Revision: D30525265\n\nfbshipit-source-id: 6e50b70fd535defc1f802d51e8bb991b2dd73741",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "            assert inputs[i].is_cuda, f\"{i}th input is not on cuda device.\""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d3de37609f2f052a7efb098ab69540458ebaaa6c",
    "Commit message": "Support fused_dropout with XPU backend (#60231)\n\nSummary:\n## Motivation\nEnable the fused dropout optimization on XPU devices.\n\n## Solution\nAdd XPU device in the fused dropout acceptable checking.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60231\n\nReviewed By: jbschlosser\n\nDifferential Revision: D29437659\n\nPulled By: ezyang\n\nfbshipit-source-id: b77245bb53d3ac93ab30a2a85994376ae5928c34",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  return input.is_cuda() && p > 0 && p < 1 && input.numel() > 0;",
    "Added code": "  return (input.is_cuda() || input.is_xpu()) && p > 0 && p < 1 && input.numel() > 0;"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7bf195f3608e0f28c30ffb6e2fecd74a1d4ee50a",
    "Commit message": "fix kernel launch check in cross kernel\n\nSummary: per title\n\nTest Plan: buck test mode/opt //caffe2/test:kernel_launch_checks -- --exact 'caffe2/test:kernel_launch_checks - test_check_cuda_launches (test_kernel_launch_checks.AlwaysCheckCudaLaunchTest)' --run-disabled\n\nReviewed By: r-barnes\n\nDifferential Revision: D29335739\n\nfbshipit-source-id: 385c66b1806886deba35f7fd83e29e0885999119",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "  C10_CUDA_KERNEL_LAUNCH_CHECK();",
    "Added code": "      C10_CUDA_KERNEL_LAUNCH_CHECK();\n      C10_CUDA_KERNEL_LAUNCH_CHECK();"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/15dbc566c57eedbd0245e786912e94586eba0fd2",
    "Commit message": "[torch][segment_reduce] Add missing cuda kernel launch check (#60114)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60114\n\nSame as title.\n\nTest Plan: Unit test (test_kernel_launch_checks.py) is passing.\n\nReviewed By: ngimel\n\nDifferential Revision: D29169538\n\nfbshipit-source-id: ba4518dcb1a4713144d92faec2bb5bdf656ff7c5",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "          C10_CUDA_KERNEL_LAUNCH_CHECK();"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/232fbd90ff6d93362120d955befeeb297179ddad",
    "Commit message": "[TorchDynamo]: fused bias for cpu convolution path (#87050)\n\nFor aten.convolution CPU path, the bias always can be fused, so this PR adds a device check: if inputs' device is CPU, we will fuse it for a good performance.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87050\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "Deleted lines": 2,
    "Added lines": 7,
    "Changed lines": 9,
    "Deleted code": "            None,  # bias handled below\n    if bias is not None:",
    "Added code": "    is_cpu = all(\n        input.get_device().type == \"cpu\"\n        for input in (x, weight, bias)\n        if input is not None\n    )\n            bias if is_cpu else None,  # For cpu path, bias can always be fused\n    if not is_cpu and bias is not None:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a8653f35de02c7fb038e3c184dda6e67a12a39e2",
    "Commit message": "One more small Perf Tweak to fill_ (#110294)\n\n# Summary\nPerf win by check which device tensors are on\n\n## Before this PR:\n``` Shell\nCPU | CPU: 1.3328152848407626\nGPU | GPU: 6.614773320034146\nCPU | GPU: 29.027153505012393\nGPU | CPU: 17.22372299991548\n```\n## After this PR\n``` Shell\nCPU | CPU: 1.4241038949694484\nGPU | GPU: 7.060713530518115\nCPU | GPU: 15.149936103262007\nGPU | CPU: 5.774620908778161\n```\n\n#### Repro Script\n``` Python\n    a = torch.tensor([0.2, 0.5], device=\"cpu\")\n    amax = torch.tensor(0.5, device=\"cpu\")\n    print(f\"CPU | CPU: {benchmark_torch_function_in_microseconds(torch.fill_, a, amax)}\")\n\n    a = torch.tensor([0.2, 0.5], device=\"cuda\")\n    amax = torch.tensor(0.5, device=\"cuda\")\n    print(f\"GPU | GPU: {benchmark_torch_function_in_microseconds(torch.fill_, a, amax)}\")\n\n    a = torch.tensor([0.2, 0.5], device=\"cpu\")\n    amax = torch.tensor(0.5, device=\"cuda\")\n    print(f\"CPU | GPU: {benchmark_torch_function_in_microseconds(torch.fill_, a, amax)}\")\n\n    a = torch.tensor([0.2, 0.5], device=\"cuda\")\n    amax = torch.tensor(0.5, device=\"cpu\")\n    print(f\"GPU | CPU: {benchmark_torch_function_in_microseconds(torch.fill_, a, amax)}\")\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110294\nApproved by: https://github.com/mikaylagawarecki",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "  if (self.device() != value.device()){\n    return fill_out(self, value.item());\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fd6c993eeaacda7ef6b83f59ad3474aed0d52eaf",
    "Commit message": "Add missing CUDA error check (#110368)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110368\nApproved by: https://github.com/Skylion007",
    "Deleted lines": 0,
    "Added lines": 21,
    "Changed lines": 21,
    "Deleted code": "",
    "Added code": "    if (status_ != cudaSuccess) {\n      C10_CUDA_CHECK(status_);\n    }\n    if (status_ != cudaSuccess) {\n      C10_CUDA_CHECK(status_);\n    }\n    if (status_ != cudaSuccess) {\n      C10_CUDA_CHECK(status_);\n    }\n    if (status_ != cudaSuccess) {\n      C10_CUDA_CHECK(status_);\n    }\n    if (status_ != cudaSuccess) {\n      C10_CUDA_CHECK(status_);\n    }\n    if (status_ != cudaSuccess) {\n      C10_CUDA_CHECK(status_);\n    }\n    if (status_ != cudaSuccess) {\n      C10_CUDA_CHECK(status_);\n    }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8b37821813b60a3ce2ae92e7a06057183578a450",
    "Commit message": "make balance check in DP only for cuda (#103311)\n\nFixes #103825\n1. if we want to use dp on other device ranther than \"cuda\", this balance  check will raise error, so I make the balance check only effective for `cuda`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103311\nApproved by: https://github.com/kit1980",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "        _check_balance(self.device_ids)",
    "Added code": "        if device_type == \"cuda\":\n            _check_balance(self.device_ids)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c1e51fcbfc70c089276530ee64fb626e3f7f4f2b",
    "Commit message": "[ONNX][Bench] Relax tolerance for cuda accuracy check (#114767)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114767\nApproved by: https://github.com/thiagocrepaldi\nghstack dependencies: #112179",
    "Deleted lines": 1,
    "Added lines": 5,
    "Changed lines": 6,
    "Deleted code": "            # Workaround for ONNX for non-tensor outputs",
    "Added code": "                # Workaround for ONNX for non-tensor outputs\n                # Relax tolerance for ONNX cuda\n                if current_device == \"cuda\":\n                    tolerance = 1e-2\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/59a3759d9787091e75d939de603981a6d32505c8",
    "Commit message": "Update cpp_extension.py (#101285)\n\nWhen we need to link extra libs, we should notice that 64-bit CUDA may be installed in \"lib\", not in \"lib64\".\n\n<!--\ncopilot:summary\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at 05c1ca6</samp>\n\nImprove CUDA compatibility in `torch.utils.cpp_extension` by checking for `lib64` or `lib` directory.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/101285\nApproved by: https://github.com/ezyang, https://github.com/malfet",
    "Deleted lines": 1,
    "Added lines": 7,
    "Changed lines": 8,
    "Deleted code": "            extra_ldflags.append(f'-L{_join_cuda_home(\"lib64\")}')",
    "Added code": "            extra_lib_dir = \"lib64\"\n            if (not os.path.exists(_join_cuda_home(extra_lib_dir)) and\n                    os.path.exists(_join_cuda_home(\"lib\"))):\n                # 64-bit CUDA may be installed in \"lib\"\n                # Note that it's also possible both don't exist (see _find_cuda_home) - in that case we stay with \"lib64\"\n                extra_lib_dir = \"lib\"\n            extra_ldflags.append(f'-L{_join_cuda_home(extra_lib_dir)}')"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e856a4d66bead8997a83f8714547c09fcbcdc263",
    "Commit message": "Add an env var to skip cudnn version compatibility check (#89184)\n\nskip the check by setting `PYTORCH_SKIP_CUDNN_COMPATIBILITY_CHECK=1`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89184\nApproved by: https://github.com/ngimel",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "                if os.environ.get('PYTORCH_SKIP_CUDNN_COMPATIBILITY_CHECK', '0') == '1':\n                    return True"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cf256ee268c30d4ca965b38b45467cf7f738542f",
    "Commit message": "Added tensor op check for cudnn rnns (#3409)",
    "Deleted lines": 0,
    "Added lines": 8,
    "Changed lines": 8,
    "Deleted code": "",
    "Added code": "from torch.version import cuda\nCUDNN_DEFAULT_MATH = 0\nCUDNN_TENSOR_OP_MATH = 1\n\n        if version() >= 7000 and int(cuda[0]) >= 9:\n            lib.cudnnSetRNNMatrixMathType(self, CUDNN_DEFAULT_MATH)\n            if datatype == CUDNN_DATA_HALF:\n                lib.cudnnSetRNNMatrixMathType(self, CUDNN_TENSOR_OP_MATH)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0fc110cdd19363f2eb5de68b6eeb82dadc933be0",
    "Commit message": "[CUDA graphs] Don't sync between replays for cuda driver version 11.4+ (#61063)\n\nSummary:\nThe bug in libcuda.so that required https://github.com/pytorch/pytorch/pull/57556 is fixed for libcuda.so versions >= 11.4.\n\nThis PR changes replay() to sync after each launch only if the process's in-use libcuda.so is < 11.4.\n\nWith all the \"enhanced\" and \"forward\" compatibility promises flying around, and the fact that \"driver\" sometimes means kernel-mode driver and sometimes means user-mode driver (libcuda.so), I wasn't sure if this PR's check suffices to trigger the sync iff the in-use libcuda.so is < 11.4, but Cuda people say what I wrote is reasonable.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61063\n\nReviewed By: mruberry\n\nDifferential Revision: D29600907\n\nPulled By: ngimel\n\nfbshipit-source-id: 71bf0bcbde43091e29f3812440abeb7a95d161e2",
    "Deleted lines": 5,
    "Added lines": 9,
    "Changed lines": 14,
    "Deleted code": "  // Temporary workaround for bug in libcuda.so that causes replayed graphs\n  // with certain topologies to be corrupted (kernels elided, internal syncs\n  // ignored) when replayed back to back without a sync in between.\n  // I hate to use a hard sync, but it's the only surefire workaround at the moment.\n  cudaDeviceSynchronize();",
    "Added code": "  int version;\n  AT_CUDA_CHECK(cudaDriverGetVersion(&version));\n  if (version < 11040) {\n    // Workaround for bug in libcuda.so that causes replayed graphs with\n    // certain topologies to be corrupted (kernels elided, internal syncs\n    // ignored) when replayed back to back without a sync in between.\n    // The bug is fixed in CUDA 11.4+.\n    cudaDeviceSynchronize();\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ef44faece2cd4045f58cbbac6c74842b84ac6c45",
    "Commit message": "check attribute existence in torch.legay.nn.SpatialFullConvolution in method type (#8740)\n\nSummary:\nThis is related to #5255\nWhen adding cuda support for the model, this error comes:\n``\nAttributeError: 'SpatialFullConvolution' object has no attribute 'finput'\n``\nhere is my short code for test.\nhttps://gist.github.com/kaleaht/26518c3deea5d1d3dda722fbf1f3ecdc\n\nI converted torch7's model also from here.\nhttps://github.com/art-programmer/FloorplanTransformation\nPull Request resolved: https://github.com/pytorch/pytorch/pull/8740\n\nDifferential Revision: D8872735\n\nPulled By: SsnL\n\nfbshipit-source-id: 8d97f8b59cdf4049e87be14b78c4608fd973d149",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "        if self.finput is not None:\n        if self.fgradInput is not None:",
    "Added code": "        if hasattr(self, 'finput') and self.finput is not None:\n        if hasattr(self, 'fgradInput') and self.fgradInput is not None:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5c93ca258bab7bd74a8ec94d64647e48c8ad8797",
    "Commit message": "check attribute existence in SpatialFullConvolution (#5255)",
    "Deleted lines": 4,
    "Added lines": 4,
    "Changed lines": 8,
    "Deleted code": "            if self.finput is None:\n            if self.fgradInput is None:\n            if self.finput is None:\n            if self.fgradInput is None:",
    "Added code": "            if not hasattr(self, 'finput') or self.finput is None:\n            if not hasattr(self, 'fgradInput') or self.fgradInput is None:\n            if not hasattr(self, 'finput') or self.finput is None:\n            if not hasattr(self, 'fgradInput') or self.fgradInput is None:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b31cf0ebd4ffc0e25801b4e40762266ad54721d6",
    "Commit message": "Added support for nInputDim parameter in legacy Padding class (#2645)\n\n* Added support for nInputDim parameter in Padding class\r\n\r\n* moved nInputDim to the end so as to not break backwards compatibilty\r\n\r\n* hasattr to check if nInputDim is actually set\r\n\r\n* check if nInputDim is positive before checking against input dim",
    "Deleted lines": 1,
    "Added lines": 10,
    "Changed lines": 11,
    "Deleted code": "    def __init__(self, dim, pad, value=0, index=0):",
    "Added code": "    # When nInputDim is provided, inputs larger than that value will be considered batches\n    # where the actual dim to be padded will be dimension dim + 1.\n    def __init__(self, dim, pad, value=0, index=0, nInputDim=0):\n        self.nInputDim = nInputDim\n        if hasattr(self, \"nInputDim\") and self.nInputDim > 0 and input.dim() != self.nInputDim:\n            dim = dim + 1\n\n        if hasattr(self, \"nInputDim\") and self.nInputDim > 0 and input.dim() != self.nInputDim:\n            dim = dim + 1\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c5fdcd85c7570b654eec45b6cba7cc75b0cf8f6b",
    "Commit message": "check pruned attributes before deleting (#41913)\n\nSummary:\nI copyed a pruned model after deleteing the derived tensors. In order to be able to reparameter the model, we should check the existence of the tensors here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41913\n\nReviewed By: izdeby\n\nDifferential Revision: D22703248\n\nPulled By: mrshenli\n\nfbshipit-source-id: f5274d2c634a4c9a038100d8a6e837f132eabd34",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "        delattr(module, self._tensor_name)",
    "Added code": "        if hasattr(module, self._tensor_name):\n            delattr(module, self._tensor_name)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/678c08bb55eef0c2e707a17d0cd6e50f5b9bd427",
    "Commit message": "[PG Wrapper] Small fix (#72657)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72657\n\n_ProcessGroupWrapper check needs to be gated on Gloo availability,\nthis fails when gloo is not avail_ProcessGroupWrapper check needs to be gated\non Gloo availability, this fails when gloo is not avail.\nghstack-source-id: 148837056\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D34144848\n\nfbshipit-source-id: 42a04918b968247f3259cd2cde5438e1265b04fe\n(cherry picked from commit ba5de989396cb621dffc6ef424938df494eb1be6)",
    "Deleted lines": 4,
    "Added lines": 6,
    "Changed lines": 10,
    "Deleted code": "    # It is not expected for PG to be wrapped many times, but support it just\n    # in case\n    while isinstance(pg, _ProcessGroupWrapper):\n        pg = pg.wrapped_pg",
    "Added code": "    # Gate PG wrapper check on Gloo availability.\n    if _GLOO_AVAILABLE:\n        # It is not expected for PG to be wrapped many times, but support it just\n        # in case\n        while isinstance(pg, _ProcessGroupWrapper):\n            pg = pg.wrapped_pg"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dd819138da4954eaf85ae095c860bdb094ae7321",
    "Commit message": "[pytorch vulkan] add tensor vulkan check for at::cat (#109936)\n\nSummary:\nSaw this issue when running pytorch vulkan on a LSTM model:\n\nhttps://www.internalfb.com/phabricator/paste/view/P834993118\n\nFound that we don't always to the vulkan transfer on `at::cat`\n\nTest Plan:\n(Not running the LSTM model yet. Since there are other crahses.)\n\n```\n[yipjustin@47884.od /data/sandcastle/boxes/fbsource (3fd2308f8|remote/fbcode/warm_fbcode_od_stable...)]$ LD_LIBRARY_PATH=third-party/swiftshader/lib/linux-x64/ buck run fbcode/mode/dev-nosan //xplat/caffe2:pt_vulkan_api_test_bin  -- --gtest_filter=\"*cat*\"\nBuilding: finished in 0.1 sec (100%) 330/330 jobs, 0/330 updated\n  Total time: 0.2 sec\nBUILD SUCCEEDED\nRunning main() from third-party/googletest/1.11.0/googletest/googletest/src/gtest_main.cc\nNote: Google Test filter = *cat*\n[==========] Running 43 tests from 1 test suite.\n[----------] Global test environment set-up.\n[----------] 43 tests from VulkanAPITest\n[ RUN      ] VulkanAPITest.replication_pad2d\n[       OK ] VulkanAPITest.replication_pad2d (102 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim0_invalidinputs_exceptions\n[       OK ] VulkanAPITest.cat_4d_dim0_invalidinputs_exceptions (67 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim0_samebatch_success\n[       OK ] VulkanAPITest.cat_4d_dim0_samebatch_success (111 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim0_diffbatch_success\n[       OK ] VulkanAPITest.cat_4d_dim0_diffbatch_success (76 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim0_singledepth_success\n[       OK ] VulkanAPITest.cat_4d_dim0_singledepth_success (40 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim0_singletensor_success\n[       OK ] VulkanAPITest.cat_4d_dim0_singletensor_success (7 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim0_twotensors_success\n[       OK ] VulkanAPITest.cat_4d_dim0_twotensors_success (30 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim0_negdim_success\n[       OK ] VulkanAPITest.cat_4d_dim0_negdim_success (78 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim1_negdim_success\n[       OK ] VulkanAPITest.cat_4d_dim1_negdim_success (130 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim2_negdim_success\n[       OK ] VulkanAPITest.cat_4d_dim2_negdim_success (75 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim3_negdim_success\n[       OK ] VulkanAPITest.cat_4d_dim3_negdim_success (68 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim1_texture2d_success\n[       OK ] VulkanAPITest.cat_4d_dim1_texture2d_success (2 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim1_singledepth_success\n[       OK ] VulkanAPITest.cat_4d_dim1_singledepth_success (65 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim1_singletensor_success\n[       OK ] VulkanAPITest.cat_4d_dim1_singletensor_success (8 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim1_bat1_mult4ch_success\n[       OK ] VulkanAPITest.cat_4d_dim1_bat1_mult4ch_success (9 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim1_bat2_mult4ch_success\n[       OK ] VulkanAPITest.cat_4d_dim1_bat2_mult4ch_success (18 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim1_mult4ch_mixed_success\n[       OK ] VulkanAPITest.cat_4d_dim1_mult4ch_mixed_success (60 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim2_sameheight_success\n[       OK ] VulkanAPITest.cat_4d_dim2_sameheight_success (80 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim2_diffheight_success\n[       OK ] VulkanAPITest.cat_4d_dim2_diffheight_success (69 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim2_singledepth_success\n[       OK ] VulkanAPITest.cat_4d_dim2_singledepth_success (12 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim2_invalidinputs_exceptions\n[       OK ] VulkanAPITest.cat_4d_dim2_invalidinputs_exceptions (63 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim3_invalidinputs_exceptions\n[       OK ] VulkanAPITest.cat_4d_dim3_invalidinputs_exceptions (86 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim3_samewidth_success\n[       OK ] VulkanAPITest.cat_4d_dim3_samewidth_success (117 ms)\n[ RUN      ] VulkanAPITest.cat_4d_dim3_diffwidth_success\n[       OK ] VulkanAPITest.cat_4d_dim3_diffwidth_success (72 ms)\n[ RUN      ] VulkanAPITest.cat_3d_dim0_mult4ch_success\n[       OK ] VulkanAPITest.cat_3d_dim0_mult4ch_success (12 ms)\n[ RUN      ] VulkanAPITest.cat_3d_dim0_diff_channel_success\n[       OK ] VulkanAPITest.cat_3d_dim0_diff_channel_success (28 ms)\n[ RUN      ] VulkanAPITest.cat_3d_dim0_same_channel_success\n[       OK ] VulkanAPITest.cat_3d_dim0_same_channel_success (15 ms)\n[ RUN      ] VulkanAPITest.cat_3d_dim1_diffheight_success\n[       OK ] VulkanAPITest.cat_3d_dim1_diffheight_success (21 ms)\n[ RUN      ] VulkanAPITest.cat_3d_dim1_same_height_success\n[       OK ] VulkanAPITest.cat_3d_dim1_same_height_success (10 ms)\n[ RUN      ] VulkanAPITest.cat_3d_dim2_diffwidth_success\n[       OK ] VulkanAPITest.cat_3d_dim2_diffwidth_success (21 ms)\n[ RUN      ] VulkanAPITest.cat_3d_dim2_samewidth_success\n[       OK ] VulkanAPITest.cat_3d_dim2_samewidth_success (11 ms)\n[ RUN      ] VulkanAPITest.cat_3d_dim0_negdim_success\n[       OK ] VulkanAPITest.cat_3d_dim0_negdim_success (25 ms)\n[ RUN      ] VulkanAPITest.cat_3d_dim1_negdim_success\n[       OK ] VulkanAPITest.cat_3d_dim1_negdim_success (23 ms)\n[ RUN      ] VulkanAPITest.cat_3d_dim2_negdim_success\n[       OK ] VulkanAPITest.cat_3d_dim2_negdim_success (10 ms)\n[ RUN      ] VulkanAPITest.cat_2d_dim0_same_height_success\n[       OK ] VulkanAPITest.cat_2d_dim0_same_height_success (3 ms)\n[ RUN      ] VulkanAPITest.cat_2d_dim0_diff_height_success\n[       OK ] VulkanAPITest.cat_2d_dim0_diff_height_success (2 ms)\n[ RUN      ] VulkanAPITest.cat_2d_dim1_same_width_success\n[       OK ] VulkanAPITest.cat_2d_dim1_same_width_success (3 ms)\n[ RUN      ] VulkanAPITest.cat_2d_dim1_diff_width_success\n[       OK ] VulkanAPITest.cat_2d_dim1_diff_width_success (4 ms)\n[ RUN      ] VulkanAPITest.cat_2d_dim0_negdim_success\n[       OK ] VulkanAPITest.cat_2d_dim0_negdim_success (3 ms)\n[ RUN      ] VulkanAPITest.cat_2d_dim1_negdim_success\n[       OK ] VulkanAPITest.cat_2d_dim1_negdim_success (3 ms)\n[ RUN      ] VulkanAPITest.cat_1d_dim0_same_width_success\n[       OK ] VulkanAPITest.cat_1d_dim0_same_width_success (52 ms)\n[ RUN      ] VulkanAPITest.cat_1d_dim0_diff_width_success\n[       OK ] VulkanAPITest.cat_1d_dim0_diff_width_success (0 ms)\n[ RUN      ] VulkanAPITest.cat_1d_dim0_negdim_success\n[       OK ] VulkanAPITest.cat_1d_dim0_negdim_success (0 ms)\n[----------] 43 tests from VulkanAPITest (1717 ms total)\n\n[----------] Global test environment tear-down\n[==========] 43 tests from 1 test suite ran. (1717 ms total)\n[  PASSED  ] 43 tests.\n\n  YOU HAVE 4 DISABLED TESTS\n```\n\nDifferential Revision: D49566743\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109936\nApproved by: https://github.com/SS-JIA",
    "Deleted lines": 6,
    "Added lines": 8,
    "Changed lines": 14,
    "Deleted code": "    const vTensor& v_self = convert(tensor);\n    vTensor& v_output,\n    uint32_t ndim) {\n    const vTensor& v_self = convert(tensor);\n    const vTensor& v_self = convert(tensor);\n      return cat_feature_mult4ch(materialized, v_output, ndim);",
    "Added code": "    const Tensor self = tensor.is_vulkan() ? tensor : tensor.vulkan();\n    const vTensor& v_self = convert(self);\n    vTensor& v_output) {\n    const Tensor self = tensor.is_vulkan() ? tensor : tensor.vulkan();\n    const vTensor& v_self = convert(self);\n    const Tensor self = tensor.is_vulkan() ? tensor : tensor.vulkan();\n    const vTensor& v_self = convert(self);\n      return cat_feature_mult4ch(materialized, v_output);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/db1ac4e29b0f557711190c8d49d4afb5da1844e8",
    "Commit message": "fix functional collective's allgather for gloo (#104681)\n\nSummary: We should explicitly check for the gloo backend instead of relying on the shard's device, because user might pass a GPU tensor as input and a process group gloo as the pg, and expect that should work.\n\nDifferential Revision: D47249172\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104681\nApproved by: https://github.com/rohan-varma, https://github.com/fduwjj",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if shard.is_cpu:",
    "Added code": "    if dist.get_backend(group) == dist.Backend.GLOO or shard.is_cpu:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3ef4d697df5bfdbd27dfc7a79c0679da2b87e3af",
    "Commit message": "[c10d] default backend need to check for nccl availability (#102470)\n\nAs titled, we can only initialize nccl backend when NCCL is available\nPull Request resolved: https://github.com/pytorch/pytorch/pull/102470\nApproved by: https://github.com/Skylion007, https://github.com/XilunWu",
    "Deleted lines": 4,
    "Added lines": 3,
    "Changed lines": 7,
    "Deleted code": "            self.device_backend_map = {\n                \"cpu\": Backend.GLOO,\n                \"cuda\": Backend.NCCL,\n            }",
    "Added code": "            self.device_backend_map = {\"cpu\": Backend.GLOO}\n            if is_nccl_available():\n                self.device_backend_map[\"cuda\"] = Backend.NCCL"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a24c11329a1bdfb00848b4af6dced5368622d637",
    "Commit message": "Fix out-of-place allocations\n\nSummary:\nAlso add int as a datatype and correctly check error codes on group\nstart, end\nCloses https://github.com/caffe2/caffe2/pull/1590\n\nDifferential Revision: D6524086\n\nPulled By: pietern\n\nfbshipit-source-id: 385aab6fe1bbf6b5c06fa905066bc576a733c856",
    "Deleted lines": 2,
    "Added lines": 11,
    "Changed lines": 13,
    "Deleted code": "    ncclGroupStart();\n    ncclGroupEnd();",
    "Added code": "template <>\nclass ncclTypeWrapper<int> {\n public:\n  static const ncclDataType_t type = ncclInt;\n};\n\n    auto& ctx = ex.elements[i];\n    DeviceGuard g(ctx.device);\n    CAFFE_NCCL_CHECK(ncclGroupStart());\n    CAFFE_NCCL_CHECK(ncclGroupEnd());\ntemplate class NCCL<int>;"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6bf0e3b697ce688bc8325440dea3b51fea571c3d",
    "Commit message": "[inductor] Check for BackendCompilerFailed on CI (#91634)\n\nSummary: https://github.com/pytorch/pytorch/pull/91283/ skips certain\nrandom triton failure on CI, but we need to check against the\nBackendCompilerFailed exception type.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/91634\nApproved by: https://github.com/ngimel",
    "Deleted lines": 5,
    "Added lines": 7,
    "Changed lines": 12,
    "Deleted code": "                if self.args.ci and (\n                    (\n                        isinstance(e, RuntimeError)\n                        and \"Internal Triton PTX codegen error\" in str(e)\n                    or (isinstance(e, KeyError) and \"cubin\" in str(e))",
    "Added code": "from torch._dynamo.exc import BackendCompilerFailed\n                if (\n                    self.args.ci\n                    and isinstance(e, BackendCompilerFailed)\n                    and (\n                        \"Internal Triton PTX codegen error\" in str(e)\n                        or \"cubin\" in str(e)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9bb1371cc20a14907dbc47bc98e3ac5de866e34b",
    "Commit message": "Disable RDYNAMIC check with MSVC (#62949)\n\nSummary:\nWhen testing with clang-cl, the flag is added though it is unsupported and that generates a few warnings. Tried a few alternatives like https://cmake.org/cmake/help/latest/module/CheckLinkerFlag.html, but they just don't work.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/62949\n\nReviewed By: zhouzhuojie, driazati\n\nDifferential Revision: D30359206\n\nPulled By: malfet\n\nfbshipit-source-id: 1bd27ad5772fe6757fa8c3a4bddf904f88d70b7b",
    "Deleted lines": 4,
    "Added lines": 6,
    "Changed lines": 10,
    "Deleted code": "check_cxx_compiler_flag(\"-rdynamic\" COMPILER_SUPPORTS_RDYNAMIC)\nif(${COMPILER_SUPPORTS_RDYNAMIC})\n  set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -rdynamic\")\n  set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -rdynamic\")",
    "Added code": "if(NOT MSVC)\n  check_cxx_compiler_flag(\"-rdynamic\" COMPILER_SUPPORTS_RDYNAMIC)\n  if(${COMPILER_SUPPORTS_RDYNAMIC})\n    set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -rdynamic\")\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -rdynamic\")\n  endif()"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/48e675ac7519666ed5e96d8d49c468dfc15a5d66",
    "Commit message": "fx quant: fix subtle bug in BinaryOpQuantizeHanlder logic in matching (#56294)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56294\n\nWhen matching a pattern to `BinaryOpQuantizeHandler`, we need to make\nsure we check for dtype support on the base node, instead of the current\nnode.  This is important in cases such as `add-relu` and `mul-relu`,\nwhen the current node is `relu`, but the base node is `add|mul`.\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeFx\n```\n\nThere is no good test case to check this in current logic.  Created an\nadd-relu model manually, and verified with pdb that the add node was\nbeing used to match against dtypes.\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27831070\n\nfbshipit-source-id: 3697f1328dff9fec3eb910bae49a73793ef36d63",
    "Deleted lines": 3,
    "Added lines": 16,
    "Changed lines": 19,
    "Deleted code": "                            this_node_qconfig = self.qconfig_map[node.name]\n                                    (node.target in binary_op_supported_dtypes) and\n                                    (dtypes not in binary_op_supported_dtypes[node.target])",
    "Added code": "                            # to properly check for dtype support, we need to\n                            # navigate to the base node of an add-relu or mul-relu\n                            # pattern\n                            base_node = node\n                            if (\n                                (node.op == 'call_function' and\n                                 node.target is torch.nn.functional.relu) or\n                                (node.op == 'call_module' and\n                                 isinstance(modules[node.target], torch.nn.ReLU))\n                            ):\n                                base_node = node.args[0]\n\n                            this_node_qconfig = \\\n                                self.qconfig_map[base_node.name]\n                                    (base_node.target in binary_op_supported_dtypes) and\n                                    (dtypes not in binary_op_supported_dtypes[base_node.target])"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d01f8b291d437a37ec8809a18c1bb2ebfa825285",
    "Commit message": "Fix visualize_overlap for Inductor comm reordering (#113066)\n\nThe following assumptions are not always valid and need checking:\n1. `snode.node` exists\n2. `snode.node.layout.size` exists\n3. `snode.node.layout.stride` exists\n4. `snode.node.name` exists\n\nAlso there is no guarantee that there won't be two collectives running at the same time. But it's hard to visualize the overlap in that case. So disable the visualization for that case for now.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113066\nApproved by: https://github.com/wanchaol",
    "Deleted lines": 9,
    "Added lines": 22,
    "Changed lines": 31,
    "Deleted code": "    out_tensor_info = (\n        f\" (size={snode.node.layout.size}, stride={snode.node.layout.stride})\"\n    )\n    return (\n        f\"{snode.node.__class__.__name__}{detail}{out_tensor_info} ({snode.node.name})\"\n    )\n                    \"Found two collectives running at the same time, which is unexpected. \"\n            visualize_overlap(order)\n            visualize_overlap(order)",
    "Added code": "    out_tensor_info = \"\"\n    if (\n        hasattr(snode.node, \"layout\")\n        and hasattr(snode.node.layout, \"size\")\n        and hasattr(snode.node.layout, \"stride\")\n    ):\n        out_tensor_info = (\n            f\" (size={snode.node.layout.size}, stride={snode.node.layout.stride})\"\n        )\n    node_name = \"\"\n    if hasattr(snode.node, \"name\"):\n        node_name = snode.node.name\n    return f\"{snode.node.__class__.__name__}{detail}{out_tensor_info} ({node_name})\"\n                    \"Found two collectives running at the same time. \"\n            try:\n                visualize_overlap(order)\n            except Exception as e:\n                overlap_log.debug(str(e))\n            try:\n                visualize_overlap(order)\n            except Exception as e:\n                overlap_log.debug(str(e))"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/62732bdcdb8b6112e01366d4ad1c2a50e61da1ed",
    "Commit message": "[ez][inductor][fx passes] quick fix for invalid nodes (#109234)\n\nSummary: As title.Need to check whether node is valid before fusion\n\nTest Plan: To add test\n\nDifferential Revision: D49241525\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109234\nApproved by: https://github.com/yanboliang",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "        and is_node_meta_valid(input)\n        and is_node_meta_valid(weight)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/aab55d6d0d7b958e32cfdbb69794e107cfceb6bc",
    "Commit message": "[Quant] Remove all the dequant nodes when the ref module has multi input args (#90157)\n\n**Summary**:\nWhen converting a ref module into a quant module, `_lower_static_weighted_ref_module` pass assumes the `ref_node` only has 1 input node, and only remove the first `dequant` node. We add a check in this PR to ensure this is the case for `_lower_static_weighted_ref_module` pass.\n\n**Test Plan**:\nWe only add a check in this PR, there is no new added test case.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90157\nApproved by: https://github.com/Xia-Weiwen, https://github.com/jgong5, https://github.com/jerryzh168",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "        assert(len(ref_node.args) == 1)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bae895cef0c12df5f64afa155ce5462e06f0e04a",
    "Commit message": "Issue 37819: Added check for kHIP in ATen/native/Copy.cpp (#38003)\n\nSummary:\nFixed https://github.com/pytorch/pytorch/issues/37819\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38003\n\nDifferential Revision: D21533134\n\nPulled By: mruberry\n\nfbshipit-source-id: 97490a8729171b95b103e00780e36518b9865087",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "  } else if (iter.device_type(1) == kHIP) {\n    device_type = kHIP;"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cf348bcdeecfe0b47a2245d95eaa8ef37fb7b53e",
    "Commit message": "tighten hasCUDA check",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "  int count;\n  cudaError_t err = cudaGetDeviceCount(&count);\n  if (err == cudaErrorInsufficientDriver) {\n    return false;\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/181b2481d338a24efc553378c837dcc48b656e3f",
    "Commit message": "add error checking to grid sampler (#2902)",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "  THCudaCheck(cudaGetLastError());\n  THCudaCheck(cudaGetLastError());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5054cb893485890c99af404cf81c0d5d8fef28ae",
    "Commit message": "fix torch.cat bug with boxed CPUFallback (#60993)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60993\n\nFixes https://github.com/pytorch/pytorch/issues/60902\n\nThe boxed fallback was written to assume that there was at least one tensor argument, which it used to figure out what device to move the cpu tensors to. That fails with an op like `torch.cat()`, which doesn't have any tensor arguments, but instead has a single `TensorList` argument.\n\nI also added handling to gracefully deal with the case where you have an empty list of tensors - in that case we don't know what device to move everything to, but that doesn't matter because an empty list of tensors implies that we have no tensors to move anyway.\n\nI tested it out though and noticed that `torch.cat(())` doesn't handle empty lists well anyway (erroring out in the dispatcher). I'm not sure that it's a huge issue, and not even sure that we want to fix it (default to CPU? add an extra codegen'd check into every op that only takes TensorList args?) but I'll file a separate bug for that: https://github.com/pytorch/pytorch/issues/60997\n\nI tested it by running the pytorch/xla suite after removing `cat` from `xla_native_functions.yaml`, and confirming that we don't segfault anymore.\n\nTest Plan: Imported from OSS\n\nReviewed By: asuhan\n\nDifferential Revision: D29471577\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 58c96e8d48d993785b8d15cfa846ec745a34e623",
    "Deleted lines": 5,
    "Added lines": 37,
    "Changed lines": 42,
    "Deleted code": "#include <torch/library.h>\n            auto tgt_device = tensor_args[0].device();\n                        \"but it has no implementation for the backend \\\"\", tgt_device, \"\\\". View operators don't support \",\n          auto tgt_device = tensor_args[0].device();\n          (*stack)[returns_begin + idx] = c10::IValue(returns[idx].toTensor().to(tgt_device));",
    "Added code": "#include <sstream>\n\nc10::optional<c10::Device> compute_target_device(std::vector<at::Tensor>& t_args, std::vector<c10::List<at::Tensor>> tlist_args) {\n  // Decide what device to move the output tensor(s) to.\n  // The current convention is that we use the first tensor arg to pick the device\n  // Barring that, we take the first tensor from a TensorList arg.\n  if (t_args.size() > 0) {\n    return t_args[0].device();\n  } else {\n    // We need to loop through all of the (potentially multiple) TensorList arguments\n    // In case, e.g. the first one is empty but the second is not.\n    for (auto& tens_list : tlist_args) {\n      for (const auto i : c10::irange(tens_list.size())) {\n        return tens_list.get(i).device();\n      }\n    }\n  }\n  return c10::nullopt;\n}\n\n  std::vector<c10::List<at::Tensor>> tensorlist_args;\n\n      tensorlist_args.push_back(ivalue.toTensorList());\n          c10::optional<c10::Device> tgt_device = compute_target_device(tensor_args, tensorlist_args);\n            std::stringstream dev_str;\n            if (tgt_device) {\n                dev_str << *tgt_device;\n            } else {\n                dev_str << \"<none>\";\n            }\n                        \"but it has no implementation for the backend \\\"\", dev_str.str(), \"\\\". View operators don't support \",\n\n          // We technically  might not have a target device, e.g. if you call torch.cat() with an empty list\n          // In that case, we shouldn't have any tensors to schlep across devices anyway.\n          if (tgt_device) {\n              (*stack)[returns_begin + idx] = c10::IValue(returns[idx].toTensor().to(*tgt_device));\n          }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/027c0d7f8e37e583c02b372df5331d73793c06b1",
    "Commit message": "fixed compilations on xla tensor print (#71147)\n\nSummary:\nFixes multiple compilation on xla tensor print. Please check the conversation here: https://github.com/pytorch/xla/pull/3253\n\nThis is done to avoid compilations during tensor printing. Torch performs some tensor operations like slicing to make the tensor readable. These operations result in compilations. Hence to avoid the compilations, copying the tensor to cpu before printing.\n\nexample:\n\n```\ndev = xm.xla_device()\ndef test_linear(input_shape=(8, 1024)):\n    import pdb\n    pdb.set_trace()\n    linear = torch.nn.Linear(in_features=1024, out_features=4096, bias=True).to(dev)\n    inp = torch.randn(*input_shape).to(dev)\n    output = linear(inp)\n    xm.mark_step()\n    return output\n```\nReturning from this function would have resulted in 63 compiles, since PDB prints the value of the return output. In this case it is a xla tensor.\n\nNow with the current change, there is no compilation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71147\n\nReviewed By: shunting314\n\nDifferential Revision: D33795177\n\nPulled By: wconstab\n\nfbshipit-source-id: 74b53d9a1cb7ef67f9d8b0a32064f3896be449b5\n(cherry picked from commit a9e0687fc5c9981fb55ea4dc406c283c80fa20c9)",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "    # Tensor printing performs tensor operations like slice, indexing, etc to make it in a\n    # representable format. These operations on xla/lazy tensor results in compilations. Hence,\n    # to avoid compilations, copying the tensor to cpu before printing.\n    if self.device.type == 'xla' or self.device.type == 'lazy':\n        self = self.to('cpu')\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ee91c328da5739ce03b3127cd7c542ce505212b8",
    "Commit message": "Fix cuda/cpu check on NoneType (#88854)\n\nSummary: Fix cuda/cpu check on NoneType\n\nTest Plan: sabdcastle/ github CI/CD\n\nDifferential Revision: D41203955\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/88854\nApproved by: https://github.com/drisspg, https://github.com/ngimel",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "            elif not all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):",
    "Added code": "            elif not all([(x is None or x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cdab6c8df9ff9331126f69ea59c23f06109f03d7",
    "Commit message": "[PT2E][Quant] Support specifying None for obs_or_fq_ctr in target_dtype_info (#99071)\n\nIt is cleaner for quantizer to say what does not need observation instead of\nputting fp32 observers. This diff add support for that by checking if\ntarget_dtype_info contains none for specific observers and if so skip inserting\nobservers for those.\n\nDifferential Revision: [D44971357](https://our.internmc.facebook.com/intern/diff/D44971357/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/99071\nApproved by: https://github.com/jerryzh168",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "        # if node.meta[\"target_dtype_info\"][\"weight_obs_or_fq_ctr\"] == None\n        # or node.meta[\"target_dtype_info\"][\"input_act_obs_or_fq_ctr\"] == None\n        needs_obs = needs_obs and (act_post_process_ctr is not None)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7f5737392d637a22d555a88a8546d8fc7ab31084",
    "Commit message": "[FSDP] fix: fix for fsdp exec order pre fwd record (#110138)\n\nWhen the sharding_strategy is set to SHARD_GRAD_OP and forward_prefetch=True, during direct validation run, self.is_first_iter will always be True (because training=False, iter+1 is not executed). Additionally, the _pre_forward_order_index of the first handle entering the record_pre_forward function is 0. This causes the handle to have a False result in the if condition at line 166 when entering the record_pre_forward function again (the expected value should be True because _pre_forward_order_index has actually been assigned a value). As a result, the first handle is repetitively added to handles_pre_forward_order, leading to incorrect prefetching order.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110138\nApproved by: https://github.com/awgu",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "        if not self.is_first_iter or handle._pre_forward_order_index:",
    "Added code": "        if not self.is_first_iter or handle._pre_forward_order_index is not None:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/87082bd025362a84fedcca02350963d21a950d12",
    "Commit message": "Reduce single reader check time for inline_container (#113328)\n\nDifferential Revision: D51089711\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113328\nApproved by: https://github.com/jiayisuse",
    "Deleted lines": 4,
    "Added lines": 13,
    "Changed lines": 17,
    "Deleted code": "  if(additionalReaders.empty() || n < additional_reader_size_threshold_){\n    // No additional readers or record too small, use single threaded version\n  if(additionalReaders.empty() || n < additional_reader_size_threshold_){\n    // No additional readers, use single threaded version",
    "Added code": "  if(additionalReaders.empty()){\n    // No additional readers or record too small, use single threaded version\n    return getRecord(name);\n  }\n  if(n < additional_reader_size_threshold_){\n    // Reader size too small, use single threaded version\n  if(additionalReaders.empty()){\n    // No additional readers, use single threaded version\n    return getRecord(name, dst, n);\n  }\n\n  if(n < additional_reader_size_threshold_){\n    // Reader size too small, use single threaded version"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dcac4dd58edefb6951a60266e53d8767dc9be002",
    "Commit message": "Add int32_t range check in packed_accessor32 in PyTorch TensorBase (#86085)\n\nSummary:\nAs ajtulloch suggested, we can make tensor.packed_accessor32<...>() raise an exception if tensor.numel() > std::numeric_limits<uint32_t>::max().\n\nTrade-off: run-time check overhead (one-time) when doing `packed_accessor32` accessor.\n\nDifferential Revision: D39996275\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86085\nApproved by: https://github.com/ngimel",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "    TORCH_CHECK(\n        impl_->numel() <=\n            static_cast<int64_t>(std::numeric_limits<int32_t>::max()),\n        \"numel needs to be smaller than int32_t max; otherwise, please use packed_accessor64\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/027e3b7910fade8950038fb5044a548319510600",
    "Commit message": "[Forward-fix] check if source is None when using tensor out variants (#108700)\n\nSummary: As title\n\nTest Plan: Sandcastle\n\nReviewed By: JacobSzwejbka\n\nDifferential Revision: D49029357\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108700\nApproved by: https://github.com/angelayi",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "                        kwargs[\"out\"] in tx.output.graphargs",
    "Added code": "                        kwargs[\"out\"].source\n                        and kwargs[\"out\"] in tx.output.graphargs"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/122587dcb41427f473b7833eaf254384919e06fc",
    "Commit message": "[ONNX] Improve error checking for large model export (#37798)\n\nSummary:\n* Add error message when onnx model file path is not a string.\n* Add error message when model size exceed 2GB when large model export is not turned on.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37798\n\nReviewed By: hl475\n\nDifferential Revision: D21440571\n\nPulled By: houseroad\n\nfbshipit-source-id: 054aaa25ab0cffc229f9b487a2c160623c89b741",
    "Deleted lines": 0,
    "Added lines": 12,
    "Changed lines": 12,
    "Deleted code": "",
    "Added code": "  if (use_external_data_format) {\n    TORCH_CHECK(\n        !onnx_file_path.empty(),\n        \"For large model export, f in torch.onnx.export must be a non-empty string \"\n        \"specifying the location of the model.\");\n  }\n\n  const size_t proto_size = graph_encoder.get_model_proto().ByteSizeLong();\n  TORCH_CHECK(\n      proto_size <= INT_MAX,\n      \"Exporting model exceed maximum protobuf size of 2GB. \"\n      \"Please call torch.onnx.export with use_external_data_format=True.\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/91066559a8c8e5978ed4de722317576b222267c5",
    "Commit message": "truthy check for empty string in NameScope()\n\nSummary:\nAs in name. LATTE translation team moving some code from Python 2 to 3 uncovered a case where comparison between unicode and str types leads NameScope('') to prepend a separator to the beginning of blob names. This fixes it.\n\nThank you so much to dzhulgakov for tracking down the cause of this so quickly!\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D6766866\n\nfbshipit-source-id: fbe46cff581f425ba10e8668400915ea40baab94",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    prefix = prefix + _NAMESCOPE_SEPARATOR if prefix is not '' else ''",
    "Added code": "    prefix = prefix + _NAMESCOPE_SEPARATOR if prefix else ''"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e3542d2c12d8aaaccf8a53873e480c20dc6b7338",
    "Commit message": "[PyTorch] avoid unnecessary call to empty_tensor_restride in empty() (#48211)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48211\n\nOur empty benchmark makes this call unconditionally. If\nMemoryFormat::Contiguous is indeed a common case (or if workloads are\nlikely to use a consistent-ish memory format), then I'd expect\nchecking first to be a win.\nghstack-source-id: 118224990\n\nTest Plan:\nProfiled empty benchmark with perf, saw time spent in empty_tensor_restride go down.\n\nRan framework overhead benchmarks. ~7% win on empty(), 0.5-1.5% regression on InPlace, ~2% win on OutOfPlace. Seems like both the In/Out of place ones are likely to be noise because they don't exercise empty?\n\nReviewed By: bhosmer\n\nDifferential Revision: D24914706\n\nfbshipit-source-id: 916771b335143f9b4ec9fae0d8118222ab6e8659",
    "Deleted lines": 2,
    "Added lines": 6,
    "Changed lines": 8,
    "Deleted code": "  auto memory_format = memory_format_opt.value_or(MemoryFormat::Contiguous);\n  tensor.unsafeGetTensorImpl()->empty_tensor_restride(memory_format);",
    "Added code": "  if (memory_format_opt.has_value()) {\n    // Restriding a just-created empty contiguous tensor does nothing.\n    if (*memory_format_opt != MemoryFormat::Contiguous) {\n      tensor.unsafeGetTensorImpl()->empty_tensor_restride(*memory_format_opt);\n    }\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/45d5b3248b430aca70111316accd165954464589",
    "Commit message": "Fixed C++ BatchNorm pretty_print() with optional momentum (#67335)\n\nSummary:\nSummary : Inserted a check for the momentum and print  \"None\" in case is not defined. See  https://github.com/pytorch/pytorch/issues/65143\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67335\n\nTest Plan:\nThe code below now prints `torch::nn::BatchNorm2d(128, eps=1e-05, momentum=None, affine=true, track_running_stats=true)` without generating errors.\n```\ntorch::nn::BatchNorm2d m(torch::nn::BatchNormOptions(128).momentum(c10::nullopt));\nstd::cerr << *m << \"\\n\";\n```\nFixes https://github.com/pytorch/pytorch/issues/65143\n\nReviewed By: mruberry\n\nDifferential Revision: D32067820\n\nPulled By: ngimel\n\nfbshipit-source-id: f40f9bbe090aa78e00f6c3a57deae393d946b88d",
    "Deleted lines": 1,
    "Added lines": 9,
    "Changed lines": 10,
    "Deleted code": "         << \"momentum=\" << this->options.momentum().value() << \", \"",
    "Added code": "         << \"momentum=\";\n\n  if (this->options.momentum().has_value()) {\n      stream << this->options.momentum().value();\n  } else {\n      stream << \"None\";\n  }\n\n   stream << \", \""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/666ff0ae220e1a5c406b0bc5cd43283e1b18b38e",
    "Commit message": "Update _create_c10d_store to check port value (#71863)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71863\n\nPort number is int in python, but needs to be uint16_t when called for TCPStore constructor.\n\nRelated to #67172\n\nTest Plan: Imported from OSS\n\nReviewed By: cbalioglu\n\nDifferential Revision: D33793270\n\nPulled By: H-Huang\n\nfbshipit-source-id: 89ab47ec8bd7518f9ecbf7d01871fe059b0e77b1\n(cherry picked from commit 84bff1f5bb11029ff3fcf7a04faa3b9c7b25286a)",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    # check if port is uint16_t\n    if not 0 <= port < 2**16:\n        raise ValueError(f\"port must have value from 0 to 65535 but was {port}.\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ba59d720cd5c5c81601b53d2c3397c46c1f87883",
    "Commit message": "Change error message for torch.linspace(). (#28274)\n\nSummary:\nFix for https://github.com/pytorch/pytorch/issues/25810\n\nBasically moves the error checking from the device-specific function to the native function.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28274\n\nDifferential Revision: D18032189\n\nPulled By: ezyang\n\nfbshipit-source-id: 9072b5980aa2057274e79bc7241db853bfc36f11",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "  TORCH_CHECK(steps >= 0, \"number of steps must be non-negative\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/babb28d2a3a755424f72518bc360d9f511a24463",
    "Commit message": "Change DHCECK to CAFFE_ENFORCE in softmax_with_loss_op.cc\n\nSummary:\nBased on discussion on the post in Caffe2 users. Changing DCHECK that works only in debug mode to CAFFE_ENFORCE that throws exception and is a better option.\n\nUpdate: Also correct the check for label_data >= 0, did not check for all elements previously. Moved it to inner loop.\n\nReviewed By: akyrola\n\nDifferential Revision: D5483788\n\nfbshipit-source-id: ccbff09e19e05e7036db772498f71795063c1fed",
    "Deleted lines": 5,
    "Added lines": 10,
    "Changed lines": 15,
    "Deleted code": "      CAFFE_ENFORCE(\n          label_data[i] >= 0,\n          \"Label prob seems incorrect: label prob value must be nonnegative: \",\n          label_data[i]);\n      DCHECK(std::abs(total_prob - 1.) < 1e-5f);",
    "Added code": "        CAFFE_ENFORCE(\n            label_data[i * D + j] >= 0,\n            \"Label prob seems incorrect: label prob value must be nonnegative:\",\n            \" \",\n            label_data[i * D + j]);\n      CAFFE_ENFORCE(\n          std::abs(total_prob - 1.) < 1e-5f,\n          \"Label prob seems incorrect: label prob values do not sum to 1.0: \",\n          total_prob,\n          \" vs 1.0 (+/- 1e-5)\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bc371a2cd03ce573f3ad4f7be141364136028905",
    "Commit message": "[quant][fx][fix] Add additional checks when tracing back during maybe share output observer function\n\nSummary:\nCurrently in `maybe_make_input_output_share_observers`  we trace back from a node to find the activation_post_process\nof the input node, we have internal use case which would error out during tracing back, this PR is adding a guard\nduring this process to return False early when the node doesn't have any input\n\nTest Plan:\nnot sure when this would happen, verify within the internal test case\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75650\n\nApproved by: https://github.com/vkuzo",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "                # failed to trace back since no input arg for the current node\n                if len(input_arg.args) < 1:\n                    return False"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7ddf167ba5db277e02f983a6bde2bc3f5fbe1caa",
    "Commit message": "Move the asserts in shape functions upsample_nearest_2d op. (#85801)\n\nThe assert check are moved to top and the function now returns out. This is needed by the downstream torch-mlir project to correctly determine the output type.\n\nFixes #ISSUE_NUMBER\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/85801\nApproved by: https://github.com/eellison",
    "Deleted lines": 3,
    "Added lines": 6,
    "Changed lines": 9,
    "Deleted code": "        return out\n        return out\n    assert 0, \"Either output_size or scale_factors must be presented\"",
    "Added code": "\n    if (scale_factors is None and output_size is None):\n        assert 0, \"Either output_size or scale_factors must be presented\"\n\n\n    return out"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3aeaa21eb02953a9cbc62b3e61215572fc28453e",
    "Commit message": "Revert \"Remove parent device mesh check (#118620)\"\n\nThis reverts commit 3f1f057adfcd4cef67fff9605a894cb075c02881.\n\nReverted https://github.com/pytorch/pytorch/pull/118620 on behalf of https://github.com/atalman due to broke periodic linux-focal-cuda11.8-py3.9-gcc9 ([comment](https://github.com/pytorch/pytorch/pull/118620#issuecomment-1924933878))",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "    parent_mesh = _mesh_resources.get_parent_mesh(device_mesh)\n    if parent_mesh is not None:\n        raise RuntimeError(\n            f\"Found device_mesh {device_mesh} passed in has a parent device_mesh {parent_mesh}.\",\n            \"Hybrid sharding + TP is not supported yet.\",\n        )"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/23631eee5ae484d8397769492b3ea36f9eca282d",
    "Commit message": "[C2] Fix the check of current scope in optimizer (#2316)\n\nscope.CurrentDeviceScope() can return a None type, which was not considered.",
    "Deleted lines": 3,
    "Added lines": 6,
    "Changed lines": 9,
    "Deleted code": "            if (current_scope.device_type == caffe2_pb2.CUDA\n                is_gpu_blob=(current_scope.device_type == caffe2_pb2.CUDA),\n                is_gpu_blob=(current_scope.device_type == caffe2_pb2.CUDA),",
    "Added code": "            if (current_scope is not None\n                    and current_scope.device_type == caffe2_pb2.CUDA\n                is_gpu_blob=(current_scope is not None\n                    and current_scope.device_type == caffe2_pb2.CUDA),\n                is_gpu_blob=(current_scope is not None\n                    and current_scope.device_type == caffe2_pb2.CUDA),"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5fc122bf3973504e619cd677ad4a7fc1011642cd",
    "Commit message": "Fix to #2236 - tensor.numpy() checks that no positional arguments are passed. (#3224)\n\n* tensor.numpy() checks that no arguments are passed\r\n\r\n* tensor.numpy() checks that no arguments are passed\r\n\r\n* Improve .numpy() argument checking performance",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "  THPUtils_assert(PyTuple_Size(args) == 0, \"numpy() takes no positional arguments\");\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/647154f82ac2c57769f080c41452b3e5960ab94f",
    "Commit message": "Assert tensor isn't sparse in enforce_invariants. (#18338)\n\nSummary:\nThere's no reason we can't check this, but I'm punting on implementing it for now.  But it currently segfaults, so this is an improvements.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18338\n\nDifferential Revision: D14580308\n\nPulled By: gchanan\n\nfbshipit-source-id: 44d4cafeab12e1beeb3453a2d4068d221c2e9c4f",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "      AT_ASSERTM(\n          !impl_->is_sparse(),\n          \"Sparse Tensors are supported by at::Tensor, but invariant checking isn't implemented.  Please file a bug.\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a6a433aecd0da3ac3c8d49cb36091623f1b5ec9e",
    "Commit message": "Add stack emptiness checks inside interpreter.cpp (#94298)\n\nHi!\n\nI've been fuzzing different pytorch modules, and found a few crashes inside one of them.\n\nSpecifically, I'm talking about a module for interpreting the JIT code and a function called `InterpreterState::run()`. Running this function with provided crash file results in a crash, which occurs while calling `dim()` on a `stack` with 0 elements ([line-686](https://github.com/pytorch/pytorch/blob/abc54f93145830b502400faa92bec86e05422fbd/torch/csrc/jit/runtime/interpreter.cpp#L686)). The crash itself occurs later, when std::move is called with incorrect value of type `IValue`.\n\nThe second crash is similar and occurs on [line 328](https://github.com/pytorch/pytorch/blob/abc54f93145830b502400faa92bec86e05422fbd/torch/csrc/jit/runtime/interpreter.cpp#LL328C15-L328C48), where `reg(inst.X + i - 1) = pop(stack);` is executed. The error here is the same, `Stack stack` might not contain enough elements.\n\nThe third crash occurs on [line 681](https://github.com/pytorch/pytorch/blob/abc54f93145830b502400faa92bec86e05422fbd/torch/csrc/jit/runtime/interpreter.cpp#L681). The problem here is the same as for previous crashes. There are not enough elements in the stack.\n\nIn addition to these places, there are many others (in the same function) where border checking is also missing. I am not sure what is the best way to fix these problems, however I suggest adding a boundary check inside each of these case statement.\n\nAll tests were performed on this pytorch version: [abc54f93145830b502400faa92bec86e05422fbd](https://github.com/pytorch/pytorch/tree/abc54f93145830b502400faa92bec86e05422fbd)\n\n### How to reproduce\n\n1. To reproduce the crash, use provided docker: [Dockerfile](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch)\n\n2. Build the container: `docker build -t oss-sydr-fuzz-pytorch-reproduce .`\n\n3. Copy these crash files to the current directory:\n\n    - [crash-4f18c5128c9a5a94343fcbbd543d7d6b02964471.zip](https://github.com/pytorch/pytorch/files/10674143/crash-4f18c5128c9a5a94343fcbbd543d7d6b02964471.zip)\n    - [crash-55384dd7c9689ed7b94ac6697cc43db4e0dd905a.zip](https://github.com/pytorch/pytorch/files/10674147/crash-55384dd7c9689ed7b94ac6697cc43db4e0dd905a.zip)\n    - [crash-06b6125d01c5f91fae112a1aa7dcc76d71b66576.zip](https://github.com/pytorch/pytorch/files/10674152/crash-06b6125d01c5f91fae112a1aa7dcc76d71b66576.zip)\n\n4. Run the container: ``docker run --privileged --network host -v `pwd`:/homedir --rm -it oss-sydr-fuzz-pytorch-reproduce /bin/bash``\n\n5. And execute the binary: `/jit_differential_fuzz /homedir/crash-4f18c5128c9a5a94343fcbbd543d7d6b02964471`\n\nAfter execution completes you will see this stacktrace:\n\n```asan\n=36==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x6060001657f8 at pc 0x00000060bc91 bp 0x7fff00b33380 sp 0x7fff00b33378\nREAD of size 4 at 0x6060001657f8 thread T0\n    #0 0x60bc90 in c10::IValue::IValue(c10::IValue&&) /pytorch_fuzz/torch/include/ATen/core/ivalue.h:214:43\n    #1 0xc20e7cd in torch::jit::pop(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/aten/src/ATen/core/stack.h:102:12\n    #2 0xc20e7cd in torch::jit::dim(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/mobile/promoted_prim_ops.cpp:119:20\n    #3 0xc893060 in torch::jit::InterpreterStateImpl::runImpl(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/runtime/interpreter.cpp:686:13\n    #4 0xc85c47b in torch::jit::InterpreterStateImpl::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/runtime/interpreter.cpp:1010:9\n    #5 0x600598 in runGraph(std::shared_ptr<torch::jit::Graph>, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) /jit_differential_fuzz.cc:66:38\n    #6 0x601d99 in LLVMFuzzerTestOneInput /jit_differential_fuzz.cc:107:25\n    #7 0x52ccf1 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n    #8 0x516c0c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n    #9 0x51c95b in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n    #10 0x545ef2 in main /llvm-project/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n    #11 0x7f9ec069a082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082)\n    #12 0x51152d in _start (/jit_differential_fuzz+0x51152d)\n\n0x6060001657f8 is located 8 bytes to the left of 64-byte region [0x606000165800,0x606000165840)\nallocated by thread T0 here:\n    #0 0x5fd42d in operator new(unsigned long) /llvm-project/compiler-rt/lib/asan/asan_new_delete.cpp:95:3\n    #1 0xa16ab5 in void std::vector<c10::IValue, std::allocator<c10::IValue> >::_M_realloc_insert<c10::IValue&>(__gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue, std::allocator<c10::IValue> > >, c10::IValue&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:440:33\n    #2 0xa168f1 in c10::IValue& std::vector<c10::IValue, std::allocator<c10::IValue> >::emplace_back<c10::IValue&>(c10::IValue&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:121:4\n    #3 0xc89b53c in torch::jit::InterpreterStateImpl::runImpl(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/runtime/interpreter.cpp:344:19\n    #4 0xc85c47b in torch::jit::InterpreterStateImpl::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/runtime/interpreter.cpp:1010:9\n    #5 0x600598 in runGraph(std::shared_ptr<torch::jit::Graph>, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) /jit_differential_fuzz.cc:66:38\n    #6 0x601d99 in LLVMFuzzerTestOneInput /jit_differential_fuzz.cc:107:25\n    #7 0x52ccf1 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n    #8 0x516c0c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n    #9 0x51c95b in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n    #10 0x545ef2 in main /llvm-project/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n    #11 0x7f9ec069a082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082)\n\nSUMMARY: AddressSanitizer: heap-buffer-overflow /pytorch_fuzz/torch/include/ATen/core/ivalue.h:214:43 in c10::IValue::IValue(c10::IValue&&)\nShadow bytes around the buggy address:\n  0x0c0c80024aa0: fd fd fd fd fd fd fd fa fa fa fa fa 00 00 00 00\n  0x0c0c80024ab0: 00 00 00 fa fa fa fa fa fd fd fd fd fd fd fd fd\n  0x0c0c80024ac0: fa fa fa fa fd fd fd fd fd fd fd fd fa fa fa fa\n  0x0c0c80024ad0: fd fd fd fd fd fd fd fd fa fa fa fa fd fd fd fd\n  0x0c0c80024ae0: fd fd fd fd fa fa fa fa 00 00 00 00 00 00 00 00\n=>0x0c0c80024af0: fa fa fa fa fd fd fd fd fd fd fd fd fa fa fa[fa]\n  0x0c0c80024b00: 00 00 00 00 00 00 00 00 fa fa fa fa fa fa fa fa\n  0x0c0c80024b10: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0c80024b20: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0c80024b30: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0c80024b40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\nShadow byte legend (one shadow byte represents 8 application bytes):\n  Addressable:           00\n  Partially addressable: 01 02 03 04 05 06 07\n  Heap left redzone:       fa\n  Freed heap region:       fd\n  Stack left redzone:      f1\n  Stack mid redzone:       f2\n  Stack right redzone:     f3\n  Stack after return:      f5\n  Stack use after scope:   f8\n  Global redzone:          f9\n  Global init order:       f6\n  Poisoned by user:        f7\n  Container overflow:      fc\n  Array cookie:            ac\n  Intra object redzone:    bb\n  ASan internal:           fe\n  Left alloca redzone:     ca\n  Right alloca redzone:    cb\n==36==ABORTING\n```\n\n6. Executing the remaining crashes gives similar crash reports\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94298\nApproved by: https://github.com/davidberard98",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(stack.size() >= inst.N);\n            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!stack.empty());\n            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!stack.empty());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2c9dd886afc656a8bfe5d8bbcb601ee5877cee21",
    "Commit message": "Modify torch.movedim to handle scalar as no-op (#69537)\n\nSummary:\n`torch.movedim` directly handle the case of a scalar tensor (0-dim) in input as a no-op by returning a view of the input tensor (after all the usual checks for the other parameters)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69537\n\nTest Plan:\nThis code now works fine and res1 is a view of tensor\n```\nimport torch\n\ntensor = torch.rand(torch.Size([]))\nres1 = torch.movedim(tensor, 0, 0)\n```\n\nFixes https://github.com/pytorch/pytorch/issues/69432\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33020014\n\nPulled By: albanD\n\nfbshipit-source-id: b3b2d380d70158bd3b3d6b40c073377104e09007",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "  // handle the case of scalar tensor as a no-op\n  if (self_dim == 0)\n    return self.alias();\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/52e76a305677dfaf26cd5d59bd1aa239375f833c",
    "Commit message": "fix ShardedTensor.gather when shard is empty (#110962)\n\nSummary:\ncurrent ShardedTensor.gather is not working as expectation when the shard is empty on any rank\n\nThe root cause is identified that when a sharded tensor has no placement on a specific rank, the metadata doesn't include that rank's placement which introduces KeyError in :                 ```shard_offset = shard_placement[shard. Metadata][1]```\n\nIt's fixed by adding an empty tensor check.\n\nTest Plan:\nbefore change:\n\nafter change:\n\nDifferential Revision: D50114085\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110962\nApproved by: https://github.com/wz337",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "                if src.nelement() == 0 :\n                    warnings.warn(\"Gathering a tensor with zero elements on rank \" + str(rank))\n                    return"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d23231fd8cd50e4eb657eb7c3cf102475634f9c6",
    "Commit message": "Fix upgrader codegen when constant list is 0 (#72199)\n\nSummary:\nWhen the constant list is empty, previous codegen will generate something like\n```\nstd::vector<c10::IValue>({\n\n}), // constants list,\n```\nHowever it will fail quick-check, because it includes trailing spaces. This pr will generate the following instead.\n```\nstd::vector<c10::IValue>(), // constants list,\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72199\n\nghstack-source-id: 148231023\n\nTest Plan: CI\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D33952046\n\nfbshipit-source-id: 359b8a418928c89bbeb446b44774b312c94f03bc\n(cherry picked from commit 060490f66724e418a43548c2eaffa3244e780557)",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "CONSTANTS_LIST_EMPTY = \"\"\"std::vector<c10::IValue>(), // constants list\"\"\"\n\n    if len(constants_list_part) == 0:\n        return CONSTANTS_LIST_EMPTY"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4ee179c9528c8c6aae17a01f2b0d7e8235219219",
    "Commit message": "Fix `ConstantVariable` init method if NumPy is missing (#109388)\n\nBy adding `np is not None` check before `isinstance(value, np.number)`\n\nPartially addresses https://github.com/pytorch/pytorch/issues/109387\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109388\nApproved by: https://github.com/ezyang",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "        if isinstance(value, np.number):",
    "Added code": "        if np is not None and isinstance(value, np.number):"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ba766ef39a4fff2d8856e17747393d469e409775",
    "Commit message": "Fix BN size check in eval mode (#2977)",
    "Deleted lines": 3,
    "Added lines": 4,
    "Changed lines": 7,
    "Deleted code": "    size = list(input.size())\n    if reduce(mul, size[2:], size[0]) == 1:\n        raise ValueError('Expected more than 1 value per channel, got input size {}'.format(size))",
    "Added code": "    if training:\n        size = list(input.size())\n        if reduce(mul, size[2:], size[0]) == 1:\n            raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b287cb816c1ac52165920a121c98643c08d31ff7",
    "Commit message": "inductor: make the vec_transpose's tiling stride doesn't depend on out_idx and tiling_idex (#103651)\n\nFor TIMM swin_base_patch4_window7_224 dynamic shape path, there has an accuracy issue with horizontal reduction with vec_transpose:\n```\n#pragma omp for\nfor(long i0=static_cast<long>(0L); i0<static_cast<long>(ks0); i0+=static_cast<long>(1L))\n{\n    #pragma GCC ivdep\n    for(long i1=static_cast<long>(0L); i1<static_cast<long>(3136L); i1+=static_cast<long>(16L))\n    {\n        {\n            #pragma omp declare reduction(+:at::vec::Vectorized<float>:omp_out = omp_out + omp_in) initializer(omp_priv={{0}})\n            float tmp_acc0 = 0;\n            auto tmp_acc0_vec = at::vec::Vectorized<float>(tmp_acc0);\n            for(long i2=static_cast<long>(0L); i2<static_cast<long>(128L); i2+=static_cast<long>(16L))\n            {\n                float tmp1[16*16] __attribute__ ((aligned (16)));\n                at::vec::transpose_mxn<float,16,16>(in_ptr1 + static_cast<long>(i2 + (128L*(static_cast<long>((static_cast<long>(i1) % static_cast<long>(56L))) % static_cast<long>(7L))) + (896L*(static_cast<long>(at::native::div_floor_integer(i1, 56L)) % static_cast<long>(7L))) + (6272L*(at::native::div_floor_integer((static_cast<long>(i1) % static_cast<long>(56L)), 7L))) + (50176L*(at::native::div_floor_integer(i1, 392L))) + (401408L*i0)), static_cast<long>(((-50176L)*(at::native::div_floor_integer(i1, 392L))) + ((-6272L)*(at::native::div_floor_integer((static_cast<long>(i1) % static_cast<long>(56L)), 7L))) + ((-896L)*(static_cast<long>(at::native::div_floor_integer(i1, 56L)) % static_cast<long>(7L))) + ((-128L)*(static_cast<long>((static_cast<long>(i1) % static_cast<long>(56L))) % static_cast<long>(7L))) + (128L*(static_cast<long>((static_cast<long>((1L + i1)) % static_cast<long>(56L))) % static_cast<long>(7L))) + (896L*(static_cast<long>(at::native::div_floor_integer((1L + i1), 56L)) % static_cast<long>(7L))) + (6272L*(at::native::div_floor_integer((static_cast<long>((1L + i1)) % static_cast<long>(56L)), 7L))) + (50176L*(at::native::div_floor_integer((1L + i1), 392L)))), tmp1, 16);\n                for (long i2_inner = 0; i2_inner < 16; i2_inner++)\n                {\n                    auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + static_cast<long>(i1 + (3136L*i2) + (3136L*i2_inner) + (401408L*i0)));\n                    auto tmp2 = at::vec::Vectorized<float>::loadu(tmp1 + static_cast<long>(16L*i2_inner));\n                    auto tmp3 = tmp0 + tmp2;\n                    tmp_acc0_vec = tmp_acc0_vec + tmp3;\n                }\n            }\n            tmp_acc0_vec.store(out_ptr0 + static_cast<long>(i1 + (3136L*i0)));\n        }\n    }\n}\n```\n\nThe ```transpose_mxn```'s ```ld_src``` depends on ```i1``` which is not expected. This PR will  add a check to make sure the tiling stride doesn't depend on out_idx(```i2```) and tiling_idex(```i1```)\n\nAfter this PR, the generated code will be like this:\n```\n#pragma omp for\nfor(long i0=static_cast<long>(0L); i0<static_cast<long>(ks0); i0+=static_cast<long>(1L))\n{\n    #pragma GCC ivdep\n    for(long i1=static_cast<long>(0L); i1<static_cast<long>(3136L); i1+=static_cast<long>(16L))\n    {\n        {\n            #pragma omp declare reduction(+:at::vec::Vectorized<float>:omp_out = omp_out + omp_in) initializer(omp_priv={{0}})\n            float tmp_acc0 = 0;\n            auto tmp_acc0_vec = at::vec::Vectorized<float>(tmp_acc0);\n            for(long i2=static_cast<long>(0L); i2<static_cast<long>(128L); i2+=static_cast<long>(16L))\n            {\n                for (long i2_inner = 0; i2_inner < 16; i2_inner++)\n                {\n                    auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + static_cast<long>(i1 + (3136L*i2) + (3136L*i2_inner) + (401408L*i0)));\n                    auto tmp1 = ([&]() { __at_align__ float tmpbuf[16]; for (long i1_inner = 0; i1_inner < 16; i1_inner++) tmpbuf[i1_inner] = in_ptr1[static_cast<long>(i2 + i2_inner + (128L*(static_cast<long>((static_cast<long>((i1 + i1_inner)) % static_cast<long>(56L))) % static_cast<long>(7L))) + (896L*(static_cast<long>(at::native::div_floor_integer((i1 + i1_inner), 56L)) % static_cast<long>(7L))) + (6272L*(at::native::div_floor_integer((static_cast<long>((i1 + i1_inner)) % static_cast<long>(56L)), 7L))) + (50176L*(at::native::div_floor_integer((i1 + i1_inner), 392L))) + (401408L*i0))]; return at::vec::Vectorized<float>::loadu(tmpbuf); })();\n                    auto tmp2 = tmp0 + tmp1;\n                    tmp_acc0_vec = tmp_acc0_vec + tmp2;\n                }\n            }\n            tmp_acc0_vec.store(out_ptr0 + static_cast<long>(i1 + (3136L*i0)));\n        }\n    }\n}\n```\n\nHow to reproduce this issue:\n```\npython -m torch.backends.xeon.run_cpu --node_id 0 benchmarks/dynamo/timm_models.py --accuracy --float32 -dcpu --inference -n5 --inductor --dynamic-shapes --only swin_base_patch4_window7_224\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103651\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "Deleted lines": 2,
    "Added lines": 9,
    "Changed lines": 11,
    "Deleted code": "        return stride_at(self.itervars[self.outer_idx], index) == 1 and index.has(\n            self.itervars[self.tiling_idx]",
    "Added code": "        return (\n            stride_at(self.itervars[self.outer_idx], index) == 1\n            and index.has(self.itervars[self.tiling_idx])\n            and not stride_at(self.itervars[self.tiling_idx], index).has(\n                self.itervars[self.tiling_idx]\n            )\n            and not stride_at(self.itervars[self.tiling_idx], index).has(\n                self.itervars[self.outer_idx]\n            )"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/999bae0f54108ffc5b7cf2524a02a83901554b16",
    "Commit message": "Add padding check for use_nnpack (#92238)\n\nFixes #90142\nnnp_convolution_output doesn't support the case of input padding > = kernel_size.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92238\nApproved by: https://github.com/jgong5, https://github.com/ganler",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "           (at::symint::size<T>(weight, 2) < 17) && (at::symint::size<T>(weight, 3) < 17) // NNPACK only supports kernels up to 16x16",
    "Added code": "           (at::symint::size<T>(weight, 2) < 17) && (at::symint::size<T>(weight, 3) < 17) && // NNPACK only supports kernels up to 16x16\n           (padding[0] < at::symint::size<T>(weight, 2)) && (padding[1] < at::symint::size<T>(weight, 3)) // NNPACK only supports padding < kernel_size. See https://github.com/pytorch/pytorch/issues/90142."
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ce3413549f5cc312f48c4e2a2f60c41674e26257",
    "Commit message": "check cloned observer in RNN Executor\n\nSummary: Ensure the clone() function didn't return a nullptr before attaching to an RNN operator\n\nReviewed By: salexspb\n\nDifferential Revision: D6341735\n\nfbshipit-source-id: acf89c32f8dae2fd9bc8cb1029bc00df5dbe9dbd",
    "Deleted lines": 2,
    "Added lines": 12,
    "Changed lines": 14,
    "Deleted code": "            rnn_op.op->AttachObserver(observer.second->clone());\n              rnn_op.op->AttachObserver(observer.second->clone());",
    "Added code": "            std::unique_ptr<ObserverBase<OperatorBase>> observer_copy =\n                observer.second->clone();\n            CAFFE_ENFORCE(\n                observer_copy,\n                \"Observers without clone() implemented cannot be attached to RNN using RNNExecutor.\");\n            rnn_op.op->AttachObserver(std::move(observer_copy));\n              std::unique_ptr<ObserverBase<OperatorBase>> observer_copy =\n                  observer.second->clone();\n              CAFFE_ENFORCE(\n                  observer_copy,\n                  \"Observers without clone() implemented cannot be attached to RNN using RNNExecutor.\");\n              rnn_op.op->AttachObserver(std::move(observer_copy));"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/66b04e3cb707d30c4698b269c83cb6221848f17a",
    "Commit message": "[nccl flight recorder] nullptr profiling name (#115851)\n\nSometimes profiling name can be a nullptr, which\nthrows on conversion to std::string. This adds a check.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115851\nApproved by: https://github.com/wconstab",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "        profiling_name,",
    "Added code": "        profiling_name == nullptr ? \"\" : profiling_name,"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/14c47fb211bc929ae4e505e7e13411faa2339f00",
    "Commit message": "fix invalid-null-argument UBSAN error in math_cpu.cc\n\nAdd an if statement to check if the destination buffer is not nullptr.",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "  if (A == nullptr) {\n    return;\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/eb8659fe81f3d4b061674bf149a6805cd292db8d",
    "Commit message": "pass inference accuracy check for detectron2_fcos_r_50_fpn (#108328)\n\nWe need a higher tolerance to pass the inference accuracy check for detectron2_fcos_r_50_fpn .\n\nCommand:\n```\npython benchmarks/dynamo/torchbench.py --backend inductor --bfloat16 --accuracy --only detectron2_fcos_r_50_fpn --disable-cudagraphs --inference\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108328\nApproved by: https://github.com/jansel",
    "Deleted lines": 0,
    "Added lines": 10,
    "Changed lines": 10,
    "Deleted code": "",
    "Added code": "\nREQUIRE_HIGHER_BF16_TOLERANCE = {\n    \"detectron2_fcos_r_50_fpn\",\n}\n\n\n        if self.args.bfloat16:\n            if name in REQUIRE_HIGHER_BF16_TOLERANCE:\n                return 1e-2, cosine\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c7748fc172553da66368fd0b7fea3fe5661e2dc1",
    "Commit message": "Added validation of mode parameter in AveragedModel (#65921)\n\nSummary:\nDiscussion: https://github.com/pytorch/pytorch/pull/65495#issuecomment-930460469\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65921\n\nReviewed By: albanD\n\nDifferential Revision: D31310105\n\nPulled By: prabhat00155\n\nfbshipit-source-id: 417691832a7c793744830c11e0ce53e3972d21a3",
    "Deleted lines": 2,
    "Added lines": 5,
    "Changed lines": 7,
    "Deleted code": "        mode (str, optional): whether to use parameters or state_dict for update\n            (default: parameters)",
    "Added code": "        mode (str, optional): whether to use ``'parameters'`` or ``'state_dict'`` for update\n            (default: ``'parameters'``)\n        modes = ['parameters', 'state_dict']\n        if mode not in modes:\n            raise ValueError(f'Invalid mode passed, valid values are {\", \".join(modes)}.')"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/40a7c317bc60713528320b9786765e4ec5707982",
    "Commit message": "Run BLAS F2C checks on host architecture (#60703)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/60351\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60703\n\nReviewed By: driazati\n\nDifferential Revision: D29379727\n\nPulled By: malfet\n\nfbshipit-source-id: dadbb1d39373887f07d59d0a05e093a5d070b016",
    "Deleted lines": 0,
    "Added lines": 7,
    "Changed lines": 7,
    "Deleted code": "",
    "Added code": "   # Push host architecture when cross-compiling otherwise check would fail\n   # when cross-compiling for arm64 on x86_64\n   cmake_push_check_state(RESET)\n  if(CMAKE_SYSTEM_NAME STREQUAL \"Darwin\" AND CMAKE_OSX_ARCHITECTURES MATCHES \"^(x86_64|arm64)$\")\n    list(APPEND CMAKE_REQUIRED_FLAGS \"-arch ${CMAKE_HOST_SYSTEM_PROCESSOR}\")\n  endif()\n  cmake_pop_check_state()"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f77f88fbc7511b405c4e493bdd74634b633f63d1",
    "Commit message": "[Quant] X86 qengine always uses fbgemm kernels on OS other than Linux (#93218)\n\n**Summary**\nX86 quantization backend (qengine) with oneDNN kernels has not been validated on OS other than Linux. So, let it fall back to fbgemm if OS is not Linux. This makes sure the behavior is the same on Windows/Mac as the previous default fbgemm qengine on x86 CPUs.\n\n**Test plan**\nCI checks.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/93218\nApproved by: https://github.com/jgong5, https://github.com/jerryzh168",
    "Deleted lines": 1,
    "Added lines": 10,
    "Changed lines": 11,
    "Deleted code": "// Check if onednn should be used w.r.t fbgemm",
    "Added code": "// When qengine is x86, use this util func to check if onednn kernel\n// is preferred than fbgemm's to get better performance.\n  // Performance of onednn is only validated on Linux right now.\n  // Also, the heuristics for dispatching are based on perf data on Linux.\n  // So, for x86 qengine, we always use fbgemm kernels if OS is not Linux.\n  // TODO Support more OSs.\n#if !defined(__linux__)\n  return false;\n#else\n#endif"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2a3ab71f285ee3ad10d39c716fe4ba90e7c849b2",
    "Commit message": "[quant][graphmode][fix] Remove useQuantizable check for dynamic quant (#41892)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41892\n\nCurrently the input of batch_norm is considered as dynamically quantizable but it shouldn't be\nthis PR fixes that\n\nTest Plan:\ninternal models\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D22681423\n\nfbshipit-source-id: 7f428751de0c4af0a811b9c952e1d01afda42d85",
    "Deleted lines": 7,
    "Added lines": 9,
    "Changed lines": 16,
    "Deleted code": "  for (const auto& func_input : _observe_inputs_aten_func) {\n    if (matchAtenFuncToUse(use, func_input.func_name, c10::nullopt)) {\n      return use.offset == func_input.arg_index;\n  }\n  for (const auto& func_input : _observe_inputs_call_func) {\n    if (matchCallFuncToUse(use, func_input.func_name, c10::nullopt)) {\n      return use.offset == func_input.arg_index;",
    "Added code": "  if (quant_type == QuantType::STATIC) {\n    for (const auto& func_input : _observe_inputs_aten_func) {\n      if (matchAtenFuncToUse(use, func_input.func_name, c10::nullopt)) {\n        return use.offset == func_input.arg_index;\n      }\n    for (const auto& func_input : _observe_inputs_call_func) {\n      if (matchCallFuncToUse(use, func_input.func_name, c10::nullopt)) {\n        return use.offset == func_input.arg_index;\n      }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/acd51e13f727af22e6c9e579518362898f1b12e6",
    "Commit message": "TorchScript add check if quantized\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32890\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19673463\n\nPulled By: z-a-f\n\nfbshipit-source-id: 453ff662810845fcaeb8e6d5919afa8e2d395768",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "                    if orig.is_quantized:\n                        orig = orig.dequantize()\n                    if ref.is_quantized:\n                        ref = ref.dequantize()"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0584fd9339af7c939ab7d955db05743ba58ff86d",
    "Commit message": "[quant][fx][graphmode][fix] Only insert observers for fixed qparam ops (#53330)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53330\n\nFixed a condition check for fixed qparam ops, previously we were including CopyNodes as well\n\nTest Plan:\npython test/test_quantization.py TestQuantizeFxOps.test_fixed_qparams_ops_fp16\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D26836867\n\nfbshipit-source-id: 8c486155244f852e675a938c3f4237f26505671c",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "            if activation_dtype(qconfig) == torch.float16:",
    "Added code": "            # insert observers for fixedqparams ops like sigmoid, since\n            # it supports fp16 static quantization\n            if isinstance(quantize_handler, FixedQParamsOpQuantizeHandler) and \\\n               activation_dtype(qconfig) == torch.float16:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/97245a06e14a5b87a0bca1908d7200603aff2c8c",
    "Commit message": "Turn on TORCH_CHECK for NT wrap_buffer (#100596)\n\nTORCH_INTERNAL_ASSERT_DEBUG_ONLY won't be enabled during non-debug builds, but for 1 dimension Tensors the check is cheap enough and not catching this can slow down development a lot.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/100596\nApproved by: https://github.com/drisspg",
    "Deleted lines": 5,
    "Added lines": 8,
    "Changed lines": 13,
    "Deleted code": "inline at::Tensor wrap_buffer(\n    at::Tensor buffer,\n    at::Tensor nested_sizes) {\n  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(\n      buffer.is_contiguous(), \"Given buffer must be contiguous.\");",
    "Added code": "inline at::Tensor wrap_buffer(at::Tensor buffer, at::Tensor nested_sizes) {\n  TORCH_CHECK(\n      buffer.dim() == 1,\n      \"Expected given buffer to be 1dim, but got \",\n      buffer.dim(),\n      \" instead.\");\n  TORCH_CHECK(\n      buffer.is_contiguous(), \"Expected given buffer to be contiguous.\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9bcb4de16878073896d8743fbd70d5abe28b595a",
    "Commit message": "check parameter k and l\n\nFixes #76715\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76719\nApproved by: https://github.com/ezyang",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  TORCH_CHECK((unsigned)l < dims.size());",
    "Added code": "  TORCH_CHECK((unsigned)l < dims.size() && (unsigned)k < dims.size());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/65496e4e67b42e52b3428b0cf2d994e0aa1a9902",
    "Commit message": "Bug fix in bound shape inferencer (#19729)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/19729\n\nAccessing dims() without boundary check is not good.\n\nReviewed By: zrphercule\n\nDifferential Revision: D15078912\n\nfbshipit-source-id: 3746d0c18261abeec0c4880c30430125928c3309",
    "Deleted lines": 1,
    "Added lines": 7,
    "Changed lines": 8,
    "Deleted code": "      channel_acc += current_input_shape.shape.dims(axis);",
    "Added code": "      if (axis < current_input_shape.shape.dims_size()) {\n        channel_acc += current_input_shape.shape.dims(axis);\n      } else {\n        LOG(INFO) << \"Mismatched input dim along axis \" << axis\n                  << \". We cannot infer missing input shape for Concat\";\n        return;\n      }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/260f66c3165ce0c48dd1514a916da6971d981578",
    "Commit message": "Fix concat dimension check bug (#17343)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/17343\n\nSee [post](https://fb.workplace.com/groups/1405155842844877/permalink/2630764056950710/)\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D14163001\n\nfbshipit-source-id: 038f15d6a58b3bc31910e7bfa47c335e25739f12",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "      const int canonical_axis = canonical_axis_index_(axis, in[0].dims_size());",
    "Added code": "      int adj_size = in[0].dims_size() + (add_axis ? 1 : 0);\n      const int canonical_axis = canonical_axis_index_(axis, adj_size);\n      CAFFE_ENFORCE_LT(\n          canonical_axis, adj_size, \"Axis not in input ndim range.\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc07102b17915f21170fae9a9d52c6f2d59726ca",
    "Commit message": "Check dim size preventively when doing shape inference for BatchMatMul (#12691)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/12691\n\nWe check input(0) but not input(1) in BatchMatMul. This may result in a protobuf exception which won't be caught by upstream and causing termination of the program. Check that with `CAFFE_ENFORCE` will be caught by upstream inference function. Plus, it will print out clean stack tracing showing where went wrong.\n\nReviewed By: bddppq, houseroad, BIT-silence\n\nDifferential Revision: D10391130\n\nfbshipit-source-id: daf8dcd8fcf9629a0626edad660dff54dd9aeae3",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "    CAFFE_ENFORCE_GE(in[1].dims_size(), 2);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8af88f3525e1908deb9ac181ebfb9f8eb49bcb46",
    "Commit message": "[Caffe2] Add ADD operator for IDEEP (#8220)\n\n* Add ADD operator for IDEEP\r\n\r\n* Add boradcast check\r\n\r\n* Comments",
    "Deleted lines": 3,
    "Added lines": 10,
    "Changed lines": 13,
    "Deleted code": "    const auto &X = Input(INPUT0);\n      vector<float> scales (InputSize(), 1.0);\n",
    "Added code": "    const auto& X = Input(INPUT0);\n      const vector<float> scales(InputSize(), 1.0);\n      const auto dims = X.get_dims();\n        if (Input(i).get_dims() != dims) {\n          CAFFE_ENFORCE_EQ(\n              dims,\n              Input(i).get_dims(),\n              \"Broadcast is not yet supported with IDEEP.\");\n        }\nREGISTER_IDEEP_OPERATOR(Add, IDEEPSumOp);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/74783f0cd83e8ef1fd32cfd04f36da8ddfc52f57",
    "Commit message": "Move the broadcast check in MKL Add/Sum to runtime (#7978)",
    "Deleted lines": 5,
    "Added lines": 5,
    "Changed lines": 10,
    "Deleted code": "    // caffe2::AddOp support broadcast but dnnSumCreate() doesn't.\n    bool broadcast = OperatorBase::GetSingleArgument<bool>(\"broadcast\", false);\n    OPERATOR_NEEDS_FEATURE(\n        !broadcast, \"Broadcast is not yet supported with MKLDNN.\");\n      CAFFE_ENFORCE_EQ(X0.dims(), Xi.dims());",
    "Added code": "    // caffe2::AddOp support broadcast but dnnSumCreate() doesn't.\n    for (auto i = 0; i < this->InputSize(); ++i) {\n      const MKLMemory<T>& Xi = Input(i);\n      CAFFE_ENFORCE_EQ(X0.dims(), Xi.dims(), \"Broadcast is not yet supported with MKLDNN.\");\n    }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/77523df413ff7f8a336b6481cfa47967c234a149",
    "Commit message": "Add more check on softmax ONNX exporting logic (#4592)\n\n* Add more check on softmax exporting logic\r\n\r\n* Add more comments about axis and dim",
    "Deleted lines": 0,
    "Added lines": 18,
    "Changed lines": 18,
    "Deleted code": "",
    "Added code": "    # Softmax does normalization at vector level.\n    # PyTorch and ONNX use different strategies to split the input tensor into vectors.\n    # Thus dim and axis have different meanings.\n    # PyTorch slices the input tensor into vectors along the `dim`-th dimension.\n    # ONNX reshapes the input into a 2-D tensor, and `axis` indicates where the input is coerced.\n    # If input is a 2 x 3 tensor:\n    # input = [[1.0, 1.0, 1.0],\n    #          [1.0, 1,0, 1,0]]\n    # with dim = 0, the result is:\n    # result = [[0.5, 0.5, 0.5],\n    #           [0.5, 0.5, 0.5]]\n    # with axis = 0, the result is:\n    # result = [[0.167, 0.167, 0.167],\n    #           [0.167, 0.167, 0.167]]\n    # So only when dim and axis both equal to ndim - 1 (the last dimension),\n    # their semantics are equivalent.\n    if len(input.type().sizes()) != dim + 1:\n        return _unimplemented(\"dim\", \"ONNX and PyTorch use different strategies to split the input.\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8d377617e73a399e5b8838cf96dfd2896576ac52",
    "Commit message": "Fix MKLMemory::CopyTo for case where shapes don't match'\n\nSummary:\nThere were cases where the direct copy succeeded, but the\ndimensions didn't match. Now, we check dimensions and reset if they\ndon't match before issuing the copy.\n\nReviewed By: salexspb\n\nDifferential Revision: D6103325\n\nfbshipit-source-id: 602605d8b119cae74e006c792bc42f355a5a9b4e",
    "Deleted lines": 13,
    "Added lines": 11,
    "Changed lines": 24,
    "Deleted code": "    if (dnnPrimitive_t(convert) == nullptr ||\n        dnnConversionExecute<T>(convert, buffer_.get(), other->buffer()) !=\n            E_SUCCESS) {\n      VLOG(2) << \"Direct copy failed, will need to allocate output.\";\n      // If CopyTo directly did not succeed, it could be because the target\n      // MKLMemory is not having the right layout. In this case we will reset\n      // the target and then do another copy.\n      other->Reset(dims_, primitive, type);\n      PrimitiveWrapper<T> convert2(\n          dnnConversionCreate<T>, layout_, other->layout_);\n      MKLDNN_SAFE_CALL(\n          dnnConversionExecute<T>(convert2, buffer_.get(), other->buffer()));\n    }",
    "Added code": "      CAFFE_ENFORCE(\n          dnnLayoutCompare<T>(other->layout_, layout_),\n          \"MKLMemory layout does not match, despite in-place buffers\");\n      CAFFE_ENFORCE(\n          other->dims() == dims(),\n          \"MKLMemory dimensions do not match, despite in-place buffers\");\n    if (dims() != other->dims()) {\n      other->Reset(dims(), primitive, type);\n    }\n    MKLDNN_SAFE_CALL(\n        dnnConversionExecute<T>(convert, buffer_.get(), other->buffer()));"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8f26d6aabcad991da88b663467ee2080a38631f7",
    "Commit message": "More shape checking for ConvNd (#3052)\n\n* check conv weight & bias dims\r\n\r\n* address comments",
    "Deleted lines": 4,
    "Added lines": 36,
    "Changed lines": 40,
    "Deleted code": "\t\t\t\t      const at::Tensor& weight,\n\t << \", so expected input\" << input.sizes() << \"  to have \"\n\t << \", so expected input\" << input.sizes() << \"  to have \"\n  check_input_shape_forward(input, weight, groups, transposed);",
    "Added code": "\t\t\t\t      const at::Tensor& weight, const at::Tensor& bias,\n  int k = input.ndimension();\n\n  if (weight.ndimension() != k) {\n      std::stringstream ss;\n      ss << \"Expected \" << k << \"-dimensional input for \" << k\n         << \"-dimensional weight \" << weight.sizes() << \", but got input of size \"\n         << input.sizes() << \" instead\";\n      throw std::runtime_error(ss.str());\n  }\n  if (weight.size(0) < groups) {\n    std::stringstream ss;\n    ss << \"Given groups=\" << groups << \", expected weight to be at least \"\n       << groups << \" at dimension 0, but got weight of size \" << weight.sizes()\n       << \" instead\";\n    throw std::runtime_error(ss.str());\n  }\n\n\t << \", so expected input\" << input.sizes() << \" to have \"\n    if (bias.defined() && (bias.ndimension() != 1 || bias.size(0) != weight.size(0))) {\n      std::stringstream ss;\n      ss << \"Given weight of size \" << weight.sizes()\n         << \", expected bias to be 1-dimensional with \" << weight.size(0) << \" elements\"\n         << \", but got bias of size \" << bias.sizes() << \" instead\";\n      throw std::runtime_error(ss.str());\n    }\n\t << \", so expected input\" << input.sizes() << \" to have \"\n    if (bias.defined() && (bias.ndimension() != 1 || bias.size(0) != weight.size(1) * groups)) {\n      std::stringstream ss;\n      ss << \"Given transposed=\" << transposed << \", weight of size \" << weight.sizes()\n         << \", expected bias to be 1-dimensional with \" << weight.size(1) * groups << \" elements\"\n         << \", but got bias of size \" << bias.sizes() << \" instead\";\n      throw std::runtime_error(ss.str());\n    }\n  check_input_shape_forward(input, weight, bias, groups, transposed);\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a53f4b0f9bbc007c0a92e4fd28dd22af027e24a8",
    "Commit message": "add dimension check to NHWC2NCHW shape inference\n\nSummary: To prevent assertion from protobuffer when accessing the dims.\n\nReviewed By: asaadaldien\n\nDifferential Revision: D5504362\n\nfbshipit-source-id: d9b55fab3126e2760a3e790615ed30a1af2ddc32",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "      CAFFE_ENFORCE_EQ(\n          in[0].dims_size(), 4, \"Input for NHWC2NCHW must be 4 dimensional\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ecd3c252b4da3056797f8a505c9ebe8d68db55c4",
    "Commit message": "Suport all length one SLS op lowering: C2 part (#33332)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33332\n\nWe check the input shape of lengths and indices of SLS and add an attribute if they are the same.\n\nTest Plan:\n```\nbuck test glow/fb/test/numerics:test_operator_onnxifinnpi -- test_slws_fused_8bit_rowwise_length1_graph\n```\n\nReviewed By: ipiszy\n\nDifferential Revision: D19874903\n\nfbshipit-source-id: 06b643b5351d0ba19ba209b5a5b599fbb38b1dfc",
    "Deleted lines": 0,
    "Added lines": 24,
    "Changed lines": 24,
    "Deleted code": "",
    "Added code": "    } else if (\n        op.type() == \"SparseLengthsSum\" ||\n        op.type() == \"SparseLengthsSumFused8BitRowwise\" ||\n        op.type() == \"SparseLengthsWeightedSum\" ||\n        op.type() == \"SparseLengthsWeightedSumFused8BitRowwise\" ||\n        op.type() == \"SparseLengthsSumFused4BitRowwise\" ||\n        op.type() == \"SparseLengthsWeightedSumFused4BitRowwise\") {\n      int weighted = (op.type() == \"SparseLengthsWeightedSum\" ||\n                      op.type() == \"SparseLengthsWeightedSumFused8BitRowwise\" ||\n                      op.type() == \"SparseLengthsWeightedSumFused4BitRowwise\")\n          ? 1\n          : 0;\n      const auto& indices_hint = shape_hints.at(op.input(1 + weighted));\n      const auto& lengths_hint = shape_hints.at(op.input(2 + weighted));\n      const auto& indices_shape = indices_hint.shape;\n      const auto& lengths_shape = lengths_hint.shape;\n      if ((indices_hint.getDimType(0) ==\n               TensorBoundShape_DimType_BATCH_OF_FEATURE_MAX ||\n           indices_hint.getDimType(0) ==\n               TensorBoundShape_DimType_BATCH_OF_FEATURE_MAX_DEFAULT) &&\n          indices_shape.dims_size() == 1 && lengths_shape.dims_size() == 1 &&\n          indices_shape.dims(0) == lengths_shape.dims(0)) {\n        op.add_arg()->CopyFrom(MakeArgument<int>(\"length1\", 1));\n      }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/55092b1cc604fad3d70d31e71bbdd3a43a279423",
    "Commit message": "Validate matching input shapes in Int8Add operator (#14520)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/14520\n\nDefault engine doesn't support broadcast semantics in Int8Add operator. This patch adds a check that shapes are equivalent.\n\nReviewed By: bertmaher\n\nDifferential Revision: D13250922\n\nfbshipit-source-id: 8526d07723bd9a34d54dee04d121c57f8b33c481",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "    CAFFE_ENFORCE_EQ(\n        A.t.sizes(),\n        B.t.sizes(),\n        \"inputs must have the same shape (broadcast semantics is not supported)\");\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1359d16fe8ca0cb7041674c455f2f99a9636fec0",
    "Commit message": "[CI] Further tighten the checking of two eager runs (#95902)\n\nSummary: To catch nondeterminism in eager if there is any.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95902\nApproved by: https://github.com/jansel",
    "Deleted lines": 2,
    "Added lines": 8,
    "Changed lines": 10,
    "Deleted code": "                fp64_ref=None,  # Two eager runs should be the same without comparing against fp64_output\n        torch.backends.cudnn.deterministic = True",
    "Added code": "            # Two eager runs should have exactly same result\n                fp64_ref=None,\n                cos_similarity=False,\n                tol=0,\n        torch.use_deterministic_algorithms(True)\n        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cuda.matmul.allow_tf32 = False"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/41902a6ebc1806e7f4d6ce1da604cc9921c6515e",
    "Commit message": "[dynamo] Optimize is_tracing checks (#118474)\n\nbenchmarks/dynamo/microbenchmarks/overheads.py\n- before: 10.4us\n- after: 9.9us\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118474\nApproved by: https://github.com/yanboliang",
    "Deleted lines": 4,
    "Added lines": 13,
    "Changed lines": 17,
    "Deleted code": "            if torch.fx._symbolic_trace.is_fx_tracing() and not isinstance(\n                self, DisableContext\n            ):\n            if torch.jit.is_tracing():",
    "Added code": "def always_false():\n    return False\n\n\n        if isinstance(self, DisableContext):\n            is_jit_tracing = always_false\n            is_fx_tracing = always_false\n        else:\n            is_jit_tracing = torch._C._is_tracing\n            is_fx_tracing = torch.fx._symbolic_trace.is_fx_tracing\n\n            if is_fx_tracing():\n            if is_jit_tracing():"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1f819ee965894b8332cb364a67c91855c91c9dcc",
    "Commit message": "Add check for no grad in transformer encoder nestedtensor conversion (#78832)\n\nBefore, we allowed inputs with grad to be converted to NestedTensors. Autograd attempts to find the size of the NestedTensor, but NestedTensor throws an exception for its size function. This causes all calls to nn.TransformerEncoder with grad enabled to fail.\n\nFix: we add a check for no grad in transformer encoder so we do not convert tensor with grad to nestedtensor.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78832\nApproved by: https://github.com/cpuhrsch, https://github.com/jbschlosser",
    "Deleted lines": 3,
    "Added lines": 4,
    "Changed lines": 7,
    "Deleted code": "                        if output.is_cuda or 'cpu' in str(output.device):\n                            convert_to_nested = True\n                            output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())",
    "Added code": "                        if not torch.is_grad_enabled() or all([not x.requires_grad for x in tensor_args]):\n                            if output.is_cuda or 'cpu' in str(output.device):\n                                convert_to_nested = True\n                                output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc43ad428603539a2051940c09b191825f66203d",
    "Commit message": "add is_grad_enabled check in runtime_wrapper before running with torch.no_grad (#117089)\n\nWe observed that `with torch.no_grad()` in runtime_wrapper introduced ~10% (0.06ms->0.066ms) inference performance regression on lennard_jones on cpu.\nFor inference tasks in benchmark, grad has been disabled, but in the current runtime_wrapper, no_grad is set again and its time is counted into the running time.\nTherefore, we add `is_grad_enabled` check in runtime_wrapper before running with torch.no_grad. If grad has been disabled, there is no need to set no_grad.\n\nBefore this pr:\n1.043x\ndev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks\ncpu,lennard_jones,1,**1.043427**,**0.068366**,4.756151,0.941846,45.056819,47.838822,9,1,0,0\n\nAfter this pr:\n1.146x\ndev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks\ncpu,lennard_jones,1,**1.146190**,**0.061844**,4.468380,0.936456,44.427264,47.441920,9,1,0,0\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117089\nApproved by: https://github.com/jgong5, https://github.com/bdhirsh",
    "Deleted lines": 1,
    "Added lines": 8,
    "Changed lines": 9,
    "Deleted code": "            with torch.no_grad():",
    "Added code": "            if torch.is_grad_enabled():\n                with torch.no_grad():\n                    all_outs = call_func_at_runtime_with_args(\n                        compiled_fn,\n                        args,\n                        disable_amp=disable_amp,\n                    )\n            else:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f118d20bea9188db1bd053dd1d1af1b32479183e",
    "Commit message": "Make requires grad check run only when grad mode is enabled (#60740)\n\nSummary:\nAs per title.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60740\n\nReviewed By: ngimel\n\nDifferential Revision: D29405934\n\nPulled By: albanD\n\nfbshipit-source-id: 35c537939a3871f5a0d2146543506e4d07465724",
    "Deleted lines": 5,
    "Added lines": 15,
    "Changed lines": 20,
    "Deleted code": "inline void check_no_requires_grad(const Tensor& tensor, const char* name, const char* fn_name=\"\") {\n  TORCH_CHECK(!(tensor.defined() && tensor.requires_grad()), \"The function '\", fn_name, \"' is not differentiable \"\n              \"with respect to argument '\", name, \"'. This input cannot have requires_grad True.\");\n    check_no_requires_grad(tensor, name, fn_name);\n      check_no_requires_grad(*tensor, name, fn_name);",
    "Added code": "inline void check_no_requires_grad(const Tensor& tensor, const char* name,\n                                   const char* fn_name=\"\", bool check_grad_mode=true) {\n  TORCH_CHECK(!(tensor.defined() && tensor.requires_grad()) || !(check_grad_mode && GradMode::is_enabled()),\n              \"The function '\", fn_name, \"' is not differentiable with respect to argument '\", name,\n              \"'. This input cannot have requires_grad True.\");\n  // GradMode check is expensive, so check it only once for TensorLists\n  if (!GradMode::is_enabled()) {\n    return;\n  }\n    check_no_requires_grad(tensor, name, fn_name, /*check_grad_mode*/ false);\n  // GradMode check is expensive, so check it only once for TensorLists\n  if (!GradMode::is_enabled()) {\n    return;\n  }\n      check_no_requires_grad(*tensor, name, fn_name, /*check_grad_mode*/ false);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c1c4882367c592d49e15268a0b99631c207d662e",
    "Commit message": "[aps] Sync thrift (#113810)\n\nSummary:\nBased on discussions with Sherlock + Zhengxu in D51118067, updated the internal thrift schema to match the OSS schema.\n\nVerifier failures:\n* Test contains a None as input, resulting in no meta[\"val\"]\n* Test contains torch.autograd.grad_mode.set_grad_enabled as an op, which also results in no meta[\"val\"]\n* torch.autograd.grad_mode.set_grad_enabled is also not a valid op\n* Test adds a \"parameter\" to the state dict but the parameter is not an nn.Parameter, causing an assertion failure\n\nSo to bypass these failures I did the following hacks(?):\n* Before creating the exported program in deserialization, populate nodes w/o meta[\"val\"] with meta[\"val\"] = None\n* Add torch.autograd.grad_mode.set_grad_enabled to the skip opset\n* Duplicated ExportGraphSignature into aot_export.py so that the graph signature checks will be skipped\n\nConfigerator changes in D51343615\n\nTest Plan: CI\n\nReviewed By: zhxchen17\n\nDifferential Revision: D51342921\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113810\nApproved by: https://github.com/zhxchen17",
    "Deleted lines": 1,
    "Added lines": 5,
    "Changed lines": 6,
    "Deleted code": "                if op not in _allowed_builtin_ops():",
    "Added code": "            # TODO Remove this allowlist.\n            _allowed_torch_functions = (torch.autograd.grad_mode.set_grad_enabled,)\n\n                if op not in _allowed_builtin_ops() and op not in _allowed_torch_functions:\n                        f\"Valid torch functions: {_allowed_torch_functions}\""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/af3cbfed9510747c776418c260c5116f662c6452",
    "Commit message": "Add validation check in fx2trt interpreter (#63424)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63424\n\nAdd validation check in fx2trt for missing converter operators. If any op missing, interpreter init will report missing operators.\n\nTest Plan:\nfor call_function and call_method:\nmanual test with feeds benchmark and verify init failed with expected message.\n{F642390780}\n\nfor call_module:\nspecify a module as leaf node and make acc_tracer trace it as a node; then in fx2trt.py, in CONVERTER initialize stage make it skip recording all modules; initialize interpreter and call validator function, verify the output includes the missing module name, return value print as screenshot below.\n\n{F643458718}\n\nReviewed By: 842974287\n\nDifferential Revision: D30294832\n\nfbshipit-source-id: 243dca3fdfc6a174ded65248938e2a234aec19c6",
    "Deleted lines": 0,
    "Added lines": 17,
    "Changed lines": 17,
    "Deleted code": "",
    "Added code": "        missing_ops = self.validate_conversion\n        if not missing_ops:\n            warnings.warn(\"Interpretation may fail due to missing operations \\n\"\n                          + \"\\n\".join(f\"{i}\" for i in missing_ops))\n    def validate_conversion(self):\n        missing_converter = set()\n\n        for node in self.module.graph.nodes:\n            if node.op in [\"call_function\", \"call_method\"] and not CONVERTERS.get(node.target):\n                missing_converter.add(f\"{node.op} {node.target}\")\n            elif node.op == \"call_module\":\n                submod = self.fetch_attr(node.target)\n                if not CONVERTERS.get(type(submod)):\n                    missing_converter.add(f\"{node.op} {type(submod)}\")\n\n        return missing_converter\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f3a2094065c8b4b7bae426e71c923a8a8abb74b5",
    "Commit message": "[Dynamo][Export] Mitigate legacy issue that aten op as export entrance function (#119528)\n\nThis is going to fix a legacy issue like:\n```\ntorch._dynamo.export(torch.ops.aten.scaled_dot_product_attention, ...)(*inputs,)\n```\nThis is not supported any more, now the top level ```torch.export``` only support ```nn.Module```, but there are still some tests using the internal APIs and caused the ```trace_rules.check``` assertion error. This PR is going to mitigate such cases.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119528\nApproved by: https://github.com/ydwu4",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "            and not isinstance(\n                call_to_inspect, (torch._ops.OpOverloadPacket, torch._ops.OpOverload)\n            )"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cc6a51c9f3ee97a06ff9c0b84477e88e33e31137",
    "Commit message": "added shape checking to WeightedRandomSampler (#78585)\n\nFixes #78236\n\nAn erronously shaped weights vector will result in the following output\n\n```\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n~/datarwe/pytorch/torch/utils/data/sampler.py in <module>\n      [274](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=273) WeightedRandomSampler([1,2,3], 10)\n----> [275](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=274) WeightedRandomSampler([[1,2,3], [4,5,6]], 10)\n\n~/datarwe/pytorch/torch/utils/data/sampler.py in __init__(self, weights, num_samples, replacement, generator)\n    [192](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=191)         weights = torch.as_tensor(weights, dtype=torch.double)\n    [193](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=192)         if len(weights.shape) != 1:\n--> [194](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=193)             raise ValueError(\"weights should be a 1d sequence but given \"\n    [195](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=194)                              \"weights have shape {}\".format(tuple(weights.shape)))\n    [196](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=195)\n\nValueError: weights should be a 1d sequence but given weights have shape (2, 3)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78585\nApproved by: https://github.com/NivekT, https://github.com/ejguan",
    "Deleted lines": 1,
    "Added lines": 7,
    "Changed lines": 8,
    "Deleted code": "        self.weights = torch.as_tensor(weights, dtype=torch.double)",
    "Added code": "\n        weights_tensor = torch.as_tensor(weights, dtype=torch.double)\n        if len(weights_tensor.shape) != 1:\n            raise ValueError(\"weights should be a 1d sequence but given \"\n                             \"weights have shape {}\".format(tuple(weights_tensor.shape)))\n\n        self.weights = weights_tensor"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8ee59280d78a4fefc4de0da04b287e067c28de0d",
    "Commit message": "Bug - check config for dynamic (#99676)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/99676\nApproved by: https://github.com/ezyang",
    "Deleted lines": 2,
    "Added lines": 3,
    "Changed lines": 5,
    "Deleted code": "                automatic_dynamic = curr_sizes is None or curr_sizes[i] is None\n",
    "Added code": "                automatic_dynamic = config.automatic_dynamic_shapes and (\n                    curr_sizes is None or curr_sizes[i] is None\n                )"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/41ad221751e57c2d2ccc82b431f56d6ed62e1741",
    "Commit message": "[PyTorch] MHA: fix contiguity assumption in transform_bias_rescale_qkv (#72465)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72465\n\nThis code path incorrectly assumed input tensors were contiguous. Now we check that.\nghstack-source-id: 149201476\n\nTest Plan: CI\n\nReviewed By: ngimel\n\nDifferential Revision: D34007665\n\nfbshipit-source-id: c43438f2495e32304ea3f7846e01eceb4a9448f7\n(cherry picked from commit 0767b225f23846c1636ac3622f46b0c5ec071d96)",
    "Deleted lines": 3,
    "Added lines": 7,
    "Changed lines": 10,
    "Deleted code": "  AT_DISPATCH_FLOATING_TYPES_AND2(\n        scalar_t* qkv_data = qkv.data_ptr<scalar_t>();\n        scalar_t* qkv_bias_data = qkv_bias.data_ptr<scalar_t>();",
    "Added code": "  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(q_k_v.is_contiguous());\n  const auto qkv_contig = qkv.expect_contiguous();\n  const auto qkv_bias_contig = qkv_bias.expect_contiguous();\n AT_DISPATCH_FLOATING_TYPES_AND2(\n        scalar_t* qkv_data = qkv_contig->data_ptr<scalar_t>();\n        scalar_t* qkv_bias_data = qkv_bias_contig->data_ptr<scalar_t>();\n  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(q_k_v_s.size() == 3);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e9e125475e94aabfb34ee239fadc760615eef429",
    "Commit message": "[Static Runtime] Add schema check to aten::repeat and fb::fast_gather (#58106)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58106\n\nFollowup for D28047955 (https://github.com/pytorch/pytorch/commit/1f83d8eec25c7339bd3e2862baf9b389e6a738a4).\n\nReviewed By: ajyu\n\nDifferential Revision: D28369472\n\nfbshipit-source-id: 36aa10082589f4b6f0cc2d79f032fe72a19cda57",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "  if (n->inputs().size() != 2) {\n    return nullptr;\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7ea6559658a6f650363f8b96f462bbc047e29124",
    "Commit message": "Add size checks to `torch.stack` (#32931)\n\nSummary:\nChecks the size of each tensor passed to `torch.stack` before calling `cat` to address https://github.com/pytorch/pytorch/issues/29510. This is done in the `get_stack_input` function as that is a common path. The function now compares the size of each tensor in the TensorList to the size of the first tensor and throws an exception when the sizes are not equal.\n\nTo compare:\n```\nx = torch.zeros([1, 2])\ny = torch.zeros([1, 3])\ntorch.stack([x, y]) # Errors due to size differences\n```\nCurrent error:\n```\nRuntimeError: invalid argument 0: Sizes of tensors must match\nexcept in dimension 0. Got 2 and 3 in dimension 2 at (path)\\aten\\src\\TH/generic/THTensor.cpp:612\n```\nNew error:\n```\nRuntimeError: stack expects each tensor to be equal size, but\ngot [1, 2] at entry 0 and [1, 3] at entry 1\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32931\n\nDifferential Revision: D19700110\n\nPulled By: ezyang\n\nfbshipit-source-id: 7e18bb00fa2c137e418e340d719b6b76170b83e3",
    "Deleted lines": 1,
    "Added lines": 7,
    "Changed lines": 8,
    "Deleted code": "  for (size_t i = 0; i < tensors.size(); ++i) {",
    "Added code": "// Precondition: tensors is non-empty\n  at::IntArrayRef entry_shape = tensors[0].sizes();\n  inputs[0] = tensors[0].unsqueeze(dim);\n  for (size_t i = 1; i < tensors.size(); ++i) {\n    TORCH_CHECK(tensors[i].sizes() == entry_shape,\n      \"stack expects each tensor to be equal size, but got \", entry_shape,\n      \" at entry 0 and \", tensors[i].sizes(), \" at entry \", i);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bbb5e106ad6228953df6c7f5c8916b26dc0cb457",
    "Commit message": "Improve error checking of CUDALoops. (#38810)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38810\n\nSame change as was applied to CPU loops -- separate out checking of the inputs and outputs.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D21670339\n\nPulled By: gchanan\n\nfbshipit-source-id: 42f208538dce1a5598d14948d8d02a1c91ba152a",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "  TORCH_INTERNAL_ASSERT(iter.ntensors() == traits::arity + 1);",
    "Added code": "  TORCH_INTERNAL_ASSERT(iter.ninputs() == traits::arity);\n  TORCH_INTERNAL_ASSERT(iter.noutputs() == 1);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b7882f9bd65de5a4c60f625d56186b583c1d6842",
    "Commit message": "Improve cpu/Loops.h arity asserts. (#38809)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38809\n\nThis splits the asserts into separate input/output asserts and makes the numbers precise, instead of ranges.\n\nThis is an ongoing effort to improve the Loops assertion and to integrate dynamic cast checking into CPU loops.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D21670263\n\nPulled By: gchanan\n\nfbshipit-source-id: b1868db5255a69158045b759dc9171690a2dcd01",
    "Deleted lines": 5,
    "Added lines": 12,
    "Changed lines": 17,
    "Deleted code": "  TORCH_INTERNAL_ASSERT(iter.ntensors() >= traits::arity + 1);\n  TORCH_INTERNAL_ASSERT(iter.ntensors() >= traits::arity + 1);\n  TORCH_INTERNAL_ASSERT((std::is_void<typename traits::result_type>::value &&\n    iter.noutputs() == 0 && iter.ntensors() == traits::arity) || (iter.ntensors() >= traits::arity + 1));\n  TORCH_INTERNAL_ASSERT(iter.ntensors() >= traits::arity + 1);",
    "Added code": "  // this could be extended to work with void return types\n  TORCH_INTERNAL_ASSERT(iter.ninputs() == traits::arity);\n  TORCH_INTERNAL_ASSERT(iter.noutputs() == 1);\n  // this could be extended to work with void return types\n  TORCH_INTERNAL_ASSERT(iter.ninputs() == traits::arity);\n  TORCH_INTERNAL_ASSERT(iter.noutputs() == 1);\n  constexpr bool result_void = std::is_void<typename traits::result_type>::value;\n  TORCH_INTERNAL_ASSERT(iter.ninputs() == traits::arity &&\n                        ((result_void && iter.noutputs() == 0) || (!result_void && iter.noutputs() == 1)));\n  // this could be extended to work with void return types\n  TORCH_INTERNAL_ASSERT(iter.ninputs() == traits::arity);\n  TORCH_INTERNAL_ASSERT(iter.noutputs() == 1);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d3bf6803b62c79f1dafd1eec49b4bd65d5a27697",
    "Commit message": "[dynamo] add sanity check that we do not wrap tracked tensors (#112025)\n\nIdentified as a result of https://github.com/pytorch/pytorch/pull/111911\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112025\nApproved by: https://github.com/ezyang",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "        # We cannot already be tracking the tensor, which implies\n        # it would have already been wrapped\n        assert value not in self.tx.output.side_effects\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/794e3971ab90611b4a63166589368a737843c8bc",
    "Commit message": "Add size check before calling stack_.at(dict_pos) in unpickler.cpp (#94300)\n\nHi!\n\nI've been fuzzing different pytorch modules, and found a crash inside one of them.\n\nSpecifically, I'm talking about a module for unpickling and a function called `Unpickler::readInstruction()`. Running this function with provided crash file results in a crash, which occurs while calling `auto dict = stack_.at(dict_pos).toGenericDict();` [unpickler.cpp:561](https://github.com/pytorch/pytorch/blob/0e94fbc0c8ab1572c88159c1a4c397b6eb824c01/torch/csrc/jit/serialization/unpickler.cpp#L561). The crash occurs, because the index `dict_pos` is out of bounds (which itself happens because the stack size is 0).\n\nBesides this pull-request, there is another one related to unpickler hardening: https://github.com/pytorch/pytorch/pull/84343\n\nAll tests were performed on this pytorch version: [abc54f93145830b502400faa92bec86e05422fbd](https://github.com/pytorch/pytorch/tree/abc54f93145830b502400faa92bec86e05422fbd)\n\n### How to reproduce\n\n1. To reproduce the crash, use provided docker: [Dockerfile](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch)\n\n2. Build the container: `docker build -t oss-sydr-fuzz-pytorch-reproduce .`\n\n3. Copy crash file to the current directory:\n\n    - [crash-042dff5e121580425d9d34d0f293918f3c9fbf1e.zip](https://github.com/pytorch/pytorch/files/10674361/crash-042dff5e121580425d9d34d0f293918f3c9fbf1e.zip)\n\n4. Run the container: ``docker run --privileged --network host -v `pwd`:/homedir --rm -it oss-sydr-fuzz-pytorch-reproduce /bin/bash``\n\n5. And execute the binary: `/message_deserialize_sydr /homedir/crash-042dff5e121580425d9d34d0f293918f3c9fbf1e`\n\nAfter execution completes you will see this error message:\n\n```txt\nterminate called after throwing an instance of 'std::out_of_range'\n  what():  vector::_M_range_check: __n (which is 18446744073709551613) >= this->size() (which is 0)\n```\n\nAnd this stacktrace:\n\n```asan\nerminate called after throwing an instance of 'std::out_of_range'\n  what():  vector::_M_range_check: __n (which is 18446744073709551613) >= this->size() (which is 0)\n==39== ERROR: libFuzzer: deadly signal\n    #0 0x5d0df1 in __sanitizer_print_stack_trace /llvm-project/compiler-rt/lib/asan/asan_stack.cpp:87:3\n    #1 0x545727 in fuzzer::PrintStackTrace() /llvm-project/compiler-rt/lib/fuzzer/FuzzerUtil.cpp:210:5\n    #2 0x52b933 in fuzzer::Fuzzer::CrashCallback() /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:233:3\n    #3 0x7f9118e0341f  (/lib/x86_64-linux-gnu/libpthread.so.0+0x1441f)\n    #4 0x7f9118c2300a in raise (/lib/x86_64-linux-gnu/libc.so.6+0x4300a)\n    #5 0x7f9118c02858 in abort (/lib/x86_64-linux-gnu/libc.so.6+0x22858)\n    #6 0x7f9119040910  (/lib/x86_64-linux-gnu/libstdc++.so.6+0x9e910)\n    #7 0x7f911904c38b  (/lib/x86_64-linux-gnu/libstdc++.so.6+0xaa38b)\n    #8 0x7f911904c3f6 in std::terminate() (/lib/x86_64-linux-gnu/libstdc++.so.6+0xaa3f6)\n    #9 0x7f911904c6a8 in __cxa_throw (/lib/x86_64-linux-gnu/libstdc++.so.6+0xaa6a8)\n    #10 0x7f91190433aa  (/lib/x86_64-linux-gnu/libstdc++.so.6+0xa13aa)\n    #11 0x63acdf in std::vector<c10::IValue, std::allocator<c10::IValue> >::_M_range_check(unsigned long) const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:1073:4\n    #12 0xce8f93e in std::vector<c10::IValue, std::allocator<c10::IValue> >::at(unsigned long) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:1094:2\n    #13 0xce8f93e in torch::jit::Unpickler::readInstruction() /pytorch_fuzz/torch/csrc/jit/serialization/unpickler.cpp:546:26\n    #14 0xce8d527 in torch::jit::Unpickler::run() /pytorch_fuzz/torch/csrc/jit/serialization/unpickler.cpp:235:27\n    #15 0xce8d1c2 in torch::jit::Unpickler::parse_ivalue() /pytorch_fuzz/torch/csrc/jit/serialization/unpickler.cpp:192:3\n    #16 0xcdf0792 in torch::jit::unpickle(std::function<unsigned long (char*, unsigned long)>, std::function<c10::StrongTypePtr (c10::QualifiedName const&)>, c10::ArrayRef<at::Tensor>, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)) /pytorch_fuzz/torch/csrc/jit/serialization/pickle.cpp:127:20\n    #17 0xcdf104d in torch::jit::unpickle(char const*, unsigned long, std::function<c10::StrongTypePtr (c10::QualifiedName const&)>, c10::ArrayRef<at::Tensor>, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)) /pytorch_fuzz/torch/csrc/jit/serialization/pickle.cpp:137:10\n    #18 0xe0532db in torch::distributed::rpc::ScriptRemoteCall::fromMessage(torch::distributed::rpc::Message const&) /pytorch_fuzz/torch/csrc/distributed/rpc/script_remote_call.cpp:74:16\n    #19 0xe0ffa10 in torch::distributed::rpc::deserializeRequest(torch::distributed::rpc::Message const&) /pytorch_fuzz/torch/csrc/distributed/rpc/utils.cpp:108:14\n    #20 0x602a41 in LLVMFuzzerTestOneInput /message_deserialize_fuzz.cc:192:27\n    #21 0x52ce61 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n    #22 0x516d7c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n    #23 0x51cacb in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n    #24 0x546062 in main /llvm-project/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n    #25 0x7f9118c04082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082)\n    #26 0x51169d in _start (/message_deserialize_fuzz+0x51169d)\n\nNOTE: libFuzzer has rudimentary signal handlers.\n      Combine libFuzzer with AddressSanitizer or similar for better crash reports.\nSUMMARY: libFuzzer: deadly signal\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94300\nApproved by: https://github.com/malfet, https://github.com/apach301",
    "Deleted lines": 0,
    "Added lines": 10,
    "Changed lines": 10,
    "Deleted code": "",
    "Added code": "      TORCH_CHECK(\n          stack_.size() >= 3,\n          \"Parsing error: stack doesn't have enough elements\");\n\n\n      TORCH_CHECK(\n          (dict_pos < stack_size) && (key_pos < stack_size) &&\n              (val_pos < stack_size),\n          \"Parsing error: attempted out-of-bounds access while processing SETITEM opcode\");\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7684044b713761abd4f51225dc5d83ce5869562a",
    "Commit message": "Add size check before calling .back() in rpc/script_call.cpp (#94297)\n\nHi!\n\nI've been fuzzing different pytorch modules, and found a crash inside one of them.\n\nSpecifically, I'm talking about a module that processes `script_call` rpc requests and a function `ScriptCall::fromIValues(std::vector<at::IValue>& ivalues)`.\n\nRunning this test case causes a crash that occurs when `ivalues.back()` is called [script_call.cpp:90](https://github.com/pytorch/pytorch/blob/abc54f93145830b502400faa92bec86e05422fbd/torch/csrc/distributed/rpc/script_call.cpp#L90). The crash occurs because the vector `ivalues` is empty.\n\nAll tests were performed on this pytorch version: [abc54f93145830b502400faa92bec86e05422fbd](https://github.com/pytorch/pytorch/tree/abc54f93145830b502400faa92bec86e05422fbd)\n\nThe provided patch checks if there are enough elements in the ivalues vector.\n\n### How to reproduce\n\n1. To reproduce the crash, use provided docker: [Dockerfile](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch)\n\n2. Build the container: `docker build -t oss-sydr-fuzz-pytorch-reproduce .`\n\n3. Copy crash file to the current directory:\n\n    - [crash-9f76d4e37a2391136a4ce07d47269db1e063e4b4.zip](https://github.com/pytorch/pytorch/files/10674059/crash-9f76d4e37a2391136a4ce07d47269db1e063e4b4.zip)\n\n4. Run the container: ``docker run --privileged --network host -v `pwd`:/homedir --rm -it oss-sydr-fuzz-pytorch-reproduce /bin/bash``\n\n5. And execute the binary: `/message_deserialize_fuzz /homedir/crash-9f76d4e37a2391136a4ce07d47269db1e063e4b4`\n\nAfter execution completes you will see this stacktrace:\n\n```asan\nAddressSanitizer:DEADLYSIGNAL\n=================================================================\n==57==ERROR: AddressSanitizer: SEGV on unknown address (pc 0x0000008e7b19 bp 0x7ffd2fdded70 sp 0x7ffd2fddec40 T0)\n==57==The signal is caused by a READ memory access.\n==57==Hint: this fault was caused by a dereference of a high value address (see register values below).  Disassemble the provided pc to learn which register was used.\n    #0 0x8e7b19 in c10::IValue::isString() const /pytorch_fuzz/aten/src/ATen/core/ivalue.h:639:27\n    #1 0x8e7b19 in c10::IValue::toStringRef[abi:cxx11]() const /pytorch_fuzz/aten/src/ATen/core/ivalue_inl.h:2179:3\n    #2 0xe04fb58 in torch::distributed::rpc::ScriptCall::fromIValues(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/distributed/rpc/script_call.cpp:90:53\n    #3 0xe0511f0 in torch::distributed::rpc::ScriptCall::fromMessage(torch::distributed::rpc::Message const&) /pytorch_fuzz/torch/csrc/distributed/rpc/script_call.cpp:133:10\n    #4 0xe0ff71e in torch::distributed::rpc::deserializeRequest(torch::distributed::rpc::Message const&) /pytorch_fuzz/torch/csrc/distributed/rpc/utils.cpp:102:14\n    #5 0x602a41 in LLVMFuzzerTestOneInput /message_deserialize_fuzz.cc:192:27\n    #6 0x52ce61 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n    #7 0x516d7c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n    #8 0x51cacb in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n    #9 0x546062 in main /llvm-project/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n    #10 0x7f41e42a8082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082)\n    #11 0x51169d in _start (/message_deserialize_fuzz+0x51169d)\n\nAddressSanitizer can not provide additional info.\nSUMMARY: AddressSanitizer: SEGV /pytorch_fuzz/aten/src/ATen/core/ivalue.h:639:27 in c10::IValue::isString() const\n==57==ABORTING\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94297\nApproved by: https://github.com/ezyang",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "  TORCH_INTERNAL_ASSERT(\n      ivalues.size() > 1,\n      \"At least 2 IValues are required to build a ScriptCall.\");\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c69b3b8d4f484cf537d98974a3a4143b77edf3c8",
    "Commit message": "[CUDA12] Autograd engine use current device only (#92354)\n\nThis is a device agnostic version #91191.\nThe reason of existence of this PR is device agnostic policy of autograd engine. Hence, the compile time `USE_CUDA` is not supported, so doing something like:\nhttps://github.com/pytorch/pytorch/blob/fa1ea9f9bcaa77c1370468059be95ad9b421f500/torch/csrc/autograd/engine.cpp#L351-L357\nis not effective.\n\nIn this PR a check upon CUDA devices in device registry is added such that threads set the same CUDA device.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92354\nApproved by: https://github.com/albanD, https://github.com/ngimel",
    "Deleted lines": 9,
    "Added lines": 5,
    "Changed lines": 14,
    "Deleted code": "\n#if defined(USE_CUDA)\n  if (at::detail::getCUDAHooks().hasPrimaryContext(device)) {\n    set_device(device);\n  }\n#else\n  set_device(device);\n#endif\n      if (impl && device < impl->deviceCount()) {",
    "Added code": "  worker_device = device;\n      set_device(worker_device);\n\n      if (impl && device < impl->deviceCount() &&\n          impl->getDevice().index() != device) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fb25fd6f865ed0532caf710ca130b6cc23a772a8",
    "Commit message": "[DTensor] Replaced neg dim normalization with assert in helper (#114141)\n\nThis is a replacement for https://github.com/pytorch/pytorch/pull/113922. I think we can still leave the check for negative shard dimension in `compute_local_shape_and_global_offset` and replace the normalization logic with an assert. This should provide us a stack trace to see which user-facing API did not normalize the dim as expected.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114141\nApproved by: https://github.com/wanchaol\nghstack dependencies: #113919, #113924, #114134, #113925, #113930",
    "Deleted lines": 2,
    "Added lines": 4,
    "Changed lines": 6,
    "Deleted code": "                # normalize shard dim to be positive\n                shard_placement.dim += len(tensor_shape)",
    "Added code": "                raise AssertionError(\n                    \"Shard placements should have negative dims normalized in \"\n                    f\"the user-facing APIs: {shard_placement}\"\n                )"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9e314bd8224f93b4ba1f9e4c065150e47a2de2cc",
    "Commit message": "[dtensor] handle the case where output of op is Optional[Tensor] (#90241)\n\nObserved by @aazzolini, some op might have Optional[Tensor] returns\nwhere it return None (i.e. native_layer_norm_backward), it's a mismatch\nbetween C++ aten op signature and python None, but we need to handle it\nin the python side\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90241\nApproved by: https://github.com/aazzolini",
    "Deleted lines": 1,
    "Added lines": 6,
    "Changed lines": 7,
    "Deleted code": "OutputSpecType = Optional[Union[DTensorSpec, Sequence[DTensorSpec]]]",
    "Added code": "OutputSpecType = Optional[Union[DTensorSpec, Sequence[Optional[DTensorSpec]]]]\n\n        # NOTE: local results might return Optional Tensor from ATen op, so we need to\n        # handle that case and make sure we don't wrap None with DTensor.\n        # (i.e. native_layer_norm.backward)\n            if e is not None and s is not None else None"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/55c19a3c6d38a49fe34e008c4c566445c43810f0",
    "Commit message": "Inductor: Increase multiplier to 3 for Inductor AMP benchmark correctness check (#109097)\n\n**Summary**\nAs reported in https://github.com/pytorch/pytorch/issues/108333, we find some of the models have failed the benchmark's correctness check. However, the end-to-end model's accuracy ([test script](https://gist.github.com/leslie-fang-intel/aac8b3c2b450532fd0517c758bb845e0)) when comparing AMP with FP32 is within a difference of less than 0.1%. Thus, it's possible that the correctness check failures for these models are false alarms. We use multiplier of 3 instead of 2 in this PR to avoid these false alarms. Model end-to-end accuracy test results are:\n\n<html xmlns:v=\"urn:schemas-microsoft-com:vml\"\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\nxmlns:x=\"urn:schemas-microsoft-com:office:excel\"\nxmlns=\"http://www.w3.org/TR/REC-html40\">\n\n<head>\n\n<meta name=ProgId content=Excel.Sheet>\n<meta name=Generator content=\"Microsoft Excel 15\">\n<link id=Main-File rel=Main-File\nhref=\"file:///C:/Users/jiahaofa/AppData/Local/Temp/msohtmlclip1/01/clip.htm\">\n<link rel=File-List\nhref=\"file:///C:/Users/jiahaofa/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml\">\n</head>\n\n<body link=\"#0563C1\" vlink=\"#954F72\">\n\nSPR | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n-- | -- | -- | -- | -- | -- | --\n\u00a0 | FP32 Imperative TOP1 Accuracy | FP32 Imperative TOP5 Accuracy | BF16 AMP Inductor TOP1 Accuracy | BF16 AMP Inductor TOP5 Accuracy | BF16/FP32 Relative Loss TOP1 Accuracy | BF16/FP32 Relative Loss TOP5 Accuracy\ngluon_inception_v3 | 73.262 | 90.774 | 73.256 | 90.802 | -0.01% | 0.03%\nmobilenetv2_100 | 72.89 | 90.996 | 72.826 | 90.946 | -0.09% | -0.05%\nmobilenetv3_large_100 | 75.72 | 92.55 | 75.764 | 92.554 | 0.06% | 0.00%\n\n</body>\n\n</html>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109097\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "Deleted lines": 1,
    "Added lines": 7,
    "Changed lines": 8,
    "Deleted code": "                multiplier = 2.0",
    "Added code": "\n                # In the case of using AMP (Automatic Mixed Precision), certain models have\n                # failed the benchmark's correctness check. However, the end-to-end model's\n                # accuracy when comparing AMP with FP32 is within a difference of less than 0.1%.\n                # Thus, it's possible that the correctness check failures for these models are\n                # false alarms. We use multiplier of 3 instead of 2 to avoid these false alarms.\n                multiplier = 3.0 if res.dtype == torch.bfloat16 else 2.0"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6e78592cbb81138ce13ad65a5f549d65b191526c",
    "Commit message": "Added type checking for ExportedProgram (#117231)\n\nFixes #116952\n\nAdded type checking for ExportedProgram in save function. Please review.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117231\nApproved by: https://github.com/avikchaudhuri",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "    if not isinstance(ep, ExportedProgram):\n        raise TypeError(\n            f\"The 'ep' parameter must be an instance of 'ExportedProgram', got '{type(ep).__name__}' instead.\"\n        )\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/442d7d72def17dba46f0b95c55c6a028428be0bc",
    "Commit message": "fixed type checking errors in options.py (#68056)\n\nSummary:\nFixes [issue#64](https://github.com/MLH-Fellowship/pyre-check/issues/64)\nThis PR fixes the type checking errors in torch/distributed/rpc/options.py.\nThe variable types in 84:8 and 85:8 were  declared to have type `List`  but were sometimes assigned a value of  `None`. This caused an incompatitble variable type error. Therefore, I changed the type from `List` to `Optional[List]` . Hence, this fixes the incompatitble variable type error.\n\nSigned-off-by: Onyemowo  Agbo\nonionymous\n0xedward\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68056\n\nReviewed By: zou3519\n\nDifferential Revision: D32282289\n\nPulled By: mrshenli\n\nfbshipit-source-id: ee410165e623834b4f5f3da8d44bd5a29306daae",
    "Deleted lines": 16,
    "Added lines": 15,
    "Changed lines": 31,
    "Deleted code": "from torch._C._distributed_rpc import _TensorPipeRpcBackendOptionsBase\nfrom . import constants as rpc_contants\n\nfrom typing import Dict, List, Optional, Union\ndef _to_device_map(device_map: Dict[DeviceType, DeviceType]) -> Dict[torch.device, torch.device]:\n    full_device_map : Dict[torch.device, torch.device] = {}\n    reverse_map : Dict[torch.device, torch.device] = {}\n\n        _transports: List = None,\n        _channels: List = None,\n            {} if device_maps is None else\n            {k : _to_device_map(v) for k, v in device_maps.items()}\n        )\n        full_device_list = (\n            [] if devices is None else\n            _to_device_list(devices)",
    "Added code": "from typing import Dict, List, Optional, Union\nfrom torch._C._distributed_rpc import _TensorPipeRpcBackendOptionsBase\nfrom . import constants as rpc_contants\ndef _to_device_map(\n    device_map: Dict[DeviceType, DeviceType]\n) -> Dict[torch.device, torch.device]:\n    full_device_map: Dict[torch.device, torch.device] = {}\n    reverse_map: Dict[torch.device, torch.device] = {}\n\n        _transports: Optional[List] = None,\n        _channels: Optional[List] = None,\n            {}\n            if device_maps is None\n            else {k: _to_device_map(v) for k, v in device_maps.items()}\n        full_device_list = [] if devices is None else _to_device_list(devices)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/828a6a3b3943a0a0701ecacacd2bcc34fc03fe03",
    "Commit message": "Use proper isnan check\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/18663\n\nDifferential Revision: D14699385\n\nPulled By: bddppq\n\nfbshipit-source-id: 596ad3371e7704802591e49f7e1c55dc6cd2896f",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  ((x != x && y == y) || (x > y))",
    "Added code": "  ((th_isnan(x) && !(th_isnan(y))) || (x > y))"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/47c531b6e80e36282dbaec60d239ae1b9f816f43",
    "Commit message": "[jit] Compare object identity first in ClassType::operator== (#65347)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65347\n\nThis check is much cheaper than anything involving actually inspecting object fields (i.e., the cost is low), and if it succeeds we can skip the expensive (e.g., it involves locking a weak_ptr and then destroying the resulting shared_ptr)  function body. It almost entirely eliminates time spent in this function during model loading according to perf.\nghstack-source-id: 140148561\n\nTest Plan: Specifically I profiled static runtime startup for the ctr_mobile_feed model and saw self time in this function go from 2-3% to 0.36%.\n\nReviewed By: ejguan\n\nDifferential Revision: D31057279\n\nfbshipit-source-id: efb6bdc0957b680112ac282e85dc1b06b1b6c0bd",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    if (this == &rhs) {\n      return true;\n    }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3611d26a25bd889627403a808ea667ac99c09904",
    "Commit message": "[JIT] Optimize FunctionSchema::checkArg for the Tensor case. (#48034)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48034\n\nThe Tensor case is one of the most common and the existing check can be\nmade faster. This results in a ~21% improvement on DeepAndWide model and\nwould improve other models as well.\n\nBefore the change:\n```\n505[ms]\n491[ms]\n514[ms]\n538[ms]\n514[ms]\n554[ms]\n556[ms]\n512[ms]\n516[ms]\n527[ms]\n```\n\nAfter the change:\n```\n406[ms]\n394[ms]\n414[ms]\n423[ms]\n449[ms]\n397[ms]\n410[ms]\n389[ms]\n395[ms]\n414[ms]\n```\n\nDifferential Revision: D24999486\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 7139a3a38f9c44e8ea793afe2fc662ff51cc0460",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "  if (value.isTensor() && argument.type() == TensorType::get()) {\n    // Fast-path for the common case\n    return;\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/324dc1623e2f91892038fb1b151450a7c6529dd9",
    "Commit message": "add dtype checking for gather and scatter (#38025)\n\nSummary:\nFixed https://github.com/pytorch/pytorch/issues/37996\n\nin the `cpu_scatter_gather_base_kernel`, it interpret a pointer as `int64_t` regardless the actual dtype.\nhttps://github.com/pytorch/pytorch/blob/2b41b9bceb01851df83d40c1280b3d3b09e1395b/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp#L106\nadd a index dtype checking will avoid the nasty index out of bound error. As using `int64_t` is convention in ATen code (a.k.a, a limitation), no further fix is needed at the moment.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38025\n\nDifferential Revision: D21498146\n\nPulled By: ezyang\n\nfbshipit-source-id: b1f96f394a460c4bc63d21ec8d4a2cfbf3e97b03",
    "Deleted lines": 0,
    "Added lines": 8,
    "Changed lines": 8,
    "Deleted code": "",
    "Added code": "  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"gather_out(): Expected dtype int64 for index\");\n  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"gather(): Expected dtype int64 for index\");\n  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter_(): Expected dtype int64 for index\");\n  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter_fill_(): Expected dtype int64 for index\");\n  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter(): Expected dtype int64 for index\");\n  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter(): Expected dtype int64 for index\");\n  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter_add_(): Expected dtype int64 for index\");\n  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter_add(): Expected dtype int64 for index\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5e50993be72bec4ad939993328dd02691ef7777d",
    "Commit message": "Better type checking for pack_padded_sequence symbolic (#7874)",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "        if lengths.type().kind() != 'TensorType':\n            raise RuntimeError(\"Lengths must be a Tensor for ONNX export\")\n        # We know it's a TensorType so this check is now safe.\n        if lengths.type().scalarType() != 'Int':\n            raise RuntimeError(\"ONNX export requires that the lengths passed \"\n                               \"to pack_padded_sequence must be of type Int\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6c91610f0c86153232bf3a66f3a23e42b96e79b6",
    "Commit message": "Check if profiler is disabled in push/pop event (#18908)\n\nSummary:\nMake sure to check if profiler is disabled in push/pop and mark event\nfunctions\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18908\n\nDifferential Revision: D14791931\n\nPulled By: ilia-cher\n\nfbshipit-source-id: e4f5149e69999ee2b9238c21cccad6d27c6a714a",
    "Deleted lines": 0,
    "Added lines": 9,
    "Changed lines": 9,
    "Deleted code": "",
    "Added code": "  if (state == ProfilerState::Disabled) {\n    return;\n  }\n  if (state == ProfilerState::Disabled) {\n    return;\n  }\n  if (state == ProfilerState::Disabled) {\n    return;\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/68ad9ae5bebd9efab127fa99e2bafd6852bbd8ed",
    "Commit message": "Ensure there aren't variables in checked_tensor_unwrap, checked_tenso\u2026 (#15105)\n\nSummary:\n\u2026r_list_unwrap.\n\nThese functions use unsafeGetTensorImpl(), which doesn't work with Variables (in a silent way that may blow up later).\nSo let's do early checking.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/15105\n\nReviewed By: ezyang\n\nDifferential Revision: D13429149\n\nPulled By: gchanan\n\nfbshipit-source-id: b85f6f5b7cdb9a6dd0c40205b924c840a3920ba0",
    "Deleted lines": 1,
    "Added lines": 8,
    "Changed lines": 9,
    "Deleted code": "               \" for sequence elment \", i , \" in sequence argument at position #\", pos, \" '\", name, \"'\");",
    "Added code": "  if (expr.is_variable()) {\n    AT_ERROR(\"Expected Tensor (not Variable) for argument #\", pos, \" '\", name, \"'\");\n  }\n               \" for sequence element \", i , \" in sequence argument at position #\", pos, \" '\", name, \"'\");\n    }\n    if (expr.is_variable()) {\n      AT_ERROR(\"Expected Tensor (not Variable) for sequence element \",\n               i , \" in sequence argument at position #\", pos, \" '\", name, \"'\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/df475aa1dc4310abc273cf26b14b6ac1cdb7dfa4",
    "Commit message": "Update Vulkan runner in benchmark binary to handle non-tensor inputs (#66123)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66123\n\nSome models may take in a list of tensors as inputs, thus the bundled inputs will contain `IValues` that are of the type `c10::List`. For Vulkan models, every tensor in the `IValue` list has to be converted to a vulkan tensor first, and this case is not currently handled by the Vulkan model wrapper in the benchmark binary.\n\nThis diff introduces `IValue` type checking to the input processor of the Vulkan model wrapper, and adds support for Tensor and List types.\n\nTest Plan:\n```\n# Build the binary\ncd ~/fbsource\nbuck build -c ndk.custom_libcxx=false -c pt.enable_qpl=0 //xplat/caffe2:ptmobile_compareAndroid\\#android-arm64 --show-output\n# Push it to the device\nadb push buck-out/gen/xplat/caffe2/ptmobile_compareAndroid\\#android-arm64 /data/local/tmp/compare_models\n\n# Run the benchmark binary\nBENCH_CMD=\"/data/local/tmp/compare_models\"\nBENCH_CMD+=\" --model=$PATH_TO_MODEL\"\nBENCH_CMD+=\" --refmodel=$PATH_TO_REFERENCE_MODEL\"\nBENCH_CMD+=\" --input_type=float --input_dims=$MODEL_INPUT_SIZE\"\nBENCH_CMD+=\" --iter=100\"\nBENCH_CMD+=\" --tolerance 1e-5\"\n```\n\nReviewed By: beback4u\n\nDifferential Revision: D31276862\n\nfbshipit-source-id: 1d9abf958963da6ecad641202f0458402bee5ced",
    "Deleted lines": 1,
    "Added lines": 21,
    "Changed lines": 22,
    "Deleted code": "      inputs_.emplace_back(input.toTensor().vulkan());",
    "Added code": "      if (input.isTensor()) {\n        inputs_.emplace_back(input.toTensor().vulkan());\n      }\n      else if (input.isList()) {\n        const c10::List<c10::IValue> input_as_list = input.toList();\n        c10::List<at::Tensor> input_vk_list;\n        input_vk_list.reserve(input_as_list.size());\n        for (int i=0; i < input_as_list.size(); ++i) {\n          const c10::IValue element = input_as_list.get(i);\n          if (element.isTensor()) {\n            input_vk_list.emplace_back(element.toTensor().vulkan());\n          }\n          else {\n            CAFFE_THROW(\"Input of type c10::List must only contain Tensors!\");\n          }\n        }\n        inputs_.emplace_back(c10::IValue(input_vk_list));\n      }\n      else {\n        CAFFE_THROW(\"Inputs must only contain IValues of type c10::Tensor or c10::List!\");\n      }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e574a8ab55b2ac4266211d30d98d32d8b849ea86",
    "Commit message": "[dynamo] Add sanity checks to ensure no double-wrapping of `FakeTensor`s produced by the current graph (#111913)\n\nPartially fixes: https://github.com/pytorch/pytorch/issues/111873\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111913\nApproved by: https://github.com/ezyang",
    "Deleted lines": 0,
    "Added lines": 12,
    "Changed lines": 12,
    "Deleted code": "",
    "Added code": "    def assert_not_wrapped_by_this_graph(self, value: torch.Tensor):\n        if is_fake(value) and maybe_get_fake_mode(value) is self.tx.fake_mode:\n            raise InternalTorchDynamoError(\n                \"Cannot wrap a Tensor that has already been\",\n                \"wrapped by this instance of Dynamo\",\n            )\n\n            self.assert_not_wrapped_by_this_graph(value)\n            self.assert_not_wrapped_by_this_graph(value)\n        # By this point, we should have deduplicated all tensors\n        self.assert_not_wrapped_by_this_graph(value)\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/468a73f0e3527c52495c864c7d48dc26684f6c0b",
    "Commit message": "Support Numpy ints in the `torch.nn.functional.interpolate` dtype check (#110778)\n\nIn https://github.com/pytorch/pytorch/pull/99243, a check was added to ensure the `size` only contained integers.\n\nThis PR updates the check to also include numpy integers based on this comment (cc @kit1980): https://github.com/pytorch/pytorch/pull/99243#issuecomment-1646736646. Similar to the other commenter, I also ran into issues where existing software broke due to this after upgrading to PT2.1:\n\n```\n                if not torch.jit.is_scripting():\n                    if not all(_is_integer(x) for x in size):\n>                       raise TypeError(\n                            \"expected size to be one of int or Tuple[int] or Tuple[int, int] or \"\n                            f\"Tuple[int, int, int], but got size with types {[type(x) for x in size]}\"\n                        )\nE                       TypeError: expected size to be one of int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], but got size with types [<class 'numpy.int64'>, <class 'numpy.int64'>]\n\n/conda-env/lib/python3.8/site-packages/torch/nn/functional.py:3924: TypeError\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110778\nApproved by: https://github.com/mikaylagawarecki",
    "Deleted lines": 1,
    "Added lines": 8,
    "Changed lines": 9,
    "Deleted code": "    Will return True for int, SymInt and Tensors with integer elements.",
    "Added code": "try:\n    import numpy as np\nexcept ModuleNotFoundError:\n    np = None\n\n    Will return True for int, SymInt, Numpy integers and Tensors with integer elements.\n    if np is not None and isinstance(x, np.integer):\n        return True"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e31038d574712d383fdc4c2f1bb63fc82f256ed0",
    "Commit message": "Check results dtype in index_out (#108167)\n\nThis logic exists for index_put and index_add, but for some reason not for `index.out`\nSkip testing, as this function is not technically exposed on the Python level.\n\n<!--\ncopilot:poem\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at c688cfd</samp>\n\n> _`index_out` checks types_\n> _avoiding errors in autumn_\n> _complex tensors work_\n\nFixes https://github.com/pytorch/pytorch/issues/107698\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108167\nApproved by: https://github.com/albanD",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    TORCH_CHECK(self.scalar_type() == result.scalar_type(),\n                \"index_out: self (\", self.scalar_type(), \") and result (\", result.scalar_type(),\n                \") must have the same scalar type\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a69f427f957a37eee9c1dd5df681f30ab38ed3e4",
    "Commit message": "aten: Ensure dim is size_t (#104201)\n\nAttempts to fix failures introduced in https://github.com/pytorch/pytorch/pull/103930 (example failures: https://github.com/pytorch/pytorch/actions/runs/5363450214/jobs/9731034104)\n\n<!--\ncopilot:all\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at 67d5076</samp>\n\n### Summary\n\ud83d\udd27\ud83d\udea8\ud83d\udea6\n\n<!--\n1.  \ud83d\udd27 (wrench) - This emoji can be used to indicate a bug fix or a minor improvement to the code quality or performance.\n2.  \ud83d\udea8 (rotating light) - This emoji can be used to indicate a change that affects the error handling or validation logic of the code, or that adds or modifies a test case.\n3.  \ud83d\udea6 (vertical traffic light) - This emoji can be used to indicate a change that affects the control flow or branching logic of the code, or that adds or modifies a condition or assertion.\n-->\nFix a compiler warning in `Expand.cpp` by casting a tensor dimension to `size_t`. This improves the code quality and correctness of the `expand` function for the Vulkan backend.\n\n> _`expand` tensor_\n> _cast `dim()` to `size_t`_\n> _autumn leaves warning_\n\n### Walkthrough\n*  Cast `self.dim()` to `size_t` to avoid signed-unsigned comparison warning in `expand` function ([link](https://github.com/pytorch/pytorch/pull/104201/files?diff=unified&w=0#diff-c175e908cbcb8595b22696e672b526202ed3a4a11341603c1522397e499b5c2bL29-R29))\n\n<details>\n<summary> Fix done using chatgpt </summary>\n\n![Screenshot 2023-06-26 at 11 52 14 AM](https://github.com/pytorch/pytorch/assets/1700823/95c141e5-36b6-4916-85ca-85415bcc507f)\n\n</details>\nSigned-off-by: Eli Uriegas <eliuriegas@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104201\nApproved by: https://github.com/lucylq, https://github.com/huydhn, https://github.com/malfet",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "      self.dim() <= output_size.size(),",
    "Added code": "      static_cast<size_t>(self.dim()) <= output_size.size(),"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8340762211e3b55caa178bac748bd902249f6fc0",
    "Commit message": "Update lr_scheduler.py to check the type of eta_min (#97003)\n\nAdd float assertion to `eta_min` parameter in `CosineAnnealingWarmRestarts`.\n\nFixes #87757\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97003\nApproved by: https://github.com/janeyx99",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "        if not isinstance(eta_min, (float, int)):\n            raise ValueError(\"Expected float or int eta_min, but got {} of type {}\".format(eta_min, type(eta_min)))"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2dafa70d61a1a5af849ab79c7aed4c84686337a0",
    "Commit message": "Add a little more error checking to minifier (#103057)\n\nPrompted by https://github.com/pytorch/pytorch/issues/101408\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103057\nApproved by: https://github.com/bdhirsh",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "    def __post_init__(self):\n        ph_nodes = get_placeholders(self.graph)\n        assert len(ph_nodes) == len(self.inps)\n\n    assert isinstance(inps, (tuple, list))\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ae55619a2bb73ebcdc80b02a6ccd72275a9ce23e",
    "Commit message": "Add check for same dtype in tensordot implementation (#98938)\n\nFixes #77517\n\nI believe[ the first bullet point in this comment](https://github.com/pytorch/pytorch/issues/77517#issuecomment-1129233539) from the linked issue is no longer a concern, but please let me know if I'm incorrect here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98938\nApproved by: https://github.com/lezcano",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "  TORCH_CHECK(input1.scalar_type() == input2.scalar_type(), \"both inputs should have same dtype\");"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b6920405da340bbd3397b80bf16d9c360b0c48d2",
    "Commit message": "reorder checks to shave 1 us off no-op dispatch time\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78316\n\nApproved by: https://github.com/Chillee, https://github.com/ezyang",
    "Deleted lines": 5,
    "Added lines": 5,
    "Changed lines": 10,
    "Deleted code": "  return (\n    !THPVariable_CheckTypeExact(tp) &&\n    // TODO: test if Python key is disabled\n    attr.ptr() != nullptr &&\n    attr.ptr() != torch::disabled_torch_dispatch_impl()",
    "Added code": "  if (THPVariable_CheckTypeExact(tp)) {\n    return false;\n  }\n  return (attr.ptr() != nullptr &&\n          attr.ptr() != torch::disabled_torch_dispatch_impl()"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/152f665dee05377f7459d985d60dc1edb782d40e",
    "Commit message": "Inserted check for PyObject_IsInstance in THPVariableCheck (#67588)\n\nSummary:\nInserted check for the return of PyObject_IsInstance to capture the case in which it raises an exception and return -1. When this happen THPVariable_Check now throws a python_error to signal the exception.\n\nFixes https://github.com/pytorch/pytorch/issues/65084\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67588\n\nReviewed By: mruberry\n\nDifferential Revision: D32064776\n\nPulled By: albanD\n\nfbshipit-source-id: 895c7682e0991ca257e27f9638a7462d83707320",
    "Deleted lines": 1,
    "Added lines": 8,
    "Changed lines": 9,
    "Deleted code": "  return THPVariableClass && PyObject_IsInstance(obj, THPVariableClass);",
    "Added code": "#include <torch/csrc/Exceptions.h>\n  if (!THPVariableClass)\n      return false;\n\n  const auto result = PyObject_IsInstance(obj, THPVariableClass);\n  if (result == -1)\n      throw python_error();\n  return result;"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/18eeccc7e8cba16d71efdd2eca831983c4abde15",
    "Commit message": "[mypy] Fix Optional type check (#62668)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/62668\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet, 842974287\n\nDifferential Revision: D30077960\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 5e423bfb65a65974ed848caa177330d6e61452e6",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "                assert self.optimization_profiles"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/402be850a8946e8967dedb3375fc6f33b379b397",
    "Commit message": "[quant] Adding zero point type check for per channel quantization (#40811)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40811\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22319417\n\nPulled By: z-a-f\n\nfbshipit-source-id: 7be3a511ddd33b5fe749a83166bbc5874d1bd539",
    "Deleted lines": 0,
    "Added lines": 8,
    "Changed lines": 8,
    "Deleted code": "",
    "Added code": "  TORCH_CHECK(scale.scalar_type() == ScalarType::Float,\n              \"Scale must be Float, found \", scale.scalar_type());\n  TORCH_CHECK(zero_point.scalar_type() == ScalarType::Long,\n              \"Zero-point must be Long, found \", zero_point.scalar_type());\n  TORCH_CHECK(scale.scalar_type() == ScalarType::Float,\n              \"Scale must be Float, found \", scale.scalar_type());\n  TORCH_CHECK(zero_point.scalar_type() == ScalarType::Long,\n              \"Zero-point must be Long, found \", zero_point.scalar_type());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4dad00b64b396ef81f16bdb896175688fc629f4d",
    "Commit message": "[rpc] special case tensor type check when getting RRef (#33582)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33582\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20009837\n\nPulled By: wanchaol\n\nfbshipit-source-id: 7e9ab87d4dddb822c7575891a2b620eff83bfa00",
    "Deleted lines": 1,
    "Added lines": 21,
    "Changed lines": 22,
    "Deleted code": "    TORCH_INTERNAL_ASSERT(ownerRRef->type() == type);",
    "Added code": "    // We have found the rref through the rrefId\n    // Now double check if the two types are matched\n    //\n    // Why we are special casing the check for tensor type here?\n    // this is because tensor types might get specialized on tensors when\n    // we pass inputs to the function, i.e. TensorType can filled with\n    // specific shape info, requires_grad info, etc. so the OwerRRef we\n    // found might already have those infos, but the `type` we passed in\n    // here is a plain TensorType, they are not equal relationship:\n    // specialized TensorType <: plain TensorType\n    //\n    // In RPC we don't care the difference as we ser/de with just the\n    // plain TensorType. This is not a issue for UserRRef creation either,\n    // since Tensor can only get specialized with a previous run of local\n    // JIT function, and we shouldn't preserve the specialized SubTensorType\n    // information on other workers because it's only information only.\n    if(type == TensorType::get()) {\n      TORCH_INTERNAL_ASSERT(ownerRRef->type()->isSubtypeOf(TensorType::get()));\n    } else {\n      TORCH_INTERNAL_ASSERT(ownerRRef->type() == type);\n    }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4cc6e6bbbe1fb114e7d7fb207ef2deb567950102",
    "Commit message": "Adding scalar to the c10 registration type check\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32886\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19673484\n\nPulled By: z-a-f\n\nfbshipit-source-id: ea8478a4fe6788dcb044ec1ab7d51dc50ab3fa60",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "          } else if (type->kind() == TypeKind::NumberType) {\n            tracer::addInputs(node, args[i].name().c_str(), iter->toScalar());"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4b1ebd2f65e49d251ac2cfdb635794c7c6eb362f",
    "Commit message": "Fast path for serializing large floating-point tensors to protobuf\n\nSummary: Our existing serialization routines take a significant amount of time for large numpy arrays in order to verify the type of each element in the array as well as converting each element to a canonical type.  For large floating-point tensors, such as model parameters, this checking and converting takes a significant amount of time.  Adding a fast track path for just float32 arrays as this is the most common use case to worry about.\n\nReviewed By: akyrola\n\nDifferential Revision: D5389953\n\nfbshipit-source-id: 26f44cb2426ea3efb849e7707b27d5485f69956c",
    "Deleted lines": 0,
    "Added lines": 8,
    "Changed lines": 8,
    "Deleted code": "",
    "Added code": "    # Fast tracking common use case where a float32 array of tensor parameters\n    # needs to be serialized.  The entire array is guaranteed to have the same\n    # dtype, so no per-element checking necessary and no need to convert each\n    # element separately.\n    if isinstance(value, np.ndarray) and value.dtype.type is np.float32:\n        argument.floats.extend(value.flatten().tolist())\n        return argument\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/65dfe1203ffab064d4e32fa8f76833042369d2f5",
    "Commit message": "add an assertion to check the param num (#18145)\n\nSummary:\nIntroduce this check to see whether it will break any existing workflow\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18145\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D14511711\n\nPulled By: houseroad\n\nfbshipit-source-id: a7bb6ac84c9133fe94d3fe2f1a8566faed14a136",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "    # make sure that the param dict and the graph match each other\n    flatten_args, _ = torch._C._jit_flatten(args)\n    assert len(params) + len(flatten_args) == sum(1 for _ in graph.inputs())\n"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5a962369e2b527b36a737723df1fe9c180aa2925",
    "Commit message": "[Gradient Compression] Check if the backend is NCCL when a DDP communication hook is registered (#51759)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51759\n\nSome unit tests actually register a comm hook on other backends like GLOO. Example: `test_ddp_comm_hook_future_passing_cpu`\n\nTherefore, only do the check on `register_builtin_comm_hook`.\n\nCurrently DDP communication hook can only be supported on NCCL. Add a check in the registration methods.\nghstack-source-id: 121115814\n\nTest Plan: unit tests.\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D26268581\n\nfbshipit-source-id: c739fa4dca6d320202dc6689d790c2761c834c30",
    "Deleted lines": 15,
    "Added lines": 18,
    "Changed lines": 33,
    "Deleted code": "      \"find_unused_parameters=True was specified in DDP constructor, \"\n      \"but did not find any unused parameters. This flag results in an extra \"\n      \"traversal of the autograd graph every iteration, which can adversely \"\n      \"affect performance. If your model indeed never has any unused \"\n      \"parameters, consider turning this flag off. Note that this warning may \"\n      \"be a false positive your model has flow control causing later iterations \"\n      \"to have unused parameters.\"\n    );\n  // TODO(@sinannasir): Single-process multiple-device mode support for DDP\n  // communication hook. Related to GH Issue #42542.\n  const std::string& module_name,\n  const std::vector<int>& device_ids,\n  int output_device,\n  bool broadcast_buffers\n) {",
    "Added code": "        \"find_unused_parameters=True was specified in DDP constructor, \"\n        \"but did not find any unused parameters. This flag results in an extra \"\n        \"traversal of the autograd graph every iteration, which can adversely \"\n        \"affect performance. If your model indeed never has any unused \"\n        \"parameters, consider turning this flag off. Note that this warning may \"\n        \"be a false positive your model has flow control causing later iterations \"\n        \"to have unused parameters.\");\n  // TODO(#42542): Single-process multiple-device mode support for DDP\n  // communication hook.\n  // TODO: Support GLOO and MPI backends for DDP communication hook.\n  TORCH_CHECK(\n      process_group_->getBackendName() == \"nccl\",\n      \"register_builtin_comm_hook currently can only support NCCL backend, but the current backend is %s.\",\n      process_group_->getBackendName());\n    const std::string& module_name,\n    const std::vector<int>& device_ids,\n    int output_device,\n    bool broadcast_buffers) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fa66a1498eb1fac5b36811d5c1d6ba1540ffc824",
    "Commit message": "Simplify _calculate_fan_in_and_fan_out (#29370)\n\nSummary:\nThe code checking `if dimensions == 2` is not needed\nbecause the case of a 2D tensor (Linear) is already handled\nby the statement:\n`receptive_field_size = 1`\nand this conditional:\n`if tensor.dim() > 2:`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29370\n\nDifferential Revision: D18372987\n\nPulled By: albanD\n\nfbshipit-source-id: fcb4dddbc76b9f4414c6d88c0aa2fb4435bf3385",
    "Deleted lines": 11,
    "Added lines": 7,
    "Changed lines": 18,
    "Deleted code": "    if dimensions == 2:  # Linear\n        fan_in = tensor.size(1)\n        fan_out = tensor.size(0)\n    else:\n        num_input_fmaps = tensor.size(1)\n        num_output_fmaps = tensor.size(0)\n        receptive_field_size = 1\n        if tensor.dim() > 2:\n            receptive_field_size = tensor[0][0].numel()\n        fan_in = num_input_fmaps * receptive_field_size\n        fan_out = num_output_fmaps * receptive_field_size",
    "Added code": "    num_input_fmaps = tensor.size(1)\n    num_output_fmaps = tensor.size(0)\n    receptive_field_size = 1\n    if tensor.dim() > 2:\n        receptive_field_size = tensor[0][0].numel()\n    fan_in = num_input_fmaps * receptive_field_size\n    fan_out = num_output_fmaps * receptive_field_size"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4fd98dfe69287914fd29b38fbccaf7ac4d7261ee",
    "Commit message": "Don't only apply DDP optimizer on forward frames (#87097)\n\nPreviously a check would only apply DDP optimizer on frames named \"forward\".\n\nBut on hf_T5_large, a graph break causes some frames like:\n\n```\n<graph break in _shift_right>\n<graph break in forward>\n```\n\nSo instead, apply DDP optimizer on all frames.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87097\nApproved by: https://github.com/wconstab",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "                if ddp_module and frame.f_code.co_name == \"forward\":",
    "Added code": "                if ddp_module:"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8a644f0c136cb12cf200050c2ae6875ec487d174",
    "Commit message": "[Shape Inference] Fix InferFC\n\nSummary: Sometimes first dim of X in FC is BATCH_OF_FEATURE_MAX instead of BATCH. This caused an issue in f207899183 (when first dim of X is 64 but is set to 1 in inferFC). Change the check from `!= BATCH` to `== UNKNOWN`\n\nTest Plan: unit test\n\nReviewed By: yinghai\n\nDifferential Revision: D22784691\n\nfbshipit-source-id: eb66ba361d6fe75672b13edbac2fbd269a7e7a00",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if (x_shape_info.getDimType(0) != TensorBoundShape_DimType_BATCH) {",
    "Added code": "    if (x_shape_info.getDimType(0) == TensorBoundShape_DimType_UNKNOWN) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bdc8b3f3e828ca7202879baa379fda6df5b078d2",
    "Commit message": "[vulkan] Re-route arithmetic ops to scalar versions when second arg is zero-dim (#73108)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73108\n\nWhen arithmetic ops are invoked from torchscript the scalar argument will sometimes be wrapped in a zero-dimensional tensor, which will cause the Vulkan implementation to complain as all input tensors are expected to have the same number of channels. The solution is to have the Tensor implementations of the op check if the second argument is zero-dimensional and re-route it to the Scalar implementation if that's the case.\n\nTest Plan: Imported from OSS\n\nReviewed By: mikaylagawarecki\n\nDifferential Revision: D34354840\n\nPulled By: SS-JIA\n\nfbshipit-source-id: b24799bb3dd4336791a39bea9382c14243ad58e4\n(cherry picked from commit c6dd8eb13b9be3800405c64a3a81e5c68da64355)",
    "Deleted lines": 0,
    "Added lines": 28,
    "Changed lines": 28,
    "Deleted code": "",
    "Added code": "  if (other_arg.sizes().size() == 0) {\n    return arithmetic_scalar(\n        self_arg,\n        other_arg.item<float>(),\n        c10::optional<Scalar>(alpha.to<float>()),\n        VK_KERNEL(add_scalar));\n  }\n  if (other_arg.sizes().size() == 0) {\n    return arithmetic_scalar(\n        self_arg,\n        other_arg.item<float>(),\n        c10::optional<Scalar>(-1 * alpha.to<float>()),\n        VK_KERNEL(add_scalar));\n  }\n  if (other_arg.sizes().size() == 0) {\n    return arithmetic_scalar(\n        self_arg,\n        other_arg.item<float>(),\n        c10::optional<Scalar>(),\n        VK_KERNEL(mul_scalar));\n  }\n  if (other_arg.sizes().size() == 0) {\n    return arithmetic_scalar(\n        self_arg,\n        1.0 / other_arg.item<float>(),\n        c10::optional<Scalar>(),\n        VK_KERNEL(mul_scalar));\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5023995292f5119c447de15c20a375b7e3aa2d0b",
    "Commit message": "fix output size adjustment for onnxifi_op\n\nSummary: this breaks if we cut the net at certain int8 ops boundary.\n\nTest Plan: with net_runner to lower a single Int8Quantize op. It used to break. Now it works.\n\nReviewed By: yinghai\n\nDifferential Revision: D22912178\n\nfbshipit-source-id: ca306068c9768df84c1cfa8b34226a1330e19912",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "      if (max_shape[j] > real_shape.dims(j)) {",
    "Added code": "      if (max_shape[j] >= real_shape.dims(j)) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/24601daa1203a9ad1232e1d18a07ff4842d53d27",
    "Commit message": "Adding check for a single batch in adaptive_avg_pool\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/23137\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D16403804\n\nPulled By: zafartahirov\n\nfbshipit-source-id: df79a8c768ffabeceb4c0044c967a623c5885484",
    "Deleted lines": 4,
    "Added lines": 7,
    "Changed lines": 11,
    "Deleted code": "    if (input.ndimension() == 3)\n      output.resize_({sizeD, osizeH, osizeW});\n\n    if (input.ndimension() == 3)",
    "Added code": "    if (input.ndimension() == 3 || input.size(-4) == 1)\n      if (input.ndimension() == 3) {\n        output.resize_({sizeD, osizeH, osizeW});\n      } else {\n        output.resize_({1, sizeD, osizeH, osizeW});\n      }\n    if (input.ndimension() == 3 || input.size(-4) == 1)"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a1edf5f63c62d88230d1f7feb26edb059551ae71",
    "Commit message": "[EASY] Do hook sizes check with SymInt (#97362)\n\nI don't think this matters for any uses right now, but I found\nit during an audit; might as well fix it.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97362\nApproved by: https://github.com/wconstab",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  if (original.sizes().vec() != result.sizes().vec()) {",
    "Added code": "  if (original.sym_sizes().vec() != result.sym_sizes().vec()) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7553c495147f3e21a1e27d392d277906a47768e7",
    "Commit message": "[S382174] Fix distributed debug w/ non-equal split (#115483)\n\nSummary:\nIn collectives, it's possible to have non-equal split that has a different implementation and the output tensor size will be different, e.g. https://www.internalfb.com/code/fbsource/[460afb1172b5]/fbcode/caffe2/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp?lines=3104. However, TORCH_DISTRIBUTED_DEBUG=DETAIL will assume the output tensor size is the same and does the check and will fail the job if they don't: https://fburl.com/code/mhte9ty8. c10d code should handle this.\n\nIdeally we should check the input size across ranks and make sure they're the same. Maybe for next diff.\n\nTest Plan: Test torchrec's TWRW w/ non-even split and it's working now.\n\nReviewed By: zhangruiskyline\n\nDifferential Revision: D52010942\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115483\nApproved by: https://github.com/kwen2501, https://github.com/fegin, https://github.com/XilunWu",
    "Deleted lines": 2,
    "Added lines": 19,
    "Changed lines": 21,
    "Deleted code": "  runCollectiveChecks(OpType::ALLGATHER, inputTensors);\n  runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);",
    "Added code": "bool check_same_size(const std::vector<at::Tensor>& input_tensors) {\n  for (const auto& input_tensor : input_tensors) {\n    if (!input_tensors[0].is_same_size(input_tensor)) {\n      return false;\n    }\n  }\n  return true;\n}\n\n  if (check_same_size(outputTensors.back())) {\n    runCollectiveChecks(OpType::ALLGATHER, inputTensors);\n  } else {\n    runCollectiveChecks(OpType::ALLGATHER, {});\n  }\n  if (check_same_size(inputTensors.back())) {\n    runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);\n  } else {\n    runCollectiveChecks(OpType::REDUCE_SCATTER, {});\n  }"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/35be57970143236d74661f2415d66d496aab5476",
    "Commit message": "Refactor TENSOR_MATCH guards to check dim (for NT support) (#97896)\n\nTweaks the TENSOR_MATCH guard logic to avoid saving sizes / strides for the case of dynamic shapes. Instead, the dim() is stored, which is enough for both dense tensors and NTs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97896\nApproved by: https://github.com/ezyang",
    "Deleted lines": 12,
    "Added lines": 17,
    "Changed lines": 29,
    "Deleted code": "    auto ndim = v.ndimension();\n    const auto& sizes = v.sizes();\n    const auto& strides = v.strides();\n    sizes_.reserve(ndim);\n    strides_.reserve(ndim);\n    for (auto i : c10::irange(ndim)) {\n      sizes_.emplace_back(sizes[i]);\n      strides_.emplace_back(strides[i]);\n    auto ndim = static_cast<size_t>(v.ndimension());\n    if (ndim != sizes_.size()) {\n    size_t ndim = static_cast<size_t>(v.ndimension());\n    if (ndim != sizes_.size()) {",
    "Added code": "    dim_ = v.ndimension();\n    if (!dynamic_shapes_) {\n      const auto& sizes = v.sizes();\n      const auto& strides = v.strides();\n      sizes_.reserve(dim_);\n      strides_.reserve(dim_);\n      for (auto i : c10::irange(dim_)) {\n        sizes_.emplace_back(sizes[i]);\n        strides_.emplace_back(strides[i]);\n      }\n    auto ndim = v.ndimension();\n    if (ndim != dim_) {\n    auto ndim = v.ndimension();\n    if (ndim != dim_) {\n  // NB: These are unset if dynamic shapes is enabled.\n  // Not strictly required for dense tensors, but nested tensors need it.\n  int64_t dim_;"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8dda19b79f2c4418f481a9f56932b3b5c5afdf39",
    "Commit message": "Remove extraneous TensorId checks in as_strided (#21045)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/21045\nghimport-source-id: e95fbf50bccf6ebc613bb13fb16915254912f22d\n\nDifferential Revision: D15528971\n\nPulled By: li-roy\n\nfbshipit-source-id: c721cc6280dff6e14c5533681d0b35aaa8f98f00",
    "Deleted lines": 6,
    "Added lines": 0,
    "Changed lines": 6,
    "Deleted code": "  TORCH_CHECK(\n      tid == CPUTensorId() || tid == CUDATensorId(),\n      \"as_strided is only implemented for strided CPU, CUDA and QuantizedCPU tensors.\");\n  TORCH_CHECK(\n      tid == QuantizedCPUTensorId(),\n      \"as_strided is only implemented for strided CPU, CUDA and QuantizedCPU tensors.\");",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9d20af50608b146fe1c3296210a05cd8e4c60af2",
    "Commit message": "remove overly restrictive checks for cudagraph (#80881)\n\nFinish fixing https://github.com/pytorch/pytorch/issues/80809\nPull Request resolved: https://github.com/pytorch/pytorch/pull/80881\nApproved by: https://github.com/jbschlosser",
    "Deleted lines": 5,
    "Added lines": 0,
    "Changed lines": 5,
    "Deleted code": "        else:\n            assert not step_t.is_cuda, \"If capturable=False, state_steps should not be CUDA tensors.\"\n    else:\n        assert all(not step.is_cuda for step in state_steps), \\\n            \"If capturable=False, state_steps should not be CUDA tensors.\"",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f6e137598ddf0b990c423b1d6412502b62e095b2",
    "Commit message": "ns for fx: fix nit in default qlinear weight extraction function (#62334)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/62334\n\nRemoves the assert for node type in default qlinear weight extraction\nfunction. Without the assert, user defined functions can now use\nthis util function without failing this check.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n\n// further tests will be in follow-up fb-only diffs\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D29963501\n\nfbshipit-source-id: a634eabb5165375bde186438318ec52fa29c970f",
    "Deleted lines": 1,
    "Added lines": 0,
    "Changed lines": 1,
    "Deleted code": "    assert node.target in (toq.linear, toq.linear_relu)",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5a20c56ebce3426397210e91693fbbeade8b46ba",
    "Commit message": "[static runtime] Remove hasOperation() check (#61496)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61496\n\nglow::FusionGroup is JitOnlyOperator that produces an Operation when passed a Node* https://fburl.com/ybwfn3bl\n\nhasOperation doesn't return true in that case https://fburl.com/19wd10aw\n\nby removing the hasOperation() check, the Operation gets successfully materialized, and static runtime enables successfully and runs ok. Will check that the outputs match with jit interpreter\n\nTest Plan:\nTest with 281805158_2\n```\n./buck-out/gen/admarket/lib/ranking/prediction_replayer/replayer --model_inference_type_target=DISAGG_ACCELERATOR --prediction_replayer_force_model_type=inline_cvr_post_imp_model --prediction_replayer_force_model=281805158_2 --prediction_replayer_target_tier=127.0.0.1:7447 --prediction_replayer_input_stream_filename=/data/users/ansha/tmp/adfinder/filter_requests_inline_cvr_post_imp_model_1000_2021_04_29 --ignore_model_id_mismatch --check_performance --fully_remote_sr_connection_options=\"overall_timeout:10000000,processing_timeout:10000000\" --use_new_encoding_for_ads_services --use_new_encoding_from_model_id_to_shard_id --sigrid_force_model_dir=/data/users/ansha/tmp/adfinder/281805158_2/ --sigrid_predictor_model_suffix=.predictor.disagg.local \u2014use_new_encoding_from_model_id_to_shard_id=true --prediction_replayer_force_model_kind=19 --pytorch_predictor_static_runtime_enable=true --prediction_replayer_target_qps=1\n```\n\n```\nNNPI_LOG_LEVEL=0 USE_INF_API=1 ./buck-out/gen/sigrid/predictor/sigrid_remote_predictor_glow_nnpi \\\n  --force_models=281805158_2 \\\n  --sigrid_predictor_model_suffix=.predictor.disagg.remote_other \\\n  --gflags_config_path=sigrid/predictor/gflags/predictor_gflags_ads_perf_glow_nnpi_pyper_v1 \\\n  --smc_server_port=7447 \\\n  --sigrid_predictor_tier_name=sigrid.predictor.perf.dianshi_staticruntime_debug_0604.test.storage \\\n  --predictor_storage_smc_tier=sigrid.predictor.perf.dianshi_staticruntime_debug_0604.test.storage \\\n  --predictor_storage_smc_tier_v2=sigrid.predictor.perf.dianshi_staticruntime_debug_0604.test.storage \\\n  --torch_glow_min_fusion_group_size=30 \\\n  --glow_enable_sanitize_inputs=100 \\\n  --sigrid_force_model_dir=/data/users/ansha/tmp/adfinder/281805158_2/ \\\n  --pytorch_predictor_static_runtime_enable=true \\\n  --pytorch_predictor_glow_enable=true \\\n  --pytorch_predictor_enable_loading_xl_format_on_cpu=false \\\n  --pytorch_disagg_acc_input_dump_path=/tmp/\n```\n\nReviewed By: hlu1\n\nDifferential Revision: D29647043\n\nfbshipit-source-id: 8ce6dc0f4f0464b65ca6a8c9d42e3d8bb392e66e",
    "Deleted lines": 1,
    "Added lines": 0,
    "Changed lines": 1,
    "Deleted code": "    TORCH_CHECK(op.hasOperation());",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/012829eb3657aff2d58cead0bd166089c6e90c7f",
    "Commit message": "[Lazy][JIT] Do not crash when target device is unsupported by fuser (#73820)\n\nSummary:\nThe `canFuseOnDevice` function now crashes when the device is not covered (i.e., CPU, GPU, XPU). However, now we have some devices, such as XLA and Lazy, that could perform fusion by themselves. This checker then prevents these devices from working on the models partially implemented in `jit.script`.\n\nThis PR proposes to remove this checker and simply return false for all uncovered cases. Another alternative is adding the following logic if it is unsafe to simply remove the checker:\n```\nelse if (device-> type() == DeviceType::XLA || device-> type() == DeviceType::Lazy) {\n  return false;\n} else {\n  TORCH_CHECK_NOT_IMPLEMENTED(false, \"Unknown device for tensorexpr fuser\")\n}\n```\n\ncc wconstab\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73820\n\nReviewed By: navahgar\n\nDifferential Revision: D34731314\n\nPulled By: wconstab\n\nfbshipit-source-id: 1c0a90dcd6c67803a27fa2f305d78b2a539f3604\n(cherry picked from commit 94c61d6c0a9c2ef6e0d046f7f06a7158b43d4d61)",
    "Deleted lines": 2,
    "Added lines": 1,
    "Changed lines": 3,
    "Deleted code": "    } else {\n      TORCH_CHECK_NOT_IMPLEMENTED(false, \"Unknown device for tensorexpr fuser\")",
    "Added code": "    return false;"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5017c5fcad5889acba1623c634d0aaeadb676aa0",
    "Commit message": "[SPMD] Remove _specify_ddp_gpu_num method (#56425)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56425\n\nAs SPMD mode is gone, `_specify_ddp_gpu_num` becomes useless. It only checks if the module is a GPU module. This actually is already checked by the caller of this function (in fairscale and some other codebases).\n\nAdditionally also remove `enable_pytorch_sync_bn` wrapper that only calls this function and does nothing else.\nghstack-source-id: 126885376\n\nTest Plan: waitforbuildbot\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27866440\n\nfbshipit-source-id: d2fd5cf43eda25c0a2bd35f647848ec0dbd6ad0f",
    "Deleted lines": 7,
    "Added lines": 0,
    "Changed lines": 7,
    "Deleted code": "import logging\n    def _specify_ddp_gpu_num(self, gpu_size):\n        if gpu_size > 1:\n            raise ValueError('SyncBatchNorm is only supported for DDP with single GPU per process')\n        logging.warning(\"WARNING: Since DDP single-process-multiple-device mode is retired, \"\n                        \"no longer need to call method `_specify_ddp_gpu_num`. The input arg `gpu_size` is ignored.\")\n",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3f1f057adfcd4cef67fff9605a894cb075c02881",
    "Commit message": "Remove parent device mesh check (#118620)\n\nRemoves raising error if a device_mesh has a parent.\n\nThe comment says that HSDP + TP is not supported, but I'm able to do 2D parallelism + HSDP fine. The only issues are:\n- this check\n- https://github.com/pytorch/pytorch/pull/118618\n- a series of PRs related to checkpointing with 3D meshes that I will open\nWe currently monkeypatch for the above which I am slowly upstreaming.\n\nI imagine torch will have a better, native integration eventually, but this check seems too aggressive in the meantime given DTensor now lets users do some things themselves (which is amazing \ud83c\udf89)!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118620\nApproved by: https://github.com/wz337, https://github.com/wanchaol",
    "Deleted lines": 6,
    "Added lines": 0,
    "Changed lines": 6,
    "Deleted code": "    parent_mesh = _mesh_resources.get_parent_mesh(device_mesh)\n    if parent_mesh is not None:\n        raise RuntimeError(\n            f\"Found device_mesh {device_mesh} passed in has a parent device_mesh {parent_mesh}.\",\n            \"Hybrid sharding + TP is not supported yet.\",\n        )",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9d09968bbe05fc6d7d7c3d8b1acfbe1b1b1413a8",
    "Commit message": "Disable check for dropout in MultiheadAttention fast_path (#88831)\n\nSince we already enforce eval mode for the fast_path, we do not need to also check for a falsy dropout value, as a model trained with dropout will have a non-zero dropout during eval mode, even though it won't be applied.\n\nFixes #88806\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/88831\nApproved by: https://github.com/drisspg",
    "Deleted lines": 3,
    "Added lines": 0,
    "Changed lines": 3,
    "Deleted code": "    - dropout is 0\n        elif self.dropout:\n            why_not_fast_path = f\"dropout was {self.dropout}, required zero\"",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f386312ec936a94bfb1abe44acdd61d498f4272b",
    "Commit message": "[PyTorch] Don't do extra numel() check in TensorImpl::data() (#98090)\n\n`is_empty()` checks `numel() == 0`, but we don't need to access `numel_` at all (or the policy that `numel()` checks) in our happy path -- we just need the data pointer from `storage_`. Let's do the check we need to do using only the data we strictly need, rather than adding instructions loading other pieces of data.\n\nDifferential Revision: [D44586464](https://our.internmc.facebook.com/intern/diff/D44586464/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98090\nApproved by: https://github.com/Skylion007",
    "Deleted lines": 4,
    "Added lines": 3,
    "Changed lines": 7,
    "Deleted code": "    if (is_empty()) {\n    return static_cast<void*>(\n        static_cast<char*>(storage_.data()) +\n        data_type_.itemsize() * storage_offset_);",
    "Added code": "    char* const data = static_cast<char*>(storage_.data());\n    if (data == nullptr) {\n    return static_cast<void*>(data + data_type_.itemsize() * storage_offset_);"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fc304bec9f550075d4c899aa0fc5b1a0a573c1e5",
    "Commit message": "[TensorExpr] Remove redundant checks from canHandle in TE fuser. (#42937)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42937\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D23084408\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 8e562e25ecc73b4e7b01e30f8b282945b96b4871",
    "Deleted lines": 3,
    "Added lines": 0,
    "Changed lines": 3,
    "Deleted code": "  if (node->kind() == prim::Loop) {\n    return false; // TODO\n  }",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f810d96806d0e767aeca9fe9cf50e0bdcaab7d52",
    "Commit message": "remove redundant index check for index_select_out_cpu_dim1_ (#74093)\n\nSummary:\nFor  **index_select_out_cpu_dim1_**, there has a redundant idex check, **check_indexarray_range** has checked  **the index>=0 and  index < slect_dim**, we don't need re-check it at copy step.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/74093\n\nReviewed By: ezyang\n\nDifferential Revision: D34823673\n\nPulled By: ngimel\n\nfbshipit-source-id: c723b5bd6254c36588063da0175470268bffca5d\n(cherry picked from commit 1791fa4e488cdf77f0d6850f364a74a7a92fedee)",
    "Deleted lines": 7,
    "Added lines": 0,
    "Changed lines": 7,
    "Deleted code": "            if (idx < 0) {\n              idx = idx + src_indexing_axis_dim;\n            }\n            if (idx < 0) {\n              idx = idx + src_indexing_axis_dim;\n            }\n",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/63e47c68a692c70bc64c49d687f85f7f5cd02ce3",
    "Commit message": "[cpp] remove checks from embedding bag impl (#92982)\n\nThese checks incur an H2D sync on every embedding bag forward. Also, the equivalent python code for embedding_bag does not have them. Kill!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92982\nApproved by: https://github.com/ezyang",
    "Deleted lines": 11,
    "Added lines": 0,
    "Changed lines": 11,
    "Deleted code": "    TORCH_CHECK(\n        offsets_[0].item<int64_t>() == 0,\n        \"offsets[0] has to be 0, i.e., the first sequence in the mini-batch has to start from position 0. However, got \",\n        offsets_[0].item<int64_t>());\n    TORCH_CHECK(\n        offsets_[-1].item<int64_t>() <= input_.size(0),\n        \"offsets[-1] can not be greater than input's length({\",\n        input_.size(0),\n        \"}), but got offsets[-1] of {\",\n        offsets_[-1].item<int64_t>(),\n        \"}\");",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5b7c72101ca8e9d4edba1d16b6121ad900ca3936",
    "Commit message": "[Quant][devs] Removed check for is_quantized in dequantize_cpu_or_cuda (#71958)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71958\n\nThis PR is part of a series of PRs addressing https://github.com/pytorch/pytorch/issues/54150,\nrelated to using dispatcher for calls to quantized backends as opposed to if/else conditionals.\nThis particular PR isn't dispatcher related but does remove the extraneous torch check for a quant tensor\nsince the dispatcher already handles a quantized backend for this particular function\n\nDifferential Revision:\nD33833765\nD33833765\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nPulled By: dzdang\n\nfbshipit-source-id: c3bb531a5c09326bdf724b5185a19ea0a379bba7\n(cherry picked from commit f053b8248f895446f6a9d352de4038df6c6d4b2d)",
    "Deleted lines": 1,
    "Added lines": 0,
    "Changed lines": 1,
    "Deleted code": "  TORCH_CHECK(!self.is_quantized());",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8da704cdb7f68bfa09516e7be17f004b98c48eb3",
    "Commit message": "[MPS] Remove incorrect asserts from `Copy.mm` (#86184)\n\nThose asserts simply do not work for views.\n\nI.e. they are erroneously triggered for  in `copy_to_mps_` when running something like `python -c \"import torch;x=torch.empty(10,device='mps');y=torch.tensor([10]);print(x.shape);x[2]=y[0]\"` And in `copy_from_mps_` when running the same script, but with order of devices inverted: `python -c \"import torch;x=torch.empty(10);y=torch.tensor([10], device=\"mps\");print(x.shape);x[2]=y[0]\"`\n\nIf this was supposed to be a boundary check, than it should have validated, that `storage_offset() + nbytes() <= storage.nbytes()`, but this check is already done by the upper layer, isn't it?\n\nFixes https://github.com/pytorch/pytorch/issues/86153\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86184\nApproved by: https://github.com/kulinseth",
    "Deleted lines": 2,
    "Added lines": 0,
    "Changed lines": 2,
    "Deleted code": "      TORCH_INTERNAL_ASSERT(dst_tensor_nbytes >= (dst.storage_offset() * dst.element_size()));\n  TORCH_INTERNAL_ASSERT(dst_.nbytes() >= dst_byte_offset);",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c9b1e09958c2b914659aa6b33a76b0e53bd94431",
    "Commit message": "[c10d] delete lengths offset checks (#98368)\n\nAccording to @kwen2501, NCCL supports up to size_t send counts, so\nPGNCCL shouldn't restrict it\n\nA follow up is to think about whether we should add overflow protection\nof offset\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98368\nApproved by: https://github.com/kwen2501",
    "Deleted lines": 8,
    "Added lines": 1,
    "Changed lines": 9,
    "Deleted code": "    TORCH_INTERNAL_ASSERT(\n        length <= std::numeric_limits<int>::max() &&\n            offset <= std::numeric_limits<int>::max(),\n        \"Length or offset larger than INT_MAX not supported\");\n    TORCH_INTERNAL_ASSERT(\n        length <= std::numeric_limits<int>::max() &&\n            offset <= std::numeric_limits<int>::max(),\n        \"Length or offset larger than INT_MAX not supported\");",
    "Added code": "    // TODO: see if we should add overflow protection for offset"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2597d5d72272d196b4cb5442ffc8cde376d1f785",
    "Commit message": "TorchDynamo: always convert flexiblelayout to be FixedLayout when given a stride_order (#89904)\n\nFor convolution, we always call **require_stride_order** to convert the input to the target stride order,  if the original input's layout is flexiblelayout, there always have a memory copy because the **is_stride_order_storage_and_layout** only checks the init stride order,  I think for flexiblelayout, means it's layout can be changed, if the user gives a stride order, I think we always need to convert the flexiblelayout to be FixedLayout using given strider order.\n\nGiven a CV user case, the max_pooling's output is used by two convolutions, there has two memory copies:\n\n```\nkernel_cpp_0 = async_compile.cpp('''\n#include \"/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h\"\nextern \"C\" void kernel(const float* __restrict__ in_ptr0,\n                       float* __restrict__ out_ptr0,\n                       float* __restrict__ out_ptr1,\n                       float* __restrict__ out_ptr2)\n{\n    #pragma GCC ivdep\n    for(long i0=0; i0<128; i0+=1)\n    {\n        #pragma GCC ivdep\n        for(long i1=0; i1<3; i1+=1)\n        {\n            #pragma GCC ivdep\n            for(long i2=0; i2<3; i2+=1)\n            {\n                #pragma GCC ivdep\n                for(long i3=0; i3<3; i3+=1)\n                {\n                    {\n                        {\n                            auto tmp0 = in_ptr0[i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp1 = in_ptr0[3 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp3 = in_ptr0[6 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp5 = in_ptr0[21 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp7 = in_ptr0[24 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp9 = in_ptr0[27 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp11 = in_ptr0[42 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp13 = in_ptr0[45 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp15 = in_ptr0[48 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp2 = (tmp0 != tmp0) ? tmp0 : std::max(tmp1, tmp0);\n                            auto tmp4 = (tmp2 != tmp2) ? tmp2 : std::max(tmp3, tmp2);\n                            auto tmp6 = (tmp4 != tmp4) ? tmp4 : std::max(tmp5, tmp4);\n                            auto tmp8 = (tmp6 != tmp6) ? tmp6 : std::max(tmp7, tmp6);\n                            auto tmp10 = (tmp8 != tmp8) ? tmp8 : std::max(tmp9, tmp8);\n                            auto tmp12 = (tmp10 != tmp10) ? tmp10 : std::max(tmp11, tmp10);\n                            auto tmp14 = (tmp12 != tmp12) ? tmp12 : std::max(tmp13, tmp12);\n                            auto tmp16 = (tmp14 != tmp14) ? tmp14 : std::max(tmp15, tmp14);\n                            out_ptr0[i3 + (3*i2) + (9*i1) + (27*i0)] = tmp16;\n                        }\n                    }\n                }\n            }\n        }\n    }\n    #pragma GCC ivdep\n    for(long i0=0; i0<128; i0+=1)\n    {\n        #pragma GCC ivdep\n        for(long i1=0; i1<3; i1+=1)\n        {\n            #pragma GCC ivdep\n            for(long i2=0; i2<9; i2+=1)\n            {\n                {\n                    {\n                        auto tmp0 = out_ptr0[i1 + (3*i2) + (27*i0)];\n                        out_ptr1[i1 + (3*i2) + (27*i0)] = tmp0;\n                        out_ptr2[i1 + (3*i2) + (27*i0)] = tmp0;\n                    }\n                }\n            }\n        }\n    }\n}\n''')\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1 = args\n    args.clear()\n    buf0 = empty_strided((128, 3, 3, 3), (27, 1, 9, 3), device='cpu', dtype=torch.float32)\n    buf2 = empty_strided((128, 3, 3, 3), (27, 1, 9, 3), device='cpu', dtype=torch.float32)\n    buf4 = empty_strided((128, 3, 3, 3), (27, 1, 9, 3), device='cpu', dtype=torch.float32)\n    kernel_cpp_0(c_void_p(arg4_1.data_ptr()), c_void_p(buf0.data_ptr()), c_void_p(buf2.data_ptr()), c_void_p(buf4.data_ptr()))\n    del arg4_1\n    del buf0\n    buf3 = torch.ops.mkldnn._convolution_pointwise(buf2, arg0_1, arg1_1, (0, 0), (1, 1), (1, 1), 1, 'none', [], '')\n    assert_size_stride(buf3, (128, 3, 3, 3), (27, 1, 9, 3))\n    del arg0_1\n    del arg1_1\n    del buf2\n    buf5 = torch.ops.mkldnn._convolution_pointwise(buf4, arg2_1, arg3_1, (0, 0), (1, 1), (1, 1), 1, 'none', [], '')\n    assert_size_stride(buf5, (128, 3, 3, 3), (27, 1, 9, 3))\n    del arg2_1\n    del arg3_1\n    return (buf3, buf5, )\n```\n\nAfter this PR, the generated  code will remove the redundant memory copy:\n\n```\nkernel_cpp_0 = async_compile.cpp('''\n#include \"/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h\"\nextern \"C\" void kernel(const float* __restrict__ in_ptr0,\n                       float* __restrict__ out_ptr0)\n{\n    #pragma GCC ivdep\n    for(long i0=0; i0<128; i0+=1)\n    {\n        #pragma GCC ivdep\n        for(long i1=0; i1<3; i1+=1)\n        {\n            #pragma GCC ivdep\n            for(long i2=0; i2<3; i2+=1)\n            {\n                #pragma GCC ivdep\n                for(long i3=0; i3<3; i3+=1)\n                {\n                    {\n                        {\n                            auto tmp0 = in_ptr0[i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp1 = in_ptr0[3 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp3 = in_ptr0[6 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp5 = in_ptr0[21 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp7 = in_ptr0[24 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp9 = in_ptr0[27 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp11 = in_ptr0[42 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp13 = in_ptr0[45 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp15 = in_ptr0[48 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp2 = (tmp0 != tmp0) ? tmp0 : std::max(tmp1, tmp0);\n                            auto tmp4 = (tmp2 != tmp2) ? tmp2 : std::max(tmp3, tmp2);\n                            auto tmp6 = (tmp4 != tmp4) ? tmp4 : std::max(tmp5, tmp4);\n                            auto tmp8 = (tmp6 != tmp6) ? tmp6 : std::max(tmp7, tmp6);\n                            auto tmp10 = (tmp8 != tmp8) ? tmp8 : std::max(tmp9, tmp8);\n                            auto tmp12 = (tmp10 != tmp10) ? tmp10 : std::max(tmp11, tmp10);\n                            auto tmp14 = (tmp12 != tmp12) ? tmp12 : std::max(tmp13, tmp12);\n                            auto tmp16 = (tmp14 != tmp14) ? tmp14 : std::max(tmp15, tmp14);\n                            out_ptr0[i3 + (3*i2) + (9*i1) + (27*i0)] = tmp16;\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n''')\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1 = args\n    args.clear()\n    buf0 = empty_strided((128, 3, 3, 3), (27, 1, 9, 3), device='cpu', dtype=torch.float32)\n    kernel_cpp_0(c_void_p(arg4_1.data_ptr()), c_void_p(buf0.data_ptr()))\n    del arg4_1\n    buf2 = torch.ops.mkldnn._convolution_pointwise(buf0, arg0_1, arg1_1, (0, 0), (1, 1), (1, 1), 1, 'none', [], '')\n    assert_size_stride(buf2, (128, 3, 3, 3), (27, 1, 9, 3))\n    del arg0_1\n    del arg1_1\n    buf3 = torch.ops.mkldnn._convolution_pointwise(buf0, arg2_1, arg3_1, (0, 0), (1, 1), (1, 1), 1, 'none', [], '')\n    assert_size_stride(buf3, (128, 3, 3, 3), (27, 1, 9, 3))\n    del arg2_1\n    del arg3_1\n    return (buf2, buf3, )\n\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89904\nApproved by: https://github.com/jansel",
    "Deleted lines": 3,
    "Added lines": 1,
    "Changed lines": 4,
    "Deleted code": "            if isinstance(\n                x.get_layout(), FlexibleLayout\n            ) and is_stride_order_storage_and_layout(x, order):",
    "Added code": "            if isinstance(x.get_layout(), FlexibleLayout):"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/201f7d330ac8c33a7bedb8f0a66954415d1d27db",
    "Commit message": "Remove duplicate check in distributions arg validation (#67741)\n\nSummary:\nPartial fix for https://github.com/pytorch/pytorch/issues/66800. (Duplicate of https://github.com/pytorch/pytorch/issues/67725 against pytorch/pytorch so as to trigger TorchBench)\n\nhttps://github.com/pytorch/pytorch/issues/61056 added a more verbose error message for distributions failing argument validation. However, it did not replace the earlier error check as was originally intended and was flagged by xuzhao9 as being the potential cause of a perf regression in `test_eval[soft_actor_critic-cuda-eager]`.\n\nxuzhao9: Is there a way for me to check if this resolves the perf issue you mentioned?\n\ncc VitalyFedyunin ngimel\n\nNote that existing tests already check for the error message and should verify that the removed lines are redundant.\n\nRUN_TORCHBENCH: soft_actor_critic\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67741\n\nReviewed By: neerajprad\n\nDifferential Revision: D32135675\n\nPulled By: xuzhao9\n\nfbshipit-source-id: 37dfd3ff53b95017c763371979ab3a2c302a72b9",
    "Deleted lines": 2,
    "Added lines": 0,
    "Changed lines": 2,
    "Deleted code": "                if not constraint.check(getattr(self, param)).all():\n                    raise ValueError(\"The parameter {} has invalid values\".format(param))",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a47cc18254ade6dee1fe4a3c4eb5aca7ba40c77c",
    "Commit message": "remove unnecessary tuple check on tensor types (#79896)\n\nDifferential Revision: [D37314981](https://our.internmc.facebook.com/intern/diff/D37314981)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/79896\nApproved by: https://github.com/jansel, https://github.com/anijain2305",
    "Deleted lines": 1,
    "Added lines": 0,
    "Changed lines": 1,
    "Deleted code": "        assert isinstance(args[0], tuple)",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2512017814fb2e3d6f3ae9dd3b315692ffc8fc71",
    "Commit message": "Fix for out of bounds read in torch mobile flatbuffer loader (#108439)\n\nRemove redundant (and unsafe) `mobile::serialization::ModuleBufferHasIdentifier(data)` as ` mobile::serialization::VerifyModuleBuffer(verifier)` validates the same thing but in boundary-check safe manner.\n\nTest Plan: Out of bounds read crash no longer reproduces\n\nDifferential Revision: D48914114\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108439\nApproved by: https://github.com/manuelcandales, https://github.com/malfet",
    "Deleted lines": 2,
    "Added lines": 0,
    "Changed lines": 2,
    "Deleted code": "  TORCH_CHECK(\n      mobile::serialization::ModuleBufferHasIdentifier(data), \"Format error\");",
    "Added code": ""
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b3a7d696b37b502d1688741eb3339da64c23ab93",
    "Commit message": "dbr quant overhead[5/x]: remove unnecessary asserts (#68370)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68370\n\nRemoves asserts which are duplicate (the same condition is checked\nwhen calculating the hook type, so there is no need to check it again).\nFor the assert in `validate_is_at_last_seen_idx`, rewrites it to\nraise an Error instead to ensure it does not get stripped in\nproduction environments.\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeDBR\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D32463766\n\nPulled By: vkuzo\n\nfbshipit-source-id: 8a7b7e0bf270bc327f49bd3e5bd156339e846381",
    "Deleted lines": 7,
    "Added lines": 3,
    "Changed lines": 10,
    "Deleted code": "        assert is_at_last_seen_idx, \\\n            f\"Cur idx: {self.idx}, expected idx: {len(self.idx_to_seen_op_infos)}\"\n        assert self.cur_op_needs_hooks(cur_op)\n        assert self.cur_op_needs_hooks(op)\n        assert self.cur_op_needs_hooks(op)\n        assert self.cur_op_needs_hooks(op)\n",
    "Added code": "        if not is_at_last_seen_idx:\n            raise AssertionError(\n                f\"Cur idx: {self.idx}, expected idx: {len(self.idx_to_seen_op_infos)}\")"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e7fc7c732cbde822f9490840704b1f57fe86c50a",
    "Commit message": "Bugfix for fusion device check (#19594)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/19594\n\nI missed a callsite\n\nReviewed By: wanchaol\n\nDifferential Revision: D15041457\n\nfbshipit-source-id: eef76ad51bee06a56d31b4ab64f19250fe2ad8f0",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if (!isFusable(producer->node())) {",
    "Added code": "    if (!isFusableDevice(producer) || !isFusable(producer->node())) {"
},
{
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4e90526a4f03d1950e8db6e8722cce8e0fb4a5f5",
    "Commit message": "[FSDP] Remove unneeded checks (#83150)\n\n@awgu pointed out these checks aren't really doing anything, as they just make sure we're setting training state in certain ways throughout FSDP and is sort of arbitrary. So, removing them to avoid confusion.\n\nWe still keep the checking around `_post_backward_called` because this is needed in `finalize_params` for now.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/83150\nApproved by: https://github.com/awgu",
    "Deleted lines": 41,
    "Added lines": 8,
    "Changed lines": 49,
    "Deleted code": "        for m in self.modules():  # includes self\n            if isinstance(m, FullyShardedDataParallel):\n                if any(p.requires_grad for p in m.parameters()):\n                    # Check if the module has params and if any of them has\n                    # the `requires_grad` field set. If `requires_grad=False` for\n                    # all the params, the post_backward hook will not fire and the\n                    # state will remain in `TrainingState_.BACKWARD_PRE`.\n                    managed_param_requires_grad = any(p.requires_grad for p in m.params)\n                    if managed_param_requires_grad:\n                        p_assert(\n                            all(hasattr(p, '_post_backward_called') for p in m.params),\n                            \"Expected all params to have flag _post_backward_called set!\"\n                        )\n                        post_backward_hook_called = any(p._post_backward_called for p in m.params)\n                        if post_backward_hook_called:\n                            m._assert_state(TrainingState_.BACKWARD_POST)\n                        else:\n                            # post backward hook was not called, meaning param\n                            # did not have a gradient computed. It was either unused\n                            # in forward, or unused in loss computation so it did\n                            # not get gradient\n                            m._assert_state([TrainingState_.BACKWARD_PRE, TrainingState_.IDLE])\n                    else:\n                        m._assert_state(TrainingState_.BACKWARD_PRE)\n                else:\n                    # When `m` and its children have no non-ignored params or\n                    # have non-ignored params but none with `requires_grad==True`,\n                    # there are two cases:\n                    # 1. output tensors are `requires_grad==True`. In this case,\n                    # pre-backward hook is still registered, so it is in BACKWARD_PRE state.\n                    # 2. output tensors are `requires_grad==False`. In this case,\n                    # pre-backward hook is not registered, so it is in IDLE state.\n                    m._assert_state([TrainingState_.BACKWARD_PRE, TrainingState_.IDLE])\n\n                _finalize_params(m)\n                m._pre_backward_hook_has_run = False\n                m.training_state = TrainingState_.IDLE\n\n                if m._is_root:\n                    # reset this flag for cases like \"one forward pass + multiple backward passes\"\n                    self._post_backward_callback_queued = False",
    "Added code": "        for m in self.fsdp_modules(self):  # includes self\n            _finalize_params(m)\n            m._pre_backward_hook_has_run = False\n            m.training_state = TrainingState_.IDLE\n\n            if m._is_root:\n                # reset this flag for cases like \"one forward pass + multiple backward passes\"\n                self._post_backward_callback_queued = False"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4a343043dbc6ce229b4dcf2258f7b6352db32b64",
    "Commit message": "Fix hasattr check in saved_model_cli\n\nPiperOrigin-RevId: 165509083",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "  if not hasattr(args.func):\n    parser.error(\"too few arguments\")",
    "Added code": "  if not hasattr(args, 'func'):\n    parser.error('too few arguments')"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/73f25fc34c69878c83ee2eeb8f030cb79a76472f",
    "Commit message": "Fix an issue caused by (#18183)\n\nWhile trying to run on my machine (Ubuntu 16.04 Python 2.7)\r\n```\r\nbazel test -s --config=opt --cache_test_results=no //tensorflow/tools/api/tests:api_compatibility_test\r\n```\r\n\r\nThe following error was encountered:\r\n```\r\n  ......\r\n  File \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py\", line 125, in get_api_imports\r\n    if not module or 'tensorflow.' not in module.__name__:\r\n  File \"/usr/lib/python2.7/dist-packages/py/_apipkg.py\", line 171, in __getattribute__\r\n    return getattr(getmod(), name)\r\n  File \"/usr/lib/python2.7/dist-packages/py/_error.py\", line 43, in __getattr__\r\n    raise AttributeError(name)\r\nAttributeError: __name__\r\n```\r\n\r\nThe issue is that `<AliasModule 'py.error' for 'py._error.error'>` does not\r\nhave a `__name__` attribute (See similiar issue in https://github.com/pytest-dev/py/issues/73).\r\n\r\nThis fix tries to address the issue by adding an `hasattr()` check so\r\nthat AttributeError is not thrown.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "    if not module or 'tensorflow.' not in module.__name__:",
    "Added code": "    if (not module or not hasattr(module, \"__name__\") or\n        'tensorflow.' not in module.__name__):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cdc36a3b1f7d227984bb5e415b555ed334737f82",
    "Commit message": "tfdbg: cosmetic fix to MonitoredSession.__del__ AttributeError\n\nPrevoiusly, calling \"del\" on a tf_debug-wrapped MonitoredSession causes a warning message like the following:\nException AttributeError: \"'MonitoredSession' object has no attribute '__del__'\" in <bound method LocalCLIDebuggerWrapperSessionForTest.__del__ of <__main__.LocalCLIDebuggerWrapperSessionForTest object at 0x558c74f642d0>> ignored\n\nAs the message states, the AttributeError is ignored and doesn't cause failures.\n\nThis CL prevents this message by checking that the underlying _sess object has the __del__ method defined before calling it.\n\nFixes: #15105\nPiperOrigin-RevId: 178272619",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "    self._sess.__del__()",
    "Added code": "    if hasattr(self._sess, \"__del__\"):\n      self._sess.__del__()"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/67b6c880e39ba02ba53c7d499e45fd136090ee32",
    "Commit message": "In tf.map_fn: skip sanity check for `shape` of first value in `elems` if it doesn't have a shape attribute.  (E.g., this can happen if it's a CompsiteTensor.)\n\nPiperOrigin-RevId: 397990479\nChange-Id: I5e8547880dee811ab534ba9f25ef7b1b4ec75510",
    "Deleted lines": 4,
    "Added lines": 5,
    "Changed lines": 9,
    "Deleted code": "    elems_static_shape = first_elem.shape\n    if elems_static_shape.ndims is not None and elems_static_shape.ndims < 1:\n      raise ValueError(\n          \"Elements in elems must be 1+ dimensional Tensors, not scalars\")",
    "Added code": "    if hasattr(first_elem, \"shape\"):\n      elems_static_shape = first_elem.shape\n      if elems_static_shape.ndims is not None and elems_static_shape.ndims < 1:\n        raise ValueError(\n            \"Elements in elems must be 1+ dimensional Tensors, not scalars\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dcdca11bcbab4b2474e7bf4d21d1806e6c2790a3",
    "Commit message": "Check both output name and output slot in duplicate scope id sanity check.\n\nBefore this change, we would throw an error if different outputs of a node were\ncommitted to different scope ids.  Since that is legal, this change fixes the\nbug by making the check based on both output name and output index.\n\nPiperOrigin-RevId: 263482156",
    "Deleted lines": 1,
    "Added lines": 3,
    "Changed lines": 4,
    "Deleted code": "      if (ss.ok()) {",
    "Added code": "      // Check that both output name and output slot match.  It is okay to have\n      // different outputs of the input committed to different scope ids.\n      if (ss.ok() && scope_ids[0] == nd.output_slot) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a607eb012b1bc4f6dbe263ad99caa76d84ae3ab2",
    "Commit message": "fix output shape check for strided slice always failing when stride != 1",
    "Deleted lines": 3,
    "Added lines": 6,
    "Changed lines": 9,
    "Deleted code": "    if (attr.ends.h - attr.starts.h != out_shape.h) {\n    if (attr.ends.w - attr.starts.w != out_shape.w) {\n    if (attr.ends.c - attr.starts.c != out_shape.c) {",
    "Added code": "    if ((attr.ends.h - attr.starts.h + attr.strides.h - 1) / attr.strides.h !=\n        out_shape.h) {\n    if ((attr.ends.w - attr.starts.w + attr.strides.w - 1) / attr.strides.w !=\n        out_shape.w) {\n    if ((attr.ends.c - attr.starts.c + attr.strides.c - 1) / attr.strides.c !=\n        out_shape.c) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2bf2799ee80791107d4fe587ff9b6c7cf6c8b418",
    "Commit message": "C API: Fail gracefully if the serialized graph would be too large.\n\nSee #19657 for some motivation.\nWithout this explicit check, a large graph would trigger an assertion failure\nin the protobuf codebase\n(https://github.com/google/protobuf/blob/0456e269ee6505766474aa8d7b8bba7ac047f457/src/google/protobuf/message_lite.cc#L68)\n\nPull Request for google/protobuf: https://github.com/google/protobuf/pull/4739\n\nPiperOrigin-RevId: 199719082",
    "Deleted lines": 1,
    "Added lines": 16,
    "Changed lines": 17,
    "Deleted code": "  in.SerializeToArray(buf, proto_size);",
    "Added code": "  // SerializeToArray takes size as an int.\n  // This next 'if' is a workaround till we update to depend on a version\n  // of protocol buffers that includes\n  // https://github.com/google/protobuf/pull/4739\n  if (proto_size > std::numeric_limits<int>::max()) {\n    return InvalidArgument(\"Cannot serialize protocol buffer of type \",\n                           in.GetTypeName(), \" as the serialized size (\",\n                           proto_size,\n                           \"bytes) would be larger than the limit (\",\n                           std::numeric_limits<int>::max(), \" bytes)\");\n  }\n  if (!in.SerializeToArray(buf, proto_size)) {\n    return InvalidArgument(\"Unable to serialize \", in.GetTypeName(),\n                           \" protocol buffer, perhaps the serialized size (\",\n                           proto_size, \" bytes) is too large?\");\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0197a2d8a3070af763cb67227835ee63df095e6d",
    "Commit message": "Add a check to catch out-of-bound access on invalid Graphs\n\nThe existing Check trying to catch malformed graph is not robust when\nan op is registered with an expected number of inputs but has data edges\nbeyond this.\n\nPiperOrigin-RevId: 266826557",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "        DCHECK(edge->dst_input() < inputs.size())\n            << \"Edge \" << edge->DebugString()\n            << \" is overflowing the expected number of inputs (\"\n            << node->num_inputs() << \") for node \" << node->DebugString();"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/314d9cd9b607460f8bfea80fc828b1521ca18443",
    "Commit message": "Fix segfault in MacOS when GPU is not available (#3448)\n\n* Fix sigsegv in MacOS when GPU is not available\r\n\r\n* Change info link to point to Apple docs\r\n\r\n* Invert NULL checking logic\r\n\r\n* fix indentation\r\n\r\n* Re-align to get change to fit in 80 chars\r\n\r\n* Run clang-format \u2014style=google on changes",
    "Deleted lines": 1,
    "Added lines": 10,
    "Changed lines": 11,
    "Deleted code": "    const char * version = CFStringGetCStringPtr((CFStringRef)CFDictionaryGetValue(cuda_driver_info, kCFBundleVersionKey), kCFStringEncodingUTF8);",
    "Added code": "    const CFStringRef str = (CFStringRef)CFDictionaryGetValue(\n        cuda_driver_info, kCFBundleVersionKey);\n    const char *version = CFStringGetCStringPtr(str, kCFStringEncodingUTF8);\n\n    // version can be NULL in which case treat it as empty string\n    // see\n    // https://developer.apple.com/library/mac/documentation/CoreFoundation/Conceptual/CFStrings/Articles/AccessingContents.html#//apple_ref/doc/uid/20001184-100980-TPXREF112\n    if (version == NULL) {\n      return StringToDriverVersion(\"\");\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/abd645085b1dd1496df847b05a1934d471a2f2c0",
    "Commit message": "Use the correct device ordinal to check whether the device the executable was\nbuilt for is equivalent to the device the it will run on.\n\nBefore this patch, if the device to run on was provided via a stream without\nsetting the device ordinal in the ExecutableRunOptions, we would check the\ndefault device against the device the executable was built for.\n\nPiperOrigin-RevId: 206892902",
    "Deleted lines": 5,
    "Added lines": 8,
    "Changed lines": 13,
    "Deleted code": "  // Verify that the device the executable was built for is equivalent to the\n  // device it will run on.\n  int run_device_ordinal = run_options.device_ordinal() == -1\n                               ? backend_->default_device_ordinal()\n                               : run_options.device_ordinal();",
    "Added code": "  // Verify that the device the executable was built for is equivalent\n  // to the device it will run on.\n  int run_device_ordinal = run_options.device_ordinal();\n  if (run_device_ordinal == -1) {\n    run_device_ordinal = run_options.stream() != nullptr\n                             ? run_options.stream()->parent()->device_ordinal()\n                             : backend_->default_device_ordinal();\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/083fd8c4b23104f6b27a871c6469629ace4ee9c3",
    "Commit message": "Don't check soname on Windows\n\nThis allow users to specify a certain CUDA version on Windows again.\n\nWithout this, we get the following error:\n\n```\nCuda Configuration Error: None of the libraries match their SONAME: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1/lib/x64/cudart.lib\n```\n\nPiperOrigin-RevId: 236646104",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "        if check_soname and objdump != None:",
    "Added code": "        if check_soname and objdump != None and not _is_windows(repository_ctx):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/867a918bd3d40afeca6b96430671a098134e7905",
    "Commit message": "CUDA Driver: do better error reporting if checking the pointer properties failed.\n\nThere are many reasons why an operation can fail, propagate the error instead\nof assuming the cause.\n\nPiperOrigin-RevId: 246226334",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "  CHECK(err == cudaSuccess || err == cudaErrorInvalidValue)\n      << \"Unexpected CUDA error: \" << cudaGetErrorString(err);\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1a73fdfa83bd50695a7d374d14a5cb3835d94d9e",
    "Commit message": "Add extra check incase segmenter does not exclude CPU in order to prevent segfault",
    "Deleted lines": 4,
    "Added lines": 10,
    "Changed lines": 14,
    "Deleted code": "    // TODO: check for CPU device here\n    // If device is CPU, we should've caught that in the segmenter. Fall back here.\n\n      segment_devices.insert(node_device);",
    "Added code": "      // If device is CPU, we should've caught that in the segmenter.\n      DeviceNameUtils::ParsedName parsed_name;\n      DeviceNameUtils::ParseFullName(node_device, &parsed_name);\n      if (parsed_name.type == \"CPU\") {\n        LOG(WARNING) << \"Node \" << node->name() << \" was assigned to the CPU \"\n                     << \"but did not get excluded by the segmenter. \"\n                     << \"Attempting to place on GPU.\";\n      } else {\n        segment_devices.insert(node_device);\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b234ff0ee4ce87d21a3e5306b678e1fb4b1fedfc",
    "Commit message": "Fixed division by zero, by checking the number of GPUs in GenericLayoutOptimizer.\n\nPiperOrigin-RevId: 261934091",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "  if (num_conv2d_gpu == 0) return false;\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6c472f6632c4864da749e7a4aee8c001a905287f",
    "Commit message": "only found `CU_MEM_LOCATION_TYPE_HOST`, `CU_MEM_LOCATION_TYPE_HOST_NUMA` and `CU_MEM_LOCATION_TYPE_HOST_NUMA_CURRENT` in [CUDA version 12.3.1 doc](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TYPES.html), and didn't find the evidence of their existence in formal versions' doc, so suggest to use check `CUDA_VERSION` at `12030` here, for `maxSize`, resolved directly in the same way\n\nPiperOrigin-RevId: 589226039",
    "Deleted lines": 4,
    "Added lines": 4,
    "Changed lines": 8,
    "Deleted code": "#if CUDA_VERSION >= 12000\n#endif  // CUDA_VERSION >= 12000\n#if CUDA_VERSION >= 12000\n#endif  // CUDA_VERSION >= 12000",
    "Added code": "#if CUDA_VERSION >= 12030\n#endif  // CUDA_VERSION >= 12030\n#if CUDA_VERSION >= 12030\n#endif  // CUDA_VERSION >= 12030"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9f8ad5ff118166537d42f87f1ee254f83ba553f0",
    "Commit message": "Fix CUDA version check (format is 1000 * major + 10 * minor).\n\nRef: https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART____VERSION.html\nPiperOrigin-RevId: 378180901\nChange-Id: Ief1c62ea9ed6df95195a7439ae3315e62222b56a",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if (CUDA_VERSION < 11300) {",
    "Added code": "    if (CUDA_VERSION < 11030) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/89334fb15c688e7dbd81878745755db01579ea70",
    "Commit message": "PR #7789: [NVIDIA] Update VersionCheck APIs for CuDNN\n\nImported from GitHub PR https://github.com/openxla/xla/pull/7789\n\nThis PR updates the cudnnXXXVersionCheck to the latest for the next CUDNN release.\n\ncc. @reedwm @nluehr\nCopybara import of the project:\n\n--\n4c564d9bf8fd4c033af942da6689c219c49f4052 by Kaixi Hou <kaixih@nvidia.com>:\n\nUse new version check APIs\n\nMerging this change closes #7789\n\nPiperOrigin-RevId: 595932222",
    "Deleted lines": 2,
    "Added lines": 15,
    "Changed lines": 17,
    "Deleted code": "#if CUDNN_VERSION >= 8004\n#endif  // CUDNN_VERSION >= 8004",
    "Added code": "#if CUDNN_VERSION >= 8004 && CUDNN_VERSION < 9000\n#endif  // CUDNN_VERSION >= 8004 && CUDNN_VERSION < 9000\n#if CUDNN >= 9000 && TF_ENABLE_CUDNN_FRONTEND\n      cudnnGraphVersionCheck();\n      cudnnOpsVersionCheck();\n#elif CUDNN_VERSION >= 9000\n      cudnnCnnVersionCheck();\n      cudnnOpsVersionCheck();\n#elif CUDNN_VERSION >= 8004\n#endif  // CUDNN >= 9000 && TF_ENABLE_CUDNN_FRONTEND\n#if CUDNN_VERSION >= 9000\n      cudnnOpsVersionCheck();\n      cudnnAdvVersionCheck();\n#elif CUDNN_VERSION >= 8004\n#endif  // CUDNN_VERSION >= 9000"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e5cfbd0eceb4dca98b388b13acff499a5420f863",
    "Commit message": "Fix more for cuda version check.",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if (version.ok() && std::get<0>(version.ValueOrDie()) >= 7) {",
    "Added code": "    if (version.ok() && version.ValueOrDie().major_version() >= 7) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ac012e26d4331919335d4bceb8abe22b68ed5434",
    "Commit message": "[xla:python] Don't assume GPU clients are PJRT-compatible.\n\nInstead explicitly check for compatibility.\n\nPiperOrigin-RevId: 583311176",
    "Deleted lines": 7,
    "Added lines": 11,
    "Changed lines": 18,
    "Deleted code": "  if ((ifrt_client_->platform_id() == xla::CudaId() ||\n       ifrt_client_->platform_id() == xla::RocmId()) &&\n      !pjrt_client()->devices().empty()) {\n    auto maybe_stats = pjrt_client()->devices()[0]->GetAllocatorStats();\n    if (maybe_stats.ok() && maybe_stats->bytes_limit) {\n      options.executable_build_options.set_device_memory_size(\n          *maybe_stats->bytes_limit);",
    "Added code": "  auto* pjrt_compatible_client =\n      llvm::dyn_cast_or_null<ifrt::PjRtCompatibleClient>(ifrt_client_.get());\n  if (pjrt_compatible_client != nullptr) {\n    auto devices = pjrt_compatible_client->pjrt_client()->devices();\n    if (!devices.empty()) {\n      auto stats = devices[0]->GetAllocatorStats();\n      if (stats.ok() && stats->bytes_limit) {\n        options.executable_build_options.set_device_memory_size(\n            *stats->bytes_limit);\n      }\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6b9189483513b0c663e23485834be64f51b076e4",
    "Commit message": "Add device compatibility check for fusion.\n\n_FusedMatMul is not supported by GPU currently.\n\nPiperOrigin-RevId: 442931704",
    "Deleted lines": 0,
    "Added lines": 51,
    "Changed lines": 51,
    "Deleted code": "",
    "Added code": "#include <string>\n  // Class users should override this method if there are any op-specific\n  // compatibility requirements for devices.\n  virtual bool IsDeviceCompatible(SrcOpT contraction_op,\n                                  PatternRewriter &rewriter) const {\n    return true;\n  }\n\n    if (!IsDeviceCompatible(contraction, rewriter)) {\n      return rewriter.notifyMatchFailure(\n          contraction,\n          \"cannot fuse with the subsequent op as it's not supported by the \"\n          \"target device.\");\n    }\n\nconst char kDeviceAttr[] = \"device\";\nconst char kDeviceGpu[] = \"GPU\";\n\nllvm::Optional<std::string> GetDevice(mlir::Operation *op) {\n  mlir::StringAttr device = op->getAttrOfType<mlir::StringAttr>(kDeviceAttr);\n  if (!device || device.getValue().empty()) {\n    return llvm::None;\n  }\n  const std::string device_name = device.str();\n  tensorflow::DeviceNameUtils::ParsedName parsed_name;\n  if (!tensorflow::DeviceNameUtils::ParseFullName(device_name, &parsed_name)) {\n    return llvm::None;\n  }\n  if (!parsed_name.has_type) {\n    return llvm::None;\n  }\n  return parsed_name.type;\n}\n\nbool IsGpuDevice(mlir::Operation *op) {\n  llvm::Optional<std::string> device = GetDevice(op);\n  if (!device) return false;\n  return *device == kDeviceGpu;\n}\n\n\n  bool IsDeviceCompatible(MatMulOp matmul,\n                          PatternRewriter &rewriter) const override {\n    if (IsGpuDevice(matmul)) {\n      (void)rewriter.notifyMatchFailure(matmul, [&](Diagnostic &diag) {\n        diag << \"_FusedMatMul is not supported by GPU\";\n      });\n      return false;\n    }\n    return true;\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e009644f034fa0ca4df910a812432cab3458d440",
    "Commit message": "Add one error check in cuda_dnn for int8 to float convolution.",
    "Deleted lines": 0,
    "Added lines": 10,
    "Changed lines": 10,
    "Deleted code": "",
    "Added code": "    if (CUDNN_VERSION < 8000) {\n      if (algorithm_desc.algo_id() ==\n              CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM &&\n          ToCudnnDataType(element_type) == CUDNN_DATA_INT8 &&\n          ToCudnnDataType(output_type) == CUDNN_DATA_FLOAT) {\n        return port::Status(\n            port::error::FAILED_PRECONDITION,\n            \"This configuration potentially produces incorrect results.\");\n      }\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e1dbfeba8acb1df8f42dfa6f76262f5cb23e1fa1",
    "Commit message": "[stream_executor] NFC: Guard new features with CUDA_VERSION check\n\nPiperOrigin-RevId: 586180704",
    "Deleted lines": 0,
    "Added lines": 9,
    "Changed lines": 9,
    "Deleted code": "",
    "Added code": "#if CUDA_VERSION >= 12000\n#else\n    case GpuDriver::MemLocationType::kHost:\n    case GpuDriver::MemLocationType::kHostNuma:\n    case GpuDriver::MemLocationType::kHostNumaCurrent:\n      return CU_MEM_LOCATION_TYPE_INVALID;\n#endif  // CUDA_VERSION >= 12000\n#if CUDA_VERSION >= 12000\n#endif  // CUDA_VERSION >= 12000"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e05f78a9b688a8ae37b1a03bfc4459e18e3b88e4",
    "Commit message": "After synchronizing CUDA device, check for errors.\n\nPiperOrigin-RevId: 158939543",
    "Deleted lines": 1,
    "Added lines": 9,
    "Changed lines": 10,
    "Deleted code": "  CUresult res = cuCtxSynchronize();",
    "Added code": "#include <cuda_runtime.h>\n  const CUresult res = cuCtxSynchronize();\n  const auto cudart_error = cudaPeekAtLastError();\n  if (cudart_error != cudaSuccess) {\n    LOG(ERROR) << \"could not synchronize on CUDA context: \"\n               << cudaGetErrorString(cudart_error)\n               << \" :: \" << port::CurrentStackTrace();\n    return false;\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/70ade1b64f65d0a2275672d27129627ff116a997",
    "Commit message": "Fix defect: shuffle_batch gives ZeroDivisionError when computing capacity stat (#10477)\n\n* Fix defect: shuffle_batch gives ZeroDivisionError when computing capacity stat\r\n\r\n* Cover < case in error checking",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    if capacity <= min_after_dequeue:\n      raise ValueError(\"capacity %d must be bigger than min_after_dequeue %d.\"\n                       % (capacity, min_after_dequeue))"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1220ba3ab332d6233a84d660cafb3d4e29958224",
    "Commit message": "Fix two potential asynchrony bounds-check bugs in transpose op.\nChange: 117518926",
    "Deleted lines": 3,
    "Added lines": 7,
    "Changed lines": 10,
    "Deleted code": "      const int32 d = Tin(i);\n      OP_REQUIRES(context, 0 <= d && d < N,\n  const int32* perm_begin = reinterpret_cast<const int32*>(Vperm.data());",
    "Added code": "#include \"tensorflow/core/kernels/bounds_check.h\"\n      const int32 d = internal::SubtleMustCopy(Tin(i));\n      OP_REQUIRES(context, FastBoundsCheck(d, N),\n  // using volatile instead of SubtleMustCopy here so that the\n  // asynchrony boundary is permutation.\n  const volatile int32* perm_begin =\n      reinterpret_cast<const volatile int32*>(Vperm.data());"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a21ec782601aca6c7e0461093d72596f26229e44",
    "Commit message": "Use `getattr` instead of `isinstance` in `tensor_conversion_registry`.\n\nUsing `isinstance` to check if an object is\nan instance of a Python `typing.Protocol` instead\nof using `getattr`/`hasattr` has negative performance\nimplications.\n\nThis change reverts `tensor_conversion_registry.convert()`\nto use `getattr` for this reason.\n\nPiperOrigin-RevId: 513547008",
    "Deleted lines": 2,
    "Added lines": 3,
    "Changed lines": 5,
    "Deleted code": "  if isinstance(value, core.TensorProtocol):\n    return value.__tf_tensor__(dtype, name)",
    "Added code": "  overload = getattr(value, \"__tf_tensor__\", None)\n  if overload is not None:\n    return overload(dtype, name)  #  pylint: disable=not-callable"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/15c186bffe51901e4a48b4b6bf1316832533743f",
    "Commit message": "Correctly handle the case if static maximum dimension size = 0.\n\nPiperOrigin-RevId: 345794998\nChange-Id: Ic112e144731c1a23879c44aa7c5e3c180ccccc1c",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "          if not s or s != maximum_static_shapes[idx][i]:\n            if s.value:",
    "Added code": "          if s is None or s != maximum_static_shapes[idx][i]:\n            if s.value is not None:"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e1ad3b74ad44b883c7b3fdc3a19adcea1d28bfbc",
    "Commit message": "[XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast is an\neffective scalar. This short-circuit avoids crashing within last_dimension when\nattempting to match and either the operand or the result of the bitcast has a\nshape with rank 0.\n\nPiperOrigin-RevId: 548645429",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  if (bitcast->shape().rank() == 0) {",
    "Added code": "  if (ShapeUtil::IsEffectiveScalar(bitcast->shape())) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3c80be9f2cfece929f5858e7df0e7f4503c9baec",
    "Commit message": "[tf.data service] Support num_consumers being a Tensor.\n\nThe `if num_consumers >= 0:` check causes an error when num_consumers is a Tensor in graph mode:\n\nOperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution\n\nComparing to `None` accomplishes the same thing without this issue. Once the forward compatibility window expires we can remove the check completely.\n\nPiperOrigin-RevId: 351894589\nChange-Id: I0f6e8b93f6ae14b92b08c7c438d6a111ae0efff7",
    "Deleted lines": 11,
    "Added lines": 11,
    "Changed lines": 22,
    "Deleted code": "    if consumer_index is None:\n      consumer_index = -1\n    if num_consumers is None:\n      num_consumers = -1\n        consumer_index, dtype=dtypes.int64, name=\"consumer_index\")\n        num_consumers, dtype=dtypes.int64, name=\"num_consumers\")\n    if num_consumers >= 0:\n      variant_tensor = gen_experimental_dataset_ops.data_service_dataset_v2(\n          consumer_index=self._consumer_index,\n          num_consumers=self._num_consumers,\n      variant_tensor = gen_experimental_dataset_ops.data_service_dataset(",
    "Added code": "        -1 if consumer_index is None else consumer_index,\n        dtype=dtypes.int64,\n        name=\"consumer_index\")\n        -1 if num_consumers is None else num_consumers,\n        dtype=dtypes.int64,\n        name=\"num_consumers\")\n    if num_consumers is None:\n      variant_tensor = gen_experimental_dataset_ops.data_service_dataset(\n      variant_tensor = gen_experimental_dataset_ops.data_service_dataset_v2(\n          consumer_index=self._consumer_index,\n          num_consumers=self._num_consumers,"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2f3b69e4976d3b14eaa6ae070eb68f37d1556d98",
    "Commit message": "Changed empty check",
    "Deleted lines": 3,
    "Added lines": 1,
    "Changed lines": 4,
    "Deleted code": "      if (isinstance(checkpointable_object,\n                     data_structures.CheckpointableDataStructure) and\n              len(checkpointable_object.variables) == 0):",
    "Added code": "      if not checkpointable_object._checkpoint_dependencies:"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5ed3c7881f1f039b1bb502eb68c65250de3bbac8",
    "Commit message": "Fix ThreadPoolHandle 0 nthreads argument.\n\nIt was reported that a value of 0 leads to a check failure.  Using 0 to indicate\n`port::MaxParallelism`, for consistency with `Dataset`.\n\nFixes #59162\n\nPiperOrigin-RevId: 508092599",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "\n    // For consistency with Dataset, use MaxParallelism if 0 threads are\n    // specified.\n    if (num_threads_ == 0) {\n      num_threads_ = port::MaxParallelism();\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6381a7b127bd276a3817a93e5423b15a06c33419",
    "Commit message": "[tf.data] Add a check for ram_budget == 0 to avoid division by 0 exception when ram_budget is not set.\n\nPiperOrigin-RevId: 410071934\nChange-Id: Ida9fb401ba24367e48066c8a899962877429c3da",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "  if (ram_budget == 0) {\n    return;\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7db8e4fbc0be952daea74a2c3f501183d6006e61",
    "Commit message": "ENH: check x and y is empty dict",
    "Deleted lines": 3,
    "Added lines": 9,
    "Changed lines": 12,
    "Deleted code": "    if y:\n      if target_keys:\n    if target_keys:",
    "Added code": "    ValueError: if x or y is a empty dict.\n    if not x:\n      raise ValueError('x cannot be empty')\n    if y is None:\n      if not y:\n        raise ValueError('y cannot be empty dict, use None instead.')\n\n      if target_keys is None:\n    if target_keys is None:"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a0fe44410e875e8e7775c6c256496bafb1a41b25",
    "Commit message": "Remove the check of NodeItem exists in unfinished_nodes_ in node callback.\n\nThis fixes the failure of RemoteAsyncTest.test_out_of_range_with_while_loop in DEBUG mode.\n\nPiperOrigin-RevId: 298364642\nChange-Id: Iabaa53bb2ee68c84893c07d5bd45dafb2b99cd69",
    "Deleted lines": 1,
    "Added lines": 10,
    "Changed lines": 11,
    "Deleted code": "      DCHECK_GT(result, 0);",
    "Added code": "      // Remove item if it exists in unfinished_nodes_.\n      // With async execution, if two separate nodes failed and enter this\n      // callback, then the second node might not find itself in\n      // unfinished_nodes_ in the following senario:\n      //   1) Callback of the first failed node clears unfinished_nodes_\n      //   2) ClearError is called and executor status_ is set to OK\n      //   3) Callback of the second failed node is triggered\n      // In this case, do not taint the executor status or other note items\n      // because they are inserted after the ClearError.\n      if (result == 0) return;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bd1f1ac1fec05d38f1b8fc98f650c1c55ac06790",
    "Commit message": "Fix operator check",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "      operator_a.is_square is not None and operator_a.is_square is not None):",
    "Added code": "      operator_a.is_square is not None and operator_b.is_square is not None):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5e0c9fff657498f9a74da38b2ce1b4721698a388",
    "Commit message": "Add bounds checks to jpeg parsing code.\nChange: 140740851",
    "Deleted lines": 2,
    "Added lines": 10,
    "Changed lines": 12,
    "Deleted code": "  src->pub.bytes_in_buffer -= jump;\n  src->pub.next_input_byte += jump;",
    "Added code": "  if (jump < 0) {\n    return;\n  }\n  if (jump > src->pub.bytes_in_buffer) {\n    src->pub.bytes_in_buffer = 0;\n    (void)MemFillInputBuffer(cinfo);  // warn with a fake EOI or error\n  } else {\n    src->pub.bytes_in_buffer -= jump;\n    src->pub.next_input_byte += jump;\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cfb13fa789bcf1cdbbf0fd38cf7568b7098ab99b",
    "Commit message": "Added an additional check on the length of the values and boundaries lists.\n\nPiperOrigin-RevId: 172510229",
    "Deleted lines": 2,
    "Added lines": 5,
    "Changed lines": 7,
    "Deleted code": "        `values` do not match.\n",
    "Added code": "        `values` do not match or\n        the number of elements in the lists does not match.\n  if len(boundaries) != len(values) - 1:\n    raise ValueError(\n        \"The length of boundaries should be 1 less than the length of values\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/28dacabab5aac2963e37e622f4b157cf00d82662",
    "Commit message": "[tf] Explicitly check that runner index is in bounds and runner is available\n\nPiperOrigin-RevId: 410516176\nChange-Id: I8caecaffb958f3cd5251d85a9f6dc7cbc5c06742",
    "Deleted lines": 2,
    "Added lines": 7,
    "Changed lines": 9,
    "Deleted code": "    DCHECK_GT(runners_.size(), index);\n    DCHECK(result.has_value());",
    "Added code": "    // Out of bounds vector access will throw an exception and anyway will crash\n    // the binary, prefer a more readable error message.\n    CHECK_GT(runners_.size(), index)  // Crash OK\n        << \"runner index is out of bounds: index=\" << index\n        << \" size=\" << runners_.size();\n    CHECK(result.has_value())  // Crash OK\n        << \"runner is not available: index=\" << index;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/eb921122119a6b6e470ee98b89e65d721663179d",
    "Commit message": "Prevent heap OOB read in TFLite's `gather.cc`.\n\nPassing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\n\nPiperOrigin-RevId: 387231300\nChange-Id: I3111b54b2f232638d795be17efc46abe4ede6bf8",
    "Deleted lines": 16,
    "Added lines": 53,
    "Changed lines": 69,
    "Deleted code": "TfLiteStatus Gather(const TfLiteGatherParams& params, const TfLiteTensor* input,\n                    const TfLiteTensor* positions, TfLiteTensor* output) {\n        return Gather<float, int32_t>(*params, input, positions, output);\n        return Gather<uint8_t, int32_t>(*params, input, positions, output);\n        return Gather<int8_t, int32_t>(*params, input, positions, output);\n        return Gather<int16_t, int32_t>(*params, input, positions, output);\n        return Gather<int32_t, int32_t>(*params, input, positions, output);\n        return Gather<int64_t, int32_t>(*params, input, positions, output);\n        return Gather<bool, int32_t>(*params, input, positions, output);\n        return Gather<float, int64_t>(*params, input, positions, output);\n        return Gather<uint8_t, int64_t>(*params, input, positions, output);\n        return Gather<int8_t, int64_t>(*params, input, positions, output);\n        return Gather<int16_t, int64_t>(*params, input, positions, output);\n        return Gather<int32_t, int64_t>(*params, input, positions, output);\n        return Gather<int64_t, int64_t>(*params, input, positions, output);\n        return Gather<bool, int64_t>(*params, input, positions, output);",
    "Added code": "TfLiteStatus Gather(TfLiteContext* context, const TfLiteGatherParams& params,\n                    const TfLiteTensor* input, const TfLiteTensor* positions,\n                    TfLiteTensor* output) {\n  const PositionsT* indexes = GetTensorData<PositionsT>(positions);\n  bool indices_has_only_positive_elements = true;\n  const size_t num_indices = positions->bytes / sizeof(PositionsT);\n  for (size_t i = 0; i < num_indices; i++) {\n    if (indexes[i] < 0) {\n      indices_has_only_positive_elements = false;\n      break;\n    }\n  }\n  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n\n\n  bool indices_has_only_positive_elements = true;\n  const size_t num_indices = positions->bytes / sizeof(PositionT);\n  for (size_t i = 0; i < num_indices; i++) {\n    if (indexes[i] < 0) {\n      indices_has_only_positive_elements = false;\n      break;\n    }\n  }\n  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n\n        return Gather<float, int32_t>(context, *params, input, positions,\n                                      output);\n        return Gather<uint8_t, int32_t>(context, *params, input, positions,\n                                        output);\n        return Gather<int8_t, int32_t>(context, *params, input, positions,\n                                       output);\n        return Gather<int16_t, int32_t>(context, *params, input, positions,\n                                        output);\n        return Gather<int32_t, int32_t>(context, *params, input, positions,\n                                        output);\n        return Gather<int64_t, int32_t>(context, *params, input, positions,\n                                        output);\n        return Gather<bool, int32_t>(context, *params, input, positions,\n                                     output);\n        return Gather<float, int64_t>(context, *params, input, positions,\n                                      output);\n        return Gather<uint8_t, int64_t>(context, *params, input, positions,\n                                        output);\n        return Gather<int8_t, int64_t>(context, *params, input, positions,\n                                       output);\n        return Gather<int16_t, int64_t>(context, *params, input, positions,\n                                        output);\n        return Gather<int32_t, int64_t>(context, *params, input, positions,\n                                        output);\n        return Gather<int64_t, int64_t>(context, *params, input, positions,\n                                        output);\n        return Gather<bool, int64_t>(context, *params, input, positions,\n                                     output);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bb6a0383ed553c286f87ca88c207f6774d5c4a8f",
    "Commit message": "Prevent heap OOB read in TFLite's `gather_nd.cc`.\n\nPassing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\n\nPiperOrigin-RevId: 387208551\nChange-Id: I6b7a8a62d3e7c13a16d81619e5bc23ae2cdbc7fd",
    "Deleted lines": 0,
    "Added lines": 11,
    "Changed lines": 11,
    "Deleted code": "",
    "Added code": "  bool indices_has_only_positive_elements = true;\n  const auto* indices_values = GetTensorData<IndicesT>(indices);\n  const size_t num_indices = indices->bytes / sizeof(IndicesT);\n  for (size_t i = 0; i < num_indices; i++) {\n    if (indices_values[i] < 0) {\n      indices_has_only_positive_elements = false;\n      break;\n    }\n  }\n  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7535f6beb7ba95bf54e1513b0c2c51b844a7a49f",
    "Commit message": "Bounds-check node ID before getting it's name (#18090)\n\nWhen the edge is either a frame enter or exit edge then\r\nDescribeCycle() would segfault.",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "#include \"tensorflow/core/kernels/bounds_check.h\"\n    if (!FastBoundsCheck(node_id, graph.num_node_ids())) {\n      return string(\"(null)\");\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/22783fdf812b700f7de9980038ab41ee0a4a2284",
    "Commit message": "Add checks recently removed\n\nPiperOrigin-RevId: 371415079\nChange-Id: I25fa185165541e940d749ba0ff76b12b7ce27394",
    "Deleted lines": 0,
    "Added lines": 10,
    "Changed lines": 10,
    "Deleted code": "",
    "Added code": "    if (index < 0 || index >= node_->inputs->size) {\n      // If larger, this can be an older model with fewer input tensors than the\n      // current implementation.\n      return absl::OutOfRangeError(\"Invalid data index found.\");\n    }\n    if (tensor_id < 0) {\n      return absl::InvalidArgumentError(\n          \"Invalid data index found. Possibly an unset optional tensor is \"\n          \"being read.\");\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/459b4bfe1f73737fae23aa1499b06a69605d0f65",
    "Commit message": "Added a check in EagerExecutor to avoid getting invalid range.\n\nPiperOrigin-RevId: 339155581\nChange-Id: Id3dd028f2ea6ae1e2889b1ef6661796d44203c5a",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    if (upperbound_id < id) {\n      return;\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cddca76312f5ae4fb92a101e79eeff6d5ac16932",
    "Commit message": "Add check for reading input tensors at an index that is out of range.\n\nPiperOrigin-RevId: 324646398\nChange-Id: I602b23b2f28504c20a6d099874cdba2ddbf5ca83",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "    if (idx < 0 || idx >= node_->inputs->size) {\n      // If larger, this can be an older model with fewer input tensors than the\n      // current implementation.\n      return absl::OutOfRangeError(\"Invalid data index found.\");\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/823b694639a3f49b6adbf9e73a08c529d583878e",
    "Commit message": "Add bounds checking when looking at the stack in TF Registry.\n\nThis was breaking integration with R, fixes #25262.\n\nPiperOrigin-RevId: 231367381",
    "Deleted lines": 2,
    "Added lines": 6,
    "Changed lines": 8,
    "Deleted code": "    user_function = stack[2]\n    location_tag = tf_stack.convert_stack([user_function])[0]",
    "Added code": "    stack_index = min(2, len(stack)-1)\n    if stack_index >= 0:\n      user_function = stack[stack_index]\n      location_tag = tf_stack.convert_stack([user_function])[0]\n    else:\n      location_tag = \"UNKNOWN\""
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b1c9e600e02b93885dbebfa5dae92436c63d6c03",
    "Commit message": "[XLA] Add range check for xla::Array<> indexing.\n\nPiperOrigin-RevId: 356981991\nChange-Id: I73343a8776b0df0f2570bcd596247164c8588cb9",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "    DCHECK_LT(index, this->num_elements());"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1908d7ef706f0f3f8c7a300068355bf795fb3d17",
    "Commit message": "Fix out-of-bounds StringPiece access in ForwardNUTF8CharPositions()\n\nEven a simple invocation like 'int p = 0; ForwardNUTF8CharPositions(\"a\", 1, &p);' will cause an invalid access to in[1]. Checking for *pos < size before that access fixes this issue.\n\nLuckily the invalid access has only ever happened when the *pos < size part of the condition is false and thus the outcome of the IsTrailByte check is irrelevant. Thus this probably hasn't had any observable impact except when extra guards against invalid memory accesses are enabled.\n\nPiperOrigin-RevId: 488292704",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    } while (IsTrailByte(in[*pos]) && *pos < size);",
    "Added code": "    } while (*pos < size && IsTrailByte(in[*pos]));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d414a925a73553e4dd0d559d2d275668a298dab4",
    "Commit message": "Check against the size of a std::vector to prevent out-of-boundary access.\n\nPiperOrigin-RevId: 343043192\nChange-Id: Icc6f7338791709c1d9dfe20e985fbcdd50ed3336",
    "Deleted lines": 1,
    "Added lines": 9,
    "Changed lines": 10,
    "Deleted code": "    expanded_dim_sizes[sparsity->block_map()->Get(i)] /= block_dim_size;",
    "Added code": "    if (original_block_dim < 0 || original_block_dim >= total_dims) {\n      return absl::nullopt;\n    }\n\n    int mapped_block_dim = sparsity->block_map()->Get(i);\n    if (mapped_block_dim < 0 || mapped_block_dim >= total_dims) {\n      return absl::nullopt;\n    }\n    expanded_dim_sizes[mapped_block_dim] /= block_dim_size;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/aa54f547f04c3007b26df2379c6cf5f081948d0b",
    "Commit message": "Updated the check_numerics function to also validate the gradient corresponding\nto the tensor it's validating\nChange: 130645982",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "  return grad",
    "Added code": "  return array_ops.check_numerics(\n      grad, \"Not a number (NaN) or infinity (Inf) values detected in gradient.\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/236660d0fccff6f59f29a1936dc731d783722e28",
    "Commit message": "[XLA:GPU] Fix host conv checker canonicalization for f16 and nans.\n\nThe GPU-side checker is correct, but the host-side checker was canonicalizing\nnan to F16_MAX.  The effect of this is that you'd get a \"conv mismatch!\" error\nbut no description of exactly what mismatched.\n\nPiperOrigin-RevId: 241062942",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "      if (std::isnan(a)) {\n        return a;\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8c6f391a2282684a25cbfec7687bd5d35261a209",
    "Commit message": "[lite] Add check for bias_size is zero to avoid division by zero. This shouldn't happen for properly converted models. Just safety check\n\nPiperOrigin-RevId: 416383645\nChange-Id: If8e508bf696ae8ecfb927e69c139a8ccf7fe60cb",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "  if (bias_size == 0) return;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/582bf0d3ac33fc10156f737c0d42f3adee54409a",
    "Commit message": "Update the tflite model \"buffers\" field checking rule.\n\nIf we don't use \"--force-empty-vectors\" flag[1] for flatc, the buffers\nmight be a null ptr if we serialize a model with zero buffers size(e.g.\nall ops in model doesn't use const weights in model).\nThis commit relaxs the \"buffers\" null ptr checking for this situation,\nand also updates the \"subgraphs\" checking for null ptr dereference.\n\n[1]\nhttps://github.com/google/flatbuffers/blob/fc4fffea41f5b682198edfc72eca536d33fd848c/src/flatc.cpp#L167",
    "Deleted lines": 8,
    "Added lines": 6,
    "Changed lines": 14,
    "Deleted code": "      if (tensor->buffer() == 0) return kTfLiteOk;\n      if (tensor->buffer() >= buffers->size()) {\n            i, tensor->buffer(), buffers->size());\n  if (subgraphs->size() == 0) {\n  if (!buffers) {\n    TF_LITE_REPORT_ERROR(error_reporter_, \"No buffers in the model.\\n\");\n    return cleanup_and_error();\n  }",
    "Added code": "      if (tensor->buffer() == 0) {\n        return kTfLiteOk;\n      }\n      if (!buffers || tensor->buffer() >= buffers->size()) {\n            i, tensor->buffer(), (buffers) ? buffers->size() : 0);\n  if (!subgraphs || subgraphs->size() == 0) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7008e41f183ae9de3f4656067932b36afa822ef2",
    "Commit message": "Fix the check for empty reduction indices\n\n- In the general case indices can be any rank.",
    "Deleted lines": 2,
    "Added lines": 7,
    "Changed lines": 9,
    "Deleted code": "  *indices_is_empty =\n      reduction_indices_tensor.tensor_shape().dim(0).size() == 0;",
    "Added code": "  *indices_is_empty = false;\n  for (const auto& dim : reduction_indices_tensor.tensor_shape().dim()) {\n    if (dim.size() == 0) {\n      *indices_is_empty = true;\n      break;\n    }\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/551a90f2e3d20420d68a2796d19f1c42b6636e0d",
    "Commit message": "Add checks in ReduceWindowOpOnTensorsConversion.\n\nThe pattern does not support ops with non-zero padding config. Add a check to\nprevent unexpected lowering.\n\nIt is not easy to add tests because other patterns will convert body ops, and\nit causes issues like invalid IRs.\n\nPiperOrigin-RevId: 367202450\nChange-Id: Ibe3a7c904cda09c2fff122e303acdd9daea208ac",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "    if (op.padding() && !isSplatValue(*op.padding(), 0)) {\n      return rewriter.notifyMatchFailure(op, \"require paddings are all zero\");\n    }\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1071f554dbd09f7e101324d366eec5f4fe5a3ece",
    "Commit message": "Add missing validation to `RaggedTensorToSparse`.\n\nThere needs to be a check that the splits allow for valid ragged tensors.\n\nPiperOrigin-RevId: 387712169\nChange-Id: I2499175324b82b65d159a260c7f83b98ceb5cc7d",
    "Deleted lines": 1,
    "Added lines": 11,
    "Changed lines": 12,
    "Deleted code": "    DCHECK_GT(rt_nested_splits_len, 0);  // Enforced by REGISTER_OP.",
    "Added code": "#include \"tensorflow/core/platform/errors.h\"\n    OP_REQUIRES(context, rt_nested_splits_len > 0,\n                errors::InvalidArgument(\"rt_nested_splits must be non empty\"));\n      for (int j = 1; j < rt_nested_splits[i].size(); ++j) {\n        if (rt_nested_splits[i](j) < rt_nested_splits[i](j - 1)) {\n          return InvalidArgument(\n              \"Ragged splits should be non decreasing, but we got \",\n              rt_nested_splits[i](j - 1), \" followed by \",\n              rt_nested_splits[i](j));\n        }\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f6f62119587baf8ccb7378ceac86bacd2db2863d",
    "Commit message": "Add missing validation in maxpooling_op.cc\n\nPiperOrigin-RevId: 372225911\nChange-Id: I85d5e2c5640a659cda0e7cdc0dcb6dc82564210d",
    "Deleted lines": 0,
    "Added lines": 9,
    "Changed lines": 9,
    "Deleted code": "",
    "Added code": "    // Given access patterns in SpatialMaxPoolWithArgMaxHelper, these tensors\n    // must have elements.\n    OP_REQUIRES(\n        context, tensor_out_arg_max.NumElements() > 0,\n        errors::InvalidArgument(\"tensor_out_arg_max must not be empty, got \",\n                                tensor_out_arg_max.DebugString()));\n    OP_REQUIRES(context, out_backprop.NumElements() > 0,\n                errors::InvalidArgument(\"out_backprop must not be empty, got \",\n                                        out_backprop.DebugString()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b86513673b98ac6c4458033fcda718365539afae",
    "Commit message": "added check for zero stride values to strided slice",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    if (attr.strides.h == 0 || attr.strides.w == 0 || attr.strides.c == 0) {\n      return InvalidArgumentError(\"stride values must be non-zero\");\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4377a561b7757ed83757f07532e6564c42c286ba",
    "Commit message": "Add a check for group size when sorting grouped AllReduces within a block.\n\nPiperOrigin-RevId: 538255219",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "        // Maintain relative order of ALLReduces within the block.\n                    if (lhs.empty() || rhs.empty()) {\n                      // Skip order check if either group is empty.\n                      return false;\n                    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/31bd5026304677faa8a0b77602c6154171b9aec1",
    "Commit message": "Prevent check fail in FFT\n\nPiperOrigin-RevId: 372031044\nChange-Id: I50994e3e8a5d1342d01bde80256f6bf2730ca299",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(ctx, temp_shape.num_elements() > 0,\n                errors::InvalidArgument(\"Obtained a FFT shape of 0 elements: \",\n                                        temp_shape.DebugString()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1c56f53be0b722ca657cbc7df461ed676c8642a2",
    "Commit message": "Fix a check fail in Fast Fourier implementation\n\nPiperOrigin-RevId: 372026629\nChange-Id: Id05c3362aa575271bc3e06b16316c9037085fc11",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "#include \"tensorflow/core/platform/errors.h\"\n    OP_REQUIRES(ctx, full_fft_shape.num_elements() > 0,\n                errors::InvalidArgument(\"Obtained a FFT shape of 0 elements: \",\n                                        full_fft_shape.DebugString()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/25bae42b3022b00788a29ae6c400922c31f88231",
    "Commit message": "Add additional length check for inputs\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  if all(shape is not None for shape in shapes_value):",
    "Added code": "  if len(shapes_value) != 0 and all(shape is not None for shape in shapes_value):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e07e48b2e0908333a36f1c5726a9406a83b3ec90",
    "Commit message": "Added a check on literal_.has_value() to avoid segfault.\n\nPiperOrigin-RevId: 363008898\nChange-Id: I1a17b58f7e05ccb7db70ded82522030258239fef",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    if (!literal_.has_value()) {\n      return \"{...}\";\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cc560f64b6e3e6724517757e9789c52cde224ee9",
    "Commit message": "Profiler: restore correct behavior of StartTracing with empty workers list.\n\nabsl::StrSplit behaves differently from str_util::Split when the passed string is empty. Restore previous behavior by explicitly checking for an empty string.\nPiperOrigin-RevId: 285460069\nChange-Id: I8fd2768d32c8ee566ce5dcd9ab9e404242703597",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "  std::vector<tensorflow::string> hostnames = absl::StrSplit(workers_list, ',');",
    "Added code": "  std::vector<tensorflow::string> hostnames;\n  if (!workers_list.empty()) {\n    hostnames = absl::StrSplit(workers_list, ',');\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fe4f74018ec6a7dff2718ea59d0f317460c0b3ad",
    "Commit message": "Temporarily check for empty proto fields to avoid a crash for old cached traces.\n\nWe can remove this code once we land all the necessary changes and invalidate all the caches.\n\nPiperOrigin-RevId: 499545306",
    "Deleted lines": 0,
    "Added lines": 11,
    "Changed lines": 11,
    "Deleted code": "",
    "Added code": "  // TODO(dfinchel): remove this temporary change to avoid crash.\n  // This is only needed while we make an update to proto version that is not\n  // backwards compatible.\n  if (peak_mem_gibibytes_per_second_per_core.size() !=\n      (MemBwType_MAX - MemBwType_MIN + 1)) {\n    peak_mem_gibibytes_per_second_per_core.clear();\n    for (int i = MemBwType_MIN; i <= MemBwType_MAX; ++i) {\n      peak_mem_gibibytes_per_second_per_core.push_back(0);\n    }\n  }\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c56d0cd8ce8239ee369fac1ae6b9cae67fd4c447",
    "Commit message": "Avoid signed integer overflow when loading tensors with both 0 and large dims.\n\n`TensorShapeBase` ensures `num_elements` doesn't overflow when adding a new\ndimension. However, this check is insufficient to prevent other functions that\nuse a different multiplication order from hitting an overflow _if any of the\ndimensions are 0_. For example, Eigen currently multiplies dimensions in\nreverse order, so dimensions of (0, 4294967296,4294967296) will trigger an\noverflow in Eigen code.\n\nTo prevent overflow for all multiplication orders, we can that `num_elements`\ndoesn't overflow if zero dimensions are skipped.\n\nPiperOrigin-RevId: 477796726",
    "Deleted lines": 0,
    "Added lines": 15,
    "Changed lines": 15,
    "Deleted code": "",
    "Added code": "    int64_t num_elements_excluding_zero_dims = 1;\n      // If one of the dimensions has size 0, multiplying the dimensions in\n      // ascending order isn't sufficient to prevent all multiplication orders\n      // from overflowing. To do that, we need to check that there would be no\n      // overflow if all zero-length dimensions were multiplied last, which is\n      // equivalent to ensuring that there's no overflow if zero-length\n      // dimensions are skipped. Unknown dimensions are also ignored.\n      if (d.size() > 0) {\n        num_elements_excluding_zero_dims =\n            MultiplyWithoutOverflow(num_elements_excluding_zero_dims, d.size());\n        if (TF_PREDICT_FALSE(num_elements_excluding_zero_dims < 0)) {\n          return errors::InvalidArgument(\n              \"Encountered overflow when multiplying shape dimensions\");\n        }\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/80bb2f5511e7d2d386c79da52ff517691e19ac54",
    "Commit message": "Add check condition for large values of range_max, which is causing session abort.\nFixes: https://github.com/tensorflow/tensorflow/issues/46938\n\nPiperOrigin-RevId: 450688945",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "  # Limiting to Max int32 value\n  if range_max > 2147483647:\n    raise ValueError(f'Value of range_max:{range_max} is too large to handle')"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0666d8bb711b41c9f03dec238d7d165bc946fc70",
    "Commit message": "Prevent crash of tensorflow if shape is too large for tf.sparse.reorder\n\nThis PR tries to address the issue raised in 45392 where\ntensorflow crashes if shape of sparse tensor is too large for\ntf.sparse.reorder\n\nThis PR adds additional checks and exit gracefully if the shape\nis too large.\n\nThis PR fixes 45392.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 0,
    "Added lines": 13,
    "Changed lines": 13,
    "Deleted code": "",
    "Added code": "#include \"tensorflow/core/util/overflow.h\"\n    // Check if the sparse tensor input shape is valid\n    int64 total = 1;\n    for (int64 i = 0; i < input_shape_in.NumElements(); ++i) {\n      int dim = input_shape_in.vec<int64>()(i);\n      OP_REQUIRES(context, (dim >= 0),\n                  errors::InvalidArgument(\"Dimension \", dim, \" must be >= 0\"));\n      total = MultiplyWithoutOverflow(total, dim);\n      OP_REQUIRES(context, (total > 0),\n                  errors::InvalidArgument(\n                      \"Shape would have more than 2**63 - 1 elements\"));\n    }\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e6390bc13471f28f211cab874cc49a123505dc3e",
    "Commit message": "Update histogram_ops.py\n\nAdded the condition to check the negative value of nbins input",
    "Deleted lines": 0,
    "Added lines": 8,
    "Changed lines": 8,
    "Deleted code": "",
    "Added code": "    ValueError: If the value of nbins is negative.\n  if nbins < 0:\n    raise ValueError(\"nbins should be a positive number.\")\n\n    ValueError: If the value of nbins is negative.\n  if nbins < 0:\n    raise ValueError(\"nbins should be a positive number.\")\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/43a8963c73718f97a4425722a65b611d2ef0b69f",
    "Commit message": "Added non-negative check for n.",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "      not `-1`, or `norm` is not `None` or `'ortho'`.",
    "Added code": "  if n is not None and n < 1:\n    raise ValueError(\"n should be an integer greater than 1 or None\")\n      not `-1`, `n` is not `None` or greater than 0,\n      or `norm` is not `None` or `'ortho'`."
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4ea68093eeaf4c4157368668afd7f809b806a504",
    "Commit message": "Add negative parameter validation to convolution layers.",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    if filters < 0:\n      raise ValueError(\"Recieved a negative value for `filters`,\n                       \"was expecting a positive value.\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1223335a8d34a8ce656dbd10b2a236ef6204ff47",
    "Commit message": "Add negative parameter validation for recurrent layers.",
    "Deleted lines": 0,
    "Added lines": 9,
    "Changed lines": 9,
    "Deleted code": "",
    "Added code": "    if units < 0:\n      raise ValueError(\"Received a negative value for `units`, \",\n                       \"expected a positive value.\")\n    if units < 0:\n      raise ValueError(\"Received an negative value for `units`, \"\n                       \"expected a positive value.\")\n    if units < 0:\n      raise ValueError(\"Received a negative value for `units`, \"\n                       \"expected a postiive value.\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/64afe2d199ec4513223bbf5176835bf681cf056b",
    "Commit message": "Add negative parameter validation to Core Keras layers.",
    "Deleted lines": 1,
    "Added lines": 9,
    "Changed lines": 10,
    "Deleted code": "",
    "Added code": "    if isinstance(rate, (int, float)) and rate < 0:\n      raise ValueError(\"Invalid value received for `rate`, expected \"\n                       \"a value between 0 and 1.\")\n    if not isinstance(n, int):\n      raise TypeError(\"Expected an integer value for `n`.\")\n\n    if self.units < 0:\n      raise ValueError(f\"Received an invalid value for `units`, expected\n                       f\"a positive integer, got {units}.\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/199f1ff12a28d571100b323ec54a5eee47078d8b",
    "Commit message": "Add necessary check in fft ops to fix crash\n\nThis PR tries to address the issue raised in 55263 where\ntf.single.rfft2d will crash when length contains negative value.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "        OP_REQUIRES(\n            ctx,\n            fft_length_as_vec(i) >= 0,\n            errors::InvalidArgument(\n                \"fft_length[\" , i,\n                \"] must >= 0, but got: \", fft_length_as_vec(i)));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/685418cd85e09bc2117fa15bc1b6a75d21248348",
    "Commit message": "maxpooling op should check that ksize must be positive.\n\nPiperOrigin-RevId: 520539022",
    "Deleted lines": 0,
    "Added lines": 7,
    "Changed lines": 7,
    "Deleted code": "",
    "Added code": "      OP_REQUIRES(\n          context,\n          ksize_[0] > 0 && ksize_[1] > 0 && ksize_[2] > 0 && ksize_[3] > 0,\n          errors::InvalidArgument(\"Sliding window ksize must be positive.\"));\n    OP_REQUIRES(\n        context, ksize[0] > 0 && ksize[1] > 0 && ksize[2] > 0 && ksize[3] > 0,\n        errors::InvalidArgument(\"Sliding window ksize must be positive.\"));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/076f909b70b251daea6c443c9b1929b9745aed20",
    "Commit message": "fix boolean expression in length check\n\nPiperOrigin-RevId: 513891216",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    OP_REQUIRES(ctx, length,",
    "Added code": "    OP_REQUIRES(ctx, length > 0,"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3acc8eaf602b3e9a009f54e1e0164644dd793831",
    "Commit message": "Add sanity check for resize-bilinear input shape.\n\nPiperOrigin-RevId: 245618186",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "  const int32* size_data = GetTensorData<int32>(size);",
    "Added code": "  const int32* size_data = GetTensorData<int32>(size);\n  // Sanity check, the up/down sampling size should always be positive.\n  TF_LITE_ENSURE(context, size_data[0] > 0);\n  TF_LITE_ENSURE(context, size_data[1] > 0);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fffbe5a26da2d6fab5a3eb648cefef49db4d38de",
    "Commit message": "Check if the session has been deleted before releasing a callable.\n\nIn some versions of Python, the Session._session field may be cleared\n(in `Session.__del__()`) before a callable that has a reference to\nthat Session is deleted. Add a defensive check in the\n`Session._Callable.__del__()` method.\n\nPiperOrigin-RevId: 192679796",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "      if self._handle is not None:",
    "Added code": "      # NOTE(mrry): It is possible that `self._session.__del__()` could be\n      # called before this destructor, in which case `self._session._session`\n      # will be `None`.\n      if self._handle is not None and self._session._session is not None:"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dcce6044dc05ed2e6cda601df5b300333859be4f",
    "Commit message": "CLN: not check None",
    "Deleted lines": 3,
    "Added lines": 3,
    "Changed lines": 6,
    "Deleted code": "    if y is None:\n      if target_keys is None:\n    if target_keys is None:",
    "Added code": "    if y:\n      if target_keys:\n    if target_keys:"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9ce847ed140702d1dd4cb204a8afe0ffedb70b15",
    "Commit message": "Remove a few check ops that no longer need to run in tf.Variable's constructor\n\nVarHandleOp ensures there is no sharing. These aren't a huge part of startup time for replicated models, but there's still no reason to run them.\n\nPiperOrigin-RevId: 335117818\nChange-Id: I38a0f944a565907630f1da0ef6d896a633b296c0",
    "Deleted lines": 16,
    "Added lines": 9,
    "Changed lines": 25,
    "Deleted code": "from tensorflow.python.ops import gen_logging_ops\n    # We do not want two distinct ResourceVariable objects for the same\n    # underlying resource in the runtime.\n    # When in eager mode, explicitly ensure so here. When in graph mode, it's\n    # ensured by always generating different variable names.\n    exists = gen_resource_variable_ops.var_is_initialized_op(handle)\n\n    # We create an assert Op instead of checking right away in order to be\n    # compatible with ASYNC execution mode. Further, since not all devices\n    # support string tensors, we encode the assertion string in the Op name\n    gen_logging_ops._assert(  # pylint: disable=protected-access\n        math_ops.logical_not(exists), [exists],\n        name=\"EagerVariableNameReuse\")\n\n          shared_name = context.shared_name()\n          shared_name = context.shared_name()",
    "Added code": "from tensorflow.python.framework import errors\n  if not graph_mode:\n    if shared_name is not None:\n      raise errors.InternalError(\n          \"Using an explicit shared_name is not supported executing eagerly.\")\n    shared_name = context.shared_name()\n\n          shared_name = None  # Never shared\n          shared_name = None  # Never shared"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ebeb598c2d1f341d6d641bf58c370cf7b43f6e37",
    "Commit message": "Correctly check shape not None in Keras `add_weight`.\nWhen calling Keras add_weight with a np list, as written the `shape or\n()` \"trick\" results in the following exception:\n\"\"\"ValueError: The truth value of an array with more than one element is\nambiguous. Use a.any() or a.all()\"\"\"\nThis change fixes the problem by using an explicit `if`.\n\nPiperOrigin-RevId: 236407103",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "    shape = shape or ()",
    "Added code": "    if shape is None:\n      shape = ()"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c7c4a42c4372ca560ea415fe3a798e18286cedec",
    "Commit message": "Fix an error in keras input_layer.Input() dtype type checking.\n\nPiperOrigin-RevId: 228215444",
    "Deleted lines": 2,
    "Added lines": 3,
    "Changed lines": 5,
    "Deleted code": "    elif input_tensor and input_tensor.dtype != dtype:\n      raise ValueError('`input_tensor.dtype` differs from `dtype`.')",
    "Added code": "    elif input_tensor is not None and input_tensor.dtype != dtype:\n      raise ValueError('`input_tensor.dtype` differs from `dtype`: %s vs. %s' %\n                       (input_tensor.dtype, dtype))"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bc7b64fe998cb0f118eace5bc29b52554eeda3f1",
    "Commit message": "Added back the channel dimension check as a known channel dimension is required by creating beta.\nChange: 150511737",
    "Deleted lines": 3,
    "Added lines": 5,
    "Changed lines": 8,
    "Deleted code": "      channels = array_ops.shape(inputs)[-1]\n      outputs = array_ops.reshape(outputs, array_ops.shape[original_inputs])\n                        functools.reduce(lambda x, y: x * y, spatial_dims)])",
    "Added code": "      channels = inputs.get_shape()[-1].value\n      if channels is None:\n        raise ValueError('`C` dimension must be known but is None')\n      outputs = array_ops.reshape(outputs, array_ops.shape(original_inputs))\n                         functools.reduce(lambda x, y: x * y, spatial_dims)])"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a5b8d6c4694e4cd3e3cc4a162053ab0dfa6e174f",
    "Commit message": "Relax the check for whether the relevant aggregation dimensions are known ahead of time.",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if x_shape.is_fully_defined():",
    "Added code": "    if all(x_shape[d].value is not None for d in axes):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0d65cfaab050295c311d9f2fb28388435359db27",
    "Commit message": "Add an additional `NoneType` check when converting a traced tensor to a `KerasTensor`.\n\nPiperOrigin-RevId: 336694750\nChange-Id: Ic79bc3b46b81d4816a7108ed9b1aa426e7f4d3d5",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "      if (type_spec.dtype == dtypes.int32 and type_spec.shape.rank < 2):",
    "Added code": "      if (type_spec.dtype == dtypes.int32 and type_spec.shape.rank is not None\n          and type_spec.shape.rank < 2):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/31849c61e0432009baabdfafc2ec1a1aed1a40e8",
    "Commit message": "Small change in tf.nn.sufficient_statistics to guard against unknown shapes.\n\nUse is_fully_defined instead of checking shape.dims[d] as the dims variable may be None, if the rank is unknown.\n\nPiperOrigin-RevId: 239829841",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "    if all(x_shape.dims[d].value is not None for d in axes):",
    "Added code": "    if x_shape.rank is not None and all(\n        x_shape.dims[d].value is not None for d in axes):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/30bd9d5bcc64097d21872486a5726d756ed7067b",
    "Commit message": "Explicitly handle Tensors in start & stop\n\nThe current check was doing a identity check in order to handle both\ntensors and integers. This becomes problematic when enabling tensor\nequality. Instead we explicitly check for Tensor type and only compare\nwith sys.maxsize for non-Tensors.\n\nPiperOrigin-RevId: 261004200",
    "Deleted lines": 2,
    "Added lines": 4,
    "Changed lines": 6,
    "Deleted code": "      if s.start is not None and s.start is not sys.maxsize:\n      if s.stop is not None and s.stop != sys.maxsize:",
    "Added code": "      if s.start is not None and (isinstance(s.start, ops.Tensor) or\n                                  s.start != sys.maxsize):\n      if s.stop is not None and (isinstance(s.stop, ops.Tensor) or\n                                 s.stop != sys.maxsize):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fb1c4cd8283f262bca95ccd04df6f9eb4ae1da0c",
    "Commit message": "Add None check for seq_len_mask before reshape.\nChange: 150477638",
    "Deleted lines": 4,
    "Added lines": 7,
    "Changed lines": 11,
    "Deleted code": "    seq_len_mask = array_ops.reshape(\n        seq_len_mask,\n        array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), 0))\n    return m * seq_len_mask if memory_sequence_length is not None else m",
    "Added code": "    if memory_sequence_length is not None:\n      seq_len_mask = array_ops.reshape(\n          seq_len_mask,\n          array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), 0))\n      return m * seq_len_mask\n    else:\n      return m"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a0ca4bcb81dfd07fdb1c7872b5852f84cfc1a081",
    "Commit message": "Fix separable convolution bias check\nChange: 150385615",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if self.bias:",
    "Added code": "    if self.bias is not None:"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1ff493ed1a2059f82f7607a7f0a0aa2ce8d5a542",
    "Commit message": "Replace a defensive check with TF_RET_CHECK\n\nPiperOrigin-RevId: 246544023",
    "Deleted lines": 4,
    "Added lines": 2,
    "Changed lines": 6,
    "Deleted code": "    if (!device_name.empty()) {\n      // TODO(sanjoy): Figure out if this is necessary.\n      device_names_set.insert(device_name);\n    }",
    "Added code": "    TF_RET_CHECK(!device_name.empty());\n    device_names_set.insert(device_name);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/201982013046116767545cda18137b38abb39468",
    "Commit message": "toco: Fix missing check for buffer in ResizeBilinear.\n\nPiperOrigin-RevId: 181209975",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "  if (!output_size_array.buffer) {\n    return;\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c676a2d7ce8884aad59ca9cd5f45e9b851574cac",
    "Commit message": "[tensorflow] Add a check that strided slice op strides argument has reasonable size\n\nPiperOrigin-RevId: 478036251",
    "Deleted lines": 1,
    "Added lines": 9,
    "Changed lines": 10,
    "Deleted code": "    return errors::InvalidArgument(\"Unexpected negative dense.dims\");",
    "Added code": "    return errors::InvalidArgument(\"Unexpected negative dense.dims: %d\",\n                                   dense->dims);\n  }\n\n  if (dense->dims >= 1024) {\n    // We do not expect to see tensors with rank >= 1024, it must mean that\n    // there is a bug somewhere.\n    return errors::InvalidArgument(\"Unexpected large dense.dims: %d\",\n                                   dense->dims);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f61175812426009a4c96e51befb2951612990903",
    "Commit message": "To add a check of `input_dims` greater than zero in embedding layers.\nFixes: https://github.com/tensorflow/tensorflow/issues/37777\n\nPiperOrigin-RevId: 303155222\nChange-Id: Ib5a693b7dc6058981f530bc78954dd5430f6982f",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    if self.input_dim <= 0:\n      raise ValueError('The argument `input_dim` should be greater than zero. '\n                       'Received: %s' % input_dim)"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f2a673bd34f0d64b8e40a551ac78989d16daad09",
    "Commit message": "Add missing validation to `matrix_diag_op.cc`\n\nPiperOrigin-RevId: 387923533\nChange-Id: Idfffeb328d5f9c6748d992d28a56d6e9e45103a0",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "      OP_REQUIRES(context, diag_index.NumElements() > 0,\n                  errors::InvalidArgument(\n                      \"Expected diag_index to have at least 1 element\"));\n      OP_REQUIRES(context, diag_index.NumElements() > 0,\n                  errors::InvalidArgument(\n                      \"Expected diag_index to have at least 1 element\"));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a0dc73569fc193c1ce26a7bd2d4a8776e7b813ac",
    "Commit message": "add check for empty cs_prev_tensor",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(ctx,\n        cs_prev_tensor->dim_size(0) > 0 && cs_prev_tensor->dim_size(1) > 0,\n                errors::InvalidArgument(\"cs_prev_tensor is empty, has shape: (\",\n                            cs_prev_tensor->dim_size(0), \",\", cs_prev_tensor->dim_size(1), \").\"));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/48d3e51a1bd128554dd129251a51b6e12918a604",
    "Commit message": "Add a check to HandleFromInput to ensure that the resource isn't empty.\n\nPiperOrigin-RevId: 510250667",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "// TODO(b/228388547) users of this method should be migrated to the one below.\n  if (tensor->NumElements() == 0) {\n    return errors::InvalidArgument(\"Empty resouce handle\");\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/258233804f2bc92b4bdb9714b396aed34b53ff0d",
    "Commit message": "sanity check of empty tensor on avgpool3d_grad",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "      // For empty tensor, avg_pool_3d_grad in oneDNN doesn't handle this case\n      if (orig_input_tensor.NumElements() == 0 ||\n          grad_tensor.NumElements() == 0)\n        return;\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/532f5c5a547126c634fefd43bbad1dc6417678ac",
    "Commit message": "Prevent nullptr deref in validation of indexes in map ops.\n\nPiperOrigin-RevId: 387738023\nChange-Id: I83d18d36a7b82ffd2a40b5124a4e5b4c72238f27",
    "Deleted lines": 6,
    "Added lines": 9,
    "Changed lines": 15,
    "Deleted code": "      return Status(errors::InvalidArgument(\n          \"' was already initialized '\", dtypes_.size(), \"'.\"));\n      return Status(\n          errors::InvalidArgument(\"Indices are not strictly ordered\"));\n      return Status(errors::ResourceExhausted(\n          \"'.\"));",
    "Added code": "      return errors::InvalidArgument(\n          \"' was already initialized '\", dtypes_.size(), \"'.\");\n    if (indices.NumElements() == 0) {\n      return errors::InvalidArgument(\"Indices are empty\");\n    }\n\n      return errors::InvalidArgument(\"Indices are not strictly ordered\");\n      return errors::ResourceExhausted(\n          \"'.\");"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/102cacf28ad5a9e7f00b5a195d1995ead8870006",
    "Commit message": "Add missing validation to `maxpooling_op.cc`\n\nPiperOrigin-RevId: 387737899\nChange-Id: I95d33dac25a3c8e1d23934e623c43e81b7da0ece",
    "Deleted lines": 0,
    "Added lines": 8,
    "Changed lines": 8,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"tensor_in must not be empty\"));\n    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n                errors::InvalidArgument(\"tensor_out must not be empty\"));\n    OP_REQUIRES(context, tensor_in.dims() == 4,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"tensor_in must not be empty\"));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a3d9f9be9ac2296615644061b40cefcee341dcc4",
    "Commit message": "Add missing validation to pooling_ops_3d\n\nPiperOrigin-RevId: 372218727\nChange-Id: I6b9ed4266aa7286c02f1f230d7bea922c1be547e",
    "Deleted lines": 0,
    "Added lines": 13,
    "Changed lines": 13,
    "Deleted code": "",
    "Added code": "    // Given access patterns in LaunchMaxPooling3dGradGradOp, these tensors must\n    // have elements.\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"received empty tensor tensor_in: \",\n                                        tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n                errors::InvalidArgument(\"received empty tensor tensor_out: \",\n                                        tensor_out.DebugString()));\n    OP_REQUIRES(\n        context, out_grad_backprop.NumElements() > 0,\n        errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\n                                out_grad_backprop.DebugString()));\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/27bd8aaa7b58d2591fed43a6c245f3037664cfb1",
    "Commit message": "Fix another Eigen missing validation\n\nPiperOrigin-RevId: 371833155\nChange-Id: I5a23d451132cb1624ad916ef46ea01d0e88ec82c",
    "Deleted lines": 0,
    "Added lines": 8,
    "Changed lines": 8,
    "Deleted code": "",
    "Added code": "\n    OP_REQUIRES(ctx, in0.NumElements() > 0,\n                errors::InvalidArgument(\"In[0] must not be an empty tensor: \",\n                                        in0.DebugString()));\n\n    OP_REQUIRES(ctx, in1.NumElements() > 0,\n                errors::InvalidArgument(\"In[1] must not be an empty tensor: \",\n                                        in1.DebugString()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ab0a5278d81ef34096775d5d56f11694cca2a785",
    "Commit message": "Fix tf.assert_equal issue when one tenor is empty and another is non-empty\n\nThis fix tries to address the issue raised in 32082 where\ntf.assert_equal([], [1.0]) doesn't raise error.\nThe reason was that in assert_equal `[1.0]` was broadcasted\nas `[]` and equal was in place in that situation.\n\nThis PR updates the _binary_asesert so that it will check if\nx, y are both empty or both non-empty. If one is empty and another is\nnon-empty, then assertion throws exception. This change is to not impact\nother ops that depends on the broadcast behavior.\n\nThis fix fixes 32082.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 4,
    "Added lines": 38,
    "Changed lines": 42,
    "Deleted code": "                   message, name):\n      condition = math_ops.reduce_all(test_op)\n      condition = math_ops.reduce_all(op_func(x, y))\n        condition_static = np.all(static_func(x_static, y_static))",
    "Added code": "def _binary_all_empty_or_all_non_empty(x, y):\n  \"\"\"Chek if x and y are either all empty or all non-empty.\n\n  Args:\n    x:  A `Tensor`.\n    y:  A `Tensor`.\n\n  Returns:\n    True if x and y are either all empty or all non-empty\n  \"\"\"\n  all_empty = math_ops.logical_and(\n      math_ops.equal(array_ops.size(x), 0),\n      math_ops.equal(array_ops.size(y), 0))\n  all_non_empty = math_ops.logical_and(\n      math_ops.not_equal(array_ops.size(x), 0),\n      math_ops.not_equal(array_ops.size(y), 0))\n  return math_ops.logical_or(all_empty, all_non_empty)\n\n                   message, name, allow_empty=False):\n      if allow_empty:\n        condition = math_ops.reduce_all(test_op)\n      else:\n        empty_check = _binary_all_empty_or_all_non_empty(x, y)\n        condition = math_ops.logical_and(\n            empty_check, math_ops.reduce_all(test_op))\n      if allow_empty:\n        condition = math_ops.reduce_all(op_func(x, y))\n      else:\n        empty_check = _binary_all_empty_or_all_non_empty(x, y)\n        condition = math_ops.logical_and(\n            empty_check, math_ops.reduce_all(op_func(x, y)))\n        if allow_empty:\n          condition_static = np.all(static_func(x_static, y_static))\n        else:\n          empty_check_static = ((x_static.size == 0 and y_static.size == 0) or\n                                (x_static.size != 0 and y_static.size != 0))\n          condition_static = empty_check_static and np.all(\n              static_func(x_static, y_static))"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dedac5053f1ca2d6a7820e330714e50d2d724cee",
    "Commit message": "Fix edge case bug in handling FP16 weights in XNNPACK delegate\n\nQuasi-static tensors may become subgraph outputs after partitioning; we need to\nexplicitly exclude them from outputs and treat as static tensors.\n\nPiperOrigin-RevId: 313522428\nChange-Id: I621cb575b52caa59910a281078c3c505e796880f",
    "Deleted lines": 3,
    "Added lines": 9,
    "Changed lines": 12,
    "Deleted code": "    const std::unordered_set<int> outputs(\n        &params->output_tensors->data[0],\n        &params->output_tensors->data[params->output_tensors->size]);",
    "Added code": "    std::unordered_set<int> outputs;\n    for (int o = 0; o < params->output_tensors->size; o++) {\n      const int output_tensor_idx = params->output_tensors->data[o];\n      // Exclude quasi-static tensors which may have become subgraph outputs\n      // after partitioning.\n      if (delegate->static_unpacked_data_map_.count(output_tensor_idx) == 0) {\n        outputs.insert(output_tensor_idx);\n      }\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ce589223a5fa78cb12efaf1efd1d8d0e5507bd08",
    "Commit message": "Update nn_ops.py\n\nAdded check for pooling_ratio",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "  if pooling_ratio < 1.0:\n    raise ValueError(\"pooling_ratio should be >= 1.0.\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/63feaf321165e1e2795f43e3834c007364921df6",
    "Commit message": "Add check for raster bits.",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "    // Stop load if no images are detected or the allocation of the last image\n    // buffer was failed.\n    if (gif_file->ImageCount <= 0 ||\n        gif_file->SavedImages[gif_file->ImageCount - 1].RasterBits == NULL) {\n    }\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e5b0eec199c2d03de54fd6a7fd9275692218e2bc",
    "Commit message": "[lite] Add validation check for dilation height/width to be positive integers.\n\nPiperOrigin-RevId: 416429178\nChange-Id: If7cdcddca54486434d9b2f06e7e2b401d7c3ee25",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "  TF_LITE_ENSURE(context, params->dilation_height_factor > 0);\n  TF_LITE_ENSURE(context, params->dilation_width_factor > 0);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5cedb0427bd4db4117182da8bc0680dd555b4f49",
    "Commit message": "Add checks for dilation_rate.\n\nPiperOrigin-RevId: 214627202",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "  TFLITE_DCHECK_GE(dilation_width_factor, 1);\n  TFLITE_DCHECK_GE(dilation_height_factor, 1);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/55aec0a33011773240f6696393952c984ca8de16",
    "Commit message": "Add explicit not-`None` checks for the height and width in `resize_images()`.\n\nThis was previously raising a `FutureWarning` when the height and/or width were dynamic.\nChange: 116699992",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "  if width == new_width_const and height == new_height_const:",
    "Added code": "  if new_width_const is not None and new_height_const is not None and (\n      width == new_width_const and height == new_height_const):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ab60b0ee51a8924a0f02b0152cd6a78ba64d3e94",
    "Commit message": "[tfg] Fix named-attribute token check.\n\nSince the name tokens are being indexed directly, we should check that list of tokens is not empty to prevent an out-of-bounds error.\n\nPiperOrigin-RevId: 511553573",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "#include <vector>\n    TF_RET_CHECK(!name_tokens.empty());"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/08370e76523d5bece9ab28e7a9a902932e9a2cb9",
    "Commit message": "Fix small issues with DispatchToVectorized\n\n- Changes the base case of template recursion from VecSize=0 to\n  VecSize=1, because VecSize=0 will never be reached.\n- Adds handling of `alignment_of(zero/nullptr)`, which should be treated\n  as infinitely aligned.\n- Adds checks for preconditions.",
    "Deleted lines": 1,
    "Added lines": 13,
    "Changed lines": 14,
    "Deleted code": "struct DispatchToVectorizedHelper<0, Functor> {",
    "Added code": "  // A zero/nullptr value means that the stride/pointer is not used, so it\n  // effectively has infinite alignment.\n  constexpr int64_t kMaxAlignment = 512;\n  if (element_stride == 0) return kMaxAlignment;\nstruct DispatchToVectorizedHelper<1, Functor> {\n// Requires sizeof(T) to be a power of 2.\n  static_assert((sizeof(T) & (sizeof(T) - 1)) == 0,\n                \"sizeof(T) must be a power of 2\");\n  if (max_vec_size <= 0) {\n    return errors::InvalidArgument(\"DispatchToVectorized: max_vec_size (\",\n                                   max_vec_size,\n                                   \") must be greater than zero.\");\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c6899c721f3a4b4f2e71ae4e6d1767341112ff93",
    "Commit message": "bug fix when iterators stops at multiple of batch_size (#4777)\n\n* bug fix when iterators stops at multiple of batch_size\r\n\r\nFor more information on the issue see\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/ZEzEa1TyYuE\r\n\r\n* reverted changes back as were unnecessary\r\n\r\n* _feed_fn() edge case fix in StreamingDataFeeder\r\n\r\n* Update graph_actions.py\r\n\r\n* Update graph_actions.py\r\n\r\n* Added space around `==`\r\n\r\n* Removed `else` for legiblity\r\n\r\n* reusing previous StopIteration to preserve stack trace",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "          if i == 0:\n            raise"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/66e0cb1d9afd251931f4f920c5d7bd638bc882b4",
    "Commit message": "validate clip_norm argument in clip_by_norm API\n\nThe API clip_by_norm have argument clip_norm which accepts  0-D (scalar) `Tensor` > 0 . But if we pass -ve value for this argument then its not raising intended error and converting the input tensor into Negative which IMO is wrong. Hence I am adding validation code for -ve values to raise value error.",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "    if clip_norm < 0:\n      raise ValueError('clip_norm should be a 0-D (scalar) Tensor > 0')"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d5862d423742ec26c46737d4526eca3b8b8a0d9b",
    "Commit message": "[TFLite] Add check in Softmax reference function to ensure exponent is within valid range\n\n* Add check to ensure the exponent does not cause an overflow in gemmlowp::RoundingDivideByPOT",
    "Deleted lines": 2,
    "Added lines": 4,
    "Changed lines": 6,
    "Deleted code": "            (shifted_scale * exp_in_0).raw(),\n            num_bits_over_unit + 31 - (sizeof(OutputT) * 8));",
    "Added code": "    const int exponent = num_bits_over_unit + 31 - (sizeof(OutputT) * 8);\n    TFLITE_CHECK(0 <= exponent && exponent <= 31);\n\n            (shifted_scale * exp_in_0).raw(), exponent);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/81ff894c113a5912ba52078ac27e36d06831112e",
    "Commit message": "[XLA] Add bounds checks to xla::Array::Slice\n\nTo guard against specifying limits that are out of bounds, which ends up\ntouching OOB data.\n\nPiperOrigin-RevId: 578475805",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "      CHECK_GE(starts[i], 0);\n      CHECK_LE(limits[i], dim(i));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b7e107eaa6dffb649d055d893a1fce734ee50d55",
    "Commit message": "PR #7922: [XLA:GPU] Error out for ptxas version - Update ptxas version check\n\nImported from GitHub PR https://github.com/openxla/xla/pull/7922\n\nThe version check is extended to all ptxas version `12.3.103`\nCopybara import of the project:\n\n--\n50c9aeb611685f1ee91771f031f9b8cafc58cb10 by Ayan Moitra <amoitra@nvidia.com>:\n\nUpdate ptxas version check\n\n--\n429b0866625761d13f51a70b404ab13714bba25b by Ayan Moitra <amoitra@nvidia.com>:\n\nnit fix\n\n--\n5dab1394c0a0370219c1d0b35f3cd9a949b5f3bd by Ayan Moitra <amoitra@nvidia.com>:\n\nSpecific ptxas version affected\n\nMerging this change closes #7922\n\nPiperOrigin-RevId: 595407179",
    "Deleted lines": 5,
    "Added lines": 8,
    "Changed lines": 13,
    "Deleted code": "  auto ptxas_version_tuple = GetAsmCompilerVersion(options.preferred_cuda_dir);\n  if (ptxas_version_tuple.value() == std::array<int64_t, 3>{12, 3, 1}) {\n    return tsl::errors::Internal(\n        absl::StrFormat(\"ptxas 12.3.1 has a bug that we think can affect XLA. \"\n                        \"Please use a different version.\"));",
    "Added code": "  TF_ASSIGN_OR_RETURN(auto ptxas_version_tuple,\n                      GetAsmCompilerVersion(options.preferred_cuda_dir));\n  if (ptxas_version_tuple == std::array<int64_t, 3>{12, 3, 103}) {\n    return absl::InternalError(absl::StrFormat(\n        \"ptxas %d.%d.%d has a bug that we think can affect XLA. \"\n        \"Please use a different version.\",\n        std::get<0>(ptxas_version_tuple), std::get<1>(ptxas_version_tuple),\n        std::get<2>(ptxas_version_tuple)));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bc6499d1fb195d8af740d769aae640b80fe16b51",
    "Commit message": "Optimize check whether a HloInstruction is a parameter of the entry computation.\n\nWe can check this in constant time instead of searching through all parameters.\n\nPiperOrigin-RevId: 243236540",
    "Deleted lines": 7,
    "Added lines": 4,
    "Changed lines": 11,
    "Deleted code": "      hlo.parent() == hlo.parent()->parent()->entry_computation()) {\n    if (hlo.opcode() == HloOpcode::kParameter) {\n      const std::vector<HloInstruction*>& parameter_instructions =\n          module_.entry_computation()->parameter_instructions();\n      if (absl::c_linear_search(parameter_instructions, &hlo)) {\n        array->MarkInvariantOverWholeProgram(context_);\n      }",
    "Added code": "      hlo.parent() == module_.entry_computation()) {\n    if (hlo.opcode() == HloOpcode::kParameter &&\n        hlo.parent() == module_.entry_computation()) {\n      array->MarkInvariantOverWholeProgram(context_);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cb164786dc891ea11d3a900e90367c339305dc7b",
    "Commit message": "Properly handle the case where `SpecializeType()` returns an error `Status`.\n\nIf the error case in `SpecializeType()` is reached, then we would get a crash when trying to access the value of an errorenous `StatusOr` object\n\nPiperOrigin-RevId: 408380069\nChange-Id: If3c3fc876dcf9384d5ec7a4985adc68c23ea7318",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "  DCHECK(ret.status().ok()) << \"while instantiating types: \" << ret.status();",
    "Added code": "  if (!ret.status().ok()) {\n    construction_status_ = ret.status();\n    return;\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c7f79bb75b5b83c3011e164ccd617a6ada910ea4",
    "Commit message": "Merged commit includes the following changes:\n155427981  by andrewharp <andrewharp@google.com>:\n\n    StatSummarizer: Put size check error message outside of if block where it belongs.\n\n--\n155427811  by A. Unique TensorFlower <gardener@tensorflow.org>:\n\n    Internal change\n\nPiperOrigin-RevId: 155427981",
    "Deleted lines": 9,
    "Added lines": 12,
    "Changed lines": 21,
    "Deleted code": "      bool do_shapes_match = true;\n      if (stored.shape().dim_size() != current.shape().dim_size()) {\n        do_shapes_match = false;\n      } else {\n            do_shapes_match = false;\n        if ((stored.dtype() != current.dtype()) || !do_shapes_match) {\n          LOG(WARNING) << \"Output tensor changed between runs for '\"\n                       << ns.node_name();\n        }",
    "Added code": "\n      bool do_tensors_match =\n          (stored.dtype() == current.dtype()) &&\n          (stored.shape().dim_size() == current.shape().dim_size());\n\n      if (do_tensors_match) {\n            do_tensors_match = false;\n            break;\n      }\n      if (!do_tensors_match) {\n        LOG(WARNING) << \"Output tensor changed between runs for '\"\n                     << ns.node_name();"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3550ef89bc66d03b6e2db8e47bf7b038d9f4ceff",
    "Commit message": "Convert CheckInputsSize to return a Status instead of CHECK-failing, and convert existing callsites to TF_QCHECK_OK the call.\n\nThis moves us towards the goal of returning Statuses instead of check-failing in ImportTensorFlowNode().\n\nPiperOrigin-RevId: 201056489",
    "Deleted lines": 48,
    "Added lines": 51,
    "Changed lines": 99,
    "Deleted code": "    return node.input_size();\n  } else {\n    return node.input_size();\nvoid CheckInputsCount(const NodeDef& node,\n                      const TensorFlowImportFlags& tf_import_flags,\n                      int expected_input_count) {\n  QCHECK_EQ(GetInputsCount(node, tf_import_flags), expected_input_count)\n      << node.op() << \" node expects \" << expected_input_count\n      << \" input(s) other than control dependencies: \" << node.DebugString();\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 1);\n  CheckInputsCount(node, tf_import_flags, 1);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 1);\n  CheckInputsCount(node, tf_import_flags, 1);\n  CheckInputsCount(node, tf_import_flags, 1);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 1);\n  CheckInputsCount(node, tf_import_flags, 1);\n  CheckInputsCount(node, tf_import_flags, 1);\n  CheckInputsCount(node, tf_import_flags, 1);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, NumInputs);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 4);\n    CheckInputsCount(node, tf_import_flags, 0);\n  CheckInputsCount(node, tf_import_flags, 1);\n  CheckInputsCount(node, tf_import_flags, 1);\n  if (node.op() == \"Gather\") CheckInputsCount(node, tf_import_flags, 2);\n  if (node.op() == \"GatherV2\") CheckInputsCount(node, tf_import_flags, 3);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 5);\n  CheckInputsCount(node, tf_import_flags, 5);\n  CheckInputsCount(node, tf_import_flags, 3);\n  CheckInputsCount(node, tf_import_flags, 3);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 3);\n  CheckInputsCount(node, tf_import_flags, 3);\n    CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, 2);\n  CheckInputsCount(node, tf_import_flags, op->num_partitions * 2);\n  CheckInputsCount(node, tf_import_flags, 4);",
    "Added code": "  return node.input_size();\ntensorflow::Status CheckInputsCount(\n    const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n    int expected_input_count) {\n  if (GetInputsCount(node, tf_import_flags) != expected_input_count) {\n    return tensorflow::errors::FailedPrecondition(\n        node.op(), \" node expects \", expected_input_count,\n        \" input(s) other than control dependencies: \", node.DebugString());\n  }\n  return tensorflow::Status::OK();\n  TF_RETURN_IF_ERROR(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, NumInputs));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 4));\n    TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 0));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n  if (node.op() == \"Gather\")\n    TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  if (node.op() == \"GatherV2\")\n    TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 3));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 5));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 5));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 3));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 3));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 3));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 3));\n    TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, op->num_partitions * 2));\n  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 4));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/feb3d1c5f8b40f1504b758b620edb25d4d3beef1",
    "Commit message": "More precise checking of resource loop arguments for eliding Switch/Merge\n\nPiperOrigin-RevId: 537988467",
    "Deleted lines": 13,
    "Added lines": 24,
    "Changed lines": 37,
    "Deleted code": "  // Returns whether the While op's input/output at `index` is a `DT_RESOURCE`.\n  bool IsResource(int index);\n    if (!IsResource(i)) {\n    if (IsResource(edge->dst_input())) {\n    if (enter_node->output_type(0) == DT_RESOURCE) {\n    if (IsResource(i)) {\n    if (IsResource(i)) {\n    if (IsResource(i)) {\n    if (IsResource(i)) {\n    if (IsResource(i)) {\n      if (IsResource(e->src_output())) {\nbool LowerWhileHelper::IsResource(int index) {\n  return while_op_->input_type(index) == DT_RESOURCE;",
    "Added code": "  // Returns true if the input at index is a resource and the same resource is\n  // returned as an output.\n  bool IsLoopCarriedResource(int index);\n    if (!IsLoopCarriedResource(i)) {\n    if (IsLoopCarriedResource(edge->dst_input())) {\n    bool is_constant = enter_node->attrs().FindByString(\"is_constant\")->b();\n    if (is_constant && enter_node->output_type(0) == DT_RESOURCE) {\n    if (IsLoopCarriedResource(i)) {\n    if (IsLoopCarriedResource(i)) {\n    if (IsLoopCarriedResource(i)) {\n    if (IsLoopCarriedResource(i)) {\n    if (IsLoopCarriedResource(i)) {\n      if (IsLoopCarriedResource(e->src_output())) {\nbool LowerWhileHelper::IsLoopCarriedResource(int index) {\n  if (while_op_->input_type(index) != DT_RESOURCE) return false;\n\n  auto body_func_name = while_op_->attrs().Find(\"body\")->func().name();\n  auto body_func = flib_def_->Find(body_func_name);\n  auto arg_name = body_func->signature().input_arg(index).name();\n  // Technically, we should check that the position in the return matches\n  // 'index' but proto2 maps have undefined order.\n  for (auto& ret : body_func->ret())\n    if (ret.second == arg_name) return true;\n  return false;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84d7bf6f64fd9c8677f7f26511ce3031fe8d35a6",
    "Commit message": "Add is_numeric to dtypes.cc to check whether a data type is numeric",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "      .def_property_readonly(\n          \"is_numeric\",\n          [](tensorflow::DataType self) {\n            return tensorflow::DataTypeIsNumeric(tensorflow::BaseType(self));\n          },\n          \"Returns whether this is a numeric data type.\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cd34289b744040974ebe81e1b1e88f1c752d68e0",
    "Commit message": "Update types.h to check if a data type is numeric",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "// Returns true iff 'dt' is a numeric type.\ninline bool DataTypeIsNumeric(DataType dt) {\n  return kNumberTypes.Contains(dt);\n}\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/43fd10302bcc8447e7a7205bae848a3a88624775",
    "Commit message": "Return error on invalid input in `tfl.atan2_custom`\n\nPiperOrigin-RevId: 556797683",
    "Deleted lines": 1,
    "Added lines": 3,
    "Changed lines": 4,
    "Deleted code": "    default:",
    "Added code": "    default: {\n      return TfLiteStatus::kTfLiteError;\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/00517642a356c5e04f009ea61c74638d89746392",
    "Commit message": "Return error on invalid input in `tfl.splitv`\n\nPiperOrigin-RevId: 555138718",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "      return kTfLiteError;\n    return kTfLiteError;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/40c7fe94824100338ef0c495143b26501b1c367e",
    "Commit message": "Return error on invalid input in `tfl.topkv2`\n\nPiperOrigin-RevId: 554830225",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "      return kTfLiteError;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b4aadb17b7aa5ea926b5220008e41f33e582baed",
    "Commit message": "Return error on invalid input in `tfl.where`\n\nPiperOrigin-RevId: 554544503",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "      return kTfLiteError;\n        return kTfLiteError;\n      return kTfLiteError;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ef049bdfc4f307c8b3a9dc480a90a5ff287f3d55",
    "Commit message": "Add check for `ResizeOutput` return value in `range.cc`\n\nPiperOrigin-RevId: 524984384",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "    ResizeOutput(context, start, limit, delta, output);",
    "Added code": "    TF_LITE_ENSURE_OK(context,\n                      ResizeOutput(context, start, limit, delta, output));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1707ed9b9b0cc5cb02df22a06718c9c738825d39",
    "Commit message": "Add a check to make sure that the allocation before an Evict() is not a prefetch.\n\nPiperOrigin-RevId: 577049438",
    "Deleted lines": 3,
    "Added lines": 9,
    "Changed lines": 12,
    "Deleted code": "  // TODO(b/306478911): prev_allocation can never be a prefetch, or we would be\n  // using an incorrect start time (we would need to wait until the copies\n  // finish)",
    "Added code": "  // We do not ever expect an Evict() to be immediately proceeded by a prefetch.\n  // If that case ever occurs, the eviction_exclusive_start_time below will be\n  // calculated incorrectly, as it will need to come after the prefetch finishes\n  // coping data.\n  CHECK(!prev_allocation->is_copy_like_allocation())\n      << \"Evict has been given copy-like previous allocation.\\nEvict \"\n         \"candidate:\\n\"\n      << request.allocation_value->ToString() << \"\\nPrevious allocation:\\n\"\n      << prev_allocation->ToString();"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f636be3bb1f556c15dba3028e61a8969d90dadd9",
    "Commit message": "Return error on invalid input in `tfl.sign_custom`\n\nPiperOrigin-RevId: 557545596",
    "Deleted lines": 5,
    "Added lines": 5,
    "Changed lines": 10,
    "Deleted code": "    default:\n      TF_LITE_KERNEL_LOG(\n          context,\n          \"Unsupported datatype for atan2 output: %s\",\n          TfLiteTypeGetName(output->type));",
    "Added code": "    default: {\n      TF_LITE_KERNEL_LOG(context, \"Unsupported datatype for sign output: %s\",\n                         TfLiteTypeGetName(output->type));\n      return TfLiteStatus::kTfLiteError;\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84a1cf61dd7239aa5d682083d34e0f7c99039734",
    "Commit message": "[XLA] Do not suggest trying to use TF_XLA_FLAGS when failing to parse XLA_FLAGS\n\nThe error can be very misleading, as we never check whether the new flag is actually supported by TF_XLA_FLAGS.\n\nPiperOrigin-RevId: 543680270",
    "Deleted lines": 18,
    "Added lines": 2,
    "Changed lines": 20,
    "Deleted code": "\n    // Some flags are set on XLA_FLAGS, others on TF_XLA_FLAGS.  If we find an\n    // unrecognized flag, suggest the alternative.\n    std::string alternate_envvar;\n    if (envvar == \"TF_XLA_FLAGS\") {\n      alternate_envvar = \"XLA_FLAGS\";\n    } else if (envvar == \"XLA_FLAGS\") {\n      alternate_envvar = \"TF_XLA_FLAGS\";\n    }\n    std::string did_you_mean;\n    if (!alternate_envvar.empty()) {\n      did_you_mean = absl::StrFormat(\n          \"\\nPerhaps you meant to specify these on the %s envvar?\",\n          alternate_envvar);\n    }\n\n                << \" in \" << envvar << \": \" << absl::StrJoin(unknown_flags, \" \")\n                << did_you_mean;",
    "Added code": "                << \" in \" << envvar << \": \"\n                << absl::StrJoin(unknown_flags, \" \");"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e99e31597c1b5cc9f0cbc8a3dea71674d81c20b1",
    "Commit message": "Fix GRUCellBlockOp message for invalid rank of x\n\nThe validation checks that x is a matrix, so rank must be 2. ff45913\nfixed the crash in #58261 but left this typo in an exception message.\n\nFixes #58261\n\nSigned-off-by: Reid Wahl <nrwahl@protonmail.com>",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "                errors::InvalidArgument(\"Rank of x must be 2\", x_tensor->dims(),\n                                        \" vs. 2\"));",
    "Added code": "                errors::InvalidArgument(\"Rank of x must be 2, got \",\n                                        x_tensor->dims()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b8431494de404b5f4def7303fb8efd6ba3575ef9",
    "Commit message": "Fix error log messages in data type checks\n\nPiperOrigin-RevId: 492303173",
    "Deleted lines": 3,
    "Added lines": 3,
    "Changed lines": 6,
    "Deleted code": "                           \"unsupported zero-point value (%f) for UINT8 tensor \"\n                           scale, t);\n                             \"unsupported zero-point value (%f) for INT8 \"",
    "Added code": "                           \"unsupported zero-point value (%d) for UINT8 tensor \"\n                           zero_point, t);\n                             \"unsupported zero-point value (%d) for INT8 \""
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/18dd91ccd4b1817cd5c34e40f76823a162bea029",
    "Commit message": "[XLA] Report that real -> complex bitcast_convert is not allowed\n\nThe check as exists is bidirectional: it prevents conversions from complex to real and real to complex alike, but the reported error message was unidirectional.\n\nPiperOrigin-RevId: 432075224",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    return InvalidArgument(\"Conversion from complex to real type %s => %s.\",",
    "Added code": "    return InvalidArgument(\"Conversion between complex and real type %s => %s.\","
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/86abddb56350bccd95d1b7140b003fb03525b890",
    "Commit message": "Add appropriate error check for nbins in tf.histogram_fixed_width_bins\n\nThis PR tries to address the issue raised in 54415 where\nnbins was not checked for tf.histogram_fixed_width_bins\nand an incorrect result was returned when nbins < 0.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "from tensorflow.python.ops import control_flow_ops\n    check = control_flow_ops.Assert(\n        math_ops.greater(nbins, 0), [\"nbins %s must > 0\" % nbins])\n    nbins = control_flow_ops.with_dependencies([check], nbins)"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1e5c11676dce37bb7c8eb58b35fd298a655c6fd3",
    "Commit message": "[tf.data service] Include dispatcher address in version check error message.\n\nThis is the error message that happens when the address was specified incorrectly, so it is useful to include the potentially-incorrect address in the error message.\n\nPiperOrigin-RevId: 366831311\nChange-Id: I000307bb15759f3d52635f047e62d04b0618c9c3",
    "Deleted lines": 1,
    "Added lines": 5,
    "Changed lines": 6,
    "Deleted code": "          return grpc_util::WrapError(\"Failed to get dispatcher version\", s);",
    "Added code": "          return grpc_util::WrapError(\n              absl::StrCat(\"Failed to get dispatcher version from dispatcher \"\n                           \"running at \",\n                           address_),\n              s);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/07898e752cf02518508f193a0be2e451450044bd",
    "Commit message": "Provide a more informative error message when the bazel version check fails\n\nCurrently, if the version check fails, the error message is:\n\n```\nsubprocess.CalledProcessError: Command '['bazel', '--batch', '--bazelrc=/dev/null', 'version']' returned non-zero exit status 1.\n```\n\nAfter this patch, it becomes:\n\n```\nError checking bazel version:  ERROR: The project you're trying to build requires Bazel 3.0.0 (specified in /usr/local/google/home/cheshire/code/opensource/docker_tf/tensorflow/.bazelversion), but it wasn't found in /usr/bin.\n\nYou can install the required Bazel version via apt:\n  sudo apt update && sudo apt install bazel-3.0.0\n```\n\nPiperOrigin-RevId: 312520687\nChange-Id: I41523f7defa3db10aa34b6b313d6b65c792b2020",
    "Deleted lines": 2,
    "Added lines": 7,
    "Changed lines": 9,
    "Deleted code": "  current_bazel_version = check_bazel_version(_TF_MIN_BAZEL_VERSION,\n                                              _TF_MAX_BAZEL_VERSION)",
    "Added code": "  try:\n    current_bazel_version = check_bazel_version(_TF_MIN_BAZEL_VERSION,\n                                                _TF_MAX_BAZEL_VERSION)\n  except subprocess.CalledProcessError as e:\n    print(\"Error checking bazel version: \", e.output.decode('UTF-8').strip())\n    raise e\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/01e84d7cc214dbf5a7a21bc418ad43afb5694fbc",
    "Commit message": "Update error message for data_adapter with validation split.\n\nRemove the user provided value in the error string in case it contains large amount of data. Dump large input data to log might crash on user side.\n\nhttps://github.com/tensorflow/tensorflow/issues/37840\n\nPiperOrigin-RevId: 303980671\nChange-Id: I1e2a6d3091c98bad0fcdd0e59c0eb88cd1d36956",
    "Deleted lines": 2,
    "Added lines": 3,
    "Changed lines": 5,
    "Deleted code": "  if not all(_can_split(t) for t in flat_arrays):\n        \"arrays, found: {}\".format(arrays))",
    "Added code": "  unsplitable = [type(t) for t in flat_arrays if not _can_split(t)]\n  if unsplitable:\n        \"arrays, found following types in the input: {}\".format(unsplitable))"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f54bc43f1117004208df6da34e422bf628fc3c23",
    "Commit message": "Update error message when tf.functions cross merge_call boundary when using MirroredStrategy\n\nPiperOrigin-RevId: 245302699",
    "Deleted lines": 10,
    "Added lines": 10,
    "Changed lines": 20,
    "Deleted code": "    # never releases and subsequent replica threads cannot proceed to define\n          \"`merge_call` called while defining a new graph. \"\n          \"This can happen if the function `fn` passed to \"\n          \"`strategy.experimental_run()` or \"\n          \"`strategy.extended.call_for_each_replica()` is decorated with \"\n          \"`@tf.function`. In this case, wrap the call to \"\n          \"`strategy.experimental_run()` or \"\n          \"`strategy.extended.call_for_each_replica()` with `@tf.function` \"\n          \"instead of `fn`. This will avoid mismatching graphs and also \"\n          \"improve performance.\")",
    "Added code": "    # never released and subsequent replica threads cannot proceed to define\n          \"`merge_call` called while defining a new graph or a tf.function. \"\n          \"This can often happen if the function `fn` passed to \"\n          \"`strategy.experimental_run()` is decorated with \"\n          \"`@tf.function` (or contains a nested `@tf.function`), and `fn` \"\n          \"contains a synchronization point, such as aggregating gradients. \"\n          \"This behavior is not yet supported. Instead, please wrap the entire \"\n          \"call `strategy.experimental_run(fn)` in a `@tf.function`, and avoid \"\n          \"nested `tf.function`s that may potentially cross a synchronization \"\n          \"boundary.\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4c75fb1cb917320acb386cf26adeb8e5151ca4f6",
    "Commit message": "Improve error message reporting for check_numerics gradient.\n\nAt present the op message is only printed if the numeric check fails during\nthe op's 'forward' computation. If the check fails during the gradient, there is no\nidentifier on *which* op's gradient failed.\n\nThis is the Python equivalent of\nhttps://github.com/tensorflow/tensorflow/commit/7e48bada\n\nPiperOrigin-RevId: 222262823",
    "Deleted lines": 2,
    "Added lines": 4,
    "Changed lines": 6,
    "Deleted code": "def _CheckNumericsGrad(_, grad):\n      grad, \"Not a number (NaN) or infinity (Inf) values detected in gradient.\")",
    "Added code": "def _CheckNumericsGrad(op, grad):\n      grad,\n      \"Not a number (NaN) or infinity (Inf) values detected in gradient. %s\" %\n      op.get_attr(\"message\"))"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/40918f36823973e816bd50766b1f447225b1bb9b",
    "Commit message": "Make the type check error message more informative for contrib.layers.fully_connected.\n\nPiperOrigin-RevId: 218244077",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "    raise ValueError('num_outputs should be int or long, got %s.' %\n                     (num_outputs,))",
    "Added code": "    raise ValueError('num_outputs type should be one of %s, got %s.' % (\n        list(six.integer_types), type(num_outputs)))"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9c1f14322484e44a93b77619ffd2e24b9b7a9b1d",
    "Commit message": "Fix error message in TF-keras dataset shape check\n\n(Dimension and tensor # were transposed in the error message)\n\nPiperOrigin-RevId: 210598445",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "              'for output shapes: %s.%s)' % (i, j, dataset.output_shapes, hint))",
    "Added code": "              'for output shapes: %s.%s)' % (j, i, dataset.output_shapes, hint))"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f0bf6c5191d224f229808f4b321158d890a481e0",
    "Commit message": "Minor change for better error msg in eager input type checking\n\nPiperOrigin-RevId: 206058281",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "          \"cannot compute \", op->Name(), \" as input #\", i,",
    "Added code": "          \"cannot compute \", op->Name(), \" as input #\", i, \"(zero-based)\","
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/178d62a63ea043a4b9969b4cd6f8983eb8eae523",
    "Commit message": "Update check failure to logging a warning for repeated computation placer registration.\n\nThis is to bypass a duplicated registration issue seen in open-source build during TF/PJRT integration.\n\nPiperOrigin-RevId: 532886771",
    "Deleted lines": 1,
    "Added lines": 7,
    "Changed lines": 8,
    "Deleted code": "  CHECK(computation_placers->find(platform_id) == computation_placers->end());",
    "Added code": "  if (computation_placers->find(platform_id) != computation_placers->end()) {\n    // TODO(b/282059652): Consider logging the platform name using\n    // MultiPlatformManager::PlatformWithId(). No doing that for now to avoid\n    // introducing unwanted dependency.\n    LOG(WARNING) << \"computation placer already registered. Please check \"\n                    \"linkage and avoid linking the same target more than once.\";\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6aece71ebf756d32ea730576a7ff12d2cfc7b242",
    "Commit message": "Reduce nest.map_structure and nest.pack_sequence_as time by ~20% for common\nuse cases (nested structures of list, tuple, dict).\n\nPlaces relatively cheap type checks for list, tuple, and dict before\nother more expensive checks. Specifically, this avoids calling expensive\nchecks like isinstance(structure, collections.abc.Mapping) and nest._is_named_tuple in the most common cases (since these abc isinstance checks take ~10x as long as normal isinstance checks).\n\nThis reduces the Python overhead of a sample 10-layer Keras Functional Model __call__ by ~5%.\n\nPiperOrigin-RevId: 313211095\nChange-Id: I227a3dc379eefef31060698d8c5be5f4bf2c1f50",
    "Deleted lines": 1,
    "Added lines": 9,
    "Changed lines": 10,
    "Deleted code": "  if isinstance(iterable, _collections_abc.Mapping):",
    "Added code": "  # Ordered to check common structure types (list, tuple, dict) first.\n  if isinstance(iterable, list):\n    for item in enumerate(iterable):\n      yield item\n  # namedtuples handled separately to avoid expensive namedtuple check.\n  elif type(iterable) == tuple:  # pylint: disable=unidiomatic-typecheck\n    for item in enumerate(iterable):\n      yield item\n  elif isinstance(iterable, (dict, _collections_abc.Mapping)):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9",
    "Commit message": "Validate real and expected type of arguments to cwise ops.\n\nWithout this validation, it is possible to trigger a `CHECK`-fail denial of service.\n\nThis is a rollforward of a previous commit which was rolled back as it was relying on RTTI. This time we don't use RTTI, we replace `typeid(Tin).name()` with a double function call, `DataTypeString(DataTypeToEnum<Tin>::v())`.\n\nPiperOrigin-RevId: 409340416\nChange-Id: I96080b2796729a3a9b65e7c68307ac276070f2f0",
    "Deleted lines": 0,
    "Added lines": 10,
    "Changed lines": 10,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(ctx, input_0.dtype() == DataTypeToEnum<Tin>::v(),\n                errors::InvalidArgument(\n                    \"Expected tensor of type \",\n                    DataTypeString(DataTypeToEnum<Tin>::v()), \" but got type \",\n                    DataTypeString(input_0.dtype())));\n    OP_REQUIRES(ctx, input_1.dtype() == DataTypeToEnum<Tin>::v(),\n                errors::InvalidArgument(\n                    \"Expected tensor of type \",\n                    DataTypeString(DataTypeToEnum<Tin>::v()), \" but got type \",\n                    DataTypeString(input_1.dtype())));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/798b2ebda0cc6f12f1ca6460611f760149771a11",
    "Commit message": "Ensure the allocation type is kTfLiteCustom when doing shallow copies in DeepOrShallowCopyTensorsShapeTypeData.\n\nThis code is correct only under the assumption that the caller has correctly prepared\nthe tensors that get passed in for shallow copying, by setting their allocation\ntypes to kTfLiteCustom. This ensures that those tensors won't be double `free`'d\nlater on. This check simply ensures that that assumption always holds, to ensure\nwe fail early if ever a bug is introduced that breaks that assumption.\n\nPiperOrigin-RevId: 597990125",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "      // Make a shallow copy of the data. This is only safe because the caller\n      // is expected to have previously set dst_tensor->allocation_type to\n      // kTfLiteCustom, to ensure the buffer is never double-freed later on.\n      TF_LITE_ENSURE_EQ(context, dst_tensor->allocation_type, kTfLiteCustom);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b65d9ec2b78c7c23e368ed4eec7b4deb89dcd712",
    "Commit message": "Fix value error generated on is_scalar check (#10391)\n\n* Fix value error generated on is_scalar check\r\n\r\n`is_scalar = shape is not None and not shape` raises a value error when shape is a scalar, \"ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\"\r\n\r\n* Update variable_scope.py\r\n\r\n* Fix",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "      is_scalar = shape is not None and not shape",
    "Added code": "      is_scalar = (shape is not None and isinstance(shape, collections_lib.Sequence)\n                   and len(shape) == 0)"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9baa064387b0a114c3fcec88abaa0568834e8e34",
    "Commit message": "Only apply check for non-tensor case\n\nPiperOrigin-RevId: 261481490",
    "Deleted lines": 3,
    "Added lines": 3,
    "Changed lines": 6,
    "Deleted code": "    if constant_values != 0:\n    else:\n      result = gen_array_ops.pad(tensor, paddings, name=name)",
    "Added code": "    if not tensor_util.is_tensor(constant_values) and constant_values == 0:\n      result = gen_array_ops.pad(tensor, paddings, name=name)\n    else:"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9c14f6ba30d96241978188998de47a388822365f",
    "Commit message": "Only apply check for non-tensor case\n\nPiperOrigin-RevId: 261408908",
    "Deleted lines": 10,
    "Added lines": 12,
    "Changed lines": 22,
    "Deleted code": "  if training == 1 or training is True:\n    if callable(x):\n      return x()\n    else:\n      return x\n  elif training == 0 or training is False:\n    if callable(alt):\n      return alt()\n    else:\n      return alt",
    "Added code": "  # TODO(b/138862903): Handle the case when training is tensor.\n  if not tensor_util.is_tensor(training):\n    if training == 1 or training is True:\n      if callable(x):\n        return x()\n      else:\n        return x\n    elif training == 0 or training is False:\n      if callable(alt):\n        return alt()\n      else:\n        return alt"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cb2828a844ccaf0394e602d15fd95e45073729a2",
    "Commit message": "Check that the type of an implicitly dereferenced tensor matches the expected input type.\n\nThe dtype of a tensor reference can change between the point when it is \"produced\" by an\noperation and consumed by the next operation. This evades checks in the executor that the\ntype of tensor on each edge matches the type signatures of the producing and consuming operation, which could lead to undefined behavior. Although there is no existing operation that changes the type of a tensor reference, it is possible to use the OpKernelContext API to do so, so we add a further check in the runtime to defend against operations that might be added in the future.\n\nPiperOrigin-RevId: 184356242",
    "Deleted lines": 0,
    "Added lines": 13,
    "Changed lines": 13,
    "Deleted code": "",
    "Added code": "        // The dtype of entry->ref could have been changed by another operation\n        // that ran after the operation that \"produced\" it executed, so\n        // re-validate that the type of the dereferenced tensor matches the\n        // expected input type.\n        if (item.input_type(i) != inp->tensor->dtype()) {\n          return AttachDef(\n              errors::InvalidArgument(\n                  i, \"-th input expects type \",\n                  DataTypeString(item.input_type(i)),\n                  \" but automatically dereferenced input tensor has type \",\n                  DataTypeString(inp->tensor->dtype())),\n              item.kernel->def());\n        }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/924f80a4fdb34230965a7a8a4476901847463645",
    "Commit message": "Add stricter type checking for tf.math.real\n\nFix for tf.math.real so that it only accepts tensors with numeric entries as input. This makes it consistent with its documentation at https://www.tensorflow.org/api_docs/python/tf/math/real and raises a TypeError saying input must have numeric entries when called incorrectly.",
    "Deleted lines": 1,
    "Added lines": 3,
    "Changed lines": 4,
    "Deleted code": "    else:",
    "Added code": "    elif tf.debugging.is_numeric_tensor(input):\n    else:\n      raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e6df768b81e973f2123bc83a18a60773fc4da99e",
    "Commit message": "[TFG] Fix IsAdd string type check in tf_op_names\n\nPiperOrigin-RevId: 469316572",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "  if (op_name == add_) return !op->getAttrOfType<StringAttr>(\"T\");",
    "Added code": "  if (op_name == add_)\n    return !op->getAttrOfType<TypeAttr>(\"T\").getValue().isa<StringType>();"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4f4a0276a2cf9186c0541072964676159368286e",
    "Commit message": "Add appropriate PyObject type check for bool\n\nThis PR fixes an issue where PyObject type in tf's C bindings\ndoes not check if an input is a boolean and will always cast\nto bool. As an result, an invalid type like int can be passed\nto an arg expecting bool:\n```\nimport tensorflow as tf\nx = [0.5, 1.0, 2.0, 4.0]\naxis = 0\nexclusive = -1\nreverse = -1\nres_1 = tf.math.cumsum(x, axis=axis, exclusive=exclusive, reverse=reverse)\nprint(res_1) # tf.Tensor([7. 6. 4. 0.], shape=(4,), dtype=float32)\n```\n\nNote other PyObejct types (e.g., in ParseIntValue) does perform check in\ntensorflow/python/eager/pywrap_tfe_src.cc\n\nThis PR adds appropriate check in the input type when PyObject path\nis invoked.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 2,
    "Added lines": 10,
    "Changed lines": 12,
    "Deleted code": "  *value = PyObject_IsTrue(py_value);\n  return true;",
    "Added code": "  if (PyBool_Check(py_value)) {\n    *value = PyObject_IsTrue(py_value);\n    return true;\n  }\n  TF_SetStatus(\n      status, TF_INVALID_ARGUMENT,\n      tensorflow::strings::StrCat(\"Expecting bool value for attr \", key,\n                                  \", got \", py_value->ob_type->tp_name)\n          .c_str());\n  return false;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/88609e2e22fa5c296de2e27e04d1cc4743b2dfcd",
    "Commit message": "Add appropriate dtype check for tf.boolean_mask's mask\n\nThis PR tries to address the issue raised in 54412 where\nmask's dtype was checked in tf.boolean_mask and an invalid\nresult has been returned instead.\n\nThis PR fixes 54412.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "    if mask.dtype != dtypes.bool:\n      raise TypeError(\"Invalid `mask`: expected bool but got %s.\" % mask.dtype)"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d79c4d435fc6e7be6cc69a3ca446716ebf6190b9",
    "Commit message": "address review concerns\n\n1. change default value of num_threads to Non\n2. set num_threads before delegate\n3. check the type of num_threads before setting it",
    "Deleted lines": 3,
    "Added lines": 6,
    "Changed lines": 9,
    "Deleted code": "               num_threads=1):\n    self._interpreter.SetNumThreads(num_threads)\n",
    "Added code": "               num_threads=None):\n    if num_threads:\n      if not isinstance(num_threads, int):\n        raise ValueError('type of num_threads should be int')\n      self._interpreter.SetNumThreads(num_threads)\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a76646d4b4ad5d56b5e63c139985bbd1eb98dd90",
    "Commit message": "Add type checking at the beginning of tpu.shard().\n\nOtherwise a message like \"TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.\" will be thrown, which is confusing.\n\nPiperOrigin-RevId: 213371676",
    "Deleted lines": 1,
    "Added lines": 5,
    "Changed lines": 6,
    "Deleted code": "  inputs = [] if inputs is None else [ops.convert_to_tensor(x) for x in inputs]",
    "Added code": "  inputs = [] if inputs is None else inputs\n  if not isinstance(inputs, list):\n    raise TypeError(\"tpu.shard()'s inputs must be a list of Tensors or None.\")\n\n  inputs = [ops.convert_to_tensor(x) for x in inputs]"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c510c1b8b1ef5be1d65971f5b9e21e61becd0bb3",
    "Commit message": "[XLA] Remove IsCalledComputation from HloComputation.\n\nThe function doesn't do what it seems. There are more types of called instruction that are not accounted in this check.\n\nPiperOrigin-RevId: 605270151",
    "Deleted lines": 6,
    "Added lines": 2,
    "Changed lines": 8,
    "Deleted code": "    CHECK(!IsCalledComputation());\n  // Returns if this computation is invoked by an Hlo instruction.\n  bool IsCalledComputation() const {\n    return IsFusionComputation() || IsCustomCallComputation();\n  }\n",
    "Added code": "    // TODO: Add instruction type for async instructions.\n    CHECK(instruction_type() == InstructionType::kUnset);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c510c1b8b1ef5be1d65971f5b9e21e61becd0bb3",
    "Commit message": "[XLA] Remove IsCalledComputation from HloComputation.\n\nThe function doesn't do what it seems. There are more types of called instruction that are not accounted in this check.\n\nPiperOrigin-RevId: 605270151",
    "Deleted lines": 6,
    "Added lines": 2,
    "Changed lines": 8,
    "Deleted code": "    CHECK(!IsCalledComputation());\n  // Returns if this computation is invoked by an Hlo instruction.\n  bool IsCalledComputation() const {\n    return IsFusionComputation() || IsCustomCallComputation();\n  }\n",
    "Added code": "    // TODO: Add instruction type for async instructions.\n    CHECK(instruction_type() == InstructionType::kUnset);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1c49c13ba59961cf7581e3e29b951db8faca94f5",
    "Commit message": "Add type check for reduction axis in reducer operation.\n\nPiperOrigin-RevId: 253543782",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "  TF_LITE_ENSURE_TYPES_EQ(context, op_context.axis->type, kTfLiteInt32);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b6f3366a716ca9b5a1e6114a3bea050c80d8a475",
    "Commit message": "Don't check for if null after already dereferenced\n\nI'm not sure how it could be null at this point (and obviously it is nowhere else we'd have seen failures), but keeping the check as is and just moving it to where it would catch it before dereferencing.\n\nPiperOrigin-RevId: 349603765\nChange-Id: Ifa792bed92ffd2652c6946d3a48bfb53aafc3477",
    "Deleted lines": 3,
    "Added lines": 5,
    "Changed lines": 8,
    "Deleted code": "    auto it = stack_traces.find(n->name());\n    if (n && it != stack_traces.end()) {\n      n->SetStackTrace(it->second);",
    "Added code": "    if (n) {\n      auto it = stack_traces.find(n->name());\n      if (it != stack_traces.end()) {\n        n->SetStackTrace(it->second);\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/89fa1ae2cb34eab0e6137e72e6fab01f6c5bc164",
    "Commit message": "Fix check for cloning FunctionLibraryRuntime",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  if (out_flr != nullptr) {",
    "Added code": "  if (*out_flr != nullptr) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3a7b36bca7f43ce4f0d0791ce0e0d84ece8683d9",
    "Commit message": "[Grappler] Remove DCHECK from a MutableGraphView CanDedupControlWithRegularInput check.\n\nPiperOrigin-RevId: 259537618",
    "Deleted lines": 2,
    "Added lines": 3,
    "Changed lines": 5,
    "Deleted code": "  DCHECK(control_node != nullptr)\n      << \"Didn't find a node for control dependency: \" << control_node_name;",
    "Added code": "  if (control_node == nullptr) {\n    return false;\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c5019e2156c749d35ec786ff7946a55006d9ba91",
    "Commit message": "Add a check.",
    "Deleted lines": 1,
    "Added lines": 5,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "  if (cuda_stream_ != nullptr) {\n    LOG(FATAL) <<  // Crash OK.\n        \"Trying to set the stream twice. This isn't supported. \";\n  }\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a7908e924af3105c3007988e219855174b26774f",
    "Commit message": "[core] Added check for output buffer",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "  if (output == nullptr)\n    LOG(ERROR) << \"Output buffer is null: \";\n    return false;\n  }\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/04b97cde86550995da57d16d81084006456ccce5",
    "Commit message": "Fix segmentation fault with tf.stack an keras's Input in TF2.0\n\nThis fix tries to address the issue raised in 26879 where\ntf.stack encounters segmantation fault with keras's Input in TF2.0:\n```\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input\n\nprint(tf.__version__)\n\ninput_ = Input((128, 128, 1), dtype='float32')\nprint(input_)\noutput = tf.stack(input_, axis=1)\n```\n\nThe issue is that in eager wrap, `PySequence_Fast_GET_ITEM`\ntries to access an object not through `PySequence_Fast`.\n\nThis fix adds the `PySequence_Fast` and checks the return value\nto make sure it is not nullptr.\n\nThis fix fixes 26879.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 2,
    "Added lines": 6,
    "Changed lines": 8,
    "Deleted code": "      for (Py_ssize_t j = 0; j < PySequence_Fast_GET_SIZE(item); j++) {\n        PyObject* inner_item = PySequence_Fast_GET_ITEM(item, j);",
    "Added code": "      tensorflow::Safe_PyObjectPtr fast_item(PySequence_Fast(item, \"Could not parse sequence.\"));\n      if (fast_item.get() == nullptr) {\n        return false;\n      }\n      for (Py_ssize_t j = 0; j < PySequence_Fast_GET_SIZE(fast_item.get()); j++) {\n        PyObject* inner_item = PySequence_Fast_GET_ITEM(fast_item.get(), j);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/95166f5960322cc784a7e8f339a701da80a41a1e",
    "Commit message": "Add a null check on enter_ctx and update the null check on merge_ctx\n\nPiperOrigin-RevId: 164058574",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "      DCHECK_NE(merge_ctx, nullptr);",
    "Added code": "  CHECK_NE(enter_ctx, nullptr);\n      CHECK_NE(merge_ctx, nullptr);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cd8d0bf58ad554588012898161c91fa453bbf7f0",
    "Commit message": "contrib/android: Fix bug.\n\nAddress edge case where runStats is null and the interface is closed.\nChange: 148139848",
    "Deleted lines": 1,
    "Added lines": 3,
    "Changed lines": 4,
    "Deleted code": "    runStats.close();",
    "Added code": "    if (runStats != null) {\n      runStats.close();\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1a1a381b5be7701843c3f1e34aa1846ae2a1d0ce",
    "Commit message": "Fix a SIGSEGV bug in `InferShapeForXlaGatherOp`\n\nSince `ComputeOutputComponent` may return nullptr, we need to check for null attributes explicitly to be safe.\n\nPiperOrigin-RevId: 601487562",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "             llvm::isa<DenseIntElementsAttr>(it->second)) {",
    "Added code": "             llvm::isa_and_nonnull<DenseIntElementsAttr>(it->second)) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/893aa7518fe3175739ac1ba70d7355a0b091115c",
    "Commit message": "Added a null check in `string_util.cc`\n\nPiperOrigin-RevId: 573300006",
    "Deleted lines": 2,
    "Added lines": 5,
    "Changed lines": 7,
    "Deleted code": "#include <limits>\n",
    "Added code": "#include <cstddef>\n  if (*buffer == nullptr) {\n    return -1;\n  }\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9720b405905dee209a3f7d003de21d388e1aaef4",
    "Commit message": "Avoid nullptr as row offsets to cusparseCreateCsr\n\nAs of CUDA 12.2 additional input validation allows NULL for the row offsets\nonly when rows=0.",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "                         nullptr, nullptr, nullptr));",
    "Added code": "                         c_row_ptr.data(), nullptr, nullptr));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/47eaa828a1dd4bf50ec4203ef4bbb348b3ef0dd0",
    "Commit message": "Add nullptr check",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "  if ((&cc_tensor) == nullptr) {\n    *tensor = nullptr;\n    return;\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2fc1f2b5a8b8152c43b81cf31394f3e0a2cb837",
    "Commit message": "Add null pointer check",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "  CHECK(a.opaque() != nullptr);\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b677392e4af8095dbde8068b0ceb60bca815e94b",
    "Commit message": "Reject non-PjRt devices in PjRtArray::Reshard()\n\nPjRt buffers traditionally support some degree of interoperability between PjRt\nclients (e.g., CPU and TPU). However, this is not universally true between\narbitrary IFRT clients that may use a non-PjRt-compatible runtime. This change\nadds extra checks to make sure that non-PjRt devices are not accidentally used\nin PjRtArray's destination devices.\n\nIn the future, ifrt::Device (currently aliasing to PjRtDevice) will introduce\nstronger type checking.\n\nPiperOrigin-RevId: 507846317",
    "Deleted lines": 0,
    "Added lines": 7,
    "Changed lines": 7,
    "Deleted code": "",
    "Added code": "      if (new_sharding->devices()[i]->client() == nullptr) {\n        return InvalidArgument(\n            \"The destination device is owned by a non-PjRt-compatible client. \"\n            \"To use this Array on the destination device, the Array must be \"\n            \"first fetched to the host and then sent to the destination \"\n            \"device.\");\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f22ca1dc88c70a0dc5696c37e6a2de6bcf8d60c7",
    "Commit message": "Avoid segfault when init_value is not on default_mesh.\n\nTo actually fix the segfault in lower level (e.g. directly users of VarHandleOp),\nI tried to add a validation in SPMD of AssignValueOp, but turns out it only\nknows the resource_layout is an 'empty' layout without any mesh information.\n\nSo there is not enough information to compare if the mesh is correct. We\nshall start tracking mesh of empty layout -- but changing the data model at\nthis point is not very easy to do or to justify.\n\nPiperOrigin-RevId: 453453817",
    "Deleted lines": 2,
    "Added lines": 5,
    "Changed lines": 7,
    "Deleted code": "      super(DVariable, self).__init__(\n          initial_value, *args, dtype=dtype, **kwargs)",
    "Added code": "import contextlib\n      mesh = self.layout.mesh if self.layout else None\n      with api.run_on(mesh) if mesh else contextlib.nullcontext():\n        super(DVariable, self).__init__(\n            initial_value, *args, dtype=dtype, **kwargs)"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/aaa3fb49374d59c89115730c8e2f672e70b9e3fa",
    "Commit message": "[TFLite] Bucketize op: Fix processing of bucket boundary array.\n\nThe param value may be a nullptr, which is an error; we should catch this and\navoid dereferencing it.\n\nPiperOrigin-RevId: 404889441\nChange-Id: I022d0d175c83e0498ab47ed93d7040ea3f3f3987",
    "Deleted lines": 0,
    "Added lines": 12,
    "Changed lines": 12,
    "Deleted code": "",
    "Added code": "        if (boundaries == nullptr) {\n          TF_LITE_REPORT_ERROR(\n              error_reporter,\n              \"boundaries array not provided for operation 'bucketize'.\\n\");\n          return kTfLiteError;\n        }\n        if (boundaries->data() == nullptr) {\n          TF_LITE_REPORT_ERROR(error_reporter,\n                               \"boundaries.data() returned nullptr for \"\n                               \"operation 'bucketize'.\\n\");\n          return kTfLiteError;\n        }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e1eb6d9cfa14368442f0d172a40f87ce4f094386",
    "Commit message": "Add CheckArraySegments() to model verifier\n\nAdd additional logic to check nullness of array segments vector.\n\nPiperOrigin-RevId: 337776748\nChange-Id: Ia6a7a9f1773b211a54e7598d85768aaf3e136753",
    "Deleted lines": 3,
    "Added lines": 40,
    "Changed lines": 43,
    "Deleted code": "      const auto* array_segments = dim_metadata->array_segments();\n      const auto* array_indices = dim_metadata->array_indices();\n      if (array_segments == nullptr || array_indices == nullptr) {",
    "Added code": "bool CheckArraySegments(const DimensionMetadata* dim_metadata) {\n  if (dim_metadata->array_segments() == nullptr) {\n    return false;\n  }\n  switch (dim_metadata->array_segments_type()) {\n    case SparseIndexVector_Int32Vector:\n      return (dim_metadata->array_segments_as_Int32Vector()->values() !=\n              nullptr);\n    case SparseIndexVector_Uint16Vector:\n      return (dim_metadata->array_segments_as_Uint16Vector()->values() !=\n              nullptr);\n    case SparseIndexVector_Uint8Vector:\n      return (dim_metadata->array_segments_as_Uint8Vector()->values() !=\n              nullptr);\n    default:\n      return false;\n  }\n}\n\nbool CheckArrayIndices(const DimensionMetadata* dim_metadata) {\n  if (dim_metadata->array_indices() == nullptr) {\n    return false;\n  }\n  switch (dim_metadata->array_indices_type()) {\n    case SparseIndexVector_Int32Vector:\n      return (dim_metadata->array_indices_as_Int32Vector()->values() !=\n              nullptr);\n    case SparseIndexVector_Uint16Vector:\n      return (dim_metadata->array_indices_as_Uint16Vector()->values() !=\n              nullptr);\n    case SparseIndexVector_Uint8Vector:\n      return (dim_metadata->array_indices_as_Uint8Vector()->values() !=\n              nullptr);\n    default:\n      return false;\n  }\n}\n\n      if (!CheckArraySegments(dim_metadata) ||\n          !CheckArrayIndices(dim_metadata)) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/26cd260fac5fa98ade11ff2a5ec38ede65631cc0",
    "Commit message": "Add additional data validation while saving and restoring iterators.\n\nPiperOrigin-RevId: 319078544\nChange-Id: I4a439934e1ba35d5eab38513cae735372d62c8d6",
    "Deleted lines": 0,
    "Added lines": 10,
    "Changed lines": 10,
    "Deleted code": "",
    "Added code": "      if (!w) {\n        return errors::Internal(\n            \"Cannot initialize an iterator from tensor \",\n            serialized_vec(i).DebugString(),\n            \". Expected a variant tensor of type IteratorStateVariant\");\n      }\n      if (variants_[i].GetData() == nullptr) {\n        return errors::Internal(\n            \"Cannot serialize an empty IteratorStateVariant\");\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a17858f3cc5e7ab4ebc2c166d71e7f85b2dad05d",
    "Commit message": "Avoid undefined behavior by checking for null Operation in TF_Input/TF_Output\n\nPiperOrigin-RevId: 318082756\nChange-Id: I5b5e9bc716cf159f22a4b89083e00efb93d9fecb",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "    Node* node = &inputs[i].oper->node;\n    Node* node = &outputs[i].oper->node;",
    "Added code": "    Node* node = inputs[i].oper ? &inputs[i].oper->node : nullptr;\n    Node* node = outputs[i].oper ? &outputs[i].oper->node : nullptr;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1835465ac5a9c823f7187cb0dd5786da9c360838",
    "Commit message": "Add error_reporter DCHECK back into SimpleMemoryAllocator.\n\nThis check was removed due to an internal build problem.\n\nPiperOrigin-RevId: 315555154\nChange-Id: I0f211aa284b2d327df52941bfbcd998a1daf9656",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "  TFLITE_DCHECK(error_reporter != nullptr);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4a8d8518fba1d70f63633775695f1a5189cd252f",
    "Commit message": "Add checks that the Allocate function returned successfully.\n\nPiperOrigin-RevId: 306737078\nChange-Id: I74abe5b6db04ed850e98117d32c6be3aa06c0211",
    "Deleted lines": 7,
    "Added lines": 56,
    "Changed lines": 63,
    "Deleted code": "      TfLiteIfParams* params = allocator->AllocatePOD<TfLiteIfParams>();\n      *builtin_data = params;\n      TfLiteWhileParams* params = allocator->AllocatePOD<TfLiteWhileParams>();\n      *builtin_data = params;\n      TfLiteBatchMatMulParams* params =\n          allocator->AllocatePOD<TfLiteBatchMatMulParams>();\n      *builtin_data = params;",
    "Added code": "      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      auto params = safe_allocator.Allocate<TfLiteIfParams>();\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      *builtin_data = params.release();\n      auto params = safe_allocator.Allocate<TfLiteWhileParams>();\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      *builtin_data = params.release();\n      auto params = safe_allocator.Allocate<TfLiteBatchMatMulParams>();\n      TF_LITE_ENSURE(error_reporter, params != nullptr);\n      *builtin_data = params.release();"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7578e120de2a3a5282ced8d41881f19363f83466",
    "Commit message": "Fix crash on closing the app when classifier failed to initialize\n\nWhen testing on an API 21 emulator, the classifier fails to initialize.\n`E/TfLiteCameraDemo: Failed to initialize an image classifier.`\n\nIn this situation, the app crashes when pressing Back to exit.  Here's the cause:\n```\njava.lang.NullPointerException: Attempt to invoke virtual method 'void com.example.android.tflitecamerademo.ImageClassifier.close()' on a null object reference\n                                                                                        at com.example.android.tflitecamerademo.Camera2BasicFragment.onDestroy(Camera2BasicFragment.java:331)\n                                                                                        at android.app.Fragment.performDestroy(Fragment.java:2266)\n```\nThe fix is to check for null before calling `.close()`.\n\nI'll investigate why the classifier is failing to initialize separately. :-)",
    "Deleted lines": 1,
    "Added lines": 3,
    "Changed lines": 4,
    "Deleted code": "    classifier.close();",
    "Added code": "    if (classifier != null) {\n      classifier.close();\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c1b9ac9f215a3a83f7f0b6233bf4cef0b3e74598",
    "Commit message": "Error checking in c/python code.\n\nPiperOrigin-RevId: 199465056",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "  if (iterator == nullptr || PyErr_Occurred()) {\n    return false;\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ca170f34d9174d6981850855190a398393aa921e",
    "Commit message": "[Tensorflow] Add check fail when user passes a tensor with nullptr to lookup.\nChange: 150474503",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "  CHECK(val != nullptr);\n  CHECK(val != nullptr);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/75c45e54bd37932f26d6e7cb36920c06a7833d52",
    "Commit message": "Added sanity checks on the existance of nodes to ConstantFolding.\n\nThis should fix null dereferences in ConstantFolding on fuzzed inputs.\n\nPiperOrigin-RevId: 373199608\nChange-Id: I3ab59dacf349f28edfb9ca900a625cade6266633",
    "Deleted lines": 8,
    "Added lines": 18,
    "Changed lines": 26,
    "Deleted code": "  const NodeDef& node = *node_map->GetNode(input_name);\n  if (!IsSwitch(node)) {\n    return AsControlDependency(node);\n    for (const NodeDef* output : node_map->GetOutputs(node.name())) {\n        if (IsSameInput(node.input(0), input_name)) {\n    const DataType output_type = node.attr().at(\"T\").type();\n      added_node->set_device(node.device());\n      node_map->AddOutput(node.name(), added_node->name());",
    "Added code": "  const NodeDef* node = node_map->GetNode(input_name);\n  // Sanity check for missing node.\n  if (!node) {\n    return input_name;\n  }\n  if (!IsSwitch(*node)) {\n    return AsControlDependency(*node);\n    for (const NodeDef* output : node_map->GetOutputs(node->name())) {\n        if (IsSameInput(node->input(0), input_name)) {\n    const DataType output_type = node->attr().at(\"T\").type();\n      added_node->set_device(node->device());\n      node_map->AddOutput(node->name(), added_node->name());\n\n  // Sanity check for missing children.\n  if (left_child == nullptr || right_child == nullptr) {\n    return false;\n  }\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ed043aec4962dfdc3c58e2ad90dacb557dafcf4e",
    "Commit message": "Lite: ResizeTensor Dim size check added to avoid reallocation if no change",
    "Deleted lines": 0,
    "Added lines": 12,
    "Changed lines": 12,
    "Deleted code": "",
    "Added code": "  // If the dimensions don't change, avoiding\n  // unnecessary (re)allocations.\n  //\n  // Note that it's required to check `tensor->data.raw != nullptr`. Otherwise\n  // the subgraph won't allocate memory for a dynamic tensor when its size\n  // is equal to the original tensor size.\n  if (tensor->data.raw != nullptr &&\n      EqualArrayAndTfLiteIntArray(tensor->dims, new_size->size,\n                                  new_size->data)) {\n    return kTfLiteOk;\n  }\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/65c5dd69676db159ddd3a1fd7b2f6836dfe37f49",
    "Commit message": "Add nullptr check for external registration init / prepare / invoke / free.\n\nPiperOrigin-RevId: 520773677",
    "Deleted lines": 0,
    "Added lines": 15,
    "Changed lines": 15,
    "Deleted code": "",
    "Added code": "    if (referenced_registration->init == nullptr) return nullptr;\n    if (referenced_registration->prepare == nullptr) {\n      if (IsUnresolvedCustomOp(op_reg)) {\n        ReportError(\n            \"Encountered unresolved custom op: %s.\\nSee instructions: \"\n            \"https://www.tensorflow.org/lite/guide/ops_custom \",\n            op_reg.custom_name ? op_reg.custom_name : \"UnknownOp\");\n        return kTfLiteUnresolvedOps;\n      } else {\n        // Resolved ops can have a null Prepare function.\n        return kTfLiteOk;\n      }\n    }\n    if (referenced_registration->invoke == nullptr) return kTfLiteError;\n    if (referenced_registration->free == nullptr) return;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/20d54796563631c23c27548b321487e8b0c982a9",
    "Commit message": "Add a nil check before init the device_name string, and also assign an empty string as a placeholder.\n\nPiperOrigin-RevId: 579225867",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "    std::string device_name = std::string([[metal_device_ name] UTF8String]);",
    "Added code": "    auto utf8_name = [[metal_device_ name] UTF8String];\n    const std::string device_name = utf8_name != nil ? utf8_name : \"\";"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/db10718b38b2884cb5ed46d33c135c079f649d16",
    "Commit message": "Handle nil return from TF_TensorData (#14082)\n\nWith some memory allocators, attempting to allocate 0 bytes will return\r\na null pointer. This specifically happens when building tensorflow with\r\nmkl support. If TF_TensorData returns null, the go code to create a\r\nslice from the data leads to a null pointer exception. This fixes the\r\nissue by checking for the nil return and returning a slice zero value to\r\n(nil) to the caller. Fixes #13764.",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "\tif cbytes == nil {\n\t\treturn nil\n\t}"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8876a1796aeced8f89c279cbc98db9c7957ddbd1",
    "Commit message": "Updated check for existence of TensorFlow objects to 'is not None' rather than 'if [object]'.\nChange: 133944683",
    "Deleted lines": 3,
    "Added lines": 3,
    "Changed lines": 6,
    "Deleted code": "  if sync_optimizer and startup_delay_steps > 0:\n    if is_chief and sync_optimizer:\n        if is_chief and sync_optimizer:",
    "Added code": "  if sync_optimizer is not None and startup_delay_steps > 0:\n    if is_chief and sync_optimizer is not None:\n        if is_chief and sync_optimizer is not None:"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/11030308c5d25df5b36f8a583f1b4607e4ea2b7f",
    "Commit message": "Add a check to check if all sharding strategies are dropped due to infinity costs\n\nPiperOrigin-RevId: 560125787",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "    size_t num_skipped_due_to_infinity_costs = 0;\n        num_skipped_due_to_infinity_costs++;\n    CHECK_LT(num_skipped_due_to_infinity_costs, strategies->leaf_vector.size())\n        << \"All strategies removed due to infinite resharding costs\";"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2465d4e77654f0d4f7799bc46d5fd5812590acc6",
    "Commit message": "Add a check in auto-sharding setup and die if the input mesh shape contains more than two shardable dimensions, which is currently not supported.\n\nPiperOrigin-RevId: 518440655",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "    if (spmd::VectorGreaterThanOneElementCount(device_mesh_shape) > 2) {\n      return tsl::errors::OutOfRange(\n          absl::StrCat(\"the auto-sharding pass currently does not support \",\n                       \"more than two shardable dims: device_mesh_shape=\",\n                       absl::StrJoin(device_mesh_shape, \",\")));\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5322fd40cd58cfa8c551e602fede7a3be19fff95",
    "Commit message": "[PJRT] Fix checking for output sharding\n\nOutput sharding for empty tuple needs to have one \"replicated\" element.\n\nPiperOrigin-RevId: 559899447",
    "Deleted lines": 3,
    "Added lines": 14,
    "Changed lines": 17,
    "Deleted code": "        (!result_hlo_sharding->IsTuple() ||\n         result_hlo_sharding->tuple_elements().size() !=\n             result_shape.tuple_shapes().size())) {",
    "Added code": "  auto check_tuple_output_sharding_condition =\n      [](const xla::Shape& shape, const xla::HloSharding& sharding) {\n        // Check that the HLO sharding of the result is a tuple and that it has\n        // the same number of elements as the output tuple shape. If the output\n        // is an empty tuple then the output sharding will have a single element\n        // for the tuple as a special case, so we will have to allow that by\n        // checking this condition specifically.\n        return sharding.IsTuple() && (shape.tuple_shapes().size() ==\n                                          sharding.tuple_elements().size() ||\n                                      (shape.tuple_shapes().empty() &&\n                                       sharding.tuple_elements().size() == 1));\n      };\n        !check_tuple_output_sharding_condition(result_shape,\n                                               *result_hlo_sharding)) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3e0152a8b4aad03dd06274e0dd3b94bd5f8bf5d3",
    "Commit message": "Fix `invalid syntax` error when `import carla` is present\n\nThis fix tries to address the issue raised in 34828 where\n`import carla` followed by `import tensorflow` caused the\nfollowing:\n```\nSyntaxError: invalid syntax\n```\n\nThe issue is that, when `import carla` is invoked,\nI/O operation for `std::ostringstream s` might fail,\nwhich caused the conversion of AttrValue to string as empty.\n\nThis PR check `s.good()` to make sure the I/O operation\nis OK, and, fallback to normal conversion if locale-neutral I/O\noperation fails.\n\nThis PR fixes 34828.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 1,
    "Added lines": 6,
    "Changed lines": 7,
    "Deleted code": "      return s.str();",
    "Added code": "      // If there is no I/O error for `std::ostringstream s` return s.str(),\n      // otherwise fallback to strings::StrCat(value.f()).\n      if (s.good()) {\n        return s.str();\n      }\n      return strings::StrCat(value.f());"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/06b89ed1bdf606adb21d66664ca7ab5eaffdd58f",
    "Commit message": "BundleReader was not waiting for concurrents reads to complete before\nchecking their result value.\n\nAlso changed the large value reading test to actually exercise the\nmulti-threaded reading path. Previously, the whole multi-threaded path\nwas being skipped because the reads were smaller than kBufferSize.\n\nPiperOrigin-RevId: 604300943",
    "Deleted lines": 1,
    "Added lines": 3,
    "Changed lines": 4,
    "Deleted code": "    if (entry.size() > kBufferSize) {",
    "Added code": "    if (entry.size() > kBufferSize || enable_multi_threading_for_testing_) {\n        reader_pool = nullptr;  // Wait for reads to finish\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0317f64491ba42376d96b157983a02d8b31b679e",
    "Commit message": "Update RNNCell._rnn_get_variable to use Variable._trainable in TF2 mode.\n\nWhen using a legacy RNNCell in TF2 mode within a tf.function the \"var in trainable_variables()\" check led to treating a tf.bool tensor as a Python bool. This change makes use within a tf.function use the same logic that is used in Eager mode.\n\nPiperOrigin-RevId: 315052160\nChange-Id: I2fb89690d780e9823ab7e399d8e5cd6fe41d9300",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "    if context.executing_eagerly():\n      trainable = variable._trainable  # pylint: disable=protected-access",
    "Added code": "    if ops.executing_eagerly_outside_functions():\n      trainable = variable.trainable"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b8c517ab4ef0bd851ef2f8187935fd3a90261af5",
    "Commit message": "Reinstate eager check inside _GradientsHelper",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "  if context.executing_eagerly():\n    raise RuntimeError(\"tf.gradients is not supported when eager execution \"\n                       \"is enabled. Use tf.GradientTape instead.\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c9b4689bc4d4024aa16b7d6cfc1c65fa1ed8486e",
    "Commit message": "Removed no longer supported call to in_eager_execution\n\nchanges to model_analyser.analyse(...):\r\n- Swapped context.in_eager_execution() to the currently supported context.executing_eagerly().\r\n- Added negation to eager check. In all likelihood, the negation was always supposed to be there since getting default graph in eager mode does not make sense. The current `if` condition is likely a bug. The proposed fix is consistent with other functions in this module, e.g., `profile(...)`, line 339.",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  if not graph and context.in_eager_execution():",
    "Added code": "  if not graph and not context.executing_eagerly():"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e5496b556734bb1d8de85311092804e0150b3009",
    "Commit message": "Remove extraneous check for Eager mode (#17125)\n\nThe check is already made once at the start of the method",
    "Deleted lines": 2,
    "Added lines": 0,
    "Changed lines": 2,
    "Deleted code": "      if context.in_eager_mode():\n        return",
    "Added code": ""
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/be5116dd131a92da298dbb68d26e0d47f66f2fe5",
    "Commit message": "Correct graph check in broadcast_to gradient.\n\nPiperOrigin-RevId: 294970783\nChange-Id: I2774ddf6949a1ba6814cefd939695010ecda0861",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  if not context.executing_eagerly():",
    "Added code": "  if not isinstance(broadcast_shape, ops.EagerTensor):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1d6dae88efef68dd7fbeeb5c39ea0f69c1c721c1",
    "Commit message": "Add check to tf.device when called with a function in eager mode.\n\nPiperOrigin-RevId: 173947845",
    "Deleted lines": 0,
    "Added lines": 7,
    "Changed lines": 7,
    "Deleted code": "",
    "Added code": "\n  Raises:\n    RuntimeError: If eager execution is enabled and a function is passed in.\n    if callable(device_name_or_function):\n      raise RuntimeError(\n          \"tf.device does not support functions when eager execution \"\n          \"is enabled.\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8c3822edbb31cf71cedaf49f2167e45c1e2d0b83",
    "Commit message": "Update the is_dtensor check to only run in eager mode.\n\nPiperOrigin-RevId: 516294602",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "\n    Raises:\n      RuntimeError: When not called eagerly.\n    if not context.executing_eagerly():\n      raise RuntimeError(\"is_dtensor must be called eagerly.\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a63f3006f703428ff980748cdbe24d6a13f761e2",
    "Commit message": "Skip checking for graph_key in V1 optimizer when running in eager mode.\n\nPiperOrigin-RevId: 509660850",
    "Deleted lines": 1,
    "Added lines": 6,
    "Changed lines": 7,
    "Deleted code": "      if variable_object._graph_key == current_graph_key:  # pylint: disable=protected-access",
    "Added code": "      # Skip checking for graph key for eager mode since there's only one graph.\n      # This is necessary because there are cases where _trackable_children() is\n      # called in a differenr thread from the main thread (e.g., async\n      # checkpoint) and hence the default graph key would be different.\n      if (context.executing_eagerly()\n          or variable_object._graph_key == current_graph_key):  # pylint: disable=protected-access"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dd7d791e02396346d98b7b2c58137d7e51756c0c",
    "Commit message": "Add isinstance check for eager execution.\n\nPiperOrigin-RevId: 507003564",
    "Deleted lines": 2,
    "Added lines": 3,
    "Changed lines": 5,
    "Deleted code": "\n  if isinstance(v, internal.NativeObject):",
    "Added code": "  if isinstance(v, EagerTensor) and not context.executing_eagerly():\n    return convert_to_tensor(v, as_ref=True).op, None\n  elif isinstance(v, internal.NativeObject):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8933b8a21280696ab119b63263babdb54c298538",
    "Commit message": "Fix a null pointer exception caused by branching on uninitialized data.\n\nThis is due to not checking that the params for the quantization exists. If there is no quantization, we should not access the `.params` field.\n\nPiperOrigin-RevId: 385173491\nChange-Id: I8fc476c4b274fdb21ba741caa0fbc6d1b8840663",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n  TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0a9b39caefd437fec742ae48b25061abd6e2699b",
    "Commit message": "When allocating GPU constants, check to see if the destination\ntensor is intialized early (because we ran out of memory) and report\nit as such.\n\nFixes #7025.\nChange: 154603030",
    "Deleted lines": 0,
    "Added lines": 8,
    "Changed lines": 8,
    "Deleted code": "",
    "Added code": "\n    // If the tensor is not initialized, we likely ran out of memory.\n    if (!copy.IsInitialized()) {\n      return errors::ResourceExhausted(\n          \"OOM when allocating tensor of shape \", parsed.shape().DebugString(),\n          \" and type \", DataTypeString(parsed.dtype()));\n    }\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d8df06a9403b1434aea8b82a193fc30a4ab29bbb",
    "Commit message": "Add assert in Operation->printAssembly to check improperly created Op's.\n\nWe allow the name of an operation to be different from the name of the\n'ConcreteType' op it was instantiated with. This can happen when you sub-class\nan existing op and provide a getOperationName for it. Such a situation leads to\nan assertion too deep and at a place seeminly unrelated, and typically when the\nmodule is printed with the trace:\n\nprintOperation, printAssembly, Op::print, getOperand, dyn_cast<OperationStmt>,\nisa. 'isa' will complain about being called on a null pointer, and the null\npointer actually comes from the getAs<> in printAssembly. This should have been\ncaught in printAssembly.\n\nOn another note, it is also weird that we allow setting the op's name to\nsomething independent of the ConcreteType that op was instantiated with - so,\ngetAs<ConcreteType> will fail since ConcreteType::isClassFor won't succeed on\nit.\n\nPiperOrigin-RevId: 216580294",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "    op->getAs<ConcreteType>()->print(p);",
    "Added code": "    auto opPointer = op->getAs<ConcreteType>();\n    assert(opPointer &&\n           \"op's name does not match name of concrete type instantiated with\");\n    opPointer->print(p);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4f38b1ac8e42727e18a2f0bde06d3bee8e77b250",
    "Commit message": "Prevent null dereference read in `GetInitOp`.\n\nWe have a map of maps. We test that the key exists in the first map but then we don't have any validation that this also means the second map has the needed key. In the scenarios where this is not the case, we'll dereference a nullptr, if we don't have this check\n\nPiperOrigin-RevId: 408739325\nChange-Id: If9bb7ed759aba1f3b56a34913f209508dbaf65ce",
    "Deleted lines": 3,
    "Added lines": 8,
    "Changed lines": 11,
    "Deleted code": "    *init_op_name = init_op_sig_it->second.outputs()\n                        .find(kSavedModelInitOpSignatureKey)\n                        ->second.name();",
    "Added code": "    const auto& sig_def_outputs = init_op_sig_it->second.outputs();\n    const auto& sig_def_outputs_it =\n        sig_def_outputs.find(kSavedModelInitOpSignatureKey);\n    if (sig_def_outputs_it == sig_def_outputs.end()) {\n      return errors::FailedPrecondition(\"Could not find output \",\n                                        kSavedModelInitOpSignatureKey);\n    }\n    *init_op_name = sig_def_outputs_it->second.name();"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a6bf104fad37bcf4c6fcfb2e08c541473f886513",
    "Commit message": "Refactoring: Set default quantization method before checking\n\nPiperOrigin-RevId: 582488375",
    "Deleted lines": 13,
    "Added lines": 13,
    "Changed lines": 26,
    "Deleted code": "  # Converter assumes options are specified. So set SRQ explicitly.\n  if (\n      quantization_options.quantization_method.preset_method\n      == _PresetMethod.METHOD_UNSPECIFIED\n  ):\n    logging.debug(\n        '\"preset_method\" for QuantizationMethod is not specified.'\n        'Static range quantization is used by default.'\n    )\n    quantization_options.quantization_method.preset_method = (\n        _PresetMethod.METHOD_STATIC_RANGE_INT8\n    )\n",
    "Added code": "  # Converter assumes options are specified. So set SRQ explicitly.\n  if (\n      quantization_options.quantization_method.preset_method\n      == _PresetMethod.METHOD_UNSPECIFIED\n  ):\n    logging.debug(\n        '\"preset_method\" for QuantizationMethod is not specified.'\n        'Static range quantization is used by default.'\n    )\n    quantization_options.quantization_method.preset_method = (\n        _PresetMethod.METHOD_STATIC_RANGE_INT8\n    )\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a93ac5f7d147ae8fe946de33ad654161ae851352",
    "Commit message": "For quantization values where range_min == range_max, use the lowest_quantized.\n Add needed checks for divide-by-zero.\nChange: 126753893",
    "Deleted lines": 5,
    "Added lines": 9,
    "Changed lines": 14,
    "Deleted code": "    return 0;\n  const int64 lowest_quantized =\n      static_cast<double>(Eigen::NumTraits<T>::lowest());\n        range_scale((number_of_steps - 1.0) / (range_max - range_min)),\n      static_cast<int64>(255.0 * (1 << fp_shift) * input_range / output_range);",
    "Added code": "  const int64 lowest_quantized =\n      static_cast<double>(Eigen::NumTraits<T>::lowest());\n    return lowest_quantized;\n        range_scale(range_max == range_min\n                        ? 0.0\n                        : (number_of_steps - 1.0) / (range_max - range_min)),\n      output_range == 0.0 ? 0.0\n                          : static_cast<int64>(255.0 * (1 << fp_shift) *\n                                               input_range / output_range);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e56206755ff0c98269eb3e50c98fccbaadb6884d",
    "Commit message": "Support quantized i64 during flatbuffer import\n\nSometimes tflite flatbuffer represent `i32` quantized values as `i64`s. In these cases we should truncate down to a lower bit width to avoid creating illegal types.\n\nWe check the bitwidth of the type at load time and pick a power-of-2 bit\nwidth where the value can be safely truncated.\n\nPiperOrigin-RevId: 558958449",
    "Deleted lines": 70,
    "Added lines": 135,
    "Changed lines": 205,
    "Deleted code": "                                         bool is_constant = false) {\n  mlir::IntegerType storage_type;\n    storage_type = builder.getIntegerType(8);\n  } else {\n  bool is_weight_buffer = is_constant && (storage_type.getWidth() == 8);\n\n  int64_t storage_min = QuantizedType::getDefaultMinimumForInteger(\n                            is_signed, storage_type.getWidth()) +\n                        static_cast<int>(is_weight_buffer);\n  int64_t storage_max = QuantizedType::getDefaultMaximumForInteger(\n      is_signed, storage_type.getWidth());\n                                         bool is_intermediate = false) {\n  if (IsQuantized(tensor)) {\nstd::vector<T> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n  std::vector<T> ret;\n    ret.push_back(llvm::support::endian::readNext<\n                  T, llvm::support::endian::system_endianness(),\n                  llvm::support::unaligned>(data_ptr));\n    mlir::RankedTensorType shaped_type, mlir::FloatType elem_type,\n    const std::vector<uint8_t>& buffer) {\n  switch (elem_type.getWidth()) {\n  return errors::InvalidArgument(\"unsupported bit width\", elem_type.getWidth());\n    mlir::RankedTensorType shaped_type, mlir::Type elem_type,\n    const std::vector<uint8_t>& buffer) {\n  } else if (auto qtype = elem_type.dyn_cast<QuantizedType>()) {\n      llvm::SmallVector<bool, 8> values;\n      values.reserve(buffer.size());\n        values.emplace_back(b != 0);\n          DenseElementsAttr::get(shaped_type, ArrayRef<bool>(values)));\n      auto values =\n          shaped_type, ArrayRef<char>(values)));\n      auto values = ReadAsHostEndian<uint16_t>(buffer);\n      return mlir::ElementsAttr(\n          DenseElementsAttr::get(shaped_type, ArrayRef<uint16_t>(values)));\n      auto values = ReadAsHostEndian<uint32_t>(buffer);\n      return mlir::ElementsAttr(\n          DenseElementsAttr::get(shaped_type, ArrayRef<uint32_t>(values)));\n      auto values = ReadAsHostEndian<uint64_t>(buffer);\n      return mlir::ElementsAttr(\n          DenseElementsAttr::get(shaped_type, ArrayRef<uint64_t>(values)));\nOperation* BuildVariableOp(const tflite::TensorT& tensor,\n                           mlir::RankedTensorType shaped_type,\n                           OpBuilder builder, Location loc) {\n    const mlir::RankedTensorType shaped_type, OpBuilder& builder,\n    Location loc) {\n  auto value_type = shaped_type;\n  if (IsQuantized(tensor)) {\n    value_type = tensorflow::GetTypeFromTFTensorShape(\n        shaped_type.getShape(), shaped_type.getElementType()\n                                    .dyn_cast<mlir::quant::QuantizedType>()\n                                    .getStorageType());\n  }\n                                               /*is_constant=*/true));\n  if (tensor.sparsity != nullptr) {\n    return BuildSparseConstOp(tensor, buffer, shaped_type, builder, loc);\n\n  mlir::ElementsAttr value;\n  if (is_variable) {\n    return BuildVariableOp(tensor, shaped_type, builder, loc);\n  } else if (auto float_type = elem_type.dyn_cast<mlir::FloatType>()) {\n    TF_ASSIGN_OR_RETURN(value,\n                        ConvertFloatBuffer(shaped_type, float_type, buffer));\n  } else if (elem_type.isa<mlir::IntegerType, QuantizedType>()) {\n    TF_ASSIGN_OR_RETURN(value,\n                        ConvertIntBuffer(shaped_type, elem_type, buffer));\n  if (IsQuantized(tensor)) {\n    auto op = builder.create<tfl::QConstOp>(\n        loc, mlir::TypeAttr::get(shaped_type), value);\n    return op.getOperation();\n  }",
    "Added code": "                                         bool is_constant = false,\n                                         mlir::Type storage_type = {}) {\n    storage_type = mlir::IntegerType::get(builder.getContext(), 8);\n  }\n\n  if (!storage_type) {\n  int bitwidth = storage_type.getIntOrFloatBitWidth();\n  bool is_weight_buffer = is_constant && (bitwidth == 8);\n\n  int64_t storage_min =\n      QuantizedType::getDefaultMinimumForInteger(is_signed, bitwidth) +\n      static_cast<int>(is_weight_buffer);\n  int64_t storage_max =\n      QuantizedType::getDefaultMaximumForInteger(is_signed, bitwidth);\n                                         bool is_intermediate = false,\n                                         bool get_storage = false) {\n  if (IsQuantized(tensor) && !get_storage) {\n  } else if (IsQuantized(tensor) && get_storage) {\n    // If the type is quantized we strip the signedness from the storage type.\n    elem_type = mlir::IntegerType::get(elem_type.getContext(),\n                                       elem_type.getIntOrFloatBitWidth());\nllvm::SmallVector<mlir::APInt> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n  llvm::SmallVector<mlir::APInt> ret;\n    T val = llvm::support::endian::readNext<\n        T, llvm::support::endian::system_endianness(),\n        llvm::support::unaligned>(data_ptr);\n    ret.push_back(mlir::APInt(sizeof(T) * 8, val));\n    mlir::RankedTensorType shaped_type, const std::vector<uint8_t>& buffer) {\n  mlir::Type elem_type = shaped_type.getElementType();\n  switch (elem_type.getIntOrFloatBitWidth()) {\n  return errors::InvalidArgument(\"unsupported bit width\",\n                                 elem_type.getIntOrFloatBitWidth());\n}\n\n// If the values in the buffer can be clamped to a bitwidth, truncate\n// and return the new clamped integer width.\nvoid truncateLimitedIntegerAPInt(llvm::SmallVector<mlir::APInt>& values) {\n  mlir::APInt min = values[0];\n  mlir::APInt max = values[0];\n  for (auto& val : values) {\n    min = llvm::APIntOps::smin(val, min);\n    max = llvm::APIntOps::smax(val, max);\n  }\n\n  for (int64_t bw = 8; bw < min.getBitWidth(); bw += bw) {\n    auto limitMin = mlir::APInt::getSignedMinValue(bw).sext(min.getBitWidth());\n    auto limitMax = mlir::APInt::getSignedMaxValue(bw).sext(min.getBitWidth());\n    if (min.sle(limitMin) || max.sle(limitMin) || min.sge(limitMax) ||\n        max.sge(limitMax)) {\n      continue;\n    }\n\n    for (int i = 0; i < values.size(); i++) {\n      values[i] = values[i].trunc(bw);\n    }\n    break;\n  }\n    mlir::RankedTensorType shaped_type, const std::vector<uint8_t>& buffer,\n    bool truncate = false) {\n  mlir::Type elem_type = shaped_type.getElementType();\n  } else if (auto qtype = elem_type.dyn_cast<mlir::quant::QuantizedType>()) {\n  llvm::SmallVector<mlir::APInt> values;\n      llvm::SmallVector<bool, 8> boolValues;\n      boolValues.reserve(buffer.size());\n        boolValues.emplace_back(b != 0);\n          DenseElementsAttr::get(shaped_type, ArrayRef<bool>(boolValues)));\n      auto i4Values =\n          shaped_type, ArrayRef<char>(i4Values)));\n      values = ReadAsHostEndian<uint16_t>(buffer);\n      break;\n      values = ReadAsHostEndian<uint32_t>(buffer);\n      break;\n      values = ReadAsHostEndian<uint64_t>(buffer);\n      break;\n\n  if (truncate) {\n    truncateLimitedIntegerAPInt(values);\n    auto sign = mlir::cast<mlir::IntegerType>(shaped_type.getElementType())\n                    .getSignedness();\n    auto ety = mlir::IntegerType::get(shaped_type.getContext(),\n                                      values[0].getBitWidth(), sign);\n    shaped_type =\n        tensorflow::GetTypeFromTFTensorShape(shaped_type.getShape(), ety);\n  }\n\n  return mlir::ElementsAttr(DenseElementsAttr::get(shaped_type, values));\nStatusOr<Operation*> BuildVariableOp(const tflite::TensorT& tensor,\n                                     OpBuilder builder, Location loc) {\n  TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n                                               /*is_constant=*/true));\n  auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n  if (!shaped_type) {\n    return errors::Internal(\"Constant doesn't have a shape\");\n  }\n\n    OpBuilder& builder, Location loc) {\n  TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n                                               /*is_constant=*/true));\n  auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n  if (!shaped_type) {\n    return errors::Internal(\"Constant doesn't have a shape\");\n  }\n\n  TF_ASSIGN_OR_RETURN(type, GetTensorType(tensor, builder,\n                                          /*is_constant=*/true,\n                                          /*is_intermediate=*/false,\n                                          /*get_storage=*/true));\n  auto value_type = mlir::dyn_cast<mlir::RankedTensorType>(type);\n\n  if (tensor.sparsity != nullptr) {\n    return BuildSparseConstOp(tensor, buffer, builder, loc);\n  }\n\n  if (is_variable) {\n    return BuildVariableOp(tensor, builder, loc);\n  }\n\n                                               /*is_constant=*/true,\n                                               /*is_intermediate=*/false,\n                                               /*get_storage=*/true));\n  mlir::ElementsAttr value;\n  if (IsQuantized(tensor)) {\n    bool truncate = shaped_type.getElementType().getIntOrFloatBitWidth() == 64;\n    TF_ASSIGN_OR_RETURN(value, ConvertIntBuffer(shaped_type, buffer, truncate));\n    TF_ASSIGN_OR_RETURN(\n        auto type, GetQuantizedType(tensor, builder, /*is_constant=*/true,\n                                    /*storage_type=*/value.getElementType()));\n    shaped_type = shaped_type.clone(type);\n    auto op = builder.create<tfl::QConstOp>(\n        loc, mlir::TypeAttr::get(shaped_type), value);\n    return op.getOperation();\n  if (auto float_type = elem_type.dyn_cast<mlir::FloatType>()) {\n    TF_ASSIGN_OR_RETURN(value, ConvertFloatBuffer(shaped_type, buffer));\n  } else if (elem_type.isa<mlir::IntegerType>()) {\n    TF_ASSIGN_OR_RETURN(value, ConvertIntBuffer(shaped_type, buffer));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e7de472681079932b2547024f31c876da54f61a0",
    "Commit message": "Fix a bug in flatbuffer importer that use tensor quantization before checking.\n\nPiperOrigin-RevId: 346193249\nChange-Id: I10fabc67ee5339c40db640477116127fb50bc575",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  if (!tensor.quantization->min.empty()) {",
    "Added code": "  if (tensor.quantization && !tensor.quantization->min.empty()) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2adf1114d4dc7ca30e5117acd2dc7aeb3279feb7",
    "Commit message": "Make NNAPI delegate only apply overflow check to quantized average_pool\n\nPiperOrigin-RevId: 295269801\nChange-Id: Iea8dc5e2da5496aca76500c207f02f67196c14b1",
    "Deleted lines": 6,
    "Added lines": 8,
    "Changed lines": 14,
    "Deleted code": "      // reference CPU path.\n      Expect(is_accelerator_specified ||\n                 (builtin->filter_width * builtin->filter_height <= 256),\n             NNAPIValidationFailureType::kUnsupportedOperandSize,\n             \"Large filter window would overflow on the reference CPU path\",\n             &val_ctx);",
    "Added code": "      // quantized reference CPU path.\n      if (IsQuantized(context->tensors[node->inputs->data[0]].type)) {\n        Expect(is_accelerator_specified ||\n                   (builtin->filter_width * builtin->filter_height <= 256),\n               NNAPIValidationFailureType::kUnsupportedOperandSize,\n               \"Large filter window would overflow on the reference CPU path\",\n               &val_ctx);\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/677866210941431b82c95d58d0798976bb40a415",
    "Commit message": "Add a nullptr check for the tensor quantization field\n\nPiperOrigin-RevId: 277009093\nChange-Id: Id7ff4ee3939f0faf1652393c021317d82b0b179f",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "  if (IsQuantized(tensor)) return nullptr;",
    "Added code": "  if (!tensor.quantization || IsQuantized(tensor)) return nullptr;\n    // TODO(fengliuai): this quantization dimension isn't correct."
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/19b2e1b5868a044df4622ef7e26fa5570ca52e5e",
    "Commit message": "Only perform scalar check for a tensor shape if it's not empty.\n\nPiperOrigin-RevId: 428100731\nChange-Id: I4b908cf448bcbb853023899f8b501b82c8ac03f0",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    DCHECK(weights.shape_.IsScalar());",
    "Added code": "    DCHECK(weights.shape_.IsEmpty() || weights.shape_.IsScalar());"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ebb292f6efcd690e6e2152ce3f69a4ab0e1194f6",
    "Commit message": "Fix boosted trees shape function issue.\n\nDimension was accessed before size was checked, leading to segfault.\nSwitching order of checks fixes this.\n\nNote: boosted trees are deprecated in favor of decision forests.\nPiperOrigin-RevId: 551971138",
    "Deleted lines": 3,
    "Added lines": 3,
    "Changed lines": 6,
    "Deleted code": "      shape_inference::DimensionHandle batch_size = c->Dim(c->input(0), 0);\n      shape_inference::DimensionHandle num_entries;\n",
    "Added code": "      shape_inference::DimensionHandle batch_size = c->Dim(c->input(0), 0);\n      shape_inference::DimensionHandle num_entries;\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9c92b50fc4b95985a0749101976d04896bf19bfe",
    "Commit message": "[conv3d_transpose] Fix dim check for bias\n\nPer discussion with @thaink, the previous way to do the dim check for bias is not correct. So we need this change.",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 4));",
    "Added code": "    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 3));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8eb773d6c23de29dccfc3cf3da441a8552ed13ed",
    "Commit message": "[XLA] Better shape size validation for sparse arrays.\n\nPiperOrigin-RevId: 205262376",
    "Deleted lines": 20,
    "Added lines": 31,
    "Changed lines": 51,
    "Deleted code": "    int64 shape_size;\n      shape_size = LayoutUtil::MaxSparseElements(shape.layout());\n      if (shape_size < 0) {\n        return shape_size;\n      shape_size = MultiplyWithoutOverflow(shape_size, ShapeUtil::Rank(shape));\n      if (shape_size < 0) {\n        return shape_size;\n      shape_size = MultiplyWithoutOverflow(shape_size, sizeof(int64));\n      if (shape_size < 0) {\n        return shape_size;\n    shape_size = 1;\n\n      return shape_size;\n      shape_size = MultiplyWithoutOverflow(shape_size, dim);\n      if (shape_size < 0) {\n        return shape_size;\n    shape_size = MultiplyWithoutOverflow(\n        shape_size, ByteSizeOfPrimitiveType(shape.element_type()));\n\n    return shape_size;",
    "Added code": "      int64 max_sparse_elements = LayoutUtil::MaxSparseElements(shape.layout());\n      if (max_sparse_elements < 0) {\n        return max_sparse_elements;\n      int64 sparse_elements_size = MultiplyWithoutOverflow(\n          max_sparse_elements, ByteSizeOfPrimitiveType(shape.element_type()));\n      if (sparse_elements_size < 0) {\n        return sparse_elements_size;\n      int64 sparse_indices_size =\n          MultiplyWithoutOverflow(max_sparse_elements, ShapeUtil::Rank(shape));\n      if (sparse_indices_size < 0) {\n        return sparse_indices_size;\n      }\n      sparse_indices_size =\n          MultiplyWithoutOverflow(sparse_indices_size, sizeof(int64));\n      if (sparse_indices_size < 0) {\n        return sparse_indices_size;\n      }\n      // At this point, both sparse_indices_size and sparse_elements_size are\n      // non-negative, so we can easily check if adding them wraps.\n      if (static_cast<uint64>(sparse_elements_size) +\n              static_cast<uint64>(sparse_indices_size) >\n          INT64_MAX) {\n        return static_cast<int64>(-1);\n    int64 dense_shape_size = 1;\n      return dense_shape_size;\n      dense_shape_size = MultiplyWithoutOverflow(dense_shape_size, dim);\n      if (dense_shape_size < 0) {\n        return dense_shape_size;\n    dense_shape_size = MultiplyWithoutOverflow(\n        dense_shape_size, ByteSizeOfPrimitiveType(shape.element_type()));\n    return dense_shape_size;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5bc536f1afbaff5d3d5a14a9185cd1e3cc31b302",
    "Commit message": "[Fix] bug fix during check static shape.\n\nPiperOrigin-RevId: 288604974\nChange-Id: I5b22b754c5396c3a6d148159642500993a5c8215",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    if (!shaped_type && !shaped_type.hasStaticShape()) {",
    "Added code": "    if (!shaped_type || !shaped_type.hasStaticShape()) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/eb2ddc0debb7e1b0c9ea68c817ca05fd59dc7914",
    "Commit message": "In TF2XLA EnsureShape kernel, don't check whether the original tensor has dynamic shapes as it is much more expensive than just blindly clear out dynamic dimension.\n\nPiperOrigin-RevId: 464003792",
    "Deleted lines": 6,
    "Added lines": 7,
    "Changed lines": 13,
    "Deleted code": "    // remove the dynamic dimensions in XLA dynamic padder.\n    std::vector<bool> dynamic_dims;\n    OP_REQUIRES_OK(ctx,\n                   ctx->ResolveInputDynamismIntoPredVector(0, &dynamic_dims));\n      if (expected_shape_.dim_size(i) > 0 && dynamic_dims[i]) {\n        VLOG(1) << \"RemoveDynamicDimension: \" << i;",
    "Added code": "    // remove the dynamic dimensions in XLA dynamic padder. Here we don't check\n    // whether the original input has dynamic shapes, because\n    // `ctx->ResolveInputDynamismIntoPredVector` runs a DFS underneath which is\n    // more expensive.\n      if (expected_shape_.dim_size(i) > 0) {\n        VLOG(1) << \"RemoveDynamicDimension: \" << i << \" of shape \"\n                << shape.DebugString();"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/264eb6ed1dbfb5e078c7dd977da8d7e633106fc5",
    "Commit message": "Fixed add bias transformation.\nAdded check for convolution with dynamic weights.\n\nPiperOrigin-RevId: 320996352\nChange-Id: Ie88eb026151c8ce49e9987867bc2807e13176cea",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "      if (graph->FindInputs(node->id).size() != 1) {\n        return {TransformStatus::DECLINED,\n                \"This transformation is only applicable to conv with one \"\n                \"runtime input.\"};\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bb9b9f32bd08bc5660343cdc09f4b467d7297e3c",
    "Commit message": "Change a recently introduced DCHECK from O(N) cost to O(1).\n\nA recent change added a DCHECK that took O(N) time for an\ninstruction with N users every time a user was added. This\nbecame quadratic and caused timeouts in some tests that\nenable assertion checking. Made the DCHECK O(1) to fix it.\n\nPiperOrigin-RevId: 595834589",
    "Deleted lines": 11,
    "Added lines": 3,
    "Changed lines": 14,
    "Deleted code": "    int64_t index = 0;\n    for (const HloInstruction* u : users_) {\n      CHECK(user_map_->contains(u));\n      CHECK_EQ((*user_map_)[u], index);\n      index++;\n    }\n    for (auto [u, index] : *user_map_) {\n      CHECK_GE(index, 0);\n      CHECK_LT(index, users_.size());\n      CHECK_EQ(users_[index], u);\n    }",
    "Added code": "    // Avoid quadratic behavior by doing a quick and dirty check on\n    // size instead of actually comparing mapped indices.\n    CHECK_EQ(users_.size(), user_map_->size());"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/653d47379b3a716f82058148c35b2f491bfa2856",
    "Commit message": "Speed up 4-bit unpacking loop in reference implementation\n\nThe new implementation moves the odd-length check out of the main loop, and unrolls slightly differently.\n\nPiperOrigin-RevId: 525759375",
    "Deleted lines": 6,
    "Added lines": 12,
    "Changed lines": 18,
    "Deleted code": "  for (int i = 0; i < num_elements; i += 2) {\n    dst_buffer[i] = static_cast<int8_t>(src_buffer[i / 2] << 4) >> 4;\n    // Break early if the tensor has odd length and the higher nibble should be\n    // ignored.\n    if (i + 1 == num_elements) break;\n    dst_buffer[i + 1] = static_cast<int8_t>(src_buffer[i / 2]) >> 4;",
    "Added code": "  for (int i = 0; i < num_elements / 2; i++) {\n    int8_t byte = src_buffer[i];\n    int8_t lower = static_cast<int8_t>(byte << 4) >> 4;\n    int8_t higher = byte >> 4;\n    dst_buffer[2 * i] = lower;\n    dst_buffer[2 * i + 1] = higher;\n  }\n\n  // If the buffer size is odd, extract the final lower nibble.\n  if (num_elements % 2 != 0) {\n    dst_buffer[num_elements - 1] =\n        static_cast<int8_t>(src_buffer[num_elements / 2] << 4) >> 4;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2e4d3951eb618a7c34d5e629fc2506ea2a62b4a7",
    "Commit message": "Correct Tensor order for dilation2D\n\n`gen_nn_ops.dilation2d` seems to be in `NHWC` while the parent function was asking for `NCHW`.\r\nI corrected the doc and the check.",
    "Deleted lines": 3,
    "Added lines": 3,
    "Changed lines": 6,
    "Deleted code": "    data_format: A `string`, only `\"NCHW\"` is currently supported.\n  if data_format != \"NCHW\":\n    raise ValueError(\"Data formats other than NCHW are not yet supported\")",
    "Added code": "    data_format: A `string`, only `\"NHWC\"` is currently supported.\n  if data_format != \"NHWC\":\n    raise ValueError(\"Data formats other than NHWC are not yet supported\")"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8cef4cda26e08256b6698e942820d9a3ac1bcc94",
    "Commit message": "Add minor checks for data_format and padding value",
    "Deleted lines": 2,
    "Added lines": 8,
    "Changed lines": 10,
    "Deleted code": "  FormatFromString(data_format.str(), &format);\n  GetPaddingFromString(paddings.str(), &padding);",
    "Added code": "  auto data_format_is_valid = FormatFromString(data_format.str(), &format);\n  if (!data_format_is_valid) {\n    return emitOptionalError(location, \"Invalid data format provided\");\n  }\n  auto padding_is_valid = GetPaddingFromString(paddings.str(), &padding);\n  if (!padding_is_valid.ok()) {\n    return emitOptionalError(location, \"Invalid padding format provided\");\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/076ea8d84c2058b0d01d56dd9ddc3221a2e0c817",
    "Commit message": "Also check dst_format",
    "Deleted lines": 2,
    "Added lines": 4,
    "Changed lines": 6,
    "Deleted code": "  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");\n  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");",
    "Added code": "  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\") &&\n                  (dst_format == \"NHWC\" || dst_format == \"NCHW\");\n  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\") &&\n                  (dst_format == \"NHWC\" || dst_format == \"NCHW\");"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ed06859189722af4dc8e4abd655926df066e587a",
    "Commit message": "Add format check.",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "      DCHECK(data_format == \"NCDHW\");\n      DCHECK(data_format == \"NCHW\");"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0d5668cbdc6b46d099bd3abd93374c09b2e8121f",
    "Commit message": "[XLA:SHAPE_UTIL] Return nullopt instead of a check failure if the input dimensions are not sorted.\n\nPiperOrigin-RevId: 439966260",
    "Deleted lines": 1,
    "Added lines": 3,
    "Changed lines": 4,
    "Deleted code": "  CHECK(std::is_sorted(input_dim_indices.begin(), input_dim_indices.end()));",
    "Added code": "  if (!std::is_sorted(input_dim_indices.begin(), input_dim_indices.end())) {\n    return absl::nullopt;\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0d5668cbdc6b46d099bd3abd93374c09b2e8121f",
    "Commit message": "[XLA:SHAPE_UTIL] Return nullopt instead of a check failure if the input dimensions are not sorted.\n\nPiperOrigin-RevId: 439966260",
    "Deleted lines": 1,
    "Added lines": 3,
    "Changed lines": 4,
    "Deleted code": "  CHECK(std::is_sorted(input_dim_indices.begin(), input_dim_indices.end()));",
    "Added code": "  if (!std::is_sorted(input_dim_indices.begin(), input_dim_indices.end())) {\n    return absl::nullopt;\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d7ec7b9415181fce88ea8fde39af9e8be5a8be97",
    "Commit message": "Added generic check that shape has not more than 4 dimensions.\n\nPiperOrigin-RevId: 380849977\nChange-Id: I94da5e4e5f557e1649f8ca5284ebf1ad363ba211",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "            \"OP is supported, but tensor type doesn't match.\";",
    "Added code": "    if (t->dims && t->dims->size >= 5) {\n      return false;\n    }\n            \"OP is supported, but tensor type/shape doesn't supported.\";"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/824af2acfa0cdf897c08d91224aea0958c1afc02",
    "Commit message": "Add ndmin check\n\nAdded ndmin check to allow maximum 32 ndmin to make same behavior as numpy.\r\nCurrently it is crashing when very large ndmin is passed.",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "  max_ndmin = 32\n  if ndmin > max_ndmin:\n    raise ValueError('ndmin bigger than allowable number of dimensions: '\n                     f'{max_ndmin}.')\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b73a3c21a224f479af8d3b8af320c220a091906c",
    "Commit message": "[XLA] Add check for potential out-of-bound access.\n\nPiperOrigin-RevId: 407330138\nChange-Id: Ib32dfd4ac963bbb79e3dde999334c81c8b87f55f",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "  TF_RET_CHECK(sort_dim >= 0 && sort_dim < increment.size())\n      << \"Unexpected out-of-bound sort dimension \" << sort_dim\n      << \" accessing increment of size \" << increment.size();"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/63753d5f1531b17cf8cbbf1d8b77c16edcfb9711",
    "Commit message": "Change DCHECK_LE to DCHECK_LT when checking invariant on original indices for sorted items\n\nIndices of items should be strictly smaller than the size of the vector.\n\nPiperOrigin-RevId: 391640935\nChange-Id: I110f2a8d269e90d9853094f68ada700d0c346c12",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    DCHECK_LE(original_index, names.size());",
    "Added code": "    DCHECK_LT(original_index, names.size());"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7f9929732ced22fe8ef42a695dae39c1caf44608",
    "Commit message": "For gather op, if params.shape[:batch_dims] is not the same as indices.shape[:batch_dims], return an error instead of check fail (according to https://www.tensorflow.org/api_docs/python/tf/gather).\n\nPiperOrigin-RevId: 274672054",
    "Deleted lines": 0,
    "Added lines": 7,
    "Changed lines": 7,
    "Deleted code": "",
    "Added code": "      for (int i = 0; i < batch_dims_; ++i) {\n        OP_REQUIRES(c, params.dim_size(i) == indices.dim_size(i),\n                    errors::InvalidArgument(\n                        \"params.shape[\", i, \"]: \", params.dim_size(i),\n                        \" should be equal to indices.shape[\", i,\n                        \"]: \", indices.dim_size(i)));\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/480641e3599775a8895254ffbc0fc45621334f68",
    "Commit message": "Validate (and ensure validation sticks) inputs for `MatrixTriangularSolve`.\n\nPiperOrigin-RevId: 370282444\nChange-Id: Iaed61a0b0727cc42c830658b72eb69f785f48dc5",
    "Deleted lines": 4,
    "Added lines": 16,
    "Changed lines": 20,
    "Deleted code": "        ctx, in0.dims() >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0.dims()));\n        ctx, in1.dims() >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in1.dims()));",
    "Added code": "    if (!ctx->status().ok()) {\n      return;\n    }\n    const auto in0_num_dims = in0.dims();\n        ctx, in0_num_dims >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0_num_dims));\n    const auto in1_num_dims = in1.dims();\n        ctx, in1_num_dims >= 2,\n        errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1_num_dims));\n\n    const auto in0_last_dim = in0.dim_size(in0_num_dims - 1);\n    const auto in0_prev_dim = in0.dim_size(in0_num_dims - 2);\n    OP_REQUIRES(ctx, in0_last_dim == in0_prev_dim,\n                errors::InvalidArgument(\n                    \"In[0] matrices in the last dimensions must be square (\",\n                    in0_last_dim, \" =/= \", in0_prev_dim, \")\"));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/480641e3599775a8895254ffbc0fc45621334f68",
    "Commit message": "Validate (and ensure validation sticks) inputs for `MatrixTriangularSolve`.\n\nPiperOrigin-RevId: 370282444\nChange-Id: Iaed61a0b0727cc42c830658b72eb69f785f48dc5",
    "Deleted lines": 4,
    "Added lines": 16,
    "Changed lines": 20,
    "Deleted code": "        ctx, in0.dims() >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0.dims()));\n        ctx, in1.dims() >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in1.dims()));",
    "Added code": "    if (!ctx->status().ok()) {\n      return;\n    }\n    const auto in0_num_dims = in0.dims();\n        ctx, in0_num_dims >= 2,\n        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0_num_dims));\n    const auto in1_num_dims = in1.dims();\n        ctx, in1_num_dims >= 2,\n        errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1_num_dims));\n\n    const auto in0_last_dim = in0.dim_size(in0_num_dims - 1);\n    const auto in0_prev_dim = in0.dim_size(in0_num_dims - 2);\n    OP_REQUIRES(ctx, in0_last_dim == in0_prev_dim,\n                errors::InvalidArgument(\n                    \"In[0] matrices in the last dimensions must be square (\",\n                    in0_last_dim, \" =/= \", in0_prev_dim, \")\"));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9e62869465573cb2d9b5053f1fa02a81fce21d69",
    "Commit message": "Add more validation to `RequantizationRangePerChannel`.\n\nPiperOrigin-RevId: 387693946\nChange-Id: Ife8dcbdb021bec4787eef6a4361dd08f17c14bd6",
    "Deleted lines": 0,
    "Added lines": 14,
    "Changed lines": 14,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(\n        ctx, input_min.NumElements() == depth,\n        errors::InvalidArgument(\"input_min must have the same number of \"\n                                \"elements as input_max, got \",\n                                input_min.NumElements(), \" and \", depth));\n    OP_REQUIRES(ctx, input.NumElements() > 0,\n                errors::InvalidArgument(\"input must not be empty\"));\n    OP_REQUIRES(ctx, input.dims() == 4,\n                errors::InvalidArgument(\"input must be in NHWC format\"));\n    OP_REQUIRES(\n        ctx, input.dim_size(3) == depth,\n        errors::InvalidArgument(\n            \"input must have same number of channels as length of input_min: \",\n            input.dim_size(3), \" vs \", depth));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ba91c04e001f417641e757a6417e5325c1c4e15e",
    "Commit message": "Add more check to sparsity parameter verifier.\n\nPiperOrigin-RevId: 286436553\nChange-Id: I43913b2a16ad7bb3b3e22fa65cd47462159b8f67",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "  if (sparsity->dim_metadata()->size() != total_dims) {",
    "Added code": "  if (total_dims < tensor.shape()->size() ||\n      sparsity->dim_metadata()->size() != total_dims) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1610f391833738972b538e4ee97f90dbd30fc745",
    "Commit message": "Replace `DCHECK` with actual validation in `AddRangeStats`\n\nPiperOrigin-RevId: 411138402\nChange-Id: I123eafe70e292f286ba384d4ed73222edf61a6ea",
    "Deleted lines": 2,
    "Added lines": 8,
    "Changed lines": 10,
    "Deleted code": "  DCHECK_LE(start_instance, end_instance);\n    DCHECK_LT(start_feature_dim, end_feature_dim);",
    "Added code": "  OP_REQUIRES(context, start_instance <= end_instance,\n              errors::InvalidArgument(\n                  \"start_instance = \", start_instance,\n                  \" which is not at most end_instance=\", end_instance));\n    OP_REQUIRES(context, start_feature_dim < end_feature_dim,\n                errors::InvalidArgument(\n                    \"start_feature_dim = \", start_feature_dim,\n                    \" which is not at most end_feature_dim=\", end_feature_dim));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/150a6c06b281246cb5a075a704fceeb257bb63af",
    "Commit message": "Add a check on the 0th dimension of filter for DepthwiseConv.\n\nPiperOrigin-RevId: 259662414",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "  // Filter in DepthwiseConv is expected to be [1, H, W, O].\n  TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 0), 1);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bf686faeddcca97be6ad7b6421cb26ab1c3cea2c",
    "Commit message": "TFLite: Enhance input check for ResizeNearestNeghbor\nPiperOrigin-RevId: 239440642",
    "Deleted lines": 2,
    "Added lines": 4,
    "Changed lines": 6,
    "Deleted code": "  // TODO(ahentz): Our current implementations rely on the inputs being 4D.\n",
    "Added code": "  // TODO(ahentz): Our current implementations rely on the input being 4D,\n  // and the size being 1D tensor with exactly 2 elements.\n  TF_LITE_ENSURE_EQ(context, size->dims->data[0], 2);\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bf686faeddcca97be6ad7b6421cb26ab1c3cea2c",
    "Commit message": "TFLite: Enhance input check for ResizeNearestNeghbor\nPiperOrigin-RevId: 239440642",
    "Deleted lines": 2,
    "Added lines": 4,
    "Changed lines": 6,
    "Deleted code": "  // TODO(ahentz): Our current implementations rely on the inputs being 4D.\n",
    "Added code": "  // TODO(ahentz): Our current implementations rely on the input being 4D,\n  // and the size being 1D tensor with exactly 2 elements.\n  TF_LITE_ENSURE_EQ(context, size->dims->data[0], 2);\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fa8381593d0cbe354cb54d691e0a8c42bf4b69d0",
    "Commit message": "Move Batch op input validation before enqueueing to the batch scheduler, because earlier error detection is better (and also so batch->size() doesn't crash if #dims==0 :).\nChange: 154470659",
    "Deleted lines": 16,
    "Added lines": 12,
    "Changed lines": 28,
    "Deleted code": "      batch_components->inputs.push_back(tensors[i]);\n\n      for (int edge = 0; edge < task.inputs.size(); ++edge) {\n        const Tensor& tensor = task.inputs[edge];\n\n        if (tensor.shape().dims() == 0) {\n          return errors::InvalidArgument(\n              \"Batching input tensors must have at least one dimension\");\n        }\n\n        if (tensor.shape().dim_size(0) != task.inputs[0].shape().dim_size(0)) {\n          return errors::InvalidArgument(\n              \"Batching input tensors supplied in a given op invocation must \"\n              \"have equal 0th-dimension size\");\n        }\n      }",
    "Added code": "      const Tensor& tensor = tensors[i];\n      if (tensor.shape().dims() == 0) {\n        return errors::InvalidArgument(\n            \"Batching input tensors must have at least one dimension\");\n      }\n      if (tensors.size() >= 2 &&\n          tensor.shape().dim_size(0) != tensors[0].shape().dim_size(0)) {\n        return errors::InvalidArgument(\n            \"Batching input tensors supplied in a given op invocation must \"\n            \"have equal 0th-dimension size\");\n      }\n      batch_components->inputs.push_back(tensor);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c040db5e9003cc20016586df9f2964db83b98c4f",
    "Commit message": "[XLA] Add a defensive check in dynamic dimension inference to prevent scalar reshape with dynamic dimension.\n\nIn theory we can just ignore a [1] -> [] reshape, but adding a check here for now.\n\nPiperOrigin-RevId: 268804389",
    "Deleted lines": 3,
    "Added lines": 8,
    "Changed lines": 11,
    "Deleted code": "      hlo, [&](HloInstruction* operand, ShapeIndex index, int64 dimension,\n               int64 operand_index, HloInstruction* dynamic_size,\n               DimensionConstraint constraint) {",
    "Added code": "      hlo,\n      [&](HloInstruction* operand, ShapeIndex index, int64 dimension,\n          int64 operand_index, HloInstruction* dynamic_size,\n          DimensionConstraint constraint) -> Status {\n        TF_RET_CHECK(reshape->shape().rank() > 0)\n            << \"Reshaping a dynamic dimension into a scalar, which has \"\n               \"undefined behavior. The offending instruction is: \"\n            << reshape->ToString();"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/48393637f8154be16088d84742485a0e153ecbb2",
    "Commit message": "Change check to allow tensors with up to 6 dims.\n\nPiperOrigin-RevId: 298531979\nChange-Id: I6b4d6196d68c32fb93c84c9fbd980197c118ebaa",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "  CHECK_LE(RequiredBufferSizeForShape(dims_array.shape()), 4)\n      << \"dims vector can be no larger than 4 values\";",
    "Added code": "  CHECK_LE(RequiredBufferSizeForShape(dims_array.shape()), 6)\n      << \"dims vector can be no larger than 6 values\";"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/662128e8ca3411286b234553a7efc1356353d0f5",
    "Commit message": "add rank checking for MEAN op\n\nThe MEAN op of NNAPI only supports a tensor with rank <= 4.\nCheck the rank of the input tensor before delegating the op.\n\n```\n...\n12-22 09:22:24.514  6130  6130 E ModelBuilder: Invalid Operation: NN_RET_CHECK failed (packages/modules/NeuralNetworks/common/types/operations/src/SimpleMath.cpp:30): inputRank <= 4u (inputRank = 5, 4u = 4) Unsupported input tensor rank for operation MEAN\n12-22 09:22:24.514  6130  6130 E tflite  : NN API returned error ANEURALNETWORKS_BAD_DATA at line 1131 while adding operation.\n12-22 09:22:24.515  6130  6130 E tflite  : Restored original execution plan after delegate application failure.\n...\n```",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "      Expect(context->tensors[node->inputs->data[0]].dims->size <= 4,\n             NNAPIValidationFailureType::kUnsupportedOperandValue,\n             \"NNAPI does not support mean of a tensor with rank > 4\",\n             &val_ctx);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1711b76f95b49dcf597fe5b2ec5f4ff79ddbc7a7",
    "Commit message": "check if boxes have logical coordinates (different than a line or a point)",
    "Deleted lines": 0,
    "Added lines": 19,
    "Changed lines": 19,
    "Deleted code": "",
    "Added code": "// This function checks that all boxes have coherant ccordinates\ntemplate <typename T>\nbool CheckBoxesCoordinates(const Tensor& boxes, int num_boxes){\n  typename TTypes<T, 2>::ConstTensor boxes_data = boxes.tensor<T, 2>();\n  for(int i=0; i<num_boxes; ++i){\n    if(boxes_data(i, 0) == boxes_data(i, 2) || boxes_data(i, 1) == boxes_data(i, 3))\n      return (false);\n  }\n  return (true);\n}\n\n    // check if the boxes have logical coordinates\n    OP_REQUIRES(context, CheckBoxesCoordinates<float>(boxes, num_boxes), errors::InvalidArgument(\"boxes coordinates shouldn't have x1=x2 or y1=y2\"));\n    // check if the boxes have logical coordinates\n    OP_REQUIRES(context, CheckBoxesCoordinates<T>(boxes, num_boxes), errors::InvalidArgument(\"boxes coordinates shouldn't have x1=x2 or y1=y2\"));\n    // check if the boxes have logical coordinates\n    OP_REQUIRES(context, CheckBoxesCoordinates<T>(boxes, num_boxes), errors::InvalidArgument(\"boxes coordinates shouldn't have x1=x2 or y1=y2\"));\n    // check if the boxes have logical coordinates\n    OP_REQUIRES(context, CheckBoxesCoordinates<T>(boxes, num_boxes), errors::InvalidArgument(\"boxes coordinates shouldn't have x1=x2 or y1=y2\"));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9b947dd6377c022091c8aa005cdcff52c53ff5f0",
    "Commit message": "Also check dst_format",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");",
    "Added code": "  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\") &&\n                  (dst_format == \"NHWC\" || dst_format == \"NCHW\");"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/450dec35448a73b3fcb5d4f82108d5fdcb3f59b4",
    "Commit message": "Internal change, add some checks on the sparseTensor format checking.\n\nPiperOrigin-RevId: 568349775",
    "Deleted lines": 1,
    "Added lines": 8,
    "Changed lines": 9,
    "Deleted code": "      *(row_ids_before_padding + i) = indices_matrix(i, 0);",
    "Added code": "    int32 previous_row_id = -1;\n      int32 current_row_id = indices_matrix(i, 0);\n      if (current_row_id < previous_row_id) {\n        return absl::InvalidArgumentError(\n            \"Invalid indices_or_row_splits input, indices of SparseTensor need \"\n            \"to be sorted in ascending order.\");\n      }\n      *(row_ids_before_padding + i) = current_row_id;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/450dec35448a73b3fcb5d4f82108d5fdcb3f59b4",
    "Commit message": "Internal change, add some checks on the sparseTensor format checking.\n\nPiperOrigin-RevId: 568349775",
    "Deleted lines": 1,
    "Added lines": 8,
    "Changed lines": 9,
    "Deleted code": "      *(row_ids_before_padding + i) = indices_matrix(i, 0);",
    "Added code": "    int32 previous_row_id = -1;\n      int32 current_row_id = indices_matrix(i, 0);\n      if (current_row_id < previous_row_id) {\n        return absl::InvalidArgumentError(\n            \"Invalid indices_or_row_splits input, indices of SparseTensor need \"\n            \"to be sorted in ascending order.\");\n      }\n      *(row_ids_before_padding + i) = current_row_id;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/356f360e8772a2697ec0d30036237342549803f5",
    "Commit message": "Add additional shape validation to `compute_accidental_hits`\n\nIn `compute_accidental_hits`, the `sampled_candidates` must\nbe a vector, as is shown in the kernel implementation in\n`tensorflow/core/kernels/candidate_sampler_ops.cc`.\n\nThis fix adds shape validation of `sampled_candidates`\nin the shape function whenever possible.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "      // Validate true_classes.",
    "Added code": "      // Validate true_classes, must be a matrix.\n      // Validate sampled_candidates, must be a vector.\n      ShapeHandle sampled_candidates;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &sampled_candidates));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7c88788e63f3a747d2794175076db551d768734e",
    "Commit message": "Shape validation of `max_features` in `QuantizedReluX`\n\nIn shape function of QuantizedReluX, `max_value` and\n`min_features` have shape validation but not `max_features`.\nThis fix add restriction to `max_features` as well.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ff6be80a1ec3c353ebd0d17e2f0b46d9097310db",
    "Commit message": "Improve the shape function for ParameterizedTruncatedNormal (#19215)\n\nThe parameters of ParameterizedTruncatedNormal should\r\nbe 0-D or 1-D, which is checked in ther kernel functions.\r\nThere is no check in the shape function of the ops.\r\n\r\nThis fix improves the shape function and checks the\r\nparameters of ParameterizedTruncatedNormal whever possible.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 1,
    "Added lines": 9,
    "Changed lines": 10,
    "Deleted code": "    .SetShapeFn(shape_inference::RandomShape);",
    "Added code": "    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      // Parameters must be 0-d or 1-d.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(1), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(3), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &unused));\n      return shape_inference::RandomShape(c);\n    });"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ff6be80a1ec3c353ebd0d17e2f0b46d9097310db",
    "Commit message": "Improve the shape function for ParameterizedTruncatedNormal (#19215)\n\nThe parameters of ParameterizedTruncatedNormal should\r\nbe 0-D or 1-D, which is checked in ther kernel functions.\r\nThere is no check in the shape function of the ops.\r\n\r\nThis fix improves the shape function and checks the\r\nparameters of ParameterizedTruncatedNormal whever possible.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 1,
    "Added lines": 9,
    "Changed lines": 10,
    "Deleted code": "    .SetShapeFn(shape_inference::RandomShape);",
    "Added code": "    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      // Parameters must be 0-d or 1-d.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(1), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(3), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &unused));\n      return shape_inference::RandomShape(c);\n    });"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c59c37e7b2d563967da813fa50fe20b21f4da683",
    "Commit message": "Prevent array write out-of-bounds.\n\nIf user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\nPiperOrigin-RevId: 371023299\nChange-Id: I9eca37ffc2b29e8e48710f500701270ef0790224",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "  TF_LITE_ENSURE(context, axis_value >= 0);\n  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e82a377de614fed51da8a7c5242a90a7967169f2",
    "Commit message": "Correct axis check",
    "Deleted lines": 3,
    "Added lines": 7,
    "Changed lines": 10,
    "Deleted code": "    if (abs(axis_value) > input_type.getRank())\n      return op.emitOpError(\"op attribute 'axis' is out of bounds, got \")\n             << axis_value;",
    "Added code": "    if (axis_value < 0)\n      axis_value += input_type.getRank() + 1;\n    if (axis_value < 0 || axis_value >= input_type.getRank() + 1)\n      return op.emitOpError()\n             << \"op attribute 'axis' should be in range [-rank - 1, rank + 1), \"\n             << \"got rank = \" << input_type.getRank()\n             << \", and axis = \" << op.axis().getSExtValue();"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/402d478a107e2931fb0e9b2f08f973997cae7f98",
    "Commit message": "Move the checking of ranks for early exit",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "  if (!ShouldProcess(*context, *node) || (rank != 4 && rank != 5) ||",
    "Added code": "  if (rank != 4 && rank != 5) {\n    return Status::OK();\n  }\n  if (!ShouldProcess(*context, *node) ||"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/27de8e717c1bec91398f5a6be6c7287b657fc960",
    "Commit message": "Improve shape function for CudnnRNNParamsSize\n\nIn cudnn_rnn_ops.cc, the CudnnRNNParamsSize does not\nhave restrictions on num_layers, num_units, and input_size,\nthough they all should be scalars.\n\nThis fix adds the shape check of num_layers, num_units, and input_size\nfor CudnnRNNParamsSize.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "      ShapeHandle unused;\n      // num_layers, num_units, and input_size should be scalars.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/02703f9525696f4788496745f6756585c1c546a3",
    "Commit message": "Fix crash in range sampler by adding a range check in the sampler op.\nChange: 147562709",
    "Deleted lines": 2,
    "Added lines": 6,
    "Changed lines": 8,
    "Deleted code": "    CHECK(sampler_) << \"CandidateSamplerOp did not set sampler_\";\n",
    "Added code": "    CHECK(sampler_) << \"CandidateSamplerOp did not set sampler_\";\n\n    if (unique_) {\n      OP_REQUIRES(context, num_sampled_ <= sampler_->range(),\n                  errors::InvalidArgument(\"Sampler's range is too small.\"));\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4a1d1c8413a3752af7dc91a7128e202660b0f05c",
    "Commit message": "Fix mismatch of shape restriction in DrawBoundingBoxes\n\nIn the kernel of DrawBoundingBoxes, the shape of the input\nimages should be 4-D. Though in the shape function,\nat the end `UnchangedShapeWithRankAtLeast(c, 3)` was used instead\n(at the beginning of the shape function the validation is\n`WithRank(c->input(0), 4, &images)` which is correct).\n\nThis fix address the discrepancy by changing to `UnchangedShape`.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 1,
    "Added lines": 3,
    "Changed lines": 4,
    "Deleted code": "      return shape_inference::UnchangedShapeWithRankAtLeast(c, 3);",
    "Added code": "      // The rank of the input image (rank = 4) has already been restricted\n      // above, and the output is of the same shape as the input.\n      return shape_inference::UnchangedShape(c);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/181ca305a7954ce86a453a39db0b4f6d10b82720",
    "Commit message": "Add shape validation in shape function of MapAndBatchDataset\n\nIn MapAndBatchDataset, batch_size, num_parallel_batches,\nand drop_remainder are 0-D scalars. This fix adds\nthe shape check to those Inputs.\n\nNote since the Input of `other_arguments` is a list and is\nbefore `batch_size`, the shape of the `batch_size` and others\ncould not be obtained through index like `c->input(2)` etc directly.\nIt is still possible to obtain the ShapeHandle with names `c->input(\"batch_size\", &batch_size)`,\nthough.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 1,
    "Added lines": 30,
    "Changed lines": 31,
    "Deleted code": "    .SetShapeFn(shape_inference::ScalarShape);",
    "Added code": "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n\n      // Use name (vs. index like c->input(1)) to retrieve the Input shapes,\n      // so that to avoid guessing the length of \"other_arguments\".\n\n      // batch_size, num_parallel_batches, and drop_remainder are 0-D scalars.\n      std::vector<shape_inference::ShapeHandle> batch_size;\n      TF_RETURN_IF_ERROR(c->input(\"batch_size\", &batch_size));\n      if (batch_size.size() != 1) {\n        return errors::InvalidArgument(\"Requires list(batch_size) == 1\");\n      }\n      TF_RETURN_IF_ERROR(c->WithRank(batch_size[0], 0, &unused));\n\n      std::vector<shape_inference::ShapeHandle> num_parallel_batches;\n      TF_RETURN_IF_ERROR(c->input(\"num_parallel_batches\", &num_parallel_batches));\n      if (num_parallel_batches.size() != 1) {\n        return errors::InvalidArgument(\"Requires list(num_parallel_batches) == 1\");\n      }\n      TF_RETURN_IF_ERROR(c->WithRank(num_parallel_batches[0], 0, &unused));\n\n      std::vector<shape_inference::ShapeHandle> drop_remainder;\n      TF_RETURN_IF_ERROR(c->input(\"drop_remainder\", &drop_remainder));\n      if (drop_remainder.size() != 1) {\n        return errors::InvalidArgument(\"Requires list(drop_remainder) == 1\");\n      }\n      TF_RETURN_IF_ERROR(c->WithRank(drop_remainder[0], 0, &unused));\n\n      return shape_inference::ScalarShape(c);\n    });"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6e153325b66330dafea4e4e8b67b5d56b1a37852",
    "Commit message": "[XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast produces a\nscalar. This avoids crashing within last_dimension when attempting to match.\n\nPiperOrigin-RevId: 548090995",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "  if (bitcast->shape().rank() == 0) {\n    return true;\n  }\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/429f009d2b2c09028647dd4bb7b3f6f414bbaad7",
    "Commit message": "Add remaining missing validation to `BoostedTreesCalculateBestFeatureSplit`\n\nPiperOrigin-RevId: 387423006\nChange-Id: I8eaf30efb223011519e60707bfa751b275d3a443",
    "Deleted lines": 1,
    "Added lines": 19,
    "Changed lines": 20,
    "Deleted code": "    std::vector<string> output_split_types;",
    "Added code": "#include <string>\n#include \"tensorflow/core/platform/errors.h\"\n    OP_REQUIRES(\n        context, node_id_range_t->NumElements() == 2,\n        errors::InvalidArgument(\"node_id_range argument must have shape [2]\"));\n    OP_REQUIRES(\n        context, stats_summary_t->shape().dims() == 4,\n        errors::InvalidArgument(\"stats_summary argument must have rank 4\"));\n    OP_REQUIRES(context, l1_t->NumElements() == 1,\n                errors::InvalidArgument(\"l1 argument must be a scalar\"));\n    OP_REQUIRES(context, l2_t->NumElements() == 1,\n                errors::InvalidArgument(\"l2 argument must be a scalar\"));\n    OP_REQUIRES(\n        context, tree_complexity_t->NumElements() == 1,\n        errors::InvalidArgument(\"tree_complexity argument must be a scalar\"));\n    OP_REQUIRES(\n        context, min_node_weight_t->NumElements() == 1,\n        errors::InvalidArgument(\"min_node_weight argument must be a scalar\"));\n    std::vector<std::string> output_split_types;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9d3cce4c7525bad6743f84302e5f6355a3fd8fe5",
    "Commit message": "Fix crash in BlockLSTM\n\nThis PR tries to address the issue raised in 58175 in addressing\nthe crash of BlockLSTM when invalid input is provided.\n\nThis PR fixes 58175.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(seq_len_max_tensor->shape()),\n                errors::InvalidArgument(\"`seq_len_max_tensor` must be rank 0 but is rank \",\n                                        seq_len_max_tensor->dims()));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(seq_len_max_tensor->shape()),\n                errors::InvalidArgument(\"`seq_len_max_tensor` must be rank 0 but is rank \",\n                                        seq_len_max_tensor->dims()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2c72ca8c439d64268e849ef81cde78f464e95ca2",
    "Commit message": "add rank checking for ADD, MUL, and DIV",
    "Deleted lines": 0,
    "Added lines": 21,
    "Changed lines": 21,
    "Deleted code": "",
    "Added code": "      const int input0_rank =\n          context->tensors[node->inputs->data[0]].dims->size;\n      const int input1_rank =\n          context->tensors[node->inputs->data[1]].dims->size;\n      Expect(input0_rank <= 4 && input1_rank <= 4,\n             NNAPIValidationFailureType::kUnsupportedOperandRank,\n             \"Input rank must be <= 4\", &val_ctx);\n      const int input0_rank =\n          context->tensors[node->inputs->data[0]].dims->size;\n      const int input1_rank =\n          context->tensors[node->inputs->data[1]].dims->size;\n      Expect(input0_rank <= 4 && input1_rank <= 4,\n             NNAPIValidationFailureType::kUnsupportedOperandRank,\n             \"Input rank must be <= 4\", &val_ctx);\n      const int input0_rank =\n          context->tensors[node->inputs->data[0]].dims->size;\n      const int input1_rank =\n          context->tensors[node->inputs->data[1]].dims->size;\n      Expect(input0_rank <= 4 && input1_rank <= 4,\n             NNAPIValidationFailureType::kUnsupportedOperandRank,\n             \"Input rank must be <= 4\", &val_ctx);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d94ffe08a65400f898241c0374e9edc6fa8ed257",
    "Commit message": "Prevent an OOB read in `expand_dims.cc`\n\nThe for loop that follows this check assumes that `axis` is between `0` and `input_dims.size`. If user supplied `axis` is negative, the if code before this check is supposed to bring it back to positive (similar to how in Python one can do `l[-3]` to mean `l[-3 + len(l)]`).\n\nPiperOrigin-RevId: 387200206\nChange-Id: I162f4feba12d547c3a4340833ae682016a2ebfab",
    "Deleted lines": 0,
    "Added lines": 1,
    "Changed lines": 1,
    "Deleted code": "",
    "Added code": "  TF_LITE_ENSURE(context, axis >= 0);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0e3574d39c66d937fa9f9d2e25554aab0066f250",
    "Commit message": "Add rank check to Sub op delegation to NNAPI\n\nPiperOrigin-RevId: 307821863\nChange-Id: Ib98448d67e9948576e6c9fb43a98d364ab434e37",
    "Deleted lines": 2,
    "Added lines": 9,
    "Changed lines": 11,
    "Deleted code": "      ExpectMaxOpVersion(version, 2, &val_ctx);\n}",
    "Added code": "      ExpectMaxOpVersion(version, 3, &val_ctx);\n      const int input0_rank =\n          context->tensors[node->inputs->data[0]].dims->size;\n      const int input1_rank =\n          context->tensors[node->inputs->data[1]].dims->size;\n      Expect(input0_rank <= 4 && input1_rank <= 4,\n             NNAPIValidationFailureType::kUnsupportedOperandRank,\n             \"Input rank must be <= 4\", &val_ctx);\n}  // NOLINT(readability/fn_size)"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a680ed0bf03d5ca3b2c4a70c0d95eeebc20da6d6",
    "Commit message": "For Substr check pos and len rank equality only when their rank is known.\n\nThis fixes a bug where len has unknown rank, while pos has known shape. The WithRank(...) check returned error in such a case. Here we compare their ranks only when both pos and len have known rank.\n\nPiperOrigin-RevId: 275370109\nChange-Id: I8df36f3d4dcf3104e246e8605689b9ded3d9c783",
    "Deleted lines": 2,
    "Added lines": 4,
    "Changed lines": 6,
    "Deleted code": "      // Check that pos/len have same rank\n      TF_RETURN_IF_ERROR(c->WithRank(pos_shape, c->Rank(len_shape), &unused));",
    "Added code": "      // If len rank is known, check that pos and len have the same rank\n      if (c->RankKnown(len_shape)) {\n        TF_RETURN_IF_ERROR(c->WithRank(pos_shape, c->Rank(len_shape), &unused));\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/50f6683ca50e6d4e7008d6d1b437b407d6a62e92",
    "Commit message": "Add shape check for batch related Dataset ops (#18683)\n\n* Add shape check for PrefetchDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add BatchDataset shape check\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for SlideDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for DenseToSparseBatchDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Sanitize with clang-format -i --style=Google\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 4,
    "Added lines": 27,
    "Changed lines": 31,
    "Deleted code": "    .SetShapeFn(shape_inference::ScalarShape);\n    .SetShapeFn(shape_inference::ScalarShape);\n    .SetShapeFn(shape_inference::ScalarShape);\n    .SetShapeFn(shape_inference::ScalarShape);",
    "Added code": "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // buffer_size should be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // batch_size should be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // window_size and stride should be scalars.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // batch_size should be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      // row_shape should be a 1-D vector.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      return shape_inference::ScalarShape(c);\n    });"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/50f6683ca50e6d4e7008d6d1b437b407d6a62e92",
    "Commit message": "Add shape check for batch related Dataset ops (#18683)\n\n* Add shape check for PrefetchDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add BatchDataset shape check\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for SlideDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for DenseToSparseBatchDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Sanitize with clang-format -i --style=Google\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 4,
    "Added lines": 27,
    "Changed lines": 31,
    "Deleted code": "    .SetShapeFn(shape_inference::ScalarShape);\n    .SetShapeFn(shape_inference::ScalarShape);\n    .SetShapeFn(shape_inference::ScalarShape);\n    .SetShapeFn(shape_inference::ScalarShape);",
    "Added code": "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // buffer_size should be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // batch_size should be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // window_size and stride should be scalars.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // batch_size should be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      // row_shape should be a 1-D vector.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      return shape_inference::ScalarShape(c);\n    });"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9187be7adff07be82856add498aa3ff4b5f95998",
    "Commit message": "add checks for compression_type and buffer_size also",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "      // `compression_type` could only be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      // `buffer_size` could only be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/779664494d43b18a812361197dcbea2f25912c02",
    "Commit message": "Add shape check to TextLineDataset op",
    "Deleted lines": 4,
    "Added lines": 6,
    "Changed lines": 10,
    "Deleted code": "    .SetShapeFn(shape_inference::ScalarShape);  // TODO(mrry): validate\n                                                // that `filenames` is\n                                                // a scalar or a\n                                                // vector.",
    "Added code": "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // `filenames` must be a scalar or a vector.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n      return shape_inference::ScalarShape(c);\n    });"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/779664494d43b18a812361197dcbea2f25912c02",
    "Commit message": "Add shape check to TextLineDataset op",
    "Deleted lines": 4,
    "Added lines": 6,
    "Changed lines": 10,
    "Deleted code": "    .SetShapeFn(shape_inference::ScalarShape);  // TODO(mrry): validate\n                                                // that `filenames` is\n                                                // a scalar or a\n                                                // vector.",
    "Added code": "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // `filenames` must be a scalar or a vector.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n      return shape_inference::ScalarShape(c);\n    });"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c4dea2255c71037c9cade9cbd1d7820b3429b3fa",
    "Commit message": "Add shape check for buffer_size with TFRecordDataset\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "      // `buffer_size` could only be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused) );"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d97ffbdf362fa7d06ef8d946c8620ff7a3a50a08",
    "Commit message": "Add shape check for compression_type in TFrecordDataset\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "      // `compression_type` could only be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused) );"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7586dee9aa8b4b63143ab658ca59658aaed0df97",
    "Commit message": "Add shape check to TFRecordDataset\n\nThe inputs of TFRecordDataset have the requirements for shapes.\nHowever, the check was not done in the shape function. This fix\nadds shape checks whenever possible.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 1,
    "Added lines": 6,
    "Changed lines": 7,
    "Deleted code": "    .SetShapeFn(shape_inference::ScalarShape);",
    "Added code": "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // `filenames` must be a scalar or a vector.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n      return shape_inference::ScalarShape(c);\n    });"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/851177fee860211e2fabcb019d644e75b7f701b0",
    "Commit message": "Add shape check for shift of tf.roll\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "      // The `shift` must be scalar or 1-D.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(1), 1, &unused));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3f796ff8c9e6d7ff88f99c056b78e88fb0b31114",
    "Commit message": "Add axis shape check for tf.roll\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "      // The `axis` must be scalar or 1-D.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/10467d29e05d9957a6e3cb2335f8eeba1fd8896e",
    "Commit message": "Improve shape function check for `tf.roll`\n\nThe `tf.roll` op has requirements for the shape of inputs. However,\nthe shape of the inputs are only done at the runtime inside the kernel.\nThis fix improve the shape function so that the check could be\ndone early if shape is already known in the shape function.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 1,
    "Added lines": 7,
    "Changed lines": 8,
    "Deleted code": "    .SetShapeFn(shape_inference::UnchangedShape);",
    "Added code": "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // The `input` must be 1-D or higher\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\n\n      return shape_inference::UnchangedShape(c);\n    });"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/10467d29e05d9957a6e3cb2335f8eeba1fd8896e",
    "Commit message": "Improve shape function check for `tf.roll`\n\nThe `tf.roll` op has requirements for the shape of inputs. However,\nthe shape of the inputs are only done at the runtime inside the kernel.\nThis fix improve the shape function so that the check could be\ndone early if shape is already known in the shape function.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 1,
    "Added lines": 7,
    "Changed lines": 8,
    "Deleted code": "    .SetShapeFn(shape_inference::UnchangedShape);",
    "Added code": "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // The `input` must be 1-D or higher\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\n\n      return shape_inference::UnchangedShape(c);\n    });"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/41deb95a7bde735d3c8b9adedd8b1fe8c1ef2732",
    "Commit message": "support unknown rank, check rank>=0",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "  if(rank == kUnknownRank) {\n    return UnknownShape();\n  }\n  CHECK_GE(rank,0) << \"rank must not be negative\";"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8b742f8559e88474735d0a2c03e00da65e40b412",
    "Commit message": "Fix check error on shape overflow.\n\nFixes #60198, #60275\n\nPiperOrigin-RevId: 529127714",
    "Deleted lines": 2,
    "Added lines": 4,
    "Changed lines": 6,
    "Deleted code": "    input_matrix_shapes->emplace_back(\n        std::initializer_list<int64_t>({num_rows, num_cols}));",
    "Added code": "    TensorShape input_shape;\n    OP_REQUIRES_OK(context, TensorShape::BuildTensorShape({num_rows, num_cols},\n                                                          &input_shape));\n    input_matrix_shapes->push_back(std::move(input_shape));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1595906c2192b7f402f746652042a592ad290378",
    "Commit message": "Prevent `CHECK`-fail DOS in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.\n\nPiperOrigin-RevId: 410952517\nChange-Id: Ica269fc5695d2a467c2e3c7b0681d717d152e2a3",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(feature_indices_t->shape()),\n                errors::InvalidArgument(\n                    \"feature_indices must be a matrix, received shape \",\n                    feature_indices_t->shape().DebugString()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/54c94431e5dd17fc46d99da1a3f132c76414c161",
    "Commit message": "Prevent `CHECK`-fail DOS in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.\n\nPiperOrigin-RevId: 410951880\nChange-Id: Id26099f022d68366eec03cc878e57bf6237ecccf",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(\n        context, TensorShapeUtils::IsMatrix(hessians_t->shape()),\n        errors::InvalidArgument(\"hessians must be a matrix, received shape \",\n                                hessians_t->shape().DebugString()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5d96267d907ac2119cbccf1416b749195e8fd8de",
    "Commit message": "Prevent `CHECK`-fail DOS in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.\n\nPiperOrigin-RevId: 410951067\nChange-Id: I73a968f2116cc14b3e0e868d8a188aa232b47643",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(\n        context, TensorShapeUtils::IsMatrix(gradients_t->shape()),\n        errors::InvalidArgument(\"gradients must be a matrix, received shape \",\n                                gradients_t->shape().DebugString()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/41ab69692ede0db3422fa70bc5889d470741e69c",
    "Commit message": "Check for tensors to be vectors in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->vec` should only happen after checking that the tensor shape implies a vector. Otherwise, we can get denial of service via `CHECK`-fails\n\nPiperOrigin-RevId: 411054482\nChange-Id: Idc7a624dcc9e84685bf328b2d0e4842b904229dd",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(context, TensorShapeUtils::IsVector(feature_values_t->shape()),\n                errors::InvalidArgument(\n                    \"feature_values must be a vector, received shape \",\n                    feature_values_t->shape().DebugString()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8d733ecdb270dd90b2b5f53fd220d5ce17a5e20f",
    "Commit message": "Check for tensors to be vectors in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->vec` should only happen after checking that the tensor shape implies a vector. Otherwise, we can get denial of service via `CHECK`-fails\n\nPiperOrigin-RevId: 410960878\nChange-Id: I7b26bec796cbaebde4696862eb855160402b4b0d",
    "Deleted lines": 0,
    "Added lines": 4,
    "Changed lines": 4,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(node_ids_t->shape()),\n        errors::InvalidArgument(\"node_ids must be a vector, received shape \",\n                                node_ids_t->shape().DebugString()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f482488b481a799ca07e7e2d153cf47b8e91a60c",
    "Commit message": "TFLite OpenGL ES delegate: out of boundary writes fixed for bhwc->phwc4 conversion.\n\nPiperOrigin-RevId: 281857556\nChange-Id: I94d17ca85965cf0ada292ad1ac3f7d21e36126a1",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  uint3 workload = uint3(shape.w, shape.h, shape.c);",
    "Added code": "  uint3 workload = uint3(shape.w, shape.h, IntegralDivideRoundUp(shape.c, 4));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/63c6a29d0f2d692b247f7bf81f8732d6442fad09",
    "Commit message": "Add missing validation, prevent heap OOB\n\nPiperOrigin-RevId: 372246723\nChange-Id: I1a454a643810e77d7d14821b342098c56a09fbbf",
    "Deleted lines": 0,
    "Added lines": 12,
    "Changed lines": 12,
    "Deleted code": "",
    "Added code": "    if (!context->status().ok()) return;  // params is invalid\n    OP_REQUIRES(context,\n                tensor_in.NumElements() == out_grad_backprop.NumElements(),\n                errors::InvalidArgument(\"tensor_in and out_grad_backprop must \"\n                                        \"have same number of elements, got <\",\n                                        tensor_in.DebugString(), \"> and <\",\n                                        out_grad_backprop.DebugString(), \">\"));\n    OP_REQUIRES(\n        context, tensor_out.NumElements() == output->NumElements(),\n        errors::InvalidArgument(\n            \"tensor_out and output must have same number of elements, got <\",\n            tensor_out.DebugString(), \"> and <\", output->DebugString(), \">\"));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/63c6a29d0f2d692b247f7bf81f8732d6442fad09",
    "Commit message": "Add missing validation, prevent heap OOB\n\nPiperOrigin-RevId: 372246723\nChange-Id: I1a454a643810e77d7d14821b342098c56a09fbbf",
    "Deleted lines": 0,
    "Added lines": 12,
    "Changed lines": 12,
    "Deleted code": "",
    "Added code": "    if (!context->status().ok()) return;  // params is invalid\n    OP_REQUIRES(context,\n                tensor_in.NumElements() == out_grad_backprop.NumElements(),\n                errors::InvalidArgument(\"tensor_in and out_grad_backprop must \"\n                                        \"have same number of elements, got <\",\n                                        tensor_in.DebugString(), \"> and <\",\n                                        out_grad_backprop.DebugString(), \">\"));\n    OP_REQUIRES(\n        context, tensor_out.NumElements() == output->NumElements(),\n        errors::InvalidArgument(\n            \"tensor_out and output must have same number of elements, got <\",\n            tensor_out.DebugString(), \"> and <\", output->DebugString(), \">\"));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/58759659ee547a957c5d36e72f2274ab34fdb6cb",
    "Commit message": "Fix OOB check for result_index in header generation",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  if (result_index < 0 || result_index > temp_sizes.size()) {",
    "Added code": "  if (result_index < 0 || result_index >= temp_sizes.size()) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d6ed5bcfe1dcab9e85a4d39931bd18d99018e75b",
    "Commit message": "Add missing validation in `QuantizedBatchNormWithGlobalNormalization`\n\nPiperOrigin-RevId: 370123451\nChange-Id: Id234d6dab1ec21230bb8e503dba30f899af87f33",
    "Deleted lines": 10,
    "Added lines": 67,
    "Changed lines": 77,
    "Deleted code": "    const float input_min = context->input(1).flat<float>()(0);\n    const float input_max = context->input(2).flat<float>()(0);\n    const float mean_min = context->input(4).flat<float>()(0);\n    const float mean_max = context->input(5).flat<float>()(0);\n    const float var_min = context->input(7).flat<float>()(0);\n    const float var_max = context->input(8).flat<float>()(0);\n    const float beta_min = context->input(10).flat<float>()(0);\n    const float beta_max = context->input(11).flat<float>()(0);\n    const float gamma_min = context->input(13).flat<float>()(0);\n    const float gamma_max = context->input(14).flat<float>()(0);",
    "Added code": "    const auto& input_min_tensor = context->input(1);\n    OP_REQUIRES(context, input_min_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"input_min must have 1 element\"));\n    const float input_min = input_min_tensor.flat<float>()(0);\n    const auto& input_max_tensor = context->input(2);\n    OP_REQUIRES(context, input_max_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"input_max must have 1 element\"));\n    const float input_max = input_max_tensor.flat<float>()(0);\n    const auto& mean_min_tensor = context->input(4);\n    OP_REQUIRES(context, mean_min_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"mean_min must have 1 element\"));\n    const float mean_min = mean_min_tensor.flat<float>()(0);\n    const auto& mean_max_tensor = context->input(5);\n    OP_REQUIRES(context, mean_max_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"mean_max must have 1 element\"));\n    const float mean_max = mean_max_tensor.flat<float>()(0);\n    const auto& var_min_tensor = context->input(7);\n    OP_REQUIRES(context, var_min_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"var_min must have 1 element\"));\n    const float var_min = var_min_tensor.flat<float>()(0);\n    const auto& var_max_tensor = context->input(8);\n    OP_REQUIRES(context, var_max_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"var_max must have 1 element\"));\n    const float var_max = var_max_tensor.flat<float>()(0);\n    const auto& beta_min_tensor = context->input(10);\n    OP_REQUIRES(context, beta_min_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"beta_min must have 1 element\"));\n    const float beta_min = beta_min_tensor.flat<float>()(0);\n    const auto& beta_max_tensor = context->input(11);\n    OP_REQUIRES(context, beta_max_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"beta_max must have 1 element\"));\n    const float beta_max = beta_max_tensor.flat<float>()(0);\n    const auto& gamma_min_tensor = context->input(13);\n    OP_REQUIRES(context, gamma_min_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"gamma_min must have 1 element\"));\n    const float gamma_min = gamma_min_tensor.flat<float>()(0);\n    const auto& gamma_max_tensor = context->input(14);\n    OP_REQUIRES(context, gamma_max_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"gamma_max must have 1 element\"));\n    const float gamma_max = gamma_max_tensor.flat<float>()(0);\n    OP_REQUIRES(context, mean.NumElements() > 1,\n                errors::InvalidArgument(\"Must have at least a mean value\",\n                                        gamma.shape().DebugString()));\n    OP_REQUIRES(context, mean.NumElements() > 1,\n                errors::InvalidArgument(\"Must have at least a mean value\"));\n    const auto last_dim = input.shape().dims() - 1;\n    OP_REQUIRES(context,\n                mean.shape().dim_size(0) == input.shape().dim_size(last_dim),\n                errors::InvalidArgument(\"Must provide as many means as the \"\n                                        \"last dimension of the input tensor: \",\n                                        mean.shape().DebugString(), \" vs. \",\n                                        input.shape().DebugString()));\n    OP_REQUIRES(\n        context, mean.shape().dim_size(0) == var.shape().dim_size(0),\n        errors::InvalidArgument(\n            \"Mean and variance tensors must have the same shape: \",\n            mean.shape().DebugString(), \" vs. \", var.shape().DebugString()));\n    OP_REQUIRES(\n        context, mean.shape().dim_size(0) == beta.shape().dim_size(0),\n        errors::InvalidArgument(\n            \"Mean and beta tensors must have the same shape: \",\n            mean.shape().DebugString(), \" vs. \", beta.shape().DebugString()));\n    OP_REQUIRES(\n        context, mean.shape().dim_size(0) == gamma.shape().dim_size(0),\n        errors::InvalidArgument(\n            \"Mean and gamma tensors must have the same shape: \",\n            mean.shape().DebugString(), \" vs. \", gamma.shape().DebugString()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/80b65ab79bf8dd6ec03c570b59a1208bb27fec24",
    "Commit message": "Small fix to axis check for tfl.pack to tosa\n\nThere was an off-by-one error when checking the axis value\nbased on the input rank.\n\nPiperOrigin-RevId: 516334935",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  if ((axis < 0) || (axis > (input_tensor_rank + 1))) {",
    "Added code": "  if ((axis < 0) || (axis > input_tensor_rank)) {"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2ff14318050e26302785a49a1719d29ddcc91b4",
    "Commit message": "[XNNPACK] Fix incorrect check in slice node\n\nbegin+size == input dimension is valid, e.g. input size is 3, begin is 2, size is 1.\n\nPiperOrigin-RevId: 523713369",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "      if (begin[i] + size[i] >= input_shape->data[i]) {\n                                 \") must be less input \"",
    "Added code": "      if (begin[i] + size[i] > input_shape->data[i]) {\n                                 \") must not be greater than input \""
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d23458fdd2655c83ff9d54725062ded31b644ba4",
    "Commit message": "[XLA:CPU] Do not check that the size of the XLA parameter buffer is exactly equal to the size of the underlying given buffer\n\nInstead, check that the underlying allocation is \"large enough\". This is also\nmore consistent with XLA:GPU behavior.\n\nThe mismatch can happen when the input comes from tf.where, which is backed by\nan allocation larger than is actually required.\n\nProviding tests is a bit of a chicken-and-an-egg problem: this is tested in\nwhere ops tests, which are disabled due to this issue.\n\nPiperOrigin-RevId: 411923802\nChange-Id: Iee3da305b9692ff37bdecba058629419e3f495c3",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    CHECK_EQ(allocation.size(), out.size())",
    "Added code": "    CHECK_LE(allocation.size(), out.size())"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4863013a3ec5b97c042a38ab567bcc4a62ccde5c",
    "Commit message": "Add checking for number of inputs in GetOptionalInputTensor to avoid indexing out of array bounds.\n\nPiperOrigin-RevId: 285438822\nChange-Id: Icbb7d7c3ffbffc574ff54082107b2fbb3fa751e4",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "  const bool use_tensor = node->inputs->data[index] != kTfLiteOptionalTensor;",
    "Added code": "  const bool use_tensor = index < node->inputs->size &&\n                          node->inputs->data[index] != kTfLiteOptionalTensor;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1e38a0025c9a983bf3229299109b5b1781215c7e",
    "Commit message": "[XLA] CHECK that sparse indices are in range in MutableLiteralBase::AppendSparseElement.\n\nPreviously there was no range-checking on sparse elements' indices.\n\nPiperOrigin-RevId: 231662246",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "  // TODO(jlebar): CHECK that multi_index is in range?",
    "Added code": "  for (int64 i = 0; i < rank; ++i) {\n    CHECK_GE(multi_index[i], 0);\n    CHECK_LT(multi_index[i], subshape.dimensions(i));\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1610da3f992487bd9a8181d1e83cae99fe1e34d9",
    "Commit message": "add more sanity check on AvgPoolGrad op",
    "Deleted lines": 0,
    "Added lines": 8,
    "Changed lines": 8,
    "Deleted code": "",
    "Added code": "\n      OP_REQUIRES(\n          context, orig_input_dims_mkl_order[0] == diff_dst_dims[0],\n          errors::InvalidArgument(\n              \"Expected first dimension of orig_input and diff_dst to match, \"\n              \"got \",\n              orig_input_dims_mkl_order[0], \" and \", diff_dst_dims[0]));\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a68f57a24203fd49c4a5c4a8f51098d4415a93f8",
    "Commit message": "[XNNPACK] Add missing return when output channels do not match in TransposeConvolution\n\nAdd a check that input channels in the filter and tensor match.\n\nPiperOrigin-RevId: 448040780",
    "Deleted lines": 0,
    "Added lines": 9,
    "Changed lines": 9,
    "Deleted code": "",
    "Added code": "      return kTfLiteError;\n    }\n    if (input_channels != input_tensor_dims[3]) {\n      TF_LITE_MAYBE_KERNEL_LOG(\n          logging_context,\n          \"transpose convolution kernel input channel dimension (%d) \"\n          \"doesn't match filter input channel (%d) in node #%d\",\n          input_channels, input_tensor_dims[3]);\n      return kTfLiteError;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1b54cadd19391b60b6fcccd8d076426f7221d5e8",
    "Commit message": "Add missing validation to sparse dense cwise ops.\n\nPiperOrigin-RevId: 415543133\nChange-Id: I5baf3284e919338afb96178c468ad3d3cb0d956c",
    "Deleted lines": 0,
    "Added lines": 13,
    "Changed lines": 13,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(shape_t->shape()),\n        errors::InvalidArgument(\"Input sp_shape must be a vector. Got: \",\n                                shape_t->shape().DebugString()));\n    OP_REQUIRES(\n        ctx, shape_t->shape().dim_size(0) == indices_t->shape().dim_size(1),\n        errors::InvalidArgument(\n            \"Number of dimensions must match second dimension of indices. \",\n            \"Got \", shape_t->shape().dim_size(0),\n            \" dimensions, indices shape: \", indices_t->shape().DebugString()));\n    OP_REQUIRES(ctx, shape_t->NumElements() > 0,\n                errors::InvalidArgument(\n                    \"The shape argument requires at least one element.\"));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b51b82fe65ebace4475e3c54eb089c18a4403f1c",
    "Commit message": "Add missing validation to `AddManySparseToTensorsMap`.\n\nSparse tensors have a set of requirements for the 3 components and not all of them were checked.\n\nPiperOrigin-RevId: 415358027\nChange-Id: I96cbb672999cd1da772c22fabbd15507e32e12dc",
    "Deleted lines": 2,
    "Added lines": 15,
    "Changed lines": 17,
    "Deleted code": "\n",
    "Added code": "    OP_REQUIRES(\n        context,\n        input_values->shape().dim_size(0) == input_indices->shape().dim_size(0),\n        errors::InvalidArgument(\n            \"Number of values must match first dimension of indices. \", \"Got \",\n            input_values->shape().dim_size(0),\n            \" values, indices shape: \", input_indices->shape().DebugString()));\n    OP_REQUIRES(\n        context,\n        input_shape->shape().dim_size(0) == input_indices->shape().dim_size(1),\n        errors::InvalidArgument(\n            \"Number of dimensions must match second dimension of indices. \",\n            \"Got \", input_shape->shape().dim_size(0),\n            \" dimensions, indices shape: \",\n            input_indices->shape().DebugString()));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e560136d757867482a93be74e108ef516920bcfc",
    "Commit message": "Fix wrong output of tf.stack with 0-dimension tensor\n\nThis PR tries to address the issue raised in 53300 where\ntf.stack will silently output wrong result with 0-dimension tensor.\nThe issue was that the shape check was skipped when num of output elements\nwas zero. This PR fixed the issue.\n\nThis PR fixes 53300.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 18,
    "Added lines": 18,
    "Changed lines": 36,
    "Deleted code": "      auto output_flat =\n          output->shaped<T, 2>({before_dim, after_dim * axis_dim});\n\n      // Except for shapes, pack is a special case of concat, so we reuse the\n      // same computational kernels.\n      ConstMatrixVector inputs_flat;\n      inputs_flat.reserve(num);\n      for (int i = 0; i < num; ++i) {\n        const Tensor& input = c->input(i);\n        OP_REQUIRES(c, first_input.shape().IsSameSize(input.shape()),\n                    errors::InvalidArgument(\n                        \"Shapes of all inputs must match: values[0].shape = \",\n                        first_input.shape().DebugString(), \" != values[\", i,\n                        \"].shape = \", input.shape().DebugString()));\n\n        inputs_flat.emplace_back(new typename TTypes<T, 2>::ConstMatrix(\n            input.shaped<T, 2>({before_dim, after_dim})));\n      }",
    "Added code": "    auto output_flat =\n        output->shaped<T, 2>({before_dim, after_dim * axis_dim});\n\n    // Except for shapes, pack is a special case of concat, so we reuse the\n    // same computational kernels.\n    ConstMatrixVector inputs_flat;\n    inputs_flat.reserve(num);\n    for (int i = 0; i < num; ++i) {\n      const Tensor& input = c->input(i);\n      OP_REQUIRES(c, first_input.shape().IsSameSize(input.shape()),\n                  errors::InvalidArgument(\n                      \"Shapes of all inputs must match: values[0].shape = \",\n                      first_input.shape().DebugString(), \" != values[\", i,\n                      \"].shape = \", input.shape().DebugString()));\n\n      inputs_flat.emplace_back(new typename TTypes<T, 2>::ConstMatrix(\n          input.shaped<T, 2>({before_dim, after_dim})));\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2b7100d6cdff36aa21010a82269bc05a6d1cc74a",
    "Commit message": "Cleanup and remove duplicate validation in `SparseCount`.\n\nWe have valdiation that is duplicated, checking different conditions, in different formats and failing to capture all cases. This should fix all the previous bugs.\n\nPiperOrigin-RevId: 414886981\nChange-Id: Ibf0bba0beb057b76d505324bb9487565daf95f01",
    "Deleted lines": 27,
    "Added lines": 21,
    "Changed lines": 48,
    "Deleted code": "    OP_REQUIRES(context, shape.NumElements() != 0,\n                errors::InvalidArgument(\n                    \"The shape argument requires at least one element.\"));\n\n    for (int b = 0; b < shape_vector.size(); b++) {\n      OP_REQUIRES(context, shape_vector(b) >= 0,\n                  errors::InvalidArgument(\n                      \"Elements in dense_shape must be >= 0. Instead got:\",\n                      shape.DebugString()));\n    }\n\n    OP_REQUIRES(context, num_values == indices.shape().dim_size(0),\n                errors::InvalidArgument(\n                    \"Number of values must match first dimension of indices.\",\n                    \"Got \", num_values,\n                    \" values, indices shape: \", indices.shape().DebugString()));\n\n    OP_REQUIRES(context, num_values <= indices.shape().dim_size(0),\n                errors::InvalidArgument(\n                    \"The first dimension of indices must be equal to or \"\n                    \"greather than number of values. ( \",\n                    indices.shape().dim_size(0), \" vs. \", num_values, \" )\"));\n    OP_REQUIRES(context, indices.shape().dim_size(1) > 0,\n                errors::InvalidArgument(\"The second dimension of indices must \"\n                                        \"be greater than 0. Received: \",\n                                        indices.shape().dim_size(1)));\n",
    "Added code": "    OP_REQUIRES(context, TensorShapeUtils::IsVector(values.shape()),\n                errors::InvalidArgument(\"Input values must be a vector. Got: \",\n                                        values.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(shape.shape()),\n                errors::InvalidArgument(\"Input shape must be a vector. Got: \",\n                                        shape.shape().DebugString()));\n    OP_REQUIRES(context,\n                values.shape().dim_size(0) == indices.shape().dim_size(0),\n                errors::InvalidArgument(\n                    \"Number of values must match first dimension of indices.\",\n                    \"Got \", values.shape().dim_size(0),\n                    \" values, indices shape: \", indices.shape().DebugString()));\n    OP_REQUIRES(\n        context, shape.shape().dim_size(0) == indices.shape().dim_size(1),\n        errors::InvalidArgument(\n            \"Number of dimensions must match second dimension of indices.\",\n            \"Got \", shape.shape().dim_size(0),\n            \" dimensions, indices shape: \", indices.shape().DebugString()));\n    OP_REQUIRES(context, shape.NumElements() > 0,\n                errors::InvalidArgument(\n                    \"The shape argument requires at least one element.\"));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/23968a8bf65b009120c43b5ebcceaf52dbc9e943",
    "Commit message": "Fix out of bound access in DequantizeOp by adding check for axis < input dimension\n\nPiperOrigin-RevId: 411214268\nChange-Id: I3249d2a69ddc82f182c589a3a5bbfb71543f4b29",
    "Deleted lines": 0,
    "Added lines": 5,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(\n        ctx, axis_ < input.dims(),\n        errors::InvalidArgument(\"Axis must be less than input dimension(\",\n                                input.dims(), \"), got \", axis_));\n"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4923de56ec94fff7770df259ab7f2288a74feb41",
    "Commit message": "Don't do any work when reshaping 0 elements sparse tensor.\n\nIf reshaping to 0 elements tensor, check that input has no elements.\nIf reshaping no elements input, check that output has no elements.\n\nPiperOrigin-RevId: 388296986\nChange-Id: Iadc9fe7252e14313ca987e69bf0d7042fd10232a",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "    OP_REQUIRES(\n        context, dense_size > 0 && product > 0,\n        errors::InvalidArgument(\n            \"Input tensor has \", nnz, \" non zero elements but input shape (\",\n            input_shape.DebugString(), \") or output shape (\",\n            output_shape.DebugString(), \") is empty\"));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/467730fe90282a75f15f67d701b278e86cfad65e",
    "Commit message": "Fix dimension check for tf.keras.losses.BinaryCrossentropy\n\nThis fix tries to address the issue raised in 30040 where\ntf.keras.losses.BinaryCrossentropy does not check the dimension match:\n```\nimport numpy as np\nimport tensorflow as tf\n\ny_true = np.array([[1.], [1.], [1.], [0.], [1.], [0.], [0.], [1.], [1.], [0.]]).astype(np.float32)\ny_pred = np.array([[0.], [0.], [0.], [1.], [1.], [0.], [0.], [1.], [0.], [1.]]).astype(np.float32)\nbce = tf.keras.losses.BinaryCrossentropy()\nprint(bce(np.squeeze(y_true), y_pred).numpy()) # should fail\n```\nThe reason was that broadcasting was applied directly.\nThis fix adds dimension check to throw an error if there is a mismatch.\n\nThis fix fixes 30040.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 0,
    "Added lines": 6,
    "Changed lines": 6,
    "Deleted code": "",
    "Added code": "      try:\n        target.get_shape().merge_with(output.get_shape())\n      except ValueError:\n        raise ValueError(\n            \"target and output must have the same shape (%s vs %s)\" %\n            (target.get_shape(), output.get_shape()))"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2cec131f107fde9c54f48a9b74248617d850549",
    "Commit message": "Add the shape and dtype validation for TensorDatasetOp",
    "Deleted lines": 3,
    "Added lines": 27,
    "Changed lines": 30,
    "Deleted code": "  explicit TensorDatasetOp(OpKernelConstruction* ctx) : DatasetOpKernel(ctx) {}\n    // TODO(mrry): Validate that the shapes of the \"components\" tensors match\n    // the \"shapes\" attr.;",
    "Added code": "  explicit TensorDatasetOp(OpKernelConstruction* ctx) : DatasetOpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"Toutput_types\", &output_types_));\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"output_shapes\", &output_shapes_));\n  }\n    OP_REQUIRES(ctx, components.size() == output_shapes_.size(),\n                errors::InvalidArgument(\n                    \"The size of components should be same with that of \"\n                    \"output_shapes, but got \",\n                    components.size(), \" vs. \", output_shapes_.size()));\n    OP_REQUIRES(ctx, components.size() == output_types_.size(),\n                errors::InvalidArgument(\n                    \"The size of components should be same with that of \"\n                    \"Toutput_types, but got \",\n                    components.size(), \" vs. \", output_types_.size()));\n\n    for (int i = 0; i < components.size(); ++i) {\n      OP_REQUIRES(ctx, components[i].dtype() == output_types_[i],\n                  errors::InvalidArgument(\"The dtypes of components should be \"\n                                          \"same with Toutput_types\"));\n      OP_REQUIRES(ctx, output_shapes_[i].IsIdenticalTo(components[i].shape()),\n                  errors::InvalidArgument(\"The shapes of components should be \"\n                                          \"same with output_shapes\"));\n    }\n\n\n  DataTypeVector output_types_;\n  std::vector<PartialTensorShape> output_shapes_;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7e2d53c1c371f38c7f0ef13c1c06336b22a195c0",
    "Commit message": "[tf.data] Adds the expected check for better debugging.\n\nPiperOrigin-RevId: 227733165",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "        DCHECK(state_and_output.size() <=\n               dataset()->state_types_.size() + output_dtypes().size());"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a12b8c4afdca3ac2945d62b3b83ca2599ab360f9",
    "Commit message": "[xla] Improve validation of Broadcast shape\n\nIf one misreads the semantics of this instruction, it's easy to cause\nan out of bounds access into the dimensions here. Add an extra check\nto return a proper error to the user rather than crashing in that\ncase.\n\nRef #22130",
    "Deleted lines": 2,
    "Added lines": 3,
    "Changed lines": 5,
    "Deleted code": "    TF_RET_CHECK(broadcast->shape().dimensions(output_dimension) ==\n                 operand_shape.dimensions(operand_dimension))",
    "Added code": "    TF_RET_CHECK((output_dimension < ShapeUtil::Rank(broadcast->shape())) &&\n                 (broadcast->shape().dimensions(output_dimension) ==\n                 operand_shape.dimensions(operand_dimension)))"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/32dc203f55a7462ddf780c68d619af574daedd46",
    "Commit message": "Improve gradient shape validation errors.\n\nPiperOrigin-RevId: 171077826",
    "Deleted lines": 2,
    "Added lines": 10,
    "Changed lines": 12,
    "Deleted code": "        for t_in, in_grad in zip(op.inputs, in_grads):\n              in_grad.set_shape(t_in.get_shape())",
    "Added code": "        for i, (t_in, in_grad) in enumerate(zip(op.inputs, in_grads)):\n              try:\n                in_grad.set_shape(t_in.get_shape())\n              except ValueError:\n                raise ValueError(\n                    \"Incompatible shapes between op input and calculated \"\n                    \"input gradient.  Forward operation: %s.  Input index: %d. \"\n                    \"Original input shape: %s.  \"\n                    \"Calculated input gradient shape: %s\"\n                    % (op.name, i, t_in.shape, in_grad.shape))"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/25821f0d91623d654bb1bdd62423e644bae9f7f8",
    "Commit message": "TensorFlow: Fix OP_REQUIRES check for depthwise pooling.\nChange: 128843893",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "      OP_REQUIRES(context, params.out_depth % params.depth_window > 0,\n          context, params.out_depth == params.depth_stride,",
    "Added code": "      OP_REQUIRES(context, params.depth % params.depth_window == 0,\n          context, params.depth_window == params.depth_stride,"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/05ec322172958f6e67e4bcaef4681e6aa54fabeb",
    "Commit message": "Return error message with illegal input rather than check-failing in op_kernel.\n\nPiperOrigin-RevId: 213653853",
    "Deleted lines": 0,
    "Added lines": 2,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "        TF_RET_CHECK(kernel->outputs[i].input_index >= 0)\n            << \"Invalid input for outputs \" << i;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6364463d6f5b6254cac3d6aedf999b6a96225038",
    "Commit message": "[lite] Add some safety checks to avoid out of bound access for sparsity format\n\nPiperOrigin-RevId: 416910386\nChange-Id: Ic0b4dc048dc4b5a6309c572b8c4c9f776e4db60a",
    "Deleted lines": 7,
    "Added lines": 11,
    "Changed lines": 18,
    "Deleted code": "      int orig_dim = traversal_order_[original_rank + block_dim];\n      block_size_[block_dim] = dense_size[orig_dim];\n      blocked_shape_[i] = dense_shape_[i] / dense_size[orig_dim];\n      block_dim++;\n  } else {\n      indices[level] = array_indices[i];\n      Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);",
    "Added code": "      if (original_rank + block_dim < traversal_order_.size()) {\n        int orig_dim = traversal_order_[original_rank + block_dim];\n        block_size_[block_dim] = dense_size[orig_dim];\n        blocked_shape_[i] = dense_shape_[i] / dense_size[orig_dim];\n        block_dim++;\n      }\n  } else if (prev_idx + 1 < dim_metadata_[metadata_idx].size()) {\n      if (i < array_indices.size() && level < indices.size()) {\n        indices[level] = array_indices[i];\n        Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/62cb54f2caf48480dc6b3c1ce9629eaac4688f83",
    "Commit message": "Set 2nd output shape for SparseSegmentReduceGradV2\n\n- Fixes a debug check failure.",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "  if (outputs_unique_indices) {\n    c->set_output(1, c->Vector(InferenceContext::kUnknownDim));\n  }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8f66dd24d1355298482afee9b5299f9f0b5b1817",
    "Commit message": "Add checks to TensorForest that help with debugging when labels are wrong.\n\nPiperOrigin-RevId: 161657633",
    "Deleted lines": 0,
    "Added lines": 8,
    "Changed lines": 8,
    "Deleted code": "",
    "Added code": "  QCHECK_LT(int_label, params_.num_outputs())\n      << \"Got label greater than indicated number of classes. Is \"\n         \"params.num_classes set correctly?\";\n  QCHECK_GE(int_label, 0);\n  QCHECK_LT(int_label, params_.num_outputs())\n      << \"Got label greater than indicated number of classes. Is \"\n         \"params.num_classes set correctly?\";\n  QCHECK_GE(int_label, 0);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9b0f99ddd27e7738732a154be5469391ee8fc977",
    "Commit message": "Add check to ensure element sizes are the same",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "",
    "Added code": "  TFLITE_DCHECK_EQ(input1_shape.FlatSize(), input2_shape.FlatSize());"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f8ec0f101bac066faa2e917ac714ca9eea310eac",
    "Commit message": "adding checks that pad fusion works only Conv2D",
    "Deleted lines": 1,
    "Added lines": 4,
    "Changed lines": 5,
    "Deleted code": "",
    "Added code": "      if(!isConv2D){\n        OP_REQUIRES(context, padEnabled,\n                errors::InvalidArgument(\"Pad+Conv fusion only works for 2D\"));\n      }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/76619c8dea0e480fd48e3b4dcfe0249eb24216b8",
    "Commit message": "Validation in shape functions of Dataset ops (#18680)\n\n* Add shape check for PrependFromQueueAndPaddedBatchDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add comment for shape check\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for FixedLengthRecordDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add check for filenames as well\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Clang-format -i --style=google for file format\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for SqlDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 3,
    "Added lines": 26,
    "Changed lines": 29,
    "Deleted code": "    .SetShapeFn(shape_inference::ScalarShape);\n    .SetShapeFn(shape_inference::ScalarShape);\n    .SetShapeFn(shape_inference::ScalarShape);",
    "Added code": "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // driver_name, data_source_name, and query should be scalars.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // `filenames` must be a scalar or a vector.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n      // header_bytes, record_bytes, footer_bytes, buffer_size should be\n      // scalars.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // batch_size should be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b71b6b8ca9ade8b39d77f0373210fe58dfccf4f4",
    "Commit message": "Shape validation with random/shuffle related Dataset ops (#18682)\n\n* Add shape check for CacheDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for ShuffleAndRepeatDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add check for ShuffleDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for RandomDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add RangeDataset shape check\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Sanitize with clang-format -i --style=Google\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Deleted lines": 5,
    "Added lines": 38,
    "Changed lines": 43,
    "Deleted code": "    .SetShapeFn(shape_inference::ScalarShape);\n    .SetShapeFn(shape_inference::ScalarShape);\n    .SetShapeFn(shape_inference::ScalarShape);\n    .SetShapeFn(shape_inference::ScalarShape);\n    .SetShapeFn(shape_inference::ScalarShape);",
    "Added code": "    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // start, stop, and step should be scalars.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // buffer_size, seed, and seed2 should be scalars.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // buffer_size, seed, and seed2 should be scalars.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // buffer_size, seed, seed2, and count should be scalars.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      shape_inference::ShapeHandle unused;\n      // filename should be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      return shape_inference::ScalarShape(c);\n    });"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9718fed7b9aba244359b3d38c2a1dc20e50428bd",
    "Commit message": "Added size check to avoid memory corruption in GraphDefImporter::ConvertNodeDef.\n\nPiperOrigin-RevId: 601277773",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "\n  if (op_def->output_arg_size() < 0)\n    return InvalidArgument(\"Node \", node.name(), \" output arg size < 0\");"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4ab6a520c94441622442747aef620939cc1d8130",
    "Commit message": "Relax the check for state_size\n\nThe behaviour of `hasattr` is to evaluate the state_size member. In the case of `tfa.seq2seq.AttentionWrapper`, that is a @property member that is built at graph runtime after calling `setup_memory`, thus `hasattr` returns an error when using AttentionWrapper with dynamic memories.\r\n\r\nMore details: https://github.com/tensorflow/addons/issues/680",
    "Deleted lines": 2,
    "Added lines": 2,
    "Changed lines": 4,
    "Deleted code": "      if not hasattr(cell, 'state_size'):\n    if not hasattr(cell, 'state_size'):",
    "Added code": "      if not ('state_size' in dir(cell) or hasattr(cell, 'state_size')):\n    if not ('state_size' in dir(cell) or hasattr(cell, 'state_size')):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/edd9fb416e04b8ca4398c4eea65f14dc6704a44a",
    "Commit message": "TfLiteTensorCopy returns an error status when src and dest bytes are not equal. So we don't need to check them specifically if we ensure the status of the call to copy (which we should do anyways).\n\nPiperOrigin-RevId: 522647125",
    "Deleted lines": 2,
    "Added lines": 1,
    "Changed lines": 3,
    "Deleted code": "    TF_LITE_ENSURE_EQ(context, src_tensor->bytes, dst_tensor->bytes);\n    TfLiteTensorCopy(src_tensor, dst_tensor);",
    "Added code": "    TF_LITE_ENSURE_OK(context, TfLiteTensorCopy(src_tensor, dst_tensor));"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e44f8a08051baa58bde9130a844a1b82a8179526",
    "Commit message": "check hasattr on the type, not the instance.\n\nhasattr on the instance triggers __getattr__ which carries very undesirable\neffects, such as running Ops on a donated buffer.\n\nLong term, we may want to audit all uses of hasattr on TensorFlow instances\nthat overrides __getattr__ in nontrival (e.g. running tf Ops) ways. They will\nalmost always cause trouble here and there because TensorFlow is quite far\nfrom being able guarantee if an Op returns or consumes is actually valid in all cases. Things will improve give it time, but if we can avoid such strong assumptions the system tend to get more robust.\n\nPiperOrigin-RevId: 578261984",
    "Deleted lines": 7,
    "Added lines": 7,
    "Changed lines": 14,
    "Deleted code": "    # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n    # object map. Also add TPUEmbedding to separate list for special handling\n    # with values copy.\n      if hasattr(t, _TPU_EMBEDDING_ATTR):\n    if not hasattr(\n        tpu_embedding, _TPU_EMBEDDING_ATTR\n    ) or not callable(tpu_embedding._create_copy_for_async_checkpoint):  # pylint: disable=protected-access",
    "Added code": "      # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n      # object map. Also add TPUEmbedding to separate list for special handling\n      # with values copy.\n      if hasattr(type(t), _TPU_EMBEDDING_ATTR):\n    if not hasattr(type(tpu_embedding), _TPU_EMBEDDING_ATTR) or not callable(\n        tpu_embedding._create_copy_for_async_checkpoint  # pylint: disable=protected-access\n    ):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/50299228e5df92b486548ee1cb856e79de69ad43",
    "Commit message": "PR #7269: Fix an incorrect static_assert\n\nImported from GitHub PR https://github.com/openxla/xla/pull/7269\n\nThe size of a uint8_t is 1 so a static_assert\nto check that it is 0 makes no sense, fix it.\nAlso fix a couple of warnings about lack of\ntypename\nCopybara import of the project:\n\n--\ne72164f0a16ec5e548927ca3583d1a31edbd95d8 by Andrew Goodbody <andrew.goodbody@linaro.org>:\n\nFix an incorrect static_assert\n\nThe size of a uint8_t is 1 so a static_assert\nto check that it is 0 will always be false.\nThe original intent was to have the assert only\ntrigger if the struct was instantiated but the\nstandard deems it ill formed if it can never be\ntrue and allows compilers to reject it. Adopt a\ndifferent workaround that avoids this by allowing\nthe possibility of an evaluation to true.\nAlso fix a couple of warnings about lack of\ntypename\n\nMerging this change closes #7269\n\nPiperOrigin-RevId: 585767871",
    "Deleted lines": 3,
    "Added lines": 9,
    "Changed lines": 12,
    "Deleted code": "  static_assert(sizeof(dtype) == 0, \"unsupported data type\");\n  internal::PtrType<dtype>::Type* data;\n    auto* data = static_cast<internal::PtrType<dtype>::Type*>(buf->data);",
    "Added code": "// A workaround for the fact that a static_assertion can be evaluated\n// whether or not the template is instantiated\ntemplate <DataType dtype>\nstruct always_false : std::false_type {};\n\n  static_assert(always_false<dtype>::value, \"unsupported data type\");\n  typename internal::PtrType<dtype>::Type* data;\n    auto* data =\n        static_cast<typename internal::PtrType<dtype>::Type*>(buf->data);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/51d72a7d7f74784b68916819edd04e890b36f957",
    "Commit message": "Modified \"_check_is_tensor_or_operation\" to check if \"x\" is \"tensor_like\"",
    "Deleted lines": 1,
    "Added lines": 2,
    "Changed lines": 3,
    "Deleted code": "  if not (isinstance(x, ops.Operation) or isinstance(x, ops.Tensor)):",
    "Added code": "from tensorflow.python.framework import tensor_util\n  if not (isinstance(x, ops.Operation) or tensor_util.is_tensor(x)):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8a2e7deb21f02e4072d6b62cf7f447b9264afe01",
    "Commit message": "Adjust checks for `type(Tensor)` to isinstance or is_eager/is_symbolic_tensor.\n\nPiperOrigin-RevId: 525801792",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "  if tensors_type is ops.Tensor:",
    "Added code": "  if isinstance(tensors, ops.Tensor):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b68b869e75916e6de37c2ca23a93643faf333011",
    "Commit message": "Fix invalid keras tensor isinstance check\n\nPiperOrigin-RevId: 315553346\nChange-Id: I120234e58cb0fb9dce007e7739639519719a9764",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "        if not isinstance(input_tensor, keras_tensor.keras_tensors_enabled()):",
    "Added code": "        if not isinstance(input_tensor, keras_tensor.KerasTensor):"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/040aaf39aebda57921991d05d29be5123e908d7c",
    "Commit message": "Don't check that bool arrays are quantized.\n\nPiperOrigin-RevId: 196541955",
    "Deleted lines": 4,
    "Added lines": 10,
    "Changed lines": 14,
    "Deleted code": "      CHECK(array.final_data_type == array.data_type)\n          << \"\\\" has mis-matching actual and final data types (\"\n          << ArrayDataTypeName(array.data_type) << \",\"\n          << ArrayDataTypeName(array.final_data_type) << \").\";",
    "Added code": "    if (array.data_type == ArrayDataType::kBool) {\n      // Boolean values are never quantized.\n      continue;\n    }\n\n      CHECK(array.data_type == array.final_data_type)\n          << \"\\\" has mis-matching actual and final data types (data_type=\"\n          << ArrayDataTypeName(array.data_type)\n          << \", final_data_type=\" << ArrayDataTypeName(array.final_data_type)\n          << \").\";"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a0de0ca6a39f3037e1be6ec740829863bcda3e8",
    "Commit message": "[XLA:GPU] Fix type check in IsMatrixMultiplication\n\nPiperOrigin-RevId: 513638308",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "       lhs_shape.element_type() == S8);",
    "Added code": "       rhs_shape.element_type() == S8);"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/db9b247cd1f3ff046359f7b64ca60c2d697fe2e1",
    "Commit message": "Fix the functional model loading with nested sequential model.\n\nThe nested sequential model is created with _is_graph_network = False, the current instance check is not strong enough.\n\nPiperOrigin-RevId: 311454248\nChange-Id: I3b36cc037474587c134eab567d42694129c5cf52",
    "Deleted lines": 1,
    "Added lines": 3,
    "Changed lines": 4,
    "Deleted code": "  return isinstance(layer, Functional)",
    "Added code": "  # For a sequential model, it is first created with _is_graph_network = False,\n  # we have to keep the _is_graph_network check here.\n  return isinstance(layer, Functional) and layer._is_graph_network"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a4b6b6bcc7a813162bf0378727950e321aca19c",
    "Commit message": "Add stricter type checking for tf.math.real (using is_numeric)",
    "Deleted lines": 1,
    "Added lines": 1,
    "Changed lines": 2,
    "Deleted code": "    elif tf.debugging.is_numeric_tensor(input):",
    "Added code": "    elif input.dtype.is_numeric:"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8a0fee00855a0e806bd5c9cc1ad6c0175a985922",
    "Commit message": "[XLA] Don't use isnan on values which can't have NaN.\n\nWhile we are here, don't upcast to double just to check if something is NaN.\n\nPiperOrigin-RevId: 558885664",
    "Deleted lines": 5,
    "Added lines": 7,
    "Changed lines": 12,
    "Deleted code": "  // msvc can't compile std::isnan(a) where `a` is uint8_t.  This is a bug\n  // according to https://en.cppreference.com/w/cpp/numeric/math/isnan, but it's\n  // easy to work around.\n  return a == b || (std::isnan(static_cast<double>(a)) &&\n                    std::isnan(static_cast<double>(b)));",
    "Added code": "  if constexpr (std::numeric_limits<NativeT>::has_quiet_NaN ||\n                std::numeric_limits<NativeT>::has_signaling_NaN) {\n    if (Eigen::numext::isnan(a) && Eigen::numext::isnan(b)) {\n      return true;\n    }\n  }\n  return a == b;"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/580140611a47413dcf6373deb1250c0ed605e873",
    "Commit message": "[XLA] Do not check fail in proto copy from if the backend config proto and desired proto type do not match.\n\nPiperOrigin-RevId: 450516623",
    "Deleted lines": 2,
    "Added lines": 4,
    "Changed lines": 6,
    "Deleted code": "    proto->CopyFrom(*proto_ptr);\n    return Status::OK();",
    "Added code": "    if (proto_ptr->GetDescriptor() == proto->GetDescriptor()) {\n      proto->CopyFrom(*proto_ptr);\n      return Status::OK();\n    }"
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d3d1cd3ad2becac5c31387f7fc483af65c7c8c84",
    "Commit message": "Fixes the crashes caused by the refcount checks for non-copyable types. For async value refs, refcounts don't equal to number of waiters as waiters can make copies of the async value refs.\n\nPiperOrigin-RevId: 549967017",
    "Deleted lines": 1,
    "Added lines": 5,
    "Changed lines": 6,
    "Deleted code": "      DCHECK_EQ(promise.value()->NumRef(), 1);",
    "Added code": "      // For non-copyable types, we have no ways to check the number of waiters\n      // but we have to move the data into the consumer callback. Registering\n      // two callbacks will lead to double-move of the data. It is users'\n      // responsibility to make sure only one waiter is registered.\n      // TODO(yunlongl): Implement `PjRtUniqueFuture`."
},
{
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/02907e867c74651a9eb74971f56559d5db2efa1c",
    "Commit message": "Use Nano seconds in Timestamp check as Pico seconds can lead to overflow.\n\nPiperOrigin-RevId: 444646715",
    "Deleted lines": 0,
    "Added lines": 3,
    "Changed lines": 3,
    "Deleted code": "",
    "Added code": "\n  int64_t EndTimestampNs() const { return TimestampNs() + DurationNs(); }\n"
},
