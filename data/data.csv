Library,Commit,bug report,Date,PR/Issue,Violation,Root Cause,Impact,Action,condition,Fixing pattern,Fixing element,,,
pytorch,https://github.com/pytorch/pytorch/commit/603097be18824a33069addec7b8f14ba5c3bc67a,"For OneDNN MaxPooling training, it will save indices as a workspace for backward, but for inference, indices are not necessary",2021-03-16 0:53:05,1,missing,gradient status,performance bug,add,if checker,tensor is tracking gradient,tensor indices,,,
pytorch,https://github.com/pytorch/pytorch/commit/0a7eef9bcf6d1f8b5531102342ffc21f24beb58d,[BE] Remove stale CUDA version check from cpp_extension.py. As at least CUDA-11.x is needed to build PyTorch on latest trunk. But still skip `--generate-dependencies-with-compile` if running on ROCm,2023-11-10 19:20:08,1,unnecessary,device version,version incompatibility,remove,if checker,device version is valid,cuda version,,,
pytorch,https://github.com/pytorch/pytorch/commit/ae2c219de2fd032036aa1d2a04101f1c23fd5bbe,"Revert ""[BE] Remove stale CUDA version check from cpp_extension.py",2023-11-10 15:46:13,1,unnecessary,device version,version incompatibility,remove,if checker,device version is valid,cuda version,,,
pytorch,https://github.com/pytorch/pytorch/commit/7ccca60927cdccde63d6a1d40480950f24e9877a, [BE] Remove stale CUDA version check from cpp_extension.py. As at least CUDA-11.x is needed to build PyTorch on latest trunk,2023-11-10 13:54:19,1,unnecessary,device version,version incompatibility,remove,if checker,device version is valid,cuda version,,,
pytorch,https://github.com/pytorch/pytorch/commit/93cea394dee1315c7a85ead7bb7af21363157c4f,Currently we compare `CUDA_INCLUDE_DIRS` and expect exact equality with `CUDAToolkit_INCLUDE_DIR` however this fails in the presense of symbolic links or for split installs where there are multiple include paths.,2023-11-07 21:51:18,3,improper,device version,version incompatibility,update,if checker,device version is valid,cuda version,,,
pytorch,https://github.com/pytorch/pytorch/commit/6b4c686b9a33a1503a4a4133f9067dd31e0822f7,"Forward fix a performance regression caused by #110510. When a model is run once, all those kernel pointers are initialized and removing the if-nullptr check will cause those loadKernel be unnecessarily executed again when we rerun the foward function.",2023-10-08 0:06:44,2,improper,device version,version incompatibility,update,if checker,device version is valid,cuda version,,,
pytorch,https://github.com/pytorch/pytorch/commit/45296f87ec865a7a500a6fd98353035c040d0cb7,"Prior to this change ROCm was not exiting check_cuda, causing an exception at packaging.version.parse(torch.version.cuda),",2023-03-20 23:19:31,1,insufficient,device version,version incompatibility,extend,if checker,device version is valid,device version,,,
pytorch,https://github.com/pytorch/pytorch/commit/bede7d999523d02e636a8981c0dff233b67f1a62,It does not check if `kind` variable fits in array of pointer called `names`,2020-04-16 11:58:03,1,improper,edge cases,crash,update,macro checker,others,vector,,,
pytorch,https://github.com/pytorch/pytorch/commit/f6639359357452de8bfc691430396ded98ea399c,"`TORCH_CHECK(i<UINT32_MAX)` is always false, it should be `TORCH_CHECK(iterShape[i] < UINT32_MAX)`",2024-01-07 23:55:35,1,improper,edge cases,runtime error,update,checker api,tensor size is valid,tensor data type,,,
pytorch,https://github.com/pytorch/pytorch/commit/cf732053e4f6b93b0a93006613552cd97f415b80," Today if we're accessing out of bound embedding rows, it'll either go through or throw IMA. This is not ideal - adding bound checks. This will probably slow things down - need to benchmark it.",2023-03-16 22:01:43,,missing,edge cases,performance bug,add,checker api,tensor size is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/175ccfc4c8443bcc65c87d9c942272d3ebf16b0b,flatbuffer module fields are not initialized,2023-09-21 19:19:17,1,missing,null pointer dereference,segmentation fault,add,macro checker,others,module buffer,,,
pytorch,https://github.com/pytorch/pytorch/commit/bde7b81f34925491fbcbb9e355697eb594e36923,"Back out ""[PyTorch] Don't do extra numel() check in TensorImpl::data()",2023-05-08 17:56:44,,improper,null pointer dereference,incorrect functionality,update,if checker,object is not null,,,,
pytorch,https://github.com/pytorch/pytorch/commit/2e224d62b6afecc78d885d0a4e160354950f6424,"We have environment variable USE_CUDNN with self-explanatory name. However cpp code is compiled based on cpp macro definition AT_CUDNN_ENABLED, even if USE_CUDNN is set to 0, cpp is compiled with cuDNN if cmake finds cuDNN in the system.",2019-08-27 21:43:11,,missing,device availability,incorrect functionality,add,if checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/30e1c74dc19ae2b622b46ebcdb7972c42775ac80,Update cuda amp to also check xla device ,2021-08-18 9:44:10,,improper,device availability,incorrect functionality,update,if checker,device is available,xla,,,
pytorch,https://github.com/pytorch/pytorch/commit/5a63c452e638dad8e077887ad8d2c94ff0e23917,This is because there are some hard-to-detect edge cases that will throw exceptions with cudnn 8.0.5 on Nvidia A40 GPU.,2021-01-08 0:20:21,,missing,device availability,crash,add,if checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/611080a118fff166c85f3200d860f3b059abac6f,uda 11.0.x doesn't support sm86.,2020-11-06 13:34:25,2,improper,device version,version incompatibility,update,if checker,device version is valid,CUDA,,,
pytorch,https://github.com/pytorch/pytorch/commit/563bbeb8905f4cea0bc5353dc12518c61113128e,undefined CUDA_VERSION warning,2020-05-05 19:31:24,1,insufficient,device version,version incompatibility,extend,if checker,device version is valid,CUDA,,,
pytorch,https://github.com/pytorch/pytorch/commit/5b51ca6808191e9f3dcea1d43fa731488cc688bb,Update CUDA compiler matrix. Switch GCC/Clang max versions to be exclusive as the `include/crt/host_config.h` checks the major version only for the upper bound. This allows to be less restrictive and match the checks in the aforementioned header. Also update the versions using that header in the CUDA SDKs.,2022-11-22 22:07:22,,improper,device version,version incompatibility,update,if checker,device version is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/cafd0f33042f5344a27ccde33b352eab676a0bdd,"Fix array index checking in mobile interpreter. Stop using non-portable out-of-range indexing in mobile interpreter, also change code types indexing to use vector.at() to catch out-of-range bugs earlier.",2022-02-24 14:39:32,1,improper,out of bound access,crash,update,checker api,tensor indexing is within range,array access,,,
pytorch,https://github.com/pytorch/pytorch/commit/8198474eb763c8d526ede3418211479c2f4cbd30,"Previous to this PR, we only checked TorchScript nodes for scope compatibility, skipping their parent's scope reference check.",2023-11-02 15:31:32,1,missing,computation graph,incorrect functionality,add,checker api,nodes in computation graph,computation graph,,,
pytorch,https://github.com/pytorch/pytorch/commit/37dea0454dd310cfe443859f717862657df6b753,add checking for number of args checking observer in same graph,2022-04-07 23:56:03,1,missing,computation graph,incorrect functionality,add,if checker,input argument is valid,computation graph,,,
pytorch,https://github.com/pytorch/pytorch/commit/7fbdc86aecd6fddb3d309a1d655efbf51e840139,Removed a local function to check for dominators and used the one added to Node class,2021-07-20 22:29:58,1,unnecessary,computation graph,performance bug,remove,checker api,nodes in computation graph,computation graph,,,
pytorch,https://github.com/pytorch/pytorch/commit/bdbd3ed312e0fc81e75302239ea78b3445fe95e7,"Although `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.",2023-01-18 19:59:33,,insufficient,computation graph,crash,extend,if checker,others,computation graph,,,
pytorch,https://github.com/pytorch/pytorch/commit/7e9bf2ed860b8b60d252eead4cc457c3fe5f1667,"Although `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.",2022-12-19 18:43:28,,insufficient,computation graph,crash,extend,if checker,others,computation graph,,,â€
pytorch,https://github.com/pytorch/pytorch/commit/9234f5026dbaf09a41b82bb6cf5f10ad4eeb03f2,"at::cuda::CUDAEvent is ""lazy"" and only creates an event when it's first recorded. Until then, at::cuda::CUDAEvent is empty. If we use at::cuda::CUDAEvent::query() this is taken into account (an empty event is always ready), but WorkNCCL extracts the raw cudaEvent_t value from at::cuda::CUDAEvent and calls cudaEventQuery manually and doesn't check this. This could cause a failure. It's unclear if this is ever supposed to happen, but we're seeing that failure, and we want to sort it out in order to see if there's something ""deeper"" going on.",2020-12-15 6:15:48,1,improper,device availability,others,update,if checker,device is available,cuda device,,,
pytorch,https://github.com/pytorch/pytorch/commit/04db1b874ff4fe902af3dd7540159645309490a7,"We only need to update locations where we actually check `shuffle` for identity with a boolean value, i.e. `shuffle is False`. For bool-ish checks like `if shuffle:`, `None` behaves just like `False`. `IterDataPipe`'s are currently not mentioned in the docstring. Since this change only applies to them, I didn't update it. LMK, if I should do that.",2022-04-12 14:26:33,2,missing,edge cases,incorrect functionality,add,if checker,others,Frontend parameter,,,
pytorch,https://github.com/pytorch/pytorch/commit/e9c1ccee2247a7746fde202067a7d47b72809968,Bug fix: allow std 0 in the meta definition of normal_. All other `normal` variants allow 0.  Looks like a mistake made while copying the check. ,2022-03-01 18:28:14,2,improper,edge cases,runtime error,update,macro checker,input argument is valid,integer argument,,,
pytorch,https://github.com/pytorch/pytorch/commit/c99277e177cf16736262251c7e92ea5e9ba2c5c2," handle the case in acc_ops.sum when dim == 0, differentiating it from the case when dim is None. handle the case in acc_ops.sum when dim == 0, differentiating it from the case when dim is None",2021-09-13 17:24:16,,improper,edge cases,crash,update,if checker,tensor dimension is valid,none dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/d72db37c4a6513c0f67f6f69870c9c45bf4880e6,"In file: combinatorics.py, the comparison of Collection length creates a logical short circuit. if isinstance(self.sampler, Sized) and len(self.sampler) >= 0: Here, the right side of the comparison will always return true. I suggested that the Collection length check should be removed since this is redundant.",2023-01-31 11:45:32,,unnecessary,edge cases,incorrect functionality,remove,if checker,others,sampler,,,
pytorch,https://github.com/pytorch/pytorch/commit/37008940999a41aedf3d7e77c86167c8e4852d7a,I moved the `torch.empty` call after the conditional so that we don't need to check whether `flat_param.grad` is None,2023-11-13 14:04:24,1,improper,edge cases,numerical error,relocate,checker api,tensor is not empty,,,,
pytorch,https://github.com/pytorch/pytorch/commit/c170d395de8ca441d3bedb20c9e45beb666f216c,Only check for xnnpack if torch installed. Fixes a bug where collect_env.py was not able to be run without having torch installed,2022-03-17 11:31:26,,missing,others,version incompatibility,add,if checker,others,pytorch installation,,,
pytorch,https://github.com/pytorch/pytorch/commit/65faf1a7eb07129d8b1f017fac341e178620dabd,Add version check for ProfilingVerbosity bulider config,2021-12-30 22:59:25,,missing,others,version incompatibility,add,if checker,others,ProfilingVerbosity,,,
pytorch,https://github.com/pytorch/pytorch/commit/6a1147d0596d49e7c2c8f4894484ffcd322f612d,"The current check that we have for dummy packages is too expansive; it will skip anything without a `__file__`, including extension modules in the standard library. So first check if a module was created by torch.package before skipping it, which should rule out anything accidentally getting skipped (as the only time torch.package creates something without a `__file__` is the dummy case).",2022-02-08 20:23:22,,improper,others,performance bug,replace,checker api,others,pytorch module,,,
pytorch,https://github.com/pytorch/pytorch/commit/6c98d904c09b69f1e7748cf3d80e2193df5fff63,"handle the case of -0.0 on tanh quantization.  this fix makes fakelowp identical to hw - mask out the floating point number with 0x7fff so we are always dealing with positive numbers - dsp implementation is correct, ice-ref suffers from this same problem",2020-09-10 4:18:45,1,improper,edge cases,inconsistent results,update,if checker,input argument is valid,Frontend parameter,,,
pytorch,https://github.com/pytorch/pytorch/commit/5fdddbbfe8ef17b2c81ed34a48f3b963944aa4c3,Fix checking of current mode in PyOperator dispatch,2023-01-18 18:08:36,1,improper,type checking,incorrect functionality,update,type checking api,object type is valid,dipatch mode,,,
pytorch,https://github.com/pytorch/pytorch/commit/c7c711bfb88fcb0ef573125a5a8655c49156055b,Adds optional tensor arguments to check handling torch function checks. The only one I didn't do this for in the functional file was `multi_head_attention_forward` since that already took care of some optional tensor arguments but not others so it seemed like arguments were specifically chosen,2021-08-30 22:21:26,2,missing,others,inconsistent results,add,checker api,others,optional tensor,,,
pytorch,https://github.com/pytorch/pytorch/commit/db6e0c7c0eb56ac45af2b9c1e70d081b6bee83ee,"Check if we are on Windows using `sys.platform` rather than `platform.system()`.  Even though `platform.system()` is more modern, it has a few downsides: this performs a runtime check of the platform type, which has non-zero overhead.  On Linux it actually executes the separate `/bin/uname` process.  On the other hand `sys.platform` is determined when the Python interpreter is compiled, so this is a simple hard-coded string. Because it is a runtime check, `platform.system()` checks also cannot be analyzed by static type checkers like Pyre and Mypy.  These type checkers do understand `sys.platform` checks, and can correctly avoid complaining about code paths that use platform-specific modules and functions.  e.g., they can avoid complaining about `ctypes.WinDLL` not existing on Linux if its use is guarded by a `sys.platform` check. ghstack-source-id: ",2021-02-11 23:09:14,,improper,platform compability,version incompatibility,update,if checker,others,windows,,,
pytorch,https://github.com/pytorch/pytorch/commit/6fde0cb507d59d2a9168f3051feba6865e9d1048,"THTensor_(newContiguous) always increments the refcount. It may return the same pointer if the tensor is always contiguous. Since we added the check for zero strides, it may be called when the tensor is already contiguous. We need to make sure that THTensor_(free) is always called in this case.",2017-11-07 12:47:13,1,missing,others,performance bug,add,if checker,others,tensor memory,,,
pytorch,https://github.com/pytorch/pytorch/commit/0c0c9e743e82b398435ed07719e998aa15ac1ce1,Fix dimensions check ,2017-10-20 13:28:01,1,improper,tensor shape mismatch,crash,replace,macro checker,tensor dimension is valid,tensor axis,,,
pytorch,https://github.com/pytorch/pytorch/commit/4d0fbb0e6f578bea14f3f52b0a927bcc20f8b109,"when adding a new axis to concatenate along, allow it to be the last axis. For example, concated 1D columns into a 2D matrix with axis=1, add_axis=1.",2017-08-17 16:03:18,0,improper,tensor shape mismatch,inconsistent results,update,macro checker,axis parameter matches tensor dimension,tensor axis,,,
pytorch,https://github.com/pytorch/pytorch/commit/b2d110447190abe5d66b0b59a775cc4881f3e30e,Fixed numpy bool check,2022-05-23 11:42:16,,improper,tensor shape mismatch,crash,update,if checker,tensor dimension is valid,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/85cbe0d8258ab06897e2f049e61f74d8aa935240,This diff is similar to D14163001. We need to handle the edge case when add_axis=1.,2019-06-24 13:32:48,,missing,tensor shape mismatch,crash,add,macro checker,axis parameter matches tensor dimension,,,,
pytorch,https://github.com/pytorch/pytorch/commit/4b45f08f8765549915417997c30ae8981f2ad125,The issue was related to not checking the dimensions of source vs destination tensors.,2019-06-13 16:15:23,,missing,tensor shape mismatch,crash,add,if checker,tensor dimension is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/4f63f348aef3da8b4d53f61098f4e32bd916c221,The bounds check was too conservative by an extra one.,2018-01-29 14:53:03,,improper,tensor shape mismatch,crash,update,if checker,axis parameter matches tensor dimension,,,,
pytorch,https://github.com/pytorch/pytorch/commit/bf32ea80942ce720b105efcd517fd11182edeb08," Fix dimension check in 1D instance norm, allowing 2D tensors alongsid e 3D.",2018-07-27 12:24:07,,insufficient,tensor shape mismatch,crash,extend,if checker,tensor dimension is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/036a159fb9c064877e32949b72c8f605fb358d5b,I went through every occurrence of AT_ASSERT in this file and thought about whether or not it should be TORCH_INTERNAL_ASSERT or TORCH_CHECK.  I think I did a good job at it. ,2019-05-20 12:54:37,1,improper,tensor shape mismatch,crash,replace,macro checker,tensor dimension is valid,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/a9deda5469a6ef73692a9dd796cc4eeba4436d6c,The at::native::_validate_sparse_coo_tensor_args only supports checking the indices on CUDA device and CPU device. To extend the function to support more device type.,2022-04-25 20:56:10,1,improper,device availability,crash,replace,if checker,device is available,device,,,
pytorch,https://github.com/pytorch/pytorch/commit/d9870d70c12dc59b0f8bce288910422bcb60b044,Exempt _foreach_norm from autograd_not_implemented_fallback check,2023-02-03 14:45:46,,insufficient,tensor operations,crash,extend,if checker,tensor operations are valid,autograd,,,
pytorch,https://github.com/pytorch/pytorch/commit/98f9ff90268ae62ab6d794cce0786121bf17edc9,"Fix an assertion failure involving Slice. Before this change, exporting a model to ONNX involving Slice crashes at `axes[i]` in line 153 if C++ assertions are enabled:",2022-02-18 13:41:47,1,improper,tensor shape mismatch,others,replace,checker api,tensor size is valid,tensor size,,,
pytorch,https://github.com/pytorch/pytorch/commit/b05c34259b5e3ff9b55fba5a24d7c5f2f2036471,relax size check in flatten_for_scatter_gather,2020-06-25 18:16:37,1,improper,tensor shape mismatch,unknown,replace,checker api,tensor size is valid,tensor size,,,
pytorch,https://github.com/pytorch/pytorch/commit/71af538e31547e5b1bc814c9e00323a21905baf3,Updated assert to remove check on 3rd dim for MHA.  Updated assert statement to remove check on 3rd dimension (features) for keys and values in MultiheadAttention / Transform. The feature dimension for keys and values can now be of different sizes,2020-06-02 16:35:39,2,insufficient,tensor shape mismatch,inconsistent results,extend,assertion statement,tensor size is valid,tensor size,,,
pytorch,https://github.com/pytorch/pytorch/commit/b8ab3080b1043a610ba2825a2be406a1833b1d70,"If kernel sizes were specified via ""kernel_w"" and ""kernel_h"", tensor size inference was incorrect in InferShapesAndTypes(): it was checking for ""helper_w"" instead of ""kernel_w"".",2017-09-21 17:50:43,0,improper,others,incorrect functionality,update,if checker,input argument is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/be253b8ee8a104997773d11ed28928a48193217d,The existing check isn't safe for 32-bit `size_t` because the max 64-bit int will overflow.,2022-03-01 1:31:52,,improper,tensor shape mismatch,numerical error,update,checker api,tensor size is valid,integer variable,,,
pytorch,https://github.com/pytorch/pytorch/commit/128dd6b1502ad3687ffb79484b72d4cfafec496e,"Relax the check that makes sure number of outs equals number of returns. We are trying to add an out variant for an existing operator, Notice the out argument is a mutable list of tensors. Given the fact that we don't support mutable tensor list as a return type and it seems not useful to add such a return type. The solution I'm proposing is to relax the constraint that the number of outs needs to be the same as the number of returns, so we can return a `void`.",2022-04-20 19:10:41,,insufficient,out of bound access,crash,extend,assertion statement,output and input tensors has same size,parameter,,,
pytorch,https://github.com/pytorch/pytorch/commit/993d8bb77e1ce79940705b1c7667dc9276f449df,"Use size to check same tensor sizes in reduce_scatter and allgather. Previous code uses tensor.numel() to check if all tensors have the same size in order to switch between reduce_scatter_v v.s. reduce_scatter, same applies to allgather. However, if the user input tensor is zero in the last dimension (e.g., [648632,0]), then numel() returns zero and check_same_numel is always true. This patch fixes the check to use size rather than numel, to cover the above case.",2022-08-26 1:45:59,,improper,tensor shape mismatch,incorrect functionality,replace,type checking api,tensor size is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/363d5300356e17e7e9eb92381a4cf8a932fca3e0," Fix decision logic for should_cast_forward_inputs in `_root_pre_for ward()` and `_pre_forward()`. There is currently no topological constraint dictating FSDP instances own ``FlatParamHandle`` s directly. If all parameters are managed by descendant FSDP instances leaving an FSDP instance with no direct ``state._handles``, the  ``should_cast_forward_inputs`` decisions below in both ``_root_pre_forward()`` and ``_pre_forward()`` respectively can return incorrect decisions [^1]. In this PR, I amend the two decision statements referenced above (in both `_root_pre_forward()` and `_pre_forward()`) to account for FSDP instances without direct handles. If one configures ``MixedPrecision`` in the example above with ``cast_forward_inputs=True`` and the ``should_cast_forward_inputs`` adjustment above, FSDP returns to the expected behavior and produces no error. Though the check is the same in both ``_root_pre_forward()`` and ``_pre_forward()`` and hence could be refactored into a separate function, I figured it may make sense to retain separate statements to preserve the ability for root-specific behavior in the future. Whichever approach the team prefers I can update this PR with.",2023-04-21 18:49:50,,insufficient,edge cases,incorrect functionality,extend,if checker,object is not empty,,,,
pytorch,https://github.com/pytorch/pytorch/commit/9c3cb6e652e6033e355cef92229fde5b0baf271b,"Fix stride checks in gemm dispatch.  lda: ""When transa = 'N' or 'n', then lda must be at least max(1, m), otherwise lda must be at least max(1, k). When transb = 'N' or 'n', then ldb must be at least max(1, k), otherwise ldb must be at least max(1, n).",2017-11-08 9:55:25,1,improper,tensor shape mismatch,inconsistent results,update,if checker,tensor dimension is valid,tensor stride,,,
pytorch,https://github.com/pytorch/pytorch/commit/7f125bca1cd42ebd8e07c97f1bd1682dff5cf387,Add pin_memory check in empty_strided. Add the false checking if pin_memory has been specified to `False`,2020-11-02 17:00:12,1,insufficient,tensor shape mismatch,crash,extend,if checker,tensor dimension is valid,tensor stride,,,
pytorch,https://github.com/pytorch/pytorch/commit/f02b7a9c36dd6182da694bc47a5c345285dfd951,"don't error when unused fill value is zero. In the python version of `F.pad`, checking that the fill value was left as default was done by comparing against zero. So if someone does explicitly pass in a zero-value, then this `TORCH_CHECK` was an accidental BC-break.",2022-04-26 9:07:56,,improper,edge cases,crash,update,macro checker,tensor is not empty,tensor value,,,
pytorch,https://github.com/pytorch/pytorch/commit/4839f73f329b38819e6f69a8662d61dc36558e52,"Fix incorrect tensor storage check. These fixes were run through the DirectML test suite, and confirm the check is now working correctly.",2022-10-13 13:54:28,2,improper,others,crash,update,checker api,tensor is not empty,tensor storage,,,
pytorch,https://github.com/pytorch/pytorch/commit/75be4f9cdb503d6eff189b2bc5c05d96bff66653," check tensor has storage before refer to tensor data ptr. In the exporter dedupe initializers passes, check the tensor has storage before reference to tensor's data_ptr, otherwise it will result in a crash.",2022-04-26 13:52:45,1,insufficient,others,crash,extend,checker api,tensor is not empty,tensor storage,,,
pytorch,https://github.com/pytorch/pytorch/commit/a3701b674046bcefb5927a6643364b186f77dbcf,"fix backward bug for custom device. In the backward on some device , it may get an error to get device index because of exchange a new thread. So just set_device and check the device index in `setDevice`  func may be better for some many kinds of devices. For CUDA, the device index check is also included in `setDevice`  func",2023-04-10 11:56:38,1,unnecessary,device availability,incorrect functionality,remove,if checker,device is available,device version,,,
pytorch,https://github.com/pytorch/pytorch/commit/6f5945e4bb1258d39a2878a08a910fcc8f659d5e,"triton supports devices < 7.0, not 6.0. triton is still buggy with Pascal devices, so make the error checker reflect that. Also, this < 6.0 never worked, as the `has_triton` definition in utils.py was checking >= 7.0.",2022-12-01 17:01:41,1,improper,device availability,version incompatibility,update,if checker,device is available,device,,,
pytorch,https://github.com/pytorch/pytorch/commit/7c44823a4e0506b2eb7f1aa667ba939ba9d56969,Fix layout/device checks in sparse-dense addmm,2023-02-14 18:23:26,,missing,device availability,incorrect functionality,add,macro checker,device is available,device,,,
pytorch,https://github.com/pytorch/pytorch/commit/871b5caae76185cff141c522b3133e7543c8dabf,"Fix hpu deserialization bug. fix hpu deserialization bug. It should check hpu model if and only if location start with hpu. Otherwise, it always raise an AssertError if hpu is not imported. This break the serialization/desirialization functionality abourt other third-party like IPEX. only assert hpu model when start with hpu",2023-09-18 20:10:51,1,improper,device availability,incorrect functionality,relocate,assertion statement,device is available,IPEX,,,
pytorch,https://github.com/pytorch/pytorch/commit/6592259ea52f45e1fc9a633ccb5b154ba5099334,"As per torch.jit.load documentation, all previously saved modules, irrespective of their device, are first loaded onto CPU, and then are moved to the devices they were saved from. So far, supported devices included CPU and CUDA only. To enable torch.jit.load for HPU, additional check for HPU is introduced.",2022-08-01 5:28:44,,insufficient,device availability,incorrect functionality,extend,if checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/1becd2c314f45bded8d3fbec91d785e7190b4afe,"Align checks in _use_cudnn_ctc_loss with those in _cudnn_ctc_loss.This PR is intended to fix the following problem: When using `CTCLoss`, there is a cudnn path gated by a call to [`_use_cudnn_ctc_loss`] which checks some conditions. However, there are more checks in `_cudnn_ctc_loss`.  some of which are not present in `_use_cudnn_ctc_loss` (e.g. the check that `targets` is on CPU which will cause a RuntimeError after dispatching to `_cudnn_ctc_loss`). Instead, these checks should be in `_use_cudnn_ctc_loss` so that the normal `_ctc_loss` path will be used if the checks are not met)",2023-12-12 17:20:20,1,insufficient,device availability,runtime error,extend,if checker,device is available,device type,,,
pytorch,https://github.com/pytorch/pytorch/commit/490f2d75700a806bdc6110e881e78493cde163e3,"Skip privateuse1's checkZeroPoints. We want to use ``quantize_per_channel`` to create a quantized tensor, but we found that ``checkZeroPoints`` for ``privateuse1`` backend failed. ``quantize_tensor_per_channel_affine`` will ``checkZeroPoints`` for all backends expect ``CUDA``:  However, our ``privateuse1`` backend will get a segmentation error if we try to cast our data to int64_t in ``checkZeroPoints``: So if we can skip ``privateuse1``'s ``checkZeroPoints`` and check this item in the actual device function? What do you think?",2023-12-05 23:44:49,1,insufficient,device availability,crash,extend,if checker,device is available,device type,,,
pytorch,https://github.com/pytorch/pytorch/commit/a076a74f1118da171cf70d00d1de4abbe27cf85d, Add xpu device in assertion for nested tensor creation,2023-11-29 0:59:35,1,insufficient,device availability,crash,extend,if checker,device is available,device type,,,
pytorch,https://github.com/pytorch/pytorch/commit/097defb1608827d82b18b27adeec0a98b72a9281,only check when world size > num_devices per host,2023-10-11 23:37:18,1,insufficient,device availability,crash,extend,if checker,device is available,device type,,,
pytorch,https://github.com/pytorch/pytorch/commit/57af1ec14594a73c8f2b73bf70c04ba7efeb6eab,"observers: use torch.all to check for valid min and max values. Using `torch.all` instead of `torch.sum` and length check. It's unclear whether the increase in perf (~5% for small inputs) is real, but should be a net benefit, especially for larger channel inputs.",2020-08-17 20:08:57,1,improper,tensor shape mismatch,performance bug,replace,checker api,tensor dimension is valid,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/07f0f383fa23e63eca164036ab58ab983e9437eb,"update tensor-like to check instance for torch function impl. tensor like should check the instance for a torch function impl, not the type",2023-10-11 22:14:38,1,unnecessary,type checking,incorrect functionality,remove,if checker,tensor type is valid,tensor,,,
pytorch,https://github.com/pytorch/pytorch/commit/a33d8133a52c7453958b70facc40bd7448d5d88d,"Slight cleanup of VariableBuilder giant if condition. Some of these changes are semantics preserving, some are not. Please review carefully. * Use `istype(x, y)` over `type(x) is y` * Use istype over isinstance in frozenset. If the user subclassed the type in question, we must treat it as a user defined class as it may have custom behavior * The `isinstance(value, (int, float))` condition for `wrap_unspecialized_primitive` is dead-ish; direct int/float values are caught earlier istype check. Technically however, if you subclassed int/float it would pass through, however this is almost assuredly not intended behavior",2023-02-24 21:23:40,1,improper,type checking,incorrect functionality,replace,checker api,others,tensor type,,,
pytorch,https://github.com/pytorch/pytorch/commit/4e1bd4abe7691f460cb021e5b314168caa42ef92,"Fix scalar type resolution for optional tenso. When TorchScript Value has an optional tensor, `dtype()` or `scalarType()` is not available and raise (by design).  The symbolic `_op_with_optional_float_cast` must check whether the tensor is otpional or not before calling the scalar type resolution API. This PR fixes that",2023-02-09 10:22:02,1,missing,type checking,others,add,type checking api,object type is valid,tensor type,,,
pytorch,https://github.com/pytorch/pytorch/commit/b90db4a78f8d760377a81a5a64d03ab4b67599de,Fix type checking to accept both Iter and Map DataPipe,2022-10-20 1:05:56,1,improper,type checking,incorrect functionality,update,type checking api,object type is valid,object,,,
pytorch,https://github.com/pytorch/pytorch/commit/63cbdc92a750a667ffdcfbdac563d02db6fd9559,switching the exact check to isinstance check. Simplifying a type check if an object is a SymIntNode in `is_symint_node`,2022-08-25 4:28:40,,improper,type checking,runtime error,update,type checking api,object type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/b7d58745c882a66f5c20d2ba05b99ddce7491d38,"remove unneeded isinstance checks in `op_con. vert_before_hook. `isinstance` has some overhead, changing the code in `op_convert_before_hook` to use the information calculate during tracing instead which is cheaper.",2021-11-21 10:08:12,,improper,type checking,performance bug,replace,type checking api,others,tensor operation,,,
pytorch,https://github.com/pytorch/pytorch/commit/6420071b43dc9f2679c22952b5051b0c28f42da2,"Disable complex dispatch on min/max functions. In issue #36377, min/max functions were disabled for complex inputs (via dtype checks). However, min/max kernels are still being compiled and dispatched for complex. The aforementioned dispatch has been disabled & we now rely on errors produced by dispatch macro to not run those ops on complex, instead of doing redundant dtype checks.",2021-01-12 10:55:18,,improper,type checking,runtime error,replace,type checking api,tensor type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/73431f087b48560ab4b65f8814809fc019061223,Allow torch.load and torch.save to take pathlib.Path. pathlib has been python standard library for filesystem path since python 3.4 But `torch.load` currently cannot take `pathlib.Path` as its filename of state dictionary. I changed `torch.load` and `_with_file_like` to check so that they can accept `pathlib.Path` typed filepath.,2017-11-11 18:50:13,,insufficient,type checking,incorrect functionality,extend,type checking api,others,,,,
pytorch,https://github.com/pytorch/pytorch/commit/24ca6aab024f4e45e462e387ffd710a1d25b59da,Improves type-checking guards. PR #38157 fixed type checking for mypy by including `if False` guards on some type-checker-only imports. However other typecheckers will respect this logic and ignore the imports,2020-09-03 10:45:53,,improper,type checking,others,update,if checker,object type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/92ebb04f9206882e6d312a8b91318545f43a53c2,added check for NumberType,2020-09-24 19:26:59,,insufficient,type checking,runtime error,extend,if checker,object type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/40d6f2a02027023216607adb892d3b9c7493904c,Update sdp_utils to check gradmode and subclassed tensors. Fix up the grad check test to check for subclassed tensors and gradmode,2023-01-17 18:14:21,,insufficient,type checking,incorrect functionality,extend,type checking api,tensor type is valid,subclass tensor,,,
pytorch,https://github.com/pytorch/pytorch/commit/fe6aa0844466e5dd2669092eac5edde153108b28,Don't delegate to `operator=` for construction. Catch hypothetical addition of a new Scalar type via debug assertion rather than checking in prod.,2022-05-13 16:49:40,,improper,type checking,incorrect functionality,replace,macro checker,tensor type is valid,scalar,,,
pytorch,https://github.com/pytorch/pytorch/commit/1c5a8125798392f8d7c57e88735f43a14ae0beca,Better type checking in disable_torch_function/dispatch ,2022-03-26 10:49:11,,improper,type checking,incorrect functionality,replace,type checking api,others,type checking,,,
pytorch,https://github.com/pytorch/pytorch/commit/35e7ac3fa17bc20d982ea69440d30cd9658dff25,This fixes a bug in singleCheckErrors introduced by #69437 (thanks Lezcano for the catch). Checking existence of a substring in a larger string is done with (name.find(text) != name.npos) but we omitted the second half of the check.,2022-01-25 0:33:36,,insufficient,others,incorrect functionality,extend,if checker,others,string varuable,,,
pytorch,https://github.com/pytorch/pytorch/commit/0f0829d88e839be1e150e917aca5b1edb64752ee," Strict bound check for SequenceFunctor. Summary: This exhibits the problem in NMT training where some out of bound data seems to have silently written over bound, and causing random segfaults elsewhere in the code. This itself does not solve the problem, but will trigger us to then fix the out of bound issues.",2017-09-14 4:30:58,,missing,edge cases,segmentation fault,add,type checking api,tensor size is valid,tensor sieze,,,
pytorch,https://github.com/pytorch/pytorch/commit/a7cc6531399300f999a404718827e2a94c115aaf,"Summary:GCC version check is currently being skipped when using the newly released CUDA 9.1. This will also handle other CUDA 9.x minor releases if any, reducing our work if there are such releases like 9.2. This assumes that the next major CUDA version will be 10.0, needing adjustment only after such major version is released.",2018-01-03 20:42:55,1,insufficient,device version,version incompatibility,extend,if checker,device version is valid,CUDA ,,,
pytorch,https://github.com/pytorch/pytorch/commit/218f4506fdcde69e3f8f2f2b2b51fefd996c577b,"Summary:this PR modifies the gcc compiler check for CUDA slightly. All ABI are compatible with eachother. According to the documentation, `CUDA_HOST_COMPILER` is set to `CMAKE_C_COMPILER` by default. This PR checks if `CMAKE_C_COMPILER` is too new for CUDA 8 and whether `CUDA_HOST_COMPILER` is set to `CMAKE_C_COMPILER`. It also modifies the message slightly.",2017-08-09 1:35:04,2,insufficient,device version,version incompatibility,extend,if checker,device version is valid,CUDA ,,,
pytorch,https://github.com/pytorch/pytorch/commit/3f5dc95b57496c4ea938be381efcdc2ea92bb4cc,Some of the tests don't specify `device` in the input configs so filter by device won't work for them. This diff fixes that issue.,2019-11-15 16:55:38,1,insufficient,device availability,incorrect functionality,extend,if checker,device is available,device,,,
pytorch,https://github.com/pytorch/pytorch/commit/1c02be1b6a0f6d02d3a0ae19c13d51a3e59a55ae,"In PyTorch 1.5, when running `torch.cuda.reset_peak_memory_stats()` on a machine where `torch.cuda.is_available() is False`, I would get: AssertionError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from. With this patch, we get a more sensible:",2020-12-01 17:55:18,,insufficient,device availability,attribute error,extend,if checker,device is available,device,,,
pytorch,https://github.com/pytorch/pytorch/commit/29b702144bf5bb96dfd8fcbd04b6562a27ca5385, Fix issue in s_addmm_out_sparse_dense_cpu only supporting CUDA device checking. ## Motivation The at::native::s_addmm_out_sparse_dense_cpu only supports the CPU tensors. But it only checks whether the tensor is on CUDA device which is not enough. ## Solution Change the tensor device type checkging from is_cuda to !is_cpu to protect other backends than the CUDA.,2022-05-07 6:35:29,1,improper,device availability,incorrect functionality,update,macro checker,device is available,device,,,
pytorch,https://github.com/pytorch/pytorch/commit/bbc7c79b20e67da450dd9b7de70cc6b68e656714,add device checks for sparse csr,2023-03-27 14:57:27,,missing,device availability,incorrect functionality,add,macro checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/faa7eb81c634492b70fcc0327622bb0aa812cacd,"change error_message for XPU Autocast data type check. XPU autocast supports bf16 and fp16 data types, we are going to change the error_message for that.",2023-05-24 4:36:43,1,misleading,misleading error message,user confusion,improve,if checker,improve error message,log,,,
pytorch,https://github.com/pytorch/pytorch/commit/48a49b2683ffa21eb1b472e503c129c043c18f87,"use more informative error message for ConstandPad2d/3d.  the current error message for `torch.nn.ConstantPad2d` and `torch.nn.ConstantPad3d` is misleading, this PR fixes the problem.",2023-07-10 15:00:47,2,misleading,misleading error message,user confusion,improve,macro checker,improve error message,log,,,
pytorch,https://github.com/pytorch/pytorch/commit/bdb3fb49bc7a73cbcc5c865dda8be32888deb584,Fix the check message of unsupported collectives ops.  Fix the check message of unsupported collectives ops.,2023-05-22 14:37:05,1,misleading,misleading error message,user confusion,improve,macro checker,improve error message,log,,,
pytorch,https://github.com/pytorch/pytorch/commit/4ab1588d9919bc1a62219a5c2393e0784ddaae70,"Enhance error message for dependency check, If python development library is missing when building pytorch from source cmake will raise the error like: CMake Error at cmake/Dependencies.cmake:1079 (if): if given arguments: ""VERSION_LESS"" ""3""  Unknown arguments specified ```it's quite a misleading information that user would consider it's a syntax error or cmake version problem. This PR add a check to ensure `PYTHONLIBS_VERSION_STRING` exist before using.",2023-03-22 4:42:48,1,misleading,misleading error message,user confusion,improve,if checker,improve error message,log,,,
pytorch,https://github.com/pytorch/pytorch/commit/577e90ae9bf257040acb68da3626d9a64d07bf7a, Improve error message for missing ops. The current error message is ill formed,2022-06-23 5:04:34,1,misleading,misleading error message,user confusion,improve,macro checker,improve error message,log,,,
pytorch,https://github.com/pytorch/pytorch/commit/22044c6f7cbdafdd340714bbe220b621e1927826,"Use TORCH_CHECK instead of AT_ASSERT in torch::cuda::gather(). The error message produced by AT_ASSERT() in gather() encouraged users to file a bug report (""please report a bug to PyTorch...""). The assertion should be a regular argument check since it can be triggered by passing tensors with different dimensionality, e.g. `torch.cuda.comm.gather([torch.rand(1, device='cuda'), torch.rand(1, 1, device='cuda')])`.",2020-01-07 13:04:24,2,misleading,misleading error message,user confusion,improve,macro checker,improve error message,log,,,
pytorch,https://github.com/pytorch/pytorch/commit/a5aceba61fc290236af955e2c4fff4513476c9ff, [static-runtime] a pass through checks throwing exceptions Summary: increasing verbosity where possible,2023-03-07 14:16:27,1,misleading,misleading error message,user confusion,improve,macro checker,improve error message,log,,,
pytorch,https://github.com/pytorch/pytorch/commit/dc0d68a1ee3800ed4024762d018f85256e80f5ad,Print out interface mismatch for prim::ModuleDictIndex. This commit augments the module interface subtyping check that is done before the emission of the `prim::ModuleDictIndex` operator so that the error message that is printed if the subtyping check fails provides more information on which methods do not match.,2020-11-03 16:07:21,,misleading,misleading error message,user confusion,improve,if checker,improve error message,,,,
pytorch,https://github.com/pytorch/pytorch/commit/871e240e6367f94966a3e2f9deefbfa98e314d6d,Improved error message for interpolation. Improved error message for CUDA interpolation with antialiasing,2022-01-31 15:50:42,,misleading,misleading error message,user confusion,improve,macro checker,improve error message,logging,,,
pytorch,https://github.com/pytorch/pytorch/commit/c9548176965557a76526ba0db23ff5c9facd3e97,print matrix dims in torch cuda matrix multiply error.  trying to improve the error message for torch matrix multiply dimension mismatch,2021-02-24 23:09:25,,misleading,misleading error message,user confusion,improve,macro checker,improve error message,,,,
pytorch,https://github.com/pytorch/pytorch/commit/7caceea6e8c352a934f8fecf21f55454007d63b2,better error messages for Conv*d input shape checking ,2017-09-25 23:53:59,,misleading,misleading error message,user confusion,improve,if checker,improve error message,,,,
pytorch,https://github.com/pytorch/pytorch/commit/93256617c8622760181dacf03c41cc0577ac0ea6,corrected messages for check of default options. Modified messages in the check of default options for the Adam optimizer.,2020-04-08 14:22:50,,misleading,misleading error message,user confusion,improve,macro checker,improve error message,,,,
pytorch,https://github.com/pytorch/pytorch/commit/99f06c0cc2a907d8fbf613768356838548f1f8c0,update errors to be more descriptive we call `_check_single_tensor` and `_check_tensor_list` as validation but don't print out the param types that were invalid,2023-12-11 16:21:10,1,misleading,misleading error message,user confusion,improve,if checker,improve error message,,,,
pytorch,https://github.com/pytorch/pytorch/commit/9a9eadacc6ac3b734a6d607ae6f63ec1a0d1438d,explicitly check device for grid_sampler,2018-06-19 11:53:46,,missing,device availability,crash,add,if checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/86fb522acdd11f8caf5f4b0e0f3269ca515745c5,"Remove cudaMemcpy on full memory overlap. TensorIterator is already checking partial overlap, so there is no trivial UB, but TensorITerator allows full overlap, and it is not a bad idea to skip the memcpy in such case.",2020-03-11 20:36:03,,unnecessary,device availability,crash,remove,if checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/157d478a30f27fd9d866c1235841721a559c8d0b,Fix omission of shape in size check in index. ,2022-06-05 19:10:55,1,improper,tensor shape mismatch,crash,update,if checker,axis parameter matches tensor dimension,tensor shape,,,
pytorch,https://github.com/pytorch/pytorch/commit/c1384ef99e7a0d8a439df8972532fe4f155a5683,Fix NDPooling gradient non-symmetric padding check. ,2017-07-18 22:08:49,0,improper,tensor shape mismatch,crash,update,if checker,axis parameter matches tensor dimension,tensor size,,,
pytorch,https://github.com/pytorch/pytorch/commit/f14887a63f83b931df9fc5d841c7d3829141ff58,check for exact shape match before loading. Use RuntimeError instead of ValueError to keep it consistent with other errors,2018-06-18 23:16:34,,missing,tensor shape mismatch,crash,add,if checker,tensor shape is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/02e2158e754bafda46e663052c838aeb6ab6b560, Fix for out of bounds read in mobile interpreter INTERFACE_CALL opcode handler. The INTERFACE_CALL opcode for the mobile TorchScript interpreter contained an out of bounds read issue leading to memory corruption. This change adds an explicit check that the number of inputs passed to the format method called when handling the INTERFACE_CALL opcode is a valid and within bounds of the stack.,2023-12-28 17:09:03,1,missing,out of bound access,crash,add,macro checker,others,tensor stack,,,
pytorch,https://github.com/pytorch/pytorch/commit/4d07428edee863e7f5920f0672957a9711a9f0b5,Fix for out of bounds read in mobile interpreter FORMAT opcode handler. Summary: The FORMAT opcode for the mobile TorchScript interpreter contained an out of bounds read issue leading to memory corruption. This change adds an explicit check that the number of inputs passed to the format method called when handling the FORMAT opcode is a valid and within bounds of the stack.,2023-11-21 20:05:42,1,missing,out of bound access,crash,add,macro checker,others,input size,,,
pytorch,https://github.com/pytorch/pytorch/commit/b3ace213f240dc0f0f2a738f825f46e0d0dffca4,The error occurs because there is not check in `deserialize_source` that `text_table_` size can be less than `fnameIndex`. To prevent the error the corresponding check must be located.,2023-06-23 20:49:14,,missing,out of bound access,crash,add,macro checker,others,,,,
pytorch,https://github.com/pytorch/pytorch/commit/d8466964b348b6172317f70b8e52de02402bad54,Add range check to multi margin loss target ,2022-11-15 15:35:51,,missing,out of bound access,crash,add,if checker,axis parameter matches tensor dimension,,,,
pytorch,https://github.com/pytorch/pytorch/commit/c22ac14969a863a00b5ebb04a3453610c7a27713,The diff sets the upper boundary on border element when presenting the error message. This is required in order to avoid unnecessary log contamination,2021-06-02 15:28:54,,missing,out of bound access,crash,add,macro checker,tensor dimension is valid,upper boundary on border element,,,
pytorch,https://github.com/pytorch/pytorch/commit/43f810fa96a0d2c40387c8c84f710926d9ede3c1,Add streams boundary check to torch::cuda::scatter`. Summary: Accessing elements of `std::vector` outside of its boundaries can lead to crashes/memory corruptions,2021-03-02 13:58:10,,insufficient,out of bound access,crash,extend,if checker,tensor dimension is valid,cuda stream,,,
pytorch,https://github.com/pytorch/pytorch/commit/f548946363ae8d2875b345b58c71b057f628cd1e,"Fix out-of-boundary access in caffe2::StartsWith. `std::mismatch( InputIt1 first1, InputIt1 last1, InputIt2 first2 )` assumes that container for `first2` iterator contains at least `last1 - first` elements, which is not the case if `prefix` is longer than `str` Found while running unit tests on Windows",2020-04-15 23:40:59,,improper,out of bound access,crash,update,checker api,tensor dimension is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/b7bb34d7625d95e5088638721dcc07c2bc5e2ade,[MPS] Add version check Use `instancesRespondToSelector:` to test the presence of `optimizationLevel` in `MPSGraphCompilationDescriptor`,2022-05-24 18:28:16,,missing,platform compability,version incompatibility,add,if checker,others,MPS,,,
pytorch,https://github.com/pytorch/pytorch/commit/b1f08e7426a56a323e6928365918093b65aa4fb6," Call uncheckedSetDevice in ~InlineDeviceGuard only when device index are different. Setting device could be expensive, especially when a debugger is present. We should check the device are different before we set.",2020-03-30 16:13:17,,missing,device availability,performance bug,add,if checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/53953316444485c8ee250022988ef87778ae1352,Stacks recorded when tensors are being freed during exit could try to acquire the GIL. Py_IsInitialized can be used to check if we are post Python exit and should not attempt to acquire the GIL.,2024-01-03 20:56:44,1,missing,others,performance bug,add,checker api,others,python initialization,,,
pytorch,https://github.com/pytorch/pytorch/commit/8269c4f3d30ad950a873d900f7de0880cdd38878,Added nullptr check for pthradpool_get_threads_count. We get seg fault without this in using XNNPACK.,2020-03-04 14:10:53,1,missing,null pointer dereference,segmentation fault,add,if checker,object is not null,object,,,
pytorch,https://github.com/pytorch/pytorch/commit/13121598efc7d86cb7ae6e05322bb95c1d0f16bc,Bug fix to update requantization and zp parameters of input. Also sneaking in change to check for realloc failure for packed activation buffer. In dynamic quantization input's quantization scale and zero point can be different on every iterations. Thus requantization scale needs to be recomputed. Earlier bug that calculated those only at op creation time results in wrong results on subsequent runs.,2021-02-25 11:44:30,,missing,null pointer dereference,crash,add,if checker,object is not null,,,,
pytorch,https://github.com/pytorch/pytorch/commit/e8e29690ef0306da25b5f191623476001d29a18b, Add has_debug_def() check to net's debug_def() ,2017-11-10 6:24:49,,missing,null pointer dereference,crash,add,macro checker,object is not null,,,,
pytorch,https://github.com/pytorch/pytorch/commit/eed22921237eb4c1f4399af177ce912147a885c3, check for null commonworld in DestroyCommonWorld. Summary: Check for nullptr before closing a common world.,2017-08-31 13:48:57,,missing,null pointer dereference,crash,add,if checker,object is not null,nullptr,,,
pytorch,https://github.com/pytorch/pytorch/commit/1f83d8eec25c7339bd3e2862baf9b389e6a738a4," [Static Runtime] Return nullptr if the number of input args doesn't match. Add checks for the number of input args and return nullptr if it doesn't match. This is intended to make Static Runtime more robust so that op schema change is less likely to break things. Imagine that a new arg is added to an op or a new overload is added that has this added arg, SR would simply ignore this added arg. If this arg has a default value, SR would run the model with the default value and give you wrong results, which can be hard to track down. ",2021-05-11 19:30:45,,missing,null pointer dereference,crash,add,if checker,object is not null,nullptr,,,
pytorch,https://github.com/pytorch/pytorch/commit/d471eaeb1d2fbc7efcde6408d7d1e513b969af25,fix inline_container.cc inplace loading,2023-09-05 20:02:42,1,missing,null pointer dereference,crash,add,if checker,object is not null,buffer,,,
pytorch,https://github.com/pytorch/pytorch/commit/a5ca445f7953711bc90c111c3cad2ec87f02e74a,"Check for corrupted ivalues. The error occurs because the `ivalues` field of flatbuffer module can be null, so the corresponding check must be inserted.",2023-06-30 18:53:49,,missing,null pointer dereference,crash,add,macro checker,object is not null,,,,
pytorch,https://github.com/pytorch/pytorch/commit/6cc0f1c20c2f87a6c7b0e4abd5419e5007920999,Checking for nullptr in get_model_bytecode_version . One-liner commit to check that the ptr is not null. Just had `test_jit` that had a segfault there.,2023-06-13 19:54:45,,missing,null pointer dereference,segmentation fault,add,macro checker,object is not null,,,,
pytorch,https://github.com/pytorch/pytorch/commit/61b9d8fccd3361f21e1f3548c2a9538b62cc7525,"It is apparently undefined behavior to do pointer arithmetic on nullptr. In the case of AppendOnlyList, `next_` will only be null if `end_` is also null and thus the `memcpy` path will only be triggered if `n == 0`. Nonetheless, it is U to `memcpy(0, 0, 0)`. The extra null check is in a `C10_LIKELY` block so the extra cost should be negligible, and indeed after dusting off the component microbenchmarks there's no observable difference.",2022-08-26 16:03:24,,improper,null pointer dereference,incorrect functionality,update,checker api,object is not null,,,,
pytorch,https://github.com/pytorch/pytorch/commit/4ddf27ba48ba3313a20d3316a8929cd42436ddbc, [op-bench] check device attribute in user inputs. Summary: The device attribute in the op benchmark can only include 'cpu' or 'cuda'. So adding a check in this diff.,2020-07-14 20:17:59,,missing,device availability,incorrect functionality,add,macro checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/f441bb1c2088e9ce6684765a75869a72817be39d,"check error status of CUDA launch after Magma kernels. I found that the stack-trace of a failed kernel launch was being recorded elsewhere, even with CUDA_LAUNCH_BLOCKING=1. So, I started debugging, and found that magma launches don't do error checking. I eventually found the issue to be that I didn't compile-in sm37 SASS into the magma binary and the failure was on `x.inverse()`, and that's somehow a problem for magma 2.5.1 (but not 2.5.0).",2019-11-08 12:43:51,,missing,device availability,incorrect functionality,add,macro checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/e24dee00d40d01bd83b7a08fbcf9cdd51a05b04b,add kernel launch checks after each kernel launch to silence the check ,2021-05-17 21:03:19,,missing,device availability,incorrect functionality,add,macro checker,device is available,cuda kernel,,,
pytorch,https://github.com/pytorch/pytorch/commit/c06dfd7c26102ac2436ca25609c92fa794e972ca,Check input device in TRTModule. Add a check to ensure all the inputs are on cuda device.,2021-08-25 13:25:34,,missing,device availability,incorrect functionality,add,assertion statement,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/d3de37609f2f052a7efb098ab69540458ebaaa6c,Support fused_dropout with XPU backend. ## Motivation Enable the fused dropout optimization on XPU devices. ## Solution Add XPU device in the fused dropout acceptable checking.,2021-06-29 17:20:17,,insufficient,device availability,incorrect functionality,extend,if checker,device is available,xpu,,,
pytorch,https://github.com/pytorch/pytorch/commit/7bf195f3608e0f28c30ffb6e2fecd74a1d4ee50a,fix kernel launch check in cross kernel,2021-06-23 14:47:50,,missing,device availability,incorrect functionality,add,macro checker,device is available,cuda,,,
pytorch,https://github.com/pytorch/pytorch/commit/15dbc566c57eedbd0245e786912e94586eba0fd2,Add missing cuda kernel launch check,2021-06-16 15:19:12,,missing,device availability,incorrect functionality,add,macro checker,device is available,device type,,,
pytorch,https://github.com/pytorch/pytorch/commit/232fbd90ff6d93362120d955befeeb297179ddad,"For aten.convolution CPU path, the bias always can be fused, so this PR adds a device check: if inputs' device is CPU, we will fuse it for a good performance.",2022-10-19 3:13:38,,missing,device availability,incorrect functionality,add,macro checker,device is available,device,,,
pytorch,https://github.com/pytorch/pytorch/commit/a8653f35de02c7fb038e3c184dda6e67a12a39e2,Perf win by check which device tensors are on,2023-10-04 22:42:57,1,missing,device availability,performance bug,add,macro checker,device is available,tensor device,,,
pytorch,https://github.com/pytorch/pytorch/commit/fd6c993eeaacda7ef6b83f59ad3474aed0d52eaf,Add missing CUDA error check,2023-10-02 13:34:31,1,missing,device availability,performance bug,add,macro checker,device is available,tensor device,,,
pytorch,https://github.com/pytorch/pytorch/commit/8b37821813b60a3ce2ae92e7a06057183578a450,"if we want to use dp on other device ranther than ""cuda"", this balance  check will raise error, so I make the balance check only effective for `cuda`",2023-06-20 17:01:57,,insufficient,device availability,others,extend,macro checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/c1e51fcbfc70c089276530ee64fb626e3f7f4f2b,Relax tolerance for cuda accuracy check,2023-11-29 23:43:46,1,missing,device availability,incorrect functionality,add,if checker,device is available,floating point,,,
pytorch,https://github.com/pytorch/pytorch/commit/59a3759d9787091e75d939de603981a6d32505c8,"When we need to link extra libs, we should notice that 64-bit CUDA may be installed in ""lib"", not in ""lib64"".",2023-05-15 18:47:41,,missing,device availability,others,add,if checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/e856a4d66bead8997a83f8714547c09fcbcdc263, Add an env var to skip cudnn version compatibility check. skip the check by setting `PYTORCH_SKIP_CUDNN_COMPATIBILITY_CHECK=1`,2022-11-17 15:10:52,,missing,device version,version incompatibility,add,if checker,device version is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/cf256ee268c30d4ca965b38b45467cf7f738542f,Added tensor op check for cudnn rnns,2017-11-01 5:51:23,,missing,device version,version incompatibility,add,if checker,device version is valid,rnn cuda,,,
pytorch,https://github.com/pytorch/pytorch/commit/0fc110cdd19363f2eb5de68b6eeb82dadc933be0,"The bug in libcuda.so that required is fixed for libcuda.so versions >= 11.4.This PR changes replay() to sync after each launch only if the process's in-use libcuda.so is < 11.4. With all the ""enhanced"" and ""forward"" compatibility promises flying around, and the fact that ""driver"" sometimes means kernel-mode driver and sometimes means user-mode driver (libcuda.so), I wasn't sure if this PR's check suffices to trigger the sync iff the in-use libcuda.so is 11.4, but Cuda people say what I wrote is reasonable.",2021-07-08 16:26:07,,missing,device version,version incompatibility,add,if checker,device version is valid,device version,,,
pytorch,https://github.com/pytorch/pytorch/commit/ef44faece2cd4045f58cbbac6c74842b84ac6c45, check attribute existence in torch.legay.nn.SpatialFullConvolution in method type,2018-08-14 13:11:13,,insufficient,attribute existence,crash,extend,if checker,attribute exists,,,,
pytorch,https://github.com/pytorch/pytorch/commit/5c93ca258bab7bd74a8ec94d64647e48c8ad8797,check attribute existence in SpatialFullConvolution,2018-02-16 21:06:08,,insufficient,attribute existence,crash,extend,if checker,attribute exists,,,,
pytorch,https://github.com/pytorch/pytorch/commit/b31cf0ebd4ffc0e25801b4e40762266ad54721d6,Added support for nInputDim parameter in legacy Padding class. * Added support for nInputDim parameter in Padding class. * moved nInputDim to the end so as to not break backwards compatibilty. * hasattr to check if nInputDim is actually set. * check if nInputDim is positive before checking against input dim,2017-09-10 13:47:34,,missing,attribute existence,attribute error,add,if checker,attribute exists,,,,
pytorch,https://github.com/pytorch/pytorch/commit/c5fdcd85c7570b654eec45b6cba7cc75b0cf8f6b,"check pruned attributes before deleting. I copyed a pruned model after deleteing the derived tensors. In order to be able to reparameter the model, we should check the existence of the tensors here.",2020-07-23 18:56:48,,insufficient,attribute existence,attribute error,extend,if checker,attribute exists,tensor,,,
pytorch,https://github.com/pytorch/pytorch/commit/678c08bb55eef0c2e707a17d0cd6e50f5b9bd427,"_ProcessGroupWrapper check needs to be gated on Gloo availability, this fails when gloo is not avail_ProcessGroupWrapper check needs to be gated on Gloo availability, this fails when gloo is not avail.",2022-02-11 10:59:13,,missing,backend type,incorrect functionality,add,if checker,backend is available,gloo backend,,,
pytorch,https://github.com/pytorch/pytorch/commit/dd819138da4954eaf85ae095c860bdb094ae7321,add tensor vulkan check for at::cat ,2023-09-26 2:08:17,1,insufficient,backend type,incorrect functionality,extend,if checker,backend is available,vulkan tensor,,,
pytorch,https://github.com/pytorch/pytorch/commit/db1ac4e29b0f557711190c8d49d4afb5da1844e8,"Summary: We should explicitly check for the gloo backend instead of relying on the shard's device, because user might pass a GPU tensor as input and a process group gloo as the pg, and expect that should work.",2023-07-06 5:52:48,1,insufficient,backend type,incorrect functionality,extend,checker api,backend is available,gloo backend,,,
pytorch,https://github.com/pytorch/pytorch/commit/3ef4d697df5bfdbd27dfc7a79c0679da2b87e3af,"default backend need to check for nccl availability. As titled, we can only initialize nccl backend when NCCL is available",2023-05-30 15:22:37,1,missing,backend type,incorrect functionality,add,checker api,backend is available,nccl backend,,,
pytorch,https://github.com/pytorch/pytorch/commit/a24c11329a1bdfb00848b4af6dced5368622d637," Fix out-of-place allocations Summary: Also add int as a datatype and correctly check error codes on group start, end",2017-12-08 18:03:49,0,missing,backend type,incorrect functionality,add,checker api,backend is available,nccl backend,,,
pytorch,https://github.com/pytorch/pytorch/commit/6bf0e3b697ce688bc8325440dea3b51fea571c3d,"Check for BackendCompilerFailed on CI. random triton failure on CI, but we need to check against the BackendCompilerFailed exception type.",2023-01-03 17:38:29,2,insufficient,backend type,incorrect functionality,extend,checker api,backend is available,backend type,,,
pytorch,https://github.com/pytorch/pytorch/commit/9bb1371cc20a14907dbc47bc98e3ac5de866e34b,"Disable RDYNAMIC check with MSVC. Summary: When testing with clang-cl, the flag is added though it is unsupported and that generates a few warnings.",2021-08-18 14:51:23,,missing,platform compability,user confusion,add,if checker,others,msvc,,,
pytorch,https://github.com/pytorch/pytorch/commit/48e675ac7519666ed5e96d8d49c468dfc15a5d66,"fx quant: fix subtle bug in BinaryOpQuantizeHanlder logic in matching. When matching a pattern to `BinaryOpQuantizeHandler`, we need to make sure we check for dtype support on the base node, instead of the current node.  This is important in cases such as `add-relu` and `mul-relu`, when the current node is `relu`, but the base node is `add|mul`.",2021-04-16 21:19:22,1,missing,computation graph,incorrect functionality,add,checker api,nodes in computation graph,node type,,,
pytorch,https://github.com/pytorch/pytorch/commit/d01f8b291d437a37ec8809a18c1bb2ebfa825285,Fix visualize_overlap for Inductor comm reordering. The following assumptions are not always valid and need checking: 1. `snode.node` exists 2. `snode.node.layout.size` exists 3. `snode.node.layout.stride` exists 4. `snode.node.name` exists Also there is no guarantee that there won't be two collectives running at the same time. But it's hard to visualize the overlap in that case. So disable the visualization for that case for now.,2023-11-08 0:27:15,1,missing,computation graph,runtime error,add,checker api,nodes in computation graph,node attribute,,,
pytorch,https://github.com/pytorch/pytorch/commit/62732bdcdb8b6112e01366d4ad1c2a50e61da1ed,quick fix for invalid nodes. Summary: As title.Need to check whether node is valid before fusion,2023-09-13 21:40:49,1,insufficient,computation graph,incorrect functionality,extend,checker api,nodes in computation graph,computation graph,,,
pytorch,https://github.com/pytorch/pytorch/commit/aab55d6d0d7b958e32cfdbb69794e107cfceb6bc,"Remove all the dequant nodes when the ref module has multi input args. When converting a ref module into a quant module, `_lower_static_weighted_ref_module` pass assumes the `ref_node` only has 1 input node, and only remove the first `dequant` node. We add a check in this PR to ensure this is the case for `_lower_static_weighted_ref_module` pass.",2023-01-05 18:58:45,1,missing,computation graph,incorrect functionality,add,assertion statement,others,computation graph,,,
pytorch,https://github.com/pytorch/pytorch/commit/bae895cef0c12df5f64afa155ce5462e06f0e04a,Added check for kHIP in ATen/native/Copy.cpp,2020-05-15 4:40:48,,missing,device availability,incorrect functionality,add,macro checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/cf348bcdeecfe0b47a2245d95eaa8ef37fb7b53e,tighten hasCUDA check,2017-11-02 19:53:36,,missing,device availability,incorrect functionality,add,macro checker,device is available,cuda,,,
pytorch,https://github.com/pytorch/pytorch/commit/181b2481d338a24efc553378c837dcc48b656e3f,add error checking to grid sampler,2017-09-29 15:18:31,,missing,device availability,incorrect functionality,add,macro checker,device is available,cuda,,,
pytorch,https://github.com/pytorch/pytorch/commit/5054cb893485890c99af404cf81c0d5d8fef28ae,"fix torch.cat bug with boxed CPUFallback. The boxed fallback was written to assume that there was at least one tensor argument, which it used to figure out what device to move the cpu tensors to. That fails with an op like `torch.cat()`, which doesn't have any tensor arguments, but instead has a single `TensorList` argument. I also added handling to gracefully deal with the case where you have an empty list of tensors - in that case we don't know what device to move everything to, but that doesn't matter because an empty list of tensors implies that we have no tensors to move anyway.",2021-07-07 22:29:17,,missing,device availability,incorrect functionality,add,macro checker,device is available,tensor size,,,
pytorch,https://github.com/pytorch/pytorch/commit/027c0d7f8e37e583c02b372df5331d73793c06b1,"fixed compilations on xla tensor prin. This is done to avoid compilations during tensor printing. Torch performs some tensor operations like slicing to make the tensor readable. These operations result in compilations. Hence to avoid the compilations, copying the tensor to cpu before printing. Returning from this function would have resulted in 63 compiles, since PDB prints the value of the return output. In this case it is a xla tensor. Now with the current change, there is no compilation.",2022-01-26 21:28:19,,missing,device availability,incorrect functionality,add,if checker,device is available,device types,,,
pytorch,https://github.com/pytorch/pytorch/commit/ee91c328da5739ce03b3127cd7c542ce505212b8,Fix cuda/cpu check on NoneType ,2022-11-11 7:19:31,1,improper,edge cases,runtime error,update,if checker,tensor is not empty,none tensor,,,
pytorch,https://github.com/pytorch/pytorch/commit/cdab6c8df9ff9331126f69ea59c23f06109f03d7,Support specifying None for obs_or_fq_ctr in target_dtype_info. It is cleaner for quantizer to say what does not need observation instead of putting fp32 observers. This diff add support for that by checking if target_dtype_info contains none for specific observers and if so skip inserting observers for those.,2023-04-17 12:37:16,,missing,edge cases,numerical error,add,if checker,object is not empty,,,,
pytorch,https://github.com/pytorch/pytorch/commit/7f5737392d637a22d555a88a8546d8fc7ab31084,"fix for fsdp exec order pre fwd record. When the sharding_strategy is set to SHARD_GRAD_OP and forward_prefetch=True, during direct validation run, self.is_first_iter will always be True (because training=False, iter+1 is not executed). Additionally, the _pre_forward_order_index of the first handle entering the record_pre_forward function is 0. This causes the handle to have a False result in the if condition at line 166 when entering the record_pre_forward function again (the expected value should be True because _pre_forward_order_index has actually been assigned a value). As a result, the first handle is repetitively added to handles_pre_forward_order, leading to incorrect prefetching order.",2023-09-28 11:45:05,1,insufficient,edge cases,numerical error,extend,if checker,object is not empty,Control Variables,,,
pytorch,https://github.com/pytorch/pytorch/commit/87082bd025362a84fedcca02350963d21a950d12,Reduce single reader check time for inline_container,2023-11-09 17:02:28,1,missing,edge cases,performance bug,add,if checker,object is not empty,ReadAdapterInterface,,,
pytorch,https://github.com/pytorch/pytorch/commit/dcac4dd58edefb6951a60266e53d8767dc9be002,"Add int32_t range check in packed_accessor32 in PyTorch TensorBase.  Summary: As ajtulloch suggested, we can make tensor.packed_accessor32<...>() raise an exception if tensor.numel() > std::numeric_limits<uint32_t>::max(). Trade-off: run-time check overhead (one-time) when doing `packed_accessor32` accessor.",2022-10-02 23:29:08,,missing,edge cases,performance bug,add,macro checker,others,,,,
pytorch,https://github.com/pytorch/pytorch/commit/027e3b7910fade8950038fb5044a548319510600,check if source is None when using tensor out variants,2023-09-06 21:51:02,1,insufficient,edge cases,incorrect functionality,extend,if checker,others,tensor,,,
pytorch,https://github.com/pytorch/pytorch/commit/122587dcb41427f473b7833eaf254384919e06fc,Improve error checking for large model export. Summary: * Add error message when onnx model file path is not a string. * Add error message when model size exceed 2GB when large model export is not turned on.,2020-05-07 1:35:00,,missing,edge cases,incorrect functionality,add,macro checker,object is not empty,,,,
pytorch,https://github.com/pytorch/pytorch/commit/91066559a8c8e5978ed4de722317576b222267c5,truthy check for empty string in NameScope(). As in name. LATTE translation team moving some code from Python 2 to 3 uncovered a case where comparison between unicode and str types leads NameScope('') to prepend a separator to the beginning of blob names. This fixes it.,2018-01-20 0:34:09,,improper,edge cases,incorrect functionality,update,macro checker,object is not empty,,,,
pytorch,https://github.com/pytorch/pytorch/commit/e3542d2c12d8aaaccf8a53873e480c20dc6b7338,"avoid unnecessary call to empty_tensor_restride in empty(). Our empty benchmark makes this call unconditionally. If MemoryFormat::Contiguous is indeed a common case (or if workloads are likely to use a consistent-ish memory format), then I'd expect checking first to be a win.",2020-12-11 16:57:57,,missing,edge cases,crash,add,if checker,tensor is not empty,empty tensor,,,
pytorch,https://github.com/pytorch/pytorch/commit/45d5b3248b430aca70111316accd165954464589,"Fixed C++ BatchNorm pretty_print() with optional momentum. Summary : Inserted a check for the momentum and print  ""None"" in case is not defined.",2021-11-01 17:45:33,,missing,edge cases,crash,add,if checker,object is not empty,empty variable,,,
pytorch,https://github.com/pytorch/pytorch/commit/666ff0ae220e1a5c406b0bc5cd43283e1b18b38e,"Update _create_c10d_store to check port value. Port number is int in python, but needs to be uint16_t when called for TCPStore constructor.",2022-01-26 17:29:33,,missing,edge cases,numerical error,add,if checker,input argument is valid,integer range precision,,,
pytorch,https://github.com/pytorch/pytorch/commit/ba59d720cd5c5c81601b53d2c3397c46c1f87883,Change error message for torch.linspace(). Basically moves the error checking from the device-specific function to the native function.,2019-10-21 16:03:02,,missing,edge cases,incorrect functionality,add,if checker,input argument is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/babb28d2a3a755424f72518bc360d9f511a24463,"Change DHCECK to CAFFE_ENFORCE in softmax_with_loss_op.cc. Summary: Based on discussion on the post in Caffe2 users. Changing DCHECK that works only in debug mode to CAFFE_ENFORCE that throws exception and is a better option. Update: Also correct the check for label_data >= 0, did not check for all elements previously. Moved it to inner loop.",2017-07-25 0:52:30,,improper,edge cases,segmentation fault,update,macro checker,tensor indexing is within range,tensor values,,,
pytorch,https://github.com/pytorch/pytorch/commit/bc371a2cd03ce573f3ad4f7be141364136028905,"Add additional checks when tracing back during maybe share output observer function. Summary: Currently in `maybe_make_input_output_share_observers`  we trace back from a node to find the activation_post_process of the input node, we have internal use case which would error out during tracing back, this PR is adding a guard during this process to return False early when the node doesn't have any input",2022-04-12 20:33:49,,missing,edge cases,others,add,if checker,input argument is valid,computation graph,,,
pytorch,https://github.com/pytorch/pytorch/commit/7ddf167ba5db277e02f983a6bde2bc3f5fbe1caa,Move the asserts in shape functions upsample_nearest_2d op. The assert check are moved to top and the function now returns out. This is needed by the downstream torch-mlir project to correctly determine the output type.,2022-09-30 14:30:06,,missing,edge cases,incorrect functionality,add,if checker,object is not empty,,,,
pytorch,https://github.com/pytorch/pytorch/commit/3aeaa21eb02953a9cbc62b3e61215572fc28453e,"Revert ""Remove parent device mesh check",2024-02-02 19:22:56,,missing,edge cases,incorrect functionality,add,if checker,object is not empty,,,,
pytorch,https://github.com/pytorch/pytorch/commit/23631eee5ae484d8397769492b3ea36f9eca282d,"Fix the check of current scope in optimizer. scope.CurrentDeviceScope() can return a None type, which was not considered.",2018-03-19 19:38:55,,insufficient,edge cases,incorrect functionality,extend,if checker,object is not empty,,,,
pytorch,https://github.com/pytorch/pytorch/commit/5fc122bf3973504e619cd677ad4a7fc1011642cd,tensor.numpy() checks that no positional arguments are passed. * tensor.numpy() checks that no arguments are passed.,2017-10-24 17:54:28,,missing,edge cases,incorrect functionality,add,macro checker,others,positional arguments,,,
pytorch,https://github.com/pytorch/pytorch/commit/647154f82ac2c57769f080c41452b3e5960ab94f,"Assert tensor isn't sparse in enforce_invariants. There's no reason we can't check this, but I'm punting on implementing it for now.  But it currently segfaults, so this is an improvement",2019-03-25 11:44:17,,missing,edge cases,segmentation fault,add,macro checker,tensor type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/a6a433aecd0da3ac3c8d49cb36091623f1b5ec9e,Add stack emptiness checks inside interpreter.cpp,2023-02-13 16:00:00,,missing,edge cases,crash,add,macro checker,object is not empty,stack,,,
pytorch,https://github.com/pytorch/pytorch/commit/2c9dd886afc656a8bfe5d8bbcb601ee5877cee21,Modify torch.movedim to handle scalar as no-op. Summary: `torch.movedim` directly handle the case of a scalar tensor (0-dim) in input as a no-op by returning a view of the input tensor (after all the usual checks for the other parameters),2021-12-14 12:55:59,,missing,edge cases,crash,add,if checker,tensor dimension is valid,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/52e76a305677dfaf26cd5d59bd1aa239375f833c,"fix ShardedTensor.gather when shard is empty. current ShardedTensor.gather is not working as expectation when the shard is empty on any rank The root cause is identified that when a sharded tensor has no placement on a specific rank, the metadata doesn't include that rank's placement which introduces KeyError in :```shard_offset = shard_placement[shard. Metadata][1]``` It's fixed by adding an empty tensor check.",2023-10-11 17:26:41,1,missing,edge cases,incorrect functionality,add,if checker,tensor is not empty,tensor element,,,
pytorch,https://github.com/pytorch/pytorch/commit/d23231fd8cd50e4eb657eb7c3cf102475634f9c6,"Fix upgrader codegen when constant list is 0. Summary: When the constant list is empty, previous codegen will generate something like ``` std::vector<c10::IValue>({ }), // constants list, ``` However it will fail quick-check, because it includes trailing spaces. This pr will generate the following instead. ``` std::vector<c10::IValue>(), // constants list,",2022-02-02 19:41:03,,missing,edge cases,incorrect functionality,add,if checker,object is not empty,variables,,,
pytorch,https://github.com/pytorch/pytorch/commit/4ee179c9528c8c6aae17a01f2b0d7e8235219219,"Fix ConstantVariable init method if NumPy is missing. By adding `np is not None` check before `isinstance(value, np.number)`",2023-09-15 20:07:19,1,insufficient,others,incorrect functionality,extend,if checker,object is not empty,numpy,,,
pytorch,https://github.com/pytorch/pytorch/commit/ba766ef39a4fff2d8856e17747393d469e409775,Fix BN size check in eval mode,2017-10-04 16:03:20,1,missing,edge cases,incorrect functionality,add,if checker,object is not empty,input data,,,
pytorch,https://github.com/pytorch/pytorch/commit/b287cb816c1ac52165920a121c98643c08d31ff7, inductor: make the vec_transpose's tiling stride doesn't depend on out_idx and tiling_idex. ,2023-06-15 23:56:39,,insufficient,invalid return types,incorrect functionality,extend,checker api,return value is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/999bae0f54108ffc5b7cf2524a02a83901554b16,Add padding check for use_nnpack. nnp_convolution_output doesn't support the case of input padding > = kernel_size.,2023-05-30 1:07:59,,insufficient,invalid return types,incorrect functionality,extend,if checker,return value is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/ce3413549f5cc312f48c4e2a2f60c41674e26257,check cloned observer in RNN Executor. Summary: Ensure the clone() function didn't return a nullptr before attaching to an RNN operator,2017-11-16 16:30:53,,missing,invalid return types,crash,add,macro checker,return value is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/66b04e3cb707d30c4698b269c83cb6221848f17a,"nullptr profiling name. Sometimes profiling name can be a nullptr, which throws on conversion to std::string. This adds a check.",2023-12-14 18:40:54,1,missing,null pointer dereference,crash,add,if checker,object is not null,profiling_name,,,
pytorch,https://github.com/pytorch/pytorch/commit/14c47fb211bc929ae4e505e7e13411faa2339f00, fix invalid-null-argument UBSAN error in math_cpu.cc. Add an if statement to check if the destination buffer is not nullptr.,2018-03-06 3:33:11,,missing,null pointer dereference,crash,add,if checker,object is not null,,,,
pytorch,https://github.com/pytorch/pytorch/commit/eb8659fe81f3d4b061674bf149a6805cd292db8d,pass inference accuracy check for detectron2_fcos_r_50_fpn. We need a higher tolerance to pass the inference accuracy check for detectron2_fcos_r_50_fpn .,2023-08-31 16:21:20,1,missing,edge cases,inconsistent results,add,if checker,others,floating point,,,
pytorch,https://github.com/pytorch/pytorch/commit/c7748fc172553da66368fd0b7fea3fe5661e2dc1,Added validation of mode parameter in AveragedModel,2021-10-03 11:42:28,,missing,others,incorrect functionality,add,if checker,input argument is valid,optional parameter,,,
pytorch,https://github.com/pytorch/pytorch/commit/40a7c317bc60713528320b9786765e4ec5707982,Run BLAS F2C checks on host architecture,2021-06-24 21:44:41,,missing,platform compability,version incompatibility,add,if checker,others,BLAS F2C,,,
pytorch,https://github.com/pytorch/pytorch/commit/f77f88fbc7511b405c4e493bdd74634b633f63d1,"X86 qengine always uses fbgemm kernels on OS other than Linux. X86 quantization backend (qengine) with oneDNN kernels has not been validated on OS other than Linux. So, let it fall back to fbgemm if OS is not Linux. This makes sure the behavior is the same on Windows/Mac as the previous default fbgemm qengine on x86 CPUs.",2023-02-01 3:12:39,,missing,tensor quantization,crash,add,if checker,tensor quantization is valid,platform,,,
pytorch,https://github.com/pytorch/pytorch/commit/2a3ab71f285ee3ad10d39c716fe4ba90e7c849b2,Remove useQuantizable check for dynamic quant. Currently the input of batch_norm is considered as dynamically quantizable but it shouldn't be this PR fixes that.,2020-07-23 0:06:48,,unnecessary,tensor quantization,incorrect functionality,remove,if checker,tensor quantization is valid,quantization,,,
pytorch,https://github.com/pytorch/pytorch/commit/acd51e13f727af22e6c9e579518362898f1b12e6,TorchScript add check if quantized,2020-02-11 20:38:49,,missing,tensor quantization,incorrect functionality,add,if checker,tensor quantization is valid,quantization,,,
pytorch,https://github.com/pytorch/pytorch/commit/0584fd9339af7c939ab7d955db05743ba58ff86d,"Only insert observers for fixed qparam ops. Fixed a condition check for fixed qparam ops, previously we were including CopyNodes as well",2021-03-10 19:51:36,,improper,tensor quantization,incorrect functionality,update,if checker,tensor quantization is valid,quantized,,,
pytorch,https://github.com/pytorch/pytorch/commit/97245a06e14a5b87a0bca1908d7200603aff2c8c,"TORCH_INTERNAL_ASSERT_DEBUG_ONLY won't be enabled during non-debug builds, but for 1 dimension Tensors the check is cheap enough and not catching this can slow down development a lot.",2023-05-04 11:06:27,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/9bcb4de16878073896d8743fbd70d5abe28b595a,check parameter k and l ,2022-05-03 7:50:36,,insufficient,tensor shape mismatch,crash,extend,macro checker,tensor dimension is valid,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/65496e4e67b42e52b3428b0cf2d994e0aa1a9902,Bug fix in bound shape inferencer. Accessing dims() without boundary check is not good.,2019-04-25 17:50:19,,missing,tensor shape mismatch,crash,add,if checker,axis parameter matches tensor dimension,,,,
pytorch,https://github.com/pytorch/pytorch/commit/260f66c3165ce0c48dd1514a916da6971d981578,Fix concat dimension check bug,2019-02-21 22:34:30,,missing,tensor shape mismatch,crash,add,macro checker,axis parameter matches tensor dimension,,,,
pytorch,https://github.com/pytorch/pytorch/commit/dc07102b17915f21170fae9a9d52c6f2d59726ca,"Check dim size preventively when doing shape inference for BatchMatMul. We check input(0) but not input(1) in BatchMatMul. This may result in a protobuf exception which won't be caught by upstream and causing termination of the program. Check that with `CAFFE_ENFORCE` will be caught by upstream inference function. Plus, it will print out clean stack tracing showing where went wrong.",2018-10-16 20:27:44,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/8af88f3525e1908deb9ac181ebfb9f8eb49bcb46,Add ADD operator for IDEEP. Add boradcast check,2018-06-06 23:20:33,,missing,tensor shape mismatch,crash,add,if checker,axis parameter matches tensor dimension,,,,
pytorch,https://github.com/pytorch/pytorch/commit/74783f0cd83e8ef1fd32cfd04f36da8ddfc52f57,Move the broadcast check in MKL Add/Sum to runtime ,2018-05-31 0:09:32,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/77523df413ff7f8a336b6481cfa47967c234a149,Add more check on softmax ONNX exporting logic. * Add more check on softmax exporting logic * Add more comments about axis and dim,2018-01-11 15:14:33,,missing,tensor shape mismatch,crash,add,if checker,tensor dimension is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/8d377617e73a399e5b8838cf96dfd2896576ac52," Fix MKLMemory::CopyTo for case where shapes don't match' Summary: There were cases where the direct copy succeeded, but the dimensions didn't match. Now, we check dimensions and reset if they don't match before issuing the copy.",2017-11-02 14:25:49,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/8f26d6aabcad991da88b663467ee2080a38631f7, More shape checking for ConvNd.* check conv weight & bias dims,2017-10-13 10:56:19,,missing,tensor shape mismatch,crash,add,if checker,axis parameter matches tensor dimension,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/a53f4b0f9bbc007c0a92e4fd28dd22af027e24a8, add dimension check to NHWC2NCHW shape inference. Summary: To prevent assertion from protobuffer when accessing the dims.,2017-07-27 12:54:44,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/ecd3c252b4da3056797f8a505c9ebe8d68db55c4,Suport all length one SLS op lowering: C2 part. We check the input shape of lengths and indices of SLS and add an attribute if they are the same.,2020-02-14 1:53:11,,missing,tensor shape mismatch,crash,add,if checker,tensor dimension is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/55092b1cc604fad3d70d31e71bbdd3a43a279423,Validate matching input shapes in Int8Add operator. Default engine doesn't support broadcast semantics in Int8Add operator. This patch adds a check that shapes are equivalent.,2018-12-05 15:00:23,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/1359d16fe8ca0cb7041674c455f2f99a9636fec0,Further tighten the checking of two eager runs. Summary: To catch nondeterminism in eager if there is any.,2023-03-05 9:53:02,1,missing,tensor execution mode,incorrect functionality,add,checker api,tensor is executed in eager mode,execution mode,,,
pytorch,https://github.com/pytorch/pytorch/commit/41902a6ebc1806e7f4d6ce1da604cc9921c6515e,Optimize is_tracing checks ,2024-01-29 3:31:26,1,missing,tensor execution mode,performance bug,add,checker api,tensor is executed in eager mode,execution mode,,,
pytorch,https://github.com/pytorch/pytorch/commit/1f819ee965894b8332cb364a67c91855c91c9dcc,"Add check for no grad in transformer encoder nestedtensor conversion.  Before, we allowed inputs with grad to be converted to NestedTensors. Autograd attempts to find the size of the NestedTensor, but NestedTensor throws an exception for its size function. This causes all calls to nn.TransformerEncoder with grad enabled to fail. Fix: we add a check for no grad in transformer encoder so we do not convert tensor with grad to nestedtensor.",2022-06-03 16:14:32,1,missing,gradient status,inconsistent results,add,checker api,tensor is tracking gradient,gradient,,,
pytorch,https://github.com/pytorch/pytorch/commit/dc43ad428603539a2051940c09b191825f66203d," add is_grad_enabled check in runtime_wrapper before running with torch.no_grad. We observed that `with torch.no_grad()` in runtime_wrapper introduced ~10% (0.06ms->0.066ms) inference performance regression on lennard_jones on cpu. For inference tasks in benchmark, grad has been disabled, but in the current runtime_wrapper, no_grad is set again and its time is counted into the running time. Therefore, we add `is_grad_enabled` check in runtime_wrapper before running with torch.no_grad. If grad has been disabled, there is no need to set no_grad. ",2024-01-10 22:37:45,1,missing,gradient status,performance bug,add,checker api,tensor is tracking gradient,gradient,,,
pytorch,https://github.com/pytorch/pytorch/commit/f118d20bea9188db1bd053dd1d1af1b32479183e,Make requires grad check run only when grad mode is enabled,2021-06-28 13:40:30,1,missing,gradient status,inconsistent results,add,checker api,tensor is tracking gradient,gradient,,,
pytorch,https://github.com/pytorch/pytorch/commit/c1c4882367c592d49e15268a0b99631c207d662e,"Based on discussions with Sherlock + Zhengxu in D51118067, updated the internal thrift schema to match the OSS schema.. So to bypass these failures I did the following hacks(?): Before creating the exported program in deserialization, populate nodes w/o meta[""val""] with meta[""val""] = None * Add torch.autograd.grad_mode.set_grad_enabled to the skip opset * Duplicated ExportGraphSignature into aot_export.py so that the graph signature checks will be skipped",2023-11-16 2:42:30,1,insufficient,tensor operations,incorrect functionality,extend,if checker,tensor operations are valid,tensor operations,,,
pytorch,https://github.com/pytorch/pytorch/commit/af3cbfed9510747c776418c260c5116f662c6452,"Add validation check in fx2trt interpreter. Add validation check in fx2trt for missing converter operators. If any op missing, interpreter init will report missing operators.",2021-08-18 13:41:10,,missing,tensor operations,incorrect functionality,add,if checker,tensor operations are valid,computation graph,,,
pytorch,https://github.com/pytorch/pytorch/commit/f3a2094065c8b4b7bae426e71c923a8a8abb74b5,"Mitigate legacy issue that aten op as export entrance function. This is not supported any more, now the top level ```torch.export``` only support ```nn.Module```, but there are still some tests using the internal APIs and caused the ```trace_rules.check``` assertion error. This PR is going to mitigate such cases.",2024-02-09 13:24:09,,insufficient,tensor operations,incorrect functionality,extend,if checker,tensor operations are valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/cc6a51c9f3ee97a06ff9c0b84477e88e33e31137,added shape checking to WeightedRandomSampler,2022-06-02 17:12:14,,missing,tensor shape mismatch,runtime error,add,macro checker,tensor shape is valid,tensor shape,,,
pytorch,https://github.com/pytorch/pytorch/commit/8ee59280d78a4fefc4de0da04b287e067c28de0d,Bug - check config for dynamic,2023-04-21 2:40:09,,insufficient,tensor shape mismatch,incorrect functionality,extend,if checker,tensor shape is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/41ad221751e57c2d2ccc82b431f56d6ed62e1741,MHA: fix contiguity assumption in transform_bias_rescale_qkv. This code path incorrectly assumed input tensors were contiguous. Now we check that.,2022-02-16 13:36:21,,missing,tensor shape mismatch,inconsistent results,add,macro checker,tensor size is valid,tensor size,,,
pytorch,https://github.com/pytorch/pytorch/commit/e9e125475e94aabfb34ee239fadc760615eef429,Add schema check to aten::repeat and fb::fast_gather,2021-05-12 1:07:21,,missing,tensor shape mismatch,inconsistent results,add,if checker,tensor size is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/7ea6559658a6f650363f8b96f462bbc047e29124,Add size checks to torch.stack. Checks the size of each tensor passed to `torch.stack` before calling `cat` to address #29510. This is done in the `get_stack_input` function as that is a common path. The function now compares the size of each tensor in the TensorList to the size of the first tensor and throws an exception when the sizes are not equal.,2020-02-04 18:00:54,,missing,tensor shape mismatch,runtime error,add,macro checker,tensor size is valid,tensor size,,,
pytorch,https://github.com/pytorch/pytorch/commit/bbb5e106ad6228953df6c7f5c8916b26dc0cb457,Improve error checking of CUDALoops. Same change as was applied to CPU loops -- separate out checking of the inputs and outputs.,2020-05-27 10:41:02,,improper,tensor shape mismatch,runtime error,update,macro checker,output and input tensors has same size,,,,
pytorch,https://github.com/pytorch/pytorch/commit/b7882f9bd65de5a4c60f625d56186b583c1d6842,"Improve cpu/Loops.h arity asserts.  This splits the asserts into separate input/output asserts and makes the numbers precise, instead of ranges. This is an ongoing effort to improve the Loops assertion and to integrate dynamic cast checking into CPU loops.",2020-05-27 10:38:58,,improper,tensor shape mismatch,runtime error,update,macro checker,output and input tensors has same size,,,,
pytorch,https://github.com/pytorch/pytorch/commit/d3bf6803b62c79f1dafd1eec49b4bd65d5a27697,add sanity check that we do not wrap tracked tensors ,2023-10-27 13:15:03,1,missing,tensor shape mismatch,incorrect functionality,add,assertion statement,tensor type is valid,tensor,,,
pytorch,https://github.com/pytorch/pytorch/commit/794e3971ab90611b4a63166589368a737843c8bc,Add size check before calling stack_.at(dict_pos) in unpickler.cpp,2023-05-02 14:50:31,,missing,out of bound access,crash,add,macro checker,others,,,,
pytorch,https://github.com/pytorch/pytorch/commit/7684044b713761abd4f51225dc5d83ce5869562a,Add size check before calling .back() in rpc/script_call.cpp,2023-04-28 20:26:35,,missing,tensor shape mismatch,segmentation fault,add,macro checker,tensor size is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/c69b3b8d4f484cf537d98974a3a4143b77edf3c8,Autograd engine use current device only. In this PR a check upon CUDA devices in device registry is added such that threads set the same CUDA device.,2023-03-13 16:04:12,,insufficient,device availability,incorrect functionality,extend,if checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/fb25fd6f865ed0532caf710ca130b6cc23a772a8,Replaced neg dim normalization with assert in helper. I think we can still leave the check for negative shard dimension in `compute_local_shape_and_global_offset` and replace the normalization logic with an assert. This should provide us a stack trace to see which user-facing API did not normalize the dim as expected.,2023-11-20 20:24:21,1,missing,misleading error message,segmentation fault,add,assertion statement,improve error message,,,,
pytorch,https://github.com/pytorch/pytorch/commit/9e314bd8224f93b4ba1f9e4c065150e47a2de2cc,"handle the case where output of op is Optional[Tensor]. some op might have Optional[Tensor] returns where it return None (i.e. native_layer_norm_backward), it's a mismatch between C++ aten op signature and python None, but we need to handle it in the python side",2022-12-06 13:17:20,1,missing,tensor operations,incorrect functionality,add,if checker,tensor type is valid,tensor type,,,
pytorch,https://github.com/pytorch/pytorch/commit/55c19a3c6d38a49fe34e008c4c566445c43810f0,"Increase multiplier to 3 for Inductor AMP benchmark correctness check. we find some of the models have failed the benchmark's correctness check. However, the end-to-end model's accuracy. when comparing AMP with FP32 is within a difference of less than 0.1%. Thus, it's possible that the correctness check failures for these models are false alarms. We use multiplier of 3 instead of 2 in this PR to avoid these false alarms.",2023-09-16 6:02:56,1,missing,type checking,inconsistent results,add,if checker,object type is valid,multiplier,,,
pytorch,https://github.com/pytorch/pytorch/commit/6e78592cbb81138ce13ad65a5f549d65b191526c,Added type checking for ExportedProgram. Added type checking for ExportedProgram in save function. ,2024-01-24 13:24:44,2,missing,type checking,incorrect functionality,add,type checking api,tensor type is valid,ExportedProgram,,,
pytorch,https://github.com/pytorch/pytorch/commit/442d7d72def17dba46f0b95c55c6a028428be0bc,"fixed type checking errors in options.py. This PR fixes the type checking errors in torch/distributed/rpc/options.py. The variable types in 84:8 and 85:8 were  declared to have type `List`  but were sometimes assigned a value of  `None`. This caused an incompatitble variable type error. Therefore, I changed the type from `List` to `Optional[List]`. Hence, this fixes the incompatitble variable type error.",2021-11-09 14:42:34,,improper,type checking,type error,update,type checking api,tensor type is valid,variable,,,
pytorch,https://github.com/pytorch/pytorch/commit/828a6a3b3943a0a0701ecacacd2bcc34fc03fe03,Use proper isnan check ,2019-03-31 5:08:11,,missing,type checking,incorrect functionality,add,checker api,tensor type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/47c531b6e80e36282dbaec60d239ae1b9f816f43,"Compare object identity first in ClassType::operator==. This check is much cheaper than anything involving actually inspecting object fields (i.e., the cost is low), and if it succeeds we can skip the expensive (e.g., it involves locking a weak_ptr and then destroying the resulting shared_ptr)  function body. It almost entirely eliminates time spent in this function during model loading according to perf.",2021-10-12 13:49:36,,missing,type checking,performance bug,update,if checker,others,object,,,
pytorch,https://github.com/pytorch/pytorch/commit/3611d26a25bd889627403a808ea667ac99c09904,Optimize FunctionSchema::checkArg for the Tensor case. The Tensor case is one of the most common and the existing check can be made faster. This results in a ~21% improvement on DeepAndWide model and would improve other models as well.,2020-11-16 23:50:24,,missing,type checking,performance bug,add,checker api,tensor type is valid,tensor data type,,,
pytorch,https://github.com/pytorch/pytorch/commit/324dc1623e2f91892038fb1b151450a7c6529dd9,"add dtype checking for gather and scatter. in the `cpu_scatter_gather_base_kernel`, it interpret a pointer as `int64_t` regardless the actual dtype. add a index dtype checking will avoid the nasty index out of bound error. As using `int64_t` is convention in ATen code (a.k.a, a limitation), no further fix is needed at the moment.",2020-05-11 2:15:45,,missing,type checking,crash,add,checker api,tensor type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/5e50993be72bec4ad939993328dd02691ef7777d,Better type checking for pack_padded_sequence symbolic,2018-05-26 11:16:41,,missing,type checking,unknown,add,checker api,tensor type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/6c91610f0c86153232bf3a66f3a23e42b96e79b6,Check if profiler is disabled in push/pop event. Make sure to check if profiler is disabled in push/pop and mark event functions. ,2019-04-07 18:06:04,,missing,type checking,incorrect functionality,add,checker api,tensor type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/68ad9ae5bebd9efab127fa99e2bafd6852bbd8ed,"Ensure there aren't variables in checked_tensor_unwrap, checked_tensor_list_unwrap. These functions use unsafeGetTensorImpl(), which doesn't work with Variables (in a silent way that may blow up later). So let's do early checking.",2018-12-12 12:58:03,,missing,type checking,incorrect functionality,add,checker api,tensor type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/df475aa1dc4310abc273cf26b14b6ac1cdb7dfa4,"Update Vulkan runner in benchmark binary to handle non-tensor inputs. Some models may take in a list of tensors as inputs, thus the bundled inputs will contain `IValues` that are of the type `c10::List`. For Vulkan models, every tensor in the `IValue` list has to be converted to a vulkan tensor first, and this case is not currently handled by the Vulkan model wrapper in the benchmark binary. This diff introduces `IValue` type checking to the input processor of the Vulkan model wrapper, and adds support for Tensor and List types.",2021-10-05 10:59:56,,missing,type checking,incorrect functionality,add,checker api,tensor type is valid,tensor type,,,
pytorch,https://github.com/pytorch/pytorch/commit/e574a8ab55b2ac4266211d30d98d32d8b849ea86, Add sanity checks to ensure no double-wrapping of `FakeTensor`s produced by the current graph,2023-10-24 21:18:32,1,missing,type checking,incorrect functionality,add,type checking api,others,,,,
pytorch,https://github.com/pytorch/pytorch/commit/468a73f0e3527c52495c864c7d48dc26684f6c0b, Support Numpy ints in the torch.nn.functional.interpolate dtype check. This PR updates the check to also include numpy integers,2023-10-09 21:46:33,1,missing,type checking,type error,add,if checker,object type is valid,numpy object,,,
pytorch,https://github.com/pytorch/pytorch/commit/e31038d574712d383fdc4c2f1bb63fc82f256ed0,"Check results dtype in index_out. This logic exists for index_put and index_add, but for some reason not for `index.out` Skip testing, as this function is not technically exposed on the Python level.",2023-08-30 10:55:18,1,missing,type checking,incorrect functionality,add,macro checker,tensor type is valid,tensor data type,,,
pytorch,https://github.com/pytorch/pytorch/commit/a69f427f957a37eee9c1dd5df681f30ab38ed3e4,aten: Ensure dim is size_t,2023-06-26 18:01:27,,improper,type checking,runtime error,update,macro checker,tensor dimension is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/8340762211e3b55caa178bac748bd902249f6fc0,Update lr_scheduler.py to check the type of eta_min. Add float assertion to `eta_min` parameter in `CosineAnnealingWarmRestarts`.,2023-06-13 22:13:05,,missing,type checking,incorrect functionality,add,type checking api,object type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/2dafa70d61a1a5af849ab79c7aed4c84686337a0,Add a little more error checking to minifier,2023-06-07 10:40:12,,missing,type checking,unknown,add,type checking api,object type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/ae55619a2bb73ebcdc80b02a6ccd72275a9ce23e,Add check for same dtype in tensordot implementation,2023-04-14 12:57:35,,missing,type checking,incorrect functionality,add,macro checker,tensor type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/b6920405da340bbd3397b80bf16d9c360b0c48d2,reorder checks to shave 1 us off no-op dispatch time ,2022-05-25 22:27:33,,improper,type checking,performance bug,relocate,if checker,tensor type is valid,python object,,,
pytorch,https://github.com/pytorch/pytorch/commit/152f665dee05377f7459d985d60dc1edb782d40e,Inserted check for PyObject_IsInstance in THPVariableCheck. Summary: Inserted check for the return of PyObject_IsInstance to capture the case in which it raises an exception and return -1. When this happen THPVariable_Check now throws a python_error to signal the exception.,2021-11-01 19:53:54,,missing,type checking,others,add,if checker,object type is valid,python object,,,
pytorch,https://github.com/pytorch/pytorch/commit/18eeccc7e8cba16d71efdd2eca831983c4abde15,Fix Optional type check,2021-08-03 19:00:55,,missing,type checking,incorrect functionality,add,assertion statement,object type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/402be850a8946e8967dedb3375fc6f33b379b397,Adding zero point type check for per channel quantization,2020-07-12 14:40:19,,missing,type checking,inconsistent results,add,macro checker,tensor type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/4dad00b64b396ef81f16bdb896175688fc629f4d,special case tensor type check when getting RRef ,2020-02-26 21:44:40,,missing,type checking,incorrect functionality,add,macro checker,tensor type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/4cc6e6bbbe1fb114e7d7fb207ef2deb567950102,Adding scalar to the c10 registration type check,2020-02-01 16:15:50,,missing,type checking,type error,add,if checker,tensor type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/4b1ebd2f65e49d251ac2cfdb635794c7c6eb362f,"Fast path for serializing large floating-point tensors to protobuf. Summary: Our existing serialization routines take a significant amount of time for large numpy arrays in order to verify the type of each element in the array as well as converting each element to a canonical type.  For large floating-point tensors, such as model parameters, this checking and converting takes a significant amount of time.  Adding a fast track path for just float32 arrays as this is the most common use case to worry about.",2017-07-10 20:52:22,,missing,type checking,performance bug,add,if checker,others,fp32,,,
pytorch,https://github.com/pytorch/pytorch/commit/65dfe1203ffab064d4e32fa8f76833042369d2f5,add an assertion to check the param num. Introduce this check to see whether it will break any existing workflow,2019-04-03 15:47:23,,missing,type checking,inconsistent results,add,assertion statement,object type is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/5a962369e2b527b36a737723df1fe9c180aa2925,"Check if the backend is NCCL when a DDP communication hook is registered. Some unit tests actually register a comm hook on other backends like GLOO. Example: `test_ddp_comm_hook_future_passing_cpu`. Therefore, only do the check on `register_builtin_comm_hook`. Currently DDP communication hook can only be supported on NCCL. Add a check in the registration methods.",2021-02-05 12:59:12,,improper,backend type,incorrect functionality,update,macro checker,backend is available,nccl ,,,
pytorch,https://github.com/pytorch/pytorch/commit/fa66a1498eb1fac5b36811d5c1d6ba1540ffc824,Summary: The code checking `if dimensions == 2` is not needed because the case of a 2D tensor (Linear) is already handled by the statement: `receptive_field_size = 1` and this conditional: `if tensor.dim() > 2:`,2019-11-07 18:53:05,,missing,tensor shape mismatch,crash,add,if checker,tensor dimension is valid,,,,
pytorch,https://github.com/pytorch/pytorch/commit/4fd98dfe69287914fd29b38fbccaf7ac4d7261ee,"Don't only apply DDP optimizer on forward frames. Previously a check would only apply DDP optimizer on frames named ""forward"".",2022-10-17 17:55:14,1,unnecessary,others,crash,remove,if checker,others,ddp module,,,
pytorch,https://github.com/pytorch/pytorch/commit/8a644f0c136cb12cf200050c2ae6875ec487d174,Summary: Sometimes first dim of X in FC is BATCH_OF_FEATURE_MAX instead of BATCH. This caused an issue in f207899183 (when first dim of X is 64 but is set to 1 in inferFC). Change the check from `!= BATCH` to `== UNKNOWN`,2020-07-28 19:43:19,0,improper,tensor shape mismatch,crash,update,if checker,tensor dimension is valid,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/bdc8b3f3e828ca7202879baa379fda6df5b078d2,"Re-route arithmetic ops to scalar versions when second arg is zero-dim. When arithmetic ops are invoked from torchscript the scalar argument will sometimes be wrapped in a zero-dimensional tensor, which will cause the Vulkan implementation to complain as all input tensors are expected to have the same number of channels. The solution is to have the Tensor implementations of the op check if the second argument is zero-dimensional and re-route it to the Scalar implementation if that's the case.",2022-02-18 20:33:51,2,missing,tensor shape mismatch,crash,add,if checker,tensor size is valid,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/5023995292f5119c447de15c20a375b7e3aa2d0b, fix output size adjustment for onnxifi_op. Summary: this breaks if we cut the net at certain int8 ops boundary.,2020-08-05 18:55:46,0,improper,tensor shape mismatch,crash,update,if checker,axis parameter matches tensor dimension,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/24601daa1203a9ad1232e1d18a07ff4842d53d27,Adding check for a single batch in adaptive_avg_pool ,2019-07-24 0:11:06,1,insufficient,tensor shape mismatch,crash,extend,if checker,tensor size is valid,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/a1edf5f63c62d88230d1f7feb26edb059551ae71,"Do hook sizes check with SymInt. I don't think this matters for any uses right now, but I found it during an audit; might as well fix it.",2023-03-22 19:26:00,1,improper,tensor shape mismatch,crash,replace,if checker,tensor size is valid,tensor size,,,
pytorch,https://github.com/pytorch/pytorch/commit/7553c495147f3e21a1e27d392d277906a47768e7,"Fix distributed debug w/ non-equal split. In collectives, it's possible to have non-equal split that has a different implementation and the output tensor size will be different.  However, TORCH_DISTRIBUTED_DEBUG=DETAIL will assume the output tensor size is the same and does the check and will fail the job if they don't. Ideally we should check the input size across ranks and make sure they're the same. Maybe for next diff.",2023-12-12 13:02:05,1,missing,tensor shape mismatch,crash,add,if checker,output and input tensors has same size,tensor shape,,,
pytorch,https://github.com/pytorch/pytorch/commit/35be57970143236d74661f2415d66d496aab5476,"Refactor TENSOR_MATCH guards to check dim (for NT support). Tweaks the TENSOR_MATCH guard logic to avoid saving sizes / strides for the case of dynamic shapes. Instead, the dim() is stored, which is enough for both dense tensors and NTs.",2023-03-29 19:08:03,,improper,tensor shape mismatch,crash,update,if checker,tensor dimension is valid,tensor shape,,,
pytorch,https://github.com/pytorch/pytorch/commit/8dda19b79f2c4418f481a9f56932b3b5c5afdf39,Remove extraneous TensorId checks in as_strided ,2019-05-29 3:53:53,,unnecessary,device availability,performance bug,remove,macro checker,device is available,,,,
pytorch,https://github.com/pytorch/pytorch/commit/9d20af50608b146fe1c3296210a05cd8e4c60af2,remove overly restrictive checks for cudagraph,2022-07-06 14:08:49,,unnecessary,device availability,performance bug,remove,assertion statement,others,cuda graph,,,
pytorch,https://github.com/pytorch/pytorch/commit/f6e137598ddf0b990c423b1d6412502b62e095b2,"ns for fx: fix nit in default qlinear weight extraction function. Removes the assert for node type in default qlinear weight extraction function. Without the assert, user defined functions can now use this util function without failing this check.",2021-07-28 19:07:13,,unnecessary,computation graph,unknown,remove,macro checker,others,computation graph,,,
pytorch,https://github.com/pytorch/pytorch/commit/5a20c56ebce3426397210e91693fbbeade8b46ba,"emove hasOperation() check. by removing the hasOperation() check, the Operation gets successfully materialized, and static runtime enables successfully and runs ok. Will check that the outputs match with jit interpreter",2021-07-12 13:09:33,,unnecessary,computation graph,performance bug,remove,macro checker,others,,,,
pytorch,https://github.com/pytorch/pytorch/commit/012829eb3657aff2d58cead0bd166089c6e90c7f,"Do not crash when target device is unsupported by fuser. The `canFuseOnDevice` function now crashes when the device is not covered (i.e., CPU, GPU, XPU). However, now we have some devices, such as XLA and Lazy, that could perform fusion by themselves. This checker then prevents these devices from working on the models partially implemented in `jit.script`. This PR proposes to remove this checker and simply return false for all uncovered cases.",2022-03-09 11:44:56,,unnecessary,device availability,crash,remove,macro checker,device is available,device,,,
pytorch,https://github.com/pytorch/pytorch/commit/5017c5fcad5889acba1623c634d0aaeadb676aa0,"Remove _specify_ddp_gpu_num method. As SPMD mode is gone, `_specify_ddp_gpu_num` becomes useless. It only checks if the module is a GPU module. This actually is already checked by the caller of this function (in fairscale and some other codebases). Additionally also remove `enable_pytorch_sync_bn` wrapper that only calls this function and does nothing else.",2021-04-20 14:17:47,,unnecessary,device availability,unknown,remove,checker api,others,device count,,,
pytorch,https://github.com/pytorch/pytorch/commit/3f1f057adfcd4cef67fff9605a894cb075c02881,Remove parent device mesh check. Removes raising error if a device_mesh has a parent.,2024-02-02 0:29:49,,unnecessary,edge cases,crash,remove,if checker,others,,,,
pytorch,https://github.com/pytorch/pytorch/commit/9d09968bbe05fc6d7d7c3d8b1acfbe1b1b1413a8,"Disable check for dropout in MultiheadAttention fast_path. Since we already enforce eval mode for the fast_path, we do not need to also check for a falsy dropout value, as a model trained with dropout will have a non-zero dropout during eval mode, even though it won't be applied.",2022-11-10 22:34:57,,unnecessary,edge cases,inconsistent results,remove,if checker,others,,,,
pytorch,https://github.com/pytorch/pytorch/commit/f386312ec936a94bfb1abe44acdd61d498f4272b,"Don't do extra numel() check in TensorImpl::data(). `is_empty()` checks `numel() == 0`, but we don't need to access `numel_` at all (or the policy that `numel()` checks) in our happy path -- we just need the data pointer from `storage_`. Let's do the check we need to do using only the data we strictly need, rather than adding instructions loading other pieces of data.",2023-04-03 20:59:52,1,improper,use of unnecessary checkers,performance bug,update,if checker,others,input data,,,
pytorch,https://github.com/pytorch/pytorch/commit/fc304bec9f550075d4c899aa0fc5b1a0a573c1e5,Remove redundant checks from canHandle in TE fuser,2020-08-14 2:49:40,,unnecessary,computation graph,performance bug,remove,if checker,nodes in computation graph,,,,
pytorch,https://github.com/pytorch/pytorch/commit/f810d96806d0e767aeca9fe9cf50e0bdcaab7d52,"emove redundant index check for index_select_out_cpu_dim1_. For  **index_select_out_cpu_dim1_**, there has a redundant idex check, **check_indexarray_range** has checked  **the index>=0 and  index < slect_dim**, we don't need re-check it at copy step.",2022-03-14 16:29:58,,unnecessary,tensor shape mismatch,crash,remove,if checker,others,tensor dimension,,,
pytorch,https://github.com/pytorch/pytorch/commit/63e47c68a692c70bc64c49d687f85f7f5cd02ce3," remove checks from embedding bag impl. These checks incur an H2D sync on every embedding bag forward. Also, the equivalent python code for embedding_bag does not have them",2023-01-25 15:36:44,,unnecessary,tensor shape mismatch,incorrect functionality,remove,macro checker,others,tensor indice,,,
pytorch,https://github.com/pytorch/pytorch/commit/5b7c72101ca8e9d4edba1d16b6121ad900ca3936,Removed check for is_quantized in dequantize_cpu_or_cuda. This particular PR isn't dispatcher related but does remove the extraneous torch check for a quant tensor since the dispatcher already handles a quantized backend for this particular function,2022-02-03 10:36:15,,unnecessary,tensor quantization,performance bug,remove,macro checker,tensor quantization is valid,tensor quantization,,,
pytorch,https://github.com/pytorch/pytorch/commit/8da704cdb7f68bfa09516e7be17f004b98c48eb3, Remove incorrect asserts from Copy.mm. Those asserts simply do not work for views.,2022-10-04 15:01:48,,unnecessary,use of unnecessary checkers,incorrect functionality,remove,macro checker,others,,,,
pytorch,https://github.com/pytorch/pytorch/commit/c9b1e09958c2b914659aa6b33a76b0e53bd94431,"delete lengths offset checks. NCCL supports up to size_t send counts, so PGNCCL shouldn't restrict it",2023-04-05 17:06:49,2,unnecessary,use of unnecessary checkers,incorrect functionality,remove,macro checker,others,tensor indices,,,
pytorch,https://github.com/pytorch/pytorch/commit/2597d5d72272d196b4cb5442ffc8cde376d1f785," TorchDynamo: always convert flexiblelayout to be FixedLayout when given a stride_order. For convolution, we always call **require_stride_order** to convert the input to the target stride order,  if the original input's layout is flexiblelayout, there always have a memory copy because the **is_stride_order_storage_and_layout** only checks the init stride order,  I think for flexiblelayout, means it's layout can be changed, if the user gives a stride order, I think we always need to convert the flexiblelayout to be FixedLayout using given strider order.",2022-12-05 22:07:53,,insufficient,type checking,performance bug,extend,type checking api,others,none tensor,,,
pytorch,https://github.com/pytorch/pytorch/commit/201f7d330ac8c33a7bedb8f0a66954415d1d27db,Remove duplicate check in distributions arg validation ,2021-11-03 13:41:41,,unnecessary,type checking,performance bug,remove,type checking api,others,parameter types,,,
pytorch,https://github.com/pytorch/pytorch/commit/a47cc18254ade6dee1fe4a3c4eb5aca7ba40c77c,remove unnecessary tuple check on tensor types,2022-06-24 13:44:31,,unnecessary,type checking,incorrect functionality,remove,type checking api,others,,,,
pytorch,https://github.com/pytorch/pytorch/commit/2512017814fb2e3d6f3ae9dd3b315692ffc8fc71,Fix for out of bounds read in torch mobile flatbuffer loader ,2023-09-22 10:26:33,1,unnecessary,out of bound access,crash,remove,macro checker,others,module buffer,,,
pytorch,https://github.com/pytorch/pytorch/commit/b3a7d696b37b502d1688741eb3339da64c23ab93," remove unnecessary asserts. Removes asserts which are duplicate (the same condition is checked when calculating the hook type, so there is no need to check it again). For the assert in `validate_is_at_last_seen_idx`, rewrites it to raise an Error instead to ensure it does not get stripped in production environments.",2021-11-21 10:08:09,,unnecessary,use of unnecessary checkers,performance bug,remove,assertion statement,others,quantization operation,,,
pytorch,https://github.com/pytorch/pytorch/commit/e7fc7c732cbde822f9490840704b1f57fe86c50a,Bugfix for fusion device check,2019-04-22 23:55:17,1,insufficient,device availability,incorrect functionality,extend,if checker,device is available,device,,,
pytorch,https://github.com/pytorch/pytorch/commit/4e90526a4f03d1950e8db6e8722cce8e0fb4a5f5,"Remove unneeded checks. these checks aren't really doing anything, as they just make sure we're setting training state in certain ways throughout FSDP and is sort of arbitrary. So, removing them to avoid confusion. We still keep the checking around `_post_backward_called` because this is needed in `finalize_params` for now.",2022-08-22 22:38:23,1,unnecessary,use of unnecessary checkers,user confusion,remove,if checker,others,N.A,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/4a343043dbc6ce229b4dcf2258f7b6352db32b64,Fix hasattr check in saved_model_cli ,2017-08-16 18:50:56,,improper,attribute existence,crash,update,checker api,attribute exists,model,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/73f25fc34c69878c83ee2eeb8f030cb79a76472f,This fix tries to address the issue by adding an `hasattr()` check so that AttributeError is not thrown.,2018-04-05 19:52:07,,insufficient,attribute existence,attribute error,extend,if checker,attribute exists,module,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/cdc36a3b1f7d227984bb5e415b555ed334737f82,cosmetic fix to MonitoredSession.__del__ AttributeError. This CL prevents this message by checking that the underlying _sess object has the __del__ method defined before calling it.,2017-12-07 14:46:42,,missing,attribute existence,attribute error,add,if checker,attribute exists,method,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/67b6c880e39ba02ba53c7d499e45fd136090ee32," In tf.map_fn: skip sanity check for shape of first value in elems if it doesn't have a shape attribute. (E.g., this can happen if it's a CompsiteTensor.)",2021-09-21 10:34:46,,missing,attribute existence,attribute error,add,if checker,attribute exists,tensor shape,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/dcdca11bcbab4b2474e7bf4d21d1806e6c2790a3," Check both output name and output slot in duplicate scope id sanity check. Before this change, we would throw an error if different outputs of a node were committed to different scope ids.  Since that is legal, this change fixes the bug by making the check based on both output name and output index.",2019-08-14 22:46:37,,insufficient,others,incorrect functionality,extend,checker api,others,computation graph,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a607eb012b1bc4f6dbe263ad99caa76d84ae3ab2,fix output shape check for strided slice always failing when stride != 1 ,2019-08-12 13:38:29,,insufficient,tensor shape mismatch,incorrect functionality,extend,if checker,axis parameter matches tensor dimension,N.A,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/2bf2799ee80791107d4fe587ff9b6c7cf6c8b418,"Fail gracefully if the serialized graph would be too large. Without this explicit check, a large graph would trigger an assertion failure in the protobuf codebase",2018-06-07 19:51:41,,missing,others,others,add,if checker,others,computation graph,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/0197a2d8a3070af763cb67227835ee63df095e6d, Add a check to catch out-of-bound access on invalid Graphs. The existing Check trying to catch malformed graph is not robust when an op is registered with an expected number of inputs but has data edges beyond this.,2019-09-02 19:57:38,,missing,out of bound access,crash,add,if checker,others,computation graph,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/314d9cd9b607460f8bfea80fc828b1521ca18443,Fix segfault in MacOS when GPU is not available ,2016-07-21 17:25:35,,missing,null pointer dereference,segmentation fault,add,if checker,object is not null,device driver version,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/abd645085b1dd1496df847b05a1934d471a2f2c0," Use the correct device ordinal to check whether the device the executable was built for is equivalent to the device the it will run on. Before this patch, if the device to run on was provided via a stream without setting the device ordinal in the ExecutableRunOptions, we would check the default device against the device the executable was built for.",2018-08-01 4:06:22,,missing,device availability,incorrect functionality,add,if checker,device is available,device status,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/083fd8c4b23104f6b27a871c6469629ace4ee9c3, Don't check soname on Windows. This allow users to specify a certain CUDA version on Windows again.,2019-03-04 10:10:05,,insufficient,device availability,others,extend,if checker,device is available,accelerator version,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/867a918bd3d40afeca6b96430671a098134e7905," CUDA Driver: do better error reporting if checking the pointer properties failed. There are many reasons why an operation can fail, propagate the error instead of assuming the cause.",2019-05-01 22:18:07,,missing,device availability,incorrect functionality,add,macro checker,device is available,device driver version,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1a73fdfa83bd50695a7d374d14a5cb3835d94d9e, Add extra check incase segmenter does not exclude CPU in order to prevent segfault,2019-01-09 17:41:51,,missing,device availability,version incompatibility,add,if checker,device is available,device attributes,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b234ff0ee4ce87d21a3e5306b678e1fb4b1fedfc," Fixed division by zero, by checking the number of GPUs in GenericLayoutOptimizer.",2019-08-06 12:59:40,,missing,device availability,numerical error,add,if checker,device is available,device status,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/6c472f6632c4864da749e7a4aee8c001a905287f," so suggest to use check `CUDA_VERSION` at `12030` here, for `maxSize`, resolved directly in the same way",2023-12-08 16:36:13,,improper,device version,version incompatibility,update,if checker,device is available,accelerator version,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9f8ad5ff118166537d42f87f1ee254f83ba553f0,Fix CUDA version check (format is 1000 * major + 10 * minor). ,2021-06-08 13:15:31,,improper,device version,version incompatibility,update,if checker,device is available,accelerator version,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/89334fb15c688e7dbd81878745755db01579ea70, [NVIDIA] Update VersionCheck APIs for CuDNN. This PR updates the cudnnXXXVersionCheck to the latest for the next CUDNN release.,2024-01-05 5:07:14,,missing,device version,version incompatibility,add,if checker,device is available,accelerator version,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e5cfbd0eceb4dca98b388b13acff499a5420f863,Fix more for cuda version check. ,2018-04-22 23:00:54,,improper,device version,version incompatibility,update,if checker,device is available,accelerator version,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/ac012e26d4331919335d4bceb8abe22b68ed5434,Don't assume GPU clients are PJRT-compatible. Instead explicitly check for compatibility.,2023-11-17 5:07:13,,improper,device version,version incompatibility,update,if checker,device is available,device status,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/6b9189483513b0c663e23485834be64f51b076e4, Add device compatibility check for fusion. _FusedMatMul is not supported by GPU currently.,2022-04-19 18:46:27,,missing,device version,version incompatibility,add,if checker,device is available,device status,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e009644f034fa0ca4df910a812432cab3458d440,Add one error check in cuda_dnn for int8 to float convolution. ,2019-08-16 0:20:41,0,missing,device version,version incompatibility,add,if checker,device is available,accelerator version,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e1dbfeba8acb1df8f42dfa6f76262f5cb23e1fa1,[stream_executor] NFC: Guard new features with CUDA_VERSION check ,2023-11-28 23:24:49,,missing,device version,version incompatibility,add,if checker,device is available,accelerator version,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e05f78a9b688a8ae37b1a03bfc4459e18e3b88e4,"After synchronizing CUDA device, check for errors. ",2017-06-14 1:19:53,,missing,device version,version incompatibility,add,if checker,device is available,accelerator version,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/70ade1b64f65d0a2275672d27129627ff116a997, Fix defect: shuffle_batch gives ZeroDivisionError when computing capacity stat. * Fix defect: shuffle_batch gives ZeroDivisionError when computing capacity stat. * Cover < case in error checking,2017-06-13 19:58:21,,missing,others,others,add,if checker,others,divisor,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1220ba3ab332d6233a84d660cafb3d4e29958224,Fix two potential asynchrony bounds-check bugs in transpose op. ,2016-03-18 11:47:58,,improper,out of bound access,crash,update,checker api,tensor indexing is within range,tensor index,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a21ec782601aca6c7e0461093d72596f26229e44, Use getattr instead of isinstance in tensor_conversion_registry. Using `isinstance` to check if an object is an instance of a Python `typing.Protocol` instead of using `getattr`/`hasattr` has negative performance implications. This change reverts `tensor_conversion_registry.convert()` to use `getattr` for this reason.,2023-03-02 11:50:47,,improper,attribute existence,performance bug,replace,checker api,attribute exists,N.A,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/15c186bffe51901e4a48b4b6bf1316832533743f,Correctly handle the case if static maximum dimension size = 0. ,2020-12-04 21:17:13,,improper,edge cases,crash,update,if checker,tensor dimension is valid,none shape,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e1ad3b74ad44b883c7b3fdc3a19adcea1d28bfbc, [XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast is an effective scalar. This short-circuit avoids crashing within last_dimension when attempting to match and either the operand or the result of the bitcast has a shape with rank 0.,2023-07-17 7:14:46,,improper,edge cases,crash,update,if checker,tensor dimension is valid,scalar tensor,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/3c80be9f2cfece929f5858e7df0e7f4503c9baec, [tf.data service] Support num_consumers being a Tensor. The `if num_consumers >= 0:` check causes an error when num_consumers is a Tensor in graph mode: OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Comparing to `None` accomplishes the same thing without this issue. Once the forward compatibility window expires we can remove the check completely.,2021-01-14 19:01:39,,improper,edge cases,others,update,if checker,tensor is not empty,none tensor,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/2f3b69e4976d3b14eaa6ae070eb68f37d1556d98,Changed empty check ,2018-10-23 16:30:07,,improper,edge cases,crash,update,if checker,object is not empty,checkpoint object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/5ed3c7881f1f039b1bb502eb68c65250de3bbac8,"Fix ThreadPoolHandle 0 nthreads argument. It was reported that a value of 0 leads to a check failure.  Using 0 to indicate `port::MaxParallelism`, for consistency with `Dataset`.",2023-02-08 11:56:20,,missing,edge cases,crash,add,if checker,integer argument is valid,zero integer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/6381a7b127bd276a3817a93e5423b15a06c33419, [tf.data] Add a check for ram_budget == 0 to avoid division by 0 exception when ram_budget is not set.,2021-11-15 17:00:13,,missing,edge cases,numerical error,add,if checker,integer argument is valid,zero integer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/7db8e4fbc0be952daea74a2c3f501183d6006e61, ENH: check x and y is empty dict,2017-09-23 2:51:33,,improper,edge cases,crash,update,if checker,object is not empty,dictionary,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a0fe44410e875e8e7775c6c256496bafb1a41b25, Remove the check of NodeItem exists in unfinished_nodes_ in node callback. This fixes the failure of RemoteAsyncTest.test_out_of_range_with_while_loop in DEBUG mode.,2020-03-02 12:07:54,,improper,edge cases,crash,replace,macro checker,integer argument is valid,zero integer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/bd1f1ac1fec05d38f1b8fc98f650c1c55ac06790,Fix operator check ,2020-10-06 9:04:33,,improper,edge cases,incorrect functionality,update,if checker,object is not empty,multiplication operator,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/5e0c9fff657498f9a74da38b2ce1b4721698a388, Add bounds checks to jpeg parsing code. ,2016-12-01 12:46:02,0,missing,edge cases,crash,add,if checker,tensor indexing is within range,tensor indicies,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/cfb13fa789bcf1cdbbf0fd38cf7568b7098ab99b, Added an additional check on the length of the values and boundaries lists.,2017-10-17 17:01:03,0,missing,out of bound access,crash,add,if checker,tensor shape match tensor indices,N.A,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/28dacabab5aac2963e37e622f4b157cf00d82662, [tf] Explicitly check that runner index is in bounds and runner is available,2021-11-17 10:06:17,,insufficient,misleading error message,user confusion,extend,macro checker,improve error message,tensor indicies,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/eb921122119a6b6e470ee98b89e65d721663179d,Prevent heap OOB read in TFLite's gather.cc. Passing negative indices is illegal but there was a missing check so that resulted in OOB accesses.,2021-07-27 20:20:51,,improper,out of bound access,crash,update,macro checker,tensor indice is valid,negative index,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/bb6a0383ed553c286f87ca88c207f6774d5c4a8f, Prevent heap OOB read in TFLite's gather_nd.cc. Passing negative indices is illegal but there was a missing check so that resulted in OOB accesses.,2021-07-27 18:25:33,,missing,out of bound access,crash,add,macro checker,tensor indice is valid,negative index,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/7535f6beb7ba95bf54e1513b0c2c51b844a7a49f, Bounds-check node ID before getting it's name. When the edge is either a frame enter or exit edge then DescribeCycle() would segfault.,2018-04-06 12:56:07,,missing,edge cases,segmentation fault,add,if checker,others,computation graph,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/22783fdf812b700f7de9980038ab41ee0a4a2284,Add checks recently removed ,2021-04-30 18:28:52,,missing,out of bound access,crash,add,macro checker,tensor indexing is within range,tensor indicies,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/459b4bfe1f73737fae23aa1499b06a69605d0f65,Added a check in EagerExecutor to avoid getting invalid range. ,2020-10-26 20:56:11,,missing,edge cases,crash,add,if checker,tensor indexing is within range,tensor indicies,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/cddca76312f5ae4fb92a101e79eeff6d5ac16932,Add check for reading input tensors at an index that is out of range. ,2020-08-03 14:41:16,,missing,edge cases,crash,add,macro checker,tensor indexing is within range,tensor indicies,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/823b694639a3f49b6adbf9e73a08c529d583878e,Add bounds checking when looking at the stack in TF Registry. ,2019-01-29 4:48:40,,missing,out of bound access,crash,add,if checker,tensor indexing is within range,tensor indicies,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b1c9e600e02b93885dbebfa5dae92436c63d6c03,[XLA] Add range check for xla::Array<> indexing. ,2021-02-11 11:43:58,,missing,out of bound access,crash,add,if checker,tensor indexing is within range,tensor indicies,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1908d7ef706f0f3f8c7a300068355bf795fb3d17,"Fix out-of-bounds StringPiece access in ForwardNUTF8CharPositions().  Even a simple invocation like 'int p = 0; ForwardNUTF8CharPositions(""a"", 1, &p);' will cause an invalid access to in[1]. Checking for *pos < size before that access fixes this issue.",2022-11-14 4:23:12,,improper,out of bound access,crash,update,macro checker,tensor indexing is within range,tensor indicies,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/d414a925a73553e4dd0d559d2d275668a298dab4, Check against the size of a std::vector to prevent out-of-boundary access.,2020-11-18 5:49:04,,missing,out of bound access,crash,add,macro checker,axis parameter matches tensor dimension,N.A,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/aa54f547f04c3007b26df2379c6cf5f081948d0b, Updated the check_numerics function to also validate the gradient corresponding to the tensor it's validating,2016-08-18 12:16:11,,missing,edge cases,incorrect functionality,add,checker api,return value is valid,NaN value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/236660d0fccff6f59f29a1936dc731d783722e28," [XLA:GPU] Fix host conv checker canonicalization for f16 and nans. The GPU-side checker is correct, but the host-side checker was canonicalizing nan to F16_MAX.  The effect of this is that you'd get a ""conv mismatch!"" error but no description of exactly what mismatched.",2019-03-29 18:32:56,,missing,edge cases,incorrect functionality,add,checker api,return value is valid,NaN value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/8c6f391a2282684a25cbfec7687bd5d35261a209, [lite] Add check for bias_size is zero to avoid division by zero. This shouldn't happen for properly converted models. Just safety check,2021-12-14 16:45:48,,missing,edge cases,numerical error,add,if checker,integer argument is valid,zero integer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/582bf0d3ac33fc10156f737c0d42f3adee54409a," Update the tflite model ""buffers"" field checking rule. If we don't use ""--force-empty-vectors"" flag[1] for flatc, the buffers might be a null ptr if we serialize a model with zero buffers size(e.g. all ops in model doesn't use const weights in model). This commit relaxs the ""buffers"" null ptr checking for this situation, and also updates the ""subgraphs"" checking for null ptr dereference.",2021-03-12 2:39:10,,insufficient,edge cases,crash,extend,if checker,tensor is not empty,tensor buffer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/7008e41f183ae9de3f4656067932b36afa822ef2, Fix the check for empty reduction indices. In the general case indices can be any rank.,2019-04-22 22:11:37,,missing,edge cases,crash,add,if checker,tensor dimension is valid,zero dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/551a90f2e3d20420d68a2796d19f1c42b6636e0d," Add checks in ReduceWindowOpOnTensorsConversion. The pattern does not support ops with non-zero padding config. Add a check to prevent unexpected lowering. It is not easy to add tests because other patterns will convert body ops, and it causes issues like invalid IRs.",2021-04-07 8:50:41,,missing,edge cases,crash,add,if checker,tensor indice is valid,zero indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1071f554dbd09f7e101324d366eec5f4fe5a3ece, Add missing validation to RaggedTensorToSparse. There needs to be a check that the splits allow for valid ragged tensors.,2021-07-29 21:27:51,,missing,edge cases,crash,add,if checker,tensor indice is valid,zero indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/f6f62119587baf8ccb7378ceac86bacd2db2863d,Add missing validation in maxpooling_op.cc ,2021-05-05 19:09:09,,missing,edge cases,crash,add,if checker,tensor is not empty,zero element,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b86513673b98ac6c4458033fcda718365539afae,added check for zero stride values to strided slice ,2019-08-13 14:32:38,,missing,edge cases,crash,add,if checker,tensor indice is valid,zero indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/4377a561b7757ed83757f07532e6564c42c286ba, Add a check for group size when sorting grouped AllReduces within a block.,2023-06-06 15:17:00,,missing,edge cases,crash,add,if checker,tensor is not empty,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/31bd5026304677faa8a0b77602c6154171b9aec1, Prevent check fail in FFT ,2021-05-04 20:46:54,,missing,edge cases,crash,add,if checker,tensor is not empty,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1c56f53be0b722ca657cbc7df461ed676c8642a2,Fix a check fail in Fast Fourier implementation ,2021-05-04 20:18:39,,missing,edge cases,crash,add,if checker,tensor is not empty,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/25bae42b3022b00788a29ae6c400922c31f88231, Add additional length check for inputs ,2018-11-27 21:46:06,,insufficient,edge cases,crash,extend,if checker,tensor is not empty,zero list,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e07e48b2e0908333a36f1c5726a9406a83b3ec90,Added a check on literal_.has_value() to avoid segfault. ,2021-03-15 16:04:19,,missing,edge cases,crash,add,if checker,object is not empty,empty object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/cc560f64b6e3e6724517757e9789c52cde224ee9, Profiler: restore correct behavior of StartTracing with empty workers list. absl::StrSplit behaves differently from str_util::Split when the passed string is empty. Restore previous behavior by explicitly checking for an empty string.,2019-12-13 20:22:18,,missing,edge cases,crash,add,if checker,object is not empty,empty string object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/fe4f74018ec6a7dff2718ea59d0f317460c0b3ad, Temporarily check for empty proto fields to avoid a crash for old cached traces. We can remove this code once we land all the necessary changes and invalidate all the caches.,2023-01-04 15:08:26,,missing,edge cases,crash,add,if checker,object is not empty,empty proto object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c56d0cd8ce8239ee369fac1ae6b9cae67fd4c447," Avoid signed integer overflow when loading tensors with both 0 and large dims. `TensorShapeBase` ensures `num_elements` doesn't overflow when adding a new dimension. However, this check is insufficient to prevent other functions that use a different multiplication order from hitting an overflow _if any of the dimensions are 0_. For example, Eigen currently multiplies dimensions in reverse order, so dimensions of (0, 4294967296,4294967296) will trigger an overflow in Eigen code. To prevent overflow for all multiplication orders, we can that `num_elements` doesn't overflow if zero dimensions are skipped.",2022-09-29 15:48:46,,missing,edge cases,numerical error,add,if checker,tensor size is valid,zero tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/80bb2f5511e7d2d386c79da52ff517691e19ac54," Add check condition for large values of range_max, which is causing session abort.",2022-05-24 11:40:36,,missing,edge cases,crash,add,if checker,tensor indice is valid,large indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/0666d8bb711b41c9f03dec238d7d165bc946fc70, Prevent crash of tensorflow if shape is too large for tf.sparse.reorder. This PR tries to address the issue raised in 45392 where tensorflow crashes if shape of sparse tensor is too large for tf.sparse.reorder. This PR adds additional checks and exit gracefully if the shape is too large.,2020-12-14 18:11:16,,missing,edge cases,crash,add,macro checker,tensor shape is valid,large shape,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e6390bc13471f28f211cab874cc49a123505dc3e, Update histogram_ops.py. Added the condition to check the negative value of nbins input,2022-01-28 18:22:05,,missing,edge cases,crash,add,if checker,integer argument is valid,negative integer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/43a8963c73718f97a4425722a65b611d2ef0b69f,Added non-negative check for n. ,2019-05-23 19:04:02,,missing,edge cases,segmentation fault,add,if checker,integer argument is valid,negative integer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/4ea68093eeaf4c4157368668afd7f809b806a504,Add negative parameter validation to convolution layers. ,2021-04-23 14:38:51,,missing,edge cases,crash,add,if checker,integer argument is valid,negative integer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1223335a8d34a8ce656dbd10b2a236ef6204ff47,Add negative parameter validation for recurrent layers. ,2021-04-23 14:35:07,,missing,edge cases,crash,add,if checker,integer argument is valid,negative integer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/64afe2d199ec4513223bbf5176835bf681cf056b,Add negative parameter validation to Core Keras layers. ,2021-04-23 14:28:05,,missing,edge cases,crash,add,if checker,integer argument is valid,negative integer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/199f1ff12a28d571100b323ec54a5eee47078d8b, Add necessary check in fft ops to fix crash. This PR tries to address the issue raised in 55263 where tf.single.rfft2d will crash when length contains negative value.,2022-03-17 11:04:54,,missing,edge cases,crash,add,macro checker,tensor indice is valid,zero indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/685418cd85e09bc2117fa15bc1b6a75d21248348,maxpooling op should check that ksize must be positive. ,2023-03-30 1:25:19,,missing,edge cases,segmentation fault,add,macro checker,tensor indice is valid,negative indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/076f909b70b251daea6c443c9b1929b9745aed20,fix boolean expression in length check ,2023-03-06 10:36:43,,improper,edge cases,segmentation fault,update,macro checker,tensor indice is valid,negative indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/3acc8eaf602b3e9a009f54e1e0164644dd793831,Add sanity check for resize-bilinear input shape. ,2019-04-28 4:06:23,,missing,edge cases,crash,add,macro checker,tensor size is valid,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/fffbe5a26da2d6fab5a3eb648cefef49db4d38de," Check if the session has been deleted before releasing a callable. In some versions of Python, the Session._session field may be cleared (in `Session.__del__()`) before a callable that has a reference to that Session is deleted. Add a defensive check in the `Session._Callable.__del__()` method.",2018-04-12 18:22:37,,missing,edge cases,unknown,add,if checker,object is not empty,none object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/dcce6044dc05ed2e6cda601df5b300333859be4f,CLN: not check None ,2017-09-19 1:15:43,,improper,edge cases,crash,update,if checker,object is not empty,none object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9ce847ed140702d1dd4cb204a8afe0ffedb70b15," Remove a few check ops that no longer need to run in tf.Variable's constructor. VarHandleOp ensures there is no sharing. These aren't a huge part of startup time for replicated models, but there's still no reason to run them.",2020-10-02 19:04:32,,unnecessary,edge cases,incorrect functionality,remove,if checker,object is not empty,none object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/ebeb598c2d1f341d6d641bf58c370cf7b43f6e37," Correctly check shape not None in Keras add_weight. When calling Keras add_weight with a np list, as written the `shape or ()` ""trick"" results in the following exception: """"""ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"""""" This change fixes the problem by using an explicit `if`.",2019-03-01 20:31:59,,missing,edge cases,unknown,add,if checker,tensor is not empty,none object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c7c4a42c4372ca560ea415fe3a798e18286cedec,Fix an error in keras input_layer.Input() dtype type checking. ,2019-01-07 16:38:08,,improper,edge cases,crash,update,if checker,tensor is not empty,none object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/bc7b64fe998cb0f118eace5bc29b52554eeda3f1, Added back the channel dimension check as a known channel dimension is required by creating beta.,2017-03-17 22:49:05,,missing,edge cases,crash,add,if checker,tensor dimension is valid,none dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a5b8d6c4694e4cd3e3cc4a162053ab0dfa6e174f,Relax the check for whether the relevant aggregation dimensions are known ahead of time.,2016-12-02 13:20:52,,insufficient,edge cases,inconsistent results,extend,if checker,tensor dimension is valid,none dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/0d65cfaab050295c311d9f2fb28388435359db27, Add an additional NoneType check when converting a traced tensor to a `KerasTensor`.,2020-10-12 14:00:39,,insufficient,edge cases,incorrect functionality,extend,if checker,tensor dimension is valid,none dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/31849c61e0432009baabdfafc2ec1a1aed1a40e8," Small change in tf.nn.sufficient_statistics to guard against unknown shapes. Use is_fully_defined instead of checking shape.dims[d] as the dims variable may be None, if the rank is unknown.",2019-03-22 15:14:52,,insufficient,edge cases,crash,extend,if checker,tensor dimension is valid,none dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/30bd9d5bcc64097d21872486a5726d756ed7067b, Explicitly handle Tensors in start & stop. The current check was doing a identity check in order to handle both tensors and integers. This becomes problematic when enabling tensor equality. Instead we explicitly check for Tensor type and only compare with sys.maxsize for non-Tensors.,2019-07-31 18:32:21,,insufficient,edge cases,unknown,extend,checker api,ensure tensor type is valid,tensor type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/fb1c4cd8283f262bca95ccd04df6f9eb4ae1da0c,Add None check for seq_len_mask before reshape.,2017-03-17 16:44:35,,missing,edge cases,crash,add,if checker,object is not empty,none object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a0ca4bcb81dfd07fdb1c7872b5852f84cfc1a081,Fix separable convolution bias check,2017-03-16 19:48:57,,improper,edge cases,crash,update,if checker,object is not empty,none object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1ff493ed1a2059f82f7607a7f0a0aa2ce8d5a542,Replace a defensive check with TF_RET_CHECK,2019-05-03 15:05:12,,improper,edge cases,crash,replace,macro checker,device is available,device status,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/201982013046116767545cda18137b38abb39468,toco: Fix missing check for buffer in ResizeBilinear.,2018-01-08 16:33:45,,missing,edge cases,crash,add,if checker,tensor is not empty,tensor buffer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c676a2d7ce8884aad59ca9cd5f45e9b851574cac, [tensorflow] Add a check that strided slice op strides argument has reasonable size,2022-09-30 14:21:09,,missing,edge cases,crash,add,if checker,tensor dimension is valid,large dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/f61175812426009a4c96e51befb2951612990903,To add a check of input_dims greater than zero in embedding layers. ,2020-03-26 14:25:22,,missing,edge cases,crash,add,if checker,tensor dimension is valid,zero dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/f2a673bd34f0d64b8e40a551ac78989d16daad09,Add missing validation to matrix_diag_op.cc,2021-07-30 22:12:41,,missing,edge cases,crash,add,macro checker,tensor size is valid,zero tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a0dc73569fc193c1ce26a7bd2d4a8776e7b813ac,add check for empty cs_prev_tensor,2023-03-22 1:07:32,,missing,edge cases,crash,add,macro checker,tensor dimension is valid,zero dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/48d3e51a1bd128554dd129251a51b6e12918a604,Add a check to HandleFromInput to ensure that the resource isn't empty. ,2023-02-16 17:56:06,,missing,edge cases,crash,add,if checker,tensor size is valid,zero tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/258233804f2bc92b4bdb9714b396aed34b53ff0d, sanity check of empty tensor on avgpool3d_grad,2022-11-10 12:37:36,,missing,edge cases,crash,add,if checker,tensor size is valid,zero tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/532f5c5a547126c634fefd43bbad1dc6417678ac,Prevent nullptr deref in validation of indexes in map ops. ,2021-07-30 1:54:59,,improper,edge cases,crash,update,if checker,tensor size is valid,zero tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/102cacf28ad5a9e7f00b5a195d1995ead8870006,Add missing validation to maxpooling_op.cc,2021-07-30 1:31:35,,missing,edge cases,crash,add,if checker,tensor size is valid,zero tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a3d9f9be9ac2296615644061b40cefcee341dcc4, Add missing validation to pooling_ops_3d ,2021-05-05 18:24:00,,missing,edge cases,crash,add,if checker,tensor size is valid,zero tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/27bd8aaa7b58d2591fed43a6c245f3037664cfb1,Fix another Eigen missing validation ,2021-05-03 23:40:32,,missing,edge cases,crash,add,if checker,tensor size is valid,zero tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/ab0a5278d81ef34096775d5d56f11694cca2a785," Fix tf.assert_equal issue when one tenor is empty and another is non-empty. This fix tries to address the issue raised in 32082 where tf.assert_equal([], [1.0]) doesn't raise error. The reason was that in assert_equal `[1.0]` was broadcasted as `[]` and equal was in place in that situation. This PR updates the _binary_asesert so that it will check if x, y are both empty or both non-empty. If one is empty and another is non-empty, then assertion throws exception. This change is to not impact other ops that depends on the broadcast behavior.",2019-10-05 3:18:42,,missing,edge cases,crash,add,if checker,tensor size is valid,zero tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/dedac5053f1ca2d6a7820e330714e50d2d724cee, Fix edge case bug in handling FP16 weights in XNNPACK delegate. Quasi-static tensors may become subgraph outputs after partitioning; we need to explicitly exclude them from outputs and treat as static tensors.,2020-05-28 0:45:36,,missing,edge cases,crash,add,if checker,tensor size is valid,zero tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/ce589223a5fa78cb12efaf1efd1d8d0e5507bd08, Update nn_ops.py. Added check for pooling_ratio,2020-08-13 12:04:20,,missing,edge cases,incorrect functionality,add,if checker,tensor indice is valid,zero indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/63feaf321165e1e2795f43e3834c007364921df6,Add check for raster bits.,2023-09-29 7:53:21,,missing,edge cases,crash,add,if checker,tensor indice is valid,negative indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e5b0eec199c2d03de54fd6a7fd9275692218e2bc, [lite] Add validation check for dilation height/width to be positive integers.,2021-12-14 20:13:47,,missing,edge cases,segmentation fault,add,macro checker,tensor indice is valid,negative indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/5cedb0427bd4db4117182da8bc0680dd555b4f49,Add checks for dilation_rate. ,2018-09-26 13:37:15,,missing,edge cases,segmentation fault,add,macro checker,tensor indice is valid,indice with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/55aec0a33011773240f6696393952c984ca8de16, Add explicit not-None checks for the height and width in `resize_images()`. This was previously raising a `FutureWarning` when the height and/or width were dynamic.,2016-03-08 20:20:28,,insufficient,edge cases,crash,extend,if checker,tensor dimension is valid,none dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/ab60b0ee51a8924a0f02b0152cd6a78ba64d3e94," [tfg] Fix named-attribute token check. Since the name tokens are being indexed directly, we should check that list of tokens is not empty to prevent an out-of-bounds error.",2023-02-22 14:57:10,,missing,out of bound access,crash,add,macro checker,object is not empty,string object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/08370e76523d5bece9ab28e7a9a902932e9a2cb9," Fix small issues with DispatchToVectorized. - Changes the base case of template recursion from VecSize=0 to   VecSize=1, because VecSize=0 will never be reached. - Adds handling of `alignment_of(zero/nullptr)`, which should be treated   as infinitely aligned. - Adds checks for preconditions.",2022-04-20 8:55:36,,missing,edge cases,crash,add,if checker,tensor indice is valid,zero indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c6899c721f3a4b4f2e71ae4e6d1767341112ff93,bug fix when iterators stops at multiple of batch_size ,2016-10-06 19:38:54,,missing,edge cases,unknown,add,if checker,tensor indice is valid,zero indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/66e0cb1d9afd251931f4f920c5d7bd638bc882b4, validate clip_norm argument in clip_by_norm API. The API clip_by_norm have argument clip_norm which accepts  0-D (scalar) `Tensor` > 0 . But if we pass -ve value for this argument then its not raising intended error and converting the input tensor into Negative which IMO is wrong. Hence I am adding validation code for -ve values to raise value error.,2023-07-11 5:43:29,,missing,edge cases,incorrect functionality,add,if checker,integer argument is valid,negative integer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/d5862d423742ec26c46737d4526eca3b8b8a0d9b, [TFLite] Add check in Softmax reference function to ensure exponent is within valid range. * Add check to ensure the exponent does not cause an overflow in gemmlowp::RoundingDivideByPOT,2024-01-26 9:54:39,,missing,edge cases,numerical error,add,macro checker,tensor indice is valid,zero indice,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/81ff894c113a5912ba52078ac27e36d06831112e,"[XLA] Add bounds checks to xla::Array::Slice. To guard against specifying limits that are out of bounds, which ends up touching OOB data.",2023-11-01 7:43:01,,missing,tensor shape mismatch,crash,add,macro checker,axis parameter matches tensor dimension,array property,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b7e107eaa6dffb649d055d893a1fce734ee50d55, [XLA:GPU] Error out for ptxas version - Update ptxas version check. ,2024-01-03 11:27:12,,missing,others,version incompatibility,add,macro checker,others,ptax,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/bc6499d1fb195d8af740d769aae640b80fe16b51, Optimize check whether a HloInstruction is a parameter of the entry computation. We can check this in constant time instead of searching through all parameters.,2019-04-12 6:37:55,,improper,others,performance bug,update,type checking api,others,HloInstruction parameter,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/cb164786dc891ea11d3a900e90367c339305dc7b," Properly handle the case where SpecializeType() returns an error `Status`. If the error case in `SpecializeType()` is reached, then we would get a crash when trying to access the value of an errorenous `StatusOr` object",2021-11-08 13:35:56,,improper,invalid return types,crash,replace,macro checker,return value is valid,data type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c7f79bb75b5b83c3011e164ccd617a6ada910ea4,StatSummarizer: Put size check error message outside of if block where it belongs.,2017-05-10 18:29:07,,improper,others,user confusion,relocate,if checker,others,log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/3550ef89bc66d03b6e2db8e47bf7b038d9f4ceff," Convert CheckInputsSize to return a Status instead of CHECK-failing, and convert existing callsites to TF_QCHECK_OK the call. This moves us towards the goal of returning Statuses instead of check-failing in ImportTensorFlowNode().",2018-06-18 17:17:23,,improper,invalid return types,user confusion,replace,checker api,return value is valid,computation graph,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/feb3d1c5f8b40f1504b758b620edb25d4d3beef1, More precise checking of resource loop arguments for eliding Switch/Merge,2023-06-05 18:01:36,,improper,others,unknown,replace,checker api,others,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/84d7bf6f64fd9c8677f7f26511ce3031fe8d35a6,Add is_numeric to dtypes.cc to check whether a data type is numeric ,2023-05-09 14:44:28,,missing,invalid return types,incorrect functionality,add,type checking api,return value is valid,tensor data type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/cd34289b744040974ebe81e1b1e88f1c752d68e0,Update types.h to check if a data type is numeric ,2023-05-09 14:42:57,,missing,invalid return types,incorrect functionality,add,type checking api,return value is valid,tensor data type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/43fd10302bcc8447e7a7205bae848a3a88624775,Return error on invalid input in tfl.atan2_custom,2023-08-14 11:27:11,,missing,invalid return types,incorrect functionality,add,assertion statement,return value is valid,tensor data type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/00517642a356c5e04f009ea61c74638d89746392,Return error on invalid input in tfl.splitv,2023-08-09 9:07:32,,missing,invalid return types,incorrect functionality,add,assertion statement,return value is valid,log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/40c7fe94824100338ef0c495143b26501b1c367e,Return error on invalid input in tfl.topkv2 ,2023-08-08 11:22:26,,missing,invalid return types,incorrect functionality,add,assertion statement,return value is valid,log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b4aadb17b7aa5ea926b5220008e41f33e582baed,Return error on invalid input in tfl.where,2023-08-07 14:49:18,,missing,invalid return types,incorrect functionality,add,assertion statement,return value is valid,log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/ef049bdfc4f307c8b3a9dc480a90a5ff287f3d55,Add check for ResizeOutput return value in range.cc,2023-04-17 19:51:00,,missing,invalid return types,inconsistent results,add,macro checker,return value is valid,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1707ed9b9b0cc5cb02df22a06718c9c738825d39, Add a check to make sure that the allocation before an Evict() is not a prefetch.,2023-10-26 22:00:50,,missing,others,unknown,add,macro checker,others,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/f636be3bb1f556c15dba3028e61a8969d90dadd9,Return error on invalid input in tfl.sign_custom,2023-08-16 14:23:04,,misleading,misleading error message,user confusion,improve,macro checker,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/84a1cf61dd7239aa5d682083d34e0f7c99039734," [XLA] Do not suggest trying to use TF_XLA_FLAGS when failing to parse XLA_FLAGS. The error can be very misleading, as we never check whether the new flag is actually supported by TF_XLA_FLAGS.",2023-06-27 5:23:38,,misleading,misleading error message,user confusion,improve,macro checker,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e99e31597c1b5cc9f0cbc8a3dea71674d81c20b1," Fix GRUCellBlockOp message for invalid rank of x. The validation checks that x is a matrix, so rank must be 2.",2023-02-15 15:44:47,,misleading,misleading error message,user confusion,improve,macro checker,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b8431494de404b5f4def7303fb8efd6ba3575ef9,Fix error log messages in data type checks,2022-12-01 17:41:46,,misleading,misleading error message,user confusion,improve,macro checker,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/18dd91ccd4b1817cd5c34e40f76823a162bea029," [XLA] Report that real -> complex bitcast_convert is not allowed. The check as exists is bidirectional: it prevents conversions from complex to real and real to complex alike, but the reported error message was unidirectional.",2022-03-02 21:18:49,,misleading,misleading error message,user confusion,improve,macro checker,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/86abddb56350bccd95d1b7140b003fb03525b890, Add appropriate error check for nbins in tf.histogram_fixed_width_bins. This PR tries to address the issue raised in 54415 where nbins was not checked for tf.histogram_fixed_width_bins and an incorrect result was returned when nbins < 0.,2022-02-17 18:11:34,,misleading,misleading error message,user confusion,improve,assertion statement,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1e5c11676dce37bb7c8eb58b35fd298a655c6fd3," [tf.data service] Include dispatcher address in version check error message. This is the error message that happens when the address was specified incorrectly, so it is useful to include the potentially-incorrect address in the error message.",2021-04-05 14:21:08,,misleading,misleading error message,user confusion,improve,macro checker,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/07898e752cf02518508f193a0be2e451450044bd, Provide a more informative error message when the bazel version check fails. ,2020-05-20 14:31:08,,misleading,misleading error message,user confusion,improve,assertion statement,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/01e84d7cc214dbf5a7a21bc418ad43afb5694fbc, Update error message for data_adapter with validation split. Remove the user provided value in the error string in case it contains large amount of data. Dump large input data to log might crash on user side.,2020-03-31 12:34:49,,misleading,misleading error message,user confusion,improve,assertion statement,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/f54bc43f1117004208df6da34e422bf628fc3c23, Update error message when tf.functions cross merge_call boundary when using MirroredStrategy,2019-04-25 17:51:27,,misleading,misleading error message,user confusion,improve,assertion statement,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/4c75fb1cb917320acb386cf26adeb8e5151ca4f6," Improve error message reporting for check_numerics gradient. At present the op message is only printed if the numeric check fails during the op's 'forward' computation. If the check fails during the gradient, there is no identifier on *which* op's gradient failed.",2018-11-20 13:01:44,,misleading,misleading error message,user confusion,improve,macro checker,Improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/40918f36823973e816bd50766b1f447225b1bb9b, Make the type check error message more informative for contrib.layers fully_connected.,2018-10-22 18:30:18,,misleading,misleading error message,user confusion,improve,assertion statement,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9c1f14322484e44a93b77619ffd2e24b9b7a9b1d, Fix error message in TF-keras dataset shape check. (Dimension and tensor # were transposed in the error message),2018-08-28 16:50:03,,misleading,misleading error message,user confusion,improve,assertion statement,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/f0bf6c5191d224f229808f4b321158d890a481e0,Minor change for better error msg in eager input type checking ,2018-07-25 17:47:41,,misleading,misleading error message,user confusion,improve,assertion statement,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/178d62a63ea043a4b9969b4cd6f8983eb8eae523, Update check failure to logging a warning for repeated computation placer registration. This is to bypass a duplicated registration issue seen in open-source build during TF/PJRT integration.,2023-05-17 16:28:42,,misleading,misleading error message,user confusion,improve,if checker,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/6aece71ebf756d32ea730576a7ff12d2cfc7b242,"Places relatively cheap type checks for list, tuple, and dict before other more expensive checks. Specifically, this avoids calling expensive checks like isinstance(structure, collections.abc.Mapping) and nest._is_named_tuple in the most common cases (since these abc isinstance checks take ~10x as long as normal isinstance checks).",2020-05-26 13:04:03,,insufficient,type checking,performance bug,extend,type checking api,ensure tensor type is valid,built-in data types,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9," Validate real and expected type of arguments to cwise ops. Without this validation, it is possible to trigger a `CHECK`-fail denial of service.",2021-11-12 3:29:24,,missing,type checking,crash,add,macro checker,ensure tensor type is valid,tensor data type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/798b2ebda0cc6f12f1ca6460611f760149771a11," Ensure the allocation type is kTfLiteCustom when doing shallow copies in DeepOrShallowCopyTensorsShapeTypeData.  This code is correct only under the assumption that the caller has correctly prepared the tensors that get passed in for shallow copying, by setting their allocation types to kTfLiteCustom. This ensures that those tensors won't be double `free`'d later on. This check simply ensures that that assumption always holds, to ensure we fail early if ever a bug is introduced that breaks that assumption.",2024-01-12 19:40:23,,missing,type checking,crash,add,macro checker,ensure tensor type is valid,tensor data type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b65d9ec2b78c7c23e368ed4eec7b4deb89dcd712,"Fix value error generated on is_scalar check. Fix value error generated on is_scalar check. `is_scalar = shape is not None and not shape` raises a value error when shape is a scalar, ""ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()""",2017-07-21 16:29:29,,insufficient,edge cases,others,extend,if checker,tensor shape is valid,none shape,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9baa064387b0a114c3fcec88abaa0568834e8e34, Only apply check for non-tensor case ,2019-08-03 10:37:29,,insufficient,type checking,incorrect functionality,extend,checker api,ensure tensor type is valid,tensor data type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9c14f6ba30d96241978188998de47a388822365f,Only apply check for non-tensor case ,2019-08-02 20:44:03,,missing,type checking,incorrect functionality,add,checker api,ensure tensor type is valid,tensor data type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/cb2828a844ccaf0394e602d15fd95e45073729a2," Check that the type of an implicitly dereferenced tensor matches the expected input type. The dtype of a tensor reference can change between the point when it is ""produced"" by an operation and consumed by the next operation. This evades checks in the executor that the type of tensor on each edge matches the type signatures of the producing and consuming operation, which could lead to undefined behavior. Although there is no existing operation that changes the type of a tensor reference, it is possible to use the OpKernelContext API to do so, so we add a further check in the runtime to defend against operations that might be added in the future.",2018-02-02 20:13:17,,missing,type checking,others,add,if checker,ensure tensor type is valid,tensor data type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/924f80a4fdb34230965a7a8a4476901847463645,Add stricter type checking for tf.math.real. Fix for tf.math.real so that it only accepts tensors with numeric entries as input.,2023-05-08 16:20:25,,missing,type checking,incorrect functionality,add,if checker,ensure tensor type is valid,tensor data type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e6df768b81e973f2123bc83a18a60773fc4da99e,[TFG] Fix IsAdd string type check in tf_op_names ,2022-08-22 19:49:41,,insufficient,invalid return types,incorrect functionality,extend,if checker,return value is valid,built-in data types,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/4f4a0276a2cf9186c0541072964676159368286e, Add appropriate PyObject type check for bool. This PR fixes an issue where PyObject type in tf's C bindings does not check if an input is a boolean and will always cast to bool.,2022-02-18 2:04:46,,missing,type checking,incorrect functionality,add,type checking api,object type is valid,built-in data types,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/88609e2e22fa5c296de2e27e04d1cc4743b2dfcd, Add appropriate dtype check for tf.boolean_mask's mask. This PR tries to address the issue raised in 54412 where mask's dtype was checked in tf.boolean_mask and an invalid result has been returned instead.,2022-02-17 19:01:10,,missing,type checking,incorrect functionality,add,if checker,object type is valid,built-in data types,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/d79c4d435fc6e7be6cc69a3ca446716ebf6190b9,change default value of num_threads to Non. set num_threads before delegate. check the type of num_threads before setting it,2020-02-14 21:24:46,,missing,type checking,crash,add,if checker,object type is valid,built-in data types,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a76646d4b4ad5d56b5e63c139985bbd1eb98dd90," Add type checking at the beginning of tpu.shard(). Otherwise a message like ""TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn."" will be thrown, which is confusing.",2018-09-17 20:58:44,,misleading,misleading error message,user confusion,add,if checker,improve error message,error log,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c510c1b8b1ef5be1d65971f5b9e21e61becd0bb3,Remove IsCalledComputation from HloComputation. The function doesn't do what it seems. There are more types of called instruction that are not accounted in this check.,2024-02-08 7:11:31,,unnecessary,others,incorrect functionality,remove,if checker,others,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/70fd126d3afb8a1d00299c28ab234623d2b88704,"In case user pass non-string as a tensor name, the code raises ValueError. One common mistake is to specify tensor object instad of its name. Unfortunately, in this case current implementation fails to produce an error message resulting in misleading errors like shown below. ",2020-06-03 19:35:48,,missing,type checking,incorrect functionality,add,if checker,ensure tensor type is valid,tensor data type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1c49c13ba59961cf7581e3e29b951db8faca94f5,Add type check for reduction axis in reducer operation. ,2019-06-17 5:56:00,,missing,type checking,inconsistent results,add,macro checker,ensure tensor type is valid,tensor data type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b6f3366a716ca9b5a1e6114a3bea050c80d8a475," Don't check for if null after already dereferenced. I'm not sure how it could be null at this point (and obviously it is nowhere else we'd have seen failures), but keeping the check as is and just moving it to where it would catch it before dereferencing.",2020-12-30 17:42:46,,missing,null pointer dereference,crash,add,if checker,object is not null,tensor node,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/89fa1ae2cb34eab0e6137e72e6fab01f6c5bc164,Fix check for cloning FunctionLibraryRuntime,2019-07-26 8:53:20,,improper,null pointer dereference,crash,update,if checker,object is not null,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/3a7b36bca7f43ce4f0d0791ce0e0d84ece8683d9, [Grappler] Remove DCHECK from a MutableGraphView CanDedupControlWithRegularInput check.,2019-07-23 11:12:53,,improper,null pointer dereference,crash,replace,if checker,others,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c5019e2156c749d35ec786ff7946a55006d9ba91,missing checking on null pointer dereference,2021-08-30 11:23:38,,missing,null pointer dereference,crash,add,if checker,object is not null,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a7908e924af3105c3007988e219855174b26774f,Added check for output buffer ,2020-02-25 18:55:18,,missing,null pointer dereference,crash,add,if checker,object is not null,string object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/04b97cde86550995da57d16d81084006456ccce5,Fix segmentation fault with tf.stack an keras's Input in TF2.0. This fix adds the `PySequence_Fast` and checks the return value to make sure it is not nullptr.,2019-03-25 22:42:20,,missing,null pointer dereference,segmentation fault,add,if checker,object is not null,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/95166f5960322cc784a7e8f339a701da80a41a1e,Add a null check on enter_ctx and update the null check on merge_ctx ,2017-08-03 22:57:12,,missing,null pointer dereference,crash,add,macro checker,object is not null,computation graph,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/cd8d0bf58ad554588012898161c91fa453bbf7f0,Address edge case where runStats is null and the interface is closed.,2017-02-21 17:07:40,,missing,null pointer dereference,crash,add,if checker,object is not null,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1a1a381b5be7701843c3f1e34aa1846ae2a1d0ce,"Fix a SIGSEGV bug in InferShapeForXlaGatherOp. Since `ComputeOutputComponent` may return nullptr, we need to check for null attributes explicitly to be safe.",2024-01-25 13:27:28,,improper,null pointer dereference,crash,update,checker api,object is not null,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/893aa7518fe3175739ac1ba70d7355a0b091115c,Added a null check in string_util.cc ,2023-10-13 16:12:05,,missing,null pointer dereference,crash,add,if checker,object is not null,buffer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9720b405905dee209a3f7d003de21d388e1aaef4, Avoid nullptr as row offsets to cusparseCreateCsr. As of CUDA 12.2 additional input validation allows NULL for the row offsets only when rows=0.,2023-07-26 11:32:37,,improper,others,crash,update,macro checker,others,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/47eaa828a1dd4bf50ec4203ef4bbb348b3ef0dd0,Add nullptr check,2023-05-04 5:01:40,,missing,null pointer dereference,crash,add,if checker,object is not null,tensor object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c2fc1f2b5a8b8152c43b81cf31394f3e0a2cb837,Add null pointer check,2023-03-17 21:55:45,,missing,null pointer dereference,crash,add,macro checker,object is not null,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b677392e4af8095dbde8068b0ceb60bca815e94b," Reject non-PjRt devices in PjRtArray::Reshard(). PjRt buffers traditionally support some degree of interoperability between PjRt clients (e.g., CPU and TPU). However, this is not universally true between arbitrary IFRT clients that may use a non-PjRt-compatible runtime. This change adds extra checks to make sure that non-PjRt devices are not accidentally used in PjRtArray's destination devices.",2023-02-07 15:11:02,,missing,null pointer dereference,crash,add,if checker,object is not null,device object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/f22ca1dc88c70a0dc5696c37e6a2de6bcf8d60c7," Avoid segfault when init_value is not on default_mesh. To actually fix the segfault in lower level (e.g. directly users of VarHandleOp), I tried to add a validation in SPMD of AssignValueOp, but turns out it only knows the resource_layout is an 'empty' layout without any mesh information. We shall start tracking mesh of empty layout -- but changing the data model at this point is not very easy to do or to justify.",2022-06-07 12:33:16,,improper,edge cases,segmentation fault,relocate,if checker,others,none object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/aaa3fb49374d59c89115730c8e2f672e70b9e3fa," [TFLite] Bucketize op: Fix processing of bucket boundary array. The param value may be a nullptr, which is an error; we should catch this and avoid dereferencing it.",2021-10-21 18:20:22,,missing,null pointer dereference,crash,add,if checker,object is not null,vector object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e1eb6d9cfa14368442f0d172a40f87ce4f094386,Add CheckArraySegments() to model verifier. Add additional logic to check nullness of array segments vector.,2020-10-18 21:45:15,,missing,null pointer dereference,crash,add,if checker,object is not null,tensor object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/26cd260fac5fa98ade11ff2a5ec38ede65631cc0,Add additional data validation while saving and restoring iterators. ,2020-06-30 15:48:22,,missing,null pointer dereference,crash,add,if checker,object is not null,vector object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a17858f3cc5e7ab4ebc2c166d71e7f85b2dad05d, Avoid undefined behavior by checking for null Operation in TF_Input/TF_Output,2020-06-24 12:54:27,,insufficient,null pointer dereference,crash,extend,if checker,object is not null,vector object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1835465ac5a9c823f7187cb0dd5786da9c360838, Add error_reporter DCHECK back into SimpleMemoryAllocator. This check was removed due to an internal build problem.,2020-06-09 17:07:19,,missing,null pointer dereference,crash,add,macro checker,object is not null,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/4a8d8518fba1d70f63633775695f1a5189cd252f, Add checks that the Allocate function returned successfully. ,2020-04-15 19:00:49,,missing,null pointer dereference,crash,add,macro checker,object is not null,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/7578e120de2a3a5282ced8d41881f19363f83466," Fix crash on closing the app when classifier failed to initialize. When testing on an API 21 emulator, the classifier fails to initialize. The fix is to check for null before calling `.close()`.",2018-12-09 11:22:52,,missing,null pointer dereference,crash,add,if checker,object is not null,tensor object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c1b9ac9f215a3a83f7f0b6233bf4cef0b3e74598,Error checking in c/python code.,2018-06-06 10:53:20,,missing,null pointer dereference,crash,add,if checker,object is not null,python object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/ca170f34d9174d6981850855190a398393aa921e, [Tensorflow] Add check fail when user passes a tensor with nullptr to lookup.,2017-03-17 16:27:50,,missing,null pointer dereference,crash,add,macro checker,object is not null,tensor object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/75c45e54bd37932f26d6e7cb36920c06a7833d52, Added sanity checks on the existance of nodes to ConstantFolding. This should fix null dereferences in ConstantFolding on fuzzed inputs.,2021-05-11 14:51:01,,improper,null pointer dereference,crash,update,if checker,object is not null,computation graph,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/ed043aec4962dfdc3c58e2ad90dacb557dafcf4e, Lite: ResizeTensor Dim size check added to avoid reallocation if no change,2019-03-19 10:29:10,,missing,null pointer dereference,crash,add,if checker,object is not null,tensor object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/65c5dd69676db159ddd3a1fd7b2f6836dfe37f49, Add nullptr check for external registration init / prepare / invoke /free.,2023-03-30 19:40:56,,improper,null pointer dereference,crash,add,if checker,object is not null,tensor object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/20d54796563631c23c27548b321487e8b0c982a9," Add a nil check before init the device_name string, and also assign an empty string as a placeholder.",2023-11-03 13:34:02,,insufficient,null pointer dereference,crash,extend,if checker,object is not null,string object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/db10718b38b2884cb5ed46d33c135c079f649d16,"With some memory allocators, attempting to allocate 0 bytes will return a null pointer. This specifically happens when building tensorflow with mkl support. If TF_TensorData returns null, the go code to create a slice from the data leads to a null pointer exception. This fixes the issue by checking for the nil return and returning a slice zero value to (nil) to the caller. ",2017-11-03 23:09:38,,missing,null pointer dereference,crash,add,if checker,object is not null,tensor object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/8876a1796aeced8f89c279cbc98db9c7957ddbd1, Updated check for existence of TensorFlow objects to 'is not None' rather than 'if [object]'.,2016-09-22 9:03:33,,improper,edge cases,crash,update,if checker,others,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/11030308c5d25df5b36f8a583f1b4607e4ea2b7f, Add a check to check if all sharding strategies are dropped due to infinity costs,2023-08-25 13:18:19,,missing,edge cases,performance bug,add,macro checker,integer argument is valid,zero integer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/2465d4e77654f0d4f7799bc46d5fd5812590acc6," Add a check in auto-sharding setup and die if the input mesh shape contains more than two shardable dimensions, which is currently not supported.",2023-03-21 21:59:56,,missing,edge cases,inconsistent results,add,if checker,tensor shape is valid,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/5322fd40cd58cfa8c551e602fede7a3be19fff95," [PJRT] Fix checking for output sharding. Output sharding for empty tuple needs to have one ""replicated"" element.",2023-08-24 19:12:36,,missing,others,performance bug,add,macro checker,others,tensor sharding,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/3e0152a8b4aad03dd06274e0dd3b94bd5f8bf5d3,"Fix invalid syntax error when import carla is present. The issue is that, when `import carla` is invoked, I/O operation for `std::ostringstream s` might fail, which caused the conversion of AttrValue to string as empty. This PR check `s.good()` to make sure the I/O operation is OK, and, fallback to normal conversion if locale-neutral I/O operation fails.",2019-12-04 22:40:35,,missing,others,incorrect functionality,add,if checker,others,string object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/06b89ed1bdf606adb21d66664ca7ab5eaffdd58f," BundleReader was not waiting for concurrents reads to complete before checking their result value. Also changed the large value reading test to actually exercise the multi-threaded reading path. Previously, the whole multi threaded path was being skipped because the reads were smaller than kBufferSize.",2024-02-05 9:44:09,,insufficient,others,others,extend,if checker,others,tensor bundle,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/0317f64491ba42376d96b157983a02d8b31b679e," Update RNNCell._rnn_get_variable to use Variable._trainable in TF2 mode. When using a legacy RNNCell in TF2 mode within a tf.function the ""var in trainable_variables()"" check led to treating a tf.bool tensor as a Python bool. This change makes use within a tf.function use the same logic that is used in Eager mode.",2020-06-06 2:02:56,,improper,tensor execution mode,incorrect functionality,replace,if checker,tensor is executed in eager mode,eager execution,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b8c517ab4ef0bd851ef2f8187935fd3a90261af5,Reinstate eager check inside _GradientsHelper,2019-03-06 7:26:33,,missing,tensor execution mode,incorrect functionality,add,if checker,tensor is executed in eager mode,eager execution,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c9b4689bc4d4024aa16b7d6cfc1c65fa1ed8486e,"Removed no longer supported call to in_eager_execution. Swapped context.in_eager_execution() to the currently supported context.executing_eagerly(). Added negation to eager check. In all likelihood, the negation was always supposed to be there since getting default graph in eager mode does not make sense",2018-11-06 12:32:01,,improper,tensor execution mode,incorrect functionality,update,if checker,tensor is executed in eager mode,eager execution,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e5496b556734bb1d8de85311092804e0150b3009, Remove extraneous check for Eager mode.The check is already made once at the start of the method,2018-02-21 8:27:17,,unnecessary,tensor execution mode,incorrect functionality,remove,if checker,tensor is executed in eager mode,eager execution,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/be5116dd131a92da298dbb68d26e0d47f66f2fe5, Correct graph check in broadcast_to gradient. ,2020-02-13 15:15:20,,improper,tensor execution mode,incorrect functionality,update,if checker,tensor is executed in eager mode,eager execution,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1d6dae88efef68dd7fbeeb5c39ea0f69c1c721c1,Add check to tf.device when called with a function in eager mode. ,2017-10-30 17:19:44,0,missing,tensor execution mode,incorrect functionality,add,if checker,tensor is executed in eager mode,eager execution,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/8c3822edbb31cf71cedaf49f2167e45c1e2d0b83,Update the is_dtensor check to only run in eager mode.,2023-03-13 15:53:59,,missing,tensor execution mode,incorrect functionality,add,if checker,tensor is executed in eager mode,eager execution,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a63f3006f703428ff980748cdbe24d6a13f761e2,Skip checking for graph_key in V1 optimizer when running in eager mode.,2023-02-14 19:05:54,,missing,tensor execution mode,incorrect functionality,add,if checker,tensor is executed in eager mode,eager execution,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/dd7d791e02396346d98b7b2c58137d7e51756c0c,Add isinstance check for eager execution.,2023-02-03 18:21:11,,missing,tensor execution mode,incorrect functionality,add,if checker,tensor is executed in eager mode,eager execution,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/8933b8a21280696ab119b63263babdb54c298538," Fix a null pointer exception caused by branching on uninitialized data. This is due to not checking that the params for the quantization exists. If there is no quantization, we should not access the `.params` field.",2021-07-16 13:27:37,,missing,tensor quantization,crash,add,if checker,tensor quantization is valid,quantization,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/0a9b39caefd437fec742ae48b25061abd6e2699b," When allocating GPU constants, check to see if the destination. tensor is intialized early (because we ran out of memory) and report it as such.",2017-04-28 21:04:24,,missing,edge cases,performance bug,add,if checker,tensor shape is valid,large shape,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/d8df06a9403b1434aea8b82a193fc30a4ab29bbb," Add assert in Operation->printAssembly to check improperly created Op's. We allow the name of an operation to be different from the name of the 'ConcreteType' op it was instantiated with. This can happen when you sub-class an existing op and provide a getOperationName for it. Such a situation leads to an assertion too deep and at a place seeminly unrelated, and typically when the module is printed with the trace: printOperation, printAssembly, Op::print, getOperand, dyn_cast<OperationStmt>, isa. 'isa' will complain about being called on a null pointer, and the null pointer actually comes from the getAs<> in printAssembly. This should have been caught in printAssembly.",2019-03-29 16:27:59,0,missing,others,incorrect functionality,add,assertion statement,others,tensor operations,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/4f38b1ac8e42727e18a2f0bde06d3bee8e77b250," Prevent null dereference read in GetInitOp. We have a map of maps. We test that the key exists in the first map but then we don't have any validation that this also means the second map has the needed key. In the scenariosc where this is not the case, we'll dereference a nullptr, if we don't have this chec",2021-11-09 20:03:42,0,missing,others,crash,add,if checker,others,tensor operations,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a6bf104fad37bcf4c6fcfb2e08c541473f886513,Refactoring: Set default quantization method before checking,2023-11-14 20:25:44,0,missing,tensor quantization,incorrect functionality,add,if checker,tensor quantization is valid,quantization,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a93ac5f7d147ae8fe946de33ad654161ae851352," For quantization values where range_min == range_max, use the lowest_quantized.  Add needed checks for divide-by-zero.",2016-07-06 20:04:56,,missing,tensor quantization,numerical error,add,if checker,tensor quantization is valid,quantization,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e56206755ff0c98269eb3e50c98fccbaadb6884d, Support quantized i64 during flatbuffer import. Sometimes tflite flatbuffer represent `i32` quantized values as `i64`s. In these cases we should truncate down to a lower bit width to avoid creating illegal types. We check the bitwidth of the type at load time and pick a power-of-2 bit width where the value can be safely truncated.,2023-08-21 21:50:57,,missing,tensor quantization,incorrect functionality,add,if checker,tensor quantization is valid,quantization,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e7de472681079932b2547024f31c876da54f61a0, Fix a bug in flatbuffer importer that use tensor quantization before checking.,2020-12-07 18:36:46,,insufficient,tensor quantization,unknown,extend,if checker,tensor quantization is valid,quantization,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/2adf1114d4dc7ca30e5117acd2dc7aeb3279feb7, Make NNAPI delegate only apply overflow check to quantized average_pool,2020-02-14 21:35:27,,missing,tensor quantization,numerical error,add,if checker,tensor quantization is valid,quantization,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/677866210941431b82c95d58d0798976bb40a415,Add a nullptr check for the tensor quantization field,2019-10-28 3:30:19,,insufficient,tensor quantization,crash,extend,if checker,tensor quantization is valid,quantization,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/19b2e1b5868a044df4622ef7e26fa5570ca52e5e,Only perform scalar check for a tensor shape if it's not empty.,2022-02-11 18:15:25,,insufficient,edge cases,crash,extend,macro checker,tensor shape is valid,empty shape,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/ebb292f6efcd690e6e2152ce3f69a4ab0e1194f6," Fix boosted trees shape function issue. Dimension was accessed before size was checked, leading to segfault. Switching order of checks fixes this.",2023-07-28 18:57:50,,improper,tensor shape mismatch,segmentation fault,relocate,checker api,others,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9c92b50fc4b95985a0749101976d04896bf19bfe," [conv3d_transpose] Fix dim check for bias. Per discussion with @thaink, the previous way to do the dim check for bias is not correct. So we need this change.",2022-03-15 20:59:22,,improper,tensor shape mismatch,crash,update,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/8eb773d6c23de29dccfc3cf3da441a8552ed13ed,XLA] Better shape size validation for sparse arrays. ,2018-07-19 13:34:48,,improper,tensor shape mismatch,others,update,if checker,tensor size is valid,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/5bc536f1afbaff5d3d5a14a9185cd1e3cc31b302,[Fix] bug fix during check static shape.,2020-01-07 20:21:14,,improper,tensor shape mismatch,crash,update,if checker,tensor shape is valid,empty shape,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/eb2ddc0debb7e1b0c9ea68c817ca05fd59dc7914," In TF2XLA EnsureShape kernel, don't check whether the original tensor has dynamic shapes as it is much more expensive than just blindly clear out dynamic dimension.",2022-07-29 1:48:28,,inefficient,others,performance bug,remove,macro checker,others,tensor shape,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/264eb6ed1dbfb5e078c7dd977da8d7e633106fc5, Fixed add bias transformation. Added check for convolution with dynamic weights.,2020-07-13 14:33:03,,missing,tensor shape mismatch,performance bug,add,if checker,tensor size is valid,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/bb9b9f32bd08bc5660343cdc09f4b467d7297e3c, Change a recently introduced DCHECK from O(N) cost to O(1). A recent change added a DCHECK that took O(N) time for an instruction with N users every time a user was added. This became quadratic and caused timeouts in some tests that enable assertion checking. Made the DCHECK O(1) to fix it.,2024-01-04 20:05:02,,inefficient,tensor shape mismatch,performance bug,update,macro checker,tensor size is valid,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/653d47379b3a716f82058148c35b2f491bfa2856," Speed up 4-bit unpacking loop in reference implementation. The new implementation moves the odd-length check out of the main loop, and unrolls slightly differently.",2023-04-20 11:55:22,,inefficient,tensor shape mismatch,performance bug,update,if checker,tensor size is valid,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/2e4d3951eb618a7c34d5e629fc2506ea2a62b4a7, Correct Tensor order for dilation2D. `gen_nn_ops.dilation2d` seems to be in `NHWC` while the parent function was asking for `NCHW`.  I corrected the doc and the check.,2019-07-01 9:53:56,,improper,tensor shape mismatch,inconsistent results,update,if checker,others,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/8cef4cda26e08256b6698e942820d9a3ac1bcc94,Add minor checks for data_format and padding value ,2020-12-11 1:22:43,,missing,tensor shape mismatch,incorrect functionality,add,if checker,others,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/076ea8d84c2058b0d01d56dd9ddc3221a2e0c817,Also check dst_format,2020-09-28 14:07:24,,insufficient,tensor shape mismatch,incorrect functionality,extend,if checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/ed06859189722af4dc8e4abd655926df066e587a,Add format check.,2019-09-19 5:01:25,,missing,tensor shape mismatch,inconsistent results,add,macro checker,others,tensor format,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/0d5668cbdc6b46d099bd3abd93374c09b2e8121f, [XLA:SHAPE_UTIL] Return nullopt instead of a check failure if the input dimensions are not sorted.,2022-04-06 19:43:53,,improper,tensor shape mismatch,crash,replace,macro checker,output and input tensors has same size,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/694b75a24bce416555425dedc58b0cdcd0d52c1e,Shape validation of `max_features` in `QuantizedReluX`,2018-05-21 17:38:18,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/d7ec7b9415181fce88ea8fde39af9e8be5a8be97,Added generic check that shape has not more than 4 dimensions.,2021-06-22 14:20:17,,missing,tensor shape mismatch,crash,add,if checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/824af2acfa0cdf897c08d91224aea0958c1afc02, Add ndmin check. Added ndmin check to allow maximum 32 ndmin to make same behavior as numpy. Currently it is crashing when very large ndmin is passed.,2022-03-29 13:44:03,,missing,tensor shape mismatch,crash,add,if checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b73a3c21a224f479af8d3b8af320c220a091906c,[XLA] Add check for potential out-of-bound access.,2021-11-03 10:25:02,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/63753d5f1531b17cf8cbbf1d8b77c16edcfb9711, Change DCHECK_LE to DCHECK_LT when checking invariant on original indices for sorted items Indices of items should be strictly smaller than the size of the vector.,2021-08-18 20:02:15,,improper,tensor shape mismatch,crash,replace,macro checker,tensor indice is valid,indice with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/7f9929732ced22fe8ef42a695dae39c1caf44608," For gather op, if params.shape[:batch_dims] is not the same as indice s.shape[:batch_dims], return an error instead of check fail ",2019-10-14 23:20:18,,missing,tensor shape mismatch,crash,add,macro checker,axis parameter matches tensor dimension,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/480641e3599775a8895254ffbc0fc45621334f68, Validate (and ensure validation sticks) inputs for `MatrixTriangularSolve`.,2021-04-24 19:51:15,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/728e6c38b9dde853da9fdaec524d9a3d043b8729,"If Tensor dimension is already same as new dimension, then avoid reallocation.",2020-04-03 16:07:20,,missing,null pointer dereference,crash,add,if checker,object is not null,tensor object,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9e62869465573cb2d9b5053f1fa02a81fce21d69,Add more validation to RequantizationRangePerChannel. ,2021-07-29 19:35:05,,missing,tensor shape mismatch,crash,add,macro checker,axis parameter matches tensor dimension,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/ba91c04e001f417641e757a6417e5325c1c4e15e,Add more check to sparsity parameter verifier.,2019-12-19 15:09:00,,insufficient,tensor shape mismatch,crash,extend,macro checker,axis parameter matches tensor dimension,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1610f391833738972b538e4ee97f90dbd30fc745, Replace DCHECK with actual validation in AddRangeStats,2021-11-19 17:14:37,,improper,tensor shape mismatch,unknown,replace,macro checker,axis parameter matches tensor dimension,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/150a6c06b281246cb5a075a704fceeb257bb63af,Add a check on the 0th dimension of filter for DepthwiseConv.,2019-07-23 22:53:07,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/bf686faeddcca97be6ad7b6421cb26ab1c3cea2c,TFLite: Enhance input check for ResizeNearestNeghbor,2019-03-20 14:50:59,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/645f2c0cef75e80cdcaaaceca94a785191b9f423,[XLA] Add a check to the HLO verifier for badly formatted Broadcasts.,2017-09-07 20:04:36,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/fa8381593d0cbe354cb54d691e0a8c42bf4b69d0," Move Batch op input validation before enqueueing to the batch scheduler, because earlier error detection is better (and also so batch->size() doesn't crash if #dims==0 :).",2017-04-27 17:47:43,,improper,tensor shape mismatch,crash,relocate,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c040db5e9003cc20016586df9f2964db83b98c4f," [XLA] Add a defensive check in dynamic dimension inference to prevent scalar reshape with dynamic dimension. In theory we can just ignore a [1] -> [] reshape, but adding a check here for now.",2019-09-12 20:48:49,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,zero dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/48393637f8154be16088d84742485a0e153ecbb2,Change check to allow tensors with up to 6 dims.,2020-03-03 1:58:36,,improper,tensor shape mismatch,inconsistent results,update,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/662128e8ca3411286b234553a7efc1356353d0f5, add rank checking for MEAN op. The MEAN op of NNAPI only supports a tensor with rank <= 4. Check the rank of the input tensor before delegating the op.,2022-12-21 20:22:55,,missing,tensor shape mismatch,crash,add,if checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1711b76f95b49dcf597fe5b2ec5f4ff79ddbc7a7, check if boxes have logical coordinates (different than a line or a point),2020-01-02 10:39:57,,missing,tensor shape mismatch,incorrect functionality,add,checker api,tensor indice is valid,tensor indices,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9b947dd6377c022091c8aa005cdcff52c53ff5f0,Also check dst_format,2020-09-23 15:04:10,,insufficient,tensor shape mismatch,inconsistent results,extend,if checker,tensor dimension is valid,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/450dec35448a73b3fcb5d4f82108d5fdcb3f59b4,"Internal change, add some checks on the sparseTensor format checking.",2023-09-25 19:10:37,,missing,tensor shape mismatch,crash,add,if checker,axis parameter matches tensor dimension,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/5daf3b9131254baa1182fc29d63bafd4b055e0ea,Add shape validation in shape function of MapAndBatchDataset,2018-05-17 18:12:24,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/356f360e8772a2697ec0d30036237342549803f5," Add additional shape validation to compute_accidental_hits. In `compute_accidental_hits`, the `sampled_candidates` must be a vector, as is shown in the kernel implementation in `tensorflow/core/kernels candidate_sampler_ops.cc`. This fix adds shape validation of `sampled_candidates` in the shape function whenever possible.",2018-05-13 20:32:50,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/7c88788e63f3a747d2794175076db551d768734e," Shape validation of max_features in QuantizedReluX. In shape function of QuantizedReluX, `max_value` and `min_features` have shape validation but not `max_features`. This fix add restriction to `max_features` as well.",2018-05-13 20:32:31,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/ff6be80a1ec3c353ebd0d17e2f0b46d9097310db," Improve the shape function for ParameterizedTruncatedNormal.  The parameters of ParameterizedTruncatedNormal should be 0-D or 1-D, which is checked in ther kernel functions. There is no check in the shape function of the ops. This fix improves the shape function and checks the parameters of ParameterizedTruncatedNormal whever possible.",2018-05-11 12:48:32,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e4471c403a9e9430839900bd92c067d04580a51b,Add additional shape validation to `compute_accidental_hits`,2018-05-21 18:48:11,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c59c37e7b2d563967da813fa50fe20b21f4da683," Prevent array write out-of-bounds. If user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.",2021-04-28 20:54:22,,missing,out of bound access,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e82a377de614fed51da8a7c5242a90a7967169f2,Correct axis check,2020-07-24 23:29:28,,missing,tensor shape mismatch,crash,add,if checker,axis parameter matches tensor dimension,tensor rank,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/402d478a107e2931fb0e9b2f08f973997cae7f98,Move the checking of ranks for early exit,2020-09-28 17:04:28,,improper,tensor shape mismatch,crash,relocate,if checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/27de8e717c1bec91398f5a6be6c7287b657fc960," Improve shape function for CudnnRNNParamsSize. In cudnn_rnn_ops.cc, the CudnnRNNParamsSize does not have restrictions on num_layers, num_units, and input_size, though they all should be scalars. This fix adds the shape check of num_layers, num_units, and input_size for CudnnRNNParamsSize.",2018-07-27 3:20:25,,missing,tensor shape mismatch,incorrect functionality,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/02703f9525696f4788496745f6756585c1c546a3,Fix crash in range sampler by adding a range check in the sampler op.,2017-02-15 1:50:18,,missing,tensor shape mismatch,crash,add,if checker,others,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/4a1d1c8413a3752af7dc91a7128e202660b0f05c," Fix mismatch of shape restriction in DrawBoundingBoxes. In the kernel of DrawBoundingBoxes, the shape of the input images should be 4-D. Though in the shape function, at the end `UnchangedShapeWithRankAtLeast(c, 3)` was used instead (at the beginning of the shape function the validation is `WithRank(c->input(0), 4, &images)` which is correct). This fix address the discrepancy by changing to `UnchangedShape`.",2018-05-29 17:15:16,,improper,tensor shape mismatch,crash,update,type checking api,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/181ca305a7954ce86a453a39db0b4f6d10b82720," Add shape validation in shape function of MapAndBatchDataset. In MapAndBatchDataset, batch_size, num_parallel_batches, and drop_remainder are 0-D scalars. This fix adds the shape check to those Inputs. Note since the Input of `other_arguments` is a list and is before `batch_size`, the shape of the `batch_size` and others could not be obtained through index like `c->input(2)` etc directly. It is still possible to obtain the ShapeHandle with names `c >input(""batch_size"", &batch_size)`, though.",2018-05-15 10:51:40,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/6e153325b66330dafea4e4e8b67b5d56b1a37852, [XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast produces a scalar. This avoids crashing within last_dimension when attempting to match.,2023-07-14 7:46:03,,missing,tensor shape mismatch,crash,add,if checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/429f009d2b2c09028647dd4bb7b3f6f414bbaad7, Add remaining missing validation to `BoostedTreesCalculateBestFeatureSplit`,2021-07-28 16:43:55,,missing,tensor shape mismatch,crash,add,if checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9d3cce4c7525bad6743f84302e5f6355a3fd8fe5, Fix crash in BlockLSTM. This PR tries to address the issue raised in 58175 in addressing the crash of BlockLSTM when invalid input is provided.,2023-01-09 17:12:32,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/2c72ca8c439d64268e849ef81cde78f464e95ca2,"add rank checking for ADD, MUL, and DIV ",2022-12-21 20:45:40,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/d94ffe08a65400f898241c0374e9edc6fa8ed257," Prevent an OOB read in expand_dims.cc. The for loop that follows this check assumes that `axis` is between `0` and `input_dims.size`. If user supplied `axis` is negative, the if code before this check is supposed to bring it back to positive (similar to how in Python one can do `l[-3]` to mean `l[-3 + len(l)]`).",2021-07-27 17:52:03,,missing,edge cases,crash,add,macro checker,tensor dimension is valid,negative dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/0e3574d39c66d937fa9f9d2e25554aab0066f250,Add rank check to Sub op delegation to NNAPI,2020-04-22 11:33:35,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a680ed0bf03d5ca3b2c4a70c0d95eeebc20da6d6," For Substr check pos and len rank equality only when their rank is known. This fixes a bug where len has unknown rank, while pos has known shape. The WithRank(...) check returned error in such a case. Here we compare their ranks only when both pos and len have known rank.",2019-10-17 20:10:41,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/50f6683ca50e6d4e7008d6d1b437b407d6a62e92,Add shape check for batch related Dataset ops. * Add shape check for PrefetchDataset * Add BatchDataset shape check  * Add shape check for SlideDataset * Add shape check for DenseToSparseBatchDataset,2018-04-19 12:13:21,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/edb29d02765b45e712042725dc06b65f5e610327,Add shape check to TextLineDataset op,2018-04-19 12:10:53,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9187be7adff07be82856add498aa3ff4b5f95998,add checks for compression_type and buffer_size also,2018-04-18 12:05:05,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/779664494d43b18a812361197dcbea2f25912c02,Add shape check to TextLineDataset op,2018-04-18 8:12:14,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/48f7e377963a951b77cbf111675931fd4248b090,Add shape check to TFRecordDataset,2018-04-17 17:59:50,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c4dea2255c71037c9cade9cbd1d7820b3429b3fa,Add shape check for buffer_size with TFRecordDataset,2018-04-16 21:31:54,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/d97ffbdf362fa7d06ef8d946c8620ff7a3a50a08,Add shape check for compression_type in TFrecordDataset,2018-04-16 21:30:42,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/7586dee9aa8b4b63143ab658ca59658aaed0df97," Add shape check to TFRecordDataset. The inputs of TFRecordDataset have the requirements for shapes. However, the check was not done in the shape function. This fix adds shape checks whenever possible.",2018-04-16 21:28:30,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/851177fee860211e2fabcb019d644e75b7f701b0,Add shape check for shift of tf.roll,2018-04-16 21:17:51,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/3f796ff8c9e6d7ff88f99c056b78e88fb0b31114,Add axis shape check for tf.roll,2018-04-16 21:17:51,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/10467d29e05d9957a6e3cb2335f8eeba1fd8896e," Improve shape function check for tf.roll. The `tf.roll` op has requirements for the shape of inputs. However, the shape of the inputs are only done at the runtime inside the kernel. This fix improve the shape function so that the check could be done early if shape is already known in the shape function.",2018-04-16 21:17:51,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/7254b098e04c5deba029b06967803422cdf329e6,"InferenceContext::UnknownShapeOfRank support unknown rank, check rank>=0",2017-05-22 15:25:48,,missing,edge cases,crash,add,macro checker,tensor dimension is valid,negative dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/41deb95a7bde735d3c8b9adedd8b1fe8c1ef2732,"support unknown rank, check rank>=0",2017-05-20 22:19:01,,missing,edge cases,crash,add,macro checker,tensor dimension is valid,negative dimension,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/8b742f8559e88474735d0a2c03e00da65e40b412,Fix check error on shape overflow.,2023-05-03 13:28:05,,missing,tensor shape mismatch,numerical error,add,macro checker,tensor shape is valid,others,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1595906c2192b7f402f746652042a592ad290378, Prevent CHECK-fail DOS in BoostedTreesSparseAggregateStatsOp. Calling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.,2021-11-18 22:27:23,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/54c94431e5dd17fc46d99da1a3f132c76414c161, Prevent CHECK-fail DOS in BoostedTreesSparseAggregateStatsOp. Calling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.,2021-11-18 22:22:30,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/5d96267d907ac2119cbccf1416b749195e8fd8de, Prevent CHECK-fail DOS in BoostedTreesSparseAggregateStatsOp. Calling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.,2021-11-18 22:15:24,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/41ab69692ede0db3422fa70bc5889d470741e69c," Check for tensors to be vectors in BoostedTreesSparseAggregateStatsOp. Calling `tensor->vec` should only happen after checking that the tensor shape implies a vector. Otherwise, we can get denial of service via `CHECK`-fails",2021-11-19 10:24:55,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/8d733ecdb270dd90b2b5f53fd220d5ce17a5e20f," Check for tensors to be vectors in BoostedTreesSparseAggregateStatsOp. Calling `tensor->vec` should only happen after checking that the tensor shape implies a vector. Otherwise, we can get denial of service via `CHECK`-fails",2021-11-18 23:21:35,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/f482488b481a799ca07e7e2d153cf47b8e91a60c, TFLite OpenGL ES delegate: out of boundary writes fixed for bhwc->phwc4 conversion.,2019-11-21 19:37:21,,missing,out of bound access,crash,add,checker api,tensor dimension is valid,dimension with specific value,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/63c6a29d0f2d692b247f7bf81f8732d6442fad09,"Add missing validation, prevent heap OOB",2021-05-05 21:18:17,,missing,out of bound access,crash,add,macro checker,output and input tensors has same size,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9596f534200201bc8206b297f17ec3c5cc9fcff8,Fix OOB check for result_index in header generation,2018-06-21 15:35:45,,improper,out of bound access,crash,update,macro checker,output and input tensors has same size,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/58759659ee547a957c5d36e72f2274ab34fdb6cb,Fix OOB check for result_index in header generation,2018-06-20 14:02:39,,improper,out of bound access,crash,update,macro checker,output and input tensors has same size,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/d6ed5bcfe1dcab9e85a4d39931bd18d99018e75b,Add missing validation in QuantizedBatchNormWithGlobalNormalization,2021-04-23 14:54:50,,missing,tensor shape mismatch,crash,add,macro checker,tensor size is valid,tensor size,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/80b65ab79bf8dd6ec03c570b59a1208bb27fec24, Small fix to axis check for tfl.pack to tosa. There was an off-by-one error when checking the axis value based on the input rank.,2023-03-13 18:16:22,,improper,tensor shape mismatch,crash,update,if checker,others,tensor axis,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c2ff14318050e26302785a49a1719d29ddcc91b4," [XNNPACK] Fix incorrect check in slice node. begin+size == input dimension is valid, e.g. input size is 3, begin is 2, size is 1.",2023-04-12 12:01:41,,improper,tensor shape mismatch,crash,update,if checker,axis parameter matches tensor dimension,tensor shape,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/d23458fdd2655c83ff9d54725062ded31b644ba4," [XLA:CPU] Do not check that the size of the XLA parameter buffer is exactly equal to the size of the underlying given buffer Instead, check that the underlying allocation is ""large enough"". This is also more consistent with XLA:GPU behavior. The mismatch can happen when the input comes from tf.where, which is backed by an allocation larger than is actually required.",2021-11-23 19:51:35,,improper,tensor shape mismatch,crash,replace,macro checker,others,tensor shape,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/4863013a3ec5b97c042a38ab567bcc4a62ccde5c, Add checking for number of inputs in GetOptionalInputTensor to avoid indexing out of array bounds.,2019-12-13 19:34:20,,insufficient,out of bound access,crash,extend,if checker,tensor indexing is within range,tensor indices,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1e38a0025c9a983bf3229299109b5b1781215c7e, [XLA] CHECK that sparse indices are in range in MutableLiteralBase::AppendSparseElement. Previously there was no range-checking on sparse elements' indices.,2019-01-30 17:18:25,,missing,out of bound access,crash,add,macro checker,tensor indexing is within range,tensor indices,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1610da3f992487bd9a8181d1e83cae99fe1e34d9,add more sanity check on AvgPoolGrad op,2023-04-10 16:56:07,,missing,tensor shape mismatch,crash,add,macro checker,output and input tensors has same size,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a68f57a24203fd49c4a5c4a8f51098d4415a93f8, [XNNPACK] Add missing return when output channels do not match in TransposeConvolution Add a check that input channels in the filter and tensor match.,2022-05-11 14:21:59,,missing,tensor shape mismatch,crash,add,macro checker,output and input tensors has same size,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/1b54cadd19391b60b6fcccd8d076426f7221d5e8,Add missing validation to sparse dense cwise ops.,2021-12-10 12:51:09,,missing,tensor shape mismatch,crash,add,macro checker,tensor shape match tensor indices,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b51b82fe65ebace4475e3c54eb089c18a4403f1c, Add missing validation to AddManySparseToTensorsMap. Sparse tensors have a set of requirements for the 3 components and not all of them were checked.,2021-12-09 17:41:08,,missing,tensor shape mismatch,crash,add,macro checker,tensor shape match tensor indices,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e560136d757867482a93be74e108ef516920bcfc, Fix wrong output of tf.stack with 0-dimension tensor. This PR tries to address a bug where tf.stack will silently output wrong result with 0-dimension tensor. The issue was that the shape check was skipped when num of output elements was zero.,2021-12-09 3:00:55,,improper,tensor shape mismatch,crash,relocate,macro checker,output and input tensors has same size,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/2b7100d6cdff36aa21010a82269bc05a6d1cc74a," Cleanup and remove duplicate validation in SparseCount. We have valdiation that is duplicated, checking different conditions, in different formats and failing to capture all cases. This should fix all the previous bugs.",2021-12-07 22:40:32,,unnecessary,tensor shape mismatch,crash,remove,macro checker,tensor shape match tensor indices,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/23968a8bf65b009120c43b5ebcceaf52dbc9e943, Fix out of bound access in DequantizeOp by adding check for axis < input dimension,2021-11-20 2:22:12,,missing,tensor shape mismatch,crash,add,macro checker,tensor shape match tensor indices,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/4923de56ec94fff7770df259ab7f2288a74feb41," Don't do any work when reshaping 0 elements sparse tensor. If reshaping to 0 elements tensor, check that input has no elements. If reshaping no elements input, check that output has no elements.",2021-08-02 16:57:19,,missing,tensor shape mismatch,crash,add,macro checker,tensor shape match tensor indices,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/467730fe90282a75f15f67d701b278e86cfad65e,Fix dimension check for tf.keras.losses.BinaryCrossentropy. The reason was that broadcasting was applied directly. This fix adds dimension check to throw an error if there is a mismatch.,2019-06-22 21:53:59,,missing,tensor shape mismatch,crash,add,macro checker,tensor shape match tensor indices,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/c2cec131f107fde9c54f48a9b74248617d850549,Add the shape and dtype validation for TensorDatasetOp,2019-03-14 13:58:26,,missing,tensor shape mismatch,crash,add,macro checker,tensor shape match tensor indices,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/7e2d53c1c371f38c7f0ef13c1c06336b22a195c0,[tf.data] Adds the expected check for better debugging.,2019-01-03 15:51:36,,missing,tensor shape mismatch,crash,add,macro checker,tensor shape match tensor indices,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/a12b8c4afdca3ac2945d62b3b83ca2599ab360f9," [xla] Improve validation of Broadcast shape. If one misreads the semantics of this instruction, it's easy to cause an out of bounds access into the dimensions here. Add an extra check to return a proper error to the user rather than crashing in that case.",2018-10-02 17:00:29,,insufficient,tensor shape mismatch,crash,extend,macro checker,tensor shape match tensor indices,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/32dc203f55a7462ddf780c68d619af574daedd46,Improve gradient shape validation errors.,2017-10-04 19:03:41,,insufficient,tensor shape mismatch,crash,extend,macro checker,tensor shape match tensor indices,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/25821f0d91623d654bb1bdd62423e644bae9f7f8,TensorFlow: Fix OP_REQUIRES check for depthwise pooling.,2016-07-29 17:17:36,,improper,tensor shape mismatch,crash,update,macro checker,tensor shape match tensor indices,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/05ec322172958f6e67e4bcaef4681e6aa54fabeb, Return error message with illegal input rather than check-failing in op_kernel.,2018-09-19 13:55:51,,missing,tensor shape mismatch,crash,add,macro checker,output and input tensors has same size,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/6364463d6f5b6254cac3d6aedf999b6a96225038, [lite] Add some safety checks to avoid out of bound access for sparsity format,2021-12-16 18:46:23,,missing,tensor shape mismatch,crash,add,macro checker,output and input tensors has same size,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/62cb54f2caf48480dc6b3c1ce9629eaac4688f83, Set 2nd output shape for SparseSegmentReduceGradV2 Fixes a debug check failure.,2023-06-13 3:16:58,,missing,tensor shape mismatch,crash,add,macro checker,output and input tensors has same size,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/8f66dd24d1355298482afee9b5299f9f0b5b1817, Add checks to TensorForest that help with debugging when labels are wrong.,2017-07-12 9:36:28,,missing,tensor shape mismatch,crash,add,macro checker,output and input tensors has same size,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9b0f99ddd27e7738732a154be5469391ee8fc977,Add check to ensure element sizes are the same,2021-02-15 21:11:49,,missing,tensor shape mismatch,crash,add,macro checker,output and input tensors has same size,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/f8ec0f101bac066faa2e917ac714ca9eea310eac,adding checks that pad fusion works only Conv2D,2018-09-14 1:40:49,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/76619c8dea0e480fd48e3b4dcfe0249eb24216b8,Validation in shape functions of Dataset ops.* Add shape check for PrependFromQueueAndPaddedBatchDatase. * Add comment for shape check.  * Add shape check for FixedLengthRecordDataset. * Add check for filenames as well. * Add shape check for SqlDataset,2018-04-19 12:13:53,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b71b6b8ca9ade8b39d77f0373210fe58dfccf4f4,Shape validation with random/shuffle related Dataset ops. * Add shape check for CacheDataset. * Add shape check for ShuffleAndRepeatDataset. * Add check for ShuffleDataset. * Add shape check for RandomDataset. * Add RangeDataset shape check,2018-04-19 12:13:35,,missing,tensor shape mismatch,crash,add,macro checker,tensor dimension is valid,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9718fed7b9aba244359b3d38c2a1dc20e50428bd, Added size check to avoid memory corruption in GraphDefImporter::ConvertNodeDef.,2024-01-24 20:18:37,,missing,tensor shape mismatch,crash,add,if checker,output and input tensors has same size,graph operation,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/4ab6a520c94441622442747aef620939cc1d8130," Relax the check for state_size. The behaviour of `hasattr` is to evaluate the state_size member. In the case of `tfa.seq2seq.AttentionWrapper`, that is a @Property member that is built at graph runtime after calling `setup_memory`, thus `hasattr` returns an error when using AttentionWrapper with dynamic memories.",2019-11-21 8:59:30,,insufficient,others,incorrect functionality,extend,if checker,others,tensor state,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/edd9fb416e04b8ca4398c4eea65f14dc6704a44a, TfLiteTensorCopy returns an error status when src and dest bytes are not equal. So we don't need to check them specifically if we ensure the status of the call to copy (which we should do anyways).,2023-04-07 14:45:48,,unnecessary,others,incorrect functionality,remove,macro checker,others,tensor state,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/e44f8a08051baa58bde9130a844a1b82a8179526,"check hasattr on the type, not the instance. hasattr on the instance triggers __getattr__ which carries very undesirable effects, such as running Ops on a donated buffer.",2023-10-31 15:18:59,,missing,type checking,incorrect functionality,add,if checker,object type is valid,tpu embeddings,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/50299228e5df92b486548ee1cb856e79de69ad43,"Fix an incorrect static_assert.  The size of a uint8_t is 1 so a static_assert to check that it is 0 makes no sense, fix it. Also fix a couple of warnings about lack of typename. Fix an incorrect static_assert The size of a uint8_t is 1 so a static_assert to check that it is 0 will always be false. The original intent was to have the assert only trigger if the struct was instantiated but the standard deems it ill formed if it can never be true and allows compilers to reject it. Adopt a different workaround that avoids this by allowing the possibility of an evaluation to true. Also fix a couple of warnings about lack of typename",2023-11-27 18:00:31,,improper,type checking,inconsistent results,replace,type checking api,object type is valid,pointer data types,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/51d72a7d7f74784b68916819edd04e890b36f957," Modified ""_check_is_tensor_or_operation"" to check if ""x"" is ""tensor_like""",2018-09-13 17:59:05,,improper,type checking,incorrect functionality,replace,type checking api,object type is valid,tensor,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/8a2e7deb21f02e4072d6b62cf7f447b9264afe01, Adjust checks for type(Tensor) to isinstance or is_eager/is_symbolic_tensor.,2023-04-20 14:41:01,,improper,type checking,unknown,update,type checking api,object type is valid,tensor,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/b68b869e75916e6de37c2ca23a93643faf333011,Fix invalid keras tensor isinstance check,2020-06-09 16:59:23,,improper,type checking,unknown,update,type checking api,object type is valid,tensor,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/040aaf39aebda57921991d05d29be5123e908d7c,Don't check that bool arrays are quantized.,2018-05-14 14:43:44,,missing,type checking,unknown,add,if checker,tensor dimension is valid,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9a0de0ca6a39f3037e1be6ec740829863bcda3e8,[XLA:GPU] Fix type check in IsMatrixMultiplication,2023-03-02 17:28:58,,improper,type checking,incorrect functionality,update,type checking api,tensor dimension is valid,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/db9b247cd1f3ff046359f7b64ca60c2d697fe2e1," Fix the functional model loading with nested sequential model. The nested sequential model is created with _is_graph_network = False, the current instance check is not strong enough.",2020-05-13 22:56:36,,insufficient,type checking,incorrect functionality,extend,type checking api,others,network layer,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/9a4b6b6bcc7a813162bf0378727950e321aca19c,Add stricter type checking for tf.math.real (using is_numeric),2023-05-09 14:49:51,,improper,type checking,others,update,type checking api,ensure tensor type is valid,tensor,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/8a0fee00855a0e806bd5c9cc1ad6c0175a985922," [XLA] Don't use isnan on values which can't have NaN. While we are here, don't upcast to double just to check if something is NaN.",2023-08-21 16:36:28,,unnecessary,type checking,performance bug,remove,type checking api,others,nan type,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/580140611a47413dcf6373deb1250c0ed605e873, [XLA] Do not check fail in proto copy from if the backend config proto and desired proto type do not match.,2022-05-23 16:53:05,,missing,others,crash,add,if checker,others,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/d3d1cd3ad2becac5c31387f7fc483af65c7c8c84," Fixes the crashes caused by the refcount checks for non-copyable types. For async value refs, refcounts don't equal to number of waiters as waiters can make copies of the async value refs.",2023-07-21 12:30:36,,unnecessary,others,crash,remove,macro checker,others,,,,
tensorflow,https://github.com/tensorflow/tensorflow/commit/02907e867c74651a9eb74971f56559d5db2efa1c, Use Nano seconds in Timestamp check as Pico seconds can lead to overflow.,2022-04-26 16:18:00,,missing,others,numerical error,add,checker api,others,timstamp,,,