[
    {
        "Bug description": "torch.view_as_complex fails with segfault for a zero dimensional tensor (#44175)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/44061\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44175\n\nReviewed By: colesbury\n\nDifferential Revision: D23628103\n\nPulled By: anjali411\n\nfbshipit-source-id: 6f70b5824150121a1617c0757499832923ae02b5",
        "Sample Code": "",
        "Bug fix": [
            "@@ -48,6 +48,7 @@ Tensor view_as_complex(const Tensor& self) {\n     self.scalar_type() == kFloat || self.scalar_type() == kDouble || self.scalar_type() == kHalf,\n     \"view_as_complex is only supported for half, float and double tensors, but got a tensor of scalar type: \", self.scalar_type());\n \n+  TORCH_CHECK(self.dim() != 0, \"Input tensor must have one or more dimensions\");\n   auto new_sizes = self.sizes().vec();\n   TORCH_CHECK(new_sizes[self.dim()-1] == 2, \"Tensor must have a last dimension of size 2\");\n   new_sizes.pop_back();\n",
            "@@ -19220,6 +19220,12 @@ class TestViewOps(TestCase):\n             RuntimeError, \"Tensor must have a last dimension of size 2\",\n             lambda: torch.view_as_complex(x))\n \n+        # zero dimension tensor\n+        z = torch.tensor(2.0)\n+        self.assertRaisesRegex(\n+            RuntimeError, \"Input tensor must have one or more dimensions\",\n+            lambda: torch.view_as_complex(z))\n+\n         y = x.reshape(0, 2)  # torch.Size([0, 2])\n         res = torch.view_as_complex(y)\n         self.assertTrue(self.is_view_of(x, res))\n"
        ]
    },
    {
        "Issue title": "Problematic handling of NaN and inf in grid_sample, causing segfaults, corrupted CUDA memory, and incorrect results",
        "Bug description": "\r\n\r\nThe `grid_sample` function does not have proper handling of `NaN` values for in its grid input.\r\n\r\nThe 2D CPU version segfaults under certain conditions and parameters, as described in https://github.com/pytorch/pytorch/issues/19826, and with simplified examples below.\r\n\r\nThe other `grid_sample` kernels (3D CPU, and 2D/3D CUDA) do not segfault, but produce incorrect results under certain conditions when the grid contains a `NaN` value.\r\n\r\nProper handling would place a `NaN` in the output for every grid location that has a `NaN`.\r\n\r\n### ",
        "Sample Code": [
            "@@ -48,6 +48,7 @@ Tensor view_as_complex(const Tensor& self) {\n     self.scalar_type() == kFloat || self.scalar_type() == kDouble || self.scalar_type() == kHalf,\n     \"view_as_complex is only supported for half, float and double tensors, but got a tensor of scalar type: \", self.scalar_type());\n \n+  TORCH_CHECK(self.dim() != 0, \"Input tensor must have one or more dimensions\");\n   auto new_sizes = self.sizes().vec();\n   TORCH_CHECK(new_sizes[self.dim()-1] == 2, \"Tensor must have a last dimension of size 2\");\n   new_sizes.pop_back();\n",
            "@@ -19220,6 +19220,12 @@ class TestViewOps(TestCase):\n             RuntimeError, \"Tensor must have a last dimension of size 2\",\n             lambda: torch.view_as_complex(x))\n \n+        # zero dimension tensor\n+        z = torch.tensor(2.0)\n+        self.assertRaisesRegex(\n+            RuntimeError, \"Input tensor must have one or more dimensions\",\n+            lambda: torch.view_as_complex(z))\n+\n         y = x.reshape(0, 2)  # torch.Size([0, 2])\n         res = torch.view_as_complex(y)\n         self.assertTrue(self.is_view_of(x, res))\n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.Generator` produces segfault when input is `device='cuda'` on a machine that is not CUDA-enabled",
        "Bug description": "\r\nThe bug is as described in the title. The bug appears to be only with v1.6.0. \r\n\r\nThe case of inputting `device='cuda'` on a machine that is not CUDA-enabled might be rare. But a segfault as outcome doesn't seem to be desirable in any scenario.\r\n\r\n## ",
        "Sample Code": [
            "@@ -48,6 +48,7 @@ Tensor view_as_complex(const Tensor& self) {\n     self.scalar_type() == kFloat || self.scalar_type() == kDouble || self.scalar_type() == kHalf,\n     \"view_as_complex is only supported for half, float and double tensors, but got a tensor of scalar type: \", self.scalar_type());\n \n+  TORCH_CHECK(self.dim() != 0, \"Input tensor must have one or more dimensions\");\n   auto new_sizes = self.sizes().vec();\n   TORCH_CHECK(new_sizes[self.dim()-1] == 2, \"Tensor must have a last dimension of size 2\");\n   new_sizes.pop_back();\n",
            "@@ -19220,6 +19220,12 @@ class TestViewOps(TestCase):\n             RuntimeError, \"Tensor must have a last dimension of size 2\",\n             lambda: torch.view_as_complex(x))\n \n+        # zero dimension tensor\n+        z = torch.tensor(2.0)\n+        self.assertRaisesRegex(\n+            RuntimeError, \"Input tensor must have one or more dimensions\",\n+            lambda: torch.view_as_complex(z))\n+\n         y = x.reshape(0, 2)  # torch.Size([0, 2])\n         res = torch.view_as_complex(y)\n         self.assertTrue(self.is_view_of(x, res))\n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "max_pool1d creates illegal memory access for large kernel sizes",
        "Bug description": "\r\nThe bug is as described in the title. The bug appears to be only with v1.6.0. \r\n\r\nThe case of inputting `device='cuda'` on a machine that is not CUDA-enabled might be rare. But a segfault as outcome doesn't seem to be desirable in any scenario.\r\n\r\n## ",
        "Sample Code": [
            "@@ -48,6 +48,7 @@ Tensor view_as_complex(const Tensor& self) {\n     self.scalar_type() == kFloat || self.scalar_type() == kDouble || self.scalar_type() == kHalf,\n     \"view_as_complex is only supported for half, float and double tensors, but got a tensor of scalar type: \", self.scalar_type());\n \n+  TORCH_CHECK(self.dim() != 0, \"Input tensor must have one or more dimensions\");\n   auto new_sizes = self.sizes().vec();\n   TORCH_CHECK(new_sizes[self.dim()-1] == 2, \"Tensor must have a last dimension of size 2\");\n   new_sizes.pop_back();\n",
            "@@ -19220,6 +19220,12 @@ class TestViewOps(TestCase):\n             RuntimeError, \"Tensor must have a last dimension of size 2\",\n             lambda: torch.view_as_complex(x))\n \n+        # zero dimension tensor\n+        z = torch.tensor(2.0)\n+        self.assertRaisesRegex(\n+            RuntimeError, \"Input tensor must have one or more dimensions\",\n+            lambda: torch.view_as_complex(z))\n+\n         y = x.reshape(0, 2)  # torch.Size([0, 2])\n         res = torch.view_as_complex(y)\n         self.assertTrue(self.is_view_of(x, res))\n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault when dividing by zero with integer tensors",
        "Bug description": "\r\nThe bug is as described in the title. The bug appears to be only with v1.6.0. \r\n\r\nThe case of inputting `device='cuda'` on a machine that is not CUDA-enabled might be rare. But a segfault as outcome doesn't seem to be desirable in any scenario.\r\n\r\n## ",
        "Sample Code": [
            "@@ -48,6 +48,7 @@ Tensor view_as_complex(const Tensor& self) {\n     self.scalar_type() == kFloat || self.scalar_type() == kDouble || self.scalar_type() == kHalf,\n     \"view_as_complex is only supported for half, float and double tensors, but got a tensor of scalar type: \", self.scalar_type());\n \n+  TORCH_CHECK(self.dim() != 0, \"Input tensor must have one or more dimensions\");\n   auto new_sizes = self.sizes().vec();\n   TORCH_CHECK(new_sizes[self.dim()-1] == 2, \"Tensor must have a last dimension of size 2\");\n   new_sizes.pop_back();\n",
            "@@ -19220,6 +19220,12 @@ class TestViewOps(TestCase):\n             RuntimeError, \"Tensor must have a last dimension of size 2\",\n             lambda: torch.view_as_complex(x))\n \n+        # zero dimension tensor\n+        z = torch.tensor(2.0)\n+        self.assertRaisesRegex(\n+            RuntimeError, \"Input tensor must have one or more dimensions\",\n+            lambda: torch.view_as_complex(z))\n+\n         y = x.reshape(0, 2)  # torch.Size([0, 2])\n         res = torch.view_as_complex(y)\n         self.assertTrue(self.is_view_of(x, res))\n"
        ],
        "Bug fix": ""
    },
    {
        "Bug description": "use non-overflowing divide in cuda kernel util GET_BLOCKS (#44391)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/43476.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44391\n\nReviewed By: mrshenli\n\nDifferential Revision: D23602424\n\nPulled By: walterddr\n\nfbshipit-source-id: 40ed81547f933194ce5bf4a5bcebdb3434298bc1",
        "Sample Code": "",
        "Bug fix": [
            "@@ -25,16 +25,15 @@ namespace at { namespace cuda { namespace detail {\n constexpr int CUDA_NUM_THREADS = 1024;\n \n // CUDA: number of blocks for threads.\n-inline int GET_BLOCKS(const int N)\n-{\n-  AT_ASSERTM(N > 0, \"CUDA kernel launch blocks must be positive, but got N=\", N);\n-  return (N + CUDA_NUM_THREADS - 1) / CUDA_NUM_THREADS;\n-}\n-\n inline int GET_BLOCKS(const int64_t N) {\n   AT_ASSERTM(N > 0, \"CUDA kernel launch blocks must be positive, but got N=\", N);\n   constexpr int64_t max_int = std::numeric_limits<int>::max();\n-  return GET_BLOCKS(static_cast<int>(std::min(max_int, N)));\n+\n+  // Round up division for positive number that cannot cause integer overflow\n+  auto block_num = (N - 1) / CUDA_NUM_THREADS + 1;\n+  AT_ASSERTM(block_num <= max_int, \"Can't schedule too many blocks on CUDA device\");\n+\n+  return static_cast<int>(block_num);\n }\n \n }}}  // namespace at::cuda::detail\n"
        ]
    },
    {
        "Bug description": "Raise error if `at::native::embedding` is given 0-D weight (#42550)\n\nSummary:\nPreviously, `at::native::embedding` implicitly assumed that the `weight` argument would be 1-D or greater. Given a 0-D tensor, it would segfault. This change makes it throw a RuntimeError instead.\n\nFixes https://github.com/pytorch/pytorch/issues/41780\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42550\n\nReviewed By: smessmer\n\nDifferential Revision: D23040744\n\nPulled By: albanD\n\nfbshipit-source-id: d3d315850a5ee2d2b6fcc0bdb30db2b76ffffb01",
        "Sample Code": "",
        "Bug fix": [
            "@@ -13,6 +13,7 @@ namespace at { namespace native {\n \n Tensor embedding(const Tensor & weight, const Tensor & indices,\n                  int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {\n+  TORCH_CHECK(weight.dim() >= 1, \"'weight' must be at least 1-D\");\n   auto indices_arg = TensorArg(indices, \"indices\", 1);\n   checkScalarType(\"embedding\", indices_arg, kLong);\n \n",
            "@@ -9901,6 +9901,12 @@ class TestNNDeviceType(NNTestCase):\n         fn = fn_wrapper(device)\n         _assertGradAndGradgradChecks(self, fn, (weight, ))\n \n+    def test_embedding_scalar_weight_error(self, device):\n+        indices = torch.rand(2, 2, device=device).long()\n+        weight = torch.tensor(1.0, device=device)\n+        with self.assertRaisesRegex(RuntimeError, \"'weight' must be at least 1-D\"):\n+            torch.nn.functional.embedding(indices, weight)\n+\n     @dtypesIfCUDA(torch.float16, torch.float64)\n     @dtypes(torch.float64)\n     def test_embedding_backward(self, device, dtype):\n",
            "@@ -10306,6 +10306,12 @@ class TestTorchDeviceType(TestCase):\n         self.assertRaisesRegex(RuntimeError, \"duplicate or invalid\", torch.norm, x, \"nuc\", (0, 0))\n         self.assertRaisesRegex(RuntimeError, \"duplicate or invalid\", torch.norm, x, \"nuc\", (0, 2))\n \n+    def test_embedding_scalar_weight_error(self, device):\n+        indices = torch.rand(2, 2, device=device).long()\n+        weight = torch.tensor(1.0)\n+        with self.assertRaisesRegex(RuntimeError, \"'weight' must be at least 1-D\"):\n+            torch.embedding(weight, indices)\n+\n     def test_dist(self, device):\n         def run_test(x, y):\n             for p in [0, 1, 2, 3, 4, inf, -inf]:\n"
        ]
    },
    {
        "Issue title": "[distributed] NCCL Backend doesn't support torch.bool data type",
        "Bug description": "\r\n\r\nIn version 1.2.0, NCCL backend doesn't support `torch.bool` datatype. Broadcasting a tensor of this type throws error \"RuntimeError: Unsupported data type for NCCL process group\".\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nCreate a file test.py with following contents:\r\n```py\r\nimport torch\r\nimport argparse\r\nfrom torch import distributed as dist\r\n\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--local_rank\", type=int)\r\n\r\nargs = parser.parse_args()\r\n\r\ntorch.distributed.init_process_group(\"nccl\")\r\n\r\nlocal_rank = args.local_rank\r\n\r\ndevice = torch.device(local_rank)\r\n\r\nif local_rank == 0:\r\n    element = False\r\nelse:\r\n    element = True\r\n\r\n\r\ndef broadcast_scalar(scalar, src=0, device=\"cpu\"):\r\n    scalar_tensor = torch.tensor(scalar).to(device)\r\n    with torch.no_grad():\r\n        scalar_tensor = dist.broadcast(scalar_tensor, src)\r\n    return scalar_tensor.item()\r\n\r\n\r\nbroadcast_scalar(element, src=0, device=device)\r\n```\r\n\r\nRun it with following command:\r\n`python -u -m torch.distributed.launch --nproc_per_node 2 test.py`\r\n\r\nThis has been tested on 2 GPUs.\r\n\r\n## ",
        "Sample Code": [
            "@@ -13,6 +13,7 @@ namespace at { namespace native {\n \n Tensor embedding(const Tensor & weight, const Tensor & indices,\n                  int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {\n+  TORCH_CHECK(weight.dim() >= 1, \"'weight' must be at least 1-D\");\n   auto indices_arg = TensorArg(indices, \"indices\", 1);\n   checkScalarType(\"embedding\", indices_arg, kLong);\n \n",
            "@@ -9901,6 +9901,12 @@ class TestNNDeviceType(NNTestCase):\n         fn = fn_wrapper(device)\n         _assertGradAndGradgradChecks(self, fn, (weight, ))\n \n+    def test_embedding_scalar_weight_error(self, device):\n+        indices = torch.rand(2, 2, device=device).long()\n+        weight = torch.tensor(1.0, device=device)\n+        with self.assertRaisesRegex(RuntimeError, \"'weight' must be at least 1-D\"):\n+            torch.nn.functional.embedding(indices, weight)\n+\n     @dtypesIfCUDA(torch.float16, torch.float64)\n     @dtypes(torch.float64)\n     def test_embedding_backward(self, device, dtype):\n",
            "@@ -10306,6 +10306,12 @@ class TestTorchDeviceType(TestCase):\n         self.assertRaisesRegex(RuntimeError, \"duplicate or invalid\", torch.norm, x, \"nuc\", (0, 0))\n         self.assertRaisesRegex(RuntimeError, \"duplicate or invalid\", torch.norm, x, \"nuc\", (0, 2))\n \n+    def test_embedding_scalar_weight_error(self, device):\n+        indices = torch.rand(2, 2, device=device).long()\n+        weight = torch.tensor(1.0)\n+        with self.assertRaisesRegex(RuntimeError, \"'weight' must be at least 1-D\"):\n+            torch.embedding(weight, indices)\n+\n     def test_dist(self, device):\n         def run_test(x, y):\n             for p in [0, 1, 2, 3, 4, inf, -inf]:\n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "max_pool1d creates illegal memory access for large kernel sizes",
        "Bug description": "\r\n\r\nIn version 1.2.0, NCCL backend doesn't support `torch.bool` datatype. Broadcasting a tensor of this type throws error \"RuntimeError: Unsupported data type for NCCL process group\".\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nCreate a file test.py with following contents:\r\n```py\r\nimport torch\r\nimport argparse\r\nfrom torch import distributed as dist\r\n\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--local_rank\", type=int)\r\n\r\nargs = parser.parse_args()\r\n\r\ntorch.distributed.init_process_group(\"nccl\")\r\n\r\nlocal_rank = args.local_rank\r\n\r\ndevice = torch.device(local_rank)\r\n\r\nif local_rank == 0:\r\n    element = False\r\nelse:\r\n    element = True\r\n\r\n\r\ndef broadcast_scalar(scalar, src=0, device=\"cpu\"):\r\n    scalar_tensor = torch.tensor(scalar).to(device)\r\n    with torch.no_grad():\r\n        scalar_tensor = dist.broadcast(scalar_tensor, src)\r\n    return scalar_tensor.item()\r\n\r\n\r\nbroadcast_scalar(element, src=0, device=device)\r\n```\r\n\r\nRun it with following command:\r\n`python -u -m torch.distributed.launch --nproc_per_node 2 test.py`\r\n\r\nThis has been tested on 2 GPUs.\r\n\r\n## ",
        "Sample Code": [
            "@@ -13,6 +13,7 @@ namespace at { namespace native {\n \n Tensor embedding(const Tensor & weight, const Tensor & indices,\n                  int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {\n+  TORCH_CHECK(weight.dim() >= 1, \"'weight' must be at least 1-D\");\n   auto indices_arg = TensorArg(indices, \"indices\", 1);\n   checkScalarType(\"embedding\", indices_arg, kLong);\n \n",
            "@@ -9901,6 +9901,12 @@ class TestNNDeviceType(NNTestCase):\n         fn = fn_wrapper(device)\n         _assertGradAndGradgradChecks(self, fn, (weight, ))\n \n+    def test_embedding_scalar_weight_error(self, device):\n+        indices = torch.rand(2, 2, device=device).long()\n+        weight = torch.tensor(1.0, device=device)\n+        with self.assertRaisesRegex(RuntimeError, \"'weight' must be at least 1-D\"):\n+            torch.nn.functional.embedding(indices, weight)\n+\n     @dtypesIfCUDA(torch.float16, torch.float64)\n     @dtypes(torch.float64)\n     def test_embedding_backward(self, device, dtype):\n",
            "@@ -10306,6 +10306,12 @@ class TestTorchDeviceType(TestCase):\n         self.assertRaisesRegex(RuntimeError, \"duplicate or invalid\", torch.norm, x, \"nuc\", (0, 0))\n         self.assertRaisesRegex(RuntimeError, \"duplicate or invalid\", torch.norm, x, \"nuc\", (0, 2))\n \n+    def test_embedding_scalar_weight_error(self, device):\n+        indices = torch.rand(2, 2, device=device).long()\n+        weight = torch.tensor(1.0)\n+        with self.assertRaisesRegex(RuntimeError, \"'weight' must be at least 1-D\"):\n+            torch.embedding(weight, indices)\n+\n     def test_dist(self, device):\n         def run_test(x, y):\n             for p in [0, 1, 2, 3, 4, inf, -inf]:\n"
        ],
        "Bug fix": ""
    },
    {
        "Bug description": "Fix overflow in torch.remainder when dividend is very large (#37758)\n\nSummary:\nThis will fix the GPU implementation in https://github.com/pytorch/pytorch/issues/37743 and https://github.com/pytorch/pytorch/issues/24861. Please also check my [comment](https://github.com/pytorch/pytorch/issues/37743#issuecomment-623285707).\n\nThe fixed `remainder_kernel` follows the similar implementation in numpy. See https://github.com/numpy/numpy/blob/79d7bc276afbe89c746e462d28d4bfbb4fc56148/numpy/core/src/npymath/npy_math_internal.h.src#L649-L658\n\nI also slightly update the doc for `torch.remainder`, to make it similar to `torch.fmod`.\n\nI'm not sure how to modify the Vec256 code of CPU remainder_kernel, so I just leave it there.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37758\n\nDifferential Revision: D21388417\n\nPulled By: ngimel\n\nfbshipit-source-id: 770ba5801cf34619b2b68b8b0cf95d8cfa52e6f6",
        "Sample Code": "",
        "Bug fix": [
            "@@ -77,7 +77,9 @@ void remainder_kernel_cuda(TensorIterator& iter) {\n     AT_DISPATCH_FLOATING_TYPES_AND_HALF(iter.dtype(), \"remainder_cuda\", [&]() {\n       gpu_kernel_with_scalars(iter,\n         []GPU_LAMBDA(scalar_t a, scalar_t b) __ubsan_ignore_float_divide_by_zero__ -> scalar_t {\n-          return a - b * static_cast<scalar_t>(std::floor(a / b));\n+          auto mod = ::fmod(a, b);\n+          if ((mod != 0) && ((b < 0) != (mod < 0))) mod += b;\n+          return mod;\n         });\n     });\n   }\n",
            "@@ -15449,6 +15449,27 @@ scipy_lobpcg  | {:10.2e}  | {:10.2e}  | {:6} | N/A\n                 long_res1 = long_m1.clone()\n                 long_res1.remainder_(long_qs.unsqueeze(0).expand_as(long_res1))\n \n+    # remove onlyCUDA after CPU impl of remainder_kernel be fixed\n+    @onlyCUDA\n+    @dtypes(torch.float, torch.double)\n+    def test_remainder_fmod_large_dividend(self, device, dtype):\n+        alarge = 1e9\n+        pi = 3.14159265358979\n+        for avalue in [alarge, -alarge]:\n+            for bvalue in [pi, -pi]:\n+                a = torch.tensor([avalue], dtype=dtype, device=device)\n+                b = torch.tensor([bvalue], dtype=dtype, device=device)\n+                c = torch.remainder(a, b)\n+                d = torch.fmod(a, b)\n+                self.assertTrue((b[0] > 0) == (c[0] > 0))  # remainder has same sign as divisor\n+                self.assertTrue((a[0] > 0) == (d[0] > 0))  # fmod has same sign as dividend\n+                self.assertTrue(abs(c[0]) < abs(b[0]))     # remainder is within range of divisor\n+                self.assertTrue(abs(d[0]) < abs(b[0]))     # fmod is within range of divisor\n+                if ((a[0] > 0) == (b[0] > 0)):\n+                    self.assertTrue(c[0] == d[0])   # remainder is same as fmod\n+                else:\n+                    self.assertTrue(abs(c[0] - d[0]) == abs(b[0]))  # differ by one divisor\n+\n     @dtypes(torch.int64, torch.float64)\n     def test_remainder_edge_cases(self, device, dtype):\n         # Test variations of negative values used as input\n",
            "@@ -4902,8 +4902,8 @@ remainder(input, other, out=None) -> Tensor\n \n Computes the element-wise remainder of division.\n \n-The divisor and dividend may contain both for integer and floating point\n-numbers. The remainder has the same sign as the divisor.\n+The dividend and divisor may contain both for integer and floating point\n+numbers. The remainder has the same sign as the divisor :attr:`other`.\n \n When :attr:`other` is a tensor, the shapes of :attr:`input` and\n :attr:`other` must be :ref:`broadcastable <broadcasting-semantics>`.\n"
        ]
    },
    {
        "Bug description": "extend gather shape check to handle incorrectly sized outputs (#37102)\n\nSummary:\nFixes a safety issue (Nonsense values and segfaults) introduced by https://github.com/pytorch/pytorch/pull/36875 when in-place gather tries to use incorrect shapes.\n\nConsider the following block of code:\n```\nk0 = 8\nk1 = 8\nm = 100\n\nx = torch.rand((k0, k1))\nind = torch.randint(0, k0, (m, k1))\noutput = torch.empty((m, k1))\n\nprint(torch.gather(x, 0, ind, out=output))\nprint(torch.gather(x, 1, ind, out=output))\n```\n\nThe first gather is legal, the second is not. (`ind` and `output` need to be transposed) Previously this was caught when the kernel tried to restride inputs for TensorIterator, but we can no longer rely on those checks and must test explicitly. If `m` is small the second gather returns gibberish; if it is large enough to push the read out of memory block the program segfaults.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37102\n\nDifferential Revision: D21190580\n\nPulled By: robieta\n\nfbshipit-source-id: 80175620d24ad3380d78995f7ec7dbf2627d2998",
        "Sample Code": "",
        "Bug fix": [
            "@@ -9,23 +9,37 @@ namespace {\n \n // Used for `gather`-like methods\n // Test:\n-// 1. index.size(d) == self.size(d) for all d != dim\n-void gather_shape_check(const Tensor& self, int64_t dim, const Tensor& index) {\n+// 1. index.size(d) == src.size(d) for all d != dim\n+// 2. index.size(d) == self.size(d) for all d\n+void gather_shape_check(const Tensor& self, int64_t dim, const Tensor& index, const Tensor& src) {\n   auto self_dims = ensure_nonempty_dim(self.dim());\n+  auto src_dims = ensure_nonempty_dim(src.dim());\n \n   TORCH_CHECK(self_dims == ensure_nonempty_dim(index.dim()),\n     \"Index tensor must have the same number of dimensions as input tensor\"\n   );\n \n+  TORCH_CHECK(src_dims == ensure_nonempty_dim(index.dim()),\n+    \"Index tensor must have the same number of dimensions as output tensor\"\n+  );\n+\n   for (int64_t i = 0; i < self_dims; ++i) {\n+    auto index_size = ensure_nonempty_size(index, i);\n     if (i != dim) {\n       TORCH_CHECK(\n-        ensure_nonempty_size(index, i) == ensure_nonempty_size(self, i),\n-        \"Size does not match at dimension \", i,\n-        \" get \", ensure_nonempty_size(self, i),\n+        index_size == ensure_nonempty_size(src, i),\n+        \"Output size does not match at dimension \", i,\n+        \" get \", ensure_nonempty_size(src, i),\n         \" vs \", ensure_nonempty_size(index, i)\n       );\n     }\n+    TORCH_CHECK(\n+      index_size == ensure_nonempty_size(self, i),\n+      \"Input size does not match at dimension \", i,\n+      \" get \", ensure_nonempty_size(self, i),\n+      \" vs \", ensure_nonempty_size(index, i)\n+    );\n+\n   }\n }\n \n@@ -131,7 +145,7 @@ struct cpu_scatter_gather_base_kernel {\n       scatter_shape_check(self, dim, index, src);\n     }\n     else {\n-      gather_shape_check(self, dim, index);\n+      gather_shape_check(self, dim, index, src);\n     }\n \n     auto iter = TensorIterator();\n",
            "@@ -2534,6 +2534,9 @@ class _TestTorchMixin(object):\n                     expected[i, j, k] = src[tuple(ii)]\n         self.assertEqual(actual, expected, 0)\n \n+        bad_src = torch.randn(*[i - 1 for i in idx_size])\n+        self.assertRaises(RuntimeError, lambda: torch.gather(bad_src, dim, idx))\n+\n         if test_bounds:\n             idx[0][0][0] = 23\n             self.assertRaises(RuntimeError, lambda: torch.gather(src, dim, idx))\n"
        ]
    },
    {
        "Issue title": "segfaults on .numpy() on cuda tensor",
        "Bug description": " of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. torch.zeros((7, ), device=\"cuda\").numpy()\r\n\r\nproduces segfaults.\r\n\r\nWith stacktrace include raising the exception with message to call .cpu method:\r\nhttps://our.intern.facebook.com/intern/diffusion/FBS/browse/master/fbcode/caffe2/torch/csrc/utils/tensor_numpy.cpp?commit=cea4fcd5898cf33ec83ebd2eac23eb035047fe27&lines=78-80\r\n\r\n## Expected behavior\r\n\r\nruntime error with message about having to call .cpu first.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): trunk\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @ezyang @gchanan @zou3519",
        "Sample Code": [
            "@@ -9,23 +9,37 @@ namespace {\n \n // Used for `gather`-like methods\n // Test:\n-// 1. index.size(d) == self.size(d) for all d != dim\n-void gather_shape_check(const Tensor& self, int64_t dim, const Tensor& index) {\n+// 1. index.size(d) == src.size(d) for all d != dim\n+// 2. index.size(d) == self.size(d) for all d\n+void gather_shape_check(const Tensor& self, int64_t dim, const Tensor& index, const Tensor& src) {\n   auto self_dims = ensure_nonempty_dim(self.dim());\n+  auto src_dims = ensure_nonempty_dim(src.dim());\n \n   TORCH_CHECK(self_dims == ensure_nonempty_dim(index.dim()),\n     \"Index tensor must have the same number of dimensions as input tensor\"\n   );\n \n+  TORCH_CHECK(src_dims == ensure_nonempty_dim(index.dim()),\n+    \"Index tensor must have the same number of dimensions as output tensor\"\n+  );\n+\n   for (int64_t i = 0; i < self_dims; ++i) {\n+    auto index_size = ensure_nonempty_size(index, i);\n     if (i != dim) {\n       TORCH_CHECK(\n-        ensure_nonempty_size(index, i) == ensure_nonempty_size(self, i),\n-        \"Size does not match at dimension \", i,\n-        \" get \", ensure_nonempty_size(self, i),\n+        index_size == ensure_nonempty_size(src, i),\n+        \"Output size does not match at dimension \", i,\n+        \" get \", ensure_nonempty_size(src, i),\n         \" vs \", ensure_nonempty_size(index, i)\n       );\n     }\n+    TORCH_CHECK(\n+      index_size == ensure_nonempty_size(self, i),\n+      \"Input size does not match at dimension \", i,\n+      \" get \", ensure_nonempty_size(self, i),\n+      \" vs \", ensure_nonempty_size(index, i)\n+    );\n+\n   }\n }\n \n@@ -131,7 +145,7 @@ struct cpu_scatter_gather_base_kernel {\n       scatter_shape_check(self, dim, index, src);\n     }\n     else {\n-      gather_shape_check(self, dim, index);\n+      gather_shape_check(self, dim, index, src);\n     }\n \n     auto iter = TensorIterator();\n",
            "@@ -2534,6 +2534,9 @@ class _TestTorchMixin(object):\n                     expected[i, j, k] = src[tuple(ii)]\n         self.assertEqual(actual, expected, 0)\n \n+        bad_src = torch.randn(*[i - 1 for i in idx_size])\n+        self.assertRaises(RuntimeError, lambda: torch.gather(bad_src, dim, idx))\n+\n         if test_bounds:\n             idx[0][0][0] = 23\n             self.assertRaises(RuntimeError, lambda: torch.gather(src, dim, idx))\n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segfault when indexing with single element array",
        "Bug description": " of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nWe are trying to use PyTorch on a Rapsberry PI 4 on Buster, but get a segfault with a very simple test program:\r\n\r\n```python\r\n$ python\r\nPython 3.7.3 (default, Apr  3 2019, 05:39:12) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n>>> v = torch.FloatTensor(1, 135).fill_(0)\r\n>>> v[0, [1]] += 2\r\nSegmentation fault\r\n```\r\n\r\nThe issue seems to only occur when we index `v` using a single element array (whether python list, numpy array or pytorch tensor). Using a multi-element list or `:`, or a single value did not seem to have the issue in our limited testing.\r\n\r\nFollowing is the gdb trace:\r\n\r\n```\r\n$ gdb --args python\r\nGNU gdb (Raspbian 8.2.1-2) 8.2.1\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\nType \"show copying\" and \"show warranty\" for details.\r\nThis GDB was configured as \"arm-linux-gnueabihf\".\r\nType \"show configuration\" for configuration details.\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>.\r\nFind the GDB manual and other documentation resources online at:\r\n    <http://www.gnu.org/software/gdb/documentation/>.\r\n\r\nFor help, type \"help\".\r\nType \"apropos word\" to search for commands related to \"word\"...\r\nReading symbols from python...(no debugging symbols found)...done.\r\n(gdb) run\r\nStarting program: /home/pi/pytorch1.3/bin/python \r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/arm-linux-gnueabihf/libthread_db.so.1\".\r\nPython 3.7.3 (default, Apr  3 2019, 05:39:12) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n[New Thread 0xb5067460 (LWP 26627)]\r\n[New Thread 0xb40c6460 (LWP 26628)]\r\n[New Thread 0xb1125460 (LWP 26629)]\r\n[Thread 0xb1125460 (LWP 26629) exited]\r\n[Thread 0xb40c6460 (LWP 26628) exited]\r\n[Thread 0xb5067460 (LWP 26627) exited]\r\n[Detaching after fork from child process 26631]\r\n>>> v = torch.FloatTensor(1, 135).fill_(0)\r\n>>> v[0, 1] += 2\r\n>>> v[0, [1]] += 2\r\n\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n0xac0f2868 in at::detail::computeStride(c10::ArrayRef<long long>, c10::ArrayRef<long long>, c10::ArrayRef<long long>) ()\r\n   from /home/pi/pytorch1.3/lib/python3.7/site-packages/torch/lib/libtorch.so\r\n```\r\n\r\n\r\n## Environment\r\n\r\nI compiled pytorch on that pi (in a virtual env), using pytorch v1.3.1 and v1.4.0 from github and both had the same issue. The basic steps was:\r\n\r\n* Install apt deps (e.g. `libatlas-base-dev`).\r\n* Checkout pytorch and submodules from github (`git clone --recursive https://github.com/pytorch/pytorch --branch=v1.3.1`).\r\n* `git submodule update --remote third_party/protobuf` to fix a bug in the current version.\r\n* Export env variables:\r\n  ```\r\n  export NO_CUDA=1\r\n  export NO_DISTRIBUTED=1\r\n  export NO_MKLDNN=1 \r\n  export NO_NNPACK=1\r\n  export NO_QNNPACK=1\r\n  export USE_NUMPY=1\r\n  ```\r\n* Install pip deps (numpy, pyyaml, etc.).\r\n* `python3 setup.py build`.\r\n* `python3 setup.py install`.\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.3.0a0+ee77ccb\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Raspbian GNU/Linux 10 (buster)\r\nGCC version: (Raspbian 8.3.0-6+rpi1) 8.3.0\r\nCMake version: version 3.13.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.3.0a0+ee77ccb\r\n[conda] Could not collect\r\n```\n\ncc @ezyang @gchanan @zou3519",
        "Sample Code": [
            "@@ -9,23 +9,37 @@ namespace {\n \n // Used for `gather`-like methods\n // Test:\n-// 1. index.size(d) == self.size(d) for all d != dim\n-void gather_shape_check(const Tensor& self, int64_t dim, const Tensor& index) {\n+// 1. index.size(d) == src.size(d) for all d != dim\n+// 2. index.size(d) == self.size(d) for all d\n+void gather_shape_check(const Tensor& self, int64_t dim, const Tensor& index, const Tensor& src) {\n   auto self_dims = ensure_nonempty_dim(self.dim());\n+  auto src_dims = ensure_nonempty_dim(src.dim());\n \n   TORCH_CHECK(self_dims == ensure_nonempty_dim(index.dim()),\n     \"Index tensor must have the same number of dimensions as input tensor\"\n   );\n \n+  TORCH_CHECK(src_dims == ensure_nonempty_dim(index.dim()),\n+    \"Index tensor must have the same number of dimensions as output tensor\"\n+  );\n+\n   for (int64_t i = 0; i < self_dims; ++i) {\n+    auto index_size = ensure_nonempty_size(index, i);\n     if (i != dim) {\n       TORCH_CHECK(\n-        ensure_nonempty_size(index, i) == ensure_nonempty_size(self, i),\n-        \"Size does not match at dimension \", i,\n-        \" get \", ensure_nonempty_size(self, i),\n+        index_size == ensure_nonempty_size(src, i),\n+        \"Output size does not match at dimension \", i,\n+        \" get \", ensure_nonempty_size(src, i),\n         \" vs \", ensure_nonempty_size(index, i)\n       );\n     }\n+    TORCH_CHECK(\n+      index_size == ensure_nonempty_size(self, i),\n+      \"Input size does not match at dimension \", i,\n+      \" get \", ensure_nonempty_size(self, i),\n+      \" vs \", ensure_nonempty_size(index, i)\n+    );\n+\n   }\n }\n \n@@ -131,7 +145,7 @@ struct cpu_scatter_gather_base_kernel {\n       scatter_shape_check(self, dim, index, src);\n     }\n     else {\n-      gather_shape_check(self, dim, index);\n+      gather_shape_check(self, dim, index, src);\n     }\n \n     auto iter = TensorIterator();\n",
            "@@ -2534,6 +2534,9 @@ class _TestTorchMixin(object):\n                     expected[i, j, k] = src[tuple(ii)]\n         self.assertEqual(actual, expected, 0)\n \n+        bad_src = torch.randn(*[i - 1 for i in idx_size])\n+        self.assertRaises(RuntimeError, lambda: torch.gather(bad_src, dim, idx))\n+\n         if test_bounds:\n             idx[0][0][0] = 23\n             self.assertRaises(RuntimeError, lambda: torch.gather(src, dim, idx))\n"
        ],
        "Bug fix": ""
    },
    {
        "Bug description": "Use int64 in pdist kernel to handle batches >= 46342 #30583 (#31593)\n\nSummary:\nCurrently `torch.pdist` yields an illegal CUDA memory access for batch sizes >= 46342 as reported by SsnL in https://github.com/pytorch/pytorch/issues/30583.\nThanks for the minimal code reproduction, btw! ;)\n\nReason for this bug:\nThe calculation if `i` in the [`pdist_kerne_cuda_impl`](https://github.com/pytorch/pytorch/blob/46ad80c8395379be5ba17624fd5dbad8e7a8e8d2/aten/src/ATen/native/cuda/DistanceKernel.cu#L112) might overflow, if a tensor with a `batch size >= 46342` is passed to `torch.pdist`.\n\nDetailed description:\n* `result` is resizes as ` n * (n - 1) / 2 = 1073767311` ([line of code](https://github.com/pytorch/pytorch/blob/46ad80c8395379be5ba17624fd5dbad8e7a8e8d2/aten/src/ATen/native/Distance.cpp#L140))\n* `grid` is initialized as `result.numel()` ([line of code](https://github.com/pytorch/pytorch/blob/46ad80c8395379be5ba17624fd5dbad8e7a8e8d2/aten/src/ATen/native/cuda/DistanceKernel.cu#L246))\n* `k` is assigned to the `blockIdx.x` as an `int32` ([line of code](https://github.com/pytorch/pytorch/blob/46ad80c8395379be5ba17624fd5dbad8e7a8e8d2/aten/src/ATen/native/cuda/DistanceKernel.cu#L108))\n* `i` is calculated using `2 * k >= 2147534622` ([line of code](https://github.com/pytorch/pytorch/blob/46ad80c8395379be5ba17624fd5dbad8e7a8e8d2/aten/src/ATen/native/cuda/DistanceKernel.cu#L112)), which overflows, since `2147534622 > 2147483647 (int32_max)`.\n\nUsing `const int64_t k = blockIdx.x;` would solve the illegal memory access. This seems also be done for [`cdist_kernel_cuda_impl`](https://github.com/pytorch/pytorch/blob/46ad80c8395379be5ba17624fd5dbad8e7a8e8d2/aten/src/ATen/native/cuda/DistanceKernel.cu#L198-L201).\n\nHowever, we might expect a slowdown, so I've timed the current PyTorch master vs. this PR:\n(tested with `x = torch.randn(x.size(0), 128)` on a V100)\n\n |x.size(0) | int32 idx | int64 idx | slowdown |\n |----------|-----------|-----------|----------|\n| 50000 | -              | 4.4460 | - |\n| 25000 | 1.02522 | 1.10869 | 7.53% |\n| 12500 | 0.25182 | 0.27277 | 7.68% |\n| 6250 | 0.06291 | 0.06817 | 7.72% |\n| 3125 | 0.01573 | 0.01704 | 7.69% |\n| 1562 | 0.00393 | 0.00426 | 7.75% |\n\nWhile checking the backward kernel, it seems I'm triggering another error with a size limit of\n```python\nx = torch.randn(1449, 1, device='cuda', requires_grad=True)\nout = torch.pdist(x)\nout.mean().backward()\n> RuntimeError: CUDA error: invalid configuration argument\n```\n, while `[<=1448, 1]` works.\n\nI'll take another look at this issue. Let me know, if the potential fix should go into this PR or if I should open a new issue.\n\nCC ngimel, csarofeen\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31593\n\nDifferential Revision: D19825571\n\nPulled By: ngimel\n\nfbshipit-source-id: ace9ccab49f3cf0ce894cdb6daef0795e2e8ec03",
        "Sample Code": "",
        "Bug fix": [
            "@@ -105,7 +105,7 @@ __device__ static inline scalar_t reduce_agg(scalar_t agg) {\n template <typename scalar_t, typename F>\n __global__ static void pdist_kernel_cuda_impl(scalar_t * result, const scalar_t * self, const int64_t n, const int64_t m, const scalar_t p,\n                                               const double n2, const double n2_squared_minus_1) {\n-  const int k = blockIdx.x;\n+  const int64_t k = blockIdx.x;\n   const int stride = blockDim.x;\n \n   // The -1 accounts for floating point truncation issues\n@@ -162,9 +162,9 @@ __global__ static void cdist_backward_kernel_cuda_impl(scalar_t * buffer, const\n template <typename scalar_t, typename F>\n __global__ static void pdist_backward_kernel_cuda_impl(scalar_t * buffer, const scalar_t * grad, const scalar_t * self, const scalar_t * dist, int64_t gs, const int64_t n, const int64_t m, const int64_t combs, const scalar_t p,\n                                                        const double n2, const double n2_squared_minus_1) {\n-  const int k = blockIdx.y * blockDim.y + threadIdx.y;\n-  const int init = blockIdx.x * blockDim.x + threadIdx.x;\n-  const int stride = blockDim.x * gridDim.x;\n+  const int64_t k = blockIdx.x * blockDim.x + threadIdx.x;\n+  const int init = blockIdx.y * blockDim.y + threadIdx.y;\n+  const int stride = blockDim.y * gridDim.y;\n \n   if (k >= combs) {\n     return;\n@@ -276,13 +276,12 @@ void pdist_backward_kernel_impl(Tensor& result, const Tensor& grad, const Tensor\n \n   const int64_t n = result.size(0);\n   int64_t m = self.size(1);\n-  const int block_x = 64;\n+  const int block_x = 16;\n   // NB: be careful with changing block_y; as it's currently written, grid_y is limited to be 2^16.\n-  // From binary search, block_y of 16 gives us max pdist dim0 of 1449,\n-  //                     block_y of  4 gives us max pdist dim0 of  725.\n-  const int block_y = 16;\n-  const int grid_x = (m + block_x * 8 - 1) / (block_x * 8);\n-  const int grid_y = (dist.numel() + block_y - 1) / block_y;\n+  // block_y of 64 gives us max pdist dim1 of 2**24\n+  const int block_y = 64;\n+  const int grid_x = (dist.numel() + block_x - 1) / block_x;\n+  const int grid_y = (m + block_y * 8 - 1) / (block_y * 8);\n   const dim3 grid(grid_x, grid_y);\n   const dim3 block(block_x, block_y);\n   // https://github.com/pytorch/pytorch/issues/15511 demonstrated we need to do\n",
            "@@ -25,8 +25,8 @@ from torch.testing._internal.common_methods_invocations import tri_tests_args, r\n from torch.testing._internal.common_utils import TestCase, iter_indices, TEST_NUMPY, TEST_SCIPY, TEST_MKL, \\\n     TEST_LIBROSA, TEST_WITH_ROCM, run_tests, skipIfNoLapack, suppress_warnings, \\\n     IS_WINDOWS, PY3, NO_MULTIPROCESSING_SPAWN, do_test_dtypes, do_test_empty_full, \\\n-    IS_SANDCASTLE, load_tests, brute_pdist, brute_cdist, slowTest, \\\n-    skipCUDANonDefaultStreamIf, skipCUDAMemoryLeakCheckIf, BytesIOContext\n+    IS_SANDCASTLE, load_tests, pdist_single, brute_cdist, slowTest, \\\n+    skipCUDANonDefaultStreamIf, skipCUDAMemoryLeakCheckIf, BytesIOContext, skipIfRocm\n from multiprocessing.reduction import ForkingPickler\n from torch.testing._internal.common_device_type import instantiate_device_type_tests, \\\n     skipCPUIfNoLapack, skipCUDAIfNoMagma, skipCUDAIfRocm, onlyCUDA, onlyCPU, \\\n@@ -10960,26 +10960,35 @@ class TestTorchDeviceType(TestCase):\n         nz = x.nonzero()\n         self.assertFalse(nz.requires_grad)\n \n-    def test_pdist_norm(self, device):\n-        def test_pdist_single(shape, device, p, dtype, trans):\n-            x = torch.randn(shape, dtype=dtype, device=device)\n-            if trans:\n-                x.transpose_(-2, -1)\n-            actual = torch.pdist(x, p=p)\n-            expected = brute_pdist(x, p=p)\n-            self.assertEqual(expected.shape, actual.shape)\n-            self.assertTrue(torch.allclose(expected, actual))\n-\n-        for shape in [(4, 5), (3, 2), (2, 1)]:\n+    def test_pdist_norm_forward(self, device):\n+        for shape in [(4, 5), (3, 2), (2, 1), (1500, 1)]:\n             for p in [0, 1, 2, 3, 1.5, 2.5, float('inf')]:\n                 for trans in [False, True]:\n                     for dtype in [torch.float32, torch.float64]:\n-                        test_pdist_single(shape, device, p, dtype, trans)\n+                        pdist_single(self, shape, device, p, dtype, trans, grad_check=False)\n \n         # do a simplified comparison with big inputs, see:\n         # https://github.com/pytorch/pytorch/issues/15511\n         for dtype in [torch.float32, torch.float64]:\n-            test_pdist_single((1000, 2), device, 2, dtype, False)\n+            pdist_single(self, (1000, 2), device, 2, dtype, trans=False, grad_check=False)\n+\n+    @skipIfRocm\n+    def test_pdist_norm_backward(self, device):\n+        for shape in [(4, 5), (3, 2), (2, 1), (1500, 1)]:\n+            for p in [0, 1, 2, 3, 1.5, 2.5, float('inf')]:\n+                for trans in [False, True]:\n+                    pdist_single(self, shape, device, p, torch.float64, trans, grad_check=True)\n+\n+    @skipIfRocm\n+    def test_pdist_norm_large(self, device):\n+        # use dim0>=46342 for forward, see:\n+        # https://github.com/pytorch/pytorch/issues/30583\n+        # Compare output using GPU with the CPU implementation, as brute_pdist uses too much memory\n+        if 'cuda' in device:\n+            x = torch.randn(50000, 1, dtype=torch.float32)\n+            expected_cpu = torch.pdist(x, p=2)\n+            actual_gpu = torch.pdist(x.to(device), p=2)\n+            self.assertTrue(torch.allclose(expected_cpu, actual_gpu.cpu()))\n \n     def test_atan2(self, device):\n         def _test_atan2_with_size(size, device):\n",
            "@@ -1319,6 +1319,26 @@ def brute_pdist(inp, p=2):\n     return unroll[..., inds.cumsum(0)]\n \n \n+def pdist_single(self, shape, device, p, dtype, trans, grad_check=False):\n+    x = torch.randn(shape, dtype=dtype, device=device)\n+    if trans:\n+        x.transpose_(-2, -1)\n+    if grad_check:\n+        x.requires_grad_()\n+        y = x.detach().clone().requires_grad_()\n+    else:\n+        y = x\n+    actual = torch.pdist(x, p=p)\n+    expected = brute_pdist(y, p=p)\n+    self.assertEqual(expected.shape, actual.shape)\n+    self.assertTrue(torch.allclose(expected, actual))\n+    if grad_check and expected.size() != torch.Size([0]):\n+        g0 = torch.rand_like(actual)\n+        actual.backward(g0)\n+        expected.backward(g0)\n+        self.assertTrue(torch.allclose(x.grad, y.grad))\n+\n+\n def brute_cdist(x, y, p=2):\n     r1 = x.shape[-2]\n     r2 = y.shape[-2]\n"
        ]
    },
    {
        "Bug description": "fixed scale_factor calculation for uint8 tensor (#31778)\n\nSummary:\nWhen calling the add_images() method on the tensorboard SummaryWriter with a uint8 NCHW tensor, the tensor is incorrectly scaled, resulting in overflow behavior. This leads to incorrect images being displayed in tensorboard.\n\nIssue: https://github.com/pytorch/pytorch/issues/31459\n\nLocal Testing (ran this code with and without the PR changes and printed scale_factor):\n\nimport torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter()\nx=torch.tensor([[[[1, 2, 3], [4, 5, 6]]]], dtype=torch.uint8)\nwriter.add_images(\"images\", x)\n\nBefore- scale_factor: 255, After- scale_factor: 1\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31778\n\nDifferential Revision: D19289189\n\nPulled By: anjali411\n\nfbshipit-source-id: 350a1650337244deae4fd8f8b7fb0e354ae6986b",
        "Sample Code": "",
        "Bug fix": [
            "@@ -171,6 +171,16 @@ class TestTensorBoardUtils(BaseTestCase):\n         converted = convert_to_HWC(test_image, 'hw')\n         self.assertEqual(converted.shape, (32, 32, 3))\n \n+    def test_convert_to_HWC_dtype_remains_same(self):\n+        # test to ensure convert_to_HWC restores the dtype of input np array and\n+        # thus the scale_factor calculated for the image is 1\n+        test_image = torch.tensor([[[[1, 2, 3], [4, 5, 6]]]], dtype=torch.uint8)\n+        tensor = make_np(test_image)\n+        tensor = convert_to_HWC(tensor, 'NCHW')\n+        scale_factor = summary._calc_scale_factor(tensor)\n+        self.assertEqual(scale_factor, 1, 'Values are already in [0, 255], scale factor should be 1')\n+\n+\n     def test_prepare_video(self):\n         # At each timeframe, the sum over all other\n         # dimensions of the video should be the same.\n",
            "@@ -79,7 +79,7 @@ def make_grid(I, ncols=8):\n     W = I.shape[3]\n     ncols = min(nimg, ncols)\n     nrows = int(np.ceil(float(nimg) / ncols))\n-    canvas = np.zeros((3, H * nrows, W * ncols))\n+    canvas = np.zeros((3, H * nrows, W * ncols), dtype=I.dtype)\n     i = 0\n     for y in range(nrows):\n         for x in range(ncols):\n"
        ]
    },
    {
        "Bug description": "torch.histc added a finite range check to resolve segfaults if tensor has inf. also added checks for nan values, min>max (#27712)\n\nSummary:\nhttps://github.com/pytorch/pytorch/issues/27464\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27712\n\nDifferential Revision: D18064544\n\nPulled By: anjali411\n\nfbshipit-source-id: c9c6d8eb4d55f2b5320409ba238bf44b0be8902e",
        "Sample Code": "",
        "Bug fix": [
            "@@ -2,6 +2,8 @@\n #include <ATen/cuda/CUDAContext.h>\n #include <ATen/cuda/CUDAApplyUtils.cuh>\n \n+#include <THC/THCNumerics.cuh>\n+\n namespace at {\n namespace cuda {\n #define THRESH_NUMBER_BINS_FOR_MULTI_BLOCK_MEM 100\n@@ -323,6 +325,29 @@ Tensor _histc_cuda_template(\n     maxvalue = maxvalue + 1;\n   }\n \n+#ifndef __HIP_PLATFORM_HCC__\n+  TORCH_CHECK(\n+      !(THCNumerics<input_t>::isinf(minvalue) ||\n+        THCNumerics<input_t>::isinf(maxvalue) ||\n+        THCNumerics<input_t>::isnan(minvalue) ||\n+        THCNumerics<input_t>::isnan(maxvalue)),\n+      \"range of [\",\n+      minvalue,\n+      \", \",\n+      maxvalue,\n+      \"] is not finite\");\n+#else\n+  TORCH_CHECK(\n+      !(std::isinf(minvalue) || std::isinf(maxvalue) || std::isnan(minvalue) ||\n+        std::isnan(maxvalue)),\n+      \"range of [\",\n+      minvalue,\n+      \", \",\n+      maxvalue,\n+      \"] is not finite\");\n+#endif\n+  TORCH_CHECK(minvalue < maxvalue, \"max must be larger than min\");\n+\n   auto ret = cuda::CUDA_tensor_histogram<input_t, input_t, false>(\n     output, self, Tensor(), nbins, minvalue, maxvalue);\n   return output;\n",
            "@@ -1283,6 +1283,9 @@ void THTensor_(histc)(THTensor *hist, THTensor *tensor, int64_t nbins, scalar_t\n     maxval = maxval + 1;\n   }\n \n+  TORCH_CHECK(!(std::isinf(minval) || std::isinf(maxval) || std::isnan(minval) || std::isnan(maxval)), \"range of [\", minval, \", \", maxval, \"] is not finite\");\n+  TORCH_CHECK(minval < maxval, \"max must be larger than min\");\n+\n   h_data = hist->data<scalar_t>();\n \n   TH_TENSOR_APPLY(scalar_t, tensor,\n",
            "@@ -9983,6 +9983,27 @@ class TestTorchDeviceType(TestCase):\n         self.assertEqual(\n             torch.tensor([1], dtype=torch.float, device=device),\n             actual)\n+        # tensors with inf; min, max not provided -- should throw a RuntimeError\n+        with self.assertRaisesRegex(RuntimeError, r'range of \\[inf, inf\\] is not finite'):\n+            torch.histc(torch.tensor([float(\"inf\")], dtype=torch.float, device=device))\n+        with self.assertRaisesRegex(RuntimeError, r'range of \\[1, inf\\] is not finite'):\n+            torch.histc(torch.tensor([1., 2., float(\"inf\")], dtype=torch.float, device=device))\n+        # tensors with inf; min, max provided\n+        self.assertEqual(\n+            torch.histc(torch.tensor([float(\"inf\")], dtype=torch.float, device=device),\n+                        bins=1, min=0, max=3),\n+            torch.tensor([0], dtype=torch.float, device=device))\n+        self.assertEqual(\n+            torch.histc(torch.tensor([1., 2., float(\"inf\")], dtype=torch.float, device=device),\n+                        bins=4, max=3),\n+            torch.tensor([0, 1, 1, 0], dtype=torch.float, device=device))\n+        # tensor with nan -- should throw a RuntimeError\n+        with self.assertRaisesRegex(RuntimeError, r'range of \\[nan, nan\\] is not finite'):\n+            torch.histc(torch.tensor([float(\"nan\")], dtype=torch.float, device=device))\n+        # tensors with min > max -- should throw a RuntimeError\n+        with self.assertRaisesRegex(RuntimeError, \"max must be larger than min\"):\n+            torch.histc(torch.tensor([1., 2., 3.], dtype=torch.float, device=device),\n+                        bins=4, min=5, max=1)\n \n         # test against numpy.histogram()\n         def test_against_np(tensor, bins=100, min=0, max=0):\n"
        ]
    },
    {
        "Bug description": "Fix int32 overflow in SummaryOps.cu getBin #25747 (#25748)\n\nSummary:\nFixes issue https://github.com/pytorch/pytorch/issues/25747 by upcasting to int64 before multiplication. Should be good enough for all reasonable nbins\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25748\n\nDifferential Revision: D17269111\n\nPulled By: ezyang\n\nfbshipit-source-id: 484be39080571203264a1bb9898ecf23d1aeafab",
        "Sample Code": "",
        "Bug fix": [
            "@@ -17,7 +17,7 @@ namespace cuda {\n enum class CUDAHistogramMemoryType { SHARED, MULTI_BLOCK, GLOBAL };\n namespace {\n   template<typename input_t, typename IndexType>\n-  __device__ static IndexType getBin(input_t bVal, input_t minvalue, input_t maxvalue, int nbins) {\n+  __device__ static IndexType getBin(input_t bVal, input_t minvalue, input_t maxvalue, int64_t nbins) {\n     IndexType bin = (int)((bVal - minvalue) * nbins / (maxvalue - minvalue));\n     // (only applicable for histc)\n     // while each bin is inclusive at the lower end and exclusive at the higher, i.e. [start, end)\n@@ -47,7 +47,7 @@ __global__ void kernelHistogram1D(\n     detail::TensorInfo<output_t, IndexType> a, /* output */\n     detail::TensorInfo<output_t, IndexType> p, /* partial output */\n     detail::TensorInfo<input_t, IndexType> b, /* input */\n-    int nbins,\n+    int64_t nbins,\n     input_t minvalue,\n     input_t maxvalue,\n     IndexType totalElements,\n",
            "@@ -2847,6 +2847,13 @@ class TestCuda(TestCase):\n         self.assertEqual(t.cpu().bincount(), t.bincount())\n         self.assertEqual(t.cpu().bincount(w_cpu), t.bincount(w))\n \n+        t = torch.zeros([10], dtype=torch.int32, device='cuda')\n+        # 35488 * 65536 as int32 would cause overflow to negative value\n+        # giving negative bin offset\n+        t[0] = 35488\n+        counted = t.bincount(minlength=65536)\n+        self.assertEqual(torch.sum(counted), 10)\n+\n     def test_tiny_half_norm_(self):\n         a = torch.arange(25).cuda().float()\n         a /= 100000000\n"
        ]
    },
    {
        "Issue title": "Illegal memory access occurs when using nn.AvgPool2d ",
        "Bug description": " of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nWe are trying to use PyTorch on a Rapsberry PI 4 on Buster, but get a segfault with a very simple test program:\r\n\r\n```python\r\n$ python\r\nPython 3.7.3 (default, Apr  3 2019, 05:39:12) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n>>> v = torch.FloatTensor(1, 135).fill_(0)\r\n>>> v[0, [1]] += 2\r\nSegmentation fault\r\n```\r\n\r\nThe issue seems to only occur when we index `v` using a single element array (whether python list, numpy array or pytorch tensor). Using a multi-element list or `:`, or a single value did not seem to have the issue in our limited testing.\r\n\r\nFollowing is the gdb trace:\r\n\r\n```\r\n$ gdb --args python\r\nGNU gdb (Raspbian 8.2.1-2) 8.2.1\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\nType \"show copying\" and \"show warranty\" for details.\r\nThis GDB was configured as \"arm-linux-gnueabihf\".\r\nType \"show configuration\" for configuration details.\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>.\r\nFind the GDB manual and other documentation resources online at:\r\n    <http://www.gnu.org/software/gdb/documentation/>.\r\n\r\nFor help, type \"help\".\r\nType \"apropos word\" to search for commands related to \"word\"...\r\nReading symbols from python...(no debugging symbols found)...done.\r\n(gdb) run\r\nStarting program: /home/pi/pytorch1.3/bin/python \r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/arm-linux-gnueabihf/libthread_db.so.1\".\r\nPython 3.7.3 (default, Apr  3 2019, 05:39:12) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n[New Thread 0xb5067460 (LWP 26627)]\r\n[New Thread 0xb40c6460 (LWP 26628)]\r\n[New Thread 0xb1125460 (LWP 26629)]\r\n[Thread 0xb1125460 (LWP 26629) exited]\r\n[Thread 0xb40c6460 (LWP 26628) exited]\r\n[Thread 0xb5067460 (LWP 26627) exited]\r\n[Detaching after fork from child process 26631]\r\n>>> v = torch.FloatTensor(1, 135).fill_(0)\r\n>>> v[0, 1] += 2\r\n>>> v[0, [1]] += 2\r\n\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n0xac0f2868 in at::detail::computeStride(c10::ArrayRef<long long>, c10::ArrayRef<long long>, c10::ArrayRef<long long>) ()\r\n   from /home/pi/pytorch1.3/lib/python3.7/site-packages/torch/lib/libtorch.so\r\n```\r\n\r\n\r\n## Environment\r\n\r\nI compiled pytorch on that pi (in a virtual env), using pytorch v1.3.1 and v1.4.0 from github and both had the same issue. The basic steps was:\r\n\r\n* Install apt deps (e.g. `libatlas-base-dev`).\r\n* Checkout pytorch and submodules from github (`git clone --recursive https://github.com/pytorch/pytorch --branch=v1.3.1`).\r\n* `git submodule update --remote third_party/protobuf` to fix a bug in the current version.\r\n* Export env variables:\r\n  ```\r\n  export NO_CUDA=1\r\n  export NO_DISTRIBUTED=1\r\n  export NO_MKLDNN=1 \r\n  export NO_NNPACK=1\r\n  export NO_QNNPACK=1\r\n  export USE_NUMPY=1\r\n  ```\r\n* Install pip deps (numpy, pyyaml, etc.).\r\n* `python3 setup.py build`.\r\n* `python3 setup.py install`.\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.3.0a0+ee77ccb\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Raspbian GNU/Linux 10 (buster)\r\nGCC version: (Raspbian 8.3.0-6+rpi1) 8.3.0\r\nCMake version: version 3.13.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.3.0a0+ee77ccb\r\n[conda] Could not collect\r\n```\n\ncc @ezyang @gchanan @zou3519",
        "Sample Code": [
            "@@ -17,7 +17,7 @@ namespace cuda {\n enum class CUDAHistogramMemoryType { SHARED, MULTI_BLOCK, GLOBAL };\n namespace {\n   template<typename input_t, typename IndexType>\n-  __device__ static IndexType getBin(input_t bVal, input_t minvalue, input_t maxvalue, int nbins) {\n+  __device__ static IndexType getBin(input_t bVal, input_t minvalue, input_t maxvalue, int64_t nbins) {\n     IndexType bin = (int)((bVal - minvalue) * nbins / (maxvalue - minvalue));\n     // (only applicable for histc)\n     // while each bin is inclusive at the lower end and exclusive at the higher, i.e. [start, end)\n@@ -47,7 +47,7 @@ __global__ void kernelHistogram1D(\n     detail::TensorInfo<output_t, IndexType> a, /* output */\n     detail::TensorInfo<output_t, IndexType> p, /* partial output */\n     detail::TensorInfo<input_t, IndexType> b, /* input */\n-    int nbins,\n+    int64_t nbins,\n     input_t minvalue,\n     input_t maxvalue,\n     IndexType totalElements,\n",
            "@@ -2847,6 +2847,13 @@ class TestCuda(TestCase):\n         self.assertEqual(t.cpu().bincount(), t.bincount())\n         self.assertEqual(t.cpu().bincount(w_cpu), t.bincount(w))\n \n+        t = torch.zeros([10], dtype=torch.int32, device='cuda')\n+        # 35488 * 65536 as int32 would cause overflow to negative value\n+        # giving negative bin offset\n+        t[0] = 35488\n+        counted = t.bincount(minlength=65536)\n+        self.assertEqual(torch.sum(counted), 10)\n+\n     def test_tiny_half_norm_(self):\n         a = torch.arange(25).cuda().float()\n         a /= 100000000\n"
        ],
        "Bug fix": ""
    },
    {
        "Bug description": "fix segfault in `cat` on CPU with tensors that can't be indexed with 32-bit ints. (#21530)\n\nSummary:\nShould be self-explanatory. This `int` variable is overflowing.\n\nReported in #21526\nPull Request resolved: https://github.com/pytorch/pytorch/pull/21530\n\nDifferential Revision: D15719275\n\nPulled By: umanwizard\n\nfbshipit-source-id: 24e917a00a5b78bc3af29ef3b8b72eea7e89d5d5",
        "Sample Code": "",
        "Bug fix": [
            "@@ -799,7 +799,7 @@ void THTensor_(catArray)(THTensor *result, THTensor **inputs, int numInputs, int\n         if (!should_skip(inputs[j])) {\n           THTensor* input0 = inputs[j];\n           scalar_t* input0_data = THStorage_(data)(THTensor_getStoragePtr(input0)) + input0->storage_offset();\n-          int local_inner = inner * input0->size(dimension);\n+          int64_t local_inner = inner * input0->size(dimension);\n           if (local_inner != 0) {\n             memcpy(result_data + offset, input0_data + o*local_inner, local_inner*sizeof(scalar_t));\n           } // input0_size != 0\n",
            "@@ -4948,6 +4948,16 @@ class _TestTorchMixin(object):\n     def test_cat_empty(self):\n         self._test_cat_empty(self)\n \n+    @slowTest\n+    def test_cat_big(self):\n+        SIZE1 = 6500\n+        SIZE2 = 4500\n+        concat_list = []\n+        concat_list.append(torch.ones((SIZE1, 1024 * 512), dtype=torch.uint8))\n+        concat_list.append(torch.ones((SIZE2, 1024 * 512), dtype=torch.uint8))\n+        result = torch.cat(concat_list)\n+        self.assertEqual(result.size(0), SIZE1 + SIZE2)\n+\n     def test_narrow(self):\n         x = torch.Tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n         self.assertEqual(x.narrow(0, 0, 1), torch.Tensor([[0, 1, 2]]))\n"
        ]
    },
    {
        "Bug description": "Fix Binomimal overflow when logits is large (#20679)\n\nSummary:\nThis PR fixes  #17843. In addition (test locally), this still maintains the continuity of log_prob which is addressed in https://github.com/pytorch/pytorch/pull/15962\n\ncc neerajprad\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20679\n\nDifferential Revision: D15413311\n\nPulled By: ezyang\n\nfbshipit-source-id: 4fc0ca755ae6a85aa7deb2206dab675f82f9aa25",
        "Sample Code": "",
        "Bug fix": [
            "@@ -953,6 +953,18 @@ class TestDistributions(TestCase):\n             logits = probs_to_logits(probs, is_binary=True)\n             self._check_log_prob(Binomial(total_count, logits=logits), ref_log_prob)\n \n+    def test_binomial_stable(self):\n+        logits = torch.tensor([-100., 100.], dtype=torch.float)\n+        total_count = 1.\n+        x = torch.tensor([0., 0.], dtype=torch.float)\n+        log_prob = Binomial(total_count, logits=logits).log_prob(x)\n+        self.assertTrue(torch.isfinite(log_prob).all())\n+\n+        # make sure that the grad at logits=0, value=0 is 0.5\n+        x = torch.tensor(0., requires_grad=True)\n+        y = Binomial(total_count, logits=x).log_prob(torch.tensor(0.))\n+        self.assertEqual(grad(y, x)[0], torch.tensor(-0.5))\n+\n     @unittest.skipIf(not TEST_NUMPY, \"NumPy not found\")\n     def test_binomial_log_prob_vectorized_count(self):\n         probs = torch.tensor([0.2, 0.7, 0.9])\n",
            "@@ -5,6 +5,11 @@ from torch.distributions.distribution import Distribution\n from torch.distributions.utils import broadcast_all, probs_to_logits, lazy_property, logits_to_probs\n \n \n+def _clamp_by_zero(x):\n+    # works like clamp(x, min=0) but has grad at 0 is 0.5\n+    return (x.clamp(min=0) + x - x.clamp(max=0)) / 2\n+\n+\n class Binomial(Distribution):\n     r\"\"\"\n     Creates a Binomial distribution parameterized by :attr:`total_count` and\n@@ -113,9 +118,15 @@ class Binomial(Distribution):\n         log_factorial_n = torch.lgamma(self.total_count + 1)\n         log_factorial_k = torch.lgamma(value + 1)\n         log_factorial_nmk = torch.lgamma(self.total_count - value + 1)\n-        # Note that: torch.log1p(-self.probs)) = - torch.log1p(self.logits.exp()))\n-        return (log_factorial_n - log_factorial_k - log_factorial_nmk +\n-                value * self.logits - self.total_count * torch.log1p(self.logits.exp()))\n+        # k * log(p) + (n - k) * log(1 - p) = k * (log(p) - log(1 - p)) + n * log(1 - p)\n+        #     (case logit < 0)              = k * logit - n * log1p(e^logit)\n+        #     (case logit > 0)              = k * logit - n * (log(p) - log(1 - p)) + n * log(p)\n+        #                                   = k * logit - n * logit - n * log1p(e^-logit)\n+        #     (merge two cases)             = k * logit - n * max(logit, 0) - n * log1p(e^-|logit|)\n+        normalize_term = (self.total_count * _clamp_by_zero(self.logits)\n+                          + self.total_count * torch.log1p(torch.exp(-torch.abs(self.logits)))\n+                          - log_factorial_n)\n+        return value * self.logits - log_factorial_k - log_factorial_nmk - normalize_term\n \n     def enumerate_support(self, expand=True):\n         total_count = int(self.total_count.max())\n"
        ]
    },
    {
        "Bug description": "Added correct isinf handling for Integral tensors (#15489)\n\nSummary:\nCurrently torch.isinf on integral tensor will raise RuntimeError: value cannot be converted to type int16_t without overflow: inf.\nThis pr will suppress the error and return false(0) for all integral tensors. The behavior will also be consistent with np.isinf\nPull Request resolved: https://github.com/pytorch/pytorch/pull/15489\n\nReviewed By: zou3519\n\nDifferential Revision: D13540786\n\nPulled By: flashhack\n\nfbshipit-source-id: e730dea849da6a59f3752d347bcfbadfd12c6483",
        "Sample Code": "",
        "Bug fix": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ]
    },
    {
        "Issue title": "torch.nn.utils.rnn.pack_padded_sequence segment fault if not in decreasing order",
        "Bug description": "\r\n\r\nInstead of raising an exception, the function `torch.nn.utils.rnn.pack_padded_sequence` forces python environment to shut down due to a segmentation fault if the input is not in decreasing order.\r\n\r\n\r\n## To Reproduce\r\n\r\n```python \r\nimport torch\r\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\r\na = torch.ones(25, 300)\r\nb = torch.ones(22, 300)\r\nb_a = pad_sequence([b, a])\r\npack_padded_sequence(b_a, [22, 25])\r\n>>> 31906 abort (core dumped)  ipython\r\na_b = pad_sequence([a, b])\r\npack_padded_sequence(a_b, [25, 22]) # it works!\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Dilated Conv3d segfaults (cpu)",
        "Bug description": "\r\n\r\nInstead of raising an exception, the function `torch.nn.utils.rnn.pack_padded_sequence` forces python environment to shut down due to a segmentation fault if the input is not in decreasing order.\r\n\r\n\r\n## To Reproduce\r\n\r\n```python \r\nimport torch\r\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\r\na = torch.ones(25, 300)\r\nb = torch.ones(22, 300)\r\nb_a = pad_sequence([b, a])\r\npack_padded_sequence(b_a, [22, 25])\r\n>>> 31906 abort (core dumped)  ipython\r\na_b = pad_sequence([a, b])\r\npack_padded_sequence(a_b, [25, 22]) # it works!\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Memory Leak in autograd using ndarray[None]",
        "Bug description": "\r\n\r\nInstead of raising an exception, the function `torch.nn.utils.rnn.pack_padded_sequence` forces python environment to shut down due to a segmentation fault if the input is not in decreasing order.\r\n\r\n\r\n## To Reproduce\r\n\r\n```python \r\nimport torch\r\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\r\na = torch.ones(25, 300)\r\nb = torch.ones(22, 300)\r\nb_a = pad_sequence([b, a])\r\npack_padded_sequence(b_a, [22, 25])\r\n>>> 31906 abort (core dumped)  ipython\r\na_b = pad_sequence([a, b])\r\npack_padded_sequence(a_b, [25, 22]) # it works!\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.special.round` doesn't support the same dtypes as `torch.round`",
        "Bug description": "\r\n\r\nInstead of raising an exception, the function `torch.nn.utils.rnn.pack_padded_sequence` forces python environment to shut down due to a segmentation fault if the input is not in decreasing order.\r\n\r\n\r\n## To Reproduce\r\n\r\n```python \r\nimport torch\r\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\r\na = torch.ones(25, 300)\r\nb = torch.ones(22, 300)\r\nb_a = pad_sequence([b, a])\r\npack_padded_sequence(b_a, [22, 25])\r\n>>> 31906 abort (core dumped)  ipython\r\na_b = pad_sequence([a, b])\r\npack_padded_sequence(a_b, [25, 22]) # it works!\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.multinomial` on MPS crashes with `Error: total bytes of NDArray > 2**32'`",
        "Bug description": "\r\n\r\nInstead of raising an exception, the function `torch.nn.utils.rnn.pack_padded_sequence` forces python environment to shut down due to a segmentation fault if the input is not in decreasing order.\r\n\r\n\r\n## To Reproduce\r\n\r\n```python \r\nimport torch\r\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\r\na = torch.ones(25, 300)\r\nb = torch.ones(22, 300)\r\nb_a = pad_sequence([b, a])\r\npack_padded_sequence(b_a, [22, 25])\r\n>>> 31906 abort (core dumped)  ipython\r\na_b = pad_sequence([a, b])\r\npack_padded_sequence(a_b, [25, 22]) # it works!\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.nn.functional.one_hot only works for int64",
        "Bug description": "\r\n\r\nInstead of raising an exception, the function `torch.nn.utils.rnn.pack_padded_sequence` forces python environment to shut down due to a segmentation fault if the input is not in decreasing order.\r\n\r\n\r\n## To Reproduce\r\n\r\n```python \r\nimport torch\r\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\r\na = torch.ones(25, 300)\r\nb = torch.ones(22, 300)\r\nb_a = pad_sequence([b, a])\r\npack_padded_sequence(b_a, [22, 25])\r\n>>> 31906 abort (core dumped)  ipython\r\na_b = pad_sequence([a, b])\r\npack_padded_sequence(a_b, [25, 22]) # it works!\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.remainder and torch.fmod produce wrong results",
        "Bug description": "\n\nFor some inputs, torch.remainder and torch.fmod produce wrong results, especially for integer datatype. When converting the int32 input to float32, they can produce correct results.\r\n\r\nI suspect this might be caused by type promotion.\r\n\r\nReproduce ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.mm` produces wrong result on cpu when using in-place computation",
        "Bug description": "\r\n\r\nIf I use `out` to specify the output tensor to be the same as the input tensor, `torch.mm` gives wrong result on cpu. Using`torch.mm`  on cpu w/o `out` or on cuda gives the same correct result.\r\n\r\nHere is the minimized ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in native_batch_norm",
        "Bug description": "\n\nSegmentation fault in `native_batch_norm`.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in _mkldnn_transpose",
        "Bug description": "\n\nSegmentation fault in `_mkldnn_transpose`.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in mkldnn_reorder_conv2d_weight and mkldnn_reorder_conv3d_weight",
        "Bug description": "\n\nSegmentation fault in `mkldnn_reorder_conv2d_weight` and  `mkldnn_reorder_conv3d_weight`.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in embedding_bag",
        "Bug description": "\n\nSegmentation fault in `embedding_bag`, `_embedding_bag` and `_embedding_bag_forward_only`.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in `torch.jit.wait`",
        "Bug description": "\n\nPassing `None` to `torch.jit.wait` can cause a Segmentation fault.\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in `torch.futures.collect_all`",
        "Bug description": "\r\n\r\nIn `torch.futures.collect_all` when `futures` got invalid input, it causes a Segmentation fault.\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.nn.functional.pad generates incomplete ONNX without explicit padding value",
        "Bug description": "\r\n\r\nIn `torch.futures.collect_all` when `futures` got invalid input, it causes a Segmentation fault.\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segfault when profiling with_stack=True on model with jit.optimize_for_inference",
        "Bug description": "\r\n\r\nIn `torch.futures.collect_all` when `futures` got invalid input, it causes a Segmentation fault.\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.pinverse` produces wrong output!",
        "Bug description": "\r\n\r\nIn `torch.futures.collect_all` when `futures` got invalid input, it causes a Segmentation fault.\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.nn.Conv2d will segfault when the type of input tensor is float16",
        "Bug description": "\n\n```\r\nresults = dict()\r\nimport torch\r\narg_class = torch.nn.Conv2d(512,2048,1)\r\narg_1 = torch.rand([128, 512, 16, 16], dtype=torch.float16)\r\nresults[\"time_low\"] = arg_class(arg_1)\r\n```\r\nThe above ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.nn.MaxUnpool2d get negative size tensor",
        "Bug description": "\n\n```\r\nresults = dict()\r\nimport torch\r\narg_class = torch.nn.Conv2d(512,2048,1)\r\narg_1 = torch.rand([128, 512, 16, 16], dtype=torch.float16)\r\nresults[\"time_low\"] = arg_class(arg_1)\r\n```\r\nThe above ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.nn.InstanceNorm{1|2|3}d doesn't verify the value type of parameter num_features",
        "Bug description": "\r\n\r\nParameter 'num_features' is the number of features or channels C of the input. However, I found that num_features can be set to negative integral / string / list and other type value. torch.nn.InstanceNorm1d doesn't verify whether the value of num_features and input channels are equal. \r\n```\r\nimport torch\r\nresults={}\r\narg_1 = 'max'\r\narg_2 = False\r\narg_class = torch.nn.InstanceNorm1d(arg_1,affine=arg_2,)\r\narg_3 = torch.rand([20, 100, 40], dtype=torch.float32)\r\nresults['res'] = arg_class(arg_3)\r\n```\r\nAbove ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.nn.GRU runs long time, when num_layers is large",
        "Bug description": "\r\n\r\nParameter 'num_features' is the number of features or channels C of the input. However, I found that num_features can be set to negative integral / string / list and other type value. torch.nn.InstanceNorm1d doesn't verify whether the value of num_features and input channels are equal. \r\n```\r\nimport torch\r\nresults={}\r\narg_1 = 'max'\r\narg_2 = False\r\narg_class = torch.nn.InstanceNorm1d(arg_1,affine=arg_2,)\r\narg_3 = torch.rand([20, 100, 40], dtype=torch.float32)\r\nresults['res'] = arg_class(arg_3)\r\n```\r\nAbove ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "When padding is big int, torch.nn.functional.fold runs too long and can't return result",
        "Bug description": "\n\nWhen I run the ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "[Torchscript] torch.min returns wrong gradient when inputs are equal",
        "Bug description": "\n\nThe same issue applies to torch.max()\r\n\r\nSteps ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.matrix_exp` doesn't handle NaN properly",
        "Bug description": "\n\nThe same issue applies to torch.max()\r\n\r\nSteps ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault is raised when torch._C._nn.adaptive_avg_pool2d is called with some parameters",
        "Bug description": "\r\n\r\nThe segmentation fault appears when `torch._C._nn.adaptive_avg_pool2d` is called for some combinations of input and output shapes.\r\n\r\n#### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch._weight_norm with specified dim returns wrong output",
        "Bug description": "\r\n`torch._weight_norm` outputs wrong result from torch 1.12\r\n\r\nThough `torch._weight_norm` is an undocumented API, I report this issue since the API is made public.\r\n\r\n#### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "[Mac M1] `torch.mm` sometimes produces incorrect results",
        "Bug description": "\n\nObserved unexpected nan outputs from the `torch.nn.functional.conv1d` function on M1 Mac\r\n\r\n```python\r\n# bug_demo.py\r\n\r\nimport torch\r\n\r\nn_trials = 100\r\nfor ii in range(n_trials):\r\n    a = torch.randn(1024, device='mps')\r\n    b = torch.randn(499, device='mps')\r\n    c = torch.nn.functional.conv1d(a.view(1, 1, -1), b.view(1, 1, -1))\r\n    if torch.isnan(torch.sum(c)):\r\n        print(f'mps: trial {ii}, nan elements {torch.isnan(c.squeeze()).nonzero().view(-1).cpu().numpy()}')\r\n        \r\nfor ii in range(n_trials):\r\n    a = torch.randn(1024, device='cpu')\r\n    b = torch.randn(499, device='cpu')\r\n    c = torch.nn.functional.conv1d(a.view(1, 1, -1), b.view(1, 1, -1))\r\n    if torch.isnan(torch.sum(c)):\r\n        print(f'cpu: trial {ii}, elements {torch.isnan(c.squeeze()).nonzero().view(-1).numpy()}')\r\n```\r\n\r\n",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.nn.functional.linear fails for multi-dimensional bias from torch 1.12",
        "Bug description": "\n\nObserved unexpected nan outputs from the `torch.nn.functional.conv1d` function on M1 Mac\r\n\r\n```python\r\n# bug_demo.py\r\n\r\nimport torch\r\n\r\nn_trials = 100\r\nfor ii in range(n_trials):\r\n    a = torch.randn(1024, device='mps')\r\n    b = torch.randn(499, device='mps')\r\n    c = torch.nn.functional.conv1d(a.view(1, 1, -1), b.view(1, 1, -1))\r\n    if torch.isnan(torch.sum(c)):\r\n        print(f'mps: trial {ii}, nan elements {torch.isnan(c.squeeze()).nonzero().view(-1).cpu().numpy()}')\r\n        \r\nfor ii in range(n_trials):\r\n    a = torch.randn(1024, device='cpu')\r\n    b = torch.randn(499, device='cpu')\r\n    c = torch.nn.functional.conv1d(a.view(1, 1, -1), b.view(1, 1, -1))\r\n    if torch.isnan(torch.sum(c)):\r\n        print(f'cpu: trial {ii}, elements {torch.isnan(c.squeeze()).nonzero().view(-1).numpy()}')\r\n```\r\n\r\n",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.einsum results in segfault",
        "Bug description": "\n\nObserved unexpected nan outputs from the `torch.nn.functional.conv1d` function on M1 Mac\r\n\r\n```python\r\n# bug_demo.py\r\n\r\nimport torch\r\n\r\nn_trials = 100\r\nfor ii in range(n_trials):\r\n    a = torch.randn(1024, device='mps')\r\n    b = torch.randn(499, device='mps')\r\n    c = torch.nn.functional.conv1d(a.view(1, 1, -1), b.view(1, 1, -1))\r\n    if torch.isnan(torch.sum(c)):\r\n        print(f'mps: trial {ii}, nan elements {torch.isnan(c.squeeze()).nonzero().view(-1).cpu().numpy()}')\r\n        \r\nfor ii in range(n_trials):\r\n    a = torch.randn(1024, device='cpu')\r\n    b = torch.randn(499, device='cpu')\r\n    c = torch.nn.functional.conv1d(a.view(1, 1, -1), b.view(1, 1, -1))\r\n    if torch.isnan(torch.sum(c)):\r\n        print(f'cpu: trial {ii}, elements {torch.isnan(c.squeeze()).nonzero().view(-1).numpy()}')\r\n```\r\n\r\n",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.renorm` gives wrong gradient for 0-valued input when `p` is even and `maxnorm=0`.",
        "Bug description": "\n\nObserved unexpected nan outputs from the `torch.nn.functional.conv1d` function on M1 Mac\r\n\r\n```python\r\n# bug_demo.py\r\n\r\nimport torch\r\n\r\nn_trials = 100\r\nfor ii in range(n_trials):\r\n    a = torch.randn(1024, device='mps')\r\n    b = torch.randn(499, device='mps')\r\n    c = torch.nn.functional.conv1d(a.view(1, 1, -1), b.view(1, 1, -1))\r\n    if torch.isnan(torch.sum(c)):\r\n        print(f'mps: trial {ii}, nan elements {torch.isnan(c.squeeze()).nonzero().view(-1).cpu().numpy()}')\r\n        \r\nfor ii in range(n_trials):\r\n    a = torch.randn(1024, device='cpu')\r\n    b = torch.randn(499, device='cpu')\r\n    c = torch.nn.functional.conv1d(a.view(1, 1, -1), b.view(1, 1, -1))\r\n    if torch.isnan(torch.sum(c)):\r\n        print(f'cpu: trial {ii}, elements {torch.isnan(c.squeeze()).nonzero().view(-1).numpy()}')\r\n```\r\n\r\n",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Semi-reproducible random torch.baddbmm NaNs",
        "Bug description": "\r\n\r\nThe following ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Negative values still produced by torch.nn.functional.kl_div",
        "Bug description": "\n\n## \ud83d\udc1b Bug\r\nDespite a fix in https://github.com/pytorch/pytorch/issues/32520, kl_div in torch.nn.functional still outputs negative values. \r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`pack_sequence` crash",
        "Bug description": "\n\n## \ud83d\udc1b Bug\r\nDespite a fix in https://github.com/pytorch/pytorch/issues/32520, kl_div in torch.nn.functional still outputs negative values. \r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segfault in _pad_packed_sequence",
        "Bug description": "\n\n\r\nFunction `torch._pad_packed_sequence` contains segmentation fault.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segfault in _grid_sampler_2d_cpu_fallback",
        "Bug description": "\n\n\r\nFunction `torch._grid_sampler_2d_cpu_fallback` contains segmentation fault.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segfault in _embedding_bag_forward_only",
        "Bug description": "\n\nFunction `torch._embedding_bag_forward_only` contains segmentation fault.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segfault in torch._C._nn.thnn_conv2d",
        "Bug description": "\r\n\r\nFunction `torch._C._nn.thnn_conv2d` contains segmentation fault.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segfault in torch._C._nn.reflection_pad2d",
        "Bug description": "\n\n\r\nFunction `torch._C._nn.reflection_pad2d` contains segmentation fault.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segfault in max_unpool3d",
        "Bug description": "\n\n\r\nFunction `torch.max_unpool3d` contains segmentation fault.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segfault in grid_sampler_3d",
        "Bug description": "\n\n\r\nFunction `torch.grid_sampler_3d` contains segmentation fault.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segfault in choose_qparams_optimized",
        "Bug description": "\n\n\r\nFunction `torch.choose_qparams_optimized` contains segmentation fault.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segfault in bincount",
        "Bug description": "\n\nFunction `torch.bincount` contains segmentation fault.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in _remove_batch_dim",
        "Bug description": "\r\n\r\nSegmentation fault in `_remove_batch_dim`.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "forward AD fails in `torch.pow`",
        "Bug description": "\r\n\r\nSegmentation fault in `_remove_batch_dim`.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.scatter_add` will succeed when the `index` is a complex tensor",
        "Bug description": "\r\n\r\nSegmentation fault in `_remove_batch_dim`.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.addmv` backward fails",
        "Bug description": "\r\n\r\nSegmentation fault in `_remove_batch_dim`.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.unique() nondeterministic behavior on nan inputs (on GPU)",
        "Bug description": "\r\n\r\nSegmentation fault in `_remove_batch_dim`.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.jit.load fails when path contains non-ascii characters",
        "Bug description": "\r\n\r\nSegmentation fault in `_remove_batch_dim`.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Incorrect results for `torch.distributed.gather` for tensor created from permuted NumPy array",
        "Bug description": "\n\nHi,\r\n\r\nWhen creating a tensor from a non-contiguous NumPy array, e.g.\r\n```python\r\nmy_array = np.reshape(np.arange(10), (2, 5))\r\nmy_array = np.transpose(my_array, (1, 0))\r\nmy_tensor = torch.from_numpy(my_array)\r\n```\r\nand then using `torch.distributed.gather`, the tensor on the destination rank is not correct.\r\n\r\n**Expected behaviour**: either the tensor on the destination rank is equal to the tensor on the source rank, **or**, `torch.distributed.gather` throws an exception to let the user know that the tensor must be contiguous. This silent error is very hard to be aware of.\r\n\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.pow` errors out on specific input",
        "Bug description": "\n\nHi,\r\n\r\nWhen creating a tensor from a non-contiguous NumPy array, e.g.\r\n```python\r\nmy_array = np.reshape(np.arange(10), (2, 5))\r\nmy_array = np.transpose(my_array, (1, 0))\r\nmy_tensor = torch.from_numpy(my_array)\r\n```\r\nand then using `torch.distributed.gather`, the tensor on the destination rank is not correct.\r\n\r\n**Expected behaviour**: either the tensor on the destination rank is equal to the tensor on the source rank, **or**, `torch.distributed.gather` throws an exception to let the user know that the tensor must be contiguous. This silent error is very hard to be aware of.\r\n\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in max_pool3d",
        "Bug description": "\n\nSegmentation fault in `max_pool3d` when containing large arguments (`kernel_size`, `dilation`).\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in max_pool1d",
        "Bug description": "\n\nSegmentation fault in `max_pool1d` when `kernel_size`, `stride` and `dilation` are large.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in fractional_max_pool3d",
        "Bug description": "\n\nSegmentation fault in `fractional_max_pool3d` when `output_size` contains 0s.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in fractional_max_pool2d",
        "Bug description": "\n\nSegmentation fault in `fractional_max_pool2d` when `output_size` contains 0s.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in _sobol_engine_scramble_",
        "Bug description": "\r\n\r\nSegmentation fault in `_sobol_engine_scramble_` when `dimension` is large.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in _sobol_engine_initialize_state_",
        "Bug description": "\n\nSegmentation fault in `_sobol_engine_initialize_state_` when `dimension` is large.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segmentation fault in _sobol_engine_ff_",
        "Bug description": "\n\nSegmentation fault in `_sobol_engine_ff_` when `n` and `dimension` are large.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.distributions.categorical.Categorical does not work with 0 batch size",
        "Bug description": "\n\nSegmentation fault in `_sobol_engine_ff_` when `n` and `dimension` are large.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.bmm backward with sparse input",
        "Bug description": "\n\nSegmentation fault in `_sobol_engine_ff_` when `n` and `dimension` are large.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.median` will return -2147483648 when input is an empty tensor",
        "Bug description": "\n\nSegmentation fault in `_sobol_engine_ff_` when `n` and `dimension` are large.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.cum{min,max}, torch.sort, argsort` do not check the `dim` when the input is 0-d tensor",
        "Bug description": "\n\nSegmentation fault in `_sobol_engine_ff_` when `n` and `dimension` are large.\r\n\r\n### ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.jit.script failed to compile nn.MultiheadAttention when specifying the kdim and vdim parameters.",
        "Bug description": "\n\nnn.MultiheadAttention cannot be correctly compiled by torch.jit.script when kdim and vdim are specified. Minimum ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.diag` unexpectedly fails",
        "Bug description": "\n\nnn.MultiheadAttention cannot be correctly compiled by torch.jit.script when kdim and vdim are specified. Minimum ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.combinations` will allocate large memory when `r` is greater than the length of input",
        "Bug description": "\n\nnn.MultiheadAttention cannot be correctly compiled by torch.jit.script when kdim and vdim are specified. Minimum ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.nn.{Constant,Zero}Pad` unexpectedly fail",
        "Bug description": "\n\nnn.MultiheadAttention cannot be correctly compiled by torch.jit.script when kdim and vdim are specified. Minimum ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.scatter` will return random value when `input` is empty tensor",
        "Bug description": "\n\nnn.MultiheadAttention cannot be correctly compiled by torch.jit.script when kdim and vdim are specified. Minimum ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.Tensor.where` cannot work when `y` is float",
        "Bug description": "\n\nnn.MultiheadAttention cannot be correctly compiled by torch.jit.script when kdim and vdim are specified. Minimum ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Accuracy problem of `torch.batch_norm_gather_stats_with_counts` when `running_mean` is half tensor",
        "Bug description": "\n\nnn.MultiheadAttention cannot be correctly compiled by torch.jit.script when kdim and vdim are specified. Minimum ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.as_strided` can create a tensor with negative dimension",
        "Bug description": "\n\nnn.MultiheadAttention cannot be correctly compiled by torch.jit.script when kdim and vdim are specified. Minimum ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.broadcast_to` can create tensor with negative dimension.",
        "Bug description": "\n\nnn.MultiheadAttention cannot be correctly compiled by torch.jit.script when kdim and vdim are specified. Minimum ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.empty_strided` works when the stride is negative!",
        "Bug description": "\n\nnn.MultiheadAttention cannot be correctly compiled by torch.jit.script when kdim and vdim are specified. Minimum ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch jit script segm fault",
        "Bug description": "\n\nPython crashes on executing the following ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.transpose` should raise an error when indexing 0 for 0 dimensional tensor.",
        "Bug description": "\r\n\r\n`torch.transpose` should raise an error when indexing 0 for 0 dimensional tensor. Because you cannot index 0 for a 0 dimensional tensor.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\ntensor = torch.rand(torch.Size([]))\r\nres1 = torch.transpose(tensor, 0, 0)\r\n```\r\nIt will succeed. But when you index 0 for this tensor it will fail\r\n```\r\ntensor[0]\r\n# IndexError: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.hstack` should raise an error when tensor is 0 dimensional",
        "Bug description": "\r\n\r\n`torch.hstack` should raise an error when tensor is 0 dimensional like `torch.cat` based on the document that \"This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.\"\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\ntensor_0 = torch.rand(torch.Size([]))\r\ntensor_1 = torch.rand(torch.Size([3]))\r\ntensors = [tensor_0, tensor_1]\r\nres1 = torch.hstack(tensors)\r\n# succeed\r\nres2 = torch.cat(tensors, dim=0)\r\n# RuntimeError: zero-dimensional tensor (at position 0) cannot be concatenated\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.unique_consecutive`: passing positional optional arguments results in empty tensors",
        "Bug description": "\r\n\r\nPassing `True` for either `return_inverse` or `return_counts` (but not both!) as **a positional argument** to `torch.unique_consecutive` results in an empty second return value.\r\n\r\n",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "DLPack no longer works on Boolean tensors after 1.10+",
        "Bug description": "\r\n\r\n`torch.utils.dlpack.to_dlpack` no longer works for Boolean Tensor.\r\n\r\n## To Reproduce\r\n\r\n```python\r\ntorch.utils.dlpack.to_dlpack(torch.BoolTensor([False, True]))   # Bool type is not supported by dlpack\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.nn.functional.l1_loss` fails gradgradcheck for complex inputs",
        "Bug description": "\r\n\r\n`torch.utils.dlpack.to_dlpack` no longer works for Boolean Tensor.\r\n\r\n## To Reproduce\r\n\r\n```python\r\ntorch.utils.dlpack.to_dlpack(torch.BoolTensor([False, True]))   # Bool type is not supported by dlpack\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.sparse.sum on scalar sparse tensor fails when dim is specified",
        "Bug description": "\r\n\r\nAs in the title.\r\n\r\n## To Reproduce\r\n\r\n```python\r\n>>> import torch\r\n>>> t = torch.tensor(1).to_sparse()\r\n>>> torch.sparse.sum(t, dim=0)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/pearu/git/pearu/pytorch/torch/sparse/__init__.py\", line 152, in sum\r\n    return torch._sparse_sum(input, dim)\r\nRuntimeError: Trying to create tensor with negative dimension -1: [-1, 1]\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.cross precision problem",
        "Bug description": "\r\n\r\n`torch.cross` can encounter precision problem when two input vectors are the same.\r\n\r\n## To Reproduce\r\n\r\nThe following code can reproduce the problem.\r\n\r\n```python\r\na = torch.Tensor([0.7533, 0.6123, -0.2404])\r\nb = torch.Tensor([0.7533, 0.6123, -0.2404])\r\nc = torch.cross(a, b)\r\nprint(c)\r\n```\r\n\r\nThe output is:\r\n\r\n```python\r\ntensor([ 2.9772e-09, -3.4915e-09, 1.4205e-08])\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "ONNX export of torch.nn.functional.linear with weight dimension 1",
        "Bug description": "\r\ntorch.nn.functional.linear function doesn't validate weight dimension == 2. \r\nIt only asserts  weight.t() expects a tensor with <= 2 dimension.\r\n\r\nFrom doc: https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html#torch.nn.functional.linear\r\n```\r\n Weight: (out_features,in_features)\r\n```\r\n\r\nFor example, it works with weight 1-d\r\n```\r\n>>> import torch\r\n>>> a = torch.ones([1,1,10])\r\n>>> b = torch.ones([10])   # weight is 1-d\r\n>>> c = torch.nn.functional.linear(a, b)\r\n>>> c\r\ntensor([[10.]])\r\n```\r\nBut, **onnx.export** is fail on torch v1.9.0, otherwise torch v1.8.0 ~ v1.4.0 works. \r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.equal does not support sparse tensors",
        "Bug description": "\r\n\r\ntorch.equal(input,other) does not support sparse tensors.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nif __name__ == \"__main__\":\r\n    x = torch.rand(4, 4).to_sparse()\r\n    assert torch.equal(x, x)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"benchmarks/distributed/rpc/parameter_server/test.py\", line 82, in <module>\r\n    assert torch.equal(x, y)\r\nNotImplementedError: Could not run 'aten::equal' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::equal' is only available for these backends: [CPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\r\n\r\nCPU: registered at /fsx/users/gcramer/work/pytorch/build/aten/src/ATen/RegisterCPU.cpp:22545 [kernel]\r\nCUDA: registered at /fsx/users/gcramer/work/pytorch/build/aten/src/ATen/RegisterCUDA.cpp:30514 [kernel]\r\nQuantizedCPU: registered at /fsx/users/gcramer/work/pytorch/build/aten/src/ATen/RegisterQuantizedCPU.cpp:1060 [kernel]\r\nBackendSelect: fallthrough registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\r\nPython: registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:38 [backend fallback]\r\nNamed: fallthrough registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\r\nConjugate: registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/ConjugateFallback.cpp:26 [backend fallback]\r\nNegative: registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/native/NegateFallback.cpp:26 [backend fallback]\r\nADInplaceOrView: fallthrough registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\r\nAutogradOther: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradCPU: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradCUDA: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradXLA: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradLazy: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradXPU: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradMLC: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradHPU: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradNestedTensor: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradPrivateUse1: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradPrivateUse2: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradPrivateUse3: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nTracer: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/TraceType_3.cpp:11391 [kernel]\r\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/autocast_mode.cpp:455 [backend fallback]\r\nAutocast: fallthrough registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/autocast_mode.cpp:294 [backend fallback]\r\nBatched: registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\r\nVmapMode: fallthrough registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\r\n```\r\n\r\n```python\r\nif __name__ == \"__main__\":\r\n    x = torch.rand(4, 4).to_sparse_csr()\r\n    assert torch.equal(x, x)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 81, in <module>\r\n    assert torch.equal(x, x)\r\nNotImplementedError: Could not run 'aten::equal' with arguments from the 'SparseCsrCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::equal' is only available for these backends: [CPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\r\n\r\nCPU: registered at /fsx/users/gcramer/work/pytorch/build/aten/src/ATen/RegisterCPU.cpp:22545 [kernel]\r\nCUDA: registered at /fsx/users/gcramer/work/pytorch/build/aten/src/ATen/RegisterCUDA.cpp:30514 [kernel]\r\nQuantizedCPU: registered at /fsx/users/gcramer/work/pytorch/build/aten/src/ATen/RegisterQuantizedCPU.cpp:1060 [kernel]\r\nBackendSelect: fallthrough registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\r\nPython: registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:38 [backend fallback]\r\nNamed: fallthrough registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\r\nConjugate: registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/ConjugateFallback.cpp:26 [backend fallback]\r\nNegative: registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/native/NegateFallback.cpp:26 [backend fallback]\r\nADInplaceOrView: fallthrough registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\r\nAutogradOther: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradCPU: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradCUDA: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradXLA: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradLazy: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradXPU: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradMLC: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradHPU: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradNestedTensor: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradPrivateUse1: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradPrivateUse2: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nAutogradPrivateUse3: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:12635 [autograd kernel]\r\nTracer: registered at /fsx/users/gcramer/work/pytorch/torch/csrc/autograd/generated/TraceType_3.cpp:11391 [kernel]\r\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/autocast_mode.cpp:455 [backend fallback]\r\nAutocast: fallthrough registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/autocast_mode.cpp:294 [backend fallback]\r\nBatched: registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\r\nVmapMode: fallthrough registered at /fsx/users/gcramer/work/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "NaN values on torch.nn.functional.conv2d (aarch64)",
        "Bug description": "\r\nNaN values randomly introduced on torch.nn.functional.conv2d\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport torch\r\nimport torch.nn.functional as F\r\nF.conv2d(torch.ones(1,3,160,240).cpu(), torch.ones(1,3,3,3).cpu())\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.jit.trace() fails on a GCN with sparse inputs and dense layers",
        "Bug description": " of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Construct a basic gcn (example code below)\r\n```\r\nimport torch\r\n\r\n### LAYER and GCN DEFINITION\r\n\r\nclass HiddenLayerSparse(torch.nn.Module):\r\n    def __init__(self, dimf_in, dimf_out):\r\n        super().__init__()\r\n        self.weights = torch.nn.Parameter(torch.rand(dimf_in, dimf_out, dtype=float))\r\n\r\n    def forward(self, adj, x):\r\n        x = torch.mm(x, self.weights)\r\n        x = torch.mm(adj, x)\r\n        return x\r\n\r\nclass ExampleSparseGCN(torch.nn.Module):\r\n    def __init__(self, sizes):\r\n        super().__init__()\r\n        self.sizes = sizes\r\n        self.hidden_layers = torch.nn.ModuleList([HiddenLayerSparse(sizes[i], sizes[i+1]) for i in range(len(sizes)-1)])\r\n        self.nonlinear = torch.nn.ReLU()\r\n        self.softmax = torch.nn.Softmax(dim=1)\r\n\r\n    def forward(self, adj, x):\r\n        for h in self.hidden_layers:\r\n            x = self.nonlinear(h(adj, x))\r\n        return self.softmax(x)\r\n\r\n### EXAMPLE SPARSE INPUTS\r\n\r\nadjacency = torch.tensor([[1,0],[0,1]], dtype=float).to_sparse()\r\nfeatures = torch.tensor([[1,0,0,1,0],[0,1,0,0,1]],dtype=float).to_sparse()\r\nlabels = torch.tensor([[1,0,0], [0,1,0]],dtype=float)\r\ninputs = (adjacency, features)\r\n\r\nmodel = ExampleSparseGCN([5, 4, 3]) # training does not fail with [5,3]\r\nloss_fn =  torch.nn.L1Loss()\r\noptimizer = torch.optim.SGD(model.parameters(), lr=0.05)\r\n\r\n_train = False      # FLAG TO ENABLE TRAINING\r\n_trace = True      # FLAG TO ENABLE TRACING\r\n\r\n# TRAINING \r\nif _train:\r\n    model.train()\r\n    for t in range(50):\r\n        optimizer.zero_grad()\r\n        output = model(*inputs)\r\n        loss = loss_fn(output, labels)\r\n        loss.backward()\r\n        optimizer.step() \r\n    print('Train Complete')\r\n\r\n# TRACING\r\nif _trace:\r\n    model.eval()\r\n    with torch.no_grad():\r\n        #torch.onnx.export(model, inputs, path, do_constant_folding=True, verbose=True, export_params=True)\r\n        traced = torch.jit.trace(model, inputs)\r\n    print('Trace Complete')\r\n```\r\n2. Attempt to trace a model without training. (`python example.py`)\r\n3. Attempt to trace a model with training (set `_train = True`)\r\n\r\nAny GCN with sparse inputs **or** dense inputs with `torch.sparse.mm` fails to trace. Using `torch.sparse.mm` breaks training with exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \".\\minimum_example.py\", line 46, in <module>\r\n    loss.backward()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\", line 221, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 130, in backward\r\n    Variable._execution_engine.run_backward(\r\nRuntimeError: sparse_.is_sparse() INTERNAL ASSERT FAILED at \"..\\\\torch\\\\csrc\\\\autograd\\\\FunctionsManual.cpp\":560, please report a bug to PyTorch.\r\n```\r\n\r\n## Expected behavior\r\n\r\nI expect the model to train and trace without triggering any exceptions using `torch.sparse.mm`. Currently tracing always fails, and using `torch.sparse.mm` causes training to fail in the backward pass (which can be alleviated by using `torch.mm`.)\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.7.1+cpu\r\nIs debug build: False\r\nCUDA used to build PyTorch: Could not collect\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Microsoft Windows 10 Education\r\nGCC version: Could not collect\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.8 (64-bit runtime)\r\nIs CUDA available: False\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce RTX 2070\r\nNvidia driver version: 456.71\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.5\r\n[pip3] numpydoc==1.1.0\r\n[pip3] torch==1.7.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2020.1                      216\r\n[conda] mkl-service               2.3.0            py38hb782905_0\r\n[conda] mkl_fft                   1.1.0            py38h45dec08_0\r\n[conda] mkl_random                1.1.1            py38h47e9c7a_0\r\n[conda] numpy                     1.18.5           py38h6530119_0\r\n[conda] numpy-base                1.18.5           py38hc3f5095_0\r\n[conda] numpydoc                  1.1.0                      py_0\r\n[conda] torch                     1.7.1                    pypi_0    pypi\r\n```\r\n\r\n## Additional context\r\n\r\nPlease let me know if more context/information/support is needed from my end.\r\n\n\ncc @gmagogsfm",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.pow(tensor, tensor) throws RuntimeError for dtype bool ",
        "Bug description": "\r\n\r\n`torch.pow(tensor, tensor)` doesn't work for dtype `bool`, although its method variant works.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Try `torch.pow(tensor, tensor)` for `bool` tensor.\r\n2. On CPU, the runtime error is:\r\n   `RuntimeError: \"pow\" not implemented for 'Bool'`.\r\n   On CUDA, the runtime error is:\r\n  `RuntimeError: \"pow_cuda\" not implemented for 'Bool'`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Caught an integer overflow or wraparound in torch.nn.MaxPool1d",
        "Bug description": "\r\n\r\nThe `MaxPool1d` takes an integer as its first argument. However, when I pass a big number to this function, I got an Runtime Errror. This error is quit normal but the log is interesting.\r\n`RuntimeError: Given input size: (16x1x50). Calculated output size: (16x1x-272823246). Output size is too small`\r\nthe calculated out put size, has a negative number. I think it meets an integer overflow. But I can't figure out what function going wrong right now.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n`import torch `\r\n`m=torch.nn.MaxPool1d(545646544,stride=2)`\r\n`input=input=torch.randn(20,16,50)`.\r\n`m(input)`\r\n\r\nthen I got the runtime error.\r\n`Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/python3.7.5/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 651, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/local/python3.7.5/lib/python3.7/site-packages/torch/nn/modules/pooling.py\", line 76, in forward\r\n    self.return_indices)\r\n  File \"/usr/local/python3.7.5/lib/python3.7/site-packages/torch/_jit_internal.py\", line 209, in fn\r\n    return if_false(*args, **kwargs)\r\n  File \"/usr/local/python3.7.5/lib/python3.7/site-packages/torch/nn/functional.py\", line 496, in _max_pool1d\r\n    input, kernel_size, stride, padding, dilation, ceil_mode)\r\nRuntimeError: Given input size: (16x1x50). Calculated output size: (16x1x-272823246). Output size is too small\r\n`\r\n\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.matmul doesn't handle zero-sized inputs in some cases, leading to batched grad failures",
        "Bug description": "\r\n\r\nI am getting the following error when running batched grad and gradgrad checks through OpInfo with empty input tensors\r\n\r\n```\r\nRuntimeError: While computing batched gradients, got: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.empty_strided doesn't test if strides are negative",
        "Bug description": "\r\n\r\nI am getting the following error when running batched grad and gradgrad checks through OpInfo with empty input tensors\r\n\r\n```\r\nRuntimeError: While computing batched gradients, got: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "[bug] torch.cumsum: behaviour for `bool` input",
        "Bug description": "\r\n\r\nI am getting the following error when running batched grad and gradgrad checks through OpInfo with empty input tensors\r\n\r\n```\r\nRuntimeError: While computing batched gradients, got: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.nn.functional.grid_sample outputs NaN",
        "Bug description": "\r\n\r\n`torch.nn.functional.grid_sample` outputs NaN if `grid` contains large value\r\n\r\n## To Reproduce\r\n~~~python\r\nimport torch\r\ntorch.nn.functional.grid_sample(input=torch.ones([1,1,1,5]), grid=torch.tensor([[[[ 2.9839e+38, -3.2406e+38]]]]))\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\ntensor([[[[nan]]]])\r\n~~~\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.nn.functional.binary_cross_entropy(_with_logits) outputs NaN",
        "Bug description": "\r\n\r\n`torch.nn.functional.binary_cross_entropy_with_logits` outputs NaN when input is empty or large\r\n`torch.nn.functional.binary_cross_entropy` outputs NaN when input is empty\r\n\r\n\r\n## To Reproduce\r\n~~~python\r\nimport torch\r\ntorch.nn.functional.binary_cross_entropy(input=torch.tensor([]), target=torch.tensor([]))\r\n~~~\r\nOutput:\r\n~~~python\r\ntensor(nan)\r\n~~~\r\n\r\n~~~python\r\nimport torch\r\ntorch.nn.functional.binary_cross_entropy_with_logits(input=torch.tensor([]), target=torch.tensor([]))\r\n~~~\r\nOutput:\r\n~~~python\r\ntensor(nan)\r\n~~~\r\n\r\n~~~python\r\nimport torch\r\ntorch.nn.functional.binary_cross_entropy_with_logits(input=torch.tensor([-2.3135e+307,  6.6756e+307]), target=torch.ones((2)))\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\ntensor(nan)\r\n~~~\r\n\r\n\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.fx.symbolic_trace fails on torch.arange with input-dependent size",
        "Bug description": "\r\n\r\nI'm trying to use torch.fx.symbolic_trace to trace an `arange` call where the first argument to `arange` is part of the size of the input tensor:\r\n```\r\nl = x.size(1)\r\ntorch.arange(l, dtype=torch.long, device='cuda')\r\n```\r\nAnd it fails.\r\n\r\n## To Reproduce\r\n\r\n```\r\nimport torch\r\nfrom torch.fx import symbolic_trace\r\ndef test(x):\r\n    l = x.size(1)\r\n    return torch.arange(l, dtype=torch.long, device='cuda')\r\ntraced = symbolic_trace(test)\r\n```\r\nTrace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"repro.py\", line 6, in <module>\r\n    traced = symbolic_trace(test)\r\n  File \"/opt/pytorch/pytorch/torch/fx/symbolic_trace.py\", line 606, in symbolic_trace\r\n    graph = tracer.trace(root, concrete_args)\r\n  File \"/opt/pytorch/pytorch/torch/fx/symbolic_trace.py\", line 355, in trace\r\n    self.create_node('output', 'output', (self.create_arg(fn(*args)),), {},\r\n  File \"repro.py\", line 5, in test\r\n    return torch.arange(l, dtype=torch.long, device='cuda')\r\nTypeError: arange() received an invalid combination of arguments - got (Proxy, device=str, dtype=torch.dtype), but expected one of:\r\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\r\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\r\n *\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.nn.functional.ctc_loss crash(segfault) ",
        "Bug description": "\r\n\r\n`torch.nn.functional.ctc_loss`  crash(segfault) when `input_lengths` contains large negative number.\r\n\r\n## To Reproduce\r\n~~~python\r\nimport torch \r\ntorch.nn.functional.ctc_loss(log_probs=torch.ones((1,2,1)), targets=torch.ones((2,1)), input_lengths=torch.tensor([-5570080269274466818, -1]), target_lengths=torch.tensor((1,1)))\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\nSegmentation fault (core dumped)\r\n~~~\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "segmentation fault in torch.nn.ReplicationPad3d/2d when padding is large",
        "Bug description": "\r\n\r\nsegmentation fault in `torch.nn.ReplicationPad3d` and `torch.nn.ReplicationPad2d` when `padding` is large\r\n\r\n## To Reproduce\r\n~~~python\r\nimport torch\r\nlayer = torch.nn.ReplicationPad3d(padding=498444555)\r\nmodel_input = torch.ones([1, 1, 1, 1, 1])\r\nlayer(model_input)\r\n~~~\r\n\r\n~~~python\r\nimport torch\r\nlayer = torch.nn.ReplicationPad2d(padding=1012756988)\r\nmodel_input = torch.ones([2,2,2,2])\r\nlayer(model_input)\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\nSegmentation fault (core dumped)\r\n~~~\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Segfault in torch.bincount",
        "Bug description": " of what the bug is. -->\r\n\r\n## To Reproduce\r\n~~~python\r\nimport torch\r\ntorch.bincount(input =torch.tensor([9223372036854775807]))\r\n~~~\r\n\r\nThe code snippet gives \r\n~~~\r\nSegmentation fault (core dumped)\r\n~~~\r\n\r\n## Expected behavior\r\nexpect no crash\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.5.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: 10.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.4 LTS (x86_64)\r\nGCC version: Could not collect\r\nClang version: Could not collect\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7 (64-bit runtime)\r\nIs CUDA available: False\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Input dimension check for `torch.gather`",
        "Bug description": "\r\n\r\nThe input dimension check was dropped somewhere between PyTorch 1.5.1 and 1.6.\r\n\r\n## To Reproduce\r\n\r\nThe following code runs successfully. However, the `index` argument has incompatible dimensions and should raise an exception.\r\n\r\n```python\r\nimport torch\r\ntorch.manual_seed(0)\r\ninput = torch.rand(4, 2)\r\nindex = torch.randint(2, size=(4,)).unsqueeze(0)  # intended to be unsqueeze(1)\r\ndim = 1\r\noutput = torch.gather(input, dim, index)\r\nprint(\"input = \", input)\r\nprint(\"index = \", index)\r\nprint(\"output = \", output)\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.mode when input has nans",
        "Bug description": " of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nOutput of `collect_env.py`:\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.6.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: 10.2\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nClang version: Could not collect\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7 (64-bit runtime)\r\nIs CUDA available: True\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: TITAN RTX\r\nGPU 1: TITAN RTX\r\nGPU 2: GeForce RTX 2080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 440.64.00\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.4.3\r\n[pip3] numpy==1.16.1\r\n[pip3] numpy-quaternion==2020.10.2.17.17.31\r\n[pip3] numpy-stl==2.10.1\r\n[pip3] torch==1.6.0\r\n[pip3] torchvision==0.6.0\r\n[conda] msgpack-numpy             0.4.4.3                  pypi_0    pypi\r\n[conda] numpy                     1.16.1                   pypi_0    pypi\r\n[conda] numpy-quaternion          2020.10.2.17.17.31          pypi_0    pypi\r\n[conda] numpy-stl                 2.10.1                   pypi_0    pypi\r\n[conda] torch                     1.6.0                    pypi_0    pypi\r\n[conda] torchvision               0.6.0                    pypi_0    pypi\r\n```\r\n\r\n\n\ncc @brianjo @mruberry @rgommers @heitorschueroff @ezyang @gchanan @zou3519 @bdhirsh @ejguan @jlin27",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "`torch.svd()` CUDA gives incorrect results when input contains `nan`",
        "Bug description": "\r\n\r\nIn at least one case, `torch.svd()`'s CUDA implementation gives an incorrect result when the input contains a `nan` value.\r\n\r\nThis issue shows up when the input is `torch.tensor([[float('nan'), 1.0]])`.\r\n\r\n## To Reproduce\r\n\r\n```\r\n>>> import torch\r\n>>> torch.svd(torch.tensor([[float('nan'), 1.0]]))\r\ntorch.return_types.svd(\r\nU=tensor([[1.]]),\r\nS=tensor([nan]),\r\nV=tensor([[nan],\r\n        [nan]]))\r\n>>> torch.svd(torch.tensor([[float('nan'), 1.0]]).cuda())\r\ntorch.return_types.svd(\r\nU=tensor([[1.]], device='cuda:0'),\r\nS=tensor([1.4142], device='cuda:0'),\r\nV=tensor([[nan],\r\n        [nan]], device='cuda:0'))\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.multinomial with replacement=True produces inaccurate results for large number of categories",
        "Bug description": "\r\n\r\n`torch.multinomial` with `replacement=True` produces very inaccurate results when number of categories is large (and/or probabilities have very different values)\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.cartesian_prod(*tensors) error when you have tensors with [x,y]",
        "Bug description": "\r\n\r\n`torch.multinomial` with `replacement=True` produces very inaccurate results when number of categories is large (and/or probabilities have very different values)\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.masked_select out argument can easily be misused, because output shape is dynamically computed",
        "Bug description": " of what the bug is. -->\r\nUnexpected behavior of `masked_select`, likely due to `out` option should be contiguous but was not clarified in the doc.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nx, m, z = torch.ones((4,3,2)), torch.zeros((4,3), dtype=torch.bool), torch.zeros((4,2,2))\r\nm[:, 1] = True\r\n\r\ntorch.masked_select(x, m[..., None], out=z[:, 1, :])\r\n```\r\n\r\n## Expected behavior\r\n\r\n```python\r\n>>> print(z)\r\ntensor([[[0., 0.],  \r\n         [1., 1.]], \r\n                    \r\n        [[0., 0.],  \r\n         [1., 1.]], \r\n                    \r\n        [[0., 0.],  \r\n         [1., 1.]], \r\n                    \r\n        [[0., 0.],  \r\n         [1., 1.]]])\r\n```\r\n\r\n## Observed behavior\r\n\r\n```python\r\n>>> print(z)\r\ntensor([[[0., 0.],  \r\n         [1., 1.]], \r\n                    \r\n        [[1., 1.],  \r\n         [1., 1.]], \r\n                    \r\n        [[1., 1.],  \r\n         [0., 0.]], \r\n                    \r\n        [[0., 0.],  \r\n         [0., 0.]]])\r\n```\r\n\r\n## Environment\r\nPyTorch version: 1.3.1\r\nCUDA used to build PyTorch: 10.1.243\r\nOS: Ubuntu 18.04\r\nPython version: 3.7\r\n\r\ncc @ezyang @gchanan @zou3519 @mruberry @rgommers @heitorschueroff",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.std() returns nan for single item tensors.",
        "Bug description": "\r\n\r\nnp.std(4) returns 0 whereas torch.std(torch.tensor(4)) returns NaN. This causes numerical instabilities in certain situations.\r\n\r\n## To Reproduce\r\n\r\nimport numpy as np\r\nnp.std(4)  # returns 0\r\n\r\nimport torch\r\ntorch.std(torch.tensor(4.))  # returns NaN\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Torch.jit.trace unexpected error with `torch.cat(\u2026, dim=-1)` ",
        "Bug description": "\r\n\r\nnp.std(4) returns 0 whereas torch.std(torch.tensor(4)) returns NaN. This causes numerical instabilities in certain situations.\r\n\r\n## To Reproduce\r\n\r\nimport numpy as np\r\nnp.std(4)  # returns 0\r\n\r\nimport torch\r\ntorch.std(torch.tensor(4.))  # returns NaN\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Can't `torch.sum(tensor, dim)` where `dim >= 64`",
        "Bug description": "\r\n\r\nCan't use TensorIterator reduction functions if the dimension to be reduced over is > 64.\r\n\r\n## To Reproduce\r\n\r\n```\r\nIn [1]: import torch\r\n\r\nIn [2]: sizes = [1]*65\r\n\r\nIn [3]: x = torch.randn(sizes)\r\n\r\nIn [4]: x.sum(0)\r\nOut[4]: tensor([[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[-0.2885]]]]]]]]]]]]]]]]\r\n]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]])\r\n\r\nIn [5]: x.sum(65)\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-5-cfca9db78f16> in <module>()\r\n----> 1 x.sum(65)\r\n\r\nIndexError: Dimension out of range (expected to be in range of [-65, 64], but got 65)\r\n\r\nIn [6]: x.sum(64)\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-6375767cd187> in <module>()\r\n----> 1 x.sum(64)\r\n\r\nRuntimeError: bitset::set: __position (which is 64) >= _Nb (which is 64)\r\n```\r\n\r\n# ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "The speed of `torch.einsum` and `torch.matmul` when using `fp16` is slow",
        "Bug description": "\r\n\r\nI found that the speed of `torch.einsum` when using fp16 is much slower than using fp32.\r\n\r\nwhen the shapes of inputs are (a,b,c) and (a,c,d), `matmul` became much slower as well.\r\n\r\n## To Reproduce\r\n```python\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES']='0'\r\nimport torch\r\nfrom time import time\r\n\r\na = torch.empty(24,32,40,48, dtype=torch.float32).to('cuda')\r\nb = torch.empty(64,32,40,48, dtype=torch.float32).to('cuda')\r\nc = torch.empty(40,80,24, dtype=torch.float32).to('cuda')\r\nd = torch.empty(40,24,16, dtype=torch.float32).to('cuda')\r\n\r\nst = time()\r\nfor _ in range(1000):\r\n    c.matmul(d)\r\nprint(time()-st)\r\n\r\nst = time()\r\nfor _ in range(1000):\r\n    torch.einsum('ibnd,jbnd->ijbn', a, b)\r\nprint(time()-st)\r\n\r\na = torch.empty(24,32,40,48, dtype=torch.float16).to('cuda')\r\nb = torch.empty(64,32,40,48, dtype=torch.float16).to('cuda')\r\nc = torch.empty(40,80,24, dtype=torch.float16).to('cuda')\r\nd = torch.empty(40,24,16, dtype=torch.float16).to('cuda')\r\n\r\nst = time()\r\nfor _ in range(1000):\r\n    torch.matmul(c,d)\r\nprint(time()-st)\r\n\r\nst = time()\r\nfor _ in range(1000):\r\n    torch.einsum('ibnd,jbnd->ijbn', a, b)\r\nprint(time()-st)\r\n```\r\nSteps to reproduce the behavior:\r\n\r\njust run it\r\n\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Pytorch deadlock from distributed multiprocessing",
        "Bug description": "\r\n\r\nPytorch hangs indefinitely when using distributed multiprocessing with Pytorch 1.1.0 after 77k iterations (77939 and 77940 iterations). In Pytorch 1.0.1.post2, there is no such bug.\r\n\r\nIt appears to be deadlock.\r\n\r\nThe following distributed modules combine together in a very nasty way for some reason:\r\n```\r\nimport torch.distributed as dist\r\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(train_data)\r\ndist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank)\r\napex.parallel.DistributedDataParallel(model)\r\nmodel, optimizer = apex.amp.initialize(model.cuda(), optimizer, opt_level=args.opt_level, keep_batchnorm_fp32=args.keep_batchnorm_fp32, loss_scale=args.loss_scale)\r\n model = apex.parallel.DistributedDataParallel(model)\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Clone the following repo: [semseg](https://github.com/hszhao/semseg)\r\n2. Use the following config:\r\n3. Run the training script [train.py](https://github.com/hszhao/semseg/blob/master/tool/train.py)\r\n4. Observe that Pytorch will hang indefinitely after the 77,939th iteration. \r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "Memory leak when utilizing pandas indices",
        "Bug description": "\r\n\r\nThere is a memory leak when utilizing pandas.Int64Index + pytorch on windows (haven't tested Linux + mac)\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "CUDA large matrix-vector product (torch.mv) causes illegal memory access",
        "Bug description": "\r\n\r\n`torch.mv` causes an \"illegal memory access\" when multiplying a matrix with more than 2^31-1 elements. Note that each dim of the first matrix can fit in an `int`.\r\n\r\nThis is likely a bug in cuBLAS. Either cuBLAS should be fixed or PyTorch should issue multiple calls to `cublasSgemv`.\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.nn.CrossEntropyLoss with \"reduction\" sum/mean is not deterministic on segmentation outputs / labels",
        "Bug description": "\r\ntorch.nn.CrossEntropyLoss doesn't output deterministic results on segmentation outputs / labels, when using reduction other than 'none'.\r\nHappens only on GPU. CPU does give a consistent behavior. \r\n\r\n## To Reproduce\r\n```\r\nimport numpy as np\r\nimport torch\r\n\r\noutputs = np.random.rand(16, 1, 256, 256)\r\noutputs = np.hstack((outputs, 1.0 - outputs))\r\ntargets = np.random.randint(2, size=(16, 256, 256))\r\n\r\nseed = 0\r\ntorch.backends.cudnn.deterministic = True\r\ntorch.backends.cudnn.benchmark = False\r\n\r\nfor reduction in ['none', 'sum', 'mean']:\r\n    print(reduction)\r\n\r\n    for i in range(10):\r\n        torch.manual_seed(seed)\r\n        np.random.seed(seed)\r\n\r\n        outputs_t, targets_t = torch.from_numpy(outputs), torch.from_numpy(targets)\r\n        outputs_t, targets_t = outputs_t.cuda(0), targets_t.cuda(0)\r\n\r\n        loss_fn = torch.nn.CrossEntropyLoss(reduction=reduction)\r\n        loss_fn = loss_fn.cuda(0)\r\n\r\n        loss = loss_fn(outputs_t, targets_t)\r\n        loss = loss.detach().cpu().numpy()\r\n        print(i, outputs.sum(), targets.sum(), outputs.mean(), targets.mean(), loss.sum(), loss.mean())\r\n```\r\n## Output\r\n```\r\nnone\r\n0 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n1 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n2 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n3 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n4 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n5 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n6 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n7 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n8 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\n9 1048576.0 524341 0.5 0.5000505447387695 769533.4950007759 0.7338843297965774\r\nsum\r\n0 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n1 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n2 1048576.0 524341 0.5 0.5000505447387695 769533.4950007757 769533.4950007757\r\n3 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n4 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n5 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n6 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n7 1048576.0 524341 0.5 0.5000505447387695 769533.4950007754 769533.4950007754\r\n8 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\n9 1048576.0 524341 0.5 0.5000505447387695 769533.4950007756 769533.4950007756\r\nmean\r\n0 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n1 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n2 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n3 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n4 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n5 1048576.0 524341 0.5 0.5000505447387695 0.7338843297965769 0.7338843297965769\r\n6 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n7 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n8 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n9 1048576.0 524341 0.5 0.5000505447387695 0.733884329796577 0.733884329796577\r\n```\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "torch.load() large tensor return 'invalid memory size'",
        "Bug description": "\r\n\r\nHello,\r\n\r\nI'm having a problem of loading a serialized tensor from a file.\r\nMy tensor shape is [309000001, 2, 5] the dtype is torch.int8\r\nWhen I deserialize the tensor using torch.load(), it yell \"invalid memory size\".\r\nThe line that it complain is:\r\nhttps://github.com/pytorch/pytorch/blob/b740b92f3600840e09d4c93f3138333838d1e474/torch/serialization.py#L528-L529\r\nIt seem like the variable it used is destructured from here:\r\nhttps://github.com/pytorch/pytorch/blob/b740b92f3600840e09d4c93f3138333838d1e474/torch/serialization.py#L525\r\nWhich also came from here:\r\nhttps://github.com/pytorch/pytorch/blob/b740b92f3600840e09d4c93f3138333838d1e474/torch/serialization.py#L517\r\nSo I insert `print(saved_id)` where it yell:\r\nNow, it print:\r\n`('storage', <class 'torch.CharStorage'>, '2149339138576', 'cpu', -1204967286, None)`\r\n\r\nIt seem that size is negative.\r\n\r\n## To Reproduce\r\n```\r\nimport torch\r\n\r\ntorch.save(torch.zeros([309237982, 2, 5], dtype=torch.int8), './sample.pt')\r\ntorch.load('./sample.pt')\r\n```\r\nIt can only be reproduce in MS.Windows environment.\r\nMy Windows version is:\r\n10.0.17134.472\r\nMy Python version is:\r\n3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)]\r\nSteps to reproduce the behavior:\r\n1. Run above code in MS. Windows\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-2-1e8f41ee3715> in <module>\r\n      2 \r\n      3 torch.save(torch.zeros([309237982, 2, 5], dtype=torch.int8), './sample.pt')\r\n----> 4 torch.load('./sample.pt')\r\n\r\nc:\\users\\nattapong\\workspace\\lstm_nlp\\lib\\site-packages\\torch\\serialization.py in load(f, map_location, pickle_module)\r\n    365         f = open(f, 'rb')\r\n    366     try:\r\n--> 367         return _load(f, map_location, pickle_module)\r\n    368     finally:\r\n    369         if new_fd:\r\n\r\nc:\\users\\nattapong\\workspace\\lstm_nlp\\lib\\site-packages\\torch\\serialization.py in _load(f, map_location, pickle_module)\r\n    538     unpickler = pickle_module.Unpickler(f)\r\n    539     unpickler.persistent_load = persistent_load\r\n--> 540     result = unpickler.load()\r\n    541 \r\n    542     deserialized_storage_keys = pickle_module.load(f)\r\n\r\nc:\\users\\nattapong\\workspace\\lstm_nlp\\lib\\site-packages\\torch\\serialization.py in persistent_load(saved_id)\r\n    504                 print(data_type, size)\r\n    505                 deserialized_objects[root_key] = restore_location(\r\n--> 506                     data_type(size), location)\r\n    507             storage = deserialized_objects[root_key]\r\n    508             if view_metadata is not None:\r\n\r\nRuntimeError: $ Torch: invalid memory size -- maybe an overflow? at ..\\aten\\src\\TH\\THGeneral.cpp:188\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    },
    {
        "Issue title": "[memory leak] [PyTorch] .backward(create_graph=True)",
        "Bug description": "\r\n\r\nHello,\r\n\r\nI'm having a problem of loading a serialized tensor from a file.\r\nMy tensor shape is [309000001, 2, 5] the dtype is torch.int8\r\nWhen I deserialize the tensor using torch.load(), it yell \"invalid memory size\".\r\nThe line that it complain is:\r\nhttps://github.com/pytorch/pytorch/blob/b740b92f3600840e09d4c93f3138333838d1e474/torch/serialization.py#L528-L529\r\nIt seem like the variable it used is destructured from here:\r\nhttps://github.com/pytorch/pytorch/blob/b740b92f3600840e09d4c93f3138333838d1e474/torch/serialization.py#L525\r\nWhich also came from here:\r\nhttps://github.com/pytorch/pytorch/blob/b740b92f3600840e09d4c93f3138333838d1e474/torch/serialization.py#L517\r\nSo I insert `print(saved_id)` where it yell:\r\nNow, it print:\r\n`('storage', <class 'torch.CharStorage'>, '2149339138576', 'cpu', -1204967286, None)`\r\n\r\nIt seem that size is negative.\r\n\r\n## To Reproduce\r\n```\r\nimport torch\r\n\r\ntorch.save(torch.zeros([309237982, 2, 5], dtype=torch.int8), './sample.pt')\r\ntorch.load('./sample.pt')\r\n```\r\nIt can only be reproduce in MS.Windows environment.\r\nMy Windows version is:\r\n10.0.17134.472\r\nMy Python version is:\r\n3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)]\r\nSteps to reproduce the behavior:\r\n1. Run above code in MS. Windows\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-2-1e8f41ee3715> in <module>\r\n      2 \r\n      3 torch.save(torch.zeros([309237982, 2, 5], dtype=torch.int8), './sample.pt')\r\n----> 4 torch.load('./sample.pt')\r\n\r\nc:\\users\\nattapong\\workspace\\lstm_nlp\\lib\\site-packages\\torch\\serialization.py in load(f, map_location, pickle_module)\r\n    365         f = open(f, 'rb')\r\n    366     try:\r\n--> 367         return _load(f, map_location, pickle_module)\r\n    368     finally:\r\n    369         if new_fd:\r\n\r\nc:\\users\\nattapong\\workspace\\lstm_nlp\\lib\\site-packages\\torch\\serialization.py in _load(f, map_location, pickle_module)\r\n    538     unpickler = pickle_module.Unpickler(f)\r\n    539     unpickler.persistent_load = persistent_load\r\n--> 540     result = unpickler.load()\r\n    541 \r\n    542     deserialized_storage_keys = pickle_module.load(f)\r\n\r\nc:\\users\\nattapong\\workspace\\lstm_nlp\\lib\\site-packages\\torch\\serialization.py in persistent_load(saved_id)\r\n    504                 print(data_type, size)\r\n    505                 deserialized_objects[root_key] = restore_location(\r\n--> 506                     data_type(size), location)\r\n    507             storage = deserialized_objects[root_key]\r\n    508             if view_metadata is not None:\r\n\r\nRuntimeError: $ Torch: invalid memory size -- maybe an overflow? at ..\\aten\\src\\TH\\THGeneral.cpp:188\r\n\r\n## ",
        "Sample Code": [
            "@@ -938,6 +938,9 @@ class TestCuda(TestCase):\n     def test_neg(self):\n         _TestTorchMixin._test_neg(self, lambda t: t.cuda())\n \n+    def test_isinf(self):\n+        _TestTorchMixin._test_isinf(self, lambda t: t.cuda())\n+\n     @unittest.skipIf(not TEST_LARGE_TENSOR, \"not enough memory\")\n     def test_arithmetic_large_tensor(self):\n         x = torch.empty(2**30, device='cuda')\n",
            "@@ -5316,9 +5316,23 @@ class _TestTorchMixin(object):\n         x = torch.tensor([1, 2, 3])\n         self.assertEqual(torch.isfinite(x), torch.ByteTensor([1, 1, 1]))\n \n+    @staticmethod\n+    def _test_isinf(self, cast):\n+        t1 = cast(torch.Tensor([1, inf, 2, -inf, nan]))\n+        t2 = cast(torch.ByteTensor([1, 2, 3]))\n+        t3 = cast(torch.CharTensor([1, 2, 3]))\n+        t4 = cast(torch.ShortTensor([1, 2, 3]))\n+        t5 = cast(torch.IntTensor([1, 2, 3]))\n+        t6 = cast(torch.LongTensor([1, 2, 3]))\n+        self.assertEqual(torch.isinf(t1), cast(torch.ByteTensor([0, 1, 0, 1, 0])))\n+        self.assertEqual(torch.isinf(t2), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t3), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t4), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t5), cast(torch.ByteTensor([0, 0, 0])))\n+        self.assertEqual(torch.isinf(t6), cast(torch.ByteTensor([0, 0, 0])))\n+\n     def test_isinf(self):\n-        x = torch.Tensor([1, inf, 2, -inf, nan])\n-        self.assertEqual(torch.isinf(x), torch.ByteTensor([0, 1, 0, 1, 0]))\n+        self._test_isinf(self, lambda t: t)\n \n     def test_isnan(self):\n         x = torch.Tensor([1, nan, 2])\n",
            "@@ -240,6 +240,8 @@ def isinf(tensor):\n     \"\"\"\n     if not isinstance(tensor, torch.Tensor):\n         raise ValueError(\"The argument is not a tensor\", str(tensor))\n+    if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]:\n+        return torch.zeros_like(tensor, dtype=torch.uint8)\n     return tensor.abs() == inf\n \n \n"
        ],
        "Bug fix": ""
    }
]
