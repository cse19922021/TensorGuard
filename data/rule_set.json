{
    "edge cases":
        {
            "test":
            {
                "deleted": "-  TORCH_CHECK(std > 0.0, \"normal_ expects std > 0.0, but found std=\", std);  // TODO: dedupe",
                "added": "+  TORCH_CHECK(std >= 0.0, \"normal_ expects std >= 0.0, but found std=\", std);  // TODO: dedupe"
            }
        },
    "type checking":
        {
            "test":
                {
                    "deleted": "-    assert isinstance(datapipe, IterDataPipe)",
                    "added": "+    assert isinstance(datapipe, (IterDataPipe, MapDataPipe))"
                }
        },
    "null checker":
        {
            "test":
                {
                    "deleted": "",
                    "added": "+    if (OperatorBase::InputBlob(0).GetRaw() == nullptr) {\n+      return true;\n+    }"
                }
        },
    "boundary check":
        {
            "test":
                {
                    "deleted": "-    TORCH_CHECK(i <= UINT32_MAX);\n-    iterShapeData[i] = (uint32_t)(iterShape[i]);\n-      strides[i][offset] = iter.strides(offset)[i];",
                    "added": "+  TORCH_CHECK(iter.can_use_32bit_indexing(), \"Can't be indexed using 32-bit iterator\");\n+    iterShapeData[i] = static_cast<uint32_t>(iterShape[i]);\n+      strides[i][offset] = static_cast<uint32_t>(iter.strides(offset)[i]);"
                }
        },
    "misleading error message":
        {
            "test":
                {
                    "deleted": "-                error_message += 'XPU Autocast only supports dtype of torch.bfloat16 currently.'",
                    "added": "+                error_message += 'XPU Autocast only supports dtypes of torch.bfloat16 and torch.float16 currently.'"
                }
        },
    "device availability":
        {
            "test":
                {
                    "deleted": "-        if not torch.cuda.is_available() and self.device == 'cuda':",
                    "added": "+        if torch.cuda.amp.common.amp_definitely_not_available() and self.device == 'cuda':"
                }
        },
    "device version":
        {
            "test":
                {
                    "deleted": "-#if CUDA_VERSION < 10000",
                    "added": "+#if defined(CUDA_VERSION) && (CUDA_VERSION < 10000)"
                }
        },
    "tensor execution mode":
        {
            "test":
                {
                    "deleted": "-    if context.executing_eagerly():\n-      trainable = variable._trainable  # pylint: disable=protected-access",
                    "added": "+    if ops.executing_eagerly_outside_functions():\n+      trainable = variable.trainable"
                }
        },
    "computation graph":
        {
            "test":
                {
                    "deleted": "-                if self.has_backedge():",
                    "added": "+                if self.has_backedge() and self.should_compile_partial_graph():"
                }
        },
    "tensor quantization":
        {
            "test":
                {
                    "deleted": "",
                    "added": "+                    if orig.is_quantized:\n+                        orig = orig.dequantize()\n+                    if ref.is_quantized:\n+                        ref = ref.dequantize()"
                }
        },
    "backend type":
        {
            "test":
                {
                    "deleted": "-                if self.args.ci and (\n-                    (\n-                        isinstance(e, RuntimeError)\n-                        and \"Internal Triton PTX codegen error\" in str(e)\n-                    or (isinstance(e, KeyError) and \"cubin\" in str(e))",
                    "added": "+from torch._dynamo.exc import BackendCompilerFailed\n+                if (\n+                    self.args.ci\n+                    and isinstance(e, BackendCompilerFailed)\n+                    and (\n+                        \"Internal Triton PTX codegen error\" in str(e)\n+                        or \"cubin\" in str(e)"
                }
        },
    "gradient status":
        {
            "test":
                {
                    "deleted": "-                        if output.is_cuda or 'cpu' in str(output.device):\n-                            convert_to_nested = True\n-                            output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())",
                    "added": "+                        if not torch.is_grad_enabled() or all([not x.requires_grad for x in tensor_args]):\n+                            if output.is_cuda or 'cpu' in str(output.device):\n+                                convert_to_nested = True\n+                                output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())"
                }
        },
        "device type":
        {
            "test":
                {
                    "deleted": "-    if device_type.lower() == \"cuda\":" ,
                    "added": "+    if device_type and device_type.lower() == \"cuda\":"
                }
        }
}