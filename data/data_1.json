[{
    "Id": 1,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/93cea394dee1315c7a85ead7bb7af21363157c4f",
    "Violation": "improper",
    "Bug report": "Currently we compare `CUDA_INCLUDE_DIRS` and expect exact equality with `CUDAToolkit_INCLUDE_DIR` however this fails in the presense of symbolic links or for split installs where there are multiple include paths.",
    "Number of deleted lines": 4,
    "Deleted lines": "-if(NOT CMAKE_CUDA_COMPILER_VERSION STREQUAL CUDAToolkit_VERSION OR\n-    NOT CUDA_INCLUDE_DIRS STREQUAL CUDAToolkit_INCLUDE_DIR)\n-  message(FATAL_ERROR \"Found two conflicting CUDA installs:\\n\"\n-                      \"V${CUDAToolkit_VERSION} in '${CUDAToolkit_INCLUDE_DIR}'\")",
    "Added lines": "+if(NOT CMAKE_CUDA_COMPILER_VERSION VERSION_EQUAL CUDAToolkit_VERSION)\n+  message(FATAL_ERROR \"Found two conflicting CUDA versions:\\n\"\n+                      \"V${CUDAToolkit_VERSION} in '${CUDAToolkit_INCLUDE_DIRS}'\")"
},
{
    "Id": 2,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6b4c686b9a33a1503a4a4133f9067dd31e0822f7",
    "Violation": "improper",
    "Bug report": "Forward fix a performance regression caused by #110510. When a model is run once, all those kernel pointers are initialized and removing the if-nullptr check will cause those loadKernel be unnecessarily executed again when we rerun the foward function.",
    "Number of deleted lines": 2,
    "Deleted lines": "-                f\"\"\"kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n-                f\"\"\"{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"",
    "Added lines": "+            self.writeline(f\"if (kernels.{name} == nullptr) {{\")\n+                f\"\"\"    kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n+            self.writeline(\"}\")\n+            self.writeline(f\"if ({name} == nullptr) {{\")\n+                f\"\"\"    {name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n+            self.writeline(\"}\")"
},
{
    "Id": 3,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/45296f87ec865a7a500a6fd98353035c040d0cb7",
    "Violation": "insufficient",
    "Bug report": "Prior to this change ROCm was not exiting check_cuda, causing an exception at packaging.version.parse(torch.version.cuda),",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if not torch.cuda.is_available():",
    "Added lines": "+    if not torch.cuda.is_available() or torch.version.hip is not None:"
},
{
    "Id": 4,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bede7d999523d02e636a8981c0dff233b67f1a62",
    "Violation": "improper",
    "Bug report": "It does not check if `kind` variable fits in array of pointer called `names`",
    "Number of deleted lines": 1,
    "Deleted lines": "-  AT_ASSERT(size_t(kind) < sizeof(names) / sizeof(AttributeKind));",
    "Added lines": "+  AT_ASSERT(size_t(kind) < sizeof(names) / sizeof(*names));"
},
{
    "Id": 5,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f6639359357452de8bfc691430396ded98ea399c",
    "Violation": "improper",
    "Bug report": " [MPS] Fix boundary checks in generateKernelOffsets.`TORCH_CHECK(i<UINT32_MAX)` is always false, it should be `TORCH_CHECK(iterShape[i] < UINT32_MAX)`",
    "Number of deleted lines": 3,
    "Deleted lines": "-    TORCH_CHECK(i <= UINT32_MAX);\n-    iterShapeData[i] = (uint32_t)(iterShape[i]);\n-      strides[i][offset] = iter.strides(offset)[i];",
    "Added lines": "+  TORCH_CHECK(iter.can_use_32bit_indexing(), \"Can't be indexed using 32-bit iterator\");\n+    iterShapeData[i] = static_cast<uint32_t>(iterShape[i]);\n+      strides[i][offset] = static_cast<uint32_t>(iter.strides(offset)[i]);"
},
{
    "Id": 6,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cf732053e4f6b93b0a93006613552cd97f415b80",
    "Violation": "missing",
    "Bug report": " Today if we're accessing out of bound embedding rows, it'll either go through or throw IMA. This is not ideal - adding bound checks. This will probably slow things down - need to benchmark it.",
    "Number of deleted lines": 4,
    "Deleted lines": "-    index_t padding_idx) {\n-    index_t padding_idx) {\n-            padding_idx);\n-            padding_idx);",
    "Added lines": "+    index_t padding_idx, int64_t numRows) {\n+        CUDA_KERNEL_ASSERT(input[emb] < numRows);\n+    index_t padding_idx, int64_t numRows) {\n+        CUDA_KERNEL_ASSERT(input[emb] < numRows);\n+            padding_idx, weight.size(0));\n+            padding_idx, weight.size(0));"
},
{
    "Id": 7,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/175ccfc4c8443bcc65c87d9c942272d3ebf16b0b",
    "Violation": "insufficient",
    "Bug report": "flatbuffer module fields are not initialized",
    "Number of deleted lines": 2,
    "Deleted lines": "-  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")\n-      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\")",
    "Added lines": "+      ivalues && module->object_types(),\n+      \"Parsing flatbuffer module: Corrupted ivalues/object_types field\");\n+  TORCH_CHECK(\n+      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\");"
},
{
    "Id": 8,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bde7b81f34925491fbcbb9e355697eb594e36923",
    "Violation": "improper",
    "Bug report": "Back out \"[PyTorch] Don't do extra numel() check in TensorImpl::data()",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (data == nullptr) {",
    "Added lines": "+    if (is_empty()) {"
},
{
    "Id": 9,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2e224d62b6afecc78d885d0a4e160354950f6424",
    "Violation": "missing",
    "Bug report": "We have environment variable USE_CUDNN with self-explanatory name. However cpp code is compiled based on cpp macro definition AT_CUDNN_ENABLED, even if USE_CUDNN is set to 0, cpp is compiled with cuDNN if cmake finds cuDNN in the system.",
    "Number of deleted lines": 2,
    "Deleted lines": "-  IF (NOT AT_CUDA_ENABLED OR NOT CUDNN_FOUND)\n-    MESSAGE(STATUS \"CuDNN not found. Compiling without CuDNN support\")",
    "Added lines": "+  IF (NOT USE_CUDNN)\n+    MESSAGE(STATUS \"USE_CUDNN is set to 0. Compiling without cuDNN support\")\n+    set(AT_CUDNN_ENABLED 0)\n+  ELSEIF (NOT CUDNN_FOUND)\n+    MESSAGE(WARNING \"CuDNN not found. Compiling without CuDNN support\")"
},
{
    "Id": 10,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/30e1c74dc19ae2b622b46ebcdb7972c42775ac80",
    "Violation": "improper",
    "Bug report": "Update cuda amp to also check xla device ",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if not torch.cuda.is_available() and self.device == 'cuda':",
    "Added lines": "+        if torch.cuda.amp.common.amp_definitely_not_available() and self.device == 'cuda':"
},
{
    "Id": 11,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5a63c452e638dad8e077887ad8d2c94ff0e23917",
    "Violation": "missing",
    "Bug report": "This is because there are some hard-to-detect edge cases that will throw exceptions with cudnn 8.0.5 on Nvidia A40 GPU.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      if (prop->minor == 6) {\n+        // Excludes sm_86 GPU devices from using persistent rnn.\n+        // This is because there are some edge cases that will throw exceptions with cudnn 8.0.5 on Nvidia A40 GPU.\n+        return false;\n+      }"
},
{
    "Id": 12,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/611080a118fff166c85f3200d860f3b059abac6f",
    "Violation": "improper",
    "Bug report": "uda 11.0.x doesn't support sm86.",
    "Number of deleted lines": 2,
    "Deleted lines": "-    set(CUDA_LIMIT_GPU_ARCHITECTURE \"8.6\")\n-if(CUDA_VERSION VERSION_GREATER \"11.0\")",
    "Added lines": "+    set(CUDA_LIMIT_GPU_ARCHITECTURE \"8.0\")\n+if(NOT CUDA_VERSION VERSION_LESS \"11.1\")\n+  set(CUDA_LIMIT_GPU_ARCHITECUTRE \"8.6\")"
},
{
    "Id": 13,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/563bbeb8905f4cea0bc5353dc12518c61113128e",
    "Violation": "insufficient",
    "Bug report": "undefined CUDA_VERSION warning",
    "Number of deleted lines": 1,
    "Deleted lines": "-#if CUDA_VERSION < 10000",
    "Added lines": "+#if defined(CUDA_VERSION) && (CUDA_VERSION < 10000)"
},
{
    "Id": 14,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cafd0f33042f5344a27ccde33b352eab676a0bdd",
    "Violation": "improper",
    "Bug report": "Fix array index checking in mobile interpreter. Stop using non-portable out-of-range indexing in mobile interpreter, also change code types indexing to use vector.at() to catch out-of-range bugs earlier.",
    "Number of deleted lines": 6,
    "Deleted lines": "-          listConstruct(stack, *code.types_[inst.X], inst.N);\n-          dictConstruct(stack, *code.types_[inst.X], inst.N);\n-          namedTupleConstruct(stack, code.types_[inst.X], inst.N);\n-          auto type = code.types_[inst.X]->expect<c10::ClassType>();\n-          at::ArrayRef<TypePtr> types(\n-              &(code.types_[inst.X]), &(code.types_[inst.X + inst.N]));",
    "Added lines": "+          listConstruct(stack, *code.types_.at(inst.X), inst.N);\n+          dictConstruct(stack, *code.types_.at(inst.X), inst.N);\n+          namedTupleConstruct(stack, code.types_.at(inst.X), inst.N);\n+          auto type = code.types_.at(inst.X)->expect<c10::ClassType>();\n+          at::ArrayRef<TypePtr> types(&code.types_.at(inst.X), inst.N);"
},
{
    "Id": 15,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8198474eb763c8d526ede3418211479c2f4cbd30",
    "Violation": "missing",
    "Bug report": "Previous to this PR, we only checked TorchScript nodes for scope compatibility, skipping their parent's scope reference check.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  while (!parent->isRoot()) {",
    "Added lines": "+  while (isCompatibleScope(parent)) {"
},
{
    "Id": 16,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/37dea0454dd310cfe443859f717862657df6b753",
    "Violation": "insufficient",
    "Bug report": "add checking for number of args checking observer in same graph",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if isinstance(node.args[0], Node):",
    "Added lines": "+    if len(node.args) > 0 and isinstance(node.args[0], Node):"
},
{
    "Id": 17,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bdbd3ed312e0fc81e75302239ea78b3445fe95e7",
    "Violation": "insufficient",
    "Bug report": "Although `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.",
    "Number of deleted lines": 1,
    "Deleted lines": "-                if self.has_backedge():",
    "Added lines": "+                if self.has_backedge() and self.should_compile_partial_graph():"
},
{
    "Id": 18,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7e9bf2ed860b8b60d252eead4cc457c3fe5f1667",
    "Violation": "insufficient",
    "Bug report": "Although `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.",
    "Number of deleted lines": 1,
    "Deleted lines": "-                if self.has_backedge():",
    "Added lines": "+                if self.has_backedge() and self.should_compile_partial_graph():"
},
{
    "Id": 19,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e9c1ccee2247a7746fde202067a7d47b72809968",
    "Violation": "improper",
    "Bug report": "Bug fix: allow std 0 in the meta definition of normal_. All other `normal` variants allow 0.  Looks like a mistake made while copying the check. ",
    "Number of deleted lines": 1,
    "Deleted lines": "-  TORCH_CHECK(std > 0.0, \"normal_ expects std > 0.0, but found std=\", std);  // TODO: dedupe",
    "Added lines": "+  TORCH_CHECK(std >= 0.0, \"normal_ expects std >= 0.0, but found std=\", std);  // TODO: dedupe"
},
{
    "Id": 20,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c99277e177cf16736262251c7e92ea5e9ba2c5c2",
    "Violation": "improper",
    "Bug report": " handle the case in acc_ops.sum when dim == 0, differentiating it from the case when dim is None. handle the case in acc_ops.sum when dim == 0, differentiating it from the case when dim is None",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if dim:",
    "Added lines": "+    if dim is not None:"
},
{
    "Id": 21,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6c98d904c09b69f1e7748cf3d80e2193df5fff63",
    "Violation": "improper",
    "Bug report": "handle the case of -0.0 on tanh quantization.  this fix makes fakelowp identical to hw - mask out the floating point number with 0x7fff so we are always dealing with positive numbers - dsp implementation is correct, ice-ref suffers from this same problem",
    "Number of deleted lines": 5,
    "Deleted lines": "-        float val = X_data[i];\n-        short shortAbsInput = _cvtss_sh(abs(val), 0);\n-        // Clamp the input in the range of\n-        //  (short)tanhLUTMinOffset to (short)(tanhLUTMaxOffset - 1)\n-        if (val < 0.0) {",
    "Added lines": "+        short val = _cvtss_sh(X_data[i], 0);\n+        unsigned short max16BitPositive = 0x7FFF;\n+        unsigned short input16Bit = (*(unsigned short*)& val);\n+        short shortAbsInput = input16Bit & max16BitPositive; // mask out negative bit\n+        if (input16Bit > max16BitPositive) {  // negative value"
},
{
    "Id": 22,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0c0c9e743e82b398435ed07719e998aa15ac1ce1",
    "Violation": "improper",
    "Bug report": "Fix dimensions check ",
    "Number of deleted lines": 1,
    "Deleted lines": "-    CHECK_LT(num_reduce_dims_, input.dims().size());",
    "Added lines": "+    CHECK_LE(num_reduce_dims_, input.dims().size());"
},
{
    "Id": 23,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4d0fbb0e6f578bea14f3f52b0a927bcc20f8b109",
    "Violation": "improper",
    "Bug report": "when adding a new axis to concatenate along, allow it to be the last axis. For example, concated 1D columns into a 2D matrix with axis=1, add_axis=1.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  CAFFE_ENFORCE_LT(axis_, input_zero.ndim(), \"Axis not in input ndim range.\");",
    "Added lines": "+  CAFFE_ENFORCE_LT(\n+      axis_,\n+      input_zero.ndim() + (add_axis_ ? 1 : 0),\n+      \"Axis not in input ndim range.\");"
},
{
    "Id": 24,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/85cbe0d8258ab06897e2f049e61f74d8aa935240",
    "Violation": "missing",
    "Bug report": "This diff is similar to D14163001. We need to handle the edge case when add_axis=1.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  const int canonical_axis = canonical_axis_index_(axis, in[0].dims_size());",
    "Added lines": "+  int adj_size = in[0].dims_size() + (add_axis ? 1 : 0);\n+  const int canonical_axis = canonical_axis_index_(axis, adj_size);\n+  CAFFE_ENFORCE_LT(canonical_axis, adj_size, \"Axis not in input ndim range.\");"
},
{
    "Id": 25,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4b45f08f8765549915417997c30ae8981f2ad125",
    "Violation": "missing",
    "Bug report": "The issue was related to not checking the dimensions of source vs destination tensors.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  } ",
    "Added lines": "+  }\n+  } else if ((source.dim() != self.dim()) && (source.dim() != 0 && self.dim() != 0)) {\n+    AT_INDEX_ERROR(\"index_copy_(): When source and destination are not scalars, their dimensionality must match. Source dimensionality (\",\n+                   source.dim(), \"), destination dimensionality (\", self.dim(), \")\");\n+"
},
{
    "Id": 26,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4f63f348aef3da8b4d53f61098f4e32bd916c221",
    "Violation": "insufficient",
    "Bug report": "The bounds check was too conservative by an extra one.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  int64_t new_stride = dim >= tensor.dim() - 1 ? 1 : sizes[dim] * strides[dim];",
    "Added lines": "+  int64_t new_stride = dim >= tensor.dim() ? 1 : sizes[dim] * strides[dim];"
},
{
    "Id": 27,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bf32ea80942ce720b105efcd517fd11182edeb08",
    "Violation": "insufficient",
    "Bug report": " Fix dimension check in 1D instance norm, allowing 2D tensors alongsid e 3D.",
    "Number of deleted lines": 2,
    "Deleted lines": "-        if input.dim() != 3:\n-            raise ValueError('expected 3D input (got {}D input)'",
    "Added lines": "+        if input.dim() != 2 and input.dim() != 3:\n+            raise ValueError('expected 2D or 3D input (got {}D input)'"
},
{
    "Id": 28,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a9deda5469a6ef73692a9dd796cc4eeba4436d6c",
    "Violation": "improper",
    "Bug report": "The at::native::_validate_sparse_coo_tensor_args only supports checking the indices on CUDA device and CPU device. To extend the function to support more device type.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (indices.is_cuda()) {",
    "Added lines": "+    if (!indices.is_cpu()) {"
},
{
    "Id": 29,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/71af538e31547e5b1bc814c9e00323a21905baf3",
    "Violation": "insufficient",
    "Bug report": "Updated assert to remove check on 3rd dim for MHA.  Updated assert statement to remove check on 3rd dimension (features) for keys and values in MultiheadAttention / Transform. The feature dimension for keys and values can now be of different sizes",
    "Number of deleted lines": 1,
    "Deleted lines": "-    assert key.size() == value.size()",
    "Added lines": "+    # allow MHA to have different sizes for the feature dimension\n+    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)"
},
{
    "Id": 30,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b8ab3080b1043a610ba2825a2be406a1833b1d70",
    "Violation": "improper",
    "Bug report": "If kernel sizes were specified via \"kernel_w\" and \"kernel_h\", tensor size inference was incorrect in InferShapesAndTypes(): it was checking for \"helper_w\" instead of \"kernel_w\".",
    "Number of deleted lines": 1,
    "Deleted lines": "-        helper.HasArgument(\"kernel_h\") && helper.HasArgument(\"helper_w\")) {",
    "Added lines": "+        helper.HasArgument(\"kernel_h\") && helper.HasArgument(\"kernel_w\")) {"
},
{
    "Id": 31,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/be253b8ee8a104997773d11ed28928a48193217d",
    "Violation": "improper",
    "Bug report": "The existing check isn't safe for 32-bit `size_t` because the max 64-bit int will overflow.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  assert(sizes.size() < static_cast<std::size_t>(std::numeric_limits<std::int64_t>::max()));",
    "Added lines": "+  assert(!overflows<std::int64_t>(sizes.size()));"
},
{
    "Id": 32,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7f125bca1cd42ebd8e07c97f1bd1682dff5cf387",
    "Violation": "insufficient",
    "Bug report": "Add pin_memory check in empty_strided. Add the false checking if pin_memory has been specified to `False`",
    "Number of deleted lines": 1,
    "Deleted lines": "-      !pin_memory.has_value(),",
    "Added lines": "+      !pin_memory.has_value() || !pin_memory.value(),"
},
{
    "Id": 33,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f02b7a9c36dd6182da694bc47a5c345285dfd951",
    "Violation": "improper",
    "Bug report": "don't error when unused fill value is zero. In the python version of `F.pad`, checking that the fill value was left as default was done by comparing against zero. So if someone does explicitly pass in a zero-value, then this `TORCH_CHECK` was an accidental BC-break.",
    "Number of deleted lines": 4,
    "Deleted lines": "-  TORCH_CHECK(\n-      !value.has_value(), \"Padding mode \\\"\",\n-      padding_mode_string(mode),\n-      \"\\\" doesn't take in value argument\");",
    "Added lines": "+  TORCH_CHECK(!value.has_value() || *value == 0,\n+              \"Padding mode \\\"\", padding_mode_string(mode),\n+              \"\\\" doesn't take in value argument\");"
},
{
    "Id": 34,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4839f73f329b38819e6f69a8662d61dc36558e52",
    "Violation": "improper",
    "Bug report": "Fix incorrect tensor storage check. These fixes were run through the DirectML test suite, and confirm the check is now working correctly.",
    "Number of deleted lines": 3,
    "Deleted lines": "-\n-                or (self.storage is None and self.device.type == \"privateuseone\")\n-            self.storage is None and self.device.type == \"privateuseone\"",
    "Added lines": "+                or (\n+                    not torch._C._has_storage(self)\n+                    and self.device.type == \"privateuseone\"\n+                )\n+            not torch._C._has_storage(self) and self.device.type == \"privateuseone\""
},
{
    "Id": 35,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/75be4f9cdb503d6eff189b2bc5c05d96bff66653",
    "Violation": "insufficient",
    "Bug report": " check tensor has storage before refer to tensor data ptr. In the exporter dedupe initializers passes, check the tensor has storage before reference to tensor's data_ptr, otherwise it will result in a crash.",
    "Number of deleted lines": 1,
    "Deleted lines": "-      (t1.data_ptr() == t2.data_ptr());",
    "Added lines": "+      (t1.has_storage() && t2.has_storage() && t1.data_ptr() == t2.data_ptr());"
},
{
    "Id": 36,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a3701b674046bcefb5927a6643364b186f77dbcf",
    "Violation": "unnecessary",
    "Bug report": "fix backward bug for custom device. In the backward on some device , it may get an error to get device index because of exchange a new thread. So just set_device and check the device index in `setDevice`  func may be better for some many kinds of devices. For CUDA, the device index check is also included in `setDevice`  func",
    "Number of deleted lines": 2,
    "Deleted lines": "-      if (impl && device < impl->deviceCount() &&\n-          impl->getDevice().index() != device) {",
    "Added lines": "+      if (impl && device < impl->deviceCount()) {"
},
{
    "Id": 37,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6f5945e4bb1258d39a2878a08a910fcc8f659d5e",
    "Violation": "improper",
    "Bug report": "triton supports devices < 7.0, not 6.0. triton is still buggy with Pascal devices, so make the error checker reflect that. Also, this < 6.0 never worked, as the `has_triton` definition in utils.py was checking >= 7.0.",
    "Number of deleted lines": 2,
    "Deleted lines": "-                if device_props.major < 6:\n-                        f\"Found {device_props.name} which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 6.0, but your device is of CUDA capability {device_props.major}.{device_props.minor}\"  # noqa: B950",
    "Added lines": "+                if device_props.major < 7:\n+                        f\"Found {device_props.name} which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability {device_props.major}.{device_props.minor}\"  # noqa: B950"
},
{
    "Id": 38,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/871b5caae76185cff141c522b3133e7543c8dabf",
    "Violation": "improper",
    "Bug report": "Fix hpu deserialization bug. fix hpu deserialization bug. It should check hpu model if and only if location start with hpu. Otherwise, it always raise an AssertError if hpu is not imported. This break the serialization/desirialization functionality abourt other third-party like IPEX. only assert hpu model when start with hpu",
    "Number of deleted lines": 2,
    "Deleted lines": "-    hpu = getattr(torch, \"hpu\", None)\n-    assert hpu is not None, \"HPU device module is not loaded\"",
    "Added lines": "+        hpu = getattr(torch, \"hpu\", None)\n+        assert hpu is not None, \"HPU device module is not loaded\""
},
{
    "Id": 39,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6592259ea52f45e1fc9a633ccb5b154ba5099334",
    "Violation": "insufficient",
    "Bug report": "As per torch.jit.load documentation, all previously saved modules, irrespective of their device, are first loaded onto CPU, and then are moved to the devices they were saved from. So far, supported devices included CPU and CUDA only. To enable torch.jit.load for HPU, additional check for HPU is introduced.",
    "Number of deleted lines": 2,
    "Deleted lines": "-      if (device.is_cuda() || device.is_xpu() || device.is_meta()) {\n-            \"supported devices include CPU and CUDA, however got \",",
    "Added lines": "+      if (device.is_cuda() || device.is_xpu() || device.is_meta() ||\n+          device.is_hpu()) {\n+            \"supported devices include CPU, CUDA and HPU, however got \","
},
{
    "Id": 40,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1becd2c314f45bded8d3fbec91d785e7190b4afe",
    "Violation": "insufficient",
    "Bug report": "Align checks in _use_cudnn_ctc_loss with those in _cudnn_ctc_loss.This PR is intended to fix the following problem: When using `CTCLoss`, there is a cudnn path gated by a call to [`_use_cudnn_ctc_loss`] which checks some conditions. However, there are more checks in `_cudnn_ctc_loss`.  some of which are not present in `_use_cudnn_ctc_loss` (e.g. the check that `targets` is on CPU which will cause a RuntimeError after dispatching to `_cudnn_ctc_loss`). Instead, these checks should be in `_use_cudnn_ctc_loss` so that the normal `_ctc_loss` path will be used if the checks are not met)",
    "Number of deleted lines": 1,
    "Deleted lines": "-      (log_probs.device().type() == at::kCUDA);",
    "Added lines": "+      (log_probs.device().type() == at::kCUDA) &&\n+      (targets.device().type() == at::kCPU) &&\n+      (targets.is_contiguous()) &&\n+      (log_probs.dim() == 3);"
},
{
    "Id": 41,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/490f2d75700a806bdc6110e881e78493cde163e3",
    "Violation": "insufficient",
    "Bug report": "Skip privateuse1's checkZeroPoints. We want to use ``quantize_per_channel`` to create a quantized tensor, but we found that ``checkZeroPoints`` for ``privateuse1`` backend failed. ``quantize_tensor_per_channel_affine`` will ``checkZeroPoints`` for all backends expect ``CUDA``:  However, our ``privateuse1`` backend will get a segmentation error if we try to cast our data to int64_t in ``checkZeroPoints``: So if we can skip ``privateuse1``'s ``checkZeroPoints`` and check this item in the actual device function? What do you think?",
    "Number of deleted lines": 4,
    "Deleted lines": "-    if(qtensor.device().type() != c10::DeviceType::CUDA){\n-    }  // for cuda, this check will occur in the actual cuda function\n-    if(qtensor.device().type() != c10::DeviceType::CUDA){\n-    }  // for cuda, this check will occur in the actual cuda function",
    "Added lines": "+    if (qtensor.device().type() != c10::DeviceType::CUDA &&\n+        qtensor.device().type() != c10::DeviceType::PrivateUse1) {\n+    }  // for cuda and privateuse1, this check will occur in the actual device function\n+    if(qtensor.device().type() != c10::DeviceType::CUDA &&\n+       qtensor.device().type() != c10::DeviceType::PrivateUse1){\n+    }  // for cuda and privateuse1, this check will occur in the actual device function"
},
{
    "Id": 42,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a076a74f1118da171cf70d00d1de4abbe27cf85d",
    "Violation": "insufficient",
    "Bug report": " Add xpu device in assertion for nested tensor creation",
    "Number of deleted lines": 2,
    "Deleted lines": "-      storage_device.is_cpu() || storage_device.is_cuda() || storage_device.is_privateuseone(),\n-      \"NestedTensorImpl storage must be either CUDA, CPU or \", get_privateuse1_backend(), \" but got \",",
    "Added lines": "+      storage_device.is_cpu() || storage_device.is_cuda() || storage_device.is_xpu() || storage_device.is_privateuseone(),\n+      \"NestedTensorImpl storage must be either CUDA, CPU, XPU or \", get_privateuse1_backend(), \" but got \","
},
{
    "Id": 43,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/097defb1608827d82b18b27adeec0a98b72a9281",
    "Violation": "insufficient",
    "Bug report": "only check when world size > num_devices per host",
    "Number of deleted lines": 1,
    "Deleted lines": "-            if world_size % num_devices_per_host != 0:",
    "Added lines": "+            if (\n+                world_size > num_devices_per_host\n+                and world_size % num_devices_per_host != 0\n+            ):"
},
{
    "Id": 44,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/57af1ec14594a73c8f2b73bf70c04ba7efeb6eab",
    "Violation": "improper",
    "Bug report": "observers: use torch.all to check for valid min and max values. Using `torch.all` instead of `torch.sum` and length check. It's unclear whether the increase in perf (~5% for small inputs) is real, but should be a net benefit, especially for larger channel inputs.",
    "Number of deleted lines": 1,
    "Deleted lines": "-            assert torch.sum(min_val <= max_val) == len(min_val), \"min {} should be less than max {}\".format(",
    "Added lines": "+            assert torch.all(min_val <= max_val), \"min {} should be less than max {}\".format("
},
{
    "Id": 45,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/07f0f383fa23e63eca164036ab58ab983e9437eb",
    "Violation": "unnecessary",
    "Bug report": "update tensor-like to check instance for torch function impl. tensor like should check the instance for a torch function impl, not the type",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return type(inp) is torch.Tensor or hasattr(type(inp), \"__torch_function__\")",
    "Added lines": "+    return type(inp) is torch.Tensor or hasattr(inp, \"__torch_function__\")"
},
{
    "Id": 46,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b90db4a78f8d760377a81a5a64d03ab4b67599de",
    "Violation": "improper",
    "Bug report": "Fix type checking to accept both Iter and Map DataPipe",
    "Number of deleted lines": 1,
    "Deleted lines": "-    assert isinstance(datapipe, IterDataPipe)",
    "Added lines": "+    assert isinstance(datapipe, (IterDataPipe, MapDataPipe))"
},
{
    "Id": 47,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/63cbdc92a750a667ffdcfbdac563d02db6fd9559",
    "Violation": "improper",
    "Bug report": "switching the exact check to isinstance check. Simplifying a type check if an object is a SymIntNode in `is_symint_node`",
    "Number of deleted lines": 2,
    "Deleted lines": "-  // TODO: switch this to `isinstance`\n-  if (obj.get_type().equal(tp_symn)) {",
    "Added lines": "+  if (py::isinstance(obj, tp_symn)) {"
},
{
    "Id": 48,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b7d58745c882a66f5c20d2ba05b99ddce7491d38",
    "Violation": "inefficient",
    "Bug report": "remove unneeded isinstance checks in `op_con. vert_before_hook. `isinstance` has some overhead, changing the code in `op_convert_before_hook` to use the information calculate during tracing instead which is cheaper.",
    "Number of deleted lines": 4,
    "Deleted lines": "-        if isinstance(args[0], (tuple, list)):  # torch.cat variants\n-                if not isinstance(arg, torch.Tensor):\n-                    new_args.append(arg)\n-                    continue",
    "Added lines": "+        if orig_op is torch.cat:  # torch.cat variants"
},
{
    "Id": 49,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6420071b43dc9f2679c22952b5051b0c28f42da2",
    "Violation": "improper",
    "Bug report": "Disable complex dispatch on min/max functions. In issue #36377, min/max functions were disabled for complex inputs (via dtype checks). However, min/max kernels are still being compiled and dispatched for complex. The aforementioned dispatch has been disabled & we now rely on errors produced by dispatch macro to not run those ops on complex, instead of doing redundant dtype checks.",
    "Number of deleted lines": 3,
    "Deleted lines": "-    AT_DISPATCH_ALL_TYPES_AND_COMPLEX(input.scalar_type(), \"min_all\", [&] {\n-    AT_DISPATCH_ALL_TYPES_AND_COMPLEX(input.scalar_type(), \"max_all\", [&] {\n-    AT_DISPATCH_ALL_TYPES_AND_COMPLEX(input.scalar_type(), \"_aminmax_all_all\", [&] {",
    "Added lines": "+    AT_DISPATCH_ALL_TYPES(input.scalar_type(), \"min_all\", [&] {\n+    AT_DISPATCH_ALL_TYPES(input.scalar_type(), \"max_all\", [&] {\n+    AT_DISPATCH_ALL_TYPES(input.scalar_type(), \"_aminmax_all_all\", [&] {"
},
{
    "Id": 50,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/92ebb04f9206882e6d312a8b91318545f43a53c2",
    "Violation": "insufficient",
    "Bug report": "added check for NumberType",
    "Number of deleted lines": 3,
    "Deleted lines": "-    // Add implicit conversion of int/float/bool types to tensors\n-    if (kind == c10::TypeKind::IntType || kind == c10::TypeKind::BoolType ||\n-        kind == c10::TypeKind::FloatType) {",
    "Added lines": "+    // Add implicit conversion of int/float/bool/number types to tensors\n+    if (kind == c10::TypeKind::NumberType || kind == c10::TypeKind::IntType ||\n+        kind == c10::TypeKind::BoolType || kind == c10::TypeKind::FloatType) {"
},
{
    "Id": 51,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/40d6f2a02027023216607adb892d3b9c7493904c",
    "Violation": "insufficient",
    "Bug report": "Update sdp_utils to check gradmode and subclassed tensors. Fix up the grad check test to check for subclassed tensors and gradmode",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (params.query.requires_grad() || params.key.requires_grad() || params.value.requires_grad()) {",
    "Added lines": "+#include <ATen/TensorSubclassLikeUtils.h>\n+  bool any_tensors_are_subclass =\n+      at::areAnyTensorSubclassLike({params.query, params.key, params.value});\n+  const bool any_inputs_require_grad = params.query.requires_grad() ||\n+      params.key.requires_grad() || params.value.requires_grad();\n+  const bool gradmode_enabled = at::GradMode::is_enabled();\n+  if ((any_inputs_require_grad && gradmode_enabled) || any_tensors_are_subclass) {"
},
{
    "Id": 52,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fe6aa0844466e5dd2669092eac5edde153108b28",
    "Violation": "improper",
    "Bug report": "Don't delegate to `operator=` for construction. Catch hypothetical addition of a new Scalar type via debug assertion rather than checking in prod.",
    "Number of deleted lines": 5,
    "Deleted lines": "-      *this = s.toDouble();\n-      *this = s.toBool();\n-    } else if (s.isIntegral(false)) {\n-      *this = s.toLong();\n-      TORCH_CHECK(false, \"Unknown type in Scalar\");",
    "Added lines": "+      tag = Tag::Double;\n+      payload.u.as_double = s.toDouble();\n+      tag = Tag::Bool;\n+      payload.u.as_bool = s.toBool();\n+      TORCH_INTERNAL_ASSERT_DEBUG_ONLY(s.isIntegral(false), \"Unknown type in Scalar\");\n+      tag  = Tag::Int;\n+      payload.u.as_int = s.toLong();"
},
{
    "Id": 53,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1c5a8125798392f8d7c57e88735f43a14ae0beca",
    "Violation": "improper",
    "Bug report": "Better type checking in disable_torch_function/dispatch ",
    "Number of deleted lines": 4,
    "Deleted lines": "-  } else if (PyList_CheckExact(args)) {\n-  } else {\n-  } else if (PyList_CheckExact(args)) {\n-  } else {",
    "Added lines": "+  } else if (PyList_Check(args)) {\n+  } else if (PyTuple_Check(args)) {\n+  } else {\n+    throw torch::TypeError(\"expected List or Tuple (got %s)\", Py_TYPE(args)->tp_name);\n+  } else if (PyList_Check(args)) {\n+  } else if (PyTuple_Check(args)) {\n+  } else {\n+    throw torch::TypeError(\"expected List or Tuple (got %s)\", Py_TYPE(args)->tp_name);"
},
{
    "Id": 54,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0f0829d88e839be1e150e917aca5b1edb64752ee",
    "Violation": "missing",
    "Bug report": " Strict bound check for SequenceFunctor. Summary: This exhibits the problem in NMT training where some out of bound data seems to have silently written over bound, and causing random segfaults elsewhere in the code. This itself does not solve the problem, but will trigger us to then fix the out of bound issues.",
    "Number of deleted lines": 4,
    "Deleted lines": "-  explicit SequenceFunctor(const int* sl) : sl(sl) {}\n-    return j >= sl[i];\n-  const int* sl;\n-        SequenceFunctor(sequence_lengths->data<int>()),",
    "Added lines": "+  explicit SequenceFunctor(const int* sl, const size_t len) : sl_(sl), len_(len) {}\n+    CAFFE_ENFORCE(i < len_, \"Out of bound.\");\n+    return j >= sl_[i];\n+  const int* sl_;\n+  const size_t len_;\n+        SequenceFunctor(sequence_lengths->data<int>(), sequence_lengths->size()),"
},
{
    "Id": 55,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a7cc6531399300f999a404718827e2a94c115aaf",
    "Violation": "insufficient",
    "Bug report": "Summary:GCC version check is currently being skipped when using the newly released CUDA 9.1. This will also handle other CUDA 9.x minor releases if any, reducing our work if there are such releases like 9.2. This assumes that the next major CUDA version will be 10.0, needing adjustment only after such major version is released.",
    "Number of deleted lines": 3,
    "Deleted lines": "-    # CUDA 9.0 requires GCC version <= 6\n-    if (CUDA_VERSION VERSION_EQUAL 9.0)\n-          \"CUDA 9.0 is not compatible with GCC version >= 7. \"",
    "Added lines": "+    # CUDA 9.x requires GCC version <= 6\n+    if ((CUDA_VERSION VERSION_EQUAL   9.0) OR\n+        (CUDA_VERSION VERSION_GREATER 9.0  AND CUDA_VERSION VERSION_LESS 10.0))\n+          \"CUDA ${CUDA_VERSION} is not compatible with GCC version >= 7. \""
},
{
    "Id": 56,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/218f4506fdcde69e3f8f2f2b2b51fefd996c577b",
    "Violation": "insufficient",
    "Bug report": "Summary:this PR modifies the gcc compiler check for CUDA slightly. All ABI are compatible with eachother. According to the documentation, `CUDA_HOST_COMPILER` is set to `CMAKE_C_COMPILER` by default. This PR checks if `CMAKE_C_COMPILER` is too new for CUDA 8 and whether `CUDA_HOST_COMPILER` is set to `CMAKE_C_COMPILER`. It also modifies the message slightly.",
    "Number of deleted lines": 6,
    "Deleted lines": "-    if (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND\n-        NOT CMAKE_CXX_COMPILER_VERSION VERSION_LESS 6.0)\n-        \"Use the following options to use another version (for example): \\n\"\n-        \"  -DCMAKE_CXX_COMPILER=/usr/bin/g++-5\\n\"\n-        \"  -DCMAKE_C_COMPILER=/usr/bin/gcc-5\\n\"\n-        \"  -DCUDA_HOST_COMPILER:FILEPATH=/usr/bin/gcc-5\\n\")",
    "Added lines": "+    if (CMAKE_C_COMPILER_ID STREQUAL \"GNU\" AND\n+        NOT CMAKE_C_COMPILER_VERSION VERSION_LESS 6.0 AND\n+        CUDA_HOST_COMPILER STREQUAL CMAKE_C_COMPILER)\n+        \"Use the following option to use another version (for example): \\n\"\n+        \"  -DCUDA_HOST_COMPILER=/usr/bin/gcc-5\\n\")"
},
{
    "Id": 57,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3f5dc95b57496c4ea938be381efcdc2ea92bb4cc",
    "Violation": "insufficient",
    "Bug report": "Some of the tests don't specify `device` in the input configs so filter by device won't work for them. This diff fixes that issue.",
    "Number of deleted lines": 1,
    "Deleted lines": "-                (self.args.device == 'None' or self.args.device in op_test_config.test_name)):",
    "Added lines": "+                (self.args.device == 'None' or 'device' not in op_test_config.test_name or \n+                    self.args.device in op_test_config.test_name)):"
},
{
    "Id": 58,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1c02be1b6a0f6d02d3a0ae19c13d51a3e59a55ae",
    "Violation": "insufficient",
    "Bug report": "In PyTorch 1.5, when running `torch.cuda.reset_peak_memory_stats()` on a machine where `torch.cuda.is_available() is False`, I would get: AssertionError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from. With this patch, we get a more sensible:",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if device_type.lower() == \"cuda\":",
    "Added lines": "+    if device_type and device_type.lower() == \"cuda\":"
},
{
    "Id": 59,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/faa7eb81c634492b70fcc0327622bb0aa812cacd",
    "Violation": "misleading",
    "Bug report": "change error_message for XPU Autocast data type check. XPU autocast supports bf16 and fp16 data types, we are going to change the error_message for that.",
    "Number of deleted lines": 1,
    "Deleted lines": "-                error_message += 'XPU Autocast only supports dtype of torch.bfloat16 currently.'",
    "Added lines": "+                error_message += 'XPU Autocast only supports dtypes of torch.bfloat16 and torch.float16 currently.'"
},
{
    "Id": 60,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/48a49b2683ffa21eb1b472e503c129c043c18f87",
    "Violation": "misleading",
    "Bug report": "use more informative error message for ConstandPad2d/3d.  the current error message for `torch.nn.ConstantPad2d` and `torch.nn.ConstantPad3d` is misleading, this PR fixes the problem.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  TORCH_CHECK(static_cast<int64_t>(pad.size()) <= input_dim * 2, \"Padding length too large\");",
    "Added lines": "+  TORCH_CHECK(static_cast<int64_t>(pad.size()) <= input_dim * 2,\n+              \"Padding length should be less than or equal to two times the input dimension but got padding length \", pad.size(), \" and input of dimension \", input_dim);"
},
{
    "Id": 61,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4ab1588d9919bc1a62219a5c2393e0784ddaae70",
    "Violation": "misleading",
    "Bug report": "Enhance error message for dependency check, If python development library is missing when building pytorch from source cmake will raise the error like: CMake Error at cmake/Dependencies.cmake:1079 (if): if given arguments: \"VERSION_LESS\" \"3\"  Unknown arguments specified ```it's quite a misleading information that user would consider it's a syntax error or cmake version problem. This PR add a check to ensure `PYTHONLIBS_VERSION_STRING` exist before using.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if(NOT PYTHONLIBS_VERSION_STRING)\n+    message(FATAL_ERROR\n+      \"Python development libraries could not be found.\")\n+  endif()\n+"
},
{
    "Id": 62,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/577e90ae9bf257040acb68da3626d9a64d07bf7a",
    "Violation": "misleading",
    "Bug report": " Improve error message for missing ops. The current error message is ill formed",
    "Number of deleted lines": 2,
    "Deleted lines": "-        \"Following ops cannot be found. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/\",\n-        c10::Join(\", \", unsupported_op_names));",
    "Added lines": "+        \"Following ops cannot be found: [\",\n+        c10::Join(\", \", unsupported_op_names),\n+        \"]. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/c/mobile/\");"
},
{
    "Id": 63,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/22044c6f7cbdafdd340714bbe220b621e1927826",
    "Violation": "misleading",
    "Bug report": "Use TORCH_CHECK instead of AT_ASSERT in torch::cuda::gather(). The error message produced by AT_ASSERT() in gather() encouraged users to file a bug report (\"please report a bug to PyTorch...\"). The assertion should be a regular argument check since it can be triggered by passing tensors with different dimensionality, e.g. `torch.cuda.comm.gather([torch.rand(1, device='cuda'), torch.rand(1, 1, device='cuda')])`.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    AT_ASSERT(tensor.ndimension() == static_cast<int64_t>(expected_size.size()));",
    "Added lines": "+    TORCH_CHECK(\n+        tensor.ndimension() == static_cast<int64_t>(expected_size.size()),\n+        \"Gather input tensors must have the same number of dimensions: got \",\n+        tensor.ndimension(), \", but expected \", expected_size.size());"
},
{
    "Id": 64,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc0d68a1ee3800ed4024762d018f85256e80f5ad",
    "Violation": "misleading",
    "Bug report": "Print out interface mismatch for prim::ModuleDictIndex. This commit augments the module interface subtyping check that is done before the emission of the `prim::ModuleDictIndex` operator so that the error message that is printed if the subtyping check fails provides more information on which methods do not match.",
    "Number of deleted lines": 2,
    "Deleted lines": "-          if (!attr_type->isSubtypeOf(type_hint)) {\n-                << \" is not of annotated type \" << type_hint->annotation_str();",
    "Added lines": "+          std::stringstream ss;\n+          if (!attr_type->isSubtypeOfExt(type_hint, &ss)) {\n+                << \" is not of annotated type \" << type_hint->annotation_str()\n+                << \": \" << ss.str();"
},
{
    "Id": 65,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/871e240e6367f94966a3e2f9deefbfa98e314d6d",
    "Violation": "misleading",
    "Bug report": "Improved error message for interpolation. Improved error message for CUDA interpolation with antialiasing",
    "Number of deleted lines": 2,
    "Deleted lines": "-            \"Too much shared memory required: \", shmem_size, \" vs \", sharedMemPerBlock);\n-            \"Too much shared memory required: \", shmem_size, \" vs \", sharedMemPerBlock);",
    "Added lines": "+            \"Provided interpolation parameters can not be handled with current algorithm implementation. \",\n+            \"Please reduce the scale factor. Too much shared memory required: \",\n+            shmem_size, \" vs \", sharedMemPerBlock);\n+            \"Provided interpolation parameters can not be handled with current algorithm implementation. \",\n+            \"Please reduce the scale factor. Too much shared memory required: \",\n+            shmem_size, \" vs \", sharedMemPerBlock);"
},
{
    "Id": 66,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c9548176965557a76526ba0db23ff5c9facd3e97",
    "Violation": "misleading",
    "Bug report": "print matrix dims in torch cuda matrix multiply error.  trying to improve the error message for torch matrix multiply dimension mismatch",
    "Number of deleted lines": 1,
    "Deleted lines": "-  TORCH_CHECK(mat1_sizes[1] == mat2_sizes[0], \"mat1 dim 1 must match mat2 dim 0\");",
    "Added lines": "+  TORCH_CHECK(\n+      mat1_sizes[1] == mat2_sizes[0],\n+      \"mat1 dim 1 must match mat2 dim 0\",\n+      \" mat1 dim1:\",\n+      mat1_sizes[1],\n+      \" mat2 dim0: \",\n+      mat2_sizes[0]);"
},
{
    "Id": 67,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/93256617c8622760181dacf03c41cc0577ac0ea6",
    "Violation": "misleading",
    "Bug report": "corrected messages for check of default options. Modified messages in the check of default options for the Adam optimizer.",
    "Number of deleted lines": 3,
    "Deleted lines": "-     TORCH_CHECK(std::get<0>(betas) >= 0, \"Invalid learning rate: \", std::get<0>(betas));\n-     TORCH_CHECK(std::get<1>(betas) >= 0, \"Invalid learning rate: \", std::get<1>(betas));\n-     TORCH_CHECK(defaults.weight_decay() >= 0, \"Invalid learning rate: \", defaults.weight_decay());",
    "Added lines": "+     TORCH_CHECK(0 <= std::get<0>(betas) && std::get<0>(betas) < 1.0, \"Invalid beta parameter at index 0: \", std::get<0>(betas));\n+     TORCH_CHECK(0 <= std::get<1>(betas) && std::get<1>(betas) < 1.0, \"Invalid beta parameter at index 1: \", std::get<1>(betas));\n+     TORCH_CHECK(defaults.weight_decay() >= 0, \"Invalid weight_decay value: \", defaults.weight_decay());"
},
{
    "Id": 68,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9a9eadacc6ac3b734a6d607ae6f63ec1a0d1438d",
    "Violation": "missing",
    "Bug report": "explicitly check device for grid_sampler",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        if input.device != grid.device:\n+            raise RuntimeError((\"input (device {}) and grid (device {}) must be on the same device\" +\n+                                \"for grid_sampler\").format(input.device, grid.device))"
},
{
    "Id": 69,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/157d478a30f27fd9d866c1235841721a559c8d0b",
    "Violation": "improper",
    "Bug report": "Fix omission of shape in size check in index. ",
    "Number of deleted lines": 1,
    "Deleted lines": "-                        index[j] <= self.shape[k + j],",
    "Added lines": "+                        index.shape[j] == self.shape[k + j],"
},
{
    "Id": 70,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f14887a63f83b931df9fc5d841c7d3829141ff58",
    "Violation": "missing",
    "Bug report": "check for exact shape match before loading. Use RuntimeError instead of ValueError to keep it consistent with other errors",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+                if input_param.shape != param.shape:\n+                    # local shape should match the one in checkpoint\n+                    error_msgs.append('Size mismatch: copying a param of {} from checkpoint, '\n+                                      'where the shape is {} in current model.'\n+                                      .format(param.shape, input_param.shape))\n+"
},
{
    "Id": 71,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/02e2158e754bafda46e663052c838aeb6ab6b560",
    "Violation": "missing",
    "Bug report": " Fix for out of bounds read in mobile interpreter INTERFACE_CALL opcode handler. The INTERFACE_CALL opcode for the mobile TorchScript interpreter contained an out of bounds read issue leading to memory corruption. This change adds an explicit check that the number of inputs passed to the format method called when handling the INTERFACE_CALL opcode is a valid and within bounds of the stack.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+          if (inst.N == 0 || inst.N > stack.size()) {\n+            TORCH_CHECK(\n+                false,\n+                \"INTERFACE_CALL N=\",\n+                inst.N,\n+                \" not in range [1, \",\n+                stack.size(),\n+                \"]\");\n+          }"
},
{
    "Id": 72,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4d07428edee863e7f5920f0672957a9711a9f0b5",
    "Violation": "missing",
    "Bug report": "Fix for out of bounds read in mobile interpreter FORMAT opcode handler. Summary: The FORMAT opcode for the mobile TorchScript interpreter contained an out of bounds read issue leading to memory corruption. This change adds an explicit check that the number of inputs passed to the format method called when handling the FORMAT opcode is a valid and within bounds of the stack.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (num_inputs == 0 || num_inputs > stack.size()) {\n+    AT_ERROR(\"Invalid number of inputs for format string: \", num_inputs);\n+  }\n+"
},
{
    "Id": 73,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b3ace213f240dc0f0f2a738f825f46e0d0dffca4",
    "Violation": "missing",
    "Bug report": "The error occurs because there is not check in `deserialize_source` that `text_table_` size can be less than `fnameIndex`. To prevent the error the corresponding check must be located.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    TORCH_CHECK(\n+        (uint64_t)fnameIndex < text_table_.size(),\n+        \"Text table index is out of range\")"
},
{
    "Id": 74,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d8466964b348b6172317f70b8e52de02402bad54",
    "Violation": "missing",
    "Bug report": "Add range check to multi margin loss target ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  CUDA_KERNEL_ASSERT(target_k >= 0 && target_k < dim && \"target index is out of bounds\");"
},
{
    "Id": 75,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/43f810fa96a0d2c40387c8c84f710926d9ede3c1",
    "Violation": "insufficient",
    "Bug report": "Add streams boundary check to torch::cuda::scatter`. Summary: Accessing elements of `std::vector` outside of its boundaries can lead to crashes/memory corruptions",
    "Number of deleted lines": 2,
    "Deleted lines": "-    if (streams && (*streams)[i]) {\n-      if (streams && (*streams)[i]) {",
    "Added lines": "+    if (i < (streams ? streams->size() : 0U) && (*streams)[i]) {\n+      if (i < (streams ? streams->size() : 0U) && (*streams)[i]) {"
},
{
    "Id": 76,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/13121598efc7d86cb7ae6e05322bb95c1d0f16bc",
    "Violation": "missing",
    "Bug report": "Bug fix to update requantization and zp parameters of input. Also sneaking in change to check for realloc failure for packed activation buffer. In dynamic quantization input's quantization scale and zero point can be different on every iterations. Thus requantization scale needs to be recomputed. Earlier bug that calculated those only at op creation time results in wrong results on subsequent runs.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      if (op->prepacked_a == NULL) {\n+        pytorch_qnnp_log_error(\n+            \"failed to allocate %zu bytes for packed activation buffer\",\n+            (k_stride * m_stride));\n+        return pytorch_qnnp_status_out_of_memory;\n+      }"
},
{
    "Id": 77,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e8e29690ef0306da25b5f191623476001d29a18b",
    "Violation": "missing",
    "Bug report": " Add has_debug_def() check to net's debug_def() ",
    "Number of deleted lines": 2,
    "Deleted lines": "-  inline const std::shared_ptr<const NetDef> debug_def() const {\n-    return net_def_;",
    "Added lines": "+  inline const NetDef& debug_def() const {\n+    CAFFE_ENFORCE(has_debug_def(), \"net_def was null!\");\n+    return *net_def_;\n+  }\n+\n+  inline bool has_debug_def() const {\n+    return net_def_ != nullptr;"
},
{
    "Id": 78,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/eed22921237eb4c1f4399af177ce912147a885c3",
    "Violation": "missing",
    "Bug report": " check for null commonworld in DestroyCommonWorld. Summary: Check for nullptr before closing a common world.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (OperatorBase::InputBlob(0).GetRaw() == nullptr) {\n+      return true;\n+    }"
},
{
    "Id": 79,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d471eaeb1d2fbc7efcde6408d7d1e513b969af25",
    "Violation": "missing",
    "Bug report": "fix inline_container.cc inplace loading",
    "Number of deleted lines": 1,
    "Deleted lines": "-",
    "Added lines": "+  std::vector<uint8_t> buffer;\n+  if (buf == nullptr) {\n+    buffer.resize(chunk_size);\n+    buf = buffer.data();\n+  }"
},
{
    "Id": 80,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a5ca445f7953711bc90c111c3cad2ec87f02e74a",
    "Violation": "missing",
    "Bug report": "Check for corrupted ivalues. The error occurs because the `ivalues` field of flatbuffer module can be null, so the corresponding check must be inserted.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")"
},
{
    "Id": 81,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6cc0f1c20c2f87a6c7b0e4abd5419e5007920999",
    "Violation": "missing",
    "Bug report": "Checking for nullptr in get_model_bytecode_version . One-liner commit to check that the ptr is not null. Just had `test_jit` that had a segfault there.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TORCH_CHECK(data != nullptr, \"Pointer to bytes is null.\");"
},
{
    "Id": 82,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/61b9d8fccd3361f21e1f3548c2a9538b62cc7525",
    "Violation": "improper",
    "Bug report": "It is apparently undefined behavior to do pointer arithmetic on nullptr. In the case of AppendOnlyList, `next_` will only be null if `end_` is also null and thus the `memcpy` path will only be triggered if `n == 0`. Nonetheless, it is U to `memcpy(0, 0, 0)`. The extra null check is in a `C10_LIKELY` block so the extra cost should be negligible, and indeed after dusting off the component microbenchmarks there's no observable difference.",
    "Number of deleted lines": 2,
    "Deleted lines": "-    int n = src.size();\n-    if (C10_LIKELY(next_ + n <= end_)) {",
    "Added lines": "+    size_t n = src.size();\n+    if (C10_LIKELY(next_ && (next_ + n <= end_))) {"
},
{
    "Id": 83,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e24dee00d40d01bd83b7a08fbcf9cdd51a05b04b",
    "Violation": "missing",
    "Bug report": "add kernel launch checks after each kernel launch to silence the check ",
    "Number of deleted lines": 1,
    "Deleted lines": "-    C10_CUDA_KERNEL_LAUNCH_CHECK();",
    "Added lines": "+          C10_CUDA_KERNEL_LAUNCH_CHECK();\n+          C10_CUDA_KERNEL_LAUNCH_CHECK();"
},
{
    "Id": 84,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c06dfd7c26102ac2436ca25609c92fa794e972ca",
    "Violation": "missing",
    "Bug report": "Check input device in TRTModule. Add a check to ensure all the inputs are on cuda device.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            assert inputs[i].is_cuda, f\"{i}th input is not on cuda device.\""
},
{
    "Id": 85,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d3de37609f2f052a7efb098ab69540458ebaaa6c",
    "Violation": "insufficient",
    "Bug report": "Support fused_dropout with XPU backend. ## Motivation Enable the fused dropout optimization on XPU devices. ## Solution Add XPU device in the fused dropout acceptable checking.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return input.is_cuda() && p > 0 && p < 1 && input.numel() > 0;",
    "Added lines": "+  return (input.is_cuda() || input.is_xpu()) && p > 0 && p < 1 && input.numel() > 0;"
},
{
    "Id": 86,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7bf195f3608e0f28c30ffb6e2fecd74a1d4ee50a",
    "Violation": "missing",
    "Bug report": "fix kernel launch check in cross kernel",
    "Number of deleted lines": 1,
    "Deleted lines": "-  C10_CUDA_KERNEL_LAUNCH_CHECK();",
    "Added lines": "+      C10_CUDA_KERNEL_LAUNCH_CHECK();\n+      C10_CUDA_KERNEL_LAUNCH_CHECK();"
},
{
    "Id": 87,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/15dbc566c57eedbd0245e786912e94586eba0fd2",
    "Violation": "missing",
    "Bug report": "Add missing cuda kernel launch check",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+          C10_CUDA_KERNEL_LAUNCH_CHECK();"
},
{
    "Id": 88,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/232fbd90ff6d93362120d955befeeb297179ddad",
    "Violation": "missing",
    "Bug report": "For aten.convolution CPU path, the bias always can be fused, so this PR adds a device check: if inputs' device is CPU, we will fuse it for a good performance.",
    "Number of deleted lines": 2,
    "Deleted lines": "-            None,  # bias handled below\n-    if bias is not None:",
    "Added lines": "+    is_cpu = all(\n+        input.get_device().type == \"cpu\"\n+        for input in (x, weight, bias)\n+        if input is not None\n+    )\n+            bias if is_cpu else None,  # For cpu path, bias can always be fused\n+    if not is_cpu and bias is not None:"
},
{
    "Id": 89,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a8653f35de02c7fb038e3c184dda6e67a12a39e2",
    "Violation": "missing",
    "Bug report": "Perf win by check which device tensors are on",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (self.device() != value.device()){\n+    return fill_out(self, value.item());\n+  }"
},
{
    "Id": 90,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8b37821813b60a3ce2ae92e7a06057183578a450",
    "Violation": "insufficient",
    "Bug report": "if we want to use dp on other device ranther than \"cuda\", this balance  check will raise error, so I make the balance check only effective for `cuda`",
    "Number of deleted lines": 1,
    "Deleted lines": "-        _check_balance(self.device_ids)",
    "Added lines": "+        if device_type == \"cuda\":\n+            _check_balance(self.device_ids)"
},
{
    "Id": 91,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c1e51fcbfc70c089276530ee64fb626e3f7f4f2b",
    "Violation": "missing",
    "Bug report": "Relax tolerance for cuda accuracy check",
    "Number of deleted lines": 1,
    "Deleted lines": "-            # Workaround for ONNX for non-tensor outputs",
    "Added lines": "+                # Workaround for ONNX for non-tensor outputs\n+                # Relax tolerance for ONNX cuda\n+                if current_device == \"cuda\":\n+                    tolerance = 1e-2\n+"
},
{
    "Id": 92,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e856a4d66bead8997a83f8714547c09fcbcdc263",
    "Violation": "missing",
    "Bug report": " Add an env var to skip cudnn version compatibility check. skip the check by setting `PYTORCH_SKIP_CUDNN_COMPATIBILITY_CHECK=1`",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                if os.environ.get('PYTORCH_SKIP_CUDNN_COMPATIBILITY_CHECK', '0') == '1':\n+                    return True"
},
{
    "Id": 93,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cf256ee268c30d4ca965b38b45467cf7f738542f",
    "Violation": "missing",
    "Bug report": "Added tensor op check for cudnn rnns",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+from torch.version import cuda\n+CUDNN_DEFAULT_MATH = 0\n+CUDNN_TENSOR_OP_MATH = 1\n+\n+        if version() >= 7000 and int(cuda[0]) >= 9:\n+            lib.cudnnSetRNNMatrixMathType(self, CUDNN_DEFAULT_MATH)\n+            if datatype == CUDNN_DATA_HALF:\n+                lib.cudnnSetRNNMatrixMathType(self, CUDNN_TENSOR_OP_MATH)"
},
{
    "Id": 94,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0fc110cdd19363f2eb5de68b6eeb82dadc933be0",
    "Violation": "missing",
    "Bug report": "The bug in libcuda.so that required is fixed for libcuda.so versions >= 11.4.This PR changes replay() to sync after each launch only if the process's in-use libcuda.so is < 11.4. With all the \"enhanced\" and \"forward\" compatibility promises flying around, and the fact that \"driver\" sometimes means kernel-mode driver and sometimes means user-mode driver (libcuda.so), I wasn't sure if this PR's check suffices to trigger the sync iff the in-use libcuda.so is 11.4, but Cuda people say what I wrote is reasonable.",
    "Number of deleted lines": 5,
    "Deleted lines": "-  // Temporary workaround for bug in libcuda.so that causes replayed graphs\n-  // with certain topologies to be corrupted (kernels elided, internal syncs\n-  // ignored) when replayed back to back without a sync in between.\n-  // I hate to use a hard sync, but it's the only surefire workaround at the moment.\n-  cudaDeviceSynchronize();",
    "Added lines": "+  int version;\n+  AT_CUDA_CHECK(cudaDriverGetVersion(&version));\n+  if (version < 11040) {\n+    // Workaround for bug in libcuda.so that causes replayed graphs with\n+    // certain topologies to be corrupted (kernels elided, internal syncs\n+    // ignored) when replayed back to back without a sync in between.\n+    // The bug is fixed in CUDA 11.4+.\n+    cudaDeviceSynchronize();\n+  }"
},
{
    "Id": 95,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c5fdcd85c7570b654eec45b6cba7cc75b0cf8f6b",
    "Violation": "insufficient",
    "Bug report": "check pruned attributes before deleting. I copyed a pruned model after deleteing the derived tensors. In order to be able to reparameter the model, we should check the existence of the tensors here.",
    "Number of deleted lines": 1,
    "Deleted lines": "-        delattr(module, self._tensor_name)",
    "Added lines": "+        if hasattr(module, self._tensor_name):\n+            delattr(module, self._tensor_name)"
},
{
    "Id": 96,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/678c08bb55eef0c2e707a17d0cd6e50f5b9bd427",
    "Violation": "missing",
    "Bug report": "_ProcessGroupWrapper check needs to be gated on Gloo availability, this fails when gloo is not avail_ProcessGroupWrapper check needs to be gated on Gloo availability, this fails when gloo is not avail.",
    "Number of deleted lines": 4,
    "Deleted lines": "-    # It is not expected for PG to be wrapped many times, but support it just\n-    # in case\n-    while isinstance(pg, _ProcessGroupWrapper):\n-        pg = pg.wrapped_pg",
    "Added lines": "+    # Gate PG wrapper check on Gloo availability.\n+    if _GLOO_AVAILABLE:\n+        # It is not expected for PG to be wrapped many times, but support it just\n+        # in case\n+        while isinstance(pg, _ProcessGroupWrapper):\n+            pg = pg.wrapped_pg"
},
{
    "Id": 97,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/db1ac4e29b0f557711190c8d49d4afb5da1844e8",
    "Violation": "insufficient",
    "Bug report": "Summary: We should explicitly check for the gloo backend instead of relying on the shard's device, because user might pass a GPU tensor as input and a process group gloo as the pg, and expect that should work.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if shard.is_cpu:",
    "Added lines": "+    if dist.get_backend(group) == dist.Backend.GLOO or shard.is_cpu:"
},
{
    "Id": 98,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3ef4d697df5bfdbd27dfc7a79c0679da2b87e3af",
    "Violation": "missing",
    "Bug report": "default backend need to check for nccl availability. As titled, we can only initialize nccl backend when NCCL is available",
    "Number of deleted lines": 4,
    "Deleted lines": "-            self.device_backend_map = {\n-                \"cpu\": Backend.GLOO,\n-                \"cuda\": Backend.NCCL,\n-            }",
    "Added lines": "+            self.device_backend_map = {\"cpu\": Backend.GLOO}\n+            if is_nccl_available():\n+                self.device_backend_map[\"cuda\"] = Backend.NCCL"
},
{
    "Id": 99,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6bf0e3b697ce688bc8325440dea3b51fea571c3d",
    "Violation": "insufficient",
    "Bug report": "Check for BackendCompilerFailed on CI. random triton failure on CI, but we need to check against the BackendCompilerFailed exception type.",
    "Number of deleted lines": 5,
    "Deleted lines": "-                if self.args.ci and (\n-                    (\n-                        isinstance(e, RuntimeError)\n-                        and \"Internal Triton PTX codegen error\" in str(e)\n-                    or (isinstance(e, KeyError) and \"cubin\" in str(e))",
    "Added lines": "+from torch._dynamo.exc import BackendCompilerFailed\n+                if (\n+                    self.args.ci\n+                    and isinstance(e, BackendCompilerFailed)\n+                    and (\n+                        \"Internal Triton PTX codegen error\" in str(e)\n+                        or \"cubin\" in str(e)"
},
{
    "Id": 100,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/62732bdcdb8b6112e01366d4ad1c2a50e61da1ed",
    "Violation": "insufficient",
    "Bug report": "quick fix for invalid nodes. Summary: As title.Need to check whether node is valid before fusion",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        and is_node_meta_valid(input)\n+        and is_node_meta_valid(weight)"
},
{
    "Id": 101,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/aab55d6d0d7b958e32cfdbb69794e107cfceb6bc",
    "Violation": "missing",
    "Bug report": "Remove all the dequant nodes when the ref module has multi input args. When converting a ref module into a quant module, `_lower_static_weighted_ref_module` pass assumes the `ref_node` only has 1 input node, and only remove the first `dequant` node. We add a check in this PR to ensure this is the case for `_lower_static_weighted_ref_module` pass.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        assert(len(ref_node.args) == 1)"
},
{
    "Id": 102,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bae895cef0c12df5f64afa155ce5462e06f0e04a",
    "Violation": "missing",
    "Bug report": "Added check for kHIP in ATen/native/Copy.cpp",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  } else if (iter.device_type(1) == kHIP) {\n+    device_type = kHIP;"
},
{
    "Id": 103,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cf348bcdeecfe0b47a2245d95eaa8ef37fb7b53e",
    "Violation": "missing",
    "Bug report": "tighten hasCUDA check",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  int count;\n+  cudaError_t err = cudaGetDeviceCount(&count);\n+  if (err == cudaErrorInsufficientDriver) {\n+    return false;\n+  }"
},
{
    "Id": 104,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/181b2481d338a24efc553378c837dcc48b656e3f",
    "Violation": "missing",
    "Bug report": "add error checking to grid sampler",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  THCudaCheck(cudaGetLastError());\n+  THCudaCheck(cudaGetLastError());"
},
{
    "Id": 105,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/027c0d7f8e37e583c02b372df5331d73793c06b1",
    "Violation": "missing",
    "Bug report": "fixed compilations on xla tensor prin. This is done to avoid compilations during tensor printing. Torch performs some tensor operations like slicing to make the tensor readable. These operations result in compilations. Hence to avoid the compilations, copying the tensor to cpu before printing. Returning from this function would have resulted in 63 compiles, since PDB prints the value of the return output. In this case it is a xla tensor. Now with the current change, there is no compilation.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    # Tensor printing performs tensor operations like slice, indexing, etc to make it in a\n+    # representable format. These operations on xla/lazy tensor results in compilations. Hence,\n+    # to avoid compilations, copying the tensor to cpu before printing.\n+    if self.device.type == 'xla' or self.device.type == 'lazy':\n+        self = self.to('cpu')\n+"
},
{
    "Id": 106,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ee91c328da5739ce03b3127cd7c542ce505212b8",
    "Violation": "improper",
    "Bug report": "Fix cuda/cpu check on NoneType ",
    "Number of deleted lines": 1,
    "Deleted lines": "-            elif not all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):",
    "Added lines": "+            elif not all([(x is None or x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):"
},
{
    "Id": 107,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/91066559a8c8e5978ed4de722317576b222267c5",
    "Violation": "improper",
    "Bug report": "truthy check for empty string in NameScope(). As in name. LATTE translation team moving some code from Python 2 to 3 uncovered a case where comparison between unicode and str types leads NameScope('') to prepend a separator to the beginning of blob names. This fixes it.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    prefix = prefix + _NAMESCOPE_SEPARATOR if prefix is not '' else ''",
    "Added lines": "+    prefix = prefix + _NAMESCOPE_SEPARATOR if prefix else ''"
},
{
    "Id": 108,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e3542d2c12d8aaaccf8a53873e480c20dc6b7338",
    "Violation": "missing",
    "Bug report": "avoid unnecessary call to empty_tensor_restride in empty(). Our empty benchmark makes this call unconditionally. If MemoryFormat::Contiguous is indeed a common case (or if workloads are likely to use a consistent-ish memory format), then I'd expect checking first to be a win.",
    "Number of deleted lines": 2,
    "Deleted lines": "-  auto memory_format = memory_format_opt.value_or(MemoryFormat::Contiguous);\n-  tensor.unsafeGetTensorImpl()->empty_tensor_restride(memory_format);",
    "Added lines": "+  if (memory_format_opt.has_value()) {\n+    // Restriding a just-created empty contiguous tensor does nothing.\n+    if (*memory_format_opt != MemoryFormat::Contiguous) {\n+      tensor.unsafeGetTensorImpl()->empty_tensor_restride(*memory_format_opt);\n+    }\n+  }"
},
{
    "Id": 109,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/45d5b3248b430aca70111316accd165954464589",
    "Violation": "missing",
    "Bug report": "Fixed C++ BatchNorm pretty_print() with optional momentum. Summary : Inserted a check for the momentum and print  \"None\" in case is not defined.",
    "Number of deleted lines": 1,
    "Deleted lines": "-         << \"momentum=\" << this->options.momentum().value() << \", \"",
    "Added lines": "+         << \"momentum=\";\n+\n+  if (this->options.momentum().has_value()) {\n+      stream << this->options.momentum().value();\n+  } else {\n+      stream << \"None\";\n+  }\n+\n+   stream << \", \""
},
{
    "Id": 110,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/666ff0ae220e1a5c406b0bc5cd43283e1b18b38e",
    "Violation": "missing",
    "Bug report": "Update _create_c10d_store to check port value. Port number is int in python, but needs to be uint16_t when called for TCPStore constructor.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    # check if port is uint16_t\n+    if not 0 <= port < 2**16:\n+        raise ValueError(f\"port must have value from 0 to 65535 but was {port}.\")"
},
{
    "Id": 111,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ba59d720cd5c5c81601b53d2c3397c46c1f87883",
    "Violation": "missing",
    "Bug report": "Change error message for torch.linspace(). Basically moves the error checking from the device-specific function to the native function.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TORCH_CHECK(steps >= 0, \"number of steps must be non-negative\");"
},
{
    "Id": 112,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bc371a2cd03ce573f3ad4f7be141364136028905",
    "Violation": "missing",
    "Bug report": "Add additional checks when tracing back during maybe share output observer function. Summary: Currently in `maybe_make_input_output_share_observers`  we trace back from a node to find the activation_post_process of the input node, we have internal use case which would error out during tracing back, this PR is adding a guard during this process to return False early when the node doesn't have any input",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                # failed to trace back since no input arg for the current node\n+                if len(input_arg.args) < 1:\n+                    return False"
},
{
    "Id": 113,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7ddf167ba5db277e02f983a6bde2bc3f5fbe1caa",
    "Violation": "missing",
    "Bug report": "Move the asserts in shape functions upsample_nearest_2d op. The assert check are moved to top and the function now returns out. This is needed by the downstream torch-mlir project to correctly determine the output type.",
    "Number of deleted lines": 3,
    "Deleted lines": "-        return out\n-        return out\n-    assert 0, \"Either output_size or scale_factors must be presented\"",
    "Added lines": "+\n+    if (scale_factors is None and output_size is None):\n+        assert 0, \"Either output_size or scale_factors must be presented\"\n+\n+\n+    return out"
},
{
    "Id": 114,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/23631eee5ae484d8397769492b3ea36f9eca282d",
    "Violation": "insufficient",
    "Bug report": "Fix the check of current scope in optimizer. scope.CurrentDeviceScope() can return a None type, which was not considered.",
    "Number of deleted lines": 3,
    "Deleted lines": "-            if (current_scope.device_type == caffe2_pb2.CUDA\n-                is_gpu_blob=(current_scope.device_type == caffe2_pb2.CUDA),\n-                is_gpu_blob=(current_scope.device_type == caffe2_pb2.CUDA),",
    "Added lines": "+            if (current_scope is not None\n+                    and current_scope.device_type == caffe2_pb2.CUDA\n+                is_gpu_blob=(current_scope is not None\n+                    and current_scope.device_type == caffe2_pb2.CUDA),\n+                is_gpu_blob=(current_scope is not None\n+                    and current_scope.device_type == caffe2_pb2.CUDA),"
},
{
    "Id": 115,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/647154f82ac2c57769f080c41452b3e5960ab94f",
    "Violation": "missing",
    "Bug report": "Assert tensor isn't sparse in enforce_invariants. There's no reason we can't check this, but I'm punting on implementing it for now.  But it currently segfaults, so this is an improvement",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      AT_ASSERTM(\n+          !impl_->is_sparse(),\n+          \"Sparse Tensors are supported by at::Tensor, but invariant checking isn't implemented.  Please file a bug.\");"
},
{
    "Id": 116,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a6a433aecd0da3ac3c8d49cb36091623f1b5ec9e",
    "Violation": "missing",
    "Bug report": "Add stack emptiness checks inside interpreter.cpp",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(stack.size() >= inst.N);\n+            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!stack.empty());\n+            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!stack.empty());"
},
{
    "Id": 117,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2c9dd886afc656a8bfe5d8bbcb601ee5877cee21",
    "Violation": "missing",
    "Bug report": "Modify torch.movedim to handle scalar as no-op. Summary: `torch.movedim` directly handle the case of a scalar tensor (0-dim) in input as a no-op by returning a view of the input tensor (after all the usual checks for the other parameters)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // handle the case of scalar tensor as a no-op\n+  if (self_dim == 0)\n+    return self.alias();\n+"
},
{
    "Id": 118,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/52e76a305677dfaf26cd5d59bd1aa239375f833c",
    "Violation": "missing",
    "Bug report": "fix ShardedTensor.gather when shard is empty. current ShardedTensor.gather is not working as expectation when the shard is empty on any rank The root cause is identified that when a sharded tensor has no placement on a specific rank, the metadata doesn't include that rank's placement which introduces KeyError in :```shard_offset = shard_placement[shard. Metadata][1]``` It's fixed by adding an empty tensor check.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                if src.nelement() == 0 :\n+                    warnings.warn(\"Gathering a tensor with zero elements on rank \" + str(rank))\n+                    return"
},
{
    "Id": 119,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d23231fd8cd50e4eb657eb7c3cf102475634f9c6",
    "Violation": "missing",
    "Bug report": "Fix upgrader codegen when constant list is 0. Summary: When the constant list is empty, previous codegen will generate something like ``` std::vector<c10::IValue>({ }), // constants list, ``` However it will fail quick-check, because it includes trailing spaces. This pr will generate the following instead. ``` std::vector<c10::IValue>(), // constants list,",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+CONSTANTS_LIST_EMPTY = \"\"\"std::vector<c10::IValue>(), // constants list\"\"\"\n+\n+    if len(constants_list_part) == 0:\n+        return CONSTANTS_LIST_EMPTY"
},
{
    "Id": 120,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4ee179c9528c8c6aae17a01f2b0d7e8235219219",
    "Violation": "insufficient",
    "Bug report": "Fix ConstantVariable init method if NumPy is missing. By adding `np is not None` check before `isinstance(value, np.number)`",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if isinstance(value, np.number):",
    "Added lines": "+        if np is not None and isinstance(value, np.number):"
},
{
    "Id": 121,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ba766ef39a4fff2d8856e17747393d469e409775",
    "Violation": "missing",
    "Bug report": "Fix BN size check in eval mode",
    "Number of deleted lines": 3,
    "Deleted lines": "-    size = list(input.size())\n-    if reduce(mul, size[2:], size[0]) == 1:\n-        raise ValueError('Expected more than 1 value per channel, got input size {}'.format(size))",
    "Added lines": "+    if training:\n+        size = list(input.size())\n+        if reduce(mul, size[2:], size[0]) == 1:\n+            raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))"
},
{
    "Id": 122,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b287cb816c1ac52165920a121c98643c08d31ff7",
    "Violation": "insufficient",
    "Bug report": " inductor: make the vec_transpose's tiling stride doesn't depend on out_idx and tiling_idex. ",
    "Number of deleted lines": 2,
    "Deleted lines": "-        return stride_at(self.itervars[self.outer_idx], index) == 1 and index.has(\n-            self.itervars[self.tiling_idx]",
    "Added lines": "+        return (\n+            stride_at(self.itervars[self.outer_idx], index) == 1\n+            and index.has(self.itervars[self.tiling_idx])\n+            and not stride_at(self.itervars[self.tiling_idx], index).has(\n+                self.itervars[self.tiling_idx]\n+            )\n+            and not stride_at(self.itervars[self.tiling_idx], index).has(\n+                self.itervars[self.outer_idx]\n+            )"
},
{
    "Id": 123,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/999bae0f54108ffc5b7cf2524a02a83901554b16",
    "Violation": "insufficient",
    "Bug report": "Add padding check for use_nnpack. nnp_convolution_output doesn't support the case of input padding > = kernel_size.",
    "Number of deleted lines": 1,
    "Deleted lines": "-           (at::symint::size<T>(weight, 2) < 17) && (at::symint::size<T>(weight, 3) < 17) // NNPACK only supports kernels up to 16x16",
    "Added lines": "+           (at::symint::size<T>(weight, 2) < 17) && (at::symint::size<T>(weight, 3) < 17) && // NNPACK only supports kernels up to 16x16\n+           (padding[0] < at::symint::size<T>(weight, 2)) && (padding[1] < at::symint::size<T>(weight, 3)) // NNPACK only supports padding < kernel_size. See https://github.com/pytorch/pytorch/issues/90142."
},
{
    "Id": 124,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/66b04e3cb707d30c4698b269c83cb6221848f17a",
    "Violation": "missing",
    "Bug report": "nullptr profiling name. Sometimes profiling name can be a nullptr, which throws on conversion to std::string. This adds a check.",
    "Number of deleted lines": 1,
    "Deleted lines": "-        profiling_name,",
    "Added lines": "+        profiling_name == nullptr ? \"\" : profiling_name,"
},
{
    "Id": 125,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/14c47fb211bc929ae4e505e7e13411faa2339f00",
    "Violation": "missing",
    "Bug report": " fix invalid-null-argument UBSAN error in math_cpu.cc. Add an if statement to check if the destination buffer is not nullptr.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (A == nullptr) {\n+    return;\n+  }"
},
{
    "Id": 126,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/40a7c317bc60713528320b9786765e4ec5707982",
    "Violation": "missing",
    "Bug report": "Run BLAS F2C checks on host architecture",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+   # Push host architecture when cross-compiling otherwise check would fail\n+   # when cross-compiling for arm64 on x86_64\n+   cmake_push_check_state(RESET)\n+  if(CMAKE_SYSTEM_NAME STREQUAL \"Darwin\" AND CMAKE_OSX_ARCHITECTURES MATCHES \"^(x86_64|arm64)$\")\n+    list(APPEND CMAKE_REQUIRED_FLAGS \"-arch ${CMAKE_HOST_SYSTEM_PROCESSOR}\")\n+  endif()\n+  cmake_pop_check_state()"
},
{
    "Id": 127,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/acd51e13f727af22e6c9e579518362898f1b12e6",
    "Violation": "missing",
    "Bug report": "TorchScript add check if quantized",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                    if orig.is_quantized:\n+                        orig = orig.dequantize()\n+                    if ref.is_quantized:\n+                        ref = ref.dequantize()"
},
{
    "Id": 128,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0584fd9339af7c939ab7d955db05743ba58ff86d",
    "Violation": "improper",
    "Bug report": "Only insert observers for fixed qparam ops. Fixed a condition check for fixed qparam ops, previously we were including CopyNodes as well",
    "Number of deleted lines": 1,
    "Deleted lines": "-            if activation_dtype(qconfig) == torch.float16:",
    "Added lines": "+            # insert observers for fixedqparams ops like sigmoid, since\n+            # it supports fp16 static quantization\n+            if isinstance(quantize_handler, FixedQParamsOpQuantizeHandler) and \\\n+               activation_dtype(qconfig) == torch.float16:"
},
{
    "Id": 129,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/97245a06e14a5b87a0bca1908d7200603aff2c8c",
    "Violation": "improper",
    "Bug report": "TORCH_INTERNAL_ASSERT_DEBUG_ONLY won't be enabled during non-debug builds, but for 1 dimension Tensors the check is cheap enough and not catching this can slow down development a lot.",
    "Number of deleted lines": 5,
    "Deleted lines": "-inline at::Tensor wrap_buffer(\n-    at::Tensor buffer,\n-    at::Tensor nested_sizes) {\n-  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(\n-      buffer.is_contiguous(), \"Given buffer must be contiguous.\");",
    "Added lines": "+inline at::Tensor wrap_buffer(at::Tensor buffer, at::Tensor nested_sizes) {\n+  TORCH_CHECK(\n+      buffer.dim() == 1,\n+      \"Expected given buffer to be 1dim, but got \",\n+      buffer.dim(),\n+      \" instead.\");\n+  TORCH_CHECK(\n+      buffer.is_contiguous(), \"Expected given buffer to be contiguous.\");"
},
{
    "Id": 130,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9bcb4de16878073896d8743fbd70d5abe28b595a",
    "Violation": "insufficient",
    "Bug report": "check parameter k and l ",
    "Number of deleted lines": 1,
    "Deleted lines": "-  TORCH_CHECK((unsigned)l < dims.size());",
    "Added lines": "+  TORCH_CHECK((unsigned)l < dims.size() && (unsigned)k < dims.size());"
},
{
    "Id": 131,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/65496e4e67b42e52b3428b0cf2d994e0aa1a9902",
    "Violation": "missing",
    "Bug report": "Bug fix in bound shape inferencer. Accessing dims() without boundary check is not good.",
    "Number of deleted lines": 1,
    "Deleted lines": "-      channel_acc += current_input_shape.shape.dims(axis);",
    "Added lines": "+      if (axis < current_input_shape.shape.dims_size()) {\n+        channel_acc += current_input_shape.shape.dims(axis);\n+      } else {\n+        LOG(INFO) << \"Mismatched input dim along axis \" << axis\n+                  << \". We cannot infer missing input shape for Concat\";\n+        return;\n+      }"
},
{
    "Id": 132,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/260f66c3165ce0c48dd1514a916da6971d981578",
    "Violation": "missing",
    "Bug report": "Fix concat dimension check bug",
    "Number of deleted lines": 1,
    "Deleted lines": "-      const int canonical_axis = canonical_axis_index_(axis, in[0].dims_size());",
    "Added lines": "+      int adj_size = in[0].dims_size() + (add_axis ? 1 : 0);\n+      const int canonical_axis = canonical_axis_index_(axis, adj_size);\n+      CAFFE_ENFORCE_LT(\n+          canonical_axis, adj_size, \"Axis not in input ndim range.\");"
},
{
    "Id": 133,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc07102b17915f21170fae9a9d52c6f2d59726ca",
    "Violation": "missing",
    "Bug report": "Check dim size preventively when doing shape inference for BatchMatMul. We check input(0) but not input(1) in BatchMatMul. This may result in a protobuf exception which won't be caught by upstream and causing termination of the program. Check that with `CAFFE_ENFORCE` will be caught by upstream inference function. Plus, it will print out clean stack tracing showing where went wrong.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    CAFFE_ENFORCE_GE(in[1].dims_size(), 2);"
},
{
    "Id": 134,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a53f4b0f9bbc007c0a92e4fd28dd22af027e24a8",
    "Violation": "missing",
    "Bug report": " add dimension check to NHWC2NCHW shape inference. Summary: To prevent assertion from protobuffer when accessing the dims.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      CAFFE_ENFORCE_EQ(\n+          in[0].dims_size(), 4, \"Input for NHWC2NCHW must be 4 dimensional\");"
},
{
    "Id": 135,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/55092b1cc604fad3d70d31e71bbdd3a43a279423",
    "Violation": "missing",
    "Bug report": "Validate matching input shapes in Int8Add operator. Default engine doesn't support broadcast semantics in Int8Add operator. This patch adds a check that shapes are equivalent.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    CAFFE_ENFORCE_EQ(\n+        A.t.sizes(),\n+        B.t.sizes(),\n+        \"inputs must have the same shape (broadcast semantics is not supported)\");\n+"
},
{
    "Id": 136,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1359d16fe8ca0cb7041674c455f2f99a9636fec0",
    "Violation": "missing",
    "Bug report": "Further tighten the checking of two eager runs. Summary: To catch nondeterminism in eager if there is any.",
    "Number of deleted lines": 2,
    "Deleted lines": "-                fp64_ref=None,  # Two eager runs should be the same without comparing against fp64_output\n-        torch.backends.cudnn.deterministic = True",
    "Added lines": "+            # Two eager runs should have exactly same result\n+                fp64_ref=None,\n+                cos_similarity=False,\n+                tol=0,\n+        torch.use_deterministic_algorithms(True)\n+        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n+        torch.backends.cudnn.deterministic = True\n+        torch.backends.cuda.matmul.allow_tf32 = False"
},
{
    "Id": 137,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1f819ee965894b8332cb364a67c91855c91c9dcc",
    "Violation": "missing",
    "Bug report": "Add check for no grad in transformer encoder nestedtensor conversion.  Before, we allowed inputs with grad to be converted to NestedTensors. Autograd attempts to find the size of the NestedTensor, but NestedTensor throws an exception for its size function. This causes all calls to nn.TransformerEncoder with grad enabled to fail. Fix: we add a check for no grad in transformer encoder so we do not convert tensor with grad to nestedtensor.",
    "Number of deleted lines": 3,
    "Deleted lines": "-                        if output.is_cuda or 'cpu' in str(output.device):\n-                            convert_to_nested = True\n-                            output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())",
    "Added lines": "+                        if not torch.is_grad_enabled() or all([not x.requires_grad for x in tensor_args]):\n+                            if output.is_cuda or 'cpu' in str(output.device):\n+                                convert_to_nested = True\n+                                output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())"
},
{
    "Id": 138,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc43ad428603539a2051940c09b191825f66203d",
    "Violation": "missing",
    "Bug report": " add is_grad_enabled check in runtime_wrapper before running with torch.no_grad. We observed that `with torch.no_grad()` in runtime_wrapper introduced ~10% (0.06ms->0.066ms) inference performance regression on lennard_jones on cpu. For inference tasks in benchmark, grad has been disabled, but in the current runtime_wrapper, no_grad is set again and its time is counted into the running time. Therefore, we add `is_grad_enabled` check in runtime_wrapper before running with torch.no_grad. If grad has been disabled, there is no need to set no_grad. ",
    "Number of deleted lines": 1,
    "Deleted lines": "-            with torch.no_grad():",
    "Added lines": "+            if torch.is_grad_enabled():\n+                with torch.no_grad():\n+                    all_outs = call_func_at_runtime_with_args(\n+                        compiled_fn,\n+                        args,\n+                        disable_amp=disable_amp,\n+                    )\n+            else:"
},
{
    "Id": 139,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f3a2094065c8b4b7bae426e71c923a8a8abb74b5",
    "Violation": "insufficient",
    "Bug report": "Mitigate legacy issue that aten op as export entrance function. This is not supported any more, now the top level ```torch.export``` only support ```nn.Module```, but there are still some tests using the internal APIs and caused the ```trace_rules.check``` assertion error. This PR is going to mitigate such cases.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            and not isinstance(\n+                call_to_inspect, (torch._ops.OpOverloadPacket, torch._ops.OpOverload)\n+            )"
},
{
    "Id": 140,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cc6a51c9f3ee97a06ff9c0b84477e88e33e31137",
    "Violation": "missing",
    "Bug report": "added shape checking to WeightedRandomSampler",
    "Number of deleted lines": 1,
    "Deleted lines": "-        self.weights = torch.as_tensor(weights, dtype=torch.double)",
    "Added lines": "+\n+        weights_tensor = torch.as_tensor(weights, dtype=torch.double)\n+        if len(weights_tensor.shape) != 1:\n+            raise ValueError(\"weights should be a 1d sequence but given \"\n+                             \"weights have shape {}\".format(tuple(weights_tensor.shape)))\n+\n+        self.weights = weights_tensor"
},
{
    "Id": 141,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8ee59280d78a4fefc4de0da04b287e067c28de0d",
    "Violation": "insufficient",
    "Bug report": "Bug - check config for dynamic",
    "Number of deleted lines": 2,
    "Deleted lines": "-                automatic_dynamic = curr_sizes is None or curr_sizes[i] is None\n-",
    "Added lines": "+                automatic_dynamic = config.automatic_dynamic_shapes and (\n+                    curr_sizes is None or curr_sizes[i] is None\n+                )"
},
{
    "Id": 142,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/41ad221751e57c2d2ccc82b431f56d6ed62e1741",
    "Violation": "missing",
    "Bug report": "MHA: fix contiguity assumption in transform_bias_rescale_qkv. This code path incorrectly assumed input tensors were contiguous. Now we check that.",
    "Number of deleted lines": 3,
    "Deleted lines": "-  AT_DISPATCH_FLOATING_TYPES_AND2(\n-        scalar_t* qkv_data = qkv.data_ptr<scalar_t>();\n-        scalar_t* qkv_bias_data = qkv_bias.data_ptr<scalar_t>();",
    "Added lines": "+  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(q_k_v.is_contiguous());\n+  const auto qkv_contig = qkv.expect_contiguous();\n+  const auto qkv_bias_contig = qkv_bias.expect_contiguous();\n+ AT_DISPATCH_FLOATING_TYPES_AND2(\n+        scalar_t* qkv_data = qkv_contig->data_ptr<scalar_t>();\n+        scalar_t* qkv_bias_data = qkv_bias_contig->data_ptr<scalar_t>();\n+  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(q_k_v_s.size() == 3);"
},
{
    "Id": 143,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e9e125475e94aabfb34ee239fadc760615eef429",
    "Violation": "missing",
    "Bug report": "Add schema check to aten::repeat and fb::fast_gather",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (n->inputs().size() != 2) {\n+    return nullptr;\n+  }"
},
{
    "Id": 144,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7ea6559658a6f650363f8b96f462bbc047e29124",
    "Violation": "missing",
    "Bug report": "Add size checks to torch.stack. Checks the size of each tensor passed to `torch.stack` before calling `cat` to address #29510. This is done in the `get_stack_input` function as that is a common path. The function now compares the size of each tensor in the TensorList to the size of the first tensor and throws an exception when the sizes are not equal.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  for (size_t i = 0; i < tensors.size(); ++i) {",
    "Added lines": "+// Precondition: tensors is non-empty\n+  at::IntArrayRef entry_shape = tensors[0].sizes();\n+  inputs[0] = tensors[0].unsqueeze(dim);\n+  for (size_t i = 1; i < tensors.size(); ++i) {\n+    TORCH_CHECK(tensors[i].sizes() == entry_shape,\n+      \"stack expects each tensor to be equal size, but got \", entry_shape,\n+      \" at entry 0 and \", tensors[i].sizes(), \" at entry \", i);"
},
{
    "Id": 145,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bbb5e106ad6228953df6c7f5c8916b26dc0cb457",
    "Violation": "improper",
    "Bug report": "Improve error checking of CUDALoops. Same change as was applied to CPU loops -- separate out checking of the inputs and outputs.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  TORCH_INTERNAL_ASSERT(iter.ntensors() == traits::arity + 1);",
    "Added lines": "+  TORCH_INTERNAL_ASSERT(iter.ninputs() == traits::arity);\n+  TORCH_INTERNAL_ASSERT(iter.noutputs() == 1);"
},
{
    "Id": 146,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7684044b713761abd4f51225dc5d83ce5869562a",
    "Violation": "missing",
    "Bug report": "Add size check before calling .back() in rpc/script_call.cpp",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TORCH_INTERNAL_ASSERT(\n+      ivalues.size() > 1,\n+      \"At least 2 IValues are required to build a ScriptCall.\");\n+"
},
{
    "Id": 147,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c69b3b8d4f484cf537d98974a3a4143b77edf3c8",
    "Violation": "insufficient",
    "Bug report": "Autograd engine use current device only. In this PR a check upon CUDA devices in device registry is added such that threads set the same CUDA device.",
    "Number of deleted lines": 9,
    "Deleted lines": "-\n-#if defined(USE_CUDA)\n-  if (at::detail::getCUDAHooks().hasPrimaryContext(device)) {\n-    set_device(device);\n-  }\n-#else\n-  set_device(device);\n-#endif\n-      if (impl && device < impl->deviceCount()) {",
    "Added lines": "+  worker_device = device;\n+      set_device(worker_device);\n+\n+      if (impl && device < impl->deviceCount() &&\n+          impl->getDevice().index() != device) {"
},
{
    "Id": 148,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fb25fd6f865ed0532caf710ca130b6cc23a772a8",
    "Violation": "misleading",
    "Bug report": "Replaced neg dim normalization with assert in helper. I think we can still leave the check for negative shard dimension in `compute_local_shape_and_global_offset` and replace the normalization logic with an assert. This should provide us a stack trace to see which user-facing API did not normalize the dim as expected.",
    "Number of deleted lines": 2,
    "Deleted lines": "-                # normalize shard dim to be positive\n-                shard_placement.dim += len(tensor_shape)",
    "Added lines": "+                raise AssertionError(\n+                    \"Shard placements should have negative dims normalized in \"\n+                    f\"the user-facing APIs: {shard_placement}\"\n+                )"
},
{
    "Id": 149,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9e314bd8224f93b4ba1f9e4c065150e47a2de2cc",
    "Violation": "missing",
    "Bug report": "handle the case where output of op is Optional[Tensor]. some op might have Optional[Tensor] returns where it return None (i.e. native_layer_norm_backward), it's a mismatch between C++ aten op signature and python None, but we need to handle it in the python side",
    "Number of deleted lines": 1,
    "Deleted lines": "-OutputSpecType = Optional[Union[DTensorSpec, Sequence[DTensorSpec]]]",
    "Added lines": "+OutputSpecType = Optional[Union[DTensorSpec, Sequence[Optional[DTensorSpec]]]]\n+\n+        # NOTE: local results might return Optional Tensor from ATen op, so we need to\n+        # handle that case and make sure we don't wrap None with DTensor.\n+        # (i.e. native_layer_norm.backward)\n+            if e is not None and s is not None else None"
},
{
    "Id": 150,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/55c19a3c6d38a49fe34e008c4c566445c43810f0",
    "Violation": "missing",
    "Bug report": "Increase multiplier to 3 for Inductor AMP benchmark correctness check. we find some of the models have failed the benchmark's correctness check. However, the end-to-end model's accuracy. when comparing AMP with FP32 is within a difference of less than 0.1%. Thus, it's possible that the correctness check failures for these models are false alarms. We use multiplier of 3 instead of 2 in this PR to avoid these false alarms.",
    "Number of deleted lines": 1,
    "Deleted lines": "-                multiplier = 2.0",
    "Added lines": "+\n+                # In the case of using AMP (Automatic Mixed Precision), certain models have\n+                # failed the benchmark's correctness check. However, the end-to-end model's\n+                # accuracy when comparing AMP with FP32 is within a difference of less than 0.1%.\n+                # Thus, it's possible that the correctness check failures for these models are\n+                # false alarms. We use multiplier of 3 instead of 2 to avoid these false alarms.\n+                multiplier = 3.0 if res.dtype == torch.bfloat16 else 2.0"
},
{
    "Id": 151,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/828a6a3b3943a0a0701ecacacd2bcc34fc03fe03",
    "Violation": "missing",
    "Bug report": "Use proper isnan check ",
    "Number of deleted lines": 1,
    "Deleted lines": "-  ((x != x && y == y) || (x > y))",
    "Added lines": "+  ((th_isnan(x) && !(th_isnan(y))) || (x > y))"
},
{
    "Id": 152,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3611d26a25bd889627403a808ea667ac99c09904",
    "Violation": "missing",
    "Bug report": "Optimize FunctionSchema::checkArg for the Tensor case. The Tensor case is one of the most common and the existing check can be made faster. This results in a ~21% improvement on DeepAndWide model and would improve other models as well.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (value.isTensor() && argument.type() == TensorType::get()) {\n+    // Fast-path for the common case\n+    return;\n+  }"
},
{
    "Id": 153,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/324dc1623e2f91892038fb1b151450a7c6529dd9",
    "Violation": "missing",
    "Bug report": "add dtype checking for gather and scatter. in the `cpu_scatter_gather_base_kernel`, it interpret a pointer as `int64_t` regardless the actual dtype. add a index dtype checking will avoid the nasty index out of bound error. As using `int64_t` is convention in ATen code (a.k.a, a limitation), no further fix is needed at the moment.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"gather_out(): Expected dtype int64 for index\");\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"gather(): Expected dtype int64 for index\");\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter_(): Expected dtype int64 for index\");\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter_fill_(): Expected dtype int64 for index\");\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter(): Expected dtype int64 for index\");\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter(): Expected dtype int64 for index\");\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter_add_(): Expected dtype int64 for index\");\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter_add(): Expected dtype int64 for index\");"
},
{
    "Id": 154,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5e50993be72bec4ad939993328dd02691ef7777d",
    "Violation": "missing",
    "Bug report": "Better type checking for pack_padded_sequence symbolic",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        if lengths.type().kind() != 'TensorType':\n+            raise RuntimeError(\"Lengths must be a Tensor for ONNX export\")\n+        # We know it's a TensorType so this check is now safe.\n+        if lengths.type().scalarType() != 'Int':\n+            raise RuntimeError(\"ONNX export requires that the lengths passed \"\n+                               \"to pack_padded_sequence must be of type Int\")"
},
{
    "Id": 155,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/68ad9ae5bebd9efab127fa99e2bafd6852bbd8ed",
    "Violation": "missing",
    "Bug report": "Ensure there aren't variables in checked_tensor_unwrap, checked_tensor_list_unwrap. These functions use unsafeGetTensorImpl(), which doesn't work with Variables (in a silent way that may blow up later). So let's do early checking.",
    "Number of deleted lines": 1,
    "Deleted lines": "-               \" for sequence elment \", i , \" in sequence argument at position #\", pos, \" '\", name, \"'\");",
    "Added lines": "+  if (expr.is_variable()) {\n+    AT_ERROR(\"Expected Tensor (not Variable) for argument #\", pos, \" '\", name, \"'\");\n+  }\n+               \" for sequence element \", i , \" in sequence argument at position #\", pos, \" '\", name, \"'\");\n+    }\n+    if (expr.is_variable()) {\n+      AT_ERROR(\"Expected Tensor (not Variable) for sequence element \",\n+               i , \" in sequence argument at position #\", pos, \" '\", name, \"'\");"
},
{
    "Id": 156,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/468a73f0e3527c52495c864c7d48dc26684f6c0b",
    "Violation": "missing",
    "Bug report": " Support Numpy ints in the torch.nn.functional.interpolate dtype check. This PR updates the check to also include numpy integers",
    "Number of deleted lines": 1,
    "Deleted lines": "-    Will return True for int, SymInt and Tensors with integer elements.",
    "Added lines": "+try:\n+    import numpy as np\n+except ModuleNotFoundError:\n+    np = None\n+\n+    Will return True for int, SymInt, Numpy integers and Tensors with integer elements.\n+    if np is not None and isinstance(x, np.integer):\n+        return True"
},
{
    "Id": 157,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e31038d574712d383fdc4c2f1bb63fc82f256ed0",
    "Violation": "missing",
    "Bug report": "Check results dtype in index_out. This logic exists for index_put and index_add, but for some reason not for `index.out` Skip testing, as this function is not technically exposed on the Python level.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    TORCH_CHECK(self.scalar_type() == result.scalar_type(),\n+                \"index_out: self (\", self.scalar_type(), \") and result (\", result.scalar_type(),\n+                \") must have the same scalar type\");"
},
{
    "Id": 158,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a69f427f957a37eee9c1dd5df681f30ab38ed3e4",
    "Violation": "improper",
    "Bug report": "aten: Ensure dim is size_t",
    "Number of deleted lines": 1,
    "Deleted lines": "-      self.dim() <= output_size.size(),",
    "Added lines": "+      static_cast<size_t>(self.dim()) <= output_size.size(),"
},
{
    "Id": 159,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8340762211e3b55caa178bac748bd902249f6fc0",
    "Violation": "missing",
    "Bug report": "Update lr_scheduler.py to check the type of eta_min. Add float assertion to `eta_min` parameter in `CosineAnnealingWarmRestarts`.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        if not isinstance(eta_min, (float, int)):\n+            raise ValueError(\"Expected float or int eta_min, but got {} of type {}\".format(eta_min, type(eta_min)))"
},
{
    "Id": 160,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2dafa70d61a1a5af849ab79c7aed4c84686337a0",
    "Violation": "missing",
    "Bug report": "Add a little more error checking to minifier",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    def __post_init__(self):\n+        ph_nodes = get_placeholders(self.graph)\n+        assert len(ph_nodes) == len(self.inps)\n+\n+    assert isinstance(inps, (tuple, list))\n+"
},
{
    "Id": 161,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ae55619a2bb73ebcdc80b02a6ccd72275a9ce23e",
    "Violation": "missing",
    "Bug report": "Add check for same dtype in tensordot implementation",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TORCH_CHECK(input1.scalar_type() == input2.scalar_type(), \"both inputs should have same dtype\");"
},
{
    "Id": 162,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b6920405da340bbd3397b80bf16d9c360b0c48d2",
    "Violation": "improper",
    "Bug report": "reorder checks to shave 1 us off no-op dispatch time ",
    "Number of deleted lines": 5,
    "Deleted lines": "-  return (\n-    !THPVariable_CheckTypeExact(tp) &&\n-    // TODO: test if Python key is disabled\n-    attr.ptr() != nullptr &&\n-    attr.ptr() != torch::disabled_torch_dispatch_impl()",
    "Added lines": "+  if (THPVariable_CheckTypeExact(tp)) {\n+    return false;\n+  }\n+  return (attr.ptr() != nullptr &&\n+          attr.ptr() != torch::disabled_torch_dispatch_impl()"
},
{
    "Id": 163,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/152f665dee05377f7459d985d60dc1edb782d40e",
    "Violation": "missing",
    "Bug report": "Inserted check for PyObject_IsInstance in THPVariableCheck. Summary: Inserted check for the return of PyObject_IsInstance to capture the case in which it raises an exception and return -1. When this happen THPVariable_Check now throws a python_error to signal the exception.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return THPVariableClass && PyObject_IsInstance(obj, THPVariableClass);",
    "Added lines": "+#include <torch/csrc/Exceptions.h>\n+  if (!THPVariableClass)\n+      return false;\n+\n+  const auto result = PyObject_IsInstance(obj, THPVariableClass);\n+  if (result == -1)\n+      throw python_error();\n+  return result;"
},
{
    "Id": 164,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/18eeccc7e8cba16d71efdd2eca831983c4abde15",
    "Violation": "missing",
    "Bug report": "Fix Optional type check",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                assert self.optimization_profiles"
},
{
    "Id": 165,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/402be850a8946e8967dedb3375fc6f33b379b397",
    "Violation": "missing",
    "Bug report": "Adding zero point type check for per channel quantization",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TORCH_CHECK(scale.scalar_type() == ScalarType::Float,\n+              \"Scale must be Float, found \", scale.scalar_type());\n+  TORCH_CHECK(zero_point.scalar_type() == ScalarType::Long,\n+              \"Zero-point must be Long, found \", zero_point.scalar_type());\n+  TORCH_CHECK(scale.scalar_type() == ScalarType::Float,\n+              \"Scale must be Float, found \", scale.scalar_type());\n+  TORCH_CHECK(zero_point.scalar_type() == ScalarType::Long,\n+              \"Zero-point must be Long, found \", zero_point.scalar_type());"
},
{
    "Id": 166,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4cc6e6bbbe1fb114e7d7fb207ef2deb567950102",
    "Violation": "missing",
    "Bug report": "Adding scalar to the c10 registration type check",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+          } else if (type->kind() == TypeKind::NumberType) {\n+            tracer::addInputs(node, args[i].name().c_str(), iter->toScalar());"
},
{
    "Id": 167,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4b1ebd2f65e49d251ac2cfdb635794c7c6eb362f",
    "Violation": "missing",
    "Bug report": "Fast path for serializing large floating-point tensors to protobuf. Summary: Our existing serialization routines take a significant amount of time for large numpy arrays in order to verify the type of each element in the array as well as converting each element to a canonical type.  For large floating-point tensors, such as model parameters, this checking and converting takes a significant amount of time.  Adding a fast track path for just float32 arrays as this is the most common use case to worry about.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    # Fast tracking common use case where a float32 array of tensor parameters\n+    # needs to be serialized.  The entire array is guaranteed to have the same\n+    # dtype, so no per-element checking necessary and no need to convert each\n+    # element separately.\n+    if isinstance(value, np.ndarray) and value.dtype.type is np.float32:\n+        argument.floats.extend(value.flatten().tolist())\n+        return argument\n+"
},
{
    "Id": 168,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/65dfe1203ffab064d4e32fa8f76833042369d2f5",
    "Violation": "missing",
    "Bug report": "add an assertion to check the param num. Introduce this check to see whether it will break any existing workflow",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    # make sure that the param dict and the graph match each other\n+    flatten_args, _ = torch._C._jit_flatten(args)\n+    assert len(params) + len(flatten_args) == sum(1 for _ in graph.inputs())\n+"
},
{
    "Id": 169,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fa66a1498eb1fac5b36811d5c1d6ba1540ffc824",
    "Violation": "missing",
    "Bug report": "Summary: The code checking `if dimensions == 2` is not needed because the case of a 2D tensor (Linear) is already handled by the statement: `receptive_field_size = 1` and this conditional: `if tensor.dim() > 2:`",
    "Number of deleted lines": 11,
    "Deleted lines": "-    if dimensions == 2:  # Linear\n-        fan_in = tensor.size(1)\n-        fan_out = tensor.size(0)\n-    else:\n-        num_input_fmaps = tensor.size(1)\n-        num_output_fmaps = tensor.size(0)\n-        receptive_field_size = 1\n-        if tensor.dim() > 2:\n-            receptive_field_size = tensor[0][0].numel()\n-        fan_in = num_input_fmaps * receptive_field_size\n-        fan_out = num_output_fmaps * receptive_field_size",
    "Added lines": "+    num_input_fmaps = tensor.size(1)\n+    num_output_fmaps = tensor.size(0)\n+    receptive_field_size = 1\n+    if tensor.dim() > 2:\n+        receptive_field_size = tensor[0][0].numel()\n+    fan_in = num_input_fmaps * receptive_field_size\n+    fan_out = num_output_fmaps * receptive_field_size"
},
{
    "Id": 170,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8a644f0c136cb12cf200050c2ae6875ec487d174",
    "Violation": "improper",
    "Bug report": "Summary: Sometimes first dim of X in FC is BATCH_OF_FEATURE_MAX instead of BATCH. This caused an issue in f207899183 (when first dim of X is 64 but is set to 1 in inferFC). Change the check from `!= BATCH` to `== UNKNOWN`",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (x_shape_info.getDimType(0) != TensorBoundShape_DimType_BATCH) {",
    "Added lines": "+    if (x_shape_info.getDimType(0) == TensorBoundShape_DimType_UNKNOWN) {"
},
{
    "Id": 171,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5023995292f5119c447de15c20a375b7e3aa2d0b",
    "Violation": "improper",
    "Bug report": " fix output size adjustment for onnxifi_op. Summary: this breaks if we cut the net at certain int8 ops boundary.",
    "Number of deleted lines": 1,
    "Deleted lines": "-      if (max_shape[j] > real_shape.dims(j)) {",
    "Added lines": "+      if (max_shape[j] >= real_shape.dims(j)) {"
},
{
    "Id": 172,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/24601daa1203a9ad1232e1d18a07ff4842d53d27",
    "Violation": "insufficient",
    "Bug report": "Adding check for a single batch in adaptive_avg_pool ",
    "Number of deleted lines": 4,
    "Deleted lines": "-    if (input.ndimension() == 3)\n-      output.resize_({sizeD, osizeH, osizeW});\n-\n-    if (input.ndimension() == 3)",
    "Added lines": "+    if (input.ndimension() == 3 || input.size(-4) == 1)\n+      if (input.ndimension() == 3) {\n+        output.resize_({sizeD, osizeH, osizeW});\n+      } else {\n+        output.resize_({1, sizeD, osizeH, osizeW});\n+      }\n+    if (input.ndimension() == 3 || input.size(-4) == 1)"
},
{
    "Id": 173,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8dda19b79f2c4418f481a9f56932b3b5c5afdf39",
    "Violation": "unnecessary",
    "Bug report": "Remove extraneous TensorId checks in as_strided ",
    "Number of deleted lines": 6,
    "Deleted lines": "-  TORCH_CHECK(\n-      tid == CPUTensorId() || tid == CUDATensorId(),\n-      \"as_strided is only implemented for strided CPU, CUDA and QuantizedCPU tensors.\");\n-  TORCH_CHECK(\n-      tid == QuantizedCPUTensorId(),\n-      \"as_strided is only implemented for strided CPU, CUDA and QuantizedCPU tensors.\");",
    "Added lines": ""
},
{
    "Id": 174,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5a20c56ebce3426397210e91693fbbeade8b46ba",
    "Violation": "unnecessary",
    "Bug report": "emove hasOperation() check. by removing the hasOperation() check, the Operation gets successfully materialized, and static runtime enables successfully and runs ok. Will check that the outputs match with jit interpreter",
    "Number of deleted lines": 1,
    "Deleted lines": "-    TORCH_CHECK(op.hasOperation());",
    "Added lines": ""
},
{
    "Id": 175,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/012829eb3657aff2d58cead0bd166089c6e90c7f",
    "Violation": "unnecessary",
    "Bug report": "Do not crash when target device is unsupported by fuser. The `canFuseOnDevice` function now crashes when the device is not covered (i.e., CPU, GPU, XPU). However, now we have some devices, such as XLA and Lazy, that could perform fusion by themselves. This checker then prevents these devices from working on the models partially implemented in `jit.script`. This PR proposes to remove this checker and simply return false for all uncovered cases.",
    "Number of deleted lines": 2,
    "Deleted lines": "-    } else {\n-      TORCH_CHECK_NOT_IMPLEMENTED(false, \"Unknown device for tensorexpr fuser\")",
    "Added lines": "+    return false;"
},
{
    "Id": 176,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f386312ec936a94bfb1abe44acdd61d498f4272b",
    "Violation": "improper",
    "Bug report": "Don't do extra numel() check in TensorImpl::data(). `is_empty()` checks `numel() == 0`, but we don't need to access `numel_` at all (or the policy that `numel()` checks) in our happy path -- we just need the data pointer from `storage_`. Let's do the check we need to do using only the data we strictly need, rather than adding instructions loading other pieces of data.",
    "Number of deleted lines": 4,
    "Deleted lines": "-    if (is_empty()) {\n-    return static_cast<void*>(\n-        static_cast<char*>(storage_.data()) +\n-        data_type_.itemsize() * storage_offset_);",
    "Added lines": "+    char* const data = static_cast<char*>(storage_.data());\n+    if (data == nullptr) {\n+    return static_cast<void*>(data + data_type_.itemsize() * storage_offset_);"
},
{
    "Id": 177,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f810d96806d0e767aeca9fe9cf50e0bdcaab7d52",
    "Violation": "unnecessary",
    "Bug report": "emove redundant index check for index_select_out_cpu_dim1_. For  **index_select_out_cpu_dim1_**, there has a redundant idex check, **check_indexarray_range** has checked  **the index>=0 and  index < slect_dim**, we don't need re-check it at copy step.",
    "Number of deleted lines": 7,
    "Deleted lines": "-            if (idx < 0) {\n-              idx = idx + src_indexing_axis_dim;\n-            }\n-            if (idx < 0) {\n-              idx = idx + src_indexing_axis_dim;\n-            }\n-",
    "Added lines": ""
},
{
    "Id": 178,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/63e47c68a692c70bc64c49d687f85f7f5cd02ce3",
    "Violation": "unnecessary",
    "Bug report": " remove checks from embedding bag impl. These checks incur an H2D sync on every embedding bag forward. Also, the equivalent python code for embedding_bag does not have them",
    "Number of deleted lines": 11,
    "Deleted lines": "-    TORCH_CHECK(\n-        offsets_[0].item<int64_t>() == 0,\n-        \"offsets[0] has to be 0, i.e., the first sequence in the mini-batch has to start from position 0. However, got \",\n-        offsets_[0].item<int64_t>());\n-    TORCH_CHECK(\n-        offsets_[-1].item<int64_t>() <= input_.size(0),\n-        \"offsets[-1] can not be greater than input's length({\",\n-        input_.size(0),\n-        \"}), but got offsets[-1] of {\",\n-        offsets_[-1].item<int64_t>(),\n-        \"}\");",
    "Added lines": ""
},
{
    "Id": 179,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5b7c72101ca8e9d4edba1d16b6121ad900ca3936",
    "Violation": "unnecessary",
    "Bug report": "Removed check for is_quantized in dequantize_cpu_or_cuda. This particular PR isn't dispatcher related but does remove the extraneous torch check for a quant tensor since the dispatcher already handles a quantized backend for this particular function",
    "Number of deleted lines": 1,
    "Deleted lines": "-  TORCH_CHECK(!self.is_quantized());",
    "Added lines": ""
},
{
    "Id": 180,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2597d5d72272d196b4cb5442ffc8cde376d1f785",
    "Violation": "insufficient",
    "Bug report": " TorchDynamo: always convert flexiblelayout to be FixedLayout when given a stride_order. For convolution, we always call **require_stride_order** to convert the input to the target stride order,  if the original input's layout is flexiblelayout, there always have a memory copy because the **is_stride_order_storage_and_layout** only checks the init stride order,  I think for flexiblelayout, means it's layout can be changed, if the user gives a stride order, I think we always need to convert the flexiblelayout to be FixedLayout using given strider order.",
    "Number of deleted lines": 3,
    "Deleted lines": "-            if isinstance(\n-                x.get_layout(), FlexibleLayout\n-            ) and is_stride_order_storage_and_layout(x, order):",
    "Added lines": "+            if isinstance(x.get_layout(), FlexibleLayout):"
},
{
    "Id": 181,
    "Library": "pytorch",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e7fc7c732cbde822f9490840704b1f57fe86c50a",
    "Violation": "insufficient",
    "Bug report": "Bugfix for fusion device check",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (!isFusable(producer->node())) {",
    "Added lines": "+    if (!isFusableDevice(producer) || !isFusable(producer->node())) {"
},
{
    "Id": 182,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/67b6c880e39ba02ba53c7d499e45fd136090ee32",
    "Violation": "missing",
    "Bug report": " In tf.map_fn: skip sanity check for shape of first value in elems if it doesn't have a shape attribute. (E.g., this can happen if it's a CompsiteTensor.)",
    "Number of deleted lines": 4,
    "Deleted lines": "-    elems_static_shape = first_elem.shape\n-    if elems_static_shape.ndims is not None and elems_static_shape.ndims < 1:\n-      raise ValueError(\n-          \"Elements in elems must be 1+ dimensional Tensors, not scalars\")",
    "Added lines": "+    if hasattr(first_elem, \"shape\"):\n+      elems_static_shape = first_elem.shape\n+      if elems_static_shape.ndims is not None and elems_static_shape.ndims < 1:\n+        raise ValueError(\n+            \"Elements in elems must be 1+ dimensional Tensors, not scalars\")"
},
{
    "Id": 183,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a607eb012b1bc4f6dbe263ad99caa76d84ae3ab2",
    "Violation": "insufficient",
    "Bug report": "fix output shape check for strided slice always failing when stride != 1 ",
    "Number of deleted lines": 3,
    "Deleted lines": "-    if (attr.ends.h - attr.starts.h != out_shape.h) {\n-    if (attr.ends.w - attr.starts.w != out_shape.w) {\n-    if (attr.ends.c - attr.starts.c != out_shape.c) {",
    "Added lines": "+    if ((attr.ends.h - attr.starts.h + attr.strides.h - 1) / attr.strides.h !=\n+        out_shape.h) {\n+    if ((attr.ends.w - attr.starts.w + attr.strides.w - 1) / attr.strides.w !=\n+        out_shape.w) {\n+    if ((attr.ends.c - attr.starts.c + attr.strides.c - 1) / attr.strides.c !=\n+        out_shape.c) {"
},
{
    "Id": 184,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0197a2d8a3070af763cb67227835ee63df095e6d",
    "Violation": "missing",
    "Bug report": " Add a check to catch out-of-bound access on invalid Graphs. The existing Check trying to catch malformed graph is not robust when an op is registered with an expected number of inputs but has data edges beyond this.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        DCHECK(edge->dst_input() < inputs.size())\n+            << \"Edge \" << edge->DebugString()\n+            << \" is overflowing the expected number of inputs (\"\n+            << node->num_inputs() << \") for node \" << node->DebugString();"
},
{
    "Id": 185,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/abd645085b1dd1496df847b05a1934d471a2f2c0",
    "Violation": "missing",
    "Bug report": " Use the correct device ordinal to check whether the device the executable was built for is equivalent to the device the it will run on. Before this patch, if the device to run on was provided via a stream without setting the device ordinal in the ExecutableRunOptions, we would check the default device against the device the executable was built for.",
    "Number of deleted lines": 5,
    "Deleted lines": "-  // Verify that the device the executable was built for is equivalent to the\n-  // device it will run on.\n-  int run_device_ordinal = run_options.device_ordinal() == -1\n-                               ? backend_->default_device_ordinal()\n-                               : run_options.device_ordinal();",
    "Added lines": "+  // Verify that the device the executable was built for is equivalent\n+  // to the device it will run on.\n+  int run_device_ordinal = run_options.device_ordinal();\n+  if (run_device_ordinal == -1) {\n+    run_device_ordinal = run_options.stream() != nullptr\n+                             ? run_options.stream()->parent()->device_ordinal()\n+                             : backend_->default_device_ordinal();\n+  }"
},
{
    "Id": 186,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/083fd8c4b23104f6b27a871c6469629ace4ee9c3",
    "Violation": "insufficient",
    "Bug report": " Don't check soname on Windows. This allow users to specify a certain CUDA version on Windows again.",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if check_soname and objdump != None:",
    "Added lines": "+        if check_soname and objdump != None and not _is_windows(repository_ctx):"
},
{
    "Id": 187,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/867a918bd3d40afeca6b96430671a098134e7905",
    "Violation": "missing",
    "Bug report": " CUDA Driver: do better error reporting if checking the pointer properties failed. There are many reasons why an operation can fail, propagate the error instead of assuming the cause.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  CHECK(err == cudaSuccess || err == cudaErrorInvalidValue)\n+      << \"Unexpected CUDA error: \" << cudaGetErrorString(err);\n+"
},
{
    "Id": 188,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b234ff0ee4ce87d21a3e5306b678e1fb4b1fedfc",
    "Violation": "missing",
    "Bug report": " Fixed division by zero, by checking the number of GPUs in GenericLayoutOptimizer.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (num_conv2d_gpu == 0) return false;\n+"
},
{
    "Id": 189,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6c472f6632c4864da749e7a4aee8c001a905287f",
    "Violation": "improper",
    "Bug report": " so suggest to use check `CUDA_VERSION` at `12030` here, for `maxSize`, resolved directly in the same way",
    "Number of deleted lines": 4,
    "Deleted lines": "-#if CUDA_VERSION >= 12000\n-#endif  // CUDA_VERSION >= 12000\n-#if CUDA_VERSION >= 12000\n-#endif  // CUDA_VERSION >= 12000",
    "Added lines": "+#if CUDA_VERSION >= 12030\n+#endif  // CUDA_VERSION >= 12030\n+#if CUDA_VERSION >= 12030\n+#endif  // CUDA_VERSION >= 12030"
},
{
    "Id": 190,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9f8ad5ff118166537d42f87f1ee254f83ba553f0",
    "Violation": "improper",
    "Bug report": "Fix CUDA version check (format is 1000 * major + 10 * minor). ",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (CUDA_VERSION < 11300) {",
    "Added lines": "+    if (CUDA_VERSION < 11030) {"
},
{
    "Id": 191,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e5cfbd0eceb4dca98b388b13acff499a5420f863",
    "Violation": "improper",
    "Bug report": "Fix more for cuda version check. ",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (version.ok() && std::get<0>(version.ValueOrDie()) >= 7) {",
    "Added lines": "+    if (version.ok() && version.ValueOrDie().major_version() >= 7) {"
},
{
    "Id": 192,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e1dbfeba8acb1df8f42dfa6f76262f5cb23e1fa1",
    "Violation": "missing",
    "Bug report": "[stream_executor] NFC: Guard new features with CUDA_VERSION check ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#if CUDA_VERSION >= 12000\n+#else\n+    case GpuDriver::MemLocationType::kHost:\n+    case GpuDriver::MemLocationType::kHostNuma:\n+    case GpuDriver::MemLocationType::kHostNumaCurrent:\n+      return CU_MEM_LOCATION_TYPE_INVALID;\n+#endif  // CUDA_VERSION >= 12000\n+#if CUDA_VERSION >= 12000\n+#endif  // CUDA_VERSION >= 12000"
},
{
    "Id": 193,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e05f78a9b688a8ae37b1a03bfc4459e18e3b88e4",
    "Violation": "missing",
    "Bug report": "After synchronizing CUDA device, check for errors. ",
    "Number of deleted lines": 1,
    "Deleted lines": "-  CUresult res = cuCtxSynchronize();",
    "Added lines": "+#include <cuda_runtime.h>\n+  const CUresult res = cuCtxSynchronize();\n+  const auto cudart_error = cudaPeekAtLastError();\n+  if (cudart_error != cudaSuccess) {\n+    LOG(ERROR) << \"could not synchronize on CUDA context: \"\n+               << cudaGetErrorString(cudart_error)\n+               << \" :: \" << port::CurrentStackTrace();\n+    return false;\n+  }"
},
{
    "Id": 194,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/70ade1b64f65d0a2275672d27129627ff116a997",
    "Violation": "missing",
    "Bug report": " Fix defect: shuffle_batch gives ZeroDivisionError when computing capacity stat. * Fix defect: shuffle_batch gives ZeroDivisionError when computing capacity stat. * Cover < case in error checking",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if capacity <= min_after_dequeue:\n+      raise ValueError(\"capacity %d must be bigger than min_after_dequeue %d.\"\n+                       % (capacity, min_after_dequeue))"
},
{
    "Id": 195,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1220ba3ab332d6233a84d660cafb3d4e29958224",
    "Violation": "improper",
    "Bug report": "Fix two potential asynchrony bounds-check bugs in transpose op. ",
    "Number of deleted lines": 3,
    "Deleted lines": "-      const int32 d = Tin(i);\n-      OP_REQUIRES(context, 0 <= d && d < N,\n-  const int32* perm_begin = reinterpret_cast<const int32*>(Vperm.data());",
    "Added lines": "+#include \"tensorflow/core/kernels/bounds_check.h\"\n+      const int32 d = internal::SubtleMustCopy(Tin(i));\n+      OP_REQUIRES(context, FastBoundsCheck(d, N),\n+  // using volatile instead of SubtleMustCopy here so that the\n+  // asynchrony boundary is permutation.\n+  const volatile int32* perm_begin =\n+      reinterpret_cast<const volatile int32*>(Vperm.data());"
},
{
    "Id": 196,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a21ec782601aca6c7e0461093d72596f26229e44",
    "Violation": "improper",
    "Bug report": " Use getattr instead of isinstance in tensor_conversion_registry. Using `isinstance` to check if an object is an instance of a Python `typing.Protocol` instead of using `getattr`/`hasattr` has negative performance implications. This change reverts `tensor_conversion_registry.convert()` to use `getattr` for this reason.",
    "Number of deleted lines": 2,
    "Deleted lines": "-  if isinstance(value, core.TensorProtocol):\n-    return value.__tf_tensor__(dtype, name)",
    "Added lines": "+  overload = getattr(value, \"__tf_tensor__\", None)\n+  if overload is not None:\n+    return overload(dtype, name)  #  pylint: disable=not-callable"
},
{
    "Id": 197,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/15c186bffe51901e4a48b4b6bf1316832533743f",
    "Violation": "improper",
    "Bug report": "Correctly handle the case if static maximum dimension size = 0. ",
    "Number of deleted lines": 2,
    "Deleted lines": "-          if not s or s != maximum_static_shapes[idx][i]:\n-            if s.value:",
    "Added lines": "+          if s is None or s != maximum_static_shapes[idx][i]:\n+            if s.value is not None:"
},
{
    "Id": 198,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e1ad3b74ad44b883c7b3fdc3a19adcea1d28bfbc",
    "Violation": "improper",
    "Bug report": " [XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast is an effective scalar. This short-circuit avoids crashing within last_dimension when attempting to match and either the operand or the result of the bitcast has a shape with rank 0.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (bitcast->shape().rank() == 0) {",
    "Added lines": "+  if (ShapeUtil::IsEffectiveScalar(bitcast->shape())) {"
},
{
    "Id": 199,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2f3b69e4976d3b14eaa6ae070eb68f37d1556d98",
    "Violation": "improper",
    "Bug report": "Changed empty check ",
    "Number of deleted lines": 3,
    "Deleted lines": "-      if (isinstance(checkpointable_object,\n-                     data_structures.CheckpointableDataStructure) and\n-              len(checkpointable_object.variables) == 0):",
    "Added lines": "+      if not checkpointable_object._checkpoint_dependencies:"
},
{
    "Id": 200,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5ed3c7881f1f039b1bb502eb68c65250de3bbac8",
    "Violation": "missing",
    "Bug report": "Fix ThreadPoolHandle 0 nthreads argument. It was reported that a value of 0 leads to a check failure.  Using 0 to indicate `port::MaxParallelism`, for consistency with `Dataset`.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+    // For consistency with Dataset, use MaxParallelism if 0 threads are\n+    // specified.\n+    if (num_threads_ == 0) {\n+      num_threads_ = port::MaxParallelism();\n+    }"
},
{
    "Id": 201,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6381a7b127bd276a3817a93e5423b15a06c33419",
    "Violation": "missing",
    "Bug report": " [tf.data] Add a check for ram_budget == 0 to avoid division by 0 exception when ram_budget is not set.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (ram_budget == 0) {\n+    return;\n+  }"
},
{
    "Id": 202,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7db8e4fbc0be952daea74a2c3f501183d6006e61",
    "Violation": "improper",
    "Bug report": " ENH: check x and y is empty dict",
    "Number of deleted lines": 3,
    "Deleted lines": "-    if y:\n-      if target_keys:\n-    if target_keys:",
    "Added lines": "+    ValueError: if x or y is a empty dict.\n+    if not x:\n+      raise ValueError('x cannot be empty')\n+    if y is None:\n+      if not y:\n+        raise ValueError('y cannot be empty dict, use None instead.')\n+\n+      if target_keys is None:\n+    if target_keys is None:"
},
{
    "Id": 203,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bd1f1ac1fec05d38f1b8fc98f650c1c55ac06790",
    "Violation": "improper",
    "Bug report": "Fix operator check ",
    "Number of deleted lines": 1,
    "Deleted lines": "-      operator_a.is_square is not None and operator_a.is_square is not None):",
    "Added lines": "+      operator_a.is_square is not None and operator_b.is_square is not None):"
},
{
    "Id": 204,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cfb13fa789bcf1cdbbf0fd38cf7568b7098ab99b",
    "Violation": "missing",
    "Bug report": " Added an additional check on the length of the values and boundaries lists.",
    "Number of deleted lines": 2,
    "Deleted lines": "-        `values` do not match.\n-",
    "Added lines": "+        `values` do not match or\n+        the number of elements in the lists does not match.\n+  if len(boundaries) != len(values) - 1:\n+    raise ValueError(\n+        \"The length of boundaries should be 1 less than the length of values\")"
},
{
    "Id": 205,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/28dacabab5aac2963e37e622f4b157cf00d82662",
    "Violation": "misleading",
    "Bug report": " [tf] Explicitly check that runner index is in bounds and runner is available",
    "Number of deleted lines": 2,
    "Deleted lines": "-    DCHECK_GT(runners_.size(), index);\n-    DCHECK(result.has_value());",
    "Added lines": "+    // Out of bounds vector access will throw an exception and anyway will crash\n+    // the binary, prefer a more readable error message.\n+    CHECK_GT(runners_.size(), index)  // Crash OK\n+        << \"runner index is out of bounds: index=\" << index\n+        << \" size=\" << runners_.size();\n+    CHECK(result.has_value())  // Crash OK\n+        << \"runner is not available: index=\" << index;"
},
{
    "Id": 206,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7535f6beb7ba95bf54e1513b0c2c51b844a7a49f",
    "Violation": "missing",
    "Bug report": " Bounds-check node ID before getting it's name. When the edge is either a frame enter or exit edge then DescribeCycle() would segfault.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include \"tensorflow/core/kernels/bounds_check.h\"\n+    if (!FastBoundsCheck(node_id, graph.num_node_ids())) {\n+      return string(\"(null)\");\n+    }"
},
{
    "Id": 207,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/459b4bfe1f73737fae23aa1499b06a69605d0f65",
    "Violation": "missing",
    "Bug report": "Added a check in EagerExecutor to avoid getting invalid range. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (upperbound_id < id) {\n+      return;\n+    }"
},
{
    "Id": 208,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cddca76312f5ae4fb92a101e79eeff6d5ac16932",
    "Violation": "missing",
    "Bug report": "Add check for reading input tensors at an index that is out of range. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (idx < 0 || idx >= node_->inputs->size) {\n+      // If larger, this can be an older model with fewer input tensors than the\n+      // current implementation.\n+      return absl::OutOfRangeError(\"Invalid data index found.\");\n+    }"
},
{
    "Id": 209,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/823b694639a3f49b6adbf9e73a08c529d583878e",
    "Violation": "missing",
    "Bug report": "Add bounds checking when looking at the stack in TF Registry. ",
    "Number of deleted lines": 2,
    "Deleted lines": "-    user_function = stack[2]\n-    location_tag = tf_stack.convert_stack([user_function])[0]",
    "Added lines": "+    stack_index = min(2, len(stack)-1)\n+    if stack_index >= 0:\n+      user_function = stack[stack_index]\n+      location_tag = tf_stack.convert_stack([user_function])[0]\n+    else:\n+      location_tag = \"UNKNOWN\""
},
{
    "Id": 210,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b1c9e600e02b93885dbebfa5dae92436c63d6c03",
    "Violation": "missing",
    "Bug report": "[XLA] Add range check for xla::Array<> indexing. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    DCHECK_LT(index, this->num_elements());"
},
{
    "Id": 211,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1908d7ef706f0f3f8c7a300068355bf795fb3d17",
    "Violation": "improper",
    "Bug report": "Fix out-of-bounds StringPiece access in ForwardNUTF8CharPositions().  Even a simple invocation like 'int p = 0; ForwardNUTF8CharPositions(\"a\", 1, &p);' will cause an invalid access to in[1]. Checking for *pos < size before that access fixes this issue.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    } while (IsTrailByte(in[*pos]) && *pos < size);",
    "Added lines": "+    } while (*pos < size && IsTrailByte(in[*pos]));"
},
{
    "Id": 212,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d414a925a73553e4dd0d559d2d275668a298dab4",
    "Violation": "missing",
    "Bug report": " Check against the size of a std::vector to prevent out-of-boundary access.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    expanded_dim_sizes[sparsity->block_map()->Get(i)] /= block_dim_size;",
    "Added lines": "+    if (original_block_dim < 0 || original_block_dim >= total_dims) {\n+      return absl::nullopt;\n+    }\n+\n+    int mapped_block_dim = sparsity->block_map()->Get(i);\n+    if (mapped_block_dim < 0 || mapped_block_dim >= total_dims) {\n+      return absl::nullopt;\n+    }\n+    expanded_dim_sizes[mapped_block_dim] /= block_dim_size;"
},
{
    "Id": 213,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/aa54f547f04c3007b26df2379c6cf5f081948d0b",
    "Violation": "missing",
    "Bug report": " Updated the check_numerics function to also validate the gradient corresponding to the tensor it's validating",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return grad",
    "Added lines": "+  return array_ops.check_numerics(\n+      grad, \"Not a number (NaN) or infinity (Inf) values detected in gradient.\")"
},
{
    "Id": 214,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/236660d0fccff6f59f29a1936dc731d783722e28",
    "Violation": "missing",
    "Bug report": " [XLA:GPU] Fix host conv checker canonicalization for f16 and nans. The GPU-side checker is correct, but the host-side checker was canonicalizing nan to F16_MAX.  The effect of this is that you'd get a \"conv mismatch!\" error but no description of exactly what mismatched.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      if (std::isnan(a)) {\n+        return a;\n+      }"
},
{
    "Id": 215,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8c6f391a2282684a25cbfec7687bd5d35261a209",
    "Violation": "missing",
    "Bug report": " [lite] Add check for bias_size is zero to avoid division by zero. This shouldn't happen for properly converted models. Just safety check",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (bias_size == 0) return;"
},
{
    "Id": 216,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/582bf0d3ac33fc10156f737c0d42f3adee54409a",
    "Violation": "insufficient",
    "Bug report": " Update the tflite model \"buffers\" field checking rule. If we don't use \"--force-empty-vectors\" flag[1] for flatc, the buffers might be a null ptr if we serialize a model with zero buffers size(e.g. all ops in model doesn't use const weights in model). This commit relaxs the \"buffers\" null ptr checking for this situation, and also updates the \"subgraphs\" checking for null ptr dereference.",
    "Number of deleted lines": 8,
    "Deleted lines": "-      if (tensor->buffer() == 0) return kTfLiteOk;\n-      if (tensor->buffer() >= buffers->size()) {\n-            i, tensor->buffer(), buffers->size());\n-  if (subgraphs->size() == 0) {\n-  if (!buffers) {\n-    TF_LITE_REPORT_ERROR(error_reporter_, \"No buffers in the model.\\n\");\n-    return cleanup_and_error();\n-  }",
    "Added lines": "+      if (tensor->buffer() == 0) {\n+        return kTfLiteOk;\n+      }\n+      if (!buffers || tensor->buffer() >= buffers->size()) {\n+            i, tensor->buffer(), (buffers) ? buffers->size() : 0);\n+  if (!subgraphs || subgraphs->size() == 0) {"
},
{
    "Id": 217,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7008e41f183ae9de3f4656067932b36afa822ef2",
    "Violation": "missing",
    "Bug report": " Fix the check for empty reduction indices. In the general case indices can be any rank.",
    "Number of deleted lines": 2,
    "Deleted lines": "-  *indices_is_empty =\n-      reduction_indices_tensor.tensor_shape().dim(0).size() == 0;",
    "Added lines": "+  *indices_is_empty = false;\n+  for (const auto& dim : reduction_indices_tensor.tensor_shape().dim()) {\n+    if (dim.size() == 0) {\n+      *indices_is_empty = true;\n+      break;\n+    }\n+  }"
},
{
    "Id": 218,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/551a90f2e3d20420d68a2796d19f1c42b6636e0d",
    "Violation": "missing",
    "Bug report": " Add checks in ReduceWindowOpOnTensorsConversion. The pattern does not support ops with non-zero padding config. Add a check to prevent unexpected lowering. It is not easy to add tests because other patterns will convert body ops, and it causes issues like invalid IRs.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (op.padding() && !isSplatValue(*op.padding(), 0)) {\n+      return rewriter.notifyMatchFailure(op, \"require paddings are all zero\");\n+    }\n+"
},
{
    "Id": 219,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f6f62119587baf8ccb7378ceac86bacd2db2863d",
    "Violation": "missing",
    "Bug report": "Add missing validation in maxpooling_op.cc ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    // Given access patterns in SpatialMaxPoolWithArgMaxHelper, these tensors\n+    // must have elements.\n+    OP_REQUIRES(\n+        context, tensor_out_arg_max.NumElements() > 0,\n+        errors::InvalidArgument(\"tensor_out_arg_max must not be empty, got \",\n+                                tensor_out_arg_max.DebugString()));\n+    OP_REQUIRES(context, out_backprop.NumElements() > 0,\n+                errors::InvalidArgument(\"out_backprop must not be empty, got \",\n+                                        out_backprop.DebugString()));"
},
{
    "Id": 220,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b86513673b98ac6c4458033fcda718365539afae",
    "Violation": "missing",
    "Bug report": "added check for zero stride values to strided slice ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (attr.strides.h == 0 || attr.strides.w == 0 || attr.strides.c == 0) {\n+      return InvalidArgumentError(\"stride values must be non-zero\");\n+    }"
},
{
    "Id": 221,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4377a561b7757ed83757f07532e6564c42c286ba",
    "Violation": "missing",
    "Bug report": " Add a check for group size when sorting grouped AllReduces within a block.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        // Maintain relative order of ALLReduces within the block.\n+                    if (lhs.empty() || rhs.empty()) {\n+                      // Skip order check if either group is empty.\n+                      return false;\n+                    }"
},
{
    "Id": 222,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/31bd5026304677faa8a0b77602c6154171b9aec1",
    "Violation": "missing",
    "Bug report": " Prevent check fail in FFT ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(ctx, temp_shape.num_elements() > 0,\n+                errors::InvalidArgument(\"Obtained a FFT shape of 0 elements: \",\n+                                        temp_shape.DebugString()));"
},
{
    "Id": 223,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1c56f53be0b722ca657cbc7df461ed676c8642a2",
    "Violation": "missing",
    "Bug report": "Fix a check fail in Fast Fourier implementation ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include \"tensorflow/core/platform/errors.h\"\n+    OP_REQUIRES(ctx, full_fft_shape.num_elements() > 0,\n+                errors::InvalidArgument(\"Obtained a FFT shape of 0 elements: \",\n+                                        full_fft_shape.DebugString()));"
},
{
    "Id": 224,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/25bae42b3022b00788a29ae6c400922c31f88231",
    "Violation": "insufficient",
    "Bug report": " Add additional length check for inputs ",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if all(shape is not None for shape in shapes_value):",
    "Added lines": "+  if len(shapes_value) != 0 and all(shape is not None for shape in shapes_value):"
},
{
    "Id": 225,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e07e48b2e0908333a36f1c5726a9406a83b3ec90",
    "Violation": "missing",
    "Bug report": "Added a check on literal_.has_value() to avoid segfault. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (!literal_.has_value()) {\n+      return \"{...}\";\n+    }"
},
{
    "Id": 226,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cc560f64b6e3e6724517757e9789c52cde224ee9",
    "Violation": "missing",
    "Bug report": " Profiler: restore correct behavior of StartTracing with empty workers list. absl::StrSplit behaves differently from str_util::Split when the passed string is empty. Restore previous behavior by explicitly checking for an empty string.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  std::vector<tensorflow::string> hostnames = absl::StrSplit(workers_list, ',');",
    "Added lines": "+  std::vector<tensorflow::string> hostnames;\n+  if (!workers_list.empty()) {\n+    hostnames = absl::StrSplit(workers_list, ',');\n+  }"
},
{
    "Id": 227,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/80bb2f5511e7d2d386c79da52ff517691e19ac54",
    "Violation": "missing",
    "Bug report": " Add check condition for large values of range_max, which is causing session abort.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  # Limiting to Max int32 value\n+  if range_max > 2147483647:\n+    raise ValueError(f'Value of range_max:{range_max} is too large to handle')"
},
{
    "Id": 228,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e6390bc13471f28f211cab874cc49a123505dc3e",
    "Violation": "missing",
    "Bug report": " Update histogram_ops.py. Added the condition to check the negative value of nbins input",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    ValueError: If the value of nbins is negative.\n+  if nbins < 0:\n+    raise ValueError(\"nbins should be a positive number.\")\n+    \n+    ValueError: If the value of nbins is negative.\n+  if nbins < 0:\n+    raise ValueError(\"nbins should be a positive number.\")\n+"
},
{
    "Id": 229,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/43a8963c73718f97a4425722a65b611d2ef0b69f",
    "Violation": "missing",
    "Bug report": "Added non-negative check for n. ",
    "Number of deleted lines": 1,
    "Deleted lines": "-      not `-1`, or `norm` is not `None` or `'ortho'`.",
    "Added lines": "+  if n is not None and n < 1:\n+    raise ValueError(\"n should be an integer greater than 1 or None\")\n+      not `-1`, `n` is not `None` or greater than 0, \n+      or `norm` is not `None` or `'ortho'`."
},
{
    "Id": 230,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4ea68093eeaf4c4157368668afd7f809b806a504",
    "Violation": "missing",
    "Bug report": "Add negative parameter validation to convolution layers. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if filters < 0:\n+      raise ValueError(\"Recieved a negative value for `filters`,\n+                       \"was expecting a positive value.\")"
},
{
    "Id": 231,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1223335a8d34a8ce656dbd10b2a236ef6204ff47",
    "Violation": "missing",
    "Bug report": "Add negative parameter validation for recurrent layers. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if units < 0:\n+      raise ValueError(\"Received a negative value for `units`, \",\n+                       \"expected a positive value.\")\n+    if units < 0:\n+      raise ValueError(\"Received an negative value for `units`, \"\n+                       \"expected a positive value.\")\n+    if units < 0:\n+      raise ValueError(\"Received a negative value for `units`, \"\n+                       \"expected a postiive value.\")"
},
{
    "Id": 232,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/64afe2d199ec4513223bbf5176835bf681cf056b",
    "Violation": "missing",
    "Bug report": "Add negative parameter validation to Core Keras layers. ",
    "Number of deleted lines": 1,
    "Deleted lines": "-",
    "Added lines": "+    if isinstance(rate, (int, float)) and rate < 0:\n+      raise ValueError(\"Invalid value received for `rate`, expected \"\n+                       \"a value between 0 and 1.\")\n+    if not isinstance(n, int):\n+      raise TypeError(\"Expected an integer value for `n`.\")\n+    \n+    if self.units < 0:\n+      raise ValueError(f\"Received an invalid value for `units`, expected\n+                       f\"a positive integer, got {units}.\")"
},
{
    "Id": 233,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/199f1ff12a28d571100b323ec54a5eee47078d8b",
    "Violation": "missing",
    "Bug report": " Add necessary check in fft ops to fix crash. This PR tries to address the issue raised in 55263 where tf.single.rfft2d will crash when length contains negative value.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        OP_REQUIRES(\n+            ctx,\n+            fft_length_as_vec(i) >= 0,\n+            errors::InvalidArgument(\n+                \"fft_length[\" , i,\n+                \"] must >= 0, but got: \", fft_length_as_vec(i)));"
},
{
    "Id": 234,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/685418cd85e09bc2117fa15bc1b6a75d21248348",
    "Violation": "missing",
    "Bug report": "maxpooling op should check that ksize must be positive. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      OP_REQUIRES(\n+          context,\n+          ksize_[0] > 0 && ksize_[1] > 0 && ksize_[2] > 0 && ksize_[3] > 0,\n+          errors::InvalidArgument(\"Sliding window ksize must be positive.\"));\n+    OP_REQUIRES(\n+        context, ksize[0] > 0 && ksize[1] > 0 && ksize[2] > 0 && ksize[3] > 0,\n+        errors::InvalidArgument(\"Sliding window ksize must be positive.\"));"
},
{
    "Id": 235,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/076f909b70b251daea6c443c9b1929b9745aed20",
    "Violation": "improper",
    "Bug report": "fix boolean expression in length check ",
    "Number of deleted lines": 1,
    "Deleted lines": "-    OP_REQUIRES(ctx, length,",
    "Added lines": "+    OP_REQUIRES(ctx, length > 0,"
},
{
    "Id": 236,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3acc8eaf602b3e9a009f54e1e0164644dd793831",
    "Violation": "missing",
    "Bug report": "Add sanity check for resize-bilinear input shape. ",
    "Number of deleted lines": 1,
    "Deleted lines": "-  const int32* size_data = GetTensorData<int32>(size);",
    "Added lines": "+  const int32* size_data = GetTensorData<int32>(size);\n+  // Sanity check, the up/down sampling size should always be positive.\n+  TF_LITE_ENSURE(context, size_data[0] > 0);\n+  TF_LITE_ENSURE(context, size_data[1] > 0);"
},
{
    "Id": 237,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fffbe5a26da2d6fab5a3eb648cefef49db4d38de",
    "Violation": "missing",
    "Bug report": " Check if the session has been deleted before releasing a callable. In some versions of Python, the Session._session field may be cleared (in `Session.__del__()`) before a callable that has a reference to that Session is deleted. Add a defensive check in the `Session._Callable.__del__()` method.",
    "Number of deleted lines": 1,
    "Deleted lines": "-      if self._handle is not None:",
    "Added lines": "+      # NOTE(mrry): It is possible that `self._session.__del__()` could be\n+      # called before this destructor, in which case `self._session._session`\n+      # will be `None`.\n+      if self._handle is not None and self._session._session is not None:"
},
{
    "Id": 238,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9ce847ed140702d1dd4cb204a8afe0ffedb70b15",
    "Violation": "unnecessary",
    "Bug report": " Remove a few check ops that no longer need to run in tf.Variable's constructor. VarHandleOp ensures there is no sharing. These aren't a huge part of startup time for replicated models, but there's still no reason to run them.",
    "Number of deleted lines": 16,
    "Deleted lines": "-from tensorflow.python.ops import gen_logging_ops\n-    # We do not want two distinct ResourceVariable objects for the same\n-    # underlying resource in the runtime.\n-    # When in eager mode, explicitly ensure so here. When in graph mode, it's\n-    # ensured by always generating different variable names.\n-    exists = gen_resource_variable_ops.var_is_initialized_op(handle)\n-\n-    # We create an assert Op instead of checking right away in order to be\n-    # compatible with ASYNC execution mode. Further, since not all devices\n-    # support string tensors, we encode the assertion string in the Op name\n-    gen_logging_ops._assert(  # pylint: disable=protected-access\n-        math_ops.logical_not(exists), [exists],\n-        name=\"EagerVariableNameReuse\")\n-\n-          shared_name = context.shared_name()\n-          shared_name = context.shared_name()",
    "Added lines": "+from tensorflow.python.framework import errors\n+  if not graph_mode:\n+    if shared_name is not None:\n+      raise errors.InternalError(\n+          \"Using an explicit shared_name is not supported executing eagerly.\")\n+    shared_name = context.shared_name()\n+\n+          shared_name = None  # Never shared\n+          shared_name = None  # Never shared"
},
{
    "Id": 239,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ebeb598c2d1f341d6d641bf58c370cf7b43f6e37",
    "Violation": "missing",
    "Bug report": " Correctly check shape not None in Keras add_weight. When calling Keras add_weight with a np list, as written the `shape or ()` \"trick\" results in the following exception: \"\"\"ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\"\"\" This change fixes the problem by using an explicit `if`.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    shape = shape or ()",
    "Added lines": "+    if shape is None:\n+      shape = ()"
},
{
    "Id": 240,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c7c4a42c4372ca560ea415fe3a798e18286cedec",
    "Violation": "improper",
    "Bug report": "Fix an error in keras input_layer.Input() dtype type checking. ",
    "Number of deleted lines": 2,
    "Deleted lines": "-    elif input_tensor and input_tensor.dtype != dtype:\n-      raise ValueError('`input_tensor.dtype` differs from `dtype`.')",
    "Added lines": "+    elif input_tensor is not None and input_tensor.dtype != dtype:\n+      raise ValueError('`input_tensor.dtype` differs from `dtype`: %s vs. %s' %\n+                       (input_tensor.dtype, dtype))"
},
{
    "Id": 241,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bc7b64fe998cb0f118eace5bc29b52554eeda3f1",
    "Violation": "missing",
    "Bug report": " Added back the channel dimension check as a known channel dimension is required by creating beta.",
    "Number of deleted lines": 3,
    "Deleted lines": "-      channels = array_ops.shape(inputs)[-1]\n-      outputs = array_ops.reshape(outputs, array_ops.shape[original_inputs])\n-                        functools.reduce(lambda x, y: x * y, spatial_dims)])",
    "Added lines": "+      channels = inputs.get_shape()[-1].value\n+      if channels is None:\n+        raise ValueError('`C` dimension must be known but is None')\n+      outputs = array_ops.reshape(outputs, array_ops.shape(original_inputs))\n+                         functools.reduce(lambda x, y: x * y, spatial_dims)])"
},
{
    "Id": 242,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a5b8d6c4694e4cd3e3cc4a162053ab0dfa6e174f",
    "Violation": "insufficient",
    "Bug report": "Relax the check for whether the relevant aggregation dimensions are known ahead of time.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if x_shape.is_fully_defined():",
    "Added lines": "+    if all(x_shape[d].value is not None for d in axes):"
},
{
    "Id": 243,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0d65cfaab050295c311d9f2fb28388435359db27",
    "Violation": "insufficient",
    "Bug report": " Add an additional NoneType check when converting a traced tensor to a `KerasTensor`.",
    "Number of deleted lines": 1,
    "Deleted lines": "-      if (type_spec.dtype == dtypes.int32 and type_spec.shape.rank < 2):",
    "Added lines": "+      if (type_spec.dtype == dtypes.int32 and type_spec.shape.rank is not None\n+          and type_spec.shape.rank < 2):"
},
{
    "Id": 244,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/31849c61e0432009baabdfafc2ec1a1aed1a40e8",
    "Violation": "insufficient",
    "Bug report": " Small change in tf.nn.sufficient_statistics to guard against unknown shapes. Use is_fully_defined instead of checking shape.dims[d] as the dims variable may be None, if the rank is unknown.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if all(x_shape.dims[d].value is not None for d in axes):",
    "Added lines": "+    if x_shape.rank is not None and all(\n+        x_shape.dims[d].value is not None for d in axes):"
},
{
    "Id": 245,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/30bd9d5bcc64097d21872486a5726d756ed7067b",
    "Violation": "insufficient",
    "Bug report": " Explicitly handle Tensors in start & stop. The current check was doing a identity check in order to handle both tensors and integers. This becomes problematic when enabling tensor equality. Instead we explicitly check for Tensor type and only compare with sys.maxsize for non-Tensors.",
    "Number of deleted lines": 2,
    "Deleted lines": "-      if s.start is not None and s.start is not sys.maxsize:\n-      if s.stop is not None and s.stop != sys.maxsize:",
    "Added lines": "+      if s.start is not None and (isinstance(s.start, ops.Tensor) or\n+                                  s.start != sys.maxsize):\n+      if s.stop is not None and (isinstance(s.stop, ops.Tensor) or\n+                                 s.stop != sys.maxsize):"
},
{
    "Id": 246,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fb1c4cd8283f262bca95ccd04df6f9eb4ae1da0c",
    "Violation": "missing",
    "Bug report": "Add None check for seq_len_mask before reshape.",
    "Number of deleted lines": 4,
    "Deleted lines": "-    seq_len_mask = array_ops.reshape(\n-        seq_len_mask,\n-        array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), 0))\n-    return m * seq_len_mask if memory_sequence_length is not None else m",
    "Added lines": "+    if memory_sequence_length is not None:\n+      seq_len_mask = array_ops.reshape(\n+          seq_len_mask,\n+          array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), 0))\n+      return m * seq_len_mask\n+    else:\n+      return m"
},
{
    "Id": 247,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a0ca4bcb81dfd07fdb1c7872b5852f84cfc1a081",
    "Violation": "improper",
    "Bug report": "Fix separable convolution bias check",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if self.bias:",
    "Added lines": "+    if self.bias is not None:"
},
{
    "Id": 248,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1ff493ed1a2059f82f7607a7f0a0aa2ce8d5a542",
    "Violation": "improper",
    "Bug report": "Replace a defensive check with TF_RET_CHECK",
    "Number of deleted lines": 4,
    "Deleted lines": "-    if (!device_name.empty()) {\n-      // TODO(sanjoy): Figure out if this is necessary.\n-      device_names_set.insert(device_name);\n-    }",
    "Added lines": "+    TF_RET_CHECK(!device_name.empty());\n+    device_names_set.insert(device_name);"
},
{
    "Id": 249,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/201982013046116767545cda18137b38abb39468",
    "Violation": "missing",
    "Bug report": "toco: Fix missing check for buffer in ResizeBilinear.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (!output_size_array.buffer) {\n+    return;\n+  }"
},
{
    "Id": 250,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c676a2d7ce8884aad59ca9cd5f45e9b851574cac",
    "Violation": "missing",
    "Bug report": " [tensorflow] Add a check that strided slice op strides argument has reasonable size",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return errors::InvalidArgument(\"Unexpected negative dense.dims\");",
    "Added lines": "+    return errors::InvalidArgument(\"Unexpected negative dense.dims: %d\",\n+                                   dense->dims);\n+  }\n+\n+  if (dense->dims >= 1024) {\n+    // We do not expect to see tensors with rank >= 1024, it must mean that\n+    // there is a bug somewhere.\n+    return errors::InvalidArgument(\"Unexpected large dense.dims: %d\",\n+                                   dense->dims);"
},
{
    "Id": 251,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f61175812426009a4c96e51befb2951612990903",
    "Violation": "missing",
    "Bug report": "To add a check of input_dims greater than zero in embedding layers. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if self.input_dim <= 0:\n+      raise ValueError('The argument `input_dim` should be greater than zero. '\n+                       'Received: %s' % input_dim)"
},
{
    "Id": 252,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f2a673bd34f0d64b8e40a551ac78989d16daad09",
    "Violation": "missing",
    "Bug report": "Add missing validation to matrix_diag_op.cc",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      OP_REQUIRES(context, diag_index.NumElements() > 0,\n+                  errors::InvalidArgument(\n+                      \"Expected diag_index to have at least 1 element\"));\n+      OP_REQUIRES(context, diag_index.NumElements() > 0,\n+                  errors::InvalidArgument(\n+                      \"Expected diag_index to have at least 1 element\"));"
},
{
    "Id": 253,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a0dc73569fc193c1ce26a7bd2d4a8776e7b813ac",
    "Violation": "missing",
    "Bug report": "add check for empty cs_prev_tensor",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(ctx, \n+        cs_prev_tensor->dim_size(0) > 0 && cs_prev_tensor->dim_size(1) > 0,\n+                errors::InvalidArgument(\"cs_prev_tensor is empty, has shape: (\",\n+                            cs_prev_tensor->dim_size(0), \",\", cs_prev_tensor->dim_size(1), \").\"));"
},
{
    "Id": 254,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/48d3e51a1bd128554dd129251a51b6e12918a604",
    "Violation": "missing",
    "Bug report": "Add a check to HandleFromInput to ensure that the resource isn't empty. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+// TODO(b/228388547) users of this method should be migrated to the one below.\n+  if (tensor->NumElements() == 0) {\n+    return errors::InvalidArgument(\"Empty resouce handle\");\n+  }"
},
{
    "Id": 255,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/258233804f2bc92b4bdb9714b396aed34b53ff0d",
    "Violation": "missing",
    "Bug report": " sanity check of empty tensor on avgpool3d_grad",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      // For empty tensor, avg_pool_3d_grad in oneDNN doesn't handle this case\n+      if (orig_input_tensor.NumElements() == 0 ||\n+          grad_tensor.NumElements() == 0)\n+        return;\n+      "
},
{
    "Id": 256,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/532f5c5a547126c634fefd43bbad1dc6417678ac",
    "Violation": "improper",
    "Bug report": "Prevent nullptr deref in validation of indexes in map ops. ",
    "Number of deleted lines": 6,
    "Deleted lines": "-      return Status(errors::InvalidArgument(\n-          \"' was already initialized '\", dtypes_.size(), \"'.\"));\n-      return Status(\n-          errors::InvalidArgument(\"Indices are not strictly ordered\"));\n-      return Status(errors::ResourceExhausted(\n-          \"'.\"));",
    "Added lines": "+      return errors::InvalidArgument(\n+          \"' was already initialized '\", dtypes_.size(), \"'.\");\n+    if (indices.NumElements() == 0) {\n+      return errors::InvalidArgument(\"Indices are empty\");\n+    }\n+\n+      return errors::InvalidArgument(\"Indices are not strictly ordered\");\n+      return errors::ResourceExhausted(\n+          \"'.\");"
},
{
    "Id": 257,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/102cacf28ad5a9e7f00b5a195d1995ead8870006",
    "Violation": "missing",
    "Bug report": "Add missing validation to maxpooling_op.cc",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n+                errors::InvalidArgument(\"tensor_in must not be empty\"));\n+    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n+                errors::InvalidArgument(\"tensor_out must not be empty\"));\n+    OP_REQUIRES(context, tensor_in.dims() == 4,\n+                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n+    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n+                errors::InvalidArgument(\"tensor_in must not be empty\"));"
},
{
    "Id": 258,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/27bd8aaa7b58d2591fed43a6c245f3037664cfb1",
    "Violation": "missing",
    "Bug report": "Fix another Eigen missing validation ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+    OP_REQUIRES(ctx, in0.NumElements() > 0,\n+                errors::InvalidArgument(\"In[0] must not be an empty tensor: \",\n+                                        in0.DebugString()));\n+\n+    OP_REQUIRES(ctx, in1.NumElements() > 0,\n+                errors::InvalidArgument(\"In[1] must not be an empty tensor: \",\n+                                        in1.DebugString()));"
},
{
    "Id": 259,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dedac5053f1ca2d6a7820e330714e50d2d724cee",
    "Violation": "missing",
    "Bug report": " Fix edge case bug in handling FP16 weights in XNNPACK delegate. Quasi-static tensors may become subgraph outputs after partitioning; we need to explicitly exclude them from outputs and treat as static tensors.",
    "Number of deleted lines": 3,
    "Deleted lines": "-    const std::unordered_set<int> outputs(\n-        &params->output_tensors->data[0],\n-        &params->output_tensors->data[params->output_tensors->size]);",
    "Added lines": "+    std::unordered_set<int> outputs;\n+    for (int o = 0; o < params->output_tensors->size; o++) {\n+      const int output_tensor_idx = params->output_tensors->data[o];\n+      // Exclude quasi-static tensors which may have become subgraph outputs\n+      // after partitioning.\n+      if (delegate->static_unpacked_data_map_.count(output_tensor_idx) == 0) {\n+        outputs.insert(output_tensor_idx);\n+      }\n+    }"
},
{
    "Id": 260,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ce589223a5fa78cb12efaf1efd1d8d0e5507bd08",
    "Violation": "missing",
    "Bug report": " Update nn_ops.py. Added check for pooling_ratio",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if pooling_ratio < 1.0:\n+    raise ValueError(\"pooling_ratio should be >= 1.0.\")"
},
{
    "Id": 261,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/63feaf321165e1e2795f43e3834c007364921df6",
    "Violation": "missing",
    "Bug report": "Add check for raster bits.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    // Stop load if no images are detected or the allocation of the last image\n+    // buffer was failed.\n+    if (gif_file->ImageCount <= 0 ||\n+        gif_file->SavedImages[gif_file->ImageCount - 1].RasterBits == NULL) {\n+    }\n+"
},
{
    "Id": 262,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e5b0eec199c2d03de54fd6a7fd9275692218e2bc",
    "Violation": "missing",
    "Bug report": " [lite] Add validation check for dilation height/width to be positive integers.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TF_LITE_ENSURE(context, params->dilation_height_factor > 0);\n+  TF_LITE_ENSURE(context, params->dilation_width_factor > 0);"
},
{
    "Id": 263,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5cedb0427bd4db4117182da8bc0680dd555b4f49",
    "Violation": "missing",
    "Bug report": "Add checks for dilation_rate. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TFLITE_DCHECK_GE(dilation_width_factor, 1);\n+  TFLITE_DCHECK_GE(dilation_height_factor, 1);"
},
{
    "Id": 264,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/55aec0a33011773240f6696393952c984ca8de16",
    "Violation": "insufficient",
    "Bug report": " Add explicit not-None checks for the height and width in `resize_images()`. This was previously raising a `FutureWarning` when the height and/or width were dynamic.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if width == new_width_const and height == new_height_const:",
    "Added lines": "+  if new_width_const is not None and new_height_const is not None and (\n+      width == new_width_const and height == new_height_const):"
},
{
    "Id": 265,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ab60b0ee51a8924a0f02b0152cd6a78ba64d3e94",
    "Violation": "missing",
    "Bug report": " [tfg] Fix named-attribute token check. Since the name tokens are being indexed directly, we should check that list of tokens is not empty to prevent an out-of-bounds error.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include <vector>\n+    TF_RET_CHECK(!name_tokens.empty());"
},
{
    "Id": 266,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c6899c721f3a4b4f2e71ae4e6d1767341112ff93",
    "Violation": "missing",
    "Bug report": "bug fix when iterators stops at multiple of batch_size ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+          if i == 0:\n+            raise"
},
{
    "Id": 267,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/66e0cb1d9afd251931f4f920c5d7bd638bc882b4",
    "Violation": "missing",
    "Bug report": " validate clip_norm argument in clip_by_norm API. The API clip_by_norm have argument clip_norm which accepts  0-D (scalar) `Tensor` > 0 . But if we pass -ve value for this argument then its not raising intended error and converting the input tensor into Negative which IMO is wrong. Hence I am adding validation code for -ve values to raise value error.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if clip_norm < 0:\n+      raise ValueError('clip_norm should be a 0-D (scalar) Tensor > 0')"
},
{
    "Id": 268,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d5862d423742ec26c46737d4526eca3b8b8a0d9b",
    "Violation": "missing",
    "Bug report": " [TFLite] Add check in Softmax reference function to ensure exponent is within valid range. * Add check to ensure the exponent does not cause an overflow in gemmlowp::RoundingDivideByPOT",
    "Number of deleted lines": 2,
    "Deleted lines": "-            (shifted_scale * exp_in_0).raw(),\n-            num_bits_over_unit + 31 - (sizeof(OutputT) * 8));",
    "Added lines": "+    const int exponent = num_bits_over_unit + 31 - (sizeof(OutputT) * 8);\n+    TFLITE_CHECK(0 <= exponent && exponent <= 31);\n+\n+            (shifted_scale * exp_in_0).raw(), exponent);"
},
{
    "Id": 269,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/81ff894c113a5912ba52078ac27e36d06831112e",
    "Violation": "missing",
    "Bug report": "[XLA] Add bounds checks to xla::Array::Slice. To guard against specifying limits that are out of bounds, which ends up touching OOB data.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      CHECK_GE(starts[i], 0);\n+      CHECK_LE(limits[i], dim(i));"
},
{
    "Id": 270,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cb164786dc891ea11d3a900e90367c339305dc7b",
    "Violation": "improper",
    "Bug report": " Properly handle the case where SpecializeType() returns an error `Status`. If the error case in `SpecializeType()` is reached, then we would get a crash when trying to access the value of an errorenous `StatusOr` object",
    "Number of deleted lines": 1,
    "Deleted lines": "-  DCHECK(ret.status().ok()) << \"while instantiating types: \" << ret.status();",
    "Added lines": "+  if (!ret.status().ok()) {\n+    construction_status_ = ret.status();\n+    return;\n+  }"
},
{
    "Id": 271,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84d7bf6f64fd9c8677f7f26511ce3031fe8d35a6",
    "Violation": "missing",
    "Bug report": "Add is_numeric to dtypes.cc to check whether a data type is numeric ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      .def_property_readonly(\n+          \"is_numeric\",\n+          [](tensorflow::DataType self) {\n+            return tensorflow::DataTypeIsNumeric(tensorflow::BaseType(self));\n+          },\n+          \"Returns whether this is a numeric data type.\")"
},
{
    "Id": 272,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cd34289b744040974ebe81e1b1e88f1c752d68e0",
    "Violation": "missing",
    "Bug report": "Update types.h to check if a data type is numeric ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+// Returns true iff 'dt' is a numeric type.\n+inline bool DataTypeIsNumeric(DataType dt) {\n+  return kNumberTypes.Contains(dt);\n+}\n+"
},
{
    "Id": 273,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/43fd10302bcc8447e7a7205bae848a3a88624775",
    "Violation": "missing",
    "Bug report": "Return error on invalid input in tfl.atan2_custom",
    "Number of deleted lines": 1,
    "Deleted lines": "-    default:",
    "Added lines": "+    default: {\n+      return TfLiteStatus::kTfLiteError;\n+    }"
},
{
    "Id": 274,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/00517642a356c5e04f009ea61c74638d89746392",
    "Violation": "missing",
    "Bug report": "Return error on invalid input in tfl.splitv",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      return kTfLiteError;\n+    return kTfLiteError;"
},
{
    "Id": 275,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/40c7fe94824100338ef0c495143b26501b1c367e",
    "Violation": "missing",
    "Bug report": "Return error on invalid input in tfl.topkv2 ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      return kTfLiteError;"
},
{
    "Id": 276,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b4aadb17b7aa5ea926b5220008e41f33e582baed",
    "Violation": "missing",
    "Bug report": "Return error on invalid input in tfl.where",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      return kTfLiteError;\n+        return kTfLiteError;\n+      return kTfLiteError;"
},
{
    "Id": 277,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ef049bdfc4f307c8b3a9dc480a90a5ff287f3d55",
    "Violation": "missing",
    "Bug report": "Add check for ResizeOutput return value in range.cc",
    "Number of deleted lines": 1,
    "Deleted lines": "-    ResizeOutput(context, start, limit, delta, output);",
    "Added lines": "+    TF_LITE_ENSURE_OK(context,\n+                      ResizeOutput(context, start, limit, delta, output));"
},
{
    "Id": 278,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1707ed9b9b0cc5cb02df22a06718c9c738825d39",
    "Violation": "missing",
    "Bug report": " Add a check to make sure that the allocation before an Evict() is not a prefetch.",
    "Number of deleted lines": 3,
    "Deleted lines": "-  // TODO(b/306478911): prev_allocation can never be a prefetch, or we would be\n-  // using an incorrect start time (we would need to wait until the copies\n-  // finish)",
    "Added lines": "+  // We do not ever expect an Evict() to be immediately proceeded by a prefetch.\n+  // If that case ever occurs, the eviction_exclusive_start_time below will be\n+  // calculated incorrectly, as it will need to come after the prefetch finishes\n+  // coping data.\n+  CHECK(!prev_allocation->is_copy_like_allocation())\n+      << \"Evict has been given copy-like previous allocation.\\nEvict \"\n+         \"candidate:\\n\"\n+      << request.allocation_value->ToString() << \"\\nPrevious allocation:\\n\"\n+      << prev_allocation->ToString();"
},
{
    "Id": 279,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f636be3bb1f556c15dba3028e61a8969d90dadd9",
    "Violation": "misleading",
    "Bug report": "Return error on invalid input in tfl.sign_custom",
    "Number of deleted lines": 5,
    "Deleted lines": "-    default:\n-      TF_LITE_KERNEL_LOG(\n-          context,\n-          \"Unsupported datatype for atan2 output: %s\",\n-          TfLiteTypeGetName(output->type));",
    "Added lines": "+    default: {\n+      TF_LITE_KERNEL_LOG(context, \"Unsupported datatype for sign output: %s\",\n+                         TfLiteTypeGetName(output->type));\n+      return TfLiteStatus::kTfLiteError;\n+    }"
},
{
    "Id": 280,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84a1cf61dd7239aa5d682083d34e0f7c99039734",
    "Violation": "misleading",
    "Bug report": " [XLA] Do not suggest trying to use TF_XLA_FLAGS when failing to parse XLA_FLAGS. The error can be very misleading, as we never check whether the new flag is actually supported by TF_XLA_FLAGS.",
    "Number of deleted lines": 18,
    "Deleted lines": "-\n-    // Some flags are set on XLA_FLAGS, others on TF_XLA_FLAGS.  If we find an\n-    // unrecognized flag, suggest the alternative.\n-    std::string alternate_envvar;\n-    if (envvar == \"TF_XLA_FLAGS\") {\n-      alternate_envvar = \"XLA_FLAGS\";\n-    } else if (envvar == \"XLA_FLAGS\") {\n-      alternate_envvar = \"TF_XLA_FLAGS\";\n-    }\n-    std::string did_you_mean;\n-    if (!alternate_envvar.empty()) {\n-      did_you_mean = absl::StrFormat(\n-          \"\\nPerhaps you meant to specify these on the %s envvar?\",\n-          alternate_envvar);\n-    }\n-\n-                << \" in \" << envvar << \": \" << absl::StrJoin(unknown_flags, \" \")\n-                << did_you_mean;",
    "Added lines": "+                << \" in \" << envvar << \": \"\n+                << absl::StrJoin(unknown_flags, \" \");"
},
{
    "Id": 281,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e99e31597c1b5cc9f0cbc8a3dea71674d81c20b1",
    "Violation": "misleading",
    "Bug report": " Fix GRUCellBlockOp message for invalid rank of x. The validation checks that x is a matrix, so rank must be 2.",
    "Number of deleted lines": 2,
    "Deleted lines": "-                errors::InvalidArgument(\"Rank of x must be 2\", x_tensor->dims(),\n-                                        \" vs. 2\"));",
    "Added lines": "+                errors::InvalidArgument(\"Rank of x must be 2, got \",\n+                                        x_tensor->dims()));"
},
{
    "Id": 282,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b8431494de404b5f4def7303fb8efd6ba3575ef9",
    "Violation": "misleading",
    "Bug report": "Fix error log messages in data type checks",
    "Number of deleted lines": 3,
    "Deleted lines": "-                           \"unsupported zero-point value (%f) for UINT8 tensor \"\n-                           scale, t);\n-                             \"unsupported zero-point value (%f) for INT8 \"",
    "Added lines": "+                           \"unsupported zero-point value (%d) for UINT8 tensor \"\n+                           zero_point, t);\n+                             \"unsupported zero-point value (%d) for INT8 \""
},
{
    "Id": 283,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/18dd91ccd4b1817cd5c34e40f76823a162bea029",
    "Violation": "misleading",
    "Bug report": " [XLA] Report that real -> complex bitcast_convert is not allowed. The check as exists is bidirectional: it prevents conversions from complex to real and real to complex alike, but the reported error message was unidirectional.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return InvalidArgument(\"Conversion from complex to real type %s => %s.\",",
    "Added lines": "+    return InvalidArgument(\"Conversion between complex and real type %s => %s.\","
},
{
    "Id": 284,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/86abddb56350bccd95d1b7140b003fb03525b890",
    "Violation": "misleading",
    "Bug report": " Add appropriate error check for nbins in tf.histogram_fixed_width_bins. This PR tries to address the issue raised in 54415 where nbins was not checked for tf.histogram_fixed_width_bins and an incorrect result was returned when nbins < 0.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+from tensorflow.python.ops import control_flow_ops\n+    check = control_flow_ops.Assert(\n+        math_ops.greater(nbins, 0), [\"nbins %s must > 0\" % nbins])\n+    nbins = control_flow_ops.with_dependencies([check], nbins)"
},
{
    "Id": 285,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1e5c11676dce37bb7c8eb58b35fd298a655c6fd3",
    "Violation": "misleading",
    "Bug report": " [tf.data service] Include dispatcher address in version check error message. This is the error message that happens when the address was specified incorrectly, so it is useful to include the potentially-incorrect address in the error message.",
    "Number of deleted lines": 1,
    "Deleted lines": "-          return grpc_util::WrapError(\"Failed to get dispatcher version\", s);",
    "Added lines": "+          return grpc_util::WrapError(\n+              absl::StrCat(\"Failed to get dispatcher version from dispatcher \"\n+                           \"running at \",\n+                           address_),\n+              s);"
},
{
    "Id": 286,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/07898e752cf02518508f193a0be2e451450044bd",
    "Violation": "misleading",
    "Bug report": " Provide a more informative error message when the bazel version check fails. ",
    "Number of deleted lines": 2,
    "Deleted lines": "-  current_bazel_version = check_bazel_version(_TF_MIN_BAZEL_VERSION,\n-                                              _TF_MAX_BAZEL_VERSION)",
    "Added lines": "+  try:\n+    current_bazel_version = check_bazel_version(_TF_MIN_BAZEL_VERSION,\n+                                                _TF_MAX_BAZEL_VERSION)\n+  except subprocess.CalledProcessError as e:\n+    print(\"Error checking bazel version: \", e.output.decode('UTF-8').strip())\n+    raise e\n+"
},
{
    "Id": 287,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/01e84d7cc214dbf5a7a21bc418ad43afb5694fbc",
    "Violation": "misleading",
    "Bug report": " Update error message for data_adapter with validation split. Remove the user provided value in the error string in case it contains large amount of data. Dump large input data to log might crash on user side.",
    "Number of deleted lines": 2,
    "Deleted lines": "-  if not all(_can_split(t) for t in flat_arrays):\n-        \"arrays, found: {}\".format(arrays))",
    "Added lines": "+  unsplitable = [type(t) for t in flat_arrays if not _can_split(t)]\n+  if unsplitable:\n+        \"arrays, found following types in the input: {}\".format(unsplitable))"
},
{
    "Id": 288,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4c75fb1cb917320acb386cf26adeb8e5151ca4f6",
    "Violation": "misleading",
    "Bug report": " Improve error message reporting for check_numerics gradient. At present the op message is only printed if the numeric check fails during the op's 'forward' computation. If the check fails during the gradient, there is no identifier on *which* op's gradient failed.",
    "Number of deleted lines": 2,
    "Deleted lines": "-def _CheckNumericsGrad(_, grad):\n-      grad, \"Not a number (NaN) or infinity (Inf) values detected in gradient.\")",
    "Added lines": "+def _CheckNumericsGrad(op, grad):\n+      grad,\n+      \"Not a number (NaN) or infinity (Inf) values detected in gradient. %s\" %\n+      op.get_attr(\"message\"))"
},
{
    "Id": 289,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/40918f36823973e816bd50766b1f447225b1bb9b",
    "Violation": "misleading",
    "Bug report": " Make the type check error message more informative for contrib.layers fully_connected.",
    "Number of deleted lines": 2,
    "Deleted lines": "-    raise ValueError('num_outputs should be int or long, got %s.' %\n-                     (num_outputs,))",
    "Added lines": "+    raise ValueError('num_outputs type should be one of %s, got %s.' % (\n+        list(six.integer_types), type(num_outputs)))"
},
{
    "Id": 290,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9c1f14322484e44a93b77619ffd2e24b9b7a9b1d",
    "Violation": "misleading",
    "Bug report": " Fix error message in TF-keras dataset shape check. (Dimension and tensor # were transposed in the error message)",
    "Number of deleted lines": 1,
    "Deleted lines": "-              'for output shapes: %s.%s)' % (i, j, dataset.output_shapes, hint))",
    "Added lines": "+              'for output shapes: %s.%s)' % (j, i, dataset.output_shapes, hint))"
},
{
    "Id": 291,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f0bf6c5191d224f229808f4b321158d890a481e0",
    "Violation": "misleading",
    "Bug report": "Minor change for better error msg in eager input type checking ",
    "Number of deleted lines": 1,
    "Deleted lines": "-          \"cannot compute \", op->Name(), \" as input #\", i,",
    "Added lines": "+          \"cannot compute \", op->Name(), \" as input #\", i, \"(zero-based)\","
},
{
    "Id": 292,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/178d62a63ea043a4b9969b4cd6f8983eb8eae523",
    "Violation": "misleading",
    "Bug report": " Update check failure to logging a warning for repeated computation placer registration. This is to bypass a duplicated registration issue seen in open-source build during TF/PJRT integration.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  CHECK(computation_placers->find(platform_id) == computation_placers->end());",
    "Added lines": "+  if (computation_placers->find(platform_id) != computation_placers->end()) {\n+    // TODO(b/282059652): Consider logging the platform name using\n+    // MultiPlatformManager::PlatformWithId(). No doing that for now to avoid\n+    // introducing unwanted dependency.\n+    LOG(WARNING) << \"computation placer already registered. Please check \"\n+                    \"linkage and avoid linking the same target more than once.\";\n+  }"
},
{
    "Id": 293,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/798b2ebda0cc6f12f1ca6460611f760149771a11",
    "Violation": "missing",
    "Bug report": " Ensure the allocation type is kTfLiteCustom when doing shallow copies in DeepOrShallowCopyTensorsShapeTypeData.  This code is correct only under the assumption that the caller has correctly prepared the tensors that get passed in for shallow copying, by setting their allocation types to kTfLiteCustom. This ensures that those tensors won't be double `free`'d later on. This check simply ensures that that assumption always holds, to ensure we fail early if ever a bug is introduced that breaks that assumption.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      // Make a shallow copy of the data. This is only safe because the caller\n+      // is expected to have previously set dst_tensor->allocation_type to\n+      // kTfLiteCustom, to ensure the buffer is never double-freed later on.\n+      TF_LITE_ENSURE_EQ(context, dst_tensor->allocation_type, kTfLiteCustom);"
},
{
    "Id": 294,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b65d9ec2b78c7c23e368ed4eec7b4deb89dcd712",
    "Violation": "insufficient",
    "Bug report": "Fix value error generated on is_scalar check. Fix value error generated on is_scalar check. `is_scalar = shape is not None and not shape` raises a value error when shape is a scalar, \"ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\"",
    "Number of deleted lines": 1,
    "Deleted lines": "-      is_scalar = shape is not None and not shape",
    "Added lines": "+      is_scalar = (shape is not None and isinstance(shape, collections_lib.Sequence)\n+                   and len(shape) == 0)"
},
{
    "Id": 295,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9baa064387b0a114c3fcec88abaa0568834e8e34",
    "Violation": "insufficient",
    "Bug report": " Only apply check for non-tensor case ",
    "Number of deleted lines": 3,
    "Deleted lines": "-    if constant_values != 0:\n-    else:\n-      result = gen_array_ops.pad(tensor, paddings, name=name)",
    "Added lines": "+    if not tensor_util.is_tensor(constant_values) and constant_values == 0:\n+      result = gen_array_ops.pad(tensor, paddings, name=name)\n+    else:"
},
{
    "Id": 296,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/924f80a4fdb34230965a7a8a4476901847463645",
    "Violation": "missing",
    "Bug report": "Add stricter type checking for tf.math.real. Fix for tf.math.real so that it only accepts tensors with numeric entries as input.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    else:",
    "Added lines": "+    elif tf.debugging.is_numeric_tensor(input):\n+    else:\n+      raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))"
},
{
    "Id": 297,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e6df768b81e973f2123bc83a18a60773fc4da99e",
    "Violation": "insufficient",
    "Bug report": "[TFG] Fix IsAdd string type check in tf_op_names ",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (op_name == add_) return !op->getAttrOfType<StringAttr>(\"T\");",
    "Added lines": "+  if (op_name == add_)\n+    return !op->getAttrOfType<TypeAttr>(\"T\").getValue().isa<StringType>();"
},
{
    "Id": 298,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/88609e2e22fa5c296de2e27e04d1cc4743b2dfcd",
    "Violation": "missing",
    "Bug report": " Add appropriate dtype check for tf.boolean_mask's mask. This PR tries to address the issue raised in 54412 where mask's dtype was checked in tf.boolean_mask and an invalid result has been returned instead.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if mask.dtype != dtypes.bool:\n+      raise TypeError(\"Invalid `mask`: expected bool but got %s.\" % mask.dtype)"
},
{
    "Id": 299,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d79c4d435fc6e7be6cc69a3ca446716ebf6190b9",
    "Violation": "missing",
    "Bug report": "change default value of num_threads to Non. set num_threads before delegate. check the type of num_threads before setting it",
    "Number of deleted lines": 3,
    "Deleted lines": "-               num_threads=1):\n-    self._interpreter.SetNumThreads(num_threads)\n-",
    "Added lines": "+               num_threads=None):\n+    if num_threads:\n+      if not isinstance(num_threads, int):\n+        raise ValueError('type of num_threads should be int')\n+      self._interpreter.SetNumThreads(num_threads)\n+"
},
{
    "Id": 300,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a76646d4b4ad5d56b5e63c139985bbd1eb98dd90",
    "Violation": "misleading",
    "Bug report": " Add type checking at the beginning of tpu.shard(). Otherwise a message like \"TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.\" will be thrown, which is confusing.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  inputs = [] if inputs is None else [ops.convert_to_tensor(x) for x in inputs]",
    "Added lines": "+  inputs = [] if inputs is None else inputs\n+  if not isinstance(inputs, list):\n+    raise TypeError(\"tpu.shard()'s inputs must be a list of Tensors or None.\")\n+\n+  inputs = [ops.convert_to_tensor(x) for x in inputs]"
},
{
    "Id": 301,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c510c1b8b1ef5be1d65971f5b9e21e61becd0bb3",
    "Violation": "unnecessary",
    "Bug report": "Remove IsCalledComputation from HloComputation. The function doesn't do what it seems. There are more types of called instruction that are not accounted in this check.",
    "Number of deleted lines": 6,
    "Deleted lines": "-    CHECK(!IsCalledComputation());\n-  // Returns if this computation is invoked by an Hlo instruction.\n-  bool IsCalledComputation() const {\n-    return IsFusionComputation() || IsCustomCallComputation();\n-  }\n-",
    "Added lines": "+    // TODO: Add instruction type for async instructions.\n+    CHECK(instruction_type() == InstructionType::kUnset);"
},
{
    "Id": 302,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1c49c13ba59961cf7581e3e29b951db8faca94f5",
    "Violation": "missing",
    "Bug report": "Add type check for reduction axis in reducer operation. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TF_LITE_ENSURE_TYPES_EQ(context, op_context.axis->type, kTfLiteInt32);"
},
{
    "Id": 303,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b6f3366a716ca9b5a1e6114a3bea050c80d8a475",
    "Violation": "missing",
    "Bug report": " Don't check for if null after already dereferenced. I'm not sure how it could be null at this point (and obviously it is nowhere else we'd have seen failures), but keeping the check as is and just moving it to where it would catch it before dereferencing.",
    "Number of deleted lines": 3,
    "Deleted lines": "-    auto it = stack_traces.find(n->name());\n-    if (n && it != stack_traces.end()) {\n-      n->SetStackTrace(it->second);",
    "Added lines": "+    if (n) {\n+      auto it = stack_traces.find(n->name());\n+      if (it != stack_traces.end()) {\n+        n->SetStackTrace(it->second);\n+      }"
},
{
    "Id": 304,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/89fa1ae2cb34eab0e6137e72e6fab01f6c5bc164",
    "Violation": "improper",
    "Bug report": "Fix check for cloning FunctionLibraryRuntime",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (out_flr != nullptr) {",
    "Added lines": "+  if (*out_flr != nullptr) {"
},
{
    "Id": 305,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3a7b36bca7f43ce4f0d0791ce0e0d84ece8683d9",
    "Violation": "improper",
    "Bug report": " [Grappler] Remove DCHECK from a MutableGraphView CanDedupControlWithRegularInput check.",
    "Number of deleted lines": 2,
    "Deleted lines": "-  DCHECK(control_node != nullptr)\n-      << \"Didn't find a node for control dependency: \" << control_node_name;",
    "Added lines": "+  if (control_node == nullptr) {\n+    return false;\n+  }"
},
{
    "Id": 306,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c5019e2156c749d35ec786ff7946a55006d9ba91",
    "Violation": "missing",
    "Bug report": "missing checking on null pointer dereference",
    "Number of deleted lines": 1,
    "Deleted lines": "-",
    "Added lines": "+  if (cuda_stream_ != nullptr) {\n+    LOG(FATAL) <<  // Crash OK.\n+        \"Trying to set the stream twice. This isn't supported. \";\n+  }\n+"
},
{
    "Id": 307,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a7908e924af3105c3007988e219855174b26774f",
    "Violation": "missing",
    "Bug report": "Added check for output buffer ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (output == nullptr)\n+    LOG(ERROR) << \"Output buffer is null: \";\n+    return false;\n+  }\n+"
},
{
    "Id": 308,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/04b97cde86550995da57d16d81084006456ccce5",
    "Violation": "missing",
    "Bug report": "Fix segmentation fault with tf.stack an keras's Input in TF2.0. This fix adds the `PySequence_Fast` and checks the return value to make sure it is not nullptr.",
    "Number of deleted lines": 2,
    "Deleted lines": "-      for (Py_ssize_t j = 0; j < PySequence_Fast_GET_SIZE(item); j++) {\n-        PyObject* inner_item = PySequence_Fast_GET_ITEM(item, j);",
    "Added lines": "+      tensorflow::Safe_PyObjectPtr fast_item(PySequence_Fast(item, \"Could not parse sequence.\"));\n+      if (fast_item.get() == nullptr) {\n+        return false;\n+      }\n+      for (Py_ssize_t j = 0; j < PySequence_Fast_GET_SIZE(fast_item.get()); j++) {\n+        PyObject* inner_item = PySequence_Fast_GET_ITEM(fast_item.get(), j);"
},
{
    "Id": 309,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/95166f5960322cc784a7e8f339a701da80a41a1e",
    "Violation": "missing",
    "Bug report": "Add a null check on enter_ctx and update the null check on merge_ctx ",
    "Number of deleted lines": 1,
    "Deleted lines": "-      DCHECK_NE(merge_ctx, nullptr);",
    "Added lines": "+  CHECK_NE(enter_ctx, nullptr);\n+      CHECK_NE(merge_ctx, nullptr);"
},
{
    "Id": 310,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cd8d0bf58ad554588012898161c91fa453bbf7f0",
    "Violation": "missing",
    "Bug report": "Address edge case where runStats is null and the interface is closed.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    runStats.close();",
    "Added lines": "+    if (runStats != null) {\n+      runStats.close();\n+    }"
},
{
    "Id": 311,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1a1a381b5be7701843c3f1e34aa1846ae2a1d0ce",
    "Violation": "improper",
    "Bug report": "Fix a SIGSEGV bug in InferShapeForXlaGatherOp. Since `ComputeOutputComponent` may return nullptr, we need to check for null attributes explicitly to be safe.",
    "Number of deleted lines": 1,
    "Deleted lines": "-             llvm::isa<DenseIntElementsAttr>(it->second)) {",
    "Added lines": "+             llvm::isa_and_nonnull<DenseIntElementsAttr>(it->second)) {"
},
{
    "Id": 312,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/893aa7518fe3175739ac1ba70d7355a0b091115c",
    "Violation": "missing",
    "Bug report": "Added a null check in string_util.cc ",
    "Number of deleted lines": 2,
    "Deleted lines": "-#include <limits>\n-",
    "Added lines": "+#include <cstddef>\n+  if (*buffer == nullptr) {\n+    return -1;\n+  }\n+"
},
{
    "Id": 313,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9720b405905dee209a3f7d003de21d388e1aaef4",
    "Violation": "improper",
    "Bug report": " Avoid nullptr as row offsets to cusparseCreateCsr. As of CUDA 12.2 additional input validation allows NULL for the row offsets only when rows=0.",
    "Number of deleted lines": 1,
    "Deleted lines": "-                         nullptr, nullptr, nullptr));",
    "Added lines": "+                         c_row_ptr.data(), nullptr, nullptr));"
},
{
    "Id": 314,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/47eaa828a1dd4bf50ec4203ef4bbb348b3ef0dd0",
    "Violation": "missing",
    "Bug report": "Add nullptr check",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if ((&cc_tensor) == nullptr) {\n+    *tensor = nullptr;\n+    return;\n+  }"
},
{
    "Id": 315,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2fc1f2b5a8b8152c43b81cf31394f3e0a2cb837",
    "Violation": "missing",
    "Bug report": "Add null pointer check",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  CHECK(a.opaque() != nullptr);\n+"
},
{
    "Id": 316,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b677392e4af8095dbde8068b0ceb60bca815e94b",
    "Violation": "missing",
    "Bug report": " Reject non-PjRt devices in PjRtArray::Reshard(). PjRt buffers traditionally support some degree of interoperability between PjRt clients (e.g., CPU and TPU). However, this is not universally true between arbitrary IFRT clients that may use a non-PjRt-compatible runtime. This change adds extra checks to make sure that non-PjRt devices are not accidentally used in PjRtArray's destination devices.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      if (new_sharding->devices()[i]->client() == nullptr) {\n+        return InvalidArgument(\n+            \"The destination device is owned by a non-PjRt-compatible client. \"\n+            \"To use this Array on the destination device, the Array must be \"\n+            \"first fetched to the host and then sent to the destination \"\n+            \"device.\");\n+      }"
},
{
    "Id": 317,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f22ca1dc88c70a0dc5696c37e6a2de6bcf8d60c7",
    "Violation": "improper",
    "Bug report": " Avoid segfault when init_value is not on default_mesh. To actually fix the segfault in lower level (e.g. directly users of VarHandleOp), I tried to add a validation in SPMD of AssignValueOp, but turns out it only knows the resource_layout is an 'empty' layout without any mesh information. We shall start tracking mesh of empty layout -- but changing the data model at this point is not very easy to do or to justify.",
    "Number of deleted lines": 2,
    "Deleted lines": "-      super(DVariable, self).__init__(\n-          initial_value, *args, dtype=dtype, **kwargs)",
    "Added lines": "+import contextlib\n+      mesh = self.layout.mesh if self.layout else None\n+      with api.run_on(mesh) if mesh else contextlib.nullcontext():\n+        super(DVariable, self).__init__(\n+            initial_value, *args, dtype=dtype, **kwargs)"
},
{
    "Id": 318,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a17858f3cc5e7ab4ebc2c166d71e7f85b2dad05d",
    "Violation": "insufficient",
    "Bug report": " Avoid undefined behavior by checking for null Operation in TF_Input/TF_Output",
    "Number of deleted lines": 2,
    "Deleted lines": "-    Node* node = &inputs[i].oper->node;\n-    Node* node = &outputs[i].oper->node;",
    "Added lines": "+    Node* node = inputs[i].oper ? &inputs[i].oper->node : nullptr;\n+    Node* node = outputs[i].oper ? &outputs[i].oper->node : nullptr;"
},
{
    "Id": 319,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1835465ac5a9c823f7187cb0dd5786da9c360838",
    "Violation": "missing",
    "Bug report": " Add error_reporter DCHECK back into SimpleMemoryAllocator. This check was removed due to an internal build problem.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TFLITE_DCHECK(error_reporter != nullptr);"
},
{
    "Id": 320,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7578e120de2a3a5282ced8d41881f19363f83466",
    "Violation": "missing",
    "Bug report": " Fix crash on closing the app when classifier failed to initialize. When testing on an API 21 emulator, the classifier fails to initialize. The fix is to check for null before calling `.close()`.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    classifier.close();",
    "Added lines": "+    if (classifier != null) {\n+      classifier.close();\n+    }"
},
{
    "Id": 321,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c1b9ac9f215a3a83f7f0b6233bf4cef0b3e74598",
    "Violation": "missing",
    "Bug report": "Error checking in c/python code.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (iterator == nullptr || PyErr_Occurred()) {\n+    return false;\n+  }"
},
{
    "Id": 322,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ca170f34d9174d6981850855190a398393aa921e",
    "Violation": "missing",
    "Bug report": " [Tensorflow] Add check fail when user passes a tensor with nullptr to lookup.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  CHECK(val != nullptr);\n+  CHECK(val != nullptr);"
},
{
    "Id": 323,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/20d54796563631c23c27548b321487e8b0c982a9",
    "Violation": "insufficient",
    "Bug report": " Add a nil check before init the device_name string, and also assign an empty string as a placeholder.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    std::string device_name = std::string([[metal_device_ name] UTF8String]);",
    "Added lines": "+    auto utf8_name = [[metal_device_ name] UTF8String];\n+    const std::string device_name = utf8_name != nil ? utf8_name : \"\";"
},
{
    "Id": 324,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/db10718b38b2884cb5ed46d33c135c079f649d16",
    "Violation": "missing",
    "Bug report": "With some memory allocators, attempting to allocate 0 bytes will return a null pointer. This specifically happens when building tensorflow with mkl support. If TF_TensorData returns null, the go code to create a slice from the data leads to a null pointer exception. This fixes the issue by checking for the nil return and returning a slice zero value to (nil) to the caller. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\tif cbytes == nil {\n+\t\treturn nil\n+\t}"
},
{
    "Id": 325,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8876a1796aeced8f89c279cbc98db9c7957ddbd1",
    "Violation": "improper",
    "Bug report": " Updated check for existence of TensorFlow objects to 'is not None' rather than 'if [object]'.",
    "Number of deleted lines": 3,
    "Deleted lines": "-  if sync_optimizer and startup_delay_steps > 0:\n-    if is_chief and sync_optimizer:\n-        if is_chief and sync_optimizer:",
    "Added lines": "+  if sync_optimizer is not None and startup_delay_steps > 0:\n+    if is_chief and sync_optimizer is not None:\n+        if is_chief and sync_optimizer is not None:"
},
{
    "Id": 326,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/11030308c5d25df5b36f8a583f1b4607e4ea2b7f",
    "Violation": "missing",
    "Bug report": " Add a check to check if all sharding strategies are dropped due to infinity costs",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    size_t num_skipped_due_to_infinity_costs = 0;\n+        num_skipped_due_to_infinity_costs++;\n+    CHECK_LT(num_skipped_due_to_infinity_costs, strategies->leaf_vector.size())\n+        << \"All strategies removed due to infinite resharding costs\";"
},
{
    "Id": 327,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2465d4e77654f0d4f7799bc46d5fd5812590acc6",
    "Violation": "missing",
    "Bug report": " Add a check in auto-sharding setup and die if the input mesh shape contains more than two shardable dimensions, which is currently not supported.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (spmd::VectorGreaterThanOneElementCount(device_mesh_shape) > 2) {\n+      return tsl::errors::OutOfRange(\n+          absl::StrCat(\"the auto-sharding pass currently does not support \",\n+                       \"more than two shardable dims: device_mesh_shape=\",\n+                       absl::StrJoin(device_mesh_shape, \",\")));\n+    }"
},
{
    "Id": 328,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3e0152a8b4aad03dd06274e0dd3b94bd5f8bf5d3",
    "Violation": "missing",
    "Bug report": "Fix invalid syntax error when import carla is present. The issue is that, when `import carla` is invoked, I/O operation for `std::ostringstream s` might fail, which caused the conversion of AttrValue to string as empty. This PR check `s.good()` to make sure the I/O operation is OK, and, fallback to normal conversion if locale-neutral I/O operation fails.",
    "Number of deleted lines": 1,
    "Deleted lines": "-      return s.str();",
    "Added lines": "+      // If there is no I/O error for `std::ostringstream s` return s.str(),\n+      // otherwise fallback to strings::StrCat(value.f()).\n+      if (s.good()) {\n+        return s.str();\n+      }\n+      return strings::StrCat(value.f());"
},
{
    "Id": 329,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/06b89ed1bdf606adb21d66664ca7ab5eaffdd58f",
    "Violation": "insufficient",
    "Bug report": " BundleReader was not waiting for concurrents reads to complete before checking their result value. Also changed the large value reading test to actually exercise the multi-threaded reading path. Previously, the whole multi threaded path was being skipped because the reads were smaller than kBufferSize.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (entry.size() > kBufferSize) {",
    "Added lines": "+    if (entry.size() > kBufferSize || enable_multi_threading_for_testing_) {\n+        reader_pool = nullptr;  // Wait for reads to finish\n+"
},
{
    "Id": 330,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0317f64491ba42376d96b157983a02d8b31b679e",
    "Violation": "improper",
    "Bug report": " Update RNNCell._rnn_get_variable to use Variable._trainable in TF2 mode. When using a legacy RNNCell in TF2 mode within a tf.function the \"var in trainable_variables()\" check led to treating a tf.bool tensor as a Python bool. This change makes use within a tf.function use the same logic that is used in Eager mode.",
    "Number of deleted lines": 2,
    "Deleted lines": "-    if context.executing_eagerly():\n-      trainable = variable._trainable  # pylint: disable=protected-access",
    "Added lines": "+    if ops.executing_eagerly_outside_functions():\n+      trainable = variable.trainable"
},
{
    "Id": 331,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b8c517ab4ef0bd851ef2f8187935fd3a90261af5",
    "Violation": "missing",
    "Bug report": "Reinstate eager check inside _GradientsHelper",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if context.executing_eagerly():\n+    raise RuntimeError(\"tf.gradients is not supported when eager execution \"\n+                       \"is enabled. Use tf.GradientTape instead.\")"
},
{
    "Id": 332,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c9b4689bc4d4024aa16b7d6cfc1c65fa1ed8486e",
    "Violation": "improper",
    "Bug report": "Removed no longer supported call to in_eager_execution. Swapped context.in_eager_execution() to the currently supported context.executing_eagerly(). Added negation to eager check. In all likelihood, the negation was always supposed to be there since getting default graph in eager mode does not make sense",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if not graph and context.in_eager_execution():",
    "Added lines": "+  if not graph and not context.executing_eagerly():"
},
{
    "Id": 333,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e5496b556734bb1d8de85311092804e0150b3009",
    "Violation": "unnecessary",
    "Bug report": " Remove extraneous check for Eager mode.The check is already made once at the start of the method",
    "Number of deleted lines": 2,
    "Deleted lines": "-      if context.in_eager_mode():\n-        return",
    "Added lines": ""
},
{
    "Id": 334,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/be5116dd131a92da298dbb68d26e0d47f66f2fe5",
    "Violation": "improper",
    "Bug report": " Correct graph check in broadcast_to gradient. ",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if not context.executing_eagerly():",
    "Added lines": "+  if not isinstance(broadcast_shape, ops.EagerTensor):"
},
{
    "Id": 335,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1d6dae88efef68dd7fbeeb5c39ea0f69c1c721c1",
    "Violation": "missing",
    "Bug report": "Add check to tf.device when called with a function in eager mode. ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+  Raises:\n+    RuntimeError: If eager execution is enabled and a function is passed in.\n+    if callable(device_name_or_function):\n+      raise RuntimeError(\n+          \"tf.device does not support functions when eager execution \"\n+          \"is enabled.\")"
},
{
    "Id": 336,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8c3822edbb31cf71cedaf49f2167e45c1e2d0b83",
    "Violation": "missing",
    "Bug report": "Update the is_dtensor check to only run in eager mode.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+    Raises:\n+      RuntimeError: When not called eagerly.\n+    if not context.executing_eagerly():\n+      raise RuntimeError(\"is_dtensor must be called eagerly.\")"
},
{
    "Id": 337,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a63f3006f703428ff980748cdbe24d6a13f761e2",
    "Violation": "missing",
    "Bug report": "Skip checking for graph_key in V1 optimizer when running in eager mode.",
    "Number of deleted lines": 1,
    "Deleted lines": "-      if variable_object._graph_key == current_graph_key:  # pylint: disable=protected-access",
    "Added lines": "+      # Skip checking for graph key for eager mode since there's only one graph.\n+      # This is necessary because there are cases where _trackable_children() is\n+      # called in a differenr thread from the main thread (e.g., async\n+      # checkpoint) and hence the default graph key would be different.\n+      if (context.executing_eagerly()\n+          or variable_object._graph_key == current_graph_key):  # pylint: disable=protected-access"
},
{
    "Id": 338,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dd7d791e02396346d98b7b2c58137d7e51756c0c",
    "Violation": "missing",
    "Bug report": "Add isinstance check for eager execution.",
    "Number of deleted lines": 2,
    "Deleted lines": "-\n-  if isinstance(v, internal.NativeObject):",
    "Added lines": "+  if isinstance(v, EagerTensor) and not context.executing_eagerly():\n+    return convert_to_tensor(v, as_ref=True).op, None\n+  elif isinstance(v, internal.NativeObject):"
},
{
    "Id": 339,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8933b8a21280696ab119b63263babdb54c298538",
    "Violation": "missing",
    "Bug report": " Fix a null pointer exception caused by branching on uninitialized data. This is due to not checking that the params for the quantization exists. If there is no quantization, we should not access the `.params` field.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n+    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n+  TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);"
},
{
    "Id": 340,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0a9b39caefd437fec742ae48b25061abd6e2699b",
    "Violation": "missing",
    "Bug report": " When allocating GPU constants, check to see if the destination. tensor is intialized early (because we ran out of memory) and report it as such.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+    // If the tensor is not initialized, we likely ran out of memory.\n+    if (!copy.IsInitialized()) {\n+      return errors::ResourceExhausted(\n+          \"OOM when allocating tensor of shape \", parsed.shape().DebugString(),\n+          \" and type \", DataTypeString(parsed.dtype()));\n+    }\n+"
},
{
    "Id": 341,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4f38b1ac8e42727e18a2f0bde06d3bee8e77b250",
    "Violation": "missing",
    "Bug report": " Prevent null dereference read in GetInitOp. We have a map of maps. We test that the key exists in the first map but then we don't have any validation that this also means the second map has the needed key. In the scenariosc where this is not the case, we'll dereference a nullptr, if we don't have this chec",
    "Number of deleted lines": 3,
    "Deleted lines": "-    *init_op_name = init_op_sig_it->second.outputs()\n-                        .find(kSavedModelInitOpSignatureKey)\n-                        ->second.name();",
    "Added lines": "+    const auto& sig_def_outputs = init_op_sig_it->second.outputs();\n+    const auto& sig_def_outputs_it =\n+        sig_def_outputs.find(kSavedModelInitOpSignatureKey);\n+    if (sig_def_outputs_it == sig_def_outputs.end()) {\n+      return errors::FailedPrecondition(\"Could not find output \",\n+                                        kSavedModelInitOpSignatureKey);\n+    }\n+    *init_op_name = sig_def_outputs_it->second.name();"
},
{
    "Id": 342,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a93ac5f7d147ae8fe946de33ad654161ae851352",
    "Violation": "missing",
    "Bug report": " For quantization values where range_min == range_max, use the lowest_quantized.  Add needed checks for divide-by-zero.",
    "Number of deleted lines": 5,
    "Deleted lines": "-    return 0;\n-  const int64 lowest_quantized =\n-      static_cast<double>(Eigen::NumTraits<T>::lowest());\n-        range_scale((number_of_steps - 1.0) / (range_max - range_min)),\n-      static_cast<int64>(255.0 * (1 << fp_shift) * input_range / output_range);",
    "Added lines": "+  const int64 lowest_quantized =\n+      static_cast<double>(Eigen::NumTraits<T>::lowest());\n+    return lowest_quantized;\n+        range_scale(range_max == range_min\n+                        ? 0.0\n+                        : (number_of_steps - 1.0) / (range_max - range_min)),\n+      output_range == 0.0 ? 0.0\n+                          : static_cast<int64>(255.0 * (1 << fp_shift) *\n+                                               input_range / output_range);"
},
{
    "Id": 343,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e7de472681079932b2547024f31c876da54f61a0",
    "Violation": "insufficient",
    "Bug report": " Fix a bug in flatbuffer importer that use tensor quantization before checking.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (!tensor.quantization->min.empty()) {",
    "Added lines": "+  if (tensor.quantization && !tensor.quantization->min.empty()) {"
},
{
    "Id": 344,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2adf1114d4dc7ca30e5117acd2dc7aeb3279feb7",
    "Violation": "missing",
    "Bug report": " Make NNAPI delegate only apply overflow check to quantized average_pool",
    "Number of deleted lines": 6,
    "Deleted lines": "-      // reference CPU path.\n-      Expect(is_accelerator_specified ||\n-                 (builtin->filter_width * builtin->filter_height <= 256),\n-             NNAPIValidationFailureType::kUnsupportedOperandSize,\n-             \"Large filter window would overflow on the reference CPU path\",\n-             &val_ctx);",
    "Added lines": "+      // quantized reference CPU path.\n+      if (IsQuantized(context->tensors[node->inputs->data[0]].type)) {\n+        Expect(is_accelerator_specified ||\n+                   (builtin->filter_width * builtin->filter_height <= 256),\n+               NNAPIValidationFailureType::kUnsupportedOperandSize,\n+               \"Large filter window would overflow on the reference CPU path\",\n+               &val_ctx);\n+      }"
},
{
    "Id": 345,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/677866210941431b82c95d58d0798976bb40a415",
    "Violation": "insufficient",
    "Bug report": "Add a nullptr check for the tensor quantization field",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (IsQuantized(tensor)) return nullptr;",
    "Added lines": "+  if (!tensor.quantization || IsQuantized(tensor)) return nullptr;\n+    // TODO(fengliuai): this quantization dimension isn't correct."
},
{
    "Id": 346,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/19b2e1b5868a044df4622ef7e26fa5570ca52e5e",
    "Violation": "insufficient",
    "Bug report": "Only perform scalar check for a tensor shape if it's not empty.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    DCHECK(weights.shape_.IsScalar());",
    "Added lines": "+    DCHECK(weights.shape_.IsEmpty() || weights.shape_.IsScalar());"
},
{
    "Id": 347,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9c92b50fc4b95985a0749101976d04896bf19bfe",
    "Violation": "improper",
    "Bug report": " [conv3d_transpose] Fix dim check for bias. Per discussion with @thaink, the previous way to do the dim check for bias is not correct. So we need this change.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 4));",
    "Added lines": "+    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 3));"
},
{
    "Id": 348,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5bc536f1afbaff5d3d5a14a9185cd1e3cc31b302",
    "Violation": "improper",
    "Bug report": "[Fix] bug fix during check static shape.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (!shaped_type && !shaped_type.hasStaticShape()) {",
    "Added lines": "+    if (!shaped_type || !shaped_type.hasStaticShape()) {"
},
{
    "Id": 349,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/eb2ddc0debb7e1b0c9ea68c817ca05fd59dc7914",
    "Violation": "unnecessary",
    "Bug report": " In TF2XLA EnsureShape kernel, don't check whether the original tensor has dynamic shapes as it is much more expensive than just blindly clear out dynamic dimension.",
    "Number of deleted lines": 6,
    "Deleted lines": "-    // remove the dynamic dimensions in XLA dynamic padder.\n-    std::vector<bool> dynamic_dims;\n-    OP_REQUIRES_OK(ctx,\n-                   ctx->ResolveInputDynamismIntoPredVector(0, &dynamic_dims));\n-      if (expected_shape_.dim_size(i) > 0 && dynamic_dims[i]) {\n-        VLOG(1) << \"RemoveDynamicDimension: \" << i;",
    "Added lines": "+    // remove the dynamic dimensions in XLA dynamic padder. Here we don't check\n+    // whether the original input has dynamic shapes, because\n+    // `ctx->ResolveInputDynamismIntoPredVector` runs a DFS underneath which is\n+    // more expensive.\n+      if (expected_shape_.dim_size(i) > 0) {\n+        VLOG(1) << \"RemoveDynamicDimension: \" << i << \" of shape \"\n+                << shape.DebugString();"
},
{
    "Id": 350,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/264eb6ed1dbfb5e078c7dd977da8d7e633106fc5",
    "Violation": "missing",
    "Bug report": " Fixed add bias transformation. Added check for convolution with dynamic weights.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      if (graph->FindInputs(node->id).size() != 1) {\n+        return {TransformStatus::DECLINED,\n+                \"This transformation is only applicable to conv with one \"\n+                \"runtime input.\"};\n+      }"
},
{
    "Id": 351,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2e4d3951eb618a7c34d5e629fc2506ea2a62b4a7",
    "Violation": "improper",
    "Bug report": " Correct Tensor order for dilation2D. `gen_nn_ops.dilation2d` seems to be in `NHWC` while the parent function was asking for `NCHW`.  I corrected the doc and the check.",
    "Number of deleted lines": 3,
    "Deleted lines": "-    data_format: A `string`, only `\"NCHW\"` is currently supported.\n-  if data_format != \"NCHW\":\n-    raise ValueError(\"Data formats other than NCHW are not yet supported\")",
    "Added lines": "+    data_format: A `string`, only `\"NHWC\"` is currently supported.\n+  if data_format != \"NHWC\":\n+    raise ValueError(\"Data formats other than NHWC are not yet supported\")"
},
{
    "Id": 352,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8cef4cda26e08256b6698e942820d9a3ac1bcc94",
    "Violation": "missing",
    "Bug report": "Add minor checks for data_format and padding value ",
    "Number of deleted lines": 2,
    "Deleted lines": "-  FormatFromString(data_format.str(), &format);\n-  GetPaddingFromString(paddings.str(), &padding);",
    "Added lines": "+  auto data_format_is_valid = FormatFromString(data_format.str(), &format);\n+  if (!data_format_is_valid) {\n+    return emitOptionalError(location, \"Invalid data format provided\");\n+  }\n+  auto padding_is_valid = GetPaddingFromString(paddings.str(), &padding);\n+  if (!padding_is_valid.ok()) {\n+    return emitOptionalError(location, \"Invalid padding format provided\");\n+  }"
},
{
    "Id": 353,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/076ea8d84c2058b0d01d56dd9ddc3221a2e0c817",
    "Violation": "insufficient",
    "Bug report": "Also check dst_format",
    "Number of deleted lines": 2,
    "Deleted lines": "-  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");\n-  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");",
    "Added lines": "+  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\") &&\n+                  (dst_format == \"NHWC\" || dst_format == \"NCHW\");\n+  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\") &&\n+                  (dst_format == \"NHWC\" || dst_format == \"NCHW\");"
},
{
    "Id": 354,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ed06859189722af4dc8e4abd655926df066e587a",
    "Violation": "missing",
    "Bug report": "Add format check.",
    "Number of deleted lines": 1,
    "Deleted lines": "-",
    "Added lines": "+      DCHECK(data_format == \"NCDHW\");\n+      DCHECK(data_format == \"NCHW\");"
},
{
    "Id": 355,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0d5668cbdc6b46d099bd3abd93374c09b2e8121f",
    "Violation": "improper",
    "Bug report": " [XLA:SHAPE_UTIL] Return nullopt instead of a check failure if the input dimensions are not sorted.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  CHECK(std::is_sorted(input_dim_indices.begin(), input_dim_indices.end()));",
    "Added lines": "+  if (!std::is_sorted(input_dim_indices.begin(), input_dim_indices.end())) {\n+    return absl::nullopt;\n+  }"
},
{
    "Id": 356,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d7ec7b9415181fce88ea8fde39af9e8be5a8be97",
    "Violation": "missing",
    "Bug report": "Added generic check that shape has not more than 4 dimensions.",
    "Number of deleted lines": 1,
    "Deleted lines": "-            \"OP is supported, but tensor type doesn't match.\";",
    "Added lines": "+    if (t->dims && t->dims->size >= 5) {\n+      return false;\n+    }\n+            \"OP is supported, but tensor type/shape doesn't supported.\";"
},
{
    "Id": 357,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/824af2acfa0cdf897c08d91224aea0958c1afc02",
    "Violation": "missing",
    "Bug report": " Add ndmin check. Added ndmin check to allow maximum 32 ndmin to make same behavior as numpy. Currently it is crashing when very large ndmin is passed.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  max_ndmin = 32\n+  if ndmin > max_ndmin:\n+    raise ValueError('ndmin bigger than allowable number of dimensions: '\n+                     f'{max_ndmin}.')\n+  "
},
{
    "Id": 358,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b73a3c21a224f479af8d3b8af320c220a091906c",
    "Violation": "missing",
    "Bug report": "[XLA] Add check for potential out-of-bound access.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TF_RET_CHECK(sort_dim >= 0 && sort_dim < increment.size())\n+      << \"Unexpected out-of-bound sort dimension \" << sort_dim\n+      << \" accessing increment of size \" << increment.size();"
},
{
    "Id": 359,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/63753d5f1531b17cf8cbbf1d8b77c16edcfb9711",
    "Violation": "improper",
    "Bug report": " Change DCHECK_LE to DCHECK_LT when checking invariant on original indices for sorted items Indices of items should be strictly smaller than the size of the vector.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    DCHECK_LE(original_index, names.size());",
    "Added lines": "+    DCHECK_LT(original_index, names.size());"
},
{
    "Id": 360,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7f9929732ced22fe8ef42a695dae39c1caf44608",
    "Violation": "missing",
    "Bug report": " For gather op, if params.shape[:batch_dims] is not the same as indice s.shape[:batch_dims], return an error instead of check fail ",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      for (int i = 0; i < batch_dims_; ++i) {\n+        OP_REQUIRES(c, params.dim_size(i) == indices.dim_size(i),\n+                    errors::InvalidArgument(\n+                        \"params.shape[\", i, \"]: \", params.dim_size(i),\n+                        \" should be equal to indices.shape[\", i,\n+                        \"]: \", indices.dim_size(i)));\n+      }"
},
{
    "Id": 361,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ba91c04e001f417641e757a6417e5325c1c4e15e",
    "Violation": "insufficient",
    "Bug report": "Add more check to sparsity parameter verifier.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (sparsity->dim_metadata()->size() != total_dims) {",
    "Added lines": "+  if (total_dims < tensor.shape()->size() ||\n+      sparsity->dim_metadata()->size() != total_dims) {"
},
{
    "Id": 362,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1610f391833738972b538e4ee97f90dbd30fc745",
    "Violation": "improper",
    "Bug report": " Replace DCHECK with actual validation in AddRangeStats",
    "Number of deleted lines": 2,
    "Deleted lines": "-  DCHECK_LE(start_instance, end_instance);\n-    DCHECK_LT(start_feature_dim, end_feature_dim);",
    "Added lines": "+  OP_REQUIRES(context, start_instance <= end_instance,\n+              errors::InvalidArgument(\n+                  \"start_instance = \", start_instance,\n+                  \" which is not at most end_instance=\", end_instance));\n+    OP_REQUIRES(context, start_feature_dim < end_feature_dim,\n+                errors::InvalidArgument(\n+                    \"start_feature_dim = \", start_feature_dim,\n+                    \" which is not at most end_feature_dim=\", end_feature_dim));"
},
{
    "Id": 363,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/150a6c06b281246cb5a075a704fceeb257bb63af",
    "Violation": "missing",
    "Bug report": "Add a check on the 0th dimension of filter for DepthwiseConv.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // Filter in DepthwiseConv is expected to be [1, H, W, O].\n+  TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 0), 1);"
},
{
    "Id": 364,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bf686faeddcca97be6ad7b6421cb26ab1c3cea2c",
    "Violation": "missing",
    "Bug report": "TFLite: Enhance input check for ResizeNearestNeghbor",
    "Number of deleted lines": 2,
    "Deleted lines": "-  // TODO(ahentz): Our current implementations rely on the inputs being 4D.\n-",
    "Added lines": "+  // TODO(ahentz): Our current implementations rely on the input being 4D,\n+  // and the size being 1D tensor with exactly 2 elements.\n+  TF_LITE_ENSURE_EQ(context, size->dims->data[0], 2);\n+"
},
{
    "Id": 365,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c040db5e9003cc20016586df9f2964db83b98c4f",
    "Violation": "missing",
    "Bug report": " [XLA] Add a defensive check in dynamic dimension inference to prevent scalar reshape with dynamic dimension. In theory we can just ignore a [1] -> [] reshape, but adding a check here for now.",
    "Number of deleted lines": 3,
    "Deleted lines": "-      hlo, [&](HloInstruction* operand, ShapeIndex index, int64 dimension,\n-               int64 operand_index, HloInstruction* dynamic_size,\n-               DimensionConstraint constraint) {",
    "Added lines": "+      hlo,\n+      [&](HloInstruction* operand, ShapeIndex index, int64 dimension,\n+          int64 operand_index, HloInstruction* dynamic_size,\n+          DimensionConstraint constraint) -> Status {\n+        TF_RET_CHECK(reshape->shape().rank() > 0)\n+            << \"Reshaping a dynamic dimension into a scalar, which has \"\n+               \"undefined behavior. The offending instruction is: \"\n+            << reshape->ToString();"
},
{
    "Id": 366,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/48393637f8154be16088d84742485a0e153ecbb2",
    "Violation": "improper",
    "Bug report": "Change check to allow tensors with up to 6 dims.",
    "Number of deleted lines": 2,
    "Deleted lines": "-  CHECK_LE(RequiredBufferSizeForShape(dims_array.shape()), 4)\n-      << \"dims vector can be no larger than 4 values\";",
    "Added lines": "+  CHECK_LE(RequiredBufferSizeForShape(dims_array.shape()), 6)\n+      << \"dims vector can be no larger than 6 values\";"
},
{
    "Id": 367,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/662128e8ca3411286b234553a7efc1356353d0f5",
    "Violation": "missing",
    "Bug report": " add rank checking for MEAN op. The MEAN op of NNAPI only supports a tensor with rank <= 4. Check the rank of the input tensor before delegating the op.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      Expect(context->tensors[node->inputs->data[0]].dims->size <= 4,\n+             NNAPIValidationFailureType::kUnsupportedOperandValue,\n+             \"NNAPI does not support mean of a tensor with rank > 4\",\n+             &val_ctx);"
},
{
    "Id": 368,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9b947dd6377c022091c8aa005cdcff52c53ff5f0",
    "Violation": "insufficient",
    "Bug report": "Also check dst_format",
    "Number of deleted lines": 1,
    "Deleted lines": "-  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");",
    "Added lines": "+  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\") &&\n+                  (dst_format == \"NHWC\" || dst_format == \"NCHW\");"
},
{
    "Id": 369,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/450dec35448a73b3fcb5d4f82108d5fdcb3f59b4",
    "Violation": "missing",
    "Bug report": "Internal change, add some checks on the sparseTensor format checking.",
    "Number of deleted lines": 1,
    "Deleted lines": "-      *(row_ids_before_padding + i) = indices_matrix(i, 0);",
    "Added lines": "+    int32 previous_row_id = -1;\n+      int32 current_row_id = indices_matrix(i, 0);\n+      if (current_row_id < previous_row_id) {\n+        return absl::InvalidArgumentError(\n+            \"Invalid indices_or_row_splits input, indices of SparseTensor need \"\n+            \"to be sorted in ascending order.\");\n+      }\n+      *(row_ids_before_padding + i) = current_row_id;"
},
{
    "Id": 370,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/356f360e8772a2697ec0d30036237342549803f5",
    "Violation": "missing",
    "Bug report": " Add additional shape validation to compute_accidental_hits. In `compute_accidental_hits`, the `sampled_candidates` must be a vector, as is shown in the kernel implementation in `tensorflow/core/kernels candidate_sampler_ops.cc`. This fix adds shape validation of `sampled_candidates` in the shape function whenever possible.",
    "Number of deleted lines": 1,
    "Deleted lines": "-      // Validate true_classes.",
    "Added lines": "+      // Validate true_classes, must be a matrix.\n+      // Validate sampled_candidates, must be a vector.\n+      ShapeHandle sampled_candidates;\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &sampled_candidates));"
},
{
    "Id": 371,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7c88788e63f3a747d2794175076db551d768734e",
    "Violation": "missing",
    "Bug report": " Shape validation of max_features in QuantizedReluX. In shape function of QuantizedReluX, `max_value` and `min_features` have shape validation but not `max_features`. This fix add restriction to `max_features` as well.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));"
},
{
    "Id": 372,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ff6be80a1ec3c353ebd0d17e2f0b46d9097310db",
    "Violation": "missing",
    "Bug report": " Improve the shape function for ParameterizedTruncatedNormal.  The parameters of ParameterizedTruncatedNormal should be 0-D or 1-D, which is checked in ther kernel functions. There is no check in the shape function of the ops. This fix improves the shape function and checks the parameters of ParameterizedTruncatedNormal whever possible.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    .SetShapeFn(shape_inference::RandomShape);",
    "Added lines": "+    .SetShapeFn([](InferenceContext* c) {\n+      ShapeHandle unused;\n+      // Parameters must be 0-d or 1-d.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(1), 1, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(3), 1, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &unused));\n+      return shape_inference::RandomShape(c);\n+    });"
},
{
    "Id": 373,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c59c37e7b2d563967da813fa50fe20b21f4da683",
    "Violation": "missing",
    "Bug report": " Prevent array write out-of-bounds. If user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TF_LITE_ENSURE(context, axis_value >= 0);\n+  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\n+"
},
{
    "Id": 374,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e82a377de614fed51da8a7c5242a90a7967169f2",
    "Violation": "missing",
    "Bug report": "Correct axis check",
    "Number of deleted lines": 3,
    "Deleted lines": "-    if (abs(axis_value) > input_type.getRank())\n-      return op.emitOpError(\"op attribute 'axis' is out of bounds, got \")\n-             << axis_value;",
    "Added lines": "+    if (axis_value < 0)\n+      axis_value += input_type.getRank() + 1;\n+    if (axis_value < 0 || axis_value >= input_type.getRank() + 1)\n+      return op.emitOpError()\n+             << \"op attribute 'axis' should be in range [-rank - 1, rank + 1), \"\n+             << \"got rank = \" << input_type.getRank()\n+             << \", and axis = \" << op.axis().getSExtValue();"
},
{
    "Id": 375,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/402d478a107e2931fb0e9b2f08f973997cae7f98",
    "Violation": "improper",
    "Bug report": "Move the checking of ranks for early exit",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (!ShouldProcess(*context, *node) || (rank != 4 && rank != 5) ||",
    "Added lines": "+  if (rank != 4 && rank != 5) {\n+    return Status::OK();\n+  }\n+  if (!ShouldProcess(*context, *node) ||"
},
{
    "Id": 376,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/27de8e717c1bec91398f5a6be6c7287b657fc960",
    "Violation": "missing",
    "Bug report": " Improve shape function for CudnnRNNParamsSize. In cudnn_rnn_ops.cc, the CudnnRNNParamsSize does not have restrictions on num_layers, num_units, and input_size, though they all should be scalars. This fix adds the shape check of num_layers, num_units, and input_size for CudnnRNNParamsSize.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      ShapeHandle unused;\n+      // num_layers, num_units, and input_size should be scalars.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n+"
},
{
    "Id": 377,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/02703f9525696f4788496745f6756585c1c546a3",
    "Violation": "missing",
    "Bug report": "Fix crash in range sampler by adding a range check in the sampler op.",
    "Number of deleted lines": 2,
    "Deleted lines": "-    CHECK(sampler_) << \"CandidateSamplerOp did not set sampler_\";\n-",
    "Added lines": "+    CHECK(sampler_) << \"CandidateSamplerOp did not set sampler_\";\n+\n+    if (unique_) {\n+      OP_REQUIRES(context, num_sampled_ <= sampler_->range(),\n+                  errors::InvalidArgument(\"Sampler's range is too small.\"));\n+    }"
},
{
    "Id": 378,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4a1d1c8413a3752af7dc91a7128e202660b0f05c",
    "Violation": "improper",
    "Bug report": " Fix mismatch of shape restriction in DrawBoundingBoxes. In the kernel of DrawBoundingBoxes, the shape of the input images should be 4-D. Though in the shape function, at the end `UnchangedShapeWithRankAtLeast(c, 3)` was used instead (at the beginning of the shape function the validation is `WithRank(c->input(0), 4, &images)` which is correct). This fix address the discrepancy by changing to `UnchangedShape`.",
    "Number of deleted lines": 1,
    "Deleted lines": "-      return shape_inference::UnchangedShapeWithRankAtLeast(c, 3);",
    "Added lines": "+      // The rank of the input image (rank = 4) has already been restricted\n+      // above, and the output is of the same shape as the input.\n+      return shape_inference::UnchangedShape(c);"
},
{
    "Id": 379,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6e153325b66330dafea4e4e8b67b5d56b1a37852",
    "Violation": "missing",
    "Bug report": " [XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast produces a scalar. This avoids crashing within last_dimension when attempting to match.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (bitcast->shape().rank() == 0) {\n+    return true;\n+  }\n+"
},
{
    "Id": 380,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9d3cce4c7525bad6743f84302e5f6355a3fd8fe5",
    "Violation": "missing",
    "Bug report": " Fix crash in BlockLSTM. This PR tries to address the issue raised in 58175 in addressing the crash of BlockLSTM when invalid input is provided.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(seq_len_max_tensor->shape()),\n+                errors::InvalidArgument(\"`seq_len_max_tensor` must be rank 0 but is rank \",\n+                                        seq_len_max_tensor->dims()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(seq_len_max_tensor->shape()),\n+                errors::InvalidArgument(\"`seq_len_max_tensor` must be rank 0 but is rank \",\n+                                        seq_len_max_tensor->dims()));"
},
{
    "Id": 381,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d94ffe08a65400f898241c0374e9edc6fa8ed257",
    "Violation": "missing",
    "Bug report": " Prevent an OOB read in expand_dims.cc. The for loop that follows this check assumes that `axis` is between `0` and `input_dims.size`. If user supplied `axis` is negative, the if code before this check is supposed to bring it back to positive (similar to how in Python one can do `l[-3]` to mean `l[-3 + len(l)]`).",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TF_LITE_ENSURE(context, axis >= 0);"
},
{
    "Id": 382,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0e3574d39c66d937fa9f9d2e25554aab0066f250",
    "Violation": "missing",
    "Bug report": "Add rank check to Sub op delegation to NNAPI",
    "Number of deleted lines": 2,
    "Deleted lines": "-      ExpectMaxOpVersion(version, 2, &val_ctx);\n-}",
    "Added lines": "+      ExpectMaxOpVersion(version, 3, &val_ctx);\n+      const int input0_rank =\n+          context->tensors[node->inputs->data[0]].dims->size;\n+      const int input1_rank =\n+          context->tensors[node->inputs->data[1]].dims->size;\n+      Expect(input0_rank <= 4 && input1_rank <= 4,\n+             NNAPIValidationFailureType::kUnsupportedOperandRank,\n+             \"Input rank must be <= 4\", &val_ctx);\n+}  // NOLINT(readability/fn_size)"
},
{
    "Id": 383,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a680ed0bf03d5ca3b2c4a70c0d95eeebc20da6d6",
    "Violation": "missing",
    "Bug report": " For Substr check pos and len rank equality only when their rank is known. This fixes a bug where len has unknown rank, while pos has known shape. The WithRank(...) check returned error in such a case. Here we compare their ranks only when both pos and len have known rank.",
    "Number of deleted lines": 2,
    "Deleted lines": "-      // Check that pos/len have same rank\n-      TF_RETURN_IF_ERROR(c->WithRank(pos_shape, c->Rank(len_shape), &unused));",
    "Added lines": "+      // If len rank is known, check that pos and len have the same rank\n+      if (c->RankKnown(len_shape)) {\n+        TF_RETURN_IF_ERROR(c->WithRank(pos_shape, c->Rank(len_shape), &unused));\n+      }"
},
{
    "Id": 384,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9187be7adff07be82856add498aa3ff4b5f95998",
    "Violation": "missing",
    "Bug report": "add checks for compression_type and buffer_size also",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      // `compression_type` could only be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      // `buffer_size` could only be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));"
},
{
    "Id": 385,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/779664494d43b18a812361197dcbea2f25912c02",
    "Violation": "missing",
    "Bug report": "Add shape check to TextLineDataset op",
    "Number of deleted lines": 4,
    "Deleted lines": "-    .SetShapeFn(shape_inference::ScalarShape);  // TODO(mrry): validate\n-                                                // that `filenames` is\n-                                                // a scalar or a\n-                                                // vector.",
    "Added lines": "+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // `filenames` must be a scalar or a vector.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });"
},
{
    "Id": 386,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c4dea2255c71037c9cade9cbd1d7820b3429b3fa",
    "Violation": "missing",
    "Bug report": "Add shape check for buffer_size with TFRecordDataset",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      // `buffer_size` could only be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused) );"
},
{
    "Id": 387,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d97ffbdf362fa7d06ef8d946c8620ff7a3a50a08",
    "Violation": "missing",
    "Bug report": "Add shape check for compression_type in TFrecordDataset",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      // `compression_type` could only be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused) );"
},
{
    "Id": 388,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7586dee9aa8b4b63143ab658ca59658aaed0df97",
    "Violation": "missing",
    "Bug report": " Add shape check to TFRecordDataset. The inputs of TFRecordDataset have the requirements for shapes. However, the check was not done in the shape function. This fix adds shape checks whenever possible.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    .SetShapeFn(shape_inference::ScalarShape);",
    "Added lines": "+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // `filenames` must be a scalar or a vector.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });"
},
{
    "Id": 389,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/851177fee860211e2fabcb019d644e75b7f701b0",
    "Violation": "missing",
    "Bug report": "Add shape check for shift of tf.roll",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      // The `shift` must be scalar or 1-D.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(1), 1, &unused));"
},
{
    "Id": 390,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3f796ff8c9e6d7ff88f99c056b78e88fb0b31114",
    "Violation": "missing",
    "Bug report": "Add axis shape check for tf.roll",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      // The `axis` must be scalar or 1-D.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));"
},
{
    "Id": 391,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/10467d29e05d9957a6e3cb2335f8eeba1fd8896e",
    "Violation": "missing",
    "Bug report": " Improve shape function check for tf.roll. The `tf.roll` op has requirements for the shape of inputs. However, the shape of the inputs are only done at the runtime inside the kernel. This fix improve the shape function so that the check could be done early if shape is already known in the shape function.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    .SetShapeFn(shape_inference::UnchangedShape);",
    "Added lines": "+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // The `input` must be 1-D or higher\n+      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\n+\n+      return shape_inference::UnchangedShape(c);\n+    });"
},
{
    "Id": 392,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/41deb95a7bde735d3c8b9adedd8b1fe8c1ef2732",
    "Violation": "missing",
    "Bug report": "support unknown rank, check rank>=0",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if(rank == kUnknownRank) {\n+    return UnknownShape();\n+  }\n+  CHECK_GE(rank,0) << \"rank must not be negative\";"
},
{
    "Id": 393,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8b742f8559e88474735d0a2c03e00da65e40b412",
    "Violation": "missing",
    "Bug report": "Fix check error on shape overflow.",
    "Number of deleted lines": 2,
    "Deleted lines": "-    input_matrix_shapes->emplace_back(\n-        std::initializer_list<int64_t>({num_rows, num_cols}));",
    "Added lines": "+    TensorShape input_shape;\n+    OP_REQUIRES_OK(context, TensorShape::BuildTensorShape({num_rows, num_cols},\n+                                                          &input_shape));\n+    input_matrix_shapes->push_back(std::move(input_shape));"
},
{
    "Id": 394,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1595906c2192b7f402f746652042a592ad290378",
    "Violation": "missing",
    "Bug report": " Prevent CHECK-fail DOS in BoostedTreesSparseAggregateStatsOp. Calling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(feature_indices_t->shape()),\n+                errors::InvalidArgument(\n+                    \"feature_indices must be a matrix, received shape \",\n+                    feature_indices_t->shape().DebugString()));"
},
{
    "Id": 395,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/54c94431e5dd17fc46d99da1a3f132c76414c161",
    "Violation": "missing",
    "Bug report": " Prevent CHECK-fail DOS in BoostedTreesSparseAggregateStatsOp. Calling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsMatrix(hessians_t->shape()),\n+        errors::InvalidArgument(\"hessians must be a matrix, received shape \",\n+                                hessians_t->shape().DebugString()));"
},
{
    "Id": 396,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5d96267d907ac2119cbccf1416b749195e8fd8de",
    "Violation": "missing",
    "Bug report": " Prevent CHECK-fail DOS in BoostedTreesSparseAggregateStatsOp. Calling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsMatrix(gradients_t->shape()),\n+        errors::InvalidArgument(\"gradients must be a matrix, received shape \",\n+                                gradients_t->shape().DebugString()));"
},
{
    "Id": 397,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/41ab69692ede0db3422fa70bc5889d470741e69c",
    "Violation": "missing",
    "Bug report": " Check for tensors to be vectors in BoostedTreesSparseAggregateStatsOp. Calling `tensor->vec` should only happen after checking that the tensor shape implies a vector. Otherwise, we can get denial of service via `CHECK`-fails",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(context, TensorShapeUtils::IsVector(feature_values_t->shape()),\n+                errors::InvalidArgument(\n+                    \"feature_values must be a vector, received shape \",\n+                    feature_values_t->shape().DebugString()));"
},
{
    "Id": 398,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8d733ecdb270dd90b2b5f53fd220d5ce17a5e20f",
    "Violation": "missing",
    "Bug report": " Check for tensors to be vectors in BoostedTreesSparseAggregateStatsOp. Calling `tensor->vec` should only happen after checking that the tensor shape implies a vector. Otherwise, we can get denial of service via `CHECK`-fails",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(node_ids_t->shape()),\n+        errors::InvalidArgument(\"node_ids must be a vector, received shape \",\n+                                node_ids_t->shape().DebugString()));"
},
{
    "Id": 399,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f482488b481a799ca07e7e2d153cf47b8e91a60c",
    "Violation": "missing",
    "Bug report": " TFLite OpenGL ES delegate: out of boundary writes fixed for bhwc->phwc4 conversion.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  uint3 workload = uint3(shape.w, shape.h, shape.c);",
    "Added lines": "+  uint3 workload = uint3(shape.w, shape.h, IntegralDivideRoundUp(shape.c, 4));"
},
{
    "Id": 400,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/58759659ee547a957c5d36e72f2274ab34fdb6cb",
    "Violation": "improper",
    "Bug report": "Fix OOB check for result_index in header generation",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (result_index < 0 || result_index > temp_sizes.size()) {",
    "Added lines": "+  if (result_index < 0 || result_index >= temp_sizes.size()) {"
},
{
    "Id": 401,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/80b65ab79bf8dd6ec03c570b59a1208bb27fec24",
    "Violation": "improper",
    "Bug report": " Small fix to axis check for tfl.pack to tosa. There was an off-by-one error when checking the axis value based on the input rank.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if ((axis < 0) || (axis > (input_tensor_rank + 1))) {",
    "Added lines": "+  if ((axis < 0) || (axis > input_tensor_rank)) {"
},
{
    "Id": 402,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2ff14318050e26302785a49a1719d29ddcc91b4",
    "Violation": "improper",
    "Bug report": " [XNNPACK] Fix incorrect check in slice node. begin+size == input dimension is valid, e.g. input size is 3, begin is 2, size is 1.",
    "Number of deleted lines": 2,
    "Deleted lines": "-      if (begin[i] + size[i] >= input_shape->data[i]) {\n-                                 \") must be less input \"",
    "Added lines": "+      if (begin[i] + size[i] > input_shape->data[i]) {\n+                                 \") must not be greater than input \""
},
{
    "Id": 403,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d23458fdd2655c83ff9d54725062ded31b644ba4",
    "Violation": "improper",
    "Bug report": " [XLA:CPU] Do not check that the size of the XLA parameter buffer is exactly equal to the size of the underlying given buffer Instead, check that the underlying allocation is \"large enough\". This is also more consistent with XLA:GPU behavior. The mismatch can happen when the input comes from tf.where, which is backed by an allocation larger than is actually required.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    CHECK_EQ(allocation.size(), out.size())",
    "Added lines": "+    CHECK_LE(allocation.size(), out.size())"
},
{
    "Id": 404,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4863013a3ec5b97c042a38ab567bcc4a62ccde5c",
    "Violation": "insufficient",
    "Bug report": " Add checking for number of inputs in GetOptionalInputTensor to avoid indexing out of array bounds.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  const bool use_tensor = node->inputs->data[index] != kTfLiteOptionalTensor;",
    "Added lines": "+  const bool use_tensor = index < node->inputs->size &&\n+                          node->inputs->data[index] != kTfLiteOptionalTensor;"
},
{
    "Id": 405,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1e38a0025c9a983bf3229299109b5b1781215c7e",
    "Violation": "missing",
    "Bug report": " [XLA] CHECK that sparse indices are in range in MutableLiteralBase::AppendSparseElement. Previously there was no range-checking on sparse elements' indices.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  // TODO(jlebar): CHECK that multi_index is in range?",
    "Added lines": "+  for (int64 i = 0; i < rank; ++i) {\n+    CHECK_GE(multi_index[i], 0);\n+    CHECK_LT(multi_index[i], subshape.dimensions(i));\n+  }"
},
{
    "Id": 406,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1610da3f992487bd9a8181d1e83cae99fe1e34d9",
    "Violation": "missing",
    "Bug report": "add more sanity check on AvgPoolGrad op",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+      OP_REQUIRES(\n+          context, orig_input_dims_mkl_order[0] == diff_dst_dims[0],\n+          errors::InvalidArgument(\n+              \"Expected first dimension of orig_input and diff_dst to match, \"\n+              \"got \",\n+              orig_input_dims_mkl_order[0], \" and \", diff_dst_dims[0]));\n+"
},
{
    "Id": 407,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a68f57a24203fd49c4a5c4a8f51098d4415a93f8",
    "Violation": "missing",
    "Bug report": " [XNNPACK] Add missing return when output channels do not match in TransposeConvolution Add a check that input channels in the filter and tensor match.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      return kTfLiteError;\n+    }\n+    if (input_channels != input_tensor_dims[3]) {\n+      TF_LITE_MAYBE_KERNEL_LOG(\n+          logging_context,\n+          \"transpose convolution kernel input channel dimension (%d) \"\n+          \"doesn't match filter input channel (%d) in node #%d\",\n+          input_channels, input_tensor_dims[3]);\n+      return kTfLiteError;"
},
{
    "Id": 408,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/23968a8bf65b009120c43b5ebcceaf52dbc9e943",
    "Violation": "missing",
    "Bug report": " Fix out of bound access in DequantizeOp by adding check for axis < input dimension",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(\n+        ctx, axis_ < input.dims(),\n+        errors::InvalidArgument(\"Axis must be less than input dimension(\",\n+                                input.dims(), \"), got \", axis_));\n+"
},
{
    "Id": 409,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4923de56ec94fff7770df259ab7f2288a74feb41",
    "Violation": "missing",
    "Bug report": " Don't do any work when reshaping 0 elements sparse tensor. If reshaping to 0 elements tensor, check that input has no elements. If reshaping no elements input, check that output has no elements.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(\n+        context, dense_size > 0 && product > 0,\n+        errors::InvalidArgument(\n+            \"Input tensor has \", nnz, \" non zero elements but input shape (\",\n+            input_shape.DebugString(), \") or output shape (\",\n+            output_shape.DebugString(), \") is empty\"));"
},
{
    "Id": 410,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/467730fe90282a75f15f67d701b278e86cfad65e",
    "Violation": "missing",
    "Bug report": "Fix dimension check for tf.keras.losses.BinaryCrossentropy. The reason was that broadcasting was applied directly. This fix adds dimension check to throw an error if there is a mismatch.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      try:\n+        target.get_shape().merge_with(output.get_shape())\n+      except ValueError:\n+        raise ValueError(\n+            \"target and output must have the same shape (%s vs %s)\" %\n+            (target.get_shape(), output.get_shape()))"
},
{
    "Id": 411,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7e2d53c1c371f38c7f0ef13c1c06336b22a195c0",
    "Violation": "missing",
    "Bug report": "[tf.data] Adds the expected check for better debugging.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        DCHECK(state_and_output.size() <=\n+               dataset()->state_types_.size() + output_dtypes().size());"
},
{
    "Id": 412,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a12b8c4afdca3ac2945d62b3b83ca2599ab360f9",
    "Violation": "insufficient",
    "Bug report": " [xla] Improve validation of Broadcast shape. If one misreads the semantics of this instruction, it's easy to cause an out of bounds access into the dimensions here. Add an extra check to return a proper error to the user rather than crashing in that case.",
    "Number of deleted lines": 2,
    "Deleted lines": "-    TF_RET_CHECK(broadcast->shape().dimensions(output_dimension) ==\n-                 operand_shape.dimensions(operand_dimension))",
    "Added lines": "+    TF_RET_CHECK((output_dimension < ShapeUtil::Rank(broadcast->shape())) &&\n+                 (broadcast->shape().dimensions(output_dimension) ==\n+                 operand_shape.dimensions(operand_dimension)))"
},
{
    "Id": 413,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/05ec322172958f6e67e4bcaef4681e6aa54fabeb",
    "Violation": "missing",
    "Bug report": " Return error message with illegal input rather than check-failing in op_kernel.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        TF_RET_CHECK(kernel->outputs[i].input_index >= 0)\n+            << \"Invalid input for outputs \" << i;"
},
{
    "Id": 414,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/62cb54f2caf48480dc6b3c1ce9629eaac4688f83",
    "Violation": "missing",
    "Bug report": " Set 2nd output shape for SparseSegmentReduceGradV2 Fixes a debug check failure.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (outputs_unique_indices) {\n+    c->set_output(1, c->Vector(InferenceContext::kUnknownDim));\n+  }"
},
{
    "Id": 415,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9b0f99ddd27e7738732a154be5469391ee8fc977",
    "Violation": "missing",
    "Bug report": "Add check to ensure element sizes are the same",
    "Number of deleted lines": 1,
    "Deleted lines": "-",
    "Added lines": "+  TFLITE_DCHECK_EQ(input1_shape.FlatSize(), input2_shape.FlatSize());"
},
{
    "Id": 416,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f8ec0f101bac066faa2e917ac714ca9eea310eac",
    "Violation": "missing",
    "Bug report": "adding checks that pad fusion works only Conv2D",
    "Number of deleted lines": 1,
    "Deleted lines": "-",
    "Added lines": "+      if(!isConv2D){\n+        OP_REQUIRES(context, padEnabled,\n+                errors::InvalidArgument(\"Pad+Conv fusion only works for 2D\"));\n+      }"
},
{
    "Id": 417,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9718fed7b9aba244359b3d38c2a1dc20e50428bd",
    "Violation": "missing",
    "Bug report": " Added size check to avoid memory corruption in GraphDefImporter::ConvertNodeDef.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+  if (op_def->output_arg_size() < 0)\n+    return InvalidArgument(\"Node \", node.name(), \" output arg size < 0\");"
},
{
    "Id": 418,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/edd9fb416e04b8ca4398c4eea65f14dc6704a44a",
    "Violation": "unnecessary",
    "Bug report": " TfLiteTensorCopy returns an error status when src and dest bytes are not equal. So we don't need to check them specifically if we ensure the status of the call to copy (which we should do anyways).",
    "Number of deleted lines": 2,
    "Deleted lines": "-    TF_LITE_ENSURE_EQ(context, src_tensor->bytes, dst_tensor->bytes);\n-    TfLiteTensorCopy(src_tensor, dst_tensor);",
    "Added lines": "+    TF_LITE_ENSURE_OK(context, TfLiteTensorCopy(src_tensor, dst_tensor));"
},
{
    "Id": 419,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e44f8a08051baa58bde9130a844a1b82a8179526",
    "Violation": "missing",
    "Bug report": "check hasattr on the type, not the instance. hasattr on the instance triggers __getattr__ which carries very undesirable effects, such as running Ops on a donated buffer.",
    "Number of deleted lines": 7,
    "Deleted lines": "-    # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n-    # object map. Also add TPUEmbedding to separate list for special handling\n-    # with values copy.\n-      if hasattr(t, _TPU_EMBEDDING_ATTR):\n-    if not hasattr(\n-        tpu_embedding, _TPU_EMBEDDING_ATTR\n-    ) or not callable(tpu_embedding._create_copy_for_async_checkpoint):  # pylint: disable=protected-access",
    "Added lines": "+      # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n+      # object map. Also add TPUEmbedding to separate list for special handling\n+      # with values copy.\n+      if hasattr(type(t), _TPU_EMBEDDING_ATTR):\n+    if not hasattr(type(tpu_embedding), _TPU_EMBEDDING_ATTR) or not callable(\n+        tpu_embedding._create_copy_for_async_checkpoint  # pylint: disable=protected-access\n+    ):"
},
{
    "Id": 420,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/51d72a7d7f74784b68916819edd04e890b36f957",
    "Violation": "improper",
    "Bug report": " Modified \"_check_is_tensor_or_operation\" to check if \"x\" is \"tensor_like\"",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if not (isinstance(x, ops.Operation) or isinstance(x, ops.Tensor)):",
    "Added lines": "+from tensorflow.python.framework import tensor_util\n+  if not (isinstance(x, ops.Operation) or tensor_util.is_tensor(x)):"
},
{
    "Id": 421,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8a2e7deb21f02e4072d6b62cf7f447b9264afe01",
    "Violation": "improper",
    "Bug report": " Adjust checks for type(Tensor) to isinstance or is_eager/is_symbolic_tensor.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if tensors_type is ops.Tensor:",
    "Added lines": "+  if isinstance(tensors, ops.Tensor):"
},
{
    "Id": 422,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b68b869e75916e6de37c2ca23a93643faf333011",
    "Violation": "improper",
    "Bug report": "Fix invalid keras tensor isinstance check",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if not isinstance(input_tensor, keras_tensor.keras_tensors_enabled()):",
    "Added lines": "+        if not isinstance(input_tensor, keras_tensor.KerasTensor):"
},
{
    "Id": 423,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a0de0ca6a39f3037e1be6ec740829863bcda3e8",
    "Violation": "improper",
    "Bug report": "[XLA:GPU] Fix type check in IsMatrixMultiplication",
    "Number of deleted lines": 1,
    "Deleted lines": "-       lhs_shape.element_type() == S8);",
    "Added lines": "+       rhs_shape.element_type() == S8);"
},
{
    "Id": 424,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/db9b247cd1f3ff046359f7b64ca60c2d697fe2e1",
    "Violation": "insufficient",
    "Bug report": " Fix the functional model loading with nested sequential model. The nested sequential model is created with _is_graph_network = False, the current instance check is not strong enough.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return isinstance(layer, Functional)",
    "Added lines": "+  # For a sequential model, it is first created with _is_graph_network = False,\n+  # we have to keep the _is_graph_network check here.\n+  return isinstance(layer, Functional) and layer._is_graph_network"
},
{
    "Id": 425,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a4b6b6bcc7a813162bf0378727950e321aca19c",
    "Violation": "improper",
    "Bug report": "Add stricter type checking for tf.math.real (using is_numeric)",
    "Number of deleted lines": 1,
    "Deleted lines": "-    elif tf.debugging.is_numeric_tensor(input):",
    "Added lines": "+    elif input.dtype.is_numeric:"
},
{
    "Id": 426,
    "Library": "tensorflow",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/580140611a47413dcf6373deb1250c0ed605e873",
    "Violation": "missing",
    "Bug report": " [XLA] Do not check fail in proto copy from if the backend config proto and desired proto type do not match.",
    "Number of deleted lines": 2,
    "Deleted lines": "-    proto->CopyFrom(*proto_ptr);\n-    return Status::OK();",
    "Added lines": "+    if (proto_ptr->GetDescriptor() == proto->GetDescriptor()) {\n+      proto->CopyFrom(*proto_ptr);\n+      return Status::OK();\n+    }"
}]