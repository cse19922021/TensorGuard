{
    "Id": 1,
    "Library": "tensorflow",
    "Date": "2024/07/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/42fbcf15fc9777c230d19c8e1f43cd2095a4fb34",
    "Root Cause": "N.A",
    "Bug report": "Remove some debug logging.\n\nProbably accidentally submitted at some point.\n\nPiperOrigin-RevId: 653959438",
    "Number of deleted lines": 2,
    "Deleted lines": "-    llvm::errs() << \"ABOUT TO FAIL\\n\";\n-    llvm::errs() << \"DID NOT FAIL\\n\";",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 2,
    "Library": "tensorflow",
    "Date": "2024/07/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a908b587f39c8fb1ec0a4805a6bb6d2225d34fcb",
    "Root Cause": "N.A",
    "Bug report": "[xla:gpu] NFC: Improve NCCL clique logging\n\nTo improve debugging experience log what cliques will be acquired for each global device id, we need this to detect when different ranks do not agree on cliques number and their acquisition order.\n\nPiperOrigin-RevId: 649203288",
    "Number of deleted lines": 1,
    "Deleted lines": "-    for (const CliqueRequest& r : GetOrderedCliqueRequests()) {",
    "Added lines": "+#include <cstddef>\n+    std::vector<CliqueRequest> ordered_cliques = GetOrderedCliqueRequests();\n+    for (size_t i = 0; i < ordered_cliques.size(); ++i) {\n+      const CliqueRequest& r = ordered_cliques[i];\n+      VLOG(2) << \"  clique #\" << i << \" (for global device id \"\n+              << params.global_device_id.value() << \")\"\n+              << \": num_local_participants=\" << r.num_local_participants\n+              << \"; id=\" << r.id << \"; key=\" << r.key.ToString();\n+    }\n+\n+    for (const CliqueRequest& r : ordered_cliques) {",
    "Label": "clean"
},
{
    "Id": 3,
    "Library": "tensorflow",
    "Date": "2024/07/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c10c6efa90468912f92ff2f67a6ae07f0863234e",
    "Root Cause": "N.A",
    "Bug report": "Remove libtensorflow_framework.so.2 dependency.\n\nThis was a workaround for a bazel bug which seems to have been fixed.\n\nPiperOrigin-RevId: 648697745",
    "Number of deleted lines": 7,
    "Deleted lines": "-        inputs = [ctx.file.mlir_op, ctx.file._tfso],\n-        # cc_binary seems not to bring its dependencies with it, so do that explicitly here.\n-        \"_tfso\": attr.label(\n-            default = Label(\"//tensorflow:libtensorflow_framework.so.2\"),\n-            cfg = \"exec\",\n-            allow_single_file = True,\n-        ),",
    "Added lines": "+        inputs = [ctx.file.mlir_op],",
    "Label": "clean"
},
{
    "Id": 4,
    "Library": "tensorflow",
    "Date": "2024/06/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/91eaa88d88011f559365d1077f4838c1065fcdc7",
    "Root Cause": "N.A",
    "Bug report": "[IFRT] Include input_specs in RemapPlan::DebugString()\n\n`RemapPlan::DebugString()` was missing `input_specs` dump in the output. This change includes it similar to `output_specs`.\n\nPiperOrigin-RevId: 647794569",
    "Number of deleted lines": 3,
    "Deleted lines": "-  return absl::StrCat(\n-      \"RemapPlan(output_specs=\", format_array_specs(output_specs), \",\",\n-      \"mappings=\", format_mappings(*mappings), \")\");",
    "Added lines": "+  return absl::StrCat(\"RemapPlan(input_specs=\", format_array_specs(input_specs),\n+                      \",output_specs=\", format_array_specs(output_specs), \",\",\n+                      \"mappings=\", format_mappings(*mappings), \")\");",
    "Label": "clean"
},
{
    "Id": 5,
    "Library": "tensorflow",
    "Date": "2024/06/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d24dadc14e6172b30ed45324c849e2666b505c3f",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in PjRtStreamExecutorLoadedExecutable::GetOutputMemoryKinds.\n\nIf there are no addressable devices, the code will crash when trying to access the default memory space of the first device.\n\nAlso changed to const & for two loop variables to avoid copies.\n\nPiperOrigin-RevId: 646715753",
    "Number of deleted lines": 2,
    "Deleted lines": "-  for (auto element_shape : shape.tuple_shapes()) {\n-  for (auto shape : shapes) {",
    "Added lines": "+  for (const auto& element_shape : shape.tuple_shapes()) {\n+  if (addressable_devices().empty()) {\n+    return Unimplemented(\n+        \"GetOutputMemoryKinds is not supported when there are no addressable \"\n+        \"devices in PjRtStreamExecutorLoadedExecutable.\");\n+  }\n+  for (const auto& shape : shapes) {",
    "Label": "clean"
},
{
    "Id": 6,
    "Library": "tensorflow",
    "Date": "2024/06/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/da6448317927642650a250a5fb3b84796eb2164b",
    "Root Cause": "N.A",
    "Bug report": "#tf-data Change the `VLOG` to `LOG(WARNING)` when overriding the buffer size of `TFRecordDataset` on GCP and S3 to help debugging.\n\nPiperOrigin-RevId: 641042045",
    "Number of deleted lines": 7,
    "Deleted lines": "-    VLOG(2) << \"User buffer size is too small for reading Cloud TPU \"\n-            << \"TFRecords stored in GCS. Overriding \" << buffer_size\n-            << \" to the minimum recommended buffer_size = \"\n-            << kCloudTpuBlockSize;\n-    VLOG(2) << \"User buffer size is too small for reading \"\n-            << \"TFRecords stored in S3. Overriding \" << buffer_size\n-            << \" to the minimum recommended buffer_size = \" << kS3BlockSize;",
    "Added lines": "+    LOG(WARNING) << \"User buffer size is too small for reading Cloud TPU \"\n+                 << \"TFRecords stored in GCS. Overriding \" << buffer_size\n+                 << \" to the minimum recommended buffer_size = \"\n+                 << kCloudTpuBlockSize;\n+    LOG(WARNING) << \"User buffer size is too small for reading \"\n+                 << \"TFRecords stored in S3. Overriding \" << buffer_size\n+                 << \" to the minimum recommended buffer_size = \"\n+                 << kS3BlockSize;",
    "Label": "clean"
},
{
    "Id": 7,
    "Library": "tensorflow",
    "Date": "2024/06/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c3bf76a0f6cf77dbebf2ae3956d82dbc8d26dfce",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Fix an OOB access bug in `HloFusionAdaptor::GetParameters()`.\n\nWhen considering a `SingleInstructionFusion` that is also a parameter\ninstruction, the parameter number enclosed within that instruction refers\nto the parent computation.\n\nAs a result, we shouldn't try to access its operands, but simply enqueue the\ninstruction itself as a parameter to skip.\n\nPiperOrigin-RevId: 640075542",
    "Number of deleted lines": 4,
    "Deleted lines": "-    if (root.opcode() == HloOpcode::kParameter &&\n-        root.instruction().user_count() <= 1) {\n-      parameters_to_skip.insert(\n-          producer_fusion.operand(root.instruction().parameter_number()));",
    "Added lines": "+    if (root.opcode() == HloOpcode::kParameter) {\n+      // If the root instruction is both a parameter and the fusion instruction,\n+      // then the producer fusion is a single instruction fusion of a parameter\n+      // instruction. In that case, the parameter number refers to a parameter\n+      // in the parent computation, and we mustn't query its operands.\n+      if (&root.instruction() == &producer_fusion) {\n+        parameters_to_skip.insert(&producer_fusion);\n+      } else if (root.instruction().user_count() <= 1) {\n+        parameters_to_skip.insert(\n+            producer_fusion.operand(root.instruction().parameter_number()));\n+      }",
    "Label": "clean"
},
{
    "Id": 8,
    "Library": "tensorflow",
    "Date": "2024/06/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3e7ca2fd3b1eeda89fcdde43dce532c1f350cfaf",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in AtomicFunction destructor.\n\nWhen RUNTIME_FUNCTION_REFS is None, the code should return instead of passing.\n\nPiperOrigin-RevId: 639698538",
    "Number of deleted lines": 1,
    "Deleted lines": "-      pass",
    "Added lines": "+      return",
    "Label": "clean"
},
{
    "Id": 9,
    "Library": "tensorflow",
    "Date": "2024/05/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0a9e492aa9c5aef2356bcf59a2d4ab7a21ebaaae",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug that tries to access RUNTIME_FUNCTION_REFS after deletion\n\nPiperOrigin-RevId: 639138453",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if RUNTIME_FUNCTION_REFS is None:\n+      pass\n+",
    "Label": "clean"
},
{
    "Id": 10,
    "Library": "tensorflow",
    "Date": "2024/05/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8d97018e3fca33a15e105b50c2040f5cb1d1a3dd",
    "Root Cause": "N.A",
    "Bug report": "Enables additional solver output if we're removing user shardings, which usually implies internal testing / debugging.\n\nPiperOrigin-RevId: 634469824",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // If we're removing user shardings, we are probably doing internal testing /\n+  // debugging where additional output from the solver might be helpful.\n+  request.set_enable_output(\n+      option.preserve_shardings ==\n+      AutoShardingOption::PreserveShardingsType::kRemoveAllShardings);",
    "Label": "clean"
},
{
    "Id": 11,
    "Library": "tensorflow",
    "Date": "2024/05/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c05f5ad3d7da282726597343b19e8c6af6e71b1c",
    "Root Cause": "N.A",
    "Bug report": "Change DebugString call to proto2::TextFormat::PrintToString.\n\nPiperOrigin-RevId: 634454849",
    "Number of deleted lines": 1,
    "Deleted lines": "-  string OpsString() const { return op_list_.DebugString(); }",
    "Added lines": "+  string OpsString() const {\n+    string result;\n+    google::protobuf::TextFormat::PrintToString(op_list_, &result);\n+    return result;\n+  }",
    "Label": "clean"
},
{
    "Id": 12,
    "Library": "tensorflow",
    "Date": "2024/05/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4cd6e86c8f967c85f17a257aa319959070f6a474",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Added more logs in the autotuner to allow for better benchmark debugging during crashes.\n\nPiperOrigin-RevId: 631027110",
    "Number of deleted lines": 2,
    "Deleted lines": "-      VLOG(2) << \"Compiling the fusion: \" << fusion->name();\n-        VLOG(5) << \"Trying configuration: \" << ToString(config);",
    "Added lines": "+// VLOG(10): Print fusion computations and each configuration\n+      VLOG(10) << \"Compiling the fusion: \" << fusion->name();\n+      VLOG(10) << \"Dumping fusion computation: \"\n+               << fusion->called_computation()->ToString();\n+          VLOG(10) << \"Trying configuration: \" << ToString(config);\n+      VLOG(10) << \"Compiling the fusion: \" << fusion->name();\n+      VLOG(10) << \"Dumping fusion computation: \"\n+               << fusion->called_computation()->ToString();\n+        VLOG(10) << \"Trying configuration: \" << ToString(config);",
    "Label": "clean"
},
{
    "Id": 13,
    "Library": "tensorflow",
    "Date": "2024/05/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e33b6a9fc398eaa316b5f0180c692cfdf5e7489e",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Raise internal error on mismatched backend config descriptors.\n\nThis is more consistent with the original goal of this function. Originally, it crashed if the descriptors mismatched. It was later refactored to avoid the crash and inadvertently changed to simply fall through instead of returning an error. This fallthrough introduced a subtle bug: If the descriptors are mismatching but the stored `proto_` is not empty, the stored `proto_` will be overwritten.\n\nPiperOrigin-RevId: 630368223",
    "Number of deleted lines": 3,
    "Deleted lines": "-    if (proto_ptr->GetDescriptor() == proto->GetDescriptor()) {\n-      proto->CopyFrom(*proto_ptr);\n-      return OkStatus();",
    "Added lines": "+    if (proto_ptr->GetDescriptor() != proto->GetDescriptor()) {\n+      return Internal(\"Mismatched backend config descriptors.\");\n+\n+    proto->CopyFrom(*proto_ptr);\n+    return OkStatus();",
    "Label": "clean"
},
{
    "Id": 14,
    "Library": "tensorflow",
    "Date": "2024/04/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5695fb751b34c1e43dd66514d8aaa8c47f7d7272",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in flatbuffer_operator.cc when importing vhlo.float_v1 attribute.\n\nFloatV1Attr expects an vhlo type instead of a regelar mlir type.\n\nPiperOrigin-RevId: 629578682",
    "Number of deleted lines": 8,
    "Deleted lines": "-static mlir::Attribute BuildVhloFloatV1Attr(::llvm::APFloat value,\n-                                            mlir::Type type,\n-  return mlir::vhlo::FloatV1Attr::get(builder.getContext(), type,\n-                                      std::move(value));\n-      mlir_vector.push_back(BuildVhloFloatV1Attr(llvm::APFloat(value.AsFloat()),\n-                                                 mlir::Float32Type(), builder));\n-        composite_attribute_pair.second = BuildVhloFloatV1Attr(\n-            llvm::APFloat(value.AsFloat()), mlir::Float32Type(), builder);",
    "Added lines": "+static mlir::Attribute BuildVhloFloatV1Attr(float value,\n+  mlir::StablehloVhloTypeConverter type_converter;\n+  auto vhlo_type =\n+      type_converter.convertType(builder.getF32FloatAttr(value).getType());\n+  return mlir::vhlo::FloatV1Attr::get(builder.getContext(), vhlo_type,\n+                                      ::llvm::APFloat(value));\n+      mlir_vector.push_back(BuildVhloFloatV1Attr(value.AsFloat(), builder));\n+        composite_attribute_pair.second =\n+            BuildVhloFloatV1Attr(value.AsFloat(), builder);",
    "Label": "clean"
},
{
    "Id": 15,
    "Library": "tensorflow",
    "Date": "2024/04/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0572875fcdcaa379df4670c5c72073db9b286033",
    "Root Cause": "N.A",
    "Bug report": "[tsl:concurrency] Workaround for MSVC compiler bug for std::enable_if_t specialization\n\nhttps://godbolt.org/z/oj9o6eraa - fixed in msvc 19.32\n\nPiperOrigin-RevId: 629414959",
    "Number of deleted lines": 3,
    "Deleted lines": "-      std::enable_if_t<(!is_status_v<T> &&\n-                        std::is_invocable_v<Waiter, absl::Status> &&\n-                        !std::is_invocable_v<Waiter, absl::StatusOr<T*>>)>;",
    "Added lines": "+      std::enable_if_t<(std::is_invocable_v<Waiter, absl::Status> &&\n+                        !std::is_invocable_v<Waiter, absl::StatusOr<T*>> &&\n+                        !is_status_v<T>)>;",
    "Label": "clean"
},
{
    "Id": 16,
    "Library": "tensorflow",
    "Date": "2024/04/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/43117ea9f137c5cc035ea764c8d09533e44e6e7e",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Avoid segfault in serializing autotune results.\n\nIn some cases, the debug options reference may become invalid in\nGpuCompiler::RunHloPasses, at which point serializing segfaults upon access to\nthe debug options.\n\nThe reason for the invalid reference is not clear, as all methods operate on\nunique_ptr::get(), not on raw pointers that would invalidate it, but taking a\ncopy of a modest-sized variable is not the end of the world from a performance\nperspective even if it's not ideal.\n\nA never-used call to GetAutotuneConfig is also removed from RunHloPasses.\n\nPiperOrigin-RevId: 627461704",
    "Number of deleted lines": 4,
    "Deleted lines": "-  const DebugOptions& debug_opts = module->config().debug_options();\n-  TF_ASSIGN_OR_RETURN(\n-      AutotuneConfig autotune_config,\n-      GetAutotuneConfig(stream_exec, debug_opts, options, gpu_target_config));",
    "Added lines": "+  const DebugOptions debug_opts = module->config().debug_options();",
    "Label": "clean"
},
{
    "Id": 17,
    "Library": "tensorflow",
    "Date": "2024/03/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/58060b4017b971435b5473c21da3d5228a9e41da",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Fix possible bug in libdevice path logic.\n\n* When copying a C string, we need to include the null terminator byte. It seems likely this code reads off the end of the array since it only copies non-null bytes. But if we use a std::string it will handle this for us.\n* dirname() may not be thread-safe. On Linux it likely is, but better to avoid it and just use the TSL utility for this.\n\nThis may be the cause of a flaky test failure in JAX CI.\n\nPiperOrigin-RevId: 618267961",
    "Number of deleted lines": 5,
    "Deleted lines": "-    auto lib = std::vector<char>{info.dli_fname,\n-                                 info.dli_fname + strlen(info.dli_fname)};\n-    auto dir = dirname(lib.data());\n-    for (auto path : {\"/../nvidia/cuda_nvcc\", \"/../../nvidia/cuda_nvcc\"})\n-      roots.emplace_back(std::string(dir) + path);",
    "Added lines": "+#include \"tsl/platform/path.h\"\n+    auto lib = std::string(info.dli_fname);\n+    auto dir = io::Dirname(lib);\n+    for (auto path : {\"../nvidia/cuda_nvcc\", \"../../nvidia/cuda_nvcc\"})\n+      roots.emplace_back(io::JoinPath(dir, path));",
    "Label": "clean"
},
{
    "Id": 18,
    "Library": "tensorflow",
    "Date": "2024/03/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/304704326d063da9fdc6713f5e8c07bd27d5f44e",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] [NFC] Switch DynCast to Cast\n\nThe pointer is dereferenced later anyway, DynCast is just hiding bugs.\n\nPiperOrigin-RevId: 618244837",
    "Number of deleted lines": 3,
    "Deleted lines": "-          DynCast<HloGetTupleElementInstruction>(operand);\n-        start = DynCast<HloSendRecvInstruction>(\n-        start = DynCast<HloSendRecvInstruction>(",
    "Added lines": "+          Cast<HloGetTupleElementInstruction>(operand);\n+        start = Cast<HloSendRecvInstruction>(\n+        start = Cast<HloSendRecvInstruction>(",
    "Label": "clean"
},
{
    "Id": 19,
    "Library": "tensorflow",
    "Date": "2024/03/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f0ae6583f5d39ac2f3d53e1ac22bedd51ccc6d8d",
    "Root Cause": "N.A",
    "Bug report": "#tf-data To help debugging, output the autotuned `max_outstanding_requests` in xprof.\n\nPiperOrigin-RevId: 618198886",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  int64_t autotuned_max_outstanding_requests = model::kAutotune;\n+    autotuned_max_outstanding_requests = max_outstanding_requests_;\n+  if (params_.max_outstanding_requests == model::kAutotune) {\n+    result.push_back(std::make_pair(\n+        \"autotuned_max_outstanding_requests\",\n+        strings::Printf(\"%lld\", static_cast<long long>(\n+                                    autotuned_max_outstanding_requests))));\n+  }",
    "Label": "clean"
},
{
    "Id": 20,
    "Library": "tensorflow",
    "Date": "2024/03/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/33e9917b87b81e82eae5a7a8cade883f3d58203a",
    "Root Cause": "N.A",
    "Bug report": "PR #10687: [CPU] Add SimplifyFPConversions only if xla_allow_excess_precision\n\nImported from GitHub PR https://github.com/openxla/xla/pull/10687\n\nSeveral weeks ago it was a change which enables \"simplify-fp-conversions\" pass in cpu_compiler.cc for intel cpus unconditionally.\n\n[PR-8402](https://github.com/openxla/xla/pull/8402) - [XLA:CPU] [oneDNN] Enable Dot op (MatMul) in BF16 Type\n\nI noticed the following issue with having \"simplify-fp-conversions\" pass in cpu_compiler.cc enabled unconditionally.\n\nMy model uses bf16 operators (e.g. convolution). I want to jit compile and run it on CPU preserving intermediate bf16 accuracy.\n\nCpu compiler uses`float-normalization-bf16` pass which converts bf16 convolution to f32_convolution + convert_to_bf16 + convert_to_f32. (because typical cpu does not support bf16 computation)\n\nCpu compiler (on XEON) also uses `simplify-fp-conversions` pass which simplifies `f32_convolution + convert_to_bf16 + convert_to_f32` to just `f32_convolution`.\n\nAs the result - the whole model was converted to f32 precision internally and conversion to bf16 happens only at the very end.\n\nIn some cases we want to execute bf16 model on CPU but get results with accuracy similar to the case when it is executed on bf16 hardware.\n\nTo control the accuracy we can use debug_option `xla_allow_excess_precision`\nBy default it is true - hence, `simplify-fp-conversions` pass is enabled.\n\nIf we need to emulate bf16 computation on intel cpu we can set `XLA_FLAGS=\"--xla_allow_excess_precision=false\"` - in this case `simplify-fp-conversions` will not be added to cpu_compiler pipeline. f32 ops results will be converted to bf16 immediately. This will preserve bf16 accuracy internally.\n\n[gpu_compiler.cc](https://github.com/openxla/xla/blob/main/xla/service/gpu/gpu_compiler.cc#L1359) already enables `SimplifyFPConversions` pass only if `debug_options.xla_allow_excess_precision()` is true.\nCopybara import of the project:\n\n--\n796dc83ef34455e53b83c02dc68cd6d71306e654 by Alexander Pivovarov <pivovaa@amazon.com>:\n\n[CPU] Add SimplifyFPConversions only if xla_allow_excess_precision\n\nMerging this change closes #10687\n\nPiperOrigin-RevId: 617460913",
    "Number of deleted lines": 2,
    "Deleted lines": "-    pipeline.AddPass<SimplifyFPConversions>();\n-    pipeline.AddPass<SimplifyFPConversions>();",
    "Added lines": "+    auto debug_options = module->config().debug_options();\n+    // Remove `f32 -> bf16 -> f32` casts inserted by bf16 normalization.\n+    if (debug_options.xla_allow_excess_precision()) {\n+      pipeline.AddPass<SimplifyFPConversions>();\n+    }\n+    if (debug_options.xla_allow_excess_precision()) {\n+      pipeline.AddPass<SimplifyFPConversions>();\n+    }",
    "Label": "clean"
},
{
    "Id": 21,
    "Library": "tensorflow",
    "Date": "2024/03/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9c1b61a664d66b15d07c25e713d8d96f6bf347c8",
    "Root Cause": "N.A",
    "Bug report": "#tf-data Set the iterator prefix and `DebugString` of `GlobalShuffleDataset` to `GlobalShuffle` to be consistent with other datasets.\n\nPiperOrigin-RevId: 617037057",
    "Number of deleted lines": 3,
    "Deleted lines": "-    return name_utils::DatasetDebugString(kGlobalShuffleDataset);\n-      Iterator::Params{\n-          this, name_utils::IteratorPrefix(kGlobalShuffleDataset, prefix)},",
    "Added lines": "+constexpr const char kDatasetType[] = \"GlobalShuffle\";\n+    return name_utils::DatasetDebugString(kDatasetType);\n+      Iterator::Params{this, name_utils::IteratorPrefix(kDatasetType, prefix)},",
    "Label": "clean"
},
{
    "Id": 22,
    "Library": "tensorflow",
    "Date": "2024/03/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6270fba5f2a619733802a8cd315e6f4dc54a42c9",
    "Root Cause": "N.A",
    "Bug report": "PR #10433: Fixes type annotation overload of `dlpack_managed_tensor_to_buffer` in `python/xla_extension`\n\nImported from GitHub PR https://github.com/openxla/xla/pull/10433\n\nEncountered bug in https://github.com/google/jax/pull/20175 (see this [comment](https://github.com/google/jax/pull/20175#issuecomment-1989572870)).\n\nThis adjusts the stub file to properly overload `dlpack_managed_tensor_to_buffer` so that both signatures can be checked against.\nCopybara import of the project:\n\n--\n75cabb5149b4a0bdd9e819fac0ea6a0ba756bff6 by Meekail Zain <zainmeekail@gmail.com>:\n\nUpdate\n\nMerging this change closes #10433\n\nPiperOrigin-RevId: 615973838",
    "Number of deleted lines": 6,
    "Deleted lines": "-# Legacy overload\n-def dlpack_managed_tensor_to_buffer(\n-    tensor: Any,\n-    cpu_backend: Optional[Client] = ...,\n-    gpu_backend: Optional[Client] = ...,\n-) -> ArrayImpl: ...",
    "Added lines": "+@overload\n+@overload\n+def dlpack_managed_tensor_to_buffer( # Legacy overload\n+    tensor: Any,\n+    cpu_backend: Optional[Client] = ...,\n+    gpu_backend: Optional[Client] = ...,\n+) -> ArrayImpl: ...",
    "Label": "clean"
},
{
    "Id": 23,
    "Library": "tensorflow",
    "Date": "2024/03/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3a25086bb131f6dee91af2b72ef515e8651dc98d",
    "Root Cause": "N.A",
    "Bug report": "Fix a build rule bug.\n\nPiperOrigin-RevId: 615168635",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"//xla:status\",\n+        \"//xla/service:hlo_parser\",",
    "Label": "clean"
},
{
    "Id": 24,
    "Library": "tensorflow",
    "Date": "2024/03/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/aa196c211622ce756ca1b04839560f7f21f41a5f",
    "Root Cause": "N.A",
    "Bug report": "[XLA:Python] Fix bug introduced by recent change that mapped int4 to int8 in a type conversion.\n\nPiperOrigin-RevId: 614756539",
    "Number of deleted lines": 2,
    "Deleted lines": "-      return to_nb_dtype(NPY_INT8);\n-      return to_nb_dtype(NPY_UINT8);",
    "Added lines": "+      return custom_dtypes.int4;\n+      return custom_dtypes.uint4;",
    "Label": "clean"
},
{
    "Id": 25,
    "Library": "tensorflow",
    "Date": "2024/03/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0be2df838182f145ffa13df7c78c75ba55b2d22f",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in which an absl hashtable is used with inconsistent hash/eq functors.\n\n`eq(k1, k2) -> hash(k1) == hash(k2)` must be true for hashtable usage to be valid.\n\nPiperOrigin-RevId: 613709475",
    "Number of deleted lines": 3,
    "Deleted lines": "-      return H::combine(std::move(h), n.has_job, n.job, n.has_replica,\n-                        n.replica, n.has_task, n.task, n.has_type, n.type,\n-                        n.has_id, n.id);",
    "Added lines": "+      return H::combine(std::move(h), n.has_job ? n.job : \"\",\n+                        n.has_replica ? n.replica : 0, n.has_task ? n.task : 0,\n+                        n.has_type ? n.type : \"\", n.has_id ? n.id : 0);",
    "Label": "clean"
},
{
    "Id": 26,
    "Library": "tensorflow",
    "Date": "2024/02/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/28756cc0ee44382c42cbd9b365ab54799c743eb2",
    "Root Cause": "N.A",
    "Bug report": "Remove a debug print.\n\nPiperOrigin-RevId: 606988923",
    "Number of deleted lines": 2,
    "Deleted lines": "-    LOG(INFO) << \"CC \" << compute_cost << \" \"\n-              << device_mesh_.dim(e.mesh_dims[0]);",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 27,
    "Library": "tensorflow",
    "Date": "2024/02/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8bbba12f90e2b31ea7069d49ca3601f25b929646",
    "Root Cause": "N.A",
    "Bug report": "Fixing bug in new mat_mul_op.h definition",
    "Number of deleted lines": 1,
    "Deleted lines": "-  ~CSRMatMulCPUOp() override;",
    "Added lines": "+  ~CSRMatMulCPUOp() override{};",
    "Label": "clean"
},
{
    "Id": 28,
    "Library": "tensorflow",
    "Date": "2024/02/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e9631117dfdc53faa84f307e36da412c9ced75eb",
    "Root Cause": "N.A",
    "Bug report": "PR #8919: Turn the HLO verifier always on around float normalization on GPU\n\nImported from GitHub PR https://github.com/openxla/xla/pull/8919\n\nCurrently, the GPU pipeline only turns the verifier around float normalization on only in debug. If float normalization generates ill-typed IR, the compiler can produce invalid code without anything failing. This patch turns the verification always on.\nCopybara import of the project:\n\n--\n8606a6b140091a099c541e1d48f6f685828c58d8 by Jaroslav Sevcik <jsevcik@nvidia.com>:\n\nTurn the HLO $verifier always on around float normalization on GPU\n\n--\ne3dc564b580f5fa3e0663a87326451d0fab3db1c by Jaroslav Sevcik <jsevcik@nvidia.com>:\n\nRun verifier only once\n\n--\n13f4f25deb24250ed71775b514fad4de3071e2b8 by Jaroslav Sevcik <jsevcik@nvidia.com>:\n\nAdd a context string to the verifier pass\n\n--\nc97786e4024aa97062f0c48c117168d48360e318 by Jaroslav Sevcik <jsevcik@nvidia.com>:\n\nOnly verify in non-debug builds\n\nMerging this change closes #8919\n\nPiperOrigin-RevId: 605295966",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+#ifdef NDEBUG\n+  // Verify the module in non-debug builds. For debug builds, the verifier\n+  // already runs after every pass.\n+  pipeline.AddPass<HloVerifier>(\n+      std::make_unique<DefaultVerifierMetadata>(\n+          HloVerifierOpts{}\n+              .MakeLayoutSensitive()\n+              .WithInstructionCanChangeLayout(\n+                  LayoutAssignment::InstructionCanChangeLayout)\n+              .VerifyBroadcastDimensionsOrder()\n+              .VerifyReshapeIsBitcast()),\n+      \"end-of-post-layout_assignment\");\n+#endif  // NDEBUG\n+",
    "Label": "clean"
},
{
    "Id": 29,
    "Library": "tensorflow",
    "Date": "2024/02/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a4709906daf8654f57f34d4ae6bae66df3b50d0f",
    "Root Cause": "N.A",
    "Bug report": "#tf-data Fix a bug in `max_outstanding_requests` autotuning. If a node does not match the predicate passed to `CollectNodes()`, it skips all nodes in its subtree. Since we are looking for the `DataService` node, we should get all nodes and iterate to filter out non `DataService` nodes instead of passing a predicate that matches the `DataService` node.\n\nPiperOrigin-RevId: 605033720",
    "Number of deleted lines": 7,
    "Deleted lines": "-// Helper function for node traversal that returns only `DataService` nodes.\n-inline bool IsDataServiceNode(const std::shared_ptr<Node> node) {\n-  return absl::StartsWith(node->name(), kDataService);\n-}\n-\n-  auto subtree_nodes =\n-      snapshot->CollectNodes(TraversalOrder::BFS, IsDataServiceNode);",
    "Added lines": "+  auto subtree_nodes = snapshot->CollectNodes(TraversalOrder::BFS, IsAnyNode);\n+    if (!absl::StartsWith(node->name(), kDataService)) {\n+      continue;\n+    }",
    "Label": "clean"
},
{
    "Id": 30,
    "Library": "tensorflow",
    "Date": "2024/01/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1f976fc26c9e031eaf34b32449d4ae096608f25d",
    "Root Cause": "N.A",
    "Bug report": "Hookup debug data logger to the old bridge XlaCompiler\n\nPiperOrigin-RevId: 602371154",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (VLOG_IS_ON(2)) {",
    "Added lines": "+#include \"tensorflow/core/util/debug_data_dumper.h\"\n+  if (VLOG_IS_ON(2) || DEBUG_DATA_DUMPER()->ShouldDump(name, kDebugGroupMain)) {",
    "Label": "clean"
},
{
    "Id": 31,
    "Library": "tensorflow",
    "Date": "2024/01/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/072c9076d56230c72ef27c948abec5e5f6101adb",
    "Root Cause": "N.A",
    "Bug report": "Register IFFT2D for complex128.\n\nIt's required for the gradient of FFT2D, which already does have that type registered.  The XLA\nimplementation also seems to already be tested in `//third_party/tensorflow/python/kernel_tests/signal:fft_ops_test_xla_gpu` -\nadding debug statements, the kernel is compiled and executed correctly, even without the registration.\n\nPiperOrigin-RevId: 600503329",
    "Number of deleted lines": 3,
    "Deleted lines": "-REGISTER_XLA_OP(Name(\"IFFT\").TypeConstraint(\"Tcomplex\", DT_COMPLEX64),\n-REGISTER_XLA_OP(Name(\"IFFT2D\").TypeConstraint(\"Tcomplex\", DT_COMPLEX64),\n-REGISTER_XLA_OP(Name(\"IFFT3D\").TypeConstraint(\"Tcomplex\", DT_COMPLEX64),",
    "Added lines": "+REGISTER_XLA_OP(Name(\"IFFT\").TypeConstraint(\"Tcomplex\",\n+                                            {DT_COMPLEX64, DT_COMPLEX128}),\n+REGISTER_XLA_OP(Name(\"IFFT2D\").TypeConstraint(\"Tcomplex\",\n+                                              {DT_COMPLEX64, DT_COMPLEX128}),\n+REGISTER_XLA_OP(Name(\"IFFT3D\").TypeConstraint(\"Tcomplex\",\n+                                              {DT_COMPLEX64, DT_COMPLEX128}),",
    "Label": "clean"
},
{
    "Id": 32,
    "Library": "tensorflow",
    "Date": "2024/01/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ffc4b2535c52c03fd8da451c3333ef5588466234",
    "Root Cause": "N.A",
    "Bug report": "Suppress debug message when TryCast fails\n\nPiperOrigin-RevId: 600388683",
    "Number of deleted lines": 2,
    "Deleted lines": "-    llvm::dbgs() << \"Failed to match \" << name << \" (\" << T::getOperationName()\n-                 << \").\\n\";",
    "Added lines": "+    DEBUG_WITH_TYPE(\"mlir-quant-attrs-and-constraints\",\n+                    llvm::dbgs() << \"Failed to match \" << name << \" (\"\n+                                 << T::getOperationName() << \").\\n\");",
    "Label": "clean"
},
{
    "Id": 33,
    "Library": "tensorflow",
    "Date": "2024/01/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/16a53751b9e48afe5302e13d0f1edd144460a36a",
    "Root Cause": "N.A",
    "Bug report": "Improves logging/debuggability in the hlo verifier.\n\nPiperOrigin-RevId: 598187586",
    "Number of deleted lines": 2,
    "Deleted lines": "-  TF_RET_CHECK(SameElementType(broadcast->shape(), operand_shape));\n-  TF_RET_CHECK(operand_shape.rank() == broadcast->dimensions().size());",
    "Added lines": "+  TF_RET_CHECK(SameElementType(broadcast->shape(), operand_shape))\n+      << broadcast->ToString();\n+  TF_RET_CHECK(operand_shape.rank() == broadcast->dimensions().size())\n+      << broadcast->ToString();",
    "Label": "clean"
},
{
    "Id": 34,
    "Library": "tensorflow",
    "Date": "2024/01/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0d6d425e00eb931fb6c69c3de16a3962f0fbab1a",
    "Root Cause": "N.A",
    "Bug report": "Avoid invalid-bool-load in GemmRewriter\n\nThe `{a,b}_mult_scale` bools are created as uninitialized variables\nand are never written if the dot arguments are of type FP8.\n\nThis trips UBSAN when these bools later are getting handed over\nto `CreateF8CustomCall` while still being uninitialized.\n\nThis is not a logic bug because the values of `{a,b}_mult_scale` doesn't\nmatter if `{a,b}_scale` is a nullptr.\n\nPiperOrigin-RevId: 597758424",
    "Number of deleted lines": 1,
    "Deleted lines": "-    bool a_mult_scale, b_mult_scale;",
    "Added lines": "+    bool a_mult_scale{}, b_mult_scale{};",
    "Label": "clean"
},
{
    "Id": 35,
    "Library": "tensorflow",
    "Date": "2024/01/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6c0ae64819af838154ebbf8816267d28387c0879",
    "Root Cause": "N.A",
    "Bug report": "TFLite Converter users are seeing a lot text printing to the terminal to display the conversion statistics. While this is really useful for debugging, this is not something every customer will want to see in production.\n\nMoving the `createPrintOpStatsPass` pass to run behind a flag that is used for debugging purposes.\n\nPiperOrigin-RevId: 597674724",
    "Number of deleted lines": 8,
    "Deleted lines": "-  pass_manager.clear();\n-  // Print out a detailed report of ops that are not converted to TFL ops.\n-  pass_manager.addPass(mlir::odml::createPrintOpStatsPass(\n-      mlir::odml::GetAcceptedTFLiteDialects()));\n-  if (failed(pass_manager.run(module))) {\n-    return statusHandler.ConsumeStatus();\n-  }\n-",
    "Added lines": "+    pass_manager.clear();\n+    // Print out a detailed report of ops that are not converted to TFL ops.\n+    pass_manager.addPass(mlir::odml::createPrintOpStatsPass(\n+        mlir::odml::GetAcceptedTFLiteDialects()));\n+    if (failed(pass_manager.run(module))) {\n+      return statusHandler.ConsumeStatus();\n+    }\n+",
    "Label": "clean"
},
{
    "Id": 36,
    "Library": "tensorflow",
    "Date": "2024/01/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c150bb6edd8d82686efeeff8c1083bcd0a57c96f",
    "Root Cause": "N.A",
    "Bug report": "PR #8051: [ROCm] Fix GraphDebugDotPrint build failure\n\nImported from GitHub PR https://github.com/openxla/xla/pull/8051\n\nCopybara import of the project:\n\n--\n34acc4f468e124c97663057c318f3149f43df4a8 by Dragan Mladjenovic <Dragan.Mladjenovic@amd.com>:\n\n[ROCm] Fix GraphDebugDotPrint build failure\n\nMerging this change closes #8051\n\nPiperOrigin-RevId: 595077142",
    "Number of deleted lines": 5,
    "Deleted lines": "-/* static */ tsl::Status GpuDriver::GraphDebugDotPrint(hipGraph_t graph,\n-                                                       const char* path) {\n-  if (VLOG_IS_ON(100)) {\n-      VLOG(200) << \"HIP graph \" << graph << \" debug file:\\n\" << data;\n-  return ::tsl::OkStatus();",
    "Added lines": "+/* static */ tsl::StatusOr<std::string> GpuDriver::GraphDebugDotPrint(\n+    hipGraph_t graph, const char* path, bool return_printed_graph) {\n+  if (return_printed_graph) {\n+      return data;\n+  return std::string(path);",
    "Label": "clean"
},
{
    "Id": 37,
    "Library": "tensorflow",
    "Date": "2024/01/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a32a1c9123a7c4fb926c1cdc3d99188a171cdfe5",
    "Root Cause": "N.A",
    "Bug report": "Do not assume that debug_options parameter is equal to flag_values.\n\nSome other callers of MakeDebugOptions() may pass some different value.\n\nPiperOrigin-RevId: 595037569",
    "Number of deleted lines": 6,
    "Deleted lines": "-  auto float_setter_for = [](void (DebugOptions::*member_setter)(float)) {\n-    return [member_setter](float value) {\n-      (flag_values->*member_setter)(value);\n-      return true;\n-    };\n-  };",
    "Added lines": "+  auto float_setter_for =\n+      [debug_options](void (DebugOptions::*member_setter)(float)) {\n+        return [debug_options, member_setter](float value) {\n+          (debug_options->*member_setter)(value);\n+          return true;\n+        };\n+      };",
    "Label": "clean"
},
{
    "Id": 38,
    "Library": "tensorflow",
    "Date": "2023/12/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cd2998837aba5e77e789fd848704f695905874e5",
    "Root Cause": "N.A",
    "Bug report": "Update bug number for bug that has been marked as a duplicate.\n\nPiperOrigin-RevId: 592623308",
    "Number of deleted lines": 1,
    "Deleted lines": "-        \"no_oss\",  # TODO(b/283016506)",
    "Added lines": "+        \"no_oss\",  # TODO(b/283033375)",
    "Label": "clean"
},
{
    "Id": 39,
    "Library": "tensorflow",
    "Date": "2023/12/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c1b5fe83a0fb5558a76155d4d3978c8c7e2b8d75",
    "Root Cause": "N.A",
    "Bug report": "[XLA:Python] Fix bug that mean XLA_PYTHON_CLIENT_PREALLOCATE was no longer parsed correctly.\n\nFixes https://github.com/google/jax/issues/19035\n\nPiperOrigin-RevId: 592074504",
    "Number of deleted lines": 7,
    "Deleted lines": "-  memory_fraction = (\n-      options['memory_fraction'] if 'memory_fraction' in options else None\n-  )\n-  preallocate = options['preallocate'] if 'preallocate' in options else None\n-  if memory_fraction:\n-    config.memory_fraction = float(memory_fraction)\n-  config.preallocate = preallocate not in ('0', 'false', 'False')",
    "Added lines": "+  if 'memory_fraction' in options:\n+    config.memory_fraction = options['memory_fraction']\n+  if 'preallocate' in options:\n+    config.preallocate = options['preallocate']",
    "Label": "clean"
},
{
    "Id": 40,
    "Library": "tensorflow",
    "Date": "2023/12/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8044abd7e8eb4558519a03906a4728d21a2797bd",
    "Root Cause": "N.A",
    "Bug report": "fixed unitialized GemmConfig fields (regression bug)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        .grad_x = false,\n+        .grad_y = false,",
    "Label": "clean"
},
{
    "Id": 41,
    "Library": "tensorflow",
    "Date": "2023/12/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84002d3519423aa5edca41e47761e5702173d7cf",
    "Root Cause": "N.A",
    "Bug report": "PR #7344: [XLA:GPU] Error out for ptxas 12.3.1 version\n\nImported from GitHub PR https://github.com/openxla/xla/pull/7344\n\nptxas 12.3.1 has a bug that we think can affect XLA. Error out for this version.\nCopybara import of the project:\n\n--\nf210fe2f386f870eba4d3ea5c786f897300b4821 by Ayan Moitra <amoitra@nvidia.com>:\n\nptxas 12.3.1 has a bug that we think can affect XLA. Error out for this version\n\n--\n9058e1607953eec3b3e7d5a3431665bedbe53866 by Ayan Moitra <amoitra@nvidia.com>:\n\nfix\n\n--\n7881b7711d9f4f6a1bb1b2c960125bebb634a80c by Ayan Moitra <amoitra@nvidia.com>:\n\nnit fix\n\nMerging this change closes #7344\n\nPiperOrigin-RevId: 588355928",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  auto ptxas_version_tuple = GetAsmCompilerVersion(options.preferred_cuda_dir);\n+  if (ptxas_version_tuple.value() == std::array<int64_t, 3>{12, 3, 1}) {\n+    return tsl::errors::Internal(\n+        absl::StrFormat(\"ptxas 12.3.1 has a bug that we think can affect XLA. \"\n+                        \"Please use a different version.\"));\n+  }",
    "Label": "clean"
},
{
    "Id": 42,
    "Library": "tensorflow",
    "Date": "2023/11/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/45889deaa34cd5982f10410bf60eb88f6e66bb0e",
    "Root Cause": "N.A",
    "Bug report": "If TensorBoard fails during reading Debugger V2 files then their names will be logged.\n\nPiperOrigin-RevId: 582124510",
    "Number of deleted lines": 1,
    "Deleted lines": "-            debug_event.debug_metadata.tensorflow_version)",
    "Added lines": "+            debug_event.debug_metadata.tensorflow_version\n+        )\n+      except Exception as e:\n+        raise errors.DataLossError(\n+            None,\n+            None,\n+            \"Error reading tfdbg metadata from paths %s\" % metadata_paths,\n+        ) from e",
    "Label": "clean"
},
{
    "Id": 43,
    "Library": "tensorflow",
    "Date": "2023/11/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1c5e88a84c70f8c3b29879634a5ea9247cfa8ac0",
    "Root Cause": "N.A",
    "Bug report": "Make the logging the max_ids/unique_ids default in the sparse core xla ops. This will make debugging easier.\n\nPiperOrigin-RevId: 582048387",
    "Number of deleted lines": 8,
    "Deleted lines": "-    VLOG(3) << \"XlaSparseDenseMatmulWithCsrInputOp: \"\n-            << \"table_name = '\" << table_name_\n-            << \"', max_ids = \" << max_ids_per_partition\n-            << \", max_uniques = \" << max_unique_ids_per_partition;\n-    VLOG(3) << \"XlaSparseDenseMatmulWithCsrInputOp: \"\n-            << \"table_name = '\" << table_name_\n-            << \"', max_ids = \" << max_ids_per_partition\n-            << \", max_uniques = \" << max_unique_ids_per_partition;",
    "Added lines": "+    LOG(INFO) << \"Lowering XlaSparseDenseMatmulWithCsrInputOp to HLO: \"\n+              << \"table_name = '\" << table_name_\n+              << \"', max_ids = \" << max_ids_per_partition\n+              << \", max_uniques = \" << max_unique_ids_per_partition;\n+    LOG(INFO) << \"Lowering XlaSparseDenseMatmulGradWithCsrInputOp to HLO: \"\n+              << \"table_name = '\" << table_name_\n+              << \"', max_ids = \" << max_ids_per_partition\n+              << \", max_uniques = \" << max_unique_ids_per_partition;",
    "Label": "clean"
},
{
    "Id": 44,
    "Library": "tensorflow",
    "Date": "2023/11/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9c492018a641ac6b5a03b65a77ff9d1ce60699eb",
    "Root Cause": "N.A",
    "Bug report": "Rollback of PR #61276\n\nRollback cl/581345279: return assert_shapes for debugging.assert_shapes\n\nDue to many breakages.\n\nPiperOrigin-RevId: 581385793",
    "Number of deleted lines": 3,
    "Deleted lines": "-  return assert_shapes(\n-      shapes, data=data, summarize=summarize, message=message, name=name\n-  )",
    "Added lines": "+  assert_shapes(\n+      shapes, data=data, summarize=summarize, message=message, name=name)",
    "Label": "clean"
},
{
    "Id": 45,
    "Library": "tensorflow",
    "Date": "2023/11/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/65bf54877e14a39a163c43f135fec6681186c153",
    "Root Cause": "N.A",
    "Bug report": "[XLA:Collective] Fix bug in reduce-scatter matching where AR->Reshape->DUS is not correctly recognized.\n\nPiperOrigin-RevId: 579323081",
    "Number of deleted lines": 2,
    "Deleted lines": "-  for (int64_t dim = 0; dim < instruction->shape().rank(); ++dim) {\n-    if (instruction->shape().dimensions(dim) == user->shape().dimensions(dim)) {",
    "Added lines": "+  for (int64_t dim = 0; dim < user->operand(0)->shape().rank(); ++dim) {\n+    if (user->operand(0)->shape().dimensions(dim) ==\n+        user->shape().dimensions(dim)) {",
    "Label": "clean"
},
{
    "Id": 46,
    "Library": "tensorflow",
    "Date": "2023/11/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4fc8f7df100bcbaceecdb6a1ce73d54515c1daa8",
    "Root Cause": "N.A",
    "Bug report": "[xla_compile] Respect XLA_FLAGS when creating DebugOptions for MLIR modules.\n\nThe HLO path already respects XLA_FLAGS via its use of HloModuleLoader.\n\nPiperOrigin-RevId: 579208999",
    "Number of deleted lines": 1,
    "Deleted lines": "-  DebugOptions debug_options = DefaultDebugOptionsIgnoringFlags();",
    "Added lines": "+  DebugOptions debug_options = GetDebugOptionsFromFlags();",
    "Label": "clean"
},
{
    "Id": 47,
    "Library": "tensorflow",
    "Date": "2023/11/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/91063eeabcca8f2aef47ea774ab6245c8afd09f5",
    "Root Cause": "N.A",
    "Bug report": "PR #6697: fix build failure of run_hlo_module and interactive_graphviz\n\nImported from GitHub PR https://github.com/openxla/xla/pull/6697\n\ncuda_executor references command_buffer here:\nhttps://github.com/openxla/xla/blob/main/xla/stream_executor/cuda/cuda_executor.cc#L426\n\ntsl::Status GpuExecutor::Submit(Stream* stream,\n                                const CommandBuffer& command_buffer) {\n  if (command_buffer.mode() != CommandBuffer::Mode::kPrimary) {\n    return absl::InvalidArgumentError(\n        \"Can't submit non-primary command buffer for execution\");\n  }\n\n  auto exec = GpuCommandBuffer::Cast(&command_buffer)->executable();\n  VLOG(3) << \"Launch command buffer execuable graph \" << exec\n          << \" on a stream: \" << stream->DebugStreamPointers();\n  return GpuDriver::GraphLaunch(exec, AsGpuStreamValue(stream));\n}\n\nThere are some missing dependencies declarations\nCopybara import of the project:\n\n--\n4eb67bf5b16319f93594a7784a546bf8d04a5c9f by Shawn Wang <shawnw@nvidia.com>:\n\nfix build failure of run_hlo_module and interactive_graphviz\n\nMerging this change closes #6697\n\nPiperOrigin-RevId: 579106507",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"//xla/stream_executor:command_buffer\",",
    "Label": "clean"
},
{
    "Id": 48,
    "Library": "tensorflow",
    "Date": "2023/10/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b5b0443099811e48471ee98657ff7e409a7ed6fc",
    "Root Cause": "N.A",
    "Bug report": "Add back function names so that debug outputs can be associated with the correct function.\n\nPiperOrigin-RevId: 578297471",
    "Number of deleted lines": 5,
    "Deleted lines": "-            module, tf2xla::v2::DeviceType::XLA_TPU_JIT, fallback_enabled));\n-            module, tsl::DeviceType(DEVICE_TPU_XLA_JIT)));\n-            module, tf2xla::v2::DeviceType::XLA_GPU_JIT, fallback_enabled));\n-            module, tsl::DeviceType(DEVICE_GPU_XLA_JIT)));\n-  return tensorflow::tf2xla::v2::ExportFromTensorflowDialectToExecutor(module);",
    "Added lines": "+            module, tf2xla::v2::DeviceType::XLA_TPU_JIT, fallback_enabled,\n+            function_name));\n+            module, tsl::DeviceType(DEVICE_TPU_XLA_JIT), function_name));\n+            module, tf2xla::v2::DeviceType::XLA_GPU_JIT, fallback_enabled,\n+            function_name));\n+            module, tsl::DeviceType(DEVICE_GPU_XLA_JIT), function_name));\n+  return tensorflow::tf2xla::v2::ExportFromTensorflowDialectToExecutor(\n+      module, function_name);",
    "Label": "clean"
},
{
    "Id": 49,
    "Library": "tensorflow",
    "Date": "2023/10/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/eaf46a8fc870a0025e36588fc7bdb18b329b73be",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug where `PyHostSendAndRecvLoadedHostCallback::serializer_` was destructed inline\n\n`PyHostSendAndRecvLoadedHostCallback` does not necessarily get destructed while holding GIL, so it's safer to use `GlobalPyRefManager()` for destructing Python objects owned by it. `callable_` already follows this pattern and this CL makes `serializer_` do the same.\n\nPiperOrigin-RevId: 576189083",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  GlobalPyRefManager()->AddGarbage(\n+      absl::MakeSpan(static_cast<pybind11::object*>(&serializer_), 1));",
    "Label": "clean"
},
{
    "Id": 50,
    "Library": "tensorflow",
    "Date": "2023/10/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/04fb826f98b92dd172ad665d8a5522a2f8201867",
    "Root Cause": "N.A",
    "Bug report": "Report errors that trigger step abort to improve debuggability.\n\nPiperOrigin-RevId: 574613218",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  LOG(WARNING) << \"Local rendezvous is aborting with status: \" << status;",
    "Label": "clean"
},
{
    "Id": 51,
    "Library": "tensorflow",
    "Date": "2023/10/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b597c9d09dced6817d50e5469da635d5f44094e0",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Add some extra debugging in collective pipeliner\n\nPiperOrigin-RevId: 573253247",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        VLOG(5) << \"Skipping \" << instr->name()\n+                << \" because didn't find compatible slice of parameter\";",
    "Label": "clean"
},
{
    "Id": 52,
    "Library": "tensorflow",
    "Date": "2023/10/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1bbd10c3bb8de86bfe126ae7ace123f7e85354cf",
    "Root Cause": "N.A",
    "Bug report": "Extend coverage of TF2XLA MLIR generic phase 1 bridge\n\nTF2XLA bridge currently only allows CPU/GPU graphs in parameter server jobs with resource variable arguments to go through the phase 1 bridge. This change removes this restriction to cover more graphs. It would allow feature parity  (outside compilation, distributed strategy, etc) on CPU/GPU/TPU graphs to provide long-term benefits (outside compilation, consistent clustering, debuggability, etc), and unblocks the unification of CPU/GPU and TPU MLIR-based phase 1 pipelines.\n\nPiperOrigin-RevId: 570504216",
    "Number of deleted lines": 46,
    "Deleted lines": "-// Check if the `graph` has parameter serverjobs and resource variable arguments\n-// that are on parameter servers\n-bool HasPsWithResourceVariable(const Graph& graph) {\n-  // Check parameter serverjobs and resource variable arguments that are\n-  // on parameter servers.\n-  const std::string jobType = \"ps\";\n-  const std::string nodeType = \"_Arg\";\n-  const std::string attrKey = \"T\";\n-  for (const Node* node : graph.nodes()) {\n-    if (node->type_string() == nodeType) {\n-      auto device_name = node->assigned_device_name();\n-      DeviceNameUtils::ParsedName device;\n-      if (DeviceNameUtils::ParseFullName(device_name, &device) &&\n-          device.has_job && device.job == jobType) {\n-        for (const auto& attr : node->attrs()) {\n-          auto attr_key = attr.first;\n-          auto attr_value = attr.second;\n-          if (attr_key == attrKey &&\n-              attr_value.value_case() == AttrValue::kType &&\n-              attr_value.type() == DT_RESOURCE) {\n-            return true;\n-            break;\n-          }\n-        }\n-      }\n-    }\n-  }\n-  return false;\n-}\n-\n-// Check if non TPU pipeline should be used\n-bool EnableNonTpuBridge(const Graph& graph) {\n-  // Remark that this is staging change. It will be expanded later for further\n-  // check based on the requirement.\n-  return HasPsWithResourceVariable(graph) && HasQualifiedNonTPUOp(graph);\n-}\n-\n-  if (!run_tpu_bridge && !EnableNonTpuBridge(graph)) {\n-    // Only record CPU/GPU graphs that are qualified but filtered out\n-    if (HasQualifiedNonTPUOp(graph)) {\n-      metrics::UpdateTfMlirBridgeFirstPhaseCounter(\n-          /*device type*/ \"cpu/gpu\",\n-          /*bridge version*/ \"tfxla\",\n-          /*fallback_enabled*/ false,\n-          /*result*/ \"invalid_graph\");\n-    }",
    "Added lines": "+  if (!run_tpu_bridge && !HasQualifiedNonTPUOp(graph)) {",
    "Label": "clean"
},
{
    "Id": 53,
    "Library": "tensorflow",
    "Date": "2023/09/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7c45b3e9692e1c92587f21ccce3274102b757e3b",
    "Root Cause": "N.A",
    "Bug report": "There was a bug in which the MSA best fit repacker was overriding GetTemporalBufferIntervalCompare(), but that override was never being executed because it's callsite was in a constructor. A previous change, removed the override.\n\nThis change makes GetTemporalBufferIntervalCompare() non-virtual, so that the mistake is not repeated.\n\nThis is a no-op since there are not overrides in the codebase.\n\nPiperOrigin-RevId: 568930191",
    "Number of deleted lines": 1,
    "Deleted lines": "-  virtual BufferIntervalCompare GetTemporalBufferIntervalCompare() const;",
    "Added lines": "+  BufferIntervalCompare GetTemporalBufferIntervalCompare() const;",
    "Label": "clean"
},
{
    "Id": 54,
    "Library": "tensorflow",
    "Date": "2023/09/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/42af713661bf7324c0a8e9b2ee21750ec417cd5e",
    "Root Cause": "N.A",
    "Bug report": "PR #5881: [XLA:GPU] remove unused buffers to fix cudnn graph\n\nImported from GitHub PR https://github.com/openxla/xla/pull/5881\n\nThis PR is to fix the bug introduced in commit https://github.com/openxla/xla/pull/5184/commits/25aa1baa87abb259e3e0350737c4d39a715183a0 in runner clean up PR: https://github.com/openxla/xla/pull/5184.\n\nUnused buffers in fused attention need to be removed from data_vec so cudnn graph finalization doesn't error out.\nCopybara import of the project:\n\n--\n0e3302504012794982a2a7acbc157767ccf07fcb by cjkkkk <ske@nvidia.com>:\n\nrm unused buffers to fix cudnn graph\n\n--\nb16f756f7f71dd6f1b5959dfe29ff8ec619bae6f by cjkkkk <ske@nvidia.com>:\n\nfix format\n\nMerging this change closes #5881\n\nPiperOrigin-RevId: 568432367",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (sizeof...(Args) == 7 || sizeof...(Args) == 11) {\n+      // is fused attention fwd and bwd\n+      // remove empty buffers from the list\n+      data_ptrs_vec.erase(\n+          std::remove(data_ptrs_vec.begin(), data_ptrs_vec.end(), nullptr),\n+          data_ptrs_vec.end());\n+      // ensure the size is equal after removing useless pointers\n+      CHECK(data_ptrs_vec.size() == data_uids_vec.size());\n+    }\n+",
    "Label": "clean"
},
{
    "Id": 55,
    "Library": "tensorflow",
    "Date": "2023/09/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fb36fd261aeb2c7a11ebe3fe1f533685b4c030dc",
    "Root Cause": "N.A",
    "Bug report": "Update debugger_cli_common.py",
    "Number of deleted lines": 1,
    "Deleted lines": "-      A list of str as the help information fo cmd_prefix. If the cmd_prefix",
    "Added lines": "+      A list of str as the help information for cmd_prefix. If the cmd_prefix",
    "Label": "clean"
},
{
    "Id": 56,
    "Library": "tensorflow",
    "Date": "2023/09/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ebe4f498df55278020b5c7a78d6a98b13b990294",
    "Root Cause": "N.A",
    "Bug report": "Fix comment bug in tsl's clean_dep\n\nPiperOrigin-RevId: 565223624",
    "Number of deleted lines": 3,
    "Deleted lines": "-    \"\"\"Returns string to 'target' in @local_tsl repository.\n-    Use this function when referring to targets in the @local_tsl\n-    # Label() call appears, i.e. @local_tsl.",
    "Added lines": "+    \"\"\"Returns string to 'target' in the TSL repository.\n+    Use this function when referring to targets in the TSL\n+    # Label() call appears, e.g. @local_tsl or tsl.",
    "Label": "clean"
},
{
    "Id": 57,
    "Library": "tensorflow",
    "Date": "2023/09/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/67c0625e50468e980d8c97fe5bf74870ec0e7fe5",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Fix theoretical bug where shmem_usage > shmem_budget\n\nPiperOrigin-RevId: 565152217",
    "Number of deleted lines": 9,
    "Deleted lines": "-  // TODO(vuson): two things are wrong here:\n-  // (1) If num_partial_results == 1, and shmem_usage is big enough (or\n-  // shmem_budget small enough), we will execute the loop once, turning\n-  // num_partial_results to 0.\n-  // (2) The loop was originally applied to both row and column reductions, we\n-    while (shmem_usage * num_partial_results > shmem_budget) {\n-      if (num_partial_results == 1) {\n-        break;\n-      }",
    "Added lines": "+  // TODO(vuson): something is wrong here:\n+  // The loop was originally applied to both row and column reductions, we\n+    while (num_partial_results != 1 &&\n+           shmem_usage * num_partial_results > shmem_budget) {",
    "Label": "clean"
},
{
    "Id": 58,
    "Library": "tensorflow",
    "Date": "2023/09/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/68cc4388bc7ea92a0eb24abd28377323da254f79",
    "Root Cause": "N.A",
    "Bug report": "#tf.data Log `tf.data.Options` used to ease debugging.\n\nPiperOrigin-RevId: 564518057",
    "Number of deleted lines": 1,
    "Deleted lines": "-  LOG(INFO) << \"`tf.data.Options` values set are \" << options.DebugString();",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 59,
    "Library": "tensorflow",
    "Date": "2023/08/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/48e400d57f48b96c069fbf1d3adcec97c34e140b",
    "Root Cause": "N.A",
    "Bug report": "#tf.data Log `tf.data.Options` used to ease debugging.\n\nPiperOrigin-RevId: 561527499",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  LOG(INFO) << \"`tf.data.Options` values set are \" << options.DebugString();",
    "Label": "clean"
},
{
    "Id": 60,
    "Library": "tensorflow",
    "Date": "2023/08/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/62296aa33e61aad357d43489ce6df0d0846b9ded",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Unset some debug options for autotuning compilations\n\nThis would probably just waste memory.\n\nPiperOrigin-RevId: 560047723",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  opts_.set_xla_embed_ir_in_executable(false);\n+  opts_.set_xla_gpu_enable_persistent_temp_buffers(false);",
    "Label": "clean"
},
{
    "Id": 61,
    "Library": "tensorflow",
    "Date": "2023/08/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ca6374646ea9e98c59994dc24dd88b57680c3059",
    "Root Cause": "N.A",
    "Bug report": "[TF][MLIR] Fix `RegionBranchOpInterface` method in `tf.WhileRegion` op\n\nThis commit fixes the following two minor bugs in the `getSuccessorRegions()` interface method of the `tf.WhileRegion` op:-\n\n1. The `cond` region successor of both `body` and the op is incorrectly marked to have the arguments of `body` (instead of `cond`) as region inputs.\n2. `body` is not included as a region successor of the op (to cover the case when the `cond` terminator doesn't forward the `cond` arguments).\n\nPiperOrigin-RevId: 559895225",
    "Number of deleted lines": 3,
    "Deleted lines": "-        RegionSuccessor(&getCond(), getBody().front().getArguments()));\n-    // The parent branches to 'cond'.\n-        RegionSuccessor(&getCond(), getBody().front().getArguments()));",
    "Added lines": "+        RegionSuccessor(&getCond(), getCond().front().getArguments()));\n+    // The parent branches to 'cond'. It is also considered to branch to `body`\n+    // in case the terminator of `cond` doesn't forward the arguments of `cond`.\n+        RegionSuccessor(&getCond(), getCond().front().getArguments()));\n+    regions.push_back(\n+        RegionSuccessor(&getBody(), getBody().front().getArguments()));",
    "Label": "clean"
},
{
    "Id": 62,
    "Library": "tensorflow",
    "Date": "2023/08/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a1203dd302aed09b0f0fc2a4b5ab3ec55cdf4a66",
    "Root Cause": "N.A",
    "Bug report": "Update XlaAotOnlyVarHandleOp to match opdef of VarHandleOp (add debug_name).\n\nPiperOrigin-RevId: 558216983",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    .Attr(\"debug_name: string = ''\")",
    "Label": "clean"
},
{
    "Id": 63,
    "Library": "tensorflow",
    "Date": "2023/08/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3758184b3d4770a9ec387e136be087010dcd9bb0",
    "Root Cause": "N.A",
    "Bug report": "[xla:gpu2] Fix a bug in IREE buffer -> DeviceMemory conversion\n\nPiperOrigin-RevId: 557260411",
    "Number of deleted lines": 1,
    "Deleted lines": "-                              iree_hal_buffer_allocation_size(buffer));",
    "Added lines": "+                              iree_hal_buffer_byte_length(buffer));",
    "Label": "clean"
},
{
    "Id": 64,
    "Library": "tensorflow",
    "Date": "2023/08/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2ad6eb0c1eb3bcd31e385de3699ab0d8f08eeb74",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in which a swisstable is moved from multiple times without being reinitialized, which is UB.\n\nAlso remove the later call to `tmp_containers.clear()` - this is unnecessary since the object is about to be destroyed.\n\nPiperOrigin-RevId: 557178517",
    "Number of deleted lines": 1,
    "Deleted lines": "-  tmp_containers.clear();",
    "Added lines": "+    containers_.clear();  // reinitialize after move.",
    "Label": "clean"
},
{
    "Id": 65,
    "Library": "tensorflow",
    "Date": "2023/08/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f1588783a4b1c223b8bf125b959e0e5f0b988574",
    "Root Cause": "N.A",
    "Bug report": "#tf-data share buffer_size constant string with model.h\n\nThis removes code duplication and potentially future bugs when someone changes \"buffer_size\" in one place but forget to change in other places\n\nPiperOrigin-RevId: 552875189",
    "Number of deleted lines": 1,
    "Deleted lines": "-  static constexpr const char* const kBufferSize = \"buffer_size\";",
    "Added lines": "+#include \"tensorflow/core/framework/model.h\"\n+  static constexpr const char* const kBufferSize = model::kBufferSize;",
    "Label": "clean"
},
{
    "Id": 66,
    "Library": "tensorflow",
    "Date": "2023/07/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b63d1b6dc65f3b76dd3564f0177528c551f753c3",
    "Root Cause": "N.A",
    "Bug report": "Add release notes for bugs fixed for *FFT* ops in DTensor.\n\nPiperOrigin-RevId: 552508463",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    * \\*fft\\* ops now support dtensors with any layout. Fixed bug in 'fft2d/\n+      fft3d', 'ifft2d/ifft3d', 'rfft2d/rfft3d', and 'irfft2d/irfft3d' for\n+      sharded input.",
    "Label": "clean"
},
{
    "Id": 67,
    "Library": "tensorflow",
    "Date": "2023/07/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0440b9d643366e07cad46c71d80eb617368cc42c",
    "Root Cause": "N.A",
    "Bug report": "Update Eigen to commit:d4ae542ed1c6f3eaad29445100052489471e38ea\n\nCHANGELOG\n=========\nd4ae542ed - Fix nullptr dereference issue in triangular product.\n7769eb1b2 - Fix problems with recent changes and Tensorflow in Power\nba1cb6e45 - Fixes #2703 by adding max_digits10 function\n9995c3da6 - Fix -Wmaybe-uninitialized in SVD\n4e9e493b4 - Fix -Waggressive-loop-optimizations\n6e7abeae6 - fix arm build warnings\n81fe2d424 - Fix more gcc compiler warnings / sort-of bugs\n21cd3fe20 - Optimize check_rows_cols_for_overflow\n9297aae66 - Fix AVX512 nomalloc issues in trsm.\n1a2bfca8f - Fix annoying warnings\n63dcb429c - Fix use of arg function in CUDA.\n8f927fb52 - Altivec: fix compilation with C++20 and higher\nd4b05454a - Fix argument for _mm256_cvtps_ph imm parameter\n15ac3765c - Fix ivcSize return type in IndexedViewMethods.h\n3791ac8a1 - Fix supportsMMA to obey EIGEN_ALTIVEC_MMA_DYNAMIC_DISPATCH compilation flag and compiler support.\nbc57b926a - Add Quaternion constructor from real scalar and imaginary vector\n31cd2ad37 - Ensure EIGEN_HAS_ARM64_FP16_VECTOR_ARITHMETIC is always defined on arm.\n7465b7651 - Disable FP16 arithmetic for arm32.\nb3267f693 - Remove unused variable in  test/svd_common.h.\n\nPiperOrigin-RevId: 552130715",
    "Number of deleted lines": 1,
    "Deleted lines": "-  static inline int digits10() { return 0; }",
    "Added lines": "+  static constexpr inline int digits10() { return 0; }\n+  static constexpr inline int max_digits10() { return 0; }",
    "Label": "clean"
},
{
    "Id": 68,
    "Library": "tensorflow",
    "Date": "2023/07/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e30d645e145bbb53aedc96912ae755f071e2e97a",
    "Root Cause": "N.A",
    "Bug report": "[JAX] Use MLIR argument locations instead of a bespoke jax.arg_info attribute.\n\nhttps://github.com/llvm/llvm-project/commit/514dddbeba643e32310c508a15d7b6ff42f2c461 allowed for specifying argument Locations in the MLIR Python bindings. We should use them, in the form of a Name location, rather than making up our own attribute.\n\nExample of new output:\n\n```\nIn [1]: import jax\nIn [2]: ir = jax.jit(lambda x, y: x + y).lower(7, 3).compiler_ir()\nIn [3]: ir.operation.print(enable_debug_info=True)\n#loc1 = loc(\"x\")\n#loc2 = loc(\"y\")\nmodule @jit__lambda_ attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<i32> {mhlo.sharding = \"{replicated}\"} loc(\"x\"), %arg1: tensor<i32> {mhlo.sharding = \"{replicated}\"} loc(\"y\")) -> (tensor<i32> {jax.result_info = \"\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<i32> loc(#loc4)\n    return %0 : tensor<i32> loc(#loc)\n  } loc(#loc)\n} loc(#loc)\n#loc = loc(unknown)\n#loc3 = loc(\"<ipython-input-2-ef5a568a0c1c>\":1:0)\n#loc4 = loc(\"jit(<lambda>)/jit(main)/add\"(#loc3))\n```\n\nNote debug information must be enabled.\n\nPiperOrigin-RevId: 549325621",
    "Number of deleted lines": 1,
    "Deleted lines": "-mlir_api_version = 53",
    "Added lines": "+mlir_api_version = 54",
    "Label": "clean"
},
{
    "Id": 69,
    "Library": "tensorflow",
    "Date": "2023/07/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ebd65be4d8a6db910247c116106dae0dc28472b2",
    "Root Cause": "N.A",
    "Bug report": "[XLA/debuggability] Retains the names of the untouched instructions in the extracted module.\n\nPiperOrigin-RevId: 549047266",
    "Number of deleted lines": 3,
    "Deleted lines": "-    for (auto computation : module_->MakeComputationPostOrder()) {\n-      for (auto instruction : computation->MakeInstructionPostOrder()) {\n-        module_->SetAndUniquifyInstrName(instruction, instruction->name());",
    "Added lines": "+    // Rename HLOs so that their name matches the original. By default,\n+    // HLOs get new unique names when adding a new entry computation to\n+    // a module.\n+    for (auto computation : old_module_.MakeComputationPostOrder()) {\n+      for (auto old_instruction : computation->MakeInstructionPostOrder()) {\n+        if (auto new_instruction =\n+                clone_context_.FindInstruction(old_instruction)) {\n+          new_instruction->SetAndSanitizeName(old_instruction->name());\n+        }\n+    // For the extra created instructions (e.g., the ones created when replacing\n+    // with broadcasted zeros), we make sure they have unique names without\n+    // breaking the matches made at above code.\n+    for (HloInstruction* instruction : extra_created_instructions_) {\n+      module_->SetAndUniquifyInstrName(instruction, instruction->name());\n+    }\n+      extra_created_instructions_.push_back(zero_tuple);\n+      extra_created_instructions_.push_back(element_zero);\n+      extra_created_instructions_.push_back(zero_broadcast);\n+  std::vector<HloInstruction*> extra_created_instructions_;",
    "Label": "clean"
},
{
    "Id": 70,
    "Library": "tensorflow",
    "Date": "2023/07/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ca1e1548c86ec7484cfedd79d49cd48dd02ff93e",
    "Root Cause": "N.A",
    "Bug report": "Add logging information to Ph1 call sites\n\nThe stack trace does not indicate a bug but is used for debugging.\n\nPiperOrigin-RevId: 548715240",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include \"tensorflow/core/platform/stacktrace.h\"\n+  VLOG(2)\n+      << \"TPU Bridge called stack trace is \"\n+      << \"(NOTE: this is not an error; rather the stack trace for debugging) : \"\n+      << tensorflow::CurrentStackTrace();\n+  VLOG(2)\n+      << \"TPU V1 Compat Bridge called stack trace is \"\n+      << \"(NOTE: this is not an error; rather the stack trace for debugging) : \"\n+      << tensorflow::CurrentStackTrace();\n+  VLOG(2)\n+      << \"CPU/GPU Bridge called stack trace is \"\n+      << \"(NOTE: this is not an error; rather the stack trace for debugging) : \"\n+      << tensorflow::CurrentStackTrace();",
    "Label": "clean"
},
{
    "Id": 71,
    "Library": "tensorflow",
    "Date": "2023/07/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4af196bdf7602a3987fbac7e114e1cb01ed25484",
    "Root Cause": "N.A",
    "Bug report": "Add metrics in LegalizeTfTypesPass to help debugging\n\nPiperOrigin-RevId: 548246041",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include <memory>\n+#include <string>\n+#include <utility>\n+\n+#include \"tensorflow/core/lib/monitoring/counter.h\"\n+// TODO: b/290366702 - Temporarily added metrics for debugging.\n+auto *mlir_tf_quant_op_count = tensorflow::monitoring::Counter<1>::New(\n+    \"/tensorflow/core/tf2xla/tf_quant_op_count\" /*metric_name*/,\n+    \"Counts the number of ops that has qint types\" /*metric description*/,\n+    \"op_name\" /*metric label*/);\n+\n+    // TODO: b/290366702 - Temporarily added metrics for debugging.\n+    if (llvm::any_of(op->getResultTypes(), IsIllegalType)) {\n+      mlir_tf_quant_op_count->GetCell(std::string(op->getName().getStringRef()))\n+          ->IncrementBy(1);\n+    }",
    "Label": "clean"
},
{
    "Id": 72,
    "Library": "tensorflow",
    "Date": "2023/07/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/40eafe0e02d98f77c13607c4902bd07d8125be6d",
    "Root Cause": "N.A",
    "Bug report": "return assert_shapes for debugging.assert_shapes\n\nreturn assert_shapes for debugging.assert_shapes.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  assert_shapes(",
    "Added lines": "+  return assert_shapes(",
    "Label": "clean"
},
{
    "Id": 73,
    "Library": "tensorflow",
    "Date": "2023/07/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9335670d00c995294f8cb3eeb70ecc93fcae6bfe",
    "Root Cause": "N.A",
    "Bug report": "Do not run pjrt_c_api_gpu_test in debug mode due to failure.\n\nPiperOrigin-RevId: 547940227",
    "Number of deleted lines": 1,
    "Deleted lines": "-    tags = tf_cuda_tests_tags(),",
    "Added lines": "+    tags = tf_cuda_tests_tags() + [\"nodebug\"],  # TODO(b/291073132): Test failing in debug mode.",
    "Label": "clean"
},
{
    "Id": 74,
    "Library": "tensorflow",
    "Date": "2023/07/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/305a5a6754b725e70d6123019a694830923d84a5",
    "Root Cause": "N.A",
    "Bug report": "Do not run `cudnn_fused_conv_rewriter_test` in debug mode while we investigate an ongoing failure.\n\nPiperOrigin-RevId: 547040235",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"nodebug\",  # TODO(b/290684889): Fails in debug mode.",
    "Label": "clean"
},
{
    "Id": 75,
    "Library": "tensorflow",
    "Date": "2023/07/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/38b006d10700ab58dc9da0f7870d6981313f4bc2",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Use the new runtime in autotuning.\n\nThe old runtime is not regularly tested anymore and contains at least one known cuBLAS-related bug.\n\nPiperOrigin-RevId: 546900287",
    "Number of deleted lines": 1,
    "Deleted lines": "-  options.set_xla_gpu_enable_xla_runtime_executable(false);",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 76,
    "Library": "tensorflow",
    "Date": "2023/07/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6b5e39278b64e1b77b75b11b91b95e39eb7b64e5",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in TPUPartitionedCall op.\n\nIf it fails to set up ordinal selector, ComputeAsync() should return immediately.\n\nPiperOrigin-RevId: 546156781",
    "Number of deleted lines": 8,
    "Deleted lines": "-      OP_REQUIRES_OK_ASYNC(\n-          ctx,\n-          GetGraphFromFunction(graph.get(), /*device_ordinal=*/0,\n-                               &enable_spmd_xla_partitioning, &tpu_metadata),\n-          done);\n-  OP_REQUIRES_ASYNC(\n-      ctx, ordinal_selector_ != nullptr,\n-      errors::Internal(\"The TPUOrdinalSelector is not initialized.\"), done);",
    "Added lines": "+      // We are not using OP_REQUIRES_OK_ASYNC here as we are inside the\n+      // call_once. It will be checked later whether `ordinal_selector_` is\n+      // initialized.\n+      if (auto status = GetGraphFromFunction(graph.get(), /*device_ordinal=*/0,\n+                                             &enable_spmd_xla_partitioning,\n+                                             &tpu_metadata);\n+          !status.ok()) {\n+        init_status = std::move(status);\n+        return;\n+      }\n+  OP_REQUIRES_ASYNC(ctx, ordinal_selector_ != nullptr,\n+                    absl::InternalError(absl::StrCat(\n+                        \"The TPUOrdinalSelector is not initialized: \",\n+                        init_status.message())),\n+                    done);",
    "Label": "clean"
},
{
    "Id": 77,
    "Library": "tensorflow",
    "Date": "2023/07/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/028d339502758f2a798108e2bd7179598cfbe448",
    "Root Cause": "N.A",
    "Bug report": "gpu_delegate: Remove unnecessary debug code\n\nPiperOrigin-RevId: 545775181",
    "Number of deleted lines": 1,
    "Deleted lines": "-      fprintf(stderr, \"Found Custom Op: %s\\n\", registration->custom_name);",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 78,
    "Library": "tensorflow",
    "Date": "2023/06/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1a2a612d70d6b539947a7cef8f20ac98332997fd",
    "Root Cause": "N.A",
    "Bug report": "Update range of cuDNN versions affected by the bug.\n\nPiperOrigin-RevId: 543905309",
    "Number of deleted lines": 2,
    "Deleted lines": "-            \"cudnn_version_end\"   : -1,\n-            \"cudnn_version_end\"   : -1,",
    "Added lines": "+            \"cudnn_version_end\"   : 8902,\n+            \"cudnn_version_end\"   : 8902,",
    "Label": "clean"
},
{
    "Id": 79,
    "Library": "tensorflow",
    "Date": "2023/06/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/60443e17534e8039b53776f6f995135cbc8d7e90",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug when estimating execution counts for while loops during auto-sharding.\n\nPiperOrigin-RevId: 542894059",
    "Number of deleted lines": 4,
    "Deleted lines": "-          instruction->while_body(), loop_iteration_count_estimate,\n-          computation_execution_count * loop_iteration_count_estimate,\n-          instruction->while_condition(), loop_iteration_count_estimate,\n-          computation_execution_count * loop_iteration_count_estimate,",
    "Added lines": "+      int64_t while_body_condition_execution_count =\n+          computation_execution_count * loop_iteration_count_estimate;\n+          instruction->while_body(),\n+          /*computation_execution_count */\n+          while_body_condition_execution_count,\n+          /*loop_iteration_count_estimate*/ loop_iteration_count_estimate,\n+          instruction->while_condition(),\n+          /*computation_execution_count */\n+          while_body_condition_execution_count,\n+          /*loop_iteration_count_estimate*/ loop_iteration_count_estimate,",
    "Label": "clean"
},
{
    "Id": 80,
    "Library": "tensorflow",
    "Date": "2023/06/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cddaac85fce50dd7a99b7e4879f6ed3259e2d531",
    "Root Cause": "N.A",
    "Bug report": "Some cleanup in the auto-sharding code: 1. Change some comments to reflect code changes, 2: Remove TODOs for fixed bugs, 3: Remove an unused variable.\n\nPiperOrigin-RevId: 542657155",
    "Number of deleted lines": 44,
    "Deleted lines": "-// Choose an operand to follow.\n-// We choose to follow the operand with the highest priority.\n-// The priority is defined as a two element tuple as below, where we compare\n-// the first key first, and if the first key is the same, we compare the second\n-// key:\n-// priority(operand) = (\n-//   max(x.output_spec.num_tiles for x in operand.strategies),\n-//   depth(operand),\n-// )\n-// For example, When one operand has a sharding strategy that splits into\n-// N slices, while another operand only has replicated strategy, we will choose\n-// the first operand to follow since it can be split into more slices. When\n-// both operands can be sliced into the same number of slices, we follow the\n-// deeper one in the computational graph. When the depth is also similar,\n-// we set these operators to be \"tied\" and let the ILP solver to pick which one\n-// to follow.\n-    const absl::flat_hash_set<const HloInstruction*>& undefined_set,\n-    if (!undefined_set.count(operand)) {\n-    }\n-  absl::flat_hash_set<const HloInstruction*> undefined_set;\n-        if (undefined_set.contains(indices)) {\n-          break;\n-        }\n-            // TODO(b/220942808) Shard non-divisible dimensions.\n-        if (undefined_set.contains(operand)) {\n-          break;\n-        }\n-        if (!undefined_set.count(operand) &&\n-            !(mesh_nn_dims >= 2 && solver_option.allow_mixed_mesh_shape)) {\n-        if (undefined_set.contains(operand)) {\n-          break;\n-        }\n-            follow_idx =\n-                ChooseOperandToFollow(strategy_map, depth_map, alias_map,\n-                                      undefined_set, max_depth, ins)\n-                    .first;\n-            strategy_map, depth_map, alias_map, undefined_set, max_depth, ins);\n-      // TODO(b/208668853) If needed, we can make auto sharding faster by using\n-      // this sharding spec when merging node using strategies->following.\n-  // TODO(b/208668853): Keep shardings in custom-calls. After auto\n-  // sharding pass, instead of fixing the shardings, mark all the replicated\n-  // dims as \"unspecified\" instead of replicated. (this would require using\n-  // custom-call annotations ops instead of in-place attributes). Then run the\n-  // sharding propagation pass after that before spmd partitioner.",
    "Added lines": "+// Choose an operand to follow.  We choose to follow the operand with the\n+// highest priority.  The priority is defined as a function of two entities as\n+// below:\n+// priority(operand) =\n+//   max(x.output_spec.num_tiles for x in operand.strategies) +\n+//   depth(operand) * depth_normalizer\n+// For example, We therefore prefer one operand with strategies with a high\n+// number of tiles and operands that have a higher depth in the graph. When the\n+// priorities are similar (within range_delta), we set these operators to be\n+// \"tied\" and let the ILP solver to pick which one to follow.\n+        if (!(mesh_nn_dims >= 2 && solver_option.allow_mixed_mesh_shape)) {\n+            follow_idx = ChooseOperandToFollow(strategy_map, depth_map,\n+                                               alias_map, max_depth, ins)\n+                             .first;\n+            strategy_map, depth_map, alias_map, max_depth, ins);",
    "Label": "clean"
},
{
    "Id": 81,
    "Library": "tensorflow",
    "Date": "2023/06/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/88a4cc25921bb1bcaff62c82ffea6650895fb4ad",
    "Root Cause": "N.A",
    "Bug report": "[Linaro:ARM_CI] Skip test that fails on gcc builds\n\n//tensorflow/compiler/mlir/lite/debug:debug_test fails\nwhen built with gcc apparently due to differences in\nmangling of anonymous namespaces.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+-//tensorflow/compiler/mlir/lite/debug:debug_test \\",
    "Label": "clean"
},
{
    "Id": 82,
    "Library": "tensorflow",
    "Date": "2023/06/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/481b90050228f51d9a9facde35b96fbf8a7a9a69",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Expose the '--xla_gpu_enable_triton_softmax_fusion' debug flag.\n\nPiperOrigin-RevId: 542254508",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  flag_list->push_back(tsl::Flag(\n+      \"xla_gpu_enable_triton_softmax_fusion\",\n+      bool_setter_for(&DebugOptions::set_xla_gpu_enable_triton_softmax_fusion),\n+      debug_options->xla_gpu_enable_triton_softmax_fusion(),\n+      \"Use Triton-based Softmax fusion.\"));",
    "Label": "clean"
},
{
    "Id": 83,
    "Library": "tensorflow",
    "Date": "2023/06/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cac6232dafc13931f046aa41237eeb715469ac6f",
    "Root Cause": "N.A",
    "Bug report": "[xla:runtime] Disable GDB and Perf listeners by default\n\nListeners pollute /tmp folder with files that no one looks at. They should be enabled optionally only for performance debugging.\n\nPiperOrigin-RevId: 539490767",
    "Number of deleted lines": 2,
    "Deleted lines": "-    bool enable_gdb_listener = true;\n-    bool enable_perf_listener = true;",
    "Added lines": "+    bool enable_gdb_listener = false;\n+    bool enable_perf_listener = false;",
    "Label": "clean"
},
{
    "Id": 84,
    "Library": "tensorflow",
    "Date": "2023/06/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/543e4be9c619de7ab4903f6570f4e89ab1a2c4f1",
    "Root Cause": "N.A",
    "Bug report": "Update pybind11 to 2.10.4.\n\nIn particular, I would like to incorporate https://github.com/pybind/pybind11/pull/4461, which means that pybind11 objects are properly untracked from the Python GC before destruction, absent which we see warnings when running in a debug build of Python 3.11.\n\nPiperOrigin-RevId: 539148388",
    "Number of deleted lines": 3,
    "Deleted lines": "-        urls = tf_mirror_urls(\"https://github.com/pybind/pybind11/archive/v2.10.0.tar.gz\"),\n-        sha256 = \"eacf582fa8f696227988d08cfc46121770823839fe9e301a20fbce67e7cd70ec\",\n-        strip_prefix = \"pybind11-2.10.0\",",
    "Added lines": "+        urls = tf_mirror_urls(\"https://github.com/pybind/pybind11/archive/v2.10.4.tar.gz\"),\n+        sha256 = \"832e2f309c57da9c1e6d4542dedd34b24e4192ecb4d62f6f4866a737454c9970\",\n+        strip_prefix = \"pybind11-2.10.4\",",
    "Label": "clean"
},
{
    "Id": 85,
    "Library": "tensorflow",
    "Date": "2023/06/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/48421345a02db49011cd4536285e76535c5d67c2",
    "Root Cause": "N.A",
    "Bug report": "Fix `StatusOr` bug\n\nPiperOrigin-RevId: 538986890",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return OkStatus();",
    "Added lines": "+    return \"\";",
    "Label": "clean"
},
{
    "Id": 86,
    "Library": "tensorflow",
    "Date": "2023/06/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4c84006ca161643dc868b090f445c7f05bd74c19",
    "Root Cause": "N.A",
    "Bug report": "Update operation_selector.cc - std to absl\n\nFix using std::any_cast instead of absl::any_cast\r\nWhen abseil is built with ABSL_USES_STD_ANY (Linux), everything works even with this bug. But if not (on Windows build), this uses std::any_cast on variable created by absl::variant, causing a throwing of exception.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  auto attr = std::any_cast<ElementwiseAttributesBase<DataTypeT, T>>(",
    "Added lines": "+  auto attr = absl::any_cast<ElementwiseAttributesBase<DataTypeT, T>>(",
    "Label": "clean"
},
{
    "Id": 87,
    "Library": "tensorflow",
    "Date": "2023/06/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5641537ec30053330595da92270a45ee429d3040",
    "Root Cause": "N.A",
    "Bug report": "[XLA/GPU] Fix a bug in `MaybeBlockAfterFirstRun` for async collectives\n- The intent is to block submitting new work to the GPU on the first run\n  to allow NCCL related memory allocation to happen without deadlocks (see\n  related comment in nccl_collective_thunk.cc).\n- As a result, the host needs to be blocked right after the NCCL calls and\n  not in the `done` as the XLA scheduler could have scheduled overlapping\n  compute in between the start and done.\n\nPiperOrigin-RevId: 537028102",
    "Number of deleted lines": 6,
    "Deleted lines": "-  auto status = to_run(is_async ? async_stream : main_stream);\n-    return async_collectives->RecordEvent(uid);\n-                           CollectivesSupport* collectives,\n-  int32_t device_ordinal = stream->parent()->device_ordinal();\n-  return collectives->MaybeBlockAfterFirstRun(uid, device_ordinal, stream);\n-        .UserData<CollectivesSupport*>()",
    "Added lines": "+  se::Stream* stream = is_async ? async_stream : main_stream;\n+  auto status = to_run(stream);\n+    status = async_collectives->RecordEvent(uid);\n+    if (!status.ok()) return status;\n+  return absl::OkStatus();",
    "Label": "clean"
},
{
    "Id": 88,
    "Library": "tensorflow",
    "Date": "2023/05/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dc3368241de879d382a85f5c1fac8df4ed810c80",
    "Root Cause": "N.A",
    "Bug report": "Add INFO logs for each graph optimization cache read and write\n\nThis will be helpful to verify that this is kicking in as expected and useful for debugging in case something goes wrong.\n\nPiperOrigin-RevId: 536467295",
    "Number of deleted lines": 3,
    "Deleted lines": "-    VLOG(3) << \"Cache existed; reading from cache; file_name: \" << file_name;\n-    VLOG(3) << \"Writing optimized graph into cache: function name: \"\n-            << function_name << \", full cache file path: \" << file_name;",
    "Added lines": "+    LOG(INFO)\n+        << \"TensorFlow graph cache existed; reading from cache; function name: \"\n+        << function_name << \", full cache file path: \" << file_name;\n+    LOG(INFO)\n+        << \"Writing optimized TensorFlow graph into cache: function name: \"\n+        << function_name << \", full cache file path: \" << file_name;",
    "Label": "clean"
},
{
    "Id": 89,
    "Library": "tensorflow",
    "Date": "2023/05/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1ae19b096a4bd23befa83d7c4ae9703a933044c4",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug where symbol renaming wasn't handled correctly during TF2XLA import in XlaCallModule lowering\n\nPiperOrigin-RevId: 535675377",
    "Number of deleted lines": 2,
    "Deleted lines": "-  mlir::SymbolUserMap symbol_users(symbol_table_collection, *imported);\n-      symbol_users.replaceAllUsesWith(func, new_name);",
    "Added lines": "+  XLA_VLOG_LINES(\n+      5, absl::StrCat(\n+             \"MHLO module lowered from TF function called by XlaCallModule: \",\n+             mlir::debugString(*imported)));\n+      if (failed(mlir::SymbolTable::replaceAllSymbolUses(func, new_name,\n+                                                         *imported))) {\n+        return absl::InternalError(\n+            absl::StrCat(\"Failed to replace all symbol uses of function '\",\n+                         absl::string_view(func.getName()), \"'\"));\n+      }\n+    XLA_VLOG_LINES(\n+        5, absl::StrCat(\n+               \"XlaCallModule MHLO module after TF function call import: \",\n+               mlir::debugString(module)));",
    "Label": "clean"
},
{
    "Id": 90,
    "Library": "tensorflow",
    "Date": "2023/05/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cc199296c36488b363409415d41d053b94d071d6",
    "Root Cause": "N.A",
    "Bug report": "[XLA:CPU Next] Outline elementwise clusters.\n\nWe didn't outline before, because it triggered completely unrelated bug (cl/530892255).\n\nPiperOrigin-RevId: 534815188",
    "Number of deleted lines": 4,
    "Deleted lines": "-constexpr llvm::StringRef kElementwiseLabel = \"__elementwise_label__\";\n-    // TODO(shyshkov): Enable outlining for elementwise clusters.\n-    if (hasLabel(fusionOp, kElementwiseLabel)) return;\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 91,
    "Library": "tensorflow",
    "Date": "2023/05/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f496b47541441549f39c9ff5fb64e7cd22c1bf0f",
    "Root Cause": "N.A",
    "Bug report": "[xla:gpu] Remove debug print in add_concurrent_regions pass\n\nPiperOrigin-RevId: 533175109",
    "Number of deleted lines": 2,
    "Deleted lines": "-\n-  module.print(llvm::errs());",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 92,
    "Library": "tensorflow",
    "Date": "2023/05/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/aab2884a6f6f479871fe82a2d151b0101a66fd80",
    "Root Cause": "N.A",
    "Bug report": "Fix bug where mmap_allocation_disabled.cc was not defining all of the MMAPAllocation constructors declared in allocation.h.\n\nPiperOrigin-RevId: 532787371",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+MMAPAllocation::MMAPAllocation(int fd, size_t offset, size_t length,\n+                               ErrorReporter* error_reporter)\n+    : MMAPAllocation(error_reporter, -1) {}\n+",
    "Label": "clean"
},
{
    "Id": 93,
    "Library": "tensorflow",
    "Date": "2023/05/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a60ab1028c4f11cb91776d5a3f8d31d93f25ac6e",
    "Root Cause": "N.A",
    "Bug report": "A bug fix: ensure that Subgraph::ReplaceNodeSubsetsWithDelegateKernels\nsets the delegate kernel op id also in the TfLiteRegistrationExternal\nobject, if present.\n\nPiperOrigin-RevId: 532767835",
    "Number of deleted lines": 3,
    "Deleted lines": "-  // Annotate the registration as DELEGATE op.\n-  registration.builtin_code = BuiltinOperator_DELEGATE;\n-",
    "Added lines": "+  // Annotate the registration as DELEGATE op.\n+  registration.builtin_code = BuiltinOperator_DELEGATE;\n+  if (registration.registration_external) {\n+    registration.registration_external->builtin_code = BuiltinOperator_DELEGATE;\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 94,
    "Library": "tensorflow",
    "Date": "2023/05/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c4af3626af4b825717418463bf5d4690936853e2",
    "Root Cause": "N.A",
    "Bug report": "Add `default_applicable_licenses` to `compiler/mlir/quantization/tensorflow/debugging/BUILD`.\n\nPiperOrigin-RevId: 532430048",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    # copybara:uncomment default_applicable_licenses = [\"//tensorflow:license\"],",
    "Label": "clean"
},
{
    "Id": 95,
    "Library": "tensorflow",
    "Date": "2023/05/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e16f0155d376173a4c1be0c727a34bcf716eda30",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in `GetShardedShape` where empty tuple sharding is not handled correctly\n\nXLA allows empty tuple shapes to have non-empty (single-element) sharding. See `HloSharding::RequiredLeaves()` for more information.\n\nThe bug is fixed by leveraging existing utility functions to iterate over tuple shapes and shardings. This also potentially addresses cases where the sharding tree is a prefix of the shape tree.\n\nPiperOrigin-RevId: 532217558",
    "Number of deleted lines": 25,
    "Deleted lines": "-  if (sharding.type() == OpSharding::TUPLE) {\n-    if (!shape.IsTuple()) {\n-      return InvalidArgument(\n-          \"Got tuple OpSharding (%s) for non-tuple shape (%s)\",\n-          sharding.DebugString(), shape.ToString());\n-    }\n-    if (sharding.tuple_shardings_size() != shape.tuple_shapes_size()) {\n-      return InvalidArgument(\n-          \"Got mismatched OpSharding tuple size (%d) and shape tuple size (%d).\"\n-          \" (OpSharding: %s, shape: %s)\",\n-          sharding.tuple_shardings_size(), shape.tuple_shapes_size(),\n-          sharding.DebugString(), shape.ToString());\n-    }\n-    std::vector<Shape> sharded_subshapes;\n-    const int tuple_shapes_size = shape.tuple_shapes_size();\n-    sharded_subshapes.reserve(tuple_shapes_size);\n-    for (int i = 0; i < tuple_shapes_size; ++i) {\n-      TF_ASSIGN_OR_RETURN(\n-          Shape sharded_subshape,\n-          GetShardedShape(shape.tuple_shapes(i), sharding.tuple_shardings(i)));\n-      sharded_subshapes.emplace_back(std::move(sharded_subshape));\n-    }\n-    return ShapeUtil::MakeTupleShape(sharded_subshapes);\n-  }\n-  return hlo_sharding.TileShape(shape);",
    "Added lines": "+#include \"tensorflow/compiler/xla/shape_util.h\"\n+  if (shape.IsTuple()) {\n+    Shape sharded_shape = shape;\n+    ShapeUtil::ForEachMutableSubshape(\n+        &sharded_shape, [&](Shape* subshape, const ShapeIndex& index) {\n+          if (!subshape->IsTuple()) {\n+            HloSharding subsharding = hlo_sharding.GetSubSharding(shape, index);\n+            *subshape = subsharding.TileShape(*subshape);\n+          }\n+        });\n+    return sharded_shape;\n+  } else {\n+    return hlo_sharding.TileShape(shape);\n+  }",
    "Label": "clean"
},
{
    "Id": 96,
    "Library": "tensorflow",
    "Date": "2023/05/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6df95afdb99c792a153b9133d8152140636955b7",
    "Root Cause": "N.A",
    "Bug report": "Fix typo in debug data dumper\n\nPiperOrigin-RevId: 532153784",
    "Number of deleted lines": 1,
    "Deleted lines": "-  // Load TF_DUMP_GRAPH_PREFIX.",
    "Added lines": "+  // Load TF_DUMP_GRAPH_WRAPPED.",
    "Label": "clean"
},
{
    "Id": 97,
    "Library": "tensorflow",
    "Date": "2023/05/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4f65cb3d1f68826c221ff2df0eabeca6122733e8",
    "Root Cause": "N.A",
    "Bug report": "#tf-data Ramp down `autotune_buffer_optimization` to a 25% task level experiment to debug why some jobs got slower.\n\nPiperOrigin-RevId: 532120751",
    "Number of deleted lines": 1,
    "Deleted lines": "-                            RandomJobSamplePercentage<50>, AllTasks);",
    "Added lines": "+                            RandomJobSamplePercentage<25>,\n+                            IndependentHostTasks);",
    "Label": "clean"
},
{
    "Id": 98,
    "Library": "tensorflow",
    "Date": "2023/05/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/16e29ce9f7b4ccc9c5f3ebe7ada1954d1b1e306d",
    "Root Cause": "N.A",
    "Bug report": "debug_string_ no longer starts empty. Always set to the new value.\n\nPiperOrigin-RevId: 531366810",
    "Number of deleted lines": 8,
    "Deleted lines": "-    if (debug_string_.empty()) {\n-      debug_string_ = std::move(debug_string);\n-    }\n-  void SetToString(std::string to_string) {\n-    if (to_string_.empty()) {\n-      to_string_ = std::move(to_string);\n-    }\n-  }",
    "Added lines": "+    debug_string_ = std::move(debug_string);\n+  void SetToString(std::string to_string) { to_string_ = std::move(to_string); }",
    "Label": "clean"
},
{
    "Id": 99,
    "Library": "tensorflow",
    "Date": "2023/05/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6181230152a626432a8a704d7df8391c6e1ad647",
    "Root Cause": "N.A",
    "Bug report": "[oneDNN v3.x]: Fixed a bug in reorder primitive cache caused by non-unique keys",
    "Number of deleted lines": 3,
    "Deleted lines": "-    // longer be queried in oneDNN v3.x. We may have to refactor reorder cache\n-    // in the future to compare two dnnl::memory::desc's directly via operator==\n-    // instead of storing all their fields in the reorder cache.",
    "Added lines": "+    // longer be queried in oneDNN v3.x. In oneDNN v2.x, this was used to\n+    // create a unique key for int8 reorder primitive cache. To overcome this\n+    // limitation in oneDNN v3.x, we are using md.get_size() instead. Note that\n+    // get_size() has the limitation that it can return the same value for both\n+    // s8s8 and zero point compensation. Since we currently support only s8s8\n+    // compensation, this needs to be refactored once we support zero point\n+    // compensation.\n+    key_creator.AddAsKey(static_cast<size_t>(from_desc.get_size()));\n+    key_creator.AddAsKey(static_cast<size_t>(to_desc.get_size()));",
    "Label": "clean"
},
{
    "Id": 100,
    "Library": "tensorflow",
    "Date": "2023/05/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bf79252333c23a92239e7eb8b764b1e95c7bfdef",
    "Root Cause": "N.A",
    "Bug report": "[oneDNN v3.x]: Fixed a bug in reorder primitive cache",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    key_creator.AddAsKey(from_strides);\n+    key_creator.AddAsKey(to_strides);",
    "Label": "clean"
},
{
    "Id": 101,
    "Library": "tensorflow",
    "Date": "2023/05/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84b1508b4a1900a9452a907a6908b79d451b0718",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in HLO Rematerialization where a rematerialized instruction is not properly tracking its output buffers.\n\nPiperOrigin-RevId: 530370758",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    remat_item->buffers_output.push_back(new_buffer.id);",
    "Label": "clean"
},
{
    "Id": 102,
    "Library": "tensorflow",
    "Date": "2023/05/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1f2947c7a493d4fdc365759cc93f073ec3abb7a0",
    "Root Cause": "N.A",
    "Bug report": "fix bug",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (ptr & 0x1ULL) {",
    "Added lines": "+    constexpr uintptr_t kTag = 0x1ULL;\n+    uintptr_t value = reinterpret_cast<uintptr_t>(ptr);\n+    if (value & kTag) {",
    "Label": "clean"
},
{
    "Id": 103,
    "Library": "tensorflow",
    "Date": "2023/05/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4e48078e06970ed7ab01d57cb4ce95291614a01e",
    "Root Cause": "N.A",
    "Bug report": "Make all targets under debug/examples/v1 have strict dependencies.\n\nPiperOrigin-RevId: 529804962",
    "Number of deleted lines": 8,
    "Deleted lines": "-py_binary(\n-py_binary(\n-py_binary(\n-        \"//third_party/py/numpy\",\n-        # copybara:uncomment \"//third_party/py/tensorflow:tensorflow_compat_v1_estimator\",\n-py_binary(\n-py_binary(\n-        \"//third_party/py/numpy\",",
    "Added lines": "+load(\"//tensorflow:strict.default.bzl\", \"py_strict_binary\")\n+\n+py_strict_binary(\n+py_strict_binary(\n+py_strict_binary(\n+        # copybara:uncomment_begin(google-only)\n+        # \"//third_party/py/tensorflow:tensorflow_compat_v1_estimator\",  # build_cleaner:keep\n+        # copybara:uncomment_end\n+py_strict_binary(\n+py_strict_binary(",
    "Label": "clean"
},
{
    "Id": 104,
    "Library": "tensorflow",
    "Date": "2023/05/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b94ce3a38a9e8759c8945e5eea584ddc339243cd",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Make BufferAllocations::ToString() const\n\nThis can be good when debugging.\n\nPiperOrigin-RevId: 529362927",
    "Number of deleted lines": 1,
    "Deleted lines": "-  std::string ToString() {",
    "Added lines": "+#include <string>\n+  std::string ToString() const {",
    "Label": "clean"
},
{
    "Id": 105,
    "Library": "tensorflow",
    "Date": "2023/05/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ddb4843fbbfcc14121ab0e1e12296eb8a02b0a30",
    "Root Cause": "N.A",
    "Bug report": "Make all targets under python/debug have strict dependencies.\n\nPiperOrigin-RevId: 528904757",
    "Number of deleted lines": 3,
    "Deleted lines": "-py_library(\n-py_library(\n-        \"//tensorflow/python/debug:debug_py\",",
    "Added lines": "+load(\"//tensorflow:strict.default.bzl\", \"py_strict_library\")\n+\n+py_strict_library(\n+    tags = [\"keep_dep\"],  # Generated files need dependencies that build_cleaner would remove\n+py_strict_library(\n+        \":debug_py\",",
    "Label": "clean"
},
{
    "Id": 106,
    "Library": "tensorflow",
    "Date": "2023/04/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d769af73c5b555cb0464179b9fd49b2fc6c5b80d",
    "Root Cause": "N.A",
    "Bug report": "Fix bypass_filter in DebugDataDumper\n\nPiperOrigin-RevId: 527607293",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (!ShouldDump(name, group)) return;",
    "Added lines": "+  if (!ShouldDump(name, group) && !bypass_filter) return;",
    "Label": "clean"
},
{
    "Id": 107,
    "Library": "tensorflow",
    "Date": "2023/04/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/273005c1fb3242d4d4b667eeccf1f79082105c86",
    "Root Cause": "N.A",
    "Bug report": "Change verbose log to exclude of specific debug flag",
    "Number of deleted lines": 3,
    "Deleted lines": "-          VLOG(2) << \"Inputs have Converts but \"\n-                     \"xla_gpu_enable_reassociation_for_converted_ar is set to \"\n-                     \"false, skipping\";",
    "Added lines": "+          VLOG(2) << \"Promotions of all_reduces for reassociation will be \"\n+                     \"disabled.\";",
    "Label": "clean"
},
{
    "Id": 108,
    "Library": "tensorflow",
    "Date": "2023/04/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/af92cf224ee8e7ed4d8f577f9e24cbeab86f1738",
    "Root Cause": "N.A",
    "Bug report": "Change from std::vector::at() to std::vector::operator[] for retrieving OpKernel for performance.\n\nThe potential bugs for retrieving OpKernel should be caught during unit tests.\n\nPiperOrigin-RevId: 526549222",
    "Number of deleted lines": 3,
    "Deleted lines": "-    DCHECK_GT(runners_.size(), index)\n-    auto& result = runners_.at(index);\n-    DCHECK(result.has_value()) << \"runner is not available: index=\" << index;",
    "Added lines": "+    CHECK_GT(runners_.size(), index)  // Crash OK\n+    CHECK(runners_[index].has_value())  // Crash OK\n+        << \"runner is not available: index=\" << index;\n+    return GetUnsafe(index);\n+  }\n+\n+  const OpKernelRunner* GetUnsafe(int64_t index) const {\n+    DCHECK_GT(runners_.size(), index);\n+    auto& result = runners_[index];\n+    DCHECK(result.has_value());",
    "Label": "clean"
},
{
    "Id": 109,
    "Library": "tensorflow",
    "Date": "2023/04/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7ddf59d89ff13fd6c9086d23c71156a4d9d29ba5",
    "Root Cause": "N.A",
    "Bug report": "#tf-data Ramp down both `autotune_buffer_optimization` and `stage_based_autotune_v2` experiments to fix a bug.\n\nPiperOrigin-RevId: 526180096",
    "Number of deleted lines": 3,
    "Deleted lines": "-                            RandomJobSamplePercentage<50>, AllTasks);\n-                            RandomJobSamplePercentage<25>,\n-                            IndependentHostTasks);",
    "Added lines": "+                            RandomJobSamplePercentage<0>, AllTasks);\n+                            RandomJobSamplePercentage<0>, IndependentHostTasks);",
    "Label": "clean"
},
{
    "Id": 110,
    "Library": "tensorflow",
    "Date": "2023/04/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6f9459f9554b6624d919a3b960711534b4b79931",
    "Root Cause": "N.A",
    "Bug report": "Display 0 when ccb and pcb are empty.\n\nOtherwise this field's debugging message is almost useless (random integers).\n\nPiperOrigin-RevId: 525306969",
    "Number of deleted lines": 5,
    "Deleted lines": "-  return absl::StrCat(\"[dev:\", (prod_dev ? prod_dev->name() : \"none\"),\n-                      \", ctx:\", reinterpret_cast<uint64>(prod_ctx),\n-                      \", val:\", reinterpret_cast<uint64>(prod_value),\n-                      \", pcb:\", reinterpret_cast<uint64>(&prod_cb),\n-                      \", ccb:\", reinterpret_cast<uint64>(&cons_cb), \"]\");",
    "Added lines": "+  return absl::StrCat(\n+      \"[dev:\", (prod_dev ? prod_dev->name() : \"none\"),\n+      \", ctx:\", reinterpret_cast<uint64>(prod_ctx),\n+      \", val:\", reinterpret_cast<uint64>(prod_value),\n+      \", pcb:\", prod_cb ? reinterpret_cast<uint64>(&prod_cb) : 0,\n+      \", ccb:\", cons_cb ? reinterpret_cast<uint64>(&cons_cb) : 0, \"]\");",
    "Label": "clean"
},
{
    "Id": 111,
    "Library": "tensorflow",
    "Date": "2023/04/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fd97a6fcce1c54f5f1a38f625a56e6f646d71d63",
    "Root Cause": "N.A",
    "Bug report": "Clean up accidentally submitted printf debugging.\n\nPiperOrigin-RevId: 524813390",
    "Number of deleted lines": 1,
    "Deleted lines": "-    llvm::errs() << \"\\n\\n\" << alloca << \"\\n\";",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 112,
    "Library": "tensorflow",
    "Date": "2023/04/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84ff5be2857a9980357d2b01d6747bd866005be9",
    "Root Cause": "N.A",
    "Bug report": "Fix null pointer dereference in mlir::GetOutermostOpsOfType\n\nThe bug was found by Svace static analyzer:\n\n1. v is null\n2. v.emitError() dereferences a null pointer",
    "Number of deleted lines": 1,
    "Deleted lines": "-          v.emitError() << \"Cannot find function \" << sym.getRootReference();",
    "Added lines": "+          op->emitError() << \"Cannot find function \" << sym.getRootReference();",
    "Label": "clean"
},
{
    "Id": 113,
    "Library": "tensorflow",
    "Date": "2023/04/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/61cc8f2c89cd4f0e263e97d6180dd8d6579676be",
    "Root Cause": "N.A",
    "Bug report": "[XLA:CPU Next] Print MLIR Pass Pipeline in text form in debug mode.\n\nThis change will provide the exact MLIR pipeline that was used in XLA. Sometimes I find it more effective to debug with `mlir-opt` tool.\n\nPiperOrigin-RevId: 524084453",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+static void PrintPassPipeline(const mlir::PassManager& pm) {\n+  llvm::errs() << \"MLIR Pass Pipeline:\\n\";\n+  pm.printAsTextualPipeline(llvm::errs());\n+  llvm::errs() << \"\\n\";\n+}\n+\n+  if (DebugJitCompiler()) {\n+    PrintPassPipeline(pm);\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 114,
    "Library": "tensorflow",
    "Date": "2023/04/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4e35e4de4d75e15cf420f601beb827337d434cf3",
    "Root Cause": "N.A",
    "Bug report": "Add emit-debug info flag to odml_to_stablehlo to preserve MLIR location info\n\nPiperOrigin-RevId: 523773467",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+// NOLINTNEXTLINE\n+opt<bool> debug_info(\n+    \"debug-info\", llvm::cl::desc(\"Inclide MLIR debug location info in output.\"),\n+    llvm::cl::Optional, llvm::cl::init(false));\n+\n+  if (debug_info) {\n+    printing_flags.enableDebugInfo();\n+  }",
    "Label": "clean"
},
{
    "Id": 115,
    "Library": "tensorflow",
    "Date": "2023/04/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a12d03de6a8a2e389d8e2f5063f0b567e0b1702",
    "Root Cause": "N.A",
    "Bug report": "[xla:cpu] Remove debug artifacts\n\nPiperOrigin-RevId: 522052883",
    "Number of deleted lines": 1,
    "Deleted lines": "-#include <iostream>",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 116,
    "Library": "tensorflow",
    "Date": "2023/04/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3c44525126c088905373484293706cfd5483940c",
    "Root Cause": "N.A",
    "Bug report": "Fix null pointer dereference in xla::cpu::CustomCallOpLowering::rewriteTypedCustomCall()\n\nThe bug was found by Svace static analyzer:\n\n1. op.getBackendConfig() can be null\n2. dict will be nullptr\n3. dict.begin() dereferences a null pointer\n\nCloses #60223",
    "Number of deleted lines": 3,
    "Deleted lines": "-    auto dict = op.getBackendConfig()\n-                    ? op.getBackendConfig()->cast<mlir::DictionaryAttr>()\n-                    : nullptr;",
    "Added lines": "+    auto config = op.getBackendConfig();\n+    if (!config)\n+      return op.emitOpError(\"Failed to get backend config\");\n+    auto dict = config->cast<mlir::DictionaryAttr>();",
    "Label": "clean"
},
{
    "Id": 117,
    "Library": "tensorflow",
    "Date": "2023/04/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0527a0a7fd77da8a02c789fa4d6ca8404ee3da63",
    "Root Cause": "N.A",
    "Bug report": "Fix null pointer dereference in grappler::GetInputs\n\nThe bug was found by Svace static analyzer:\n\n1. there may be zero for-loop iterations and inode stays nullptr\n2. later it is zero-dereferenced by inode->name()",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (inode == nullptr) {\n+      return errors::Internal(\"Did not find node\");\n+    }",
    "Label": "clean"
},
{
    "Id": 118,
    "Library": "tensorflow",
    "Date": "2023/04/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/232683a9b9e567f43239592b958e4302c257d85d",
    "Root Cause": "N.A",
    "Bug report": "Fix null pointer dereference in tsl::profiler::CreateStub\n\nThe bug was found by Svace static analyzer:\n\n1. channel may be null\n2. it is passed to grpc::ProfileAnalysis::NewStub(channel)\n3. then passed to grpc::ProfileAnalysis::Stub::Stub(channel)\n4. then constructor grpc::internal::RpcMethod::RpcMethod() dereferences\n   channel via channel->RegisterMethod(name)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    return nullptr;",
    "Label": "clean"
},
{
    "Id": 119,
    "Library": "tensorflow",
    "Date": "2023/04/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7880014be998032d349419886a17481fe67bc28e",
    "Root Cause": "N.A",
    "Bug report": "#tf-data-service Add more information to SnapshotReader error message.\n\nThe test job is getting `Failed to get element: Snappy_Uncompress failed.`\nerrors. This will help debug which file causes the error.\n\nPiperOrigin-RevId: 521494593",
    "Number of deleted lines": 3,
    "Deleted lines": "-      Status s = reader_->ReadTensors(out_tensors);\n-      if (errors::IsOutOfRange(s)) {\n-      return s;",
    "Added lines": "+      Status status = reader_->ReadTensors(out_tensors);\n+      if (errors::IsOutOfRange(status)) {\n+      TF_RETURN_WITH_CONTEXT_IF_ERROR(\n+          status,\n+          \" Failed to read tf.data snapshot file: \", dataset()->chunk_file_);\n+      return status;",
    "Label": "clean"
},
{
    "Id": 120,
    "Library": "tensorflow",
    "Date": "2023/03/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/17cda590d67a3eb6e63bd12f3c423b8dd80fab10",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Propagate `xla_dump_to` to XLA DebugOptions only if not empty\n- This preserves the value set in XLA_FLAGS if no value is provided\n  on the command line.\n\nPiperOrigin-RevId: 520370129",
    "Number of deleted lines": 1,
    "Deleted lines": "-    debug_options.set_xla_dump_to(raw_options.xla_dump_to);",
    "Added lines": "+    // Overwrite xla_dump_to only if its not empty, to preserve `xla_dump_to`\n+    // from parsed XLA_FLAGS env (already populated in debug_options).\n+    if (!raw_options.xla_dump_to.empty()) {\n+      debug_options.set_xla_dump_to(raw_options.xla_dump_to);\n+    }",
    "Label": "clean"
},
{
    "Id": 121,
    "Library": "tensorflow",
    "Date": "2023/03/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/48abc8ee20876d1a159bbb836e09135664118e44",
    "Root Cause": "N.A",
    "Bug report": "Annotate Iterator::GetNext with activity watcher.\n\nThis might provide useful information for dataset deadlock debugging.\n\nPiperOrigin-RevId: 519245937",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include \"tensorflow/core/activity_watcher/activity.h\"\n+  activity_watcher::ActivityScope activity_scope([&]() {\n+    activity_watcher::Activity::Attributes attributes;\n+    attributes[\"iterator_prefix\"] = prefix();\n+    return std::make_unique<activity_watcher::Activity>(\n+        \"Iterator::GetNext\", activity_watcher::ActivityCategory::kDatasetOp,\n+        std::move(attributes));\n+  });",
    "Label": "clean"
},
{
    "Id": 122,
    "Library": "tensorflow",
    "Date": "2023/03/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6073faead9acd26646cfb1121443ebb147bd3b15",
    "Root Cause": "N.A",
    "Bug report": "After the SPMD bug fix, always take the _rewriting_take route for getitem instead of bouncing to host.\n\nPiperOrigin-RevId: 519170785",
    "Number of deleted lines": 1,
    "Deleted lines": "-_version = 143",
    "Added lines": "+_version = 144",
    "Label": "clean"
},
{
    "Id": 123,
    "Library": "tensorflow",
    "Date": "2023/03/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ba298d672c3f04444fb38a906d62f9a5f642121c",
    "Root Cause": "N.A",
    "Bug report": "Fix endless loop in tensorflow::wav::DecodeLin16WaveAsFloatVector\n\nThis bug was originally fixed by #56455\n\nRegression was introduced in 50b4baf",
    "Number of deleted lines": 5,
    "Deleted lines": "-  *new_offset = old_offset + increment;\n-  if (*new_offset > max_size) {\n-  if (*new_offset < 0) {\n-    return errors::InvalidArgument(\"Offset too large, overflowed: \",\n-                                   *new_offset);",
    "Added lines": "+  int64_t sum = old_offset + increment;\n+  if (sum > max_size) {\n+  if (sum < 0) {\n+    return errors::InvalidArgument(\"Offset too large, overflowed: \", sum);\n+  *new_offset = sum;",
    "Label": "clean"
},
{
    "Id": 124,
    "Library": "tensorflow",
    "Date": "2023/03/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ca388405c617415aebe5669a52f51d36fdb5f255",
    "Root Cause": "N.A",
    "Bug report": "#tf-data-service Add more information to FailedPrecondition error msg.\n\nThis is to help debug flaky tests when there are multiple workers in\ndistributed_save_test.\n\nPiperOrigin-RevId: 517995098",
    "Number of deleted lines": 2,
    "Deleted lines": "-    LOG(INFO) << \"Stopping writing distributed tf.data snapshot stream: \"\n-              << status.error_message();",
    "Added lines": "+    LOG(INFO) << \"Stopped writing distributed tf.data snapshot stream due to a \"\n+                 \"transient error: \"\n+              << params_.DebugString()\n+              << \". It will be retried. Status: \" << status;",
    "Label": "clean"
},
{
    "Id": 125,
    "Library": "tensorflow",
    "Date": "2023/03/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1ed40e77af71f98015bccb342a0ceb349f736b52",
    "Root Cause": "N.A",
    "Bug report": "[TF-TRT] Fix repeated layer name bug",
    "Number of deleted lines": 1,
    "Deleted lines": "-      /*validation_only=*/false, output, params->node_def));",
    "Added lines": "+      /*validation_only=*/false, output, params->node_def, op_instance));",
    "Label": "clean"
},
{
    "Id": 126,
    "Library": "tensorflow",
    "Date": "2023/03/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8805f02dd9764c9f6e47a80e538b99f20318289d",
    "Root Cause": "N.A",
    "Bug report": "Disable tflite_kernel_use_xnnpack everywhere until cpuinfo bug is fixed.\n\nPiperOrigin-RevId: 517180793",
    "Number of deleted lines": 3,
    "Deleted lines": "-        \"//conditions:default\": [\n-            \"TFLITE_KERNEL_USE_XNNPACK\",\n-        ],",
    "Added lines": "+        \"//conditions:default\": [],",
    "Label": "clean"
},
{
    "Id": 127,
    "Library": "tensorflow",
    "Date": "2023/03/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/329261a2e7fc0d85b73d0de024c6b010967b6fe1",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Fix debug build failure when CopySliceFrom is provided a empty slice.\n\nPiperOrigin-RevId: 517174634",
    "Number of deleted lines": 3,
    "Deleted lines": "-             !ShapeUtil::IsZeroElementArray(src_literal.shape())) {\n-    // Perform copy if neither src nor dest has dimensions with zero element,\n-    // otherwise it's a no-op.",
    "Added lines": "+             !ShapeUtil::IsZeroElementArray(src_literal.shape()) &&\n+             absl::c_none_of(copy_size, [](auto d) { return d == 0; })) {\n+    // Perform copy if none of src, dest and copy_size has dimensions with zero\n+    // element, otherwise it's a no-op.",
    "Label": "clean"
},
{
    "Id": 128,
    "Library": "tensorflow",
    "Date": "2023/03/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84d0615272667671f60e785d978e6b84422be3a7",
    "Root Cause": "N.A",
    "Bug report": "Remove xla_platform_info_test from TAP while debugging.\n\nPiperOrigin-RevId: 516986330",
    "Number of deleted lines": 1,
    "Deleted lines": "-    tags = tf_cuda_tests_tags(),",
    "Added lines": "+    tags = [\n+        \"no_oss\",\n+        \"notap\",\n+    ],  #TODO(b/273833440): Remove this once the test is fixed.",
    "Label": "clean"
},
{
    "Id": 129,
    "Library": "tensorflow",
    "Date": "2023/03/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ea5d849be1ea2943a8873ef68d631f702b3d4594",
    "Root Cause": "N.A",
    "Bug report": "fix bug",
    "Number of deleted lines": 1,
    "Deleted lines": "-        \"//tensorflow/core/tfrt/common:pjrt_state\",",
    "Added lines": "+        \"//tensorflow/core/tfrt/common:pjrt_state\",",
    "Label": "clean"
},
{
    "Id": 130,
    "Library": "tensorflow",
    "Date": "2023/03/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/868c6725d1122eeb321c52298dc14c37f5cb4889",
    "Root Cause": "N.A",
    "Bug report": "fix bug",
    "Number of deleted lines": 1,
    "Deleted lines": "-        \"tensorflow/core/tfrt/common:async_value_tensor\",",
    "Added lines": "+        \"//tensorflow/core/tfrt/common:async_value_tensor\",",
    "Label": "clean"
},
{
    "Id": 131,
    "Library": "tensorflow",
    "Date": "2023/03/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f1945921a30f2b914c2b8bcfb20f2c224301f24f",
    "Root Cause": "N.A",
    "Bug report": "#tf-data Change the `id_` and `parent_id_` from `int64_t` to `uint64_t` to ease debugging in xprof because they can be negative numbers otherwise.\n\nPiperOrigin-RevId: 514775214",
    "Number of deleted lines": 2,
    "Deleted lines": "-  int64_t id_ = 0;\n-  int64_t parent_id_ = 0;",
    "Added lines": "+  uint64_t id_ = 0;\n+  uint64_t parent_id_ = 0;",
    "Label": "clean"
},
{
    "Id": 132,
    "Library": "tensorflow",
    "Date": "2023/02/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e5a244d8ae2e51e4355b968f51116ec6e8460b31",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Dump GPU runtime host code in GpuCompiler\n\nThis can help us when debugging the generation of the mentioned host code.\n\nPiperOrigin-RevId: 512957579",
    "Number of deleted lines": 4,
    "Deleted lines": "-    std::unique_ptr<ThunkSequence> thunk_sequence) {\n-      entry_function_name.str(), llvm_ir::DumpToString(mlir_module),\n-      buffer_sizes.vec(), module_config.debug_options());\n-                     hlo_module->config(), ir_emitter->ConsumeThunkSequence()));",
    "Added lines": "+#include \"tensorflow/compiler/xla/hlo/ir/hlo_module.h\"\n+    std::unique_ptr<ThunkSequence> thunk_sequence,\n+    const HloModule* hlo_module_for_dump = nullptr) {\n+  std::string module_str = llvm_ir::DumpToString(mlir_module);\n+\n+  if (hlo_module_for_dump != nullptr) {\n+    DumpToFileInDirOrStdout(*hlo_module_for_dump, \"gpu_rt_host\", \"mlir\",\n+                            module_str);\n+  }\n+\n+      entry_function_name.str(), std::move(module_str), buffer_sizes.vec(),\n+      module_config.debug_options());\n+                     hlo_module->config(), ir_emitter->ConsumeThunkSequence(),\n+                     /*hlo_module_for_dump=*/hlo_module));",
    "Label": "clean"
},
{
    "Id": 133,
    "Library": "tensorflow",
    "Date": "2023/02/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/683de93d5f12edd5e9dcd9843dd32ce8b48a27eb",
    "Root Cause": "N.A",
    "Bug report": "Internal Tensorflow bug fixes\n\nPiperOrigin-RevId: 512753677",
    "Number of deleted lines": 7,
    "Deleted lines": "-// content, and computing a hash of it's string representation. This is unsafe\n-// operation, because large tensors can be represented as TensorProto, but can't\n-// be serialized to tensor content.\n-  DCHECK(success);\n-  TensorProto p;\n-  tensor.AsProtoTensorContent(&p);\n-  return DeterministicProtoHash64(p);",
    "Added lines": "+// content, and computing a hash of it's string representation. If it's failed\n+// to serialize, compute hash based on TensorProto string representation.\n+// This approach may result different hash codes with identical Tensors if they\n+// are defined with different TensorProto representations.\n+  if (success) {\n+    TensorProto p;\n+    tensor.AsProtoTensorContent(&p);\n+    return DeterministicProtoHash64(p);\n+  } else {\n+    return DeterministicProtoHash64(tp);\n+  }",
    "Label": "clean"
},
{
    "Id": 134,
    "Library": "tensorflow",
    "Date": "2023/02/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/56d215967aa7ac97e10b05b680aed52381e611ab",
    "Root Cause": "N.A",
    "Bug report": "fix debug build test failure\n\nuse an explicit float value for 0\n\nChange-Id: I5dc26705242a1fd17c5cd3b97aebc9652cbed704\nSigned-off-by: Luke Hutton <luke.hutton@arm.com>",
    "Number of deleted lines": 1,
    "Deleted lines": "-        rewriter, op, input_ty, DenseElementsAttr::get(input_ty, {0}));",
    "Added lines": "+        rewriter, op, input_ty, DenseElementsAttr::get(input_ty, {0.0f}));",
    "Label": "clean"
},
{
    "Id": 135,
    "Library": "tensorflow",
    "Date": "2023/02/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6b167daef1d6953fd908c1ced6f347e1a7758138",
    "Root Cause": "N.A",
    "Bug report": "Make `save_type` an optional argument in CapurableResource.\n\nWe don't expect `save_type` to be optional for all Trackable objects, but might as well put in a default to address a bug with the async checkpointer (this bug is being fixed in cl/511823919)\n\nPiperOrigin-RevId: 511842976",
    "Number of deleted lines": 1,
    "Deleted lines": "-  def _trackable_children(self, save_type, **kwargs):",
    "Added lines": "+  def _trackable_children(self, save_type=base.SaveType.CHECKPOINT, **kwargs):",
    "Label": "clean"
},
{
    "Id": 136,
    "Library": "tensorflow",
    "Date": "2023/02/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a1b9edad9aee7475d176e29d4af73c5e809f8be1",
    "Root Cause": "N.A",
    "Bug report": "Add extra vlogs for coordinated reads to help with debugging.\n\nPiperOrigin-RevId: 511245481",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      VLOG(3) << \"Returning from GetNext with internal error\";\n+    if (result->skip) {\n+      VLOG(3) << \"Skipping result from task \" << result->task_id;\n+    }\n+    VLOG(1) << \"Returning end_of_sequence\";",
    "Label": "clean"
},
{
    "Id": 137,
    "Library": "tensorflow",
    "Date": "2023/02/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ca291170e701b8b81fe9ffba1475c1261fcb990a",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Add missing xla_gpu_triton_gemm_any flag.\n\nxla_gpu_triton_gemm_any was only added to debug options.\n\nPiperOrigin-RevId: 511106367",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  flag_list->push_back(\n+      tsl::Flag(\"xla_gpu_triton_gemm_any\",\n+                bool_setter_for(&DebugOptions::set_xla_gpu_triton_gemm_any),\n+                debug_options->xla_gpu_triton_gemm_any(),\n+                \"Use Triton-based matrix multiplication for any GEMM it \"\n+                \"supports without filtering only faster ones.\"));",
    "Label": "clean"
},
{
    "Id": 138,
    "Library": "tensorflow",
    "Date": "2023/02/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8de1b71a23c990bc70dfa67a99a13559ee03e30a",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug where the code was storing JNI local references and expecting\nthem to still remain valid after returning from the JNI function.\n\nThis CL also ensures that we cache 'jclass' values consistently in\nall the places where we are fetching them.\n\nPiperOrigin-RevId: 510224396",
    "Number of deleted lines": 5,
    "Deleted lines": "-  jclass string_class = env->FindClass(\"java/lang/String\");\n-  jclass string_class = env->FindClass(\"java/lang/String\");\n-  jclass string_class = env->FindClass(\"java/lang/String\");\n-  static jclass list_class = env->FindClass(\"java/util/List\");\n-  static jclass long_class = env->FindClass(\"java/lang/Long\");",
    "Added lines": "+// Like JNIEnv's FindClass method, but converts the result to a\n+// JNI global reference rather than returning a local reference.\n+jclass FindClassAndMakeGlobalRef(JNIEnv* env, const char* class_name) {\n+  jclass local_ref = env->FindClass(class_name);\n+  jclass global_ref = static_cast<jclass>(env->NewGlobalRef(local_ref));\n+  env->DeleteLocalRef(local_ref);\n+  return global_ref;\n+}\n+\n+  static jclass string_class =\n+      FindClassAndMakeGlobalRef(env, \"java/lang/String\");\n+  static jclass string_class =\n+      FindClassAndMakeGlobalRef(env, \"java/lang/String\");\n+  static jclass string_class =\n+      FindClassAndMakeGlobalRef(env, \"java/lang/String\");\n+  static jclass list_class = FindClassAndMakeGlobalRef(env, \"java/util/List\");\n+  static jclass long_class = FindClassAndMakeGlobalRef(env, \"java/lang/Long\");",
    "Label": "clean"
},
{
    "Id": 139,
    "Library": "tensorflow",
    "Date": "2023/02/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c69eda182c544a38698c3b2c7071e5283bbb5849",
    "Root Cause": "N.A",
    "Bug report": "Fix debug build failure in math_approximation.cc due to 'isnan' was not declared",
    "Number of deleted lines": 2,
    "Deleted lines": "-  assert(!isnan(lower_bound));\n-  assert(!isnan(upper_bound));",
    "Added lines": "+  assert(!std::isnan(lower_bound));\n+  assert(!std::isnan(upper_bound));",
    "Label": "clean"
},
{
    "Id": 140,
    "Library": "tensorflow",
    "Date": "2023/02/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9dd17a3bf1f23e39040e85681369a8237cbf7146",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Delete debug printf\n\nPiperOrigin-RevId: 508731712",
    "Number of deleted lines": 1,
    "Deleted lines": "-    llvm::errs() << \"fond fill op = \" << fillOp;",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 141,
    "Library": "tensorflow",
    "Date": "2023/02/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3231f89230a0b1f8f0e4816604a2a36bf244e763",
    "Root Cause": "N.A",
    "Bug report": "Fix debug build failure in SnapshotStreamWriter::SnapshotStreamWriter",
    "Number of deleted lines": 1,
    "Deleted lines": "-  DCHECK_NE(iterator_, nullptr);",
    "Added lines": "+  DCHECK_NE(iterator_.get(), nullptr);",
    "Label": "clean"
},
{
    "Id": 142,
    "Library": "tensorflow",
    "Date": "2023/02/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f74375a4c18672dca65514957d9c830094b6f39c",
    "Root Cause": "N.A",
    "Bug report": "Warn and continue on GIF errors if there is at least one image read.\n\nSome GIF images seem to contain extraneous data after the image section that causes\nthe giflib library to fail to fully parse the file.  It is unclear whether\nthis is actually a bug in giflib, or if other image libraries like PIL\nsimply ignore the offending data.  Here, if at least one image is successfully read but\nthe full GIF file fails to parse, we log the error but continue loading the image.\n\nPiperOrigin-RevId: 508144639",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return nullptr;",
    "Added lines": "+\n+    // Only stop load if no images are detected.\n+    if (gif_file->ImageCount <= 0) {\n+      return nullptr;\n+    }\n+    LOG(WARNING) << *error_string;\n+",
    "Label": "clean"
},
{
    "Id": 143,
    "Library": "tensorflow",
    "Date": "2023/02/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b5eae3752e28071e805db0925e3661742ee1a5cd",
    "Root Cause": "N.A",
    "Bug report": "Remove noisy LOG info left over from debugging.\n\nPiperOrigin-RevId: 508101026",
    "Number of deleted lines": 1,
    "Deleted lines": "-    LOG(INFO) << \"Infer spmd local result \" << op->getNumResults();",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 144,
    "Library": "tensorflow",
    "Date": "2023/02/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/79f8cd01bd871bffa14c63a6562a55a71cea4398",
    "Root Cause": "N.A",
    "Bug report": "Fix bug with compilation metric.  TPU is always compiled whether jit_compile is set or not.\n\nPiperOrigin-RevId: 507546157",
    "Number of deleted lines": 1,
    "Deleted lines": "-  string compilation_option = kDisabled;",
    "Added lines": "+  string compilation_option = kDisabled;\n+    compilation_option = kEnabled;",
    "Label": "clean"
},
{
    "Id": 145,
    "Library": "tensorflow",
    "Date": "2023/02/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5d72568187569d40e6455c9a83aa8c51e730351e",
    "Root Cause": "N.A",
    "Bug report": "#tf-data-service Add more details to `rename` error message.\n\nBy default, `rename` gives only a terse error message:\nrename: No such file or directory.\n\nAdding the source and destination file names gives more information to\nhelp debug.\n\nPiperOrigin-RevId: 507502178",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return env->RenameFile(uncommitted_filename, std::string(filename));",
    "Added lines": "+  Status status = env->RenameFile(uncommitted_filename, std::string(filename));\n+  if (!status.ok()) {\n+    return tsl::errors::Internal(\"Failed to rename file: \", status.ToString(),\n+                                 \". Source: \", uncommitted_filename,\n+                                 \", destination: \", filename);\n+  }\n+  return status;",
    "Label": "clean"
},
{
    "Id": 146,
    "Library": "tensorflow",
    "Date": "2023/02/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f024cd92f64190f0efc06253ace34043c7f478ed",
    "Root Cause": "N.A",
    "Bug report": "Work around compiler bug returning an optional unique_ptr.\n\nIt looks like some compilers (e.g. gcc-7) don't like returning\na moveable value directly when the return type is `std::optional`\n(i.e. it fails to treat the returned value as an r-value and\nautomatically construct an optional instance around it).\nExplicitly creating the `std::optional` and returning _that_\nseems to work around the issue.\n\nPiperOrigin-RevId: 507062621",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return value;",
    "Added lines": "+    // Explicitly create an optional to avoid a compiler bug with gcc-7.\n+    return std::optional<Value>(std::move(value));",
    "Label": "clean"
},
{
    "Id": 147,
    "Library": "tensorflow",
    "Date": "2023/02/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0f8f19fb0ddeede2836a4431b19e2fe35018d271",
    "Root Cause": "N.A",
    "Bug report": "[tflite-gpu] Fix OpenGL slice calculation bug.\n\nPiperOrigin-RevId: 506971865",
    "Number of deleted lines": 3,
    "Deleted lines": "-      code += \"      offset.z += $channels.x$;\\n\";\n-        code += \"      offset.z += $channels.z$;\\n\";\n-        code += \"      offset.z += src_channels + $channels.z$;\\n\";",
    "Added lines": "+      code += \"      offset.z = $channels.x$;\\n\";\n+        code += \"      offset.z = $channels.z$;\\n\";\n+        code += \"      offset.z = src_channels + $channels.z$;\\n\";",
    "Label": "clean"
},
{
    "Id": 148,
    "Library": "tensorflow",
    "Date": "2023/02/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5552cb0d2200a37aa1a4d7a3a0846a6a4756e264",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in the validation_model build macro.\n\nThis build macro uses a temporary file named $(@D)/tmp, which is a problem, because this build macro is instantiated twice in the same package, and both of the generated rules are executed in parallel during the build, and both try to write to the same tmp file, and then both of them try to remove it.  This leads to one of the rules failing due to the file not being found, on account of it having been removed by the other rule.\n\nThe fix is to instead use $(@D)/<name>.tflite.tmp as the name of the temporary file, where <name> is the name of the rule.  This is sufficient to ensure that the temporary file names used by the different instantiations of this build macro are distinct.\n\nPiperOrigin-RevId: 506882948",
    "Number of deleted lines": 4,
    "Deleted lines": "-              --output=$(@D)/tmp\n-              $(@D)/tmp \\\n-          rm $(@D)/tmp\n-        \"\"\" % (jpegs, main_model, metrics_model, scale_arg, zeropoint_arg, use_ondevice_cpu_for_golden, main_model, name),",
    "Added lines": "+              --output='$(@D)/%s.tflite.tmp'\n+              '$(@D)/%s.tflite.tmp' \\\n+          rm '$(@D)/%s.tflite.tmp'\n+        \"\"\" % (\n+            jpegs,\n+            main_model,\n+            metrics_model,\n+            scale_arg,\n+            zeropoint_arg,\n+            use_ondevice_cpu_for_golden,\n+            name,\n+            name,\n+            main_model,\n+            name,\n+            name,\n+        ),",
    "Label": "clean"
},
{
    "Id": 149,
    "Library": "tensorflow",
    "Date": "2023/02/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8b1ff1161554fd18717d9cc2043ca97b099f7606",
    "Root Cause": "N.A",
    "Bug report": "Fix dimension mismatch bug in MultinomialOp GPU implementation.\n\nPiperOrigin-RevId: 506744108",
    "Number of deleted lines": 3,
    "Deleted lines": "-    To32Bit(scores).device(d) =\n-        To32Bit(logits).reshape(boc).broadcast(oso).template cast<float>() -\n-        ((-((To32Bit(noises) + 2e-30f).log())).log());",
    "Added lines": "+    Eigen::IndexList<int> flat_shape;\n+    flat_shape.set(0, batch_size * num_samples * num_classes);\n+\n+    To32Bit(scores).device(d) = To32Bit(logits)\n+                                    .reshape(boc)\n+                                    .broadcast(oso)\n+                                    .template cast<float>()\n+                                    .reshape(flat_shape) -\n+                                ((-((To32Bit(noises) + 2e-30f).log())).log());",
    "Label": "clean"
},
{
    "Id": 150,
    "Library": "tensorflow",
    "Date": "2023/02/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5bf253cb1da3a7b862f332ccdc9a277d6669ba71",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in which an invalidated reference to a hash table element is used after a potential rehash.\n\n`emplace` can cause a rehash that invalidates references to elements in the hashtable.\n\nPiperOrigin-RevId: 506313210",
    "Number of deleted lines": 8,
    "Deleted lines": "-    fanouts->emplace(to_port, std::move(from_fanouts->second));\n-    fanouts->erase(from_port);\n-    fanouts->emplace(from_port, std::move(to_fanouts->second));\n-    fanouts->erase(to_port);\n-      auto from_fanouts = fanouts->find(from_port);\n-      if (from_fanouts != fanouts->end()) {\n-        fanouts->emplace(to_port, std::move(from_fanouts->second));\n-        fanouts->erase(from_port);",
    "Added lines": "+    auto node = fanouts->extract(from_fanouts);\n+    fanouts->emplace(to_port, std::move(node.mapped()));\n+    auto node = fanouts->extract(to_port);\n+    fanouts->emplace(from_port, std::move(node.mapped()));\n+      auto node = fanouts->extract(from_port);\n+      if (!node.empty()) {\n+        fanouts->emplace(to_port, std::move(node.mapped()));",
    "Label": "clean"
},
{
    "Id": 151,
    "Library": "tensorflow",
    "Date": "2023/01/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/39234407956f746a363935d0a4dd70eb72bde99c",
    "Root Cause": "N.A",
    "Bug report": "Remove comments linking to already fixed bugs in `extension_type.py`.\n\nPiperOrigin-RevId: 506098951",
    "Number of deleted lines": 4,
    "Deleted lines": "-# TODO(b/184565242) Support customizing type relaxation for tracing.\n-# TODO(b/184565242) Support conversion to/from FullType.\n-# TODO(b/195884675) Support batch and unbatch.\n-# TODO(b/184565242) Consider using the templating system from autograph here.",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 152,
    "Library": "tensorflow",
    "Date": "2023/01/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3b716f81d39c7dbf7cf2a1be61c1493b4dacfde0",
    "Root Cause": "N.A",
    "Bug report": "[xla:gpu] Fix a bug that string constructor can receive nullptr as input\n\nPiperOrigin-RevId: 506082695",
    "Number of deleted lines": 4,
    "Deleted lines": "-  std::string xla_flags(std::getenv(\"XLA_FLAGS\"));\n-  if (!absl::StrContains(xla_flags,\n-                         \"--xla_gpu_enable_xla_runtime_executable=true\")) {\n-    return InternalError(\"Desirialization requires enabling JitRt\");",
    "Added lines": "+  if (char* xla_flags = std::getenv(\"XLA_FLAGS\")) {\n+    std::string xla_flags_str(xla_flags);\n+    if (!absl::StrContains(xla_flags_str,\n+                           \"--xla_gpu_enable_xla_runtime_executable=true\")) {\n+      return InternalError(\"Deserialization requires XLA Runtime enabled\");\n+    }\n+  } else {\n+    return InternalError(\"Deserialization requires XLA Runtime enabled\");",
    "Label": "clean"
},
{
    "Id": 153,
    "Library": "tensorflow",
    "Date": "2023/01/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/55938f8f0f9456d7a0c8dc788352813fa20cc718",
    "Root Cause": "N.A",
    "Bug report": "Fix segfault caused by registering duplicate numpy float8 casts\n\nThis is fixing the same bug as\nhttps://github.com/tensorflow/tensorflow/pull/59425 but in a different\nplace.",
    "Number of deleted lines": 3,
    "Deleted lines": "-          numpy.get())) {\n-  // Register casts between float8 types.\n-  if (!tsl::custom_float_internal::RegisterCustomFloatCast<float8_e4m3fn,",
    "Added lines": "+  bool float8_already_registered;\n+          numpy.get(), &float8_already_registered)) {\n+  // Register casts between float8 types. Only perform the cast if\n+  // float8_e4m3b11 hasn't been previously registered, presumably by a different\n+  // library. In this case, we assume the cast has also already been registered,\n+  // and registering it again can cause segfaults due to accessing an\n+  // uninitialized type descriptor in this library.\n+  if (!float8_already_registered &&\n+      !tsl::custom_float_internal::RegisterCustomFloatCast<float8_e4m3fn,",
    "Label": "clean"
},
{
    "Id": 154,
    "Library": "tensorflow",
    "Date": "2023/01/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/555dd16a4cbb4078ddf549f06617b500b8e8fe99",
    "Root Cause": "N.A",
    "Bug report": "Update tf_saved_model dialect to use FoldAdaptors\n\nThis was attempted before but the refactor script had a bug. This properly updates the dialect.\n\nPiperOrigin-RevId: 504806891",
    "Number of deleted lines": 1,
    "Deleted lines": "-    let  useFoldAPI = kEmitFoldAdaptorFolder;",
    "Added lines": "+  let  useFoldAPI = kEmitFoldAdaptorFolder;",
    "Label": "clean"
},
{
    "Id": 155,
    "Library": "tensorflow",
    "Date": "2023/01/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d9f4f05f2bf31b38e6ca5636d1a2d3048643fba6",
    "Root Cause": "N.A",
    "Bug report": "Remove uses of linkstatic from tsl/platform/cloud as the relevant bug has been fixed for years\n\nPiperOrigin-RevId: 502896091",
    "Number of deleted lines": 2,
    "Deleted lines": "-    linkstatic = 1,  # Needed since alwayslink is broken in bazel b/27630669\n-    linkstatic = 1,  # Needed since alwayslink is broken in bazel b/27630669",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 156,
    "Library": "tensorflow",
    "Date": "2023/01/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2eb474d28997b78dfac285fe7b19b0f51a15a23b",
    "Root Cause": "N.A",
    "Bug report": "[xla:gpu] Fix multiple bugs in cuda graphs outlining\n\n1. Assign contiguous ordinals to exported cuda graph capture functions\n2. Reverse block traversal to make sure we do not create use-before-defined SSA chains\n3. Do not include cloneable operations into the initial sequence, rely on post-processing pass.\n\nPiperOrigin-RevId: 502700773",
    "Number of deleted lines": 10,
    "Deleted lines": "-      // Append matched operation to the current sequence.\n-      if (succeeded(matched)) seq->emplace_back(&op, *matched);\n-    for (Block* block : blocks) {\n-      for (Operation* op : llvm::reverse(cloneable[block])) {\n-        seq.insert(seq.begin(), {op, OpCapturePattern::Capture::kClone});\n-      }\n-static void Outline(CustomCallDeclarations& custom_calls, CaptureSequence& seq,\n-                    unsigned ordinal) {\n-  if (num_move_captures < 2) return;\n-    Outline(custom_calls, seq, ordinal++);",
    "Added lines": "+      // Append matched operation to the current sequence. We only append\n+      // operations that must be moved into the graph capture function (ops with\n+      // side effects), and add cloneable operations later.\n+      if (succeeded(matched) && *matched == kMove)\n+        seq->emplace_back(&op, *matched);\n+\n+    for (Block* block : llvm::reverse(blocks)) {\n+      auto cloned = llvm::map_range(cloneable[block], [](Operation* op) {\n+        return std::make_pair(op, OpCapturePattern::Capture::kClone);\n+      });\n+      seq.insert(seq.begin(), cloned.begin(), cloned.end());\n+static LogicalResult Outline(unsigned ordinal,\n+                             CustomCallDeclarations& custom_calls,\n+                             CaptureSequence& seq) {\n+  if (num_move_captures < 2) return failure();\n+\n+  return success();\n+    if (succeeded(Outline(ordinal, custom_calls, seq))) ordinal++;",
    "Label": "clean"
},
{
    "Id": 157,
    "Library": "tensorflow",
    "Date": "2023/01/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/74e2e7d5ef9c24dd08f657309c8b5c3bd20cb0f2",
    "Root Cause": "N.A",
    "Bug report": "[xla:gpu] Do not lower lmhlo_gpu operations without any corresponding thunks\n\nFix for a bug discovered by: https://reviews.llvm.org/D141921\n\nPiperOrigin-RevId: 502675808",
    "Number of deleted lines": 5,
    "Deleted lines": "-  // Operation did not exist in the original HLO module and has no corresponding\n-  // thunks lowered by XLA:GPU compiler.\n-  if (thunks_for_op->empty())\n-    return absl::InternalError(\"No thunks emitted for the operation\");\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 158,
    "Library": "tensorflow",
    "Date": "2023/01/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6b265a78cabc368c60e555d43ecb63bd6944805d",
    "Root Cause": "N.A",
    "Bug report": "[xla:gpu] Do not lower lmhlo_gpu operations without any corresponding thunks\n\nFix for a bug discovered by: https://reviews.llvm.org/D141921\n\nPiperOrigin-RevId: 502661981",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // Operation did not exist in the original HLO module and has no corresponding\n+  // thunks lowered by XLA:GPU compiler.\n+  if (thunks_for_op->empty())\n+    return absl::InternalError(\"No thunks emitted for the operation\");\n+",
    "Label": "clean"
},
{
    "Id": 159,
    "Library": "tensorflow",
    "Date": "2023/01/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d31b064eb5d3381c32b0da600e8cd9c9c299a47d",
    "Root Cause": "N.A",
    "Bug report": "[pjrt] Override CanShapedBufferBeAccessedNow and CanBufferBeAccessedNow for\nthe interpreter TransferManager.\n\nThe methods are overridden in the same way as they are overriden for the CPU\ntransfer manager.\n\nWithout this, an assertion that transfer_manager->CanShapedBufferBeAccessedNow\nfails in AllocateDestinationBuffer when debug assertions are enabled.\n\nPiperOrigin-RevId: 502510571",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  bool CanShapedBufferBeAccessedNow(\n+      se::StreamExecutor* executor,\n+      const ShapedBuffer& device_buffer) const override {\n+    return true;\n+  }\n+\n+  bool CanBufferBeAccessedNow(\n+      se::StreamExecutor* executor,\n+      const se::DeviceMemoryBase& device_buffer) const override {\n+    return true;\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 160,
    "Library": "tensorflow",
    "Date": "2023/01/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cbc2bae6851aa5fba10fa969607bc11dec92d1b6",
    "Root Cause": "N.A",
    "Bug report": "[StableHLO] Populate TF->StableHLO errors correctly.\n\nThe old implementation ignores TF->StableHLO errors and returns debug_mhlo.mlir\nexporting errors. Ideally, if TF->StableHLO fails, the tool should fail.\n\nPiperOrigin-RevId: 501386049",
    "Number of deleted lines": 4,
    "Deleted lines": "-  auto export_path = conversion_status.ok()\n-                         ? output_path\n-                         : absl::StrCat(verbose_dir, \"/debug_stablehlo.mlir\");\n-  return ExportModule(*module, export_path, elide_large_elements_attrs);",
    "Added lines": "+  if (!conversion_status.ok()) {\n+    LOG(ERROR) << \"TF to StableHLO conversion failed: \"\n+               << conversion_status.error_message();\n+\n+    auto export_status = ExportModule(\n+        *module, absl::StrCat(verbose_dir, \"/debug_stablehlo.mlir\"),\n+        elide_large_elements_attrs);\n+    if (!export_status.ok()) {\n+      LOG(ERROR) << \"Failed to export debug_stablehlo.mlir: \"\n+                 << export_status.error_message();\n+    }\n+\n+    return conversion_status;\n+  }\n+  return ExportModule(*module, output_path, elide_large_elements_attrs);",
    "Label": "clean"
},
{
    "Id": 161,
    "Library": "tensorflow",
    "Date": "2023/01/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5dadfe4fb9629d4ca54054104754d4cc03d67623",
    "Root Cause": "N.A",
    "Bug report": "Fix a stack-use-after-free bug.\n\nPiperOrigin-RevId: 501273345",
    "Number of deleted lines": 3,
    "Deleted lines": "-  mlir::PassManager pm(module.getContext());\n-  SetupPassDebugging(module.getContext(), pm);\n-",
    "Added lines": "+\n+  mlir::PassManager pm(module.getContext());\n+  SetupPassDebugging(module.getContext(), pm);\n+",
    "Label": "clean"
},
{
    "Id": 162,
    "Library": "tensorflow",
    "Date": "2023/01/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8405f33e590e1639927dcf0658724b03a5ee6ddc",
    "Root Cause": "N.A",
    "Bug report": "Exports solver model for debugging.\n\nPiperOrigin-RevId: 500847965",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include \"file/base/helpers.h\"\n+#ifdef PLATFORM_GOOGLE\n+  // Exports the model for debugging.\n+  bool dump_model = false;\n+  if (dump_model) {\n+    operations_research::MPModelProto model_proto;\n+    solver->ExportModelToProto(&model_proto);\n+    auto write_status = file::SetTextProto(\n+        // Modify this file path if needed.\n+        absl::StrCat(\"/tmp/model_\", solver->NumVariables(), \".proto\"),\n+        model_proto, file::Defaults());\n+    if (!write_status.ok()) {\n+      LOG(ERROR) << write_status.message();\n+    }\n+  }\n+#endif",
    "Label": "clean"
},
{
    "Id": 163,
    "Library": "tensorflow",
    "Date": "2023/01/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/281e8e61224bd196b765989ef911ea1002a0f926",
    "Root Cause": "N.A",
    "Bug report": "Use `std::string` instead of `absl::string_view` for fields in `ExportOptions`.\n\n`absl::string_view` member variables is not ideal for the case where the object lifetime of the references do not exceed `RunExportPasses`.\nExample of this is happening for the `debug_name` field, which is passed a temporary object (the output of `absl::StrCat`),\nessentially providing garbage values to the mlir dump file of the export step.\n\nPiperOrigin-RevId: 500839575",
    "Number of deleted lines": 2,
    "Deleted lines": "-  absl::string_view checkpoint_dir = \"\";\n-  absl::string_view debug_name = \"tf_quant\";",
    "Added lines": "+  std::string checkpoint_dir = \"\";\n+  std::string debug_name = \"tf_quant\";",
    "Label": "clean"
},
{
    "Id": 164,
    "Library": "tensorflow",
    "Date": "2023/01/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a6dffe783a268b0270927673c8b8317aaf5bd96",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in which an iterator is compared to an end iterator from a different hashtable.\n\nPiperOrigin-RevId: 499990724",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (iter != session_key_map_.end()) {",
    "Added lines": "+  if (iter != fingerprint_key_map_.end()) {",
    "Label": "clean"
},
{
    "Id": 165,
    "Library": "tensorflow",
    "Date": "2022/12/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a61749e41165c557fc62b6e40370705f113ed8a6",
    "Root Cause": "N.A",
    "Bug report": "Remove completed TODO bugs.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+// TODO(b/132087118): move static_assert to c_api_internal when compiled with\n+// C++.",
    "Label": "clean"
},
{
    "Id": 166,
    "Library": "tensorflow",
    "Date": "2022/12/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e341c098200600138a0726a4d10a35c9e04bd60e",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Fix a bug in space-to-batch conversion wherein we avoid propagating on really\ntiny spatial dims (i.e. those smaller than low_padding)\n\nPiperOrigin-RevId: 497243932",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      ConvolutionDimensionNumbers dim_numbers =\n+          consumer->convolution_dimension_numbers();\n+\n+      ConvDetails c = GetConvolutionDetails(consumer, dim_numbers);\n+\n+      auto retval = GetSpatialDimsToSplit(consumer->mutable_operand(0));\n+      std::vector<int64_t> new_spatial_dims = retval.second;\n+\n+      auto new_activations = old_to_new_instrs_[consumer->mutable_operand(0)];\n+      // If low padding is large, there's no benefit in propagating. This\n+      // also makes halo creation unnecessarily difficult (b/246862180).\n+      if (new_activations->shape().dimensions(retval.second[0]) <\n+          c.inherent_low_padding) {\n+        return false;\n+      }\n+",
    "Label": "clean"
},
{
    "Id": 167,
    "Library": "tensorflow",
    "Date": "2022/12/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/798bd2c84a5ba83292fcd98404ad04a263ddc6df",
    "Root Cause": "N.A",
    "Bug report": "Fixx null dereference in optional_debug_tools",
    "Number of deleted lines": 10,
    "Deleted lines": "-      PrintTfLiteIntVector(\n-          node.inputs,\n-          /*collapse_consecutives=*/(node.delegate != nullptr));\n-      PrintTotalBytesOfTensors(\n-          subgraph, is_node_delegated ? TfLiteIntArrayView(&empty_int_array)\n-                                      : TfLiteIntArrayView(node.inputs));\n-      PrintTfLiteIntVector(node.outputs);\n-      PrintTotalBytesOfTensors(\n-          subgraph, is_node_delegated ? TfLiteIntArrayView(&empty_int_array)\n-                                      : TfLiteIntArrayView(node.outputs));",
    "Added lines": "+      if (node.inputs) {\n+        PrintTfLiteIntVector(\n+            node.inputs,\n+            /*collapse_consecutives=*/(node.delegate != nullptr));\n+        PrintTotalBytesOfTensors(\n+            subgraph, is_node_delegated ? TfLiteIntArrayView(&empty_int_array)\n+                                        : TfLiteIntArrayView(node.inputs));\n+      }\n+      if (node.outputs) {\n+        PrintTfLiteIntVector(node.outputs);\n+        PrintTotalBytesOfTensors(\n+            subgraph, is_node_delegated ? TfLiteIntArrayView(&empty_int_array)\n+                                        : TfLiteIntArrayView(node.outputs));\n+      }",
    "Label": "clean"
},
{
    "Id": 168,
    "Library": "tensorflow",
    "Date": "2022/12/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1523318e903209bf149db7a6a1a8eea6918c07b9",
    "Root Cause": "N.A",
    "Bug report": "Remove debugging log message.\n\nPiperOrigin-RevId: 495342284",
    "Number of deleted lines": 2,
    "Deleted lines": "-  TF_LITE_REPORT_ERROR(error_reporter_, \"ifndef\");\n-  TF_LITE_REPORT_ERROR(error_reporter_, \"endif\");",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 169,
    "Library": "tensorflow",
    "Date": "2022/12/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/01fa5e1e9485b9d1ba4cabd50edcfd214c65b9bc",
    "Root Cause": "N.A",
    "Bug report": "Use VLOG for debugging in jit_compiler\n\n-DDEBUG_XLA_RUNTIME_COMPILER is still supported for people that prefer that, but vmodule=jit_compiler=5 will now behave the same with the advantage that one does not have to recompile their code to debug.\n\nPiperOrigin-RevId: 493890994",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return false;",
    "Added lines": "+  return VLOG_IS_ON(5);",
    "Label": "clean"
},
{
    "Id": 170,
    "Library": "tensorflow",
    "Date": "2022/12/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b371e9840324f7277ebec8f20bfbb0863fca4fb0",
    "Root Cause": "N.A",
    "Bug report": "Update TODO bugs for CollectivePermute in DTensor.\n\nPiperOrigin-RevId: 493159550",
    "Number of deleted lines": 1,
    "Deleted lines": "-  // TODO(hongjunchoi): Add support fof halo exchange for GPU/CPU.",
    "Added lines": "+  // TODO(b/261485237): Add support for halo exchange for GPU/CPU.",
    "Label": "clean"
},
{
    "Id": 171,
    "Library": "tensorflow",
    "Date": "2022/12/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c68a562ce6faa98680c97e9c8404f8d486656b8d",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Don't run HloVerifier if the pass it's verifying didn't change anything.\n\nHloVerifier is consistently one of the most expensive passes in profiles.  This\nis because it runs many times!\n\nBut it turns out, we're running the verifier even after passes that claim they\ndidn't make any modifications.  If we stop doing this, we save a lot of\ncompilation time.\n\nOf course, this requires us to trust the pass's return value of \"I didn't\nmodify anything\".  There *can* be bugs in this.  But if there's a bug, we have\nbigger problems than the verifier, because e.g. HloPassFix will not work\nproperly.\n\nPiperOrigin-RevId: 493152659",
    "Number of deleted lines": 1,
    "Deleted lines": "-    TF_RETURN_IF_ERROR(RunInvariantCheckers(hlo, pass_name));",
    "Added lines": "+      TF_RETURN_IF_ERROR(RunInvariantCheckers(hlo, pass_name));",
    "Label": "clean"
},
{
    "Id": 172,
    "Library": "tensorflow",
    "Date": "2022/12/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6b790504471057ac3368aa7d69c3678e1e7b1e45",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in bidirection rnn\n\nsetup batchsize and timestamp according to the time major config\n\nSigned-off-by: xiang.zhang <xiang.zhang@verisilicon.com>",
    "Number of deleted lines": 2,
    "Deleted lines": "-    bw_output_size_array->data[0] = batch_size;\n-    bw_output_size_array->data[1] = max_time;",
    "Added lines": "+    bw_output_size_array->data[0] = (time_major) ? max_time : batch_size;\n+    bw_output_size_array->data[1] = (time_major) ? batch_size : max_time;",
    "Label": "clean"
},
{
    "Id": 173,
    "Library": "tensorflow",
    "Date": "2022/11/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/468e19b1d95126e21e0e2bfcebbd6fe8d305235c",
    "Root Cause": "N.A",
    "Bug report": "Remove completed and closed TODO bugs.\n\nRemoved completed and closed bugs internally to avoid confusion.",
    "Number of deleted lines": 3,
    "Deleted lines": "-// TODO(b/132087118): move static_assert to c_api_internal when compiled with\n-// C++.\n-  // TODO(b/128420794): Include the TFLite runtime version in the log.",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 174,
    "Library": "tensorflow",
    "Date": "2022/11/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/aae5d9f251a50cb5bf7ebe87fce18f78e03233e5",
    "Root Cause": "N.A",
    "Bug report": "Remove completed TODO bugs.\n\nRemoved completed todo list to avoid confusion, which is marked as fixed internally.",
    "Number of deleted lines": 3,
    "Deleted lines": "-# TODO(b/128420794): Migrate clients to use :version directly.\n-        \"no_windows\",  # TODO(b/194459105): the test is flaky.\n-        \"nomsan\",  # TODO(b/186359792)",
    "Added lines": "+        \"no_windows\",\n+        \"nomsan\",",
    "Label": "clean"
},
{
    "Id": 175,
    "Library": "tensorflow",
    "Date": "2022/11/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/72d57d68121257c089a39cf99c406591ad9ccb5d",
    "Root Cause": "N.A",
    "Bug report": "Host send/recv with maximal sharding shouldn't be treated as MPMD\n\nSharding propagation has a heuristic from MPMD days that propagates MPMD single-device instructions to their containing while loop. But it accidentally triggers on host send/recv instructions with maximal sharding, which are common in an SPMD context for outside compilation/debugging workflows. We add a \"not a host transfer\" condition to this heuristic.\n\nPiperOrigin-RevId: 491722069",
    "Number of deleted lines": 4,
    "Deleted lines": "-      } else if (opcode == HloOpcode::kSend || opcode == HloOpcode::kRecv ||\n-                 ((opcode == HloOpcode::kAllReduce ||\n-                   opcode == HloOpcode::kReduceScatter) &&\n-                  instruction->channel_id())) {",
    "Added lines": "+      } else if (((opcode == HloOpcode::kSend || opcode == HloOpcode::kRecv) &&\n+                  !Cast<HloSendRecvInstruction>(instruction)\n+                       ->is_host_transfer())\n+                 || ((opcode == HloOpcode::kAllReduce ||\n+                      opcode == HloOpcode::kReduceScatter) &&\n+                     instruction->channel_id())) {",
    "Label": "clean"
},
{
    "Id": 176,
    "Library": "tensorflow",
    "Date": "2022/11/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4eac0efea5fc5d1d24deb062a5f4096175ead812",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in Gather SPMD Expansion\n\nMaking a dtensor user's colab crash because tf.gather was called on a negative batch dim and we didnt take care of that\n\nPiperOrigin-RevId: 490615623",
    "Number of deleted lines": 2,
    "Deleted lines": "-\n-",
    "Added lines": "+  if (batch_dims < 0) batch_dims += indices_rank;\n+  if (batch_dims < 0) batch_dims += indices_rank;\n+    if (batch_dims < 0) batch_dims += indices_rank;",
    "Label": "clean"
},
{
    "Id": 177,
    "Library": "tensorflow",
    "Date": "2022/11/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/40e2cbf3dfd56022d1ad91880e63f77c950e4d28",
    "Root Cause": "N.A",
    "Bug report": "Add an example for tf.debugging.assert_all_finite\n\nPiperOrigin-RevId: 489311873",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  >>> @tf.function\n+  ... def f(x):\n+  ...   x = tf.debugging.assert_all_finite(x, 'Input x must be all finite')\n+  ...   return x + 1\n+\n+  >>> f(tf.constant([np.inf, 1, 2]))\n+  Traceback (most recent call last):\n+     ...\n+  InvalidArgumentError: ...\n+",
    "Label": "clean"
},
{
    "Id": 178,
    "Library": "tensorflow",
    "Date": "2022/11/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b9175486c37414542f73bf11739620e27f6cb6c8",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in UnpackOperationParser for GPU delegate.\n\nPiperOrigin-RevId: 488953206",
    "Number of deleted lines": 1,
    "Deleted lines": "-      RETURN_IF_ERROR(reader->AddInput(node, 1));",
    "Added lines": "+      RETURN_IF_ERROR(reader->AddInput(node, 0));",
    "Label": "clean"
},
{
    "Id": 179,
    "Library": "tensorflow",
    "Date": "2022/11/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f320f5c91e4e70b1ef383aef32b50cc6a268493b",
    "Root Cause": "N.A",
    "Bug report": "Adds a flag to set `xla_gpu_shape_checks` debug option.\n\nCurrently, the debug option is no configurable through flags. Adds a flag to configure it.\n\nPiperOrigin-RevId: 485714990",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include \"tensorflow/compiler/xla/xla.pb.h\"\n+  // Custom \"sub-parser\" lambda for xla_gpu_shape_checks.\n+  auto setter_for_xla_gpu_shape_checks = [](const std::string& value) {\n+    DebugOptions::ShapeChecks shape_checks;\n+    if (!DebugOptions::ShapeChecks_Parse(value, &shape_checks)) {\n+      return false;\n+    }\n+    flag_values->set_xla_gpu_shape_checks(shape_checks);\n+    return true;\n+  };\n+\n+  flag_objects->push_back(tsl::Flag(\n+      \"xla_gpu_shape_checks\", setter_for_xla_gpu_shape_checks,\n+      DebugOptions::ShapeChecks_Name(flag_values->xla_gpu_shape_checks()),\n+      \"When to perform shape checks in XLA:GPU.\"));",
    "Label": "clean"
},
{
    "Id": 180,
    "Library": "tensorflow",
    "Date": "2022/10/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/df411d343d16160332a5d7e2d54d653ce43e51a8",
    "Root Cause": "N.A",
    "Bug report": "[xla:cpu-next] expose SparsificationPasses via --hlo-xla-runtime-sparsification\n\nThis can helps us debug the Sparsification passes.\n\nFor example, for dense inputs we can compare the output from\n  xla-opt --hlo-xla-runtime-sparsification input_file.mlir\nagainst that of\n  xla-opt --one-shot-bufferize input_file.mlir\n\nPiperOrigin-RevId: 484682603",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+static mlir::PassPipelineRegistration<> sparsification_pipeline(\n+    \"hlo-xla-runtime-sparsification\",\n+    \"Sparsification passes from HLO-XLA Runtime pipeline\",\n+    AddSparsificationPasses);\n+",
    "Label": "clean"
},
{
    "Id": 181,
    "Library": "tensorflow",
    "Date": "2022/10/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/76f728c35e5c34133cdd832c4e494de80331667a",
    "Root Cause": "N.A",
    "Bug report": "Updated list of apis in ops_compatibility.md\n\nUpdated list of apis in ops_compatibility.md with new api.\r\nDepreciated api - tf.check_numerics\r\nNew Api- tf.debugging.check_numerics",
    "Number of deleted lines": 1,
    "Deleted lines": "-*   `tf.check_numerics`",
    "Added lines": "+*   `tf.debugging.check_numerics`",
    "Label": "clean"
},
{
    "Id": 182,
    "Library": "tensorflow",
    "Date": "2022/10/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/38af575aa32c5633395a538114fd85e5e7fe049b",
    "Root Cause": "N.A",
    "Bug report": "[gml-st] Trim hlo-to-gpu-pipeline.\n\nThe interleaved `cononicalize` and `cse` passes make the IR easier to read when debugging, but we might accidentally start requiring them for the follow-up passes to work. I recommend adding them just locally before the pass that you are currently debugging.\n\nPiperOrigin-RevId: 483925760",
    "Number of deleted lines": 34,
    "Deleted lines": "-    pm.addPass(createCanonicalizerPass());\n-    pm.addPass(createCSEPass());\n-  }\n-\n-  // TODO(b/244313563): This is a workaround to avoid temporary allocs within\n-  // threads. It works for as long as all of our operations are cwise. Vectorize\n-  // the inner loops instead.\n-  if (!options.experimentalSoftmax) {\n-    // TODO(frgossen): We should not have to skip this pass for softmax.\n-    pm.addNestedPass<FuncOp>(createLinalgElementwiseOpFusionPass());\n-  }\n-  // Softmax-specific tiling.\n-  if (options.experimentalSoftmax) {\n-    pm.addPass(createCanonicalizerPass());\n-    pm.addPass(createCSEPass());\n-    pm.addPass(createCanonicalizerPass());\n-    pm.addPass(createCSEPass());\n-    pm.addPass(createCanonicalizerPass());\n-    pm.addPass(createCSEPass());\n-    pm.addPass(createCanonicalizerPass());\n-    pm.addPass(createCSEPass());\n-    pm.addPass(createCanonicalizerPass());\n-    pm.addPass(createCSEPass());\n-    pm.addPass(createCanonicalizerPass());\n-    pm.addPass(createCSEPass());\n-        /*distribute=*/true, options.blockTileDim));\n-        /*distribute=*/true, options.warpTileDim));\n-        /*distribute=*/true, options.threadTileDim));\n-    pm.addPass(createCanonicalizerPass());\n-    pm.addPass(createCSEPass());\n-  if (options.experimentalSoftmax) {\n-    pm.addNestedPass<FuncOp>(gml_st::createVectorizeGmlStLoopsPass(\n-        /*vectorizeGmlStOps=*/true, /*distributionLabels=*/{\"warp\", \"thread\"}));\n-  }",
    "Added lines": "+  pm.addPass(createCanonicalizerPass());  // Clean up shape.assuming ops.\n+  // Perform tiling either for softmax or for element-wise.\n+\n+    pm.addNestedPass<FuncOp>(gml_st::createVectorizeGmlStLoopsPass(\n+        /*vectorizeGmlStOps=*/true, /*distributionLabels=*/{\"warp\", \"thread\"}));\n+    // TODO(b/244313563): This is a workaround to avoid temporary allocs within\n+    // threads. It works for as long as all of our operations are cwise.\n+    // Vectorize the inner loops instead.\n+    // TODO(frgossen): We should not have to skip this pass for softmax.\n+    pm.addNestedPass<FuncOp>(createLinalgElementwiseOpFusionPass());\n+\n+        /*distribute=*/true, options.blockTileDim, \"block\"));\n+        /*distribute=*/true, options.warpTileDim, \"warp\"));\n+        /*distribute=*/true, options.threadTileDim, \"thread\"));\n+  pm.addPass(createCanonicalizerPass());\n+  pm.addPass(createCSEPass());\n+",
    "Label": "clean"
},
{
    "Id": 183,
    "Library": "tensorflow",
    "Date": "2022/10/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a6d197a36d3f771ce2d4e67706b73969ea6536d6",
    "Root Cause": "N.A",
    "Bug report": "Add debug logs for GPU device propagation.\n\nPiperOrigin-RevId: 483747061",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  VLOG(3) << \"GPU Local Topology:\\n\" << local_topology.DebugString();\n+  VLOG(3) << \"GPU Global Topology:\\n\" << global_topology.DebugString();",
    "Label": "clean"
},
{
    "Id": 184,
    "Library": "tensorflow",
    "Date": "2022/10/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a6d3c055cd6364cfd2a285a7015c6e6f1abff8fb",
    "Root Cause": "N.A",
    "Bug report": "Apply clang-tidy fixes for bugprone-argument-comment in mhlo_canonicalize_scatter.cc (NFC)\n\nPiperOrigin-RevId: 482906281",
    "Number of deleted lines": 4,
    "Deleted lines": "-        /*update_window_dims=*/\n-        /*inserted_window_dims=*/llvm::None,\n-        /*scatter_dims_to_operand_dims=*/\n-        /*index_vector_dim=*/1);",
    "Added lines": "+        /*updateWindowDims=*/\n+        /*insertedWindowDims=*/llvm::None,\n+        /*scatterDimsToOperandDims=*/\n+        /*indexVectorDim=*/1);",
    "Label": "clean"
},
{
    "Id": 185,
    "Library": "tensorflow",
    "Date": "2022/10/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5573a6f4b39e2b553d70696e36b0d4f06fd51db1",
    "Root Cause": "N.A",
    "Bug report": "Apply clang-tidy fixes for bugprone-argument-comment in mhlo_canonicalize_gather.cc (NFC)\n\nPiperOrigin-RevId: 482905002",
    "Number of deleted lines": 1,
    "Deleted lines": "-        /*indexVectoDim=*/1);",
    "Added lines": "+        /*indexVectorDim=*/1);",
    "Label": "clean"
},
{
    "Id": 186,
    "Library": "tensorflow",
    "Date": "2022/10/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4623c32b57ed95c5b2f47cb5691603e46e92eb03",
    "Root Cause": "N.A",
    "Bug report": "Add back the input shardings calculations for instructions that have user sharding, and add input_sharding when constructing ShardingStrategy in TrimOrGenerateStrategiesBasedOnExistingSharding() to resolve the bug.\n\nPiperOrigin-RevId: 481967670",
    "Number of deleted lines": 11,
    "Deleted lines": "-    StableHashMap<int64_t, std::vector<ShardingStrategy>>&\n-        trimmed_strategy_map) {\n-          cluster_env, trimmed_strategy_map);\n-        strategies->leaf_vector.push_back(ShardingStrategy({name,\n-                                                            existing_sharding,\n-                                                            0,\n-                                                            0,\n-                                                            memory_cost,\n-                                                            resharding_costs,\n-                                                            {}}));\n-          ins->sharding(), cluster_env, trimmed_strategy_map);",
    "Added lines": "+    StableHashMap<int64_t, std::vector<ShardingStrategy>>& trimmed_strategy_map,\n+    const CallGraph& call_graph) {\n+          cluster_env, trimmed_strategy_map, call_graph);\n+        std::vector<HloSharding> input_shardings;\n+            HloInstruction* ins = instructions.at(strategies->instruction_id);\n+            std::optional<HloSharding> input_sharding_or =\n+                ShardingPropagation::GetShardingFromUser(*operand, *ins, 10,\n+                                                         true, call_graph);\n+            if (input_sharding_or.has_value()) {\n+              input_shardings.push_back(input_sharding_or.value());\n+            }\n+        strategies->leaf_vector.push_back(\n+            ShardingStrategy({name, existing_sharding, 0, 0, memory_cost,\n+                              resharding_costs, input_shardings}));\n+          ins->sharding(), cluster_env, trimmed_strategy_map, call_graph);\n+        CHECK(stra.input_shardings.size() == 2)\n+            << \"Dot op requires both operands to have input shardings, \"\n+               \"but get instruction: \"\n+            << inst->ToString() << \", strategy : \" << stra.ToString();",
    "Label": "clean"
},
{
    "Id": 187,
    "Library": "tensorflow",
    "Date": "2022/10/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0de42bcd5a44ff9b484bd3a3d34f88e464a8182e",
    "Root Cause": "N.A",
    "Bug report": "Fixing a bug in primitive cache key",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        key_creator.AddAsKey(post_op_param.alg);",
    "Label": "clean"
},
{
    "Id": 188,
    "Library": "tensorflow",
    "Date": "2022/09/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/aa6d0555a2f99762809f50e3cb339795b02d653e",
    "Root Cause": "N.A",
    "Bug report": "Deprecate ImplicitBatchModeCompatible dynamic shape strategy\n\nThis strategy is intended for ease of use for people familiar with\nthe implicit batch mode profile, in that it does not require the user\nto specifically call build() before trying to run an inference with\nTFTRT converted graph. Since this is actually a dynamic shape mode,\nand input shapes are required in dynamic shape mode for TensorRT\nprofile generation; this mode makes some educated guesses for\nminimum and maximum shapes for inputs the TensorRT engine.\n\nThis has proven to be buggy for models that include transpose and\nreshape operations, among others.\n\nDue to the above, and since dynamic shape mode requires users to\ncall build() with the correct input shapes to generate TensorRT\nprofiles correctly, this mode is being deprecated.\n\nSigned-off-by: Meenakshi Venkataraman <meenakshiv@nvidia.com>",
    "Number of deleted lines": 2,
    "Deleted lines": "-     * `ImplicitBatchModeCompatible`: create the profiles that will produce the\n-       same GPU engines as the implicit_batch_mode would produce.",
    "Added lines": "+    if strategy == \"ImplicitBatchModeCompatible\":\n+      logging.warn(\"ImplicitBatchModeCompatible strategy is deprecated, and\"\n+          \" using it may result in errors during engine building. Please\"\n+          \" consider using a different profile strategy.\")",
    "Label": "clean"
},
{
    "Id": 189,
    "Library": "tensorflow",
    "Date": "2022/09/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/50e1ba1ec4952af896a68ff4844f7046f7d28f03",
    "Root Cause": "N.A",
    "Bug report": "Add more info of input tensor for debugging purpose\n\nPiperOrigin-RevId: 477522405",
    "Number of deleted lines": 1,
    "Deleted lines": "-                 \"Trying to copy a non-replicated DTensor is not supported.\");",
    "Added lines": "+                 absl::StrCat(\"Trying to copy a non-replicated DTensor is not \"\n+                              \"supported. Input tensor is: \",\n+                              typed_input->DebugString())\n+                     .c_str());\n+",
    "Label": "clean"
},
{
    "Id": 190,
    "Library": "tensorflow",
    "Date": "2022/09/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/afcd6eb47b9e5b806a7885e9579f566f9eb17d0b",
    "Root Cause": "N.A",
    "Bug report": "[TFTRT]: Deprecate ImplicitBatchModeCompatible dynamic shape strategy\n\nThis strategy is intended for ease of use for people familiar with\nthe implicit batch mode profile, in that it does not require the user\nto specifically call build() before trying to run an inference with\nTFTRT converted graph. Since this is actually a dynamic shape mode,\nand input shapes are required in dynamic shape mode for TensorRT\nprofile generation; this mode makes some educated guesses for\nminimum and maximum shapes for inputs the TensorRT engine.\n\nThis has proven to be buggy for models that include transpose and\nreshape operations, among others.\n\nDue to the above, and since dynamic shape mode requires users to\ncall build() with the correct input shapes to generate TensorRT\nprofiles correctly, this mode is being deprecated.\n\nSigned-off-by: Meenakshi Venkataraman <meenakshiv@nvidia.com>",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if strategy is \"ImplicitBatchModeCompatible\":\n+      logging.warn(\"ImplicitBatchModeCompatible strategy is deprecated, and\"\n+          \" using it may result in errors during engine building. Please\"\n+          \" consider using a different profile strategy.\")",
    "Label": "clean"
},
{
    "Id": 191,
    "Library": "tensorflow",
    "Date": "2022/09/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5b6f7d228f4bb57ec1282eb7bc0c8f738233432d",
    "Root Cause": "N.A",
    "Bug report": "[xla:runtime] NFC: Add explicit std::move to work around compiler bugs\n\nPiperOrigin-RevId: 476468768",
    "Number of deleted lines": 3,
    "Deleted lines": "-#include \"tensorflow/compiler/xla/mlir/ir/runtime/rt_ops.h\"\n-  if (auto converted = ConvertCanonicalType(type, *this)) return converted;\n-    if (auto converted = conversion(type)) return converted;",
    "Added lines": "+#include \"tensorflow/compiler/xla/mlir/ir/runtime/rt_dialect.h\"\n+  if (std::unique_ptr<Type> converted = ConvertCanonicalType(type, *this))\n+    return std::move(converted);\n+    if (std::unique_ptr<Type> converted = conversion(type))\n+      return std::move(converted);",
    "Label": "clean"
},
{
    "Id": 192,
    "Library": "tensorflow",
    "Date": "2022/09/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/345d064279aeaba924d961322a129555d850bb8b",
    "Root Cause": "N.A",
    "Bug report": "Enable debug info in pretty form for LMHLO so that it is easier to see the corresponding HLO node for each instruction.\n\nPiperOrigin-RevId: 476144750",
    "Number of deleted lines": 1,
    "Deleted lines": "-  op->print(outputFile->os(), mlir::OpPrintingFlags().useLocalScope());",
    "Added lines": "+#include \"mlir/IR/OperationSupport.h\"  // from @llvm-project\n+  mlir::OpPrintingFlags print_flags = mlir::OpPrintingFlags().useLocalScope();\n+  // Enable debug info so that it is easier to see the corresponding HLO node.\n+  if (file_prefix == \"lmhlo\") {\n+    print_flags.enableDebugInfo(/*prettyForm=*/true);\n+  }\n+  op->print(outputFile->os(), print_flags);",
    "Label": "clean"
},
{
    "Id": 193,
    "Library": "tensorflow",
    "Date": "2022/09/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1d8a0787f55b305620e56f857182ab9f742d3e27",
    "Root Cause": "N.A",
    "Bug report": "Remove ineffective workaround for libxsmm bug.\nUp to v.17, libxsmm can ignore up to the first four values in a smarse matrix product, in every 32nd row.\nThis does not only affect small matrices.\n\nPiperOrigin-RevId: 476134734",
    "Number of deleted lines": 9,
    "Deleted lines": "-#if 0  // this issue seems to be resolved\n-  if (left_dim0 < 32 || left_dim1 < 32 || right_dim1 < 32) {\n-    // Causes problems in libxsmm\n-    SparseMatMul<TL, TR>::Compute(\n-        nullptr /* Assumes no cached data for fallback */, left, right,\n-        transpose_left, thread_pool, transpose_output, output);\n-    return;\n-  }\n-#endif",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 194,
    "Library": "tensorflow",
    "Date": "2022/09/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d73d564010f8b3634984c63c7edaa947298dcb03",
    "Root Cause": "N.A",
    "Bug report": "Don't rely on DebugString stability in python_op_gen. No behavior change.\n\nPiperOrigin-RevId: 475958826",
    "Number of deleted lines": 4,
    "Deleted lines": "-  return strings::StrCat(\"\\\"\\\"\\\"\", pb.ShortDebugString(), \"\\\"\\\"\\\"\");\n-    strings::StrAppend(&result_, \"_dispatcher_for_\", function_name_,\n-                       \" = \", function_name_,\n-                       \"._tf_type_based_dispatcher.Dispatch\\n\");",
    "Added lines": "+#include <string>\n+  // Explicitly not using ShortDebugString, because ShortDebugString should\n+  // not be used as a format for transporting information (it's e.g. subject\n+  // to redaction of sensitive information). There is a PrintShortTextProto\n+  // helper, but it's not feasible to depend on that library).\n+\n+  std::string message_short_text;\n+\n+  ::tensorflow::protobuf::TextFormat::Printer printer;\n+  printer.SetSingleLineMode(true);\n+  printer.SetExpandAny(true);\n+\n+  printer.PrintToString(pb, &message_short_text);\n+\n+  return strings::StrCat(\"\\\"\\\"\\\"\", message_short_text, \"\\\"\\\"\\\"\");\n+    strings::StrAppend(&result_, \"_dispatcher_for_\", function_name_, \" = \",\n+                       function_name_, \"._tf_type_based_dispatcher.Dispatch\\n\");",
    "Label": "clean"
},
{
    "Id": 195,
    "Library": "tensorflow",
    "Date": "2022/09/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/62202efa4236f3fd6b8551796b4aece13844e6be",
    "Root Cause": "N.A",
    "Bug report": "Apply clang-tidy fixes for bugprone-argument-comment in generic_host_to_llvm.cc (NFC)\n\nPiperOrigin-RevId: 474714606",
    "Number of deleted lines": 1,
    "Deleted lines": "-                                         /*logp1Benefit=*/{2});",
    "Added lines": "+                                         /*log1pBenefit=*/{2});",
    "Label": "clean"
},
{
    "Id": 196,
    "Library": "tensorflow",
    "Date": "2022/09/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/96119f22d7daff06e4bc7c02c8b3d2b48d294e00",
    "Root Cause": "N.A",
    "Bug report": "Apply clang-tidy fixes for bugprone-argument-comment in legalize_to_linalg.cc (NFC)\n\nPiperOrigin-RevId: 474712770",
    "Number of deleted lines": 1,
    "Deleted lines": "-          /*origigInputNo=*/idx + numOperands,",
    "Added lines": "+          /*origInputNo=*/idx + numOperands,",
    "Label": "clean"
},
{
    "Id": 197,
    "Library": "tensorflow",
    "Date": "2022/09/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/471101427983a1e591e2661a873eb20790b93199",
    "Root Cause": "N.A",
    "Bug report": "Apply clang-tidy fixes for bugprone-argument-comment in hlo_ops.cc (NFC)\n\nPiperOrigin-RevId: 474710269",
    "Number of deleted lines": 4,
    "Deleted lines": "-  if (failed(mlir::hlo::verifyReplicaGroups(*this, /*is_uniform_sized=*/true)))\n-      /*operand_types=*/{operand().getType()},\n-      /*result_types=*/{getType()},\n-      /*scatter_dimension=*/scatter_dimension());",
    "Added lines": "+  if (failed(mlir::hlo::verifyReplicaGroups(*this, /*isUniformSized=*/true)))\n+      /*operandTypes=*/{operand().getType()},\n+      /*resultTypes=*/{getType()},\n+      /*scatterDimension=*/scatter_dimension());",
    "Label": "clean"
},
{
    "Id": 198,
    "Library": "tensorflow",
    "Date": "2022/09/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1c1b85815a411023dff3167f276c4fb373ecd311",
    "Root Cause": "N.A",
    "Bug report": "[xla:DebugOptions] drop xla_gpu_jitrt_executable flag\n\nIt has been superseded by xla_gpu_enable_xla_runtime_executable.\n\nPiperOrigin-RevId: 474665679",
    "Number of deleted lines": 7,
    "Deleted lines": "-  // Deprecated flag.\n-  // TODO(ecg): remove once TAP presubmits don't set it.\n-  flag_objects->push_back(tensorflow::Flag(\n-      \"xla_gpu_jitrt_executable\",\n-      bool_setter_for(&DebugOptions::set_xla_gpu_enable_xla_runtime_executable),\n-      flag_values->xla_gpu_enable_xla_runtime_executable(),\n-      \"Whether to enable XLIR to compile gpu programs to XLA Runtime.\"));",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 199,
    "Library": "tensorflow",
    "Date": "2022/09/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fe33f1a740eb95ed78741d6e74ba74031735c000",
    "Root Cause": "N.A",
    "Bug report": "Skip ops whose results can not be refined during TF shape inference\n\nCurrently, it skips shape inference only in cases of TF dialect ops and this change generalizes that to cover all ops.\n\nThis was discovered while debugging an issue with mhlo.dot whose output type was changed incorrectly after TF shape inference pass using InferTypeOpInterface. Shape inference doesn't handle cases of mixed precision that has a different output type than the operands. For example, i8 accumulating to i32.\n\nPiperOrigin-RevId: 473197385",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (none_of(op->getResultTypes(), CanBeRefined)) {\n+    LLVM_DEBUG(llvm::dbgs() << \"Skipping inference for statically shaped op '\"\n+                            << op->getName() << \"'.\\n\");\n+    return false;\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 200,
    "Library": "tensorflow",
    "Date": "2022/08/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3472c8b8e16f5bad52b00896c4e55b53ac99d7ce",
    "Root Cause": "N.A",
    "Bug report": "Fixing bug of outfeed operand sharding inconsistency.\n\nPiperOrigin-RevId: 470831361",
    "Number of deleted lines": 1,
    "Deleted lines": "-  xla::XlaOp operand = Tuple(ctx.builder, operands);",
    "Added lines": "+  const auto sharding = ctx.builder->sharding();\n+  xla::XlaOp operand;\n+  if (sharding.has_value() &&\n+      sharding->tuple_shardings_size() != operands.size()) {\n+    xla::XlaScopedShardingAssignment scoped_sharding(ctx.builder,\n+                                                     xla::OpSharding());\n+    operand = Tuple(ctx.builder, operands);\n+  } else {\n+    operand = Tuple(ctx.builder, operands);\n+  }",
    "Label": "clean"
},
{
    "Id": 201,
    "Library": "tensorflow",
    "Date": "2022/08/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bff190beb474873ed46ef2b0e210f663b1d74fad",
    "Root Cause": "N.A",
    "Bug report": "Remove debug logging messages.\n\nPiperOrigin-RevId: 470795832",
    "Number of deleted lines": 21,
    "Deleted lines": "-from tensorflow.core.framework import types_pb2\n-def _log_tensor_details(tensor_info):\n-  \"\"\"Log tensor details: name, shape, and type.\"\"\"\n-  for key in tensor_info:\n-    val = tensor_info[key]\n-    dtype = types_pb2.DataType.Name(val.dtype)\n-    if val.tensor_shape.unknown_rank:\n-      shape = \"unknown_rank\"\n-    else:\n-      dims = [str(dim.size) for dim in val.tensor_shape.dim]\n-      shape = \"({})\".format(\", \".join(dims))\n-\n-    logging.info(\"Tensor's key in saved_model's tensor_map: %s\", key)\n-    logging.info(\" tensor name: %s, shape: %s, type: %s\", val.name, shape,\n-                 dtype)\n-\n-\n-  logging.info(\"input tensors info: \")\n-  _log_tensor_details(inputs_tensor_info)\n-  logging.info(\"output tensors info: \")\n-  _log_tensor_details(outputs_tensor_info)",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 202,
    "Library": "tensorflow",
    "Date": "2022/08/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d87676a10811808a17a6adfd8972fcd1f1be9622",
    "Root Cause": "N.A",
    "Bug report": "[lite] Fix shared library build on Windows\n\nBuilding a shared TensorFlow Lite library with CMake on Windows\ncurrently fails with\nsimple_memory_arena_debug_dump.obj : error LNK2005: \"void __cdecl tflite::DumpArenaInfo(...) already defined in simple_memory_arena.obj\n\nThis is because MSVC doesn't have the notion of weak linking required\nto solve the conflict between the two implementations of\ntflite::DumpArenaInfo(). Resolve the conflict by removing the\nexcluding the conflicting implementation in CMakeLists.txt for now.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+if(CMAKE_SYSTEM_NAME MATCHES \"Windows\" AND BUILD_SHARED_LIBS)\n+  list(FILTER TFLITE_SRCS EXCLUDE REGEX \".*simple_memory_arena_debug_dump\\\\.cc$\")\n+endif()",
    "Label": "clean"
},
{
    "Id": 203,
    "Library": "tensorflow",
    "Date": "2022/08/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9df90de03556a778a959ba286c644041480f03ae",
    "Root Cause": "N.A",
    "Bug report": "Fix naming bug for reference type\n\nPiperOrigin-RevId: 468742163",
    "Number of deleted lines": 4,
    "Deleted lines": "-                          local_id: Hashable) -> trace.TraceType:\n-    if local_id not in self._global_to_local_id:\n-      self._global_to_local_id[local_id] = len(self._global_to_local_id)\n-                                   self._global_to_local_id[local_id])",
    "Added lines": "+                          global_id: Hashable) -> trace.TraceType:\n+    if global_id not in self._global_to_local_id:\n+      self._global_to_local_id[global_id] = len(self._global_to_local_id)\n+                                   self._global_to_local_id[global_id])",
    "Label": "clean"
},
{
    "Id": 204,
    "Library": "tensorflow",
    "Date": "2022/08/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/72d1ca0c3cb67eab29e0983db5fae23ca556f6cf",
    "Root Cause": "N.A",
    "Bug report": "Replace `tf_copts` in `debug_options_flags` with only necessary flags\nfor ACL.\n\nCredit: @agramesh1",
    "Number of deleted lines": 1,
    "Deleted lines": "-    copts = tf_copts(),",
    "Added lines": "+load(\"//third_party/compute_library:build_defs.bzl\", \"if_enable_acl\")\n+    copts = if_enable_acl([\"-DXLA_CPU_USE_ACL=1\", \"-fexceptions\"]),",
    "Label": "clean"
},
{
    "Id": 205,
    "Library": "tensorflow",
    "Date": "2022/08/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/68b45d1aa988faf80d2efe870d3a715e54cd66cc",
    "Root Cause": "N.A",
    "Bug report": "Replace TODO to reference a bug.\n\nPiperOrigin-RevId: 467945580",
    "Number of deleted lines": 2,
    "Deleted lines": "-// TODO(hinsu): Support operands with complex element types separately using\n-// the following formula.",
    "Added lines": "+// TODO(b/237376133): Support operands with complex element types separately\n+// using the following formula.",
    "Label": "clean"
},
{
    "Id": 206,
    "Library": "tensorflow",
    "Date": "2022/08/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4d448426f5fbb0107b1c1904ded357242855d55c",
    "Root Cause": "N.A",
    "Bug report": "Fix a typo and add debug helpers\n\nI mistyped 2.11-python3.10, and found that the error was not obvious. This addition of debug lines should help that.\n\nPiperOrigin-RevId: 466768581",
    "Number of deleted lines": 1,
    "Deleted lines": "-        map sigbuild-r2.11-python3.11 2.11-python3.11",
    "Added lines": "+          echo -n \"Trying to map name $1 to tag $2... \"\n+          echo \"success.\"\n+        map sigbuild-r2.11-python3.11 2.11-python3.10",
    "Label": "clean"
},
{
    "Id": 207,
    "Library": "tensorflow",
    "Date": "2022/08/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2768a9e9073ac46f2be1779dfeaa635ef5bbe2d3",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Revert work-around for temporary MLIR bug.\n\nThe [fix](https://github.com/llvm/llvm-project/commit/00a52c75655bb352f875729a93c3f2ae990e5b78) has landed and the work-around is no longer needed.\n\nPiperOrigin-RevId: 466018381",
    "Number of deleted lines": 9,
    "Deleted lines": "-#include \"mlir/IR/BuiltinAttributes.h\"\n-    if (symbol == gpuModuleOp.getNameAttr()) {\n-      continue;\n-    }\n-    // gpu.module name changed, update symbol uses in gpu.launch_func.\n-    funcOp->walk([&](gpu::LaunchFuncOp launch) {\n-      launch.kernelAttr(\n-          SymbolRefAttr::get(symbol, launch.kernel().getNestedReferences()));\n-    });",
    "Added lines": "+    if (failed(symbolTable.replaceAllSymbolUses(gpuModuleOp, symbol, funcOp)))\n+      return rewriter.notifyMatchFailure(fusionOp, \"failed to replace symbol\");",
    "Label": "clean"
},
{
    "Id": 208,
    "Library": "tensorflow",
    "Date": "2022/08/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/33f81cae638156e9268d8b12537ebd6ad5d4e77c",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Work around temporary MLIR bug.\n\nSee https://github.com/llvm/llvm-project/commit/ea460b7ddb8adc9c9c48a968f1155d8757849d76\n\nThe `replaceAllSymbolUses(Operation* op, ...)` now replaces all uses of op's symbol name, which includes nested names. This is a bug which is fixed in a later revision. Work around it for now.\n\nPiperOrigin-RevId: 464984719",
    "Number of deleted lines": 2,
    "Deleted lines": "-    if (failed(symbolTable.replaceAllSymbolUses(gpuModuleOp, symbol, funcOp)))\n-      return rewriter.notifyMatchFailure(fusionOp, \"failed to replace symbol\");",
    "Added lines": "+#include \"mlir/IR/BuiltinAttributes.h\"\n+    if (symbol == gpuModuleOp.getNameAttr()) {\n+      continue;\n+    }\n+    // gpu.module name changed, update symbol uses in gpu.launch_func.\n+    funcOp->walk([&](gpu::LaunchFuncOp launch) {\n+      launch.kernelAttr(\n+          SymbolRefAttr::get(symbol, launch.kernel().getNestedReferences()));\n+    });",
    "Label": "clean"
},
{
    "Id": 209,
    "Library": "tensorflow",
    "Date": "2022/07/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5be21c86bd961022fb62bf915391b8544cfd94c8",
    "Root Cause": "N.A",
    "Bug report": "Add arguments in prof tracing for collectives for more debug info\n\nPiperOrigin-RevId: 463408939",
    "Number of deleted lines": 3,
    "Deleted lines": "-        [ctx] {\n-          return profiler::TraceMeEncode(std::move(op),\n-                                         {{\"id\", ctx->step_id()}});",
    "Added lines": "+        [ctx, col_ctx] {\n+          return profiler::TraceMeEncode(\n+              std::move(op),\n+              {{\"id\", ctx->step_id()},\n+               {\"instance_key\", col_ctx->col_params->instance.instance_key},\n+               {\"collective\", col_ctx->col_params->instance.type}});\n+  profiler::TraceMe trace_me([cp]() {\n+    return profiler::TraceMeEncode(\"CollectiveExecutor::CompleteParams\",\n+                                   {{\"group_key\", cp->group.group_key},\n+                                    {\"group_size\", cp->group.group_size}});\n+  });",
    "Label": "clean"
},
{
    "Id": 210,
    "Library": "tensorflow",
    "Date": "2022/07/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/35833acda373cf40d957639c1ba3978b809d3c6a",
    "Root Cause": "N.A",
    "Bug report": "Add arguments in prof tracing for collectives for more debug info\n\nPiperOrigin-RevId: 463272460",
    "Number of deleted lines": 12,
    "Deleted lines": "-              context_id = producer.GetContextId(), &col_params]() {\n-        [ctx, &col_params] {\n-          return profiler::TraceMeEncode(\n-              std::move(op),\n-              {{\"id\", ctx->step_id()},\n-               {\"instance_key\", col_params->instance.instance_key},\n-               {\"collective\", col_params->instance.type}});\n-  profiler::TraceMe trace_me([cp]() {\n-    return profiler::TraceMeEncode(\"CollectiveExecutor::CompleteParams\",\n-                                   {{\"group_key\", cp->group.group_key},\n-                                    {\"group_size\", cp->group.group_size}});\n-  });",
    "Added lines": "+              context_id = producer.GetContextId()]() {\n+        [ctx] {\n+          return profiler::TraceMeEncode(std::move(op),\n+                                         {{\"id\", ctx->step_id()}});",
    "Label": "clean"
},
{
    "Id": 211,
    "Library": "tensorflow",
    "Date": "2022/07/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0c5b7d51a400ee4f180cc6096db18ef96f41625e",
    "Root Cause": "N.A",
    "Bug report": "Add arguments in prof tracing for collectives for more debug info\n\nPiperOrigin-RevId: 463261705",
    "Number of deleted lines": 4,
    "Deleted lines": "-              context_id = producer.GetContextId()]() {\n-        [ctx] {\n-          return profiler::TraceMeEncode(std::move(op),\n-                                         {{\"id\", ctx->step_id()}});",
    "Added lines": "+              context_id = producer.GetContextId(), &col_params]() {\n+        [ctx, &col_params] {\n+          return profiler::TraceMeEncode(\n+              std::move(op),\n+              {{\"id\", ctx->step_id()},\n+               {\"instance_key\", col_params->instance.instance_key},\n+               {\"collective\", col_params->instance.type}});\n+  profiler::TraceMe trace_me([cp]() {\n+    return profiler::TraceMeEncode(\"CollectiveExecutor::CompleteParams\",\n+                                   {{\"group_key\", cp->group.group_key},\n+                                    {\"group_size\", cp->group.group_size}});\n+  });",
    "Label": "clean"
},
{
    "Id": 212,
    "Library": "tensorflow",
    "Date": "2022/07/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/67352edf6c6a99c6e3d16bd10a55828428ad8f04",
    "Root Cause": "N.A",
    "Bug report": "Add logging when keeping data on host in SameWorkerRecvDone\n\nSome data is always kept on host (CPU) by SameWorkerRecvDone in core/common_runtime/rendezvous_mgr.cc even if there is \"fake\" placement information due to the src or dst op for the data being on device (GPU, etc.). Add debug logging when this case occurs.\n\nPiperOrigin-RevId: 462469508",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (VLOG_IS_ON(3)) {\n+      bool src_override =\n+          send_args.alloc_attrs.on_host() && !(parsed.src.type == \"CPU\");\n+      bool dst_override =\n+          recv_args.alloc_attrs.on_host() && !(parsed.dst.type == \"CPU\");\n+      if (src_override || dst_override) {\n+        VLOG(3) << \"Shortcut to keep tensor on host (src_override \"\n+                << src_override << \" and dst_override \" << dst_override\n+                << \") tensor dtype:\" << DataTypeString(in.dtype()) << \" \"\n+                << parsed.FullKey();\n+      }\n+    }",
    "Label": "clean"
},
{
    "Id": 213,
    "Library": "tensorflow",
    "Date": "2022/07/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/32c4149b329110c454068bf33dfb0d75ca0b8789",
    "Root Cause": "N.A",
    "Bug report": "Apply clang-tidy fixes for bugprone-argument-comment in hlo_ops.cc (NFC)\n\nPiperOrigin-RevId: 462266395",
    "Number of deleted lines": 4,
    "Deleted lines": "-  if (failed(mlir::hlo::verifyReplicaGroups(*this, /*isUniformSized=*/true)))\n-      /*operandTypes=*/{operand().getType()},\n-      /*resultTypes=*/{getType()},\n-      /*scatterDimension=*/scatter_dimension());",
    "Added lines": "+  if (failed(mlir::hlo::verifyReplicaGroups(*this, /*is_uniform_sized=*/true)))\n+      /*operand_types=*/{operand().getType()},\n+      /*result_types=*/{getType()},\n+      /*scatter_dimension=*/scatter_dimension());",
    "Label": "clean"
},
{
    "Id": 214,
    "Library": "tensorflow",
    "Date": "2022/07/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f410da3371cfde957411555ac295c05e08350cf6",
    "Root Cause": "N.A",
    "Bug report": "Apply clang-tidy fixes for bugprone-argument-comment in chlo_legalize_to_hlo.cc (NFC)\n\nPiperOrigin-RevId: 462227568",
    "Number of deleted lines": 1,
    "Deleted lines": "-                               /*is_stable=*/true,",
    "Added lines": "+                               /*isStable=*/true,",
    "Label": "clean"
},
{
    "Id": 215,
    "Library": "tensorflow",
    "Date": "2022/07/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0f2ca64a4fa6bc78d0819dc7ea3272882024d0e0",
    "Root Cause": "N.A",
    "Bug report": "Apply clang-tidy fixes for bugprone-argument-comment in hlo_ops.cc (NFC)\n\nPiperOrigin-RevId: 462226328",
    "Number of deleted lines": 4,
    "Deleted lines": "-  if (failed(mlir::hlo::verifyReplicaGroups(*this, /*is_uniform_sized=*/true)))\n-      /*operand_types=*/{operand().getType()},\n-      /*result_types=*/{getType()},\n-      /*scatter_dimension=*/scatter_dimension());",
    "Added lines": "+  if (failed(mlir::hlo::verifyReplicaGroups(*this, /*isUniformSized=*/true)))\n+      /*operandTypes=*/{operand().getType()},\n+      /*resultTypes=*/{getType()},\n+      /*scatterDimension=*/scatter_dimension());",
    "Label": "clean"
},
{
    "Id": 216,
    "Library": "tensorflow",
    "Date": "2022/07/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6ef6485babe8bca7cb9e524fd6caa879549740d9",
    "Root Cause": "N.A",
    "Bug report": "Apply clang-tidy fixes for bugprone-argument-comment in lhlo_ops.cc (NFC)\n\nPiperOrigin-RevId: 462224710",
    "Number of deleted lines": 6,
    "Deleted lines": "-  return mlir::hlo::verifyReplicaGroups(op, /*is_uniform_sized=*/true);\n-  return mlir::hlo::verifyReplicaGroups(op, /*is_uniform_sized=*/true);\n-  if (failed(mlir::hlo::verifyReplicaGroups(op, /*is_uniform_sized=*/true)))\n-          op, /*operand_types=*/op.getInputs().getTypes(),\n-          /*result_types=*/op.getOutputs().getTypes(),\n-          /*scatter_dimension=*/op.getScatterDimension())))",
    "Added lines": "+  return mlir::hlo::verifyReplicaGroups(op, /*isUniformSized=*/true);\n+  return mlir::hlo::verifyReplicaGroups(op, /*isUniformSized=*/true);\n+  if (failed(mlir::hlo::verifyReplicaGroups(op, /*isUniformSized=*/true)))\n+          op, /*operandTypes=*/op.getInputs().getTypes(),\n+          /*resultTypes=*/op.getOutputs().getTypes(),\n+          /*scatterDimension=*/op.getScatterDimension())))",
    "Label": "clean"
},
{
    "Id": 217,
    "Library": "tensorflow",
    "Date": "2022/07/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/20f316cacc40320f638b10adc00cb475f8ac83a5",
    "Root Cause": "N.A",
    "Bug report": "Remove debug comment\n\nPiperOrigin-RevId: 462008879",
    "Number of deleted lines": 3,
    "Deleted lines": "-  VLOG(0) << \"Inferring shape for ReduceDataset with #states = \" << num_states\n-          << \" , #input_elements = \" << num_input_elements\n-          << \" , and #captured_arguments = \" << num_captured_arguments;",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 218,
    "Library": "tensorflow",
    "Date": "2022/07/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2bd4767084d4621d67a5548a46437e8103533184",
    "Root Cause": "N.A",
    "Bug report": "[Bug fix] Fix a bug where \"filename\" field is assigned model flatbuffer when `create_html` is called with a model object instead of file.\n\nPiperOrigin-RevId: 461264043",
    "Number of deleted lines": 1,
    "Deleted lines": "-  data[\"filename\"] = tflite_input  # Avoid special case",
    "Added lines": "+  data[\"filename\"] = tflite_input if input_is_filepath else (\n+      \"Null (used model object)\")  # Avoid special case\n+",
    "Label": "clean"
},
{
    "Id": 219,
    "Library": "tensorflow",
    "Date": "2022/07/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fb6c188d85b3abbe56a7dc5c0867d2f8f9059d59",
    "Root Cause": "N.A",
    "Bug report": "[attempt to] fix the tricky atomic order bug.\n\nPiperOrigin-RevId: 460529494",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include <atomic>\n+\n+    std::atomic_thread_fence(std::memory_order_release);\n+    std::atomic_thread_fence(std::memory_order_release);",
    "Label": "clean"
},
{
    "Id": 220,
    "Library": "tensorflow",
    "Date": "2022/07/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fbff9fd15e4241e7e2beedf9fbaae0c73e0dd66c",
    "Root Cause": "N.A",
    "Bug report": "[NFC] Rename pass manager instance, add comment for debugging pass pipeline.\n\nPiperOrigin-RevId: 458446274",
    "Number of deleted lines": 3,
    "Deleted lines": "-  OpPassManager pm;\n-  createHloToGpuPipeline(pm, /*tileSizes=*/{}, /*unrollFactors=*/{});\n-  pm.getDependentDialects(registry);",
    "Added lines": "+  OpPassManager passManager;\n+  createHloToGpuPipeline(passManager, /*tileSizes=*/{}, /*unrollFactors=*/{});\n+  passManager.getDependentDialects(registry);\n+  // Note: passManager.enableIRPrinting() doesn't do anything on dynamic pass\n+  // pipelines. Printing needs to be enabled on the parent pass manager.",
    "Label": "clean"
},
{
    "Id": 221,
    "Library": "tensorflow",
    "Date": "2022/06/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8800c48b5507bfcb0acda79554ad484f05116c12",
    "Root Cause": "N.A",
    "Bug report": "Apply clang-tidy fixes for bugprone-argument-comment in lower_general_dot.cc (NFC)\n\nPiperOrigin-RevId: 457720613",
    "Number of deleted lines": 2,
    "Deleted lines": "-                        /*outer_dims_first=*/true, rewriter);\n-                        /*outer_dims_first=*/false, rewriter);",
    "Added lines": "+                        /*outerDimsFirst=*/true, rewriter);\n+                        /*outerDimsFirst=*/false, rewriter);",
    "Label": "clean"
},
{
    "Id": 222,
    "Library": "tensorflow",
    "Date": "2022/06/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/094ca7ad309a899fb1ba11b4b4e2a883d53567dc",
    "Root Cause": "N.A",
    "Bug report": "Apply clang-tidy fixes for bugprone-argument-comment in legalize_control_flow.cc (NFC)\n\nPiperOrigin-RevId: 457719598",
    "Number of deleted lines": 1,
    "Deleted lines": "-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 223,
    "Library": "tensorflow",
    "Date": "2022/06/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3e3bcde61541761c75a8ca1aafed447e7676abb5",
    "Root Cause": "N.A",
    "Bug report": "Raise error in MLIR on int32 GPU all reduce.\n\nAt MLIR we have more information about the provenance of the all reduce, which\nmay help users to debug the problem. We run into this when adopt bert keras on GPU.\n\nLonger term we shall find a way to support int32 GPU all reduce.\n\nPiperOrigin-RevId: 456287353",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include \"mlir/Support/DebugStringHelper.h\"  // from @llvm-project\n+  const bool is_gpu = all_reduce.device_type().endswith(\"GPU\");\n+    if (is_gpu) {\n+      const mlir::TensorType input_type =\n+          all_reduce.input().getType().dyn_cast<mlir::TensorType>();\n+      if (input_type && input_type.getElementType().isInteger(32)) {\n+        return mlir::emitError(\n+            loc, \"On GPU, collective reduce of int32 is not supported.\");\n+      }\n+    }",
    "Label": "clean"
},
{
    "Id": 224,
    "Library": "tensorflow",
    "Date": "2022/06/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/712b22feb176c2ca03706d596288e75110ef4156",
    "Root Cause": "N.A",
    "Bug report": "Strip debug info from IR produced by HloToGpuPipeline\n\nPTXAS crashes when compiling some instructions (e.g., llvm.intr.round) when debug info is attached.\n\nPiperOrigin-RevId: 456122671",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // Some instructions crash ptxas down the line if they have debug info\n+  // attached.\n+  pm.addNestedPass<GPUModuleOp>(createStripDebugInfoPass());",
    "Label": "clean"
},
{
    "Id": 225,
    "Library": "tensorflow",
    "Date": "2022/06/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/387c75450ecfc0943938972ed89c02a6251fc1ce",
    "Root Cause": "N.A",
    "Bug report": "Add a function for modifying the dtensor_device singleton.\n\nFor debugging, this API allows using a new DTensor device with async=False globally.\n\nPiperOrigin-RevId: 455416118",
    "Number of deleted lines": 2,
    "Deleted lines": "-def _dtensor_device() -> dtensor_device.DTensorDevice:\n-      _dtensor_singleton = dtensor_device.DTensorDevice(meshes=[])",
    "Added lines": "+def _set_dtensor_device(device: dtensor_device.DTensorDevice) -> None:\n+  _dtensor_singleton = device\n+\n+\n+def _dtensor_device() -> dtensor_device.DTensorDevice:\n+      _set_dtensor_device(dtensor_device.DTensorDevice(meshes=[]))",
    "Label": "clean"
},
{
    "Id": 226,
    "Library": "tensorflow",
    "Date": "2022/06/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/87c892726d1ca89d24d4e384482a1c6bbbb4e1aa",
    "Root Cause": "N.A",
    "Bug report": "Fix a potential wrong operator bug",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if ast_node is self.node_index:",
    "Added lines": "+    if ast_node in self.node_index:",
    "Label": "clean"
},
{
    "Id": 227,
    "Library": "tensorflow",
    "Date": "2022/06/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6d6677a929c6b91c8bfe52297c156dacc136b239",
    "Root Cause": "N.A",
    "Bug report": "Fixed a bug in the xnnpack delegate option for label_image.",
    "Number of deleted lines": 1,
    "Deleted lines": "-        params_.Set<bool>(\"num_threads\", s.number_of_threads);",
    "Added lines": "+        params_.Set<int32_t>(\"num_threads\", s.number_of_threads);",
    "Label": "clean"
},
{
    "Id": 228,
    "Library": "tensorflow",
    "Date": "2022/06/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/66a2d21d097cf32ec87e148ae87b10adf1ab5331",
    "Root Cause": "N.A",
    "Bug report": "Fix selective-build script\n\nMake \"--debug\" option work for easier debugging with Docker container.\n$ tensorflow/lite/tools/build_aar_with_docker.sh \\\n  --input_models=\"${PWD}/tensorflow/lite/testdata/softplus_flex.bin\" \\\n  --target_archs=x86,x86_64,arm64-v8a,armeabi-v7a --checkpoint=master \\\n  --cache_dir=/tmp/bazel_cache \\\n  --debug\n\nPiperOrigin-RevId: 452839024",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    --debug)\n+      DEBUG_MODE=true\n+      shift;;",
    "Label": "clean"
},
{
    "Id": 229,
    "Library": "tensorflow",
    "Date": "2022/05/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4d6e6bafaa0e3e014d40d0ff3a8d305d14698b5e",
    "Root Cause": "N.A",
    "Bug report": "Remove call to HloModule::RemoveUnusedComputations in HloModule::CreateFromProto\n\nThe call was working around a bug that should be fixed by now.\n\nPiperOrigin-RevId: 452217255",
    "Number of deleted lines": 3,
    "Deleted lines": "-  if (proto.has_schedule()) {\n-    TF_RETURN_IF_ERROR(module->RemoveUnusedComputations());\n-  }",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 230,
    "Library": "tensorflow",
    "Date": "2022/05/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a9470a65bd3f70c2aefa0bfe3d6007432b578284",
    "Root Cause": "N.A",
    "Bug report": "Missed one TODO that should have been a bug\n\nPiperOrigin-RevId: 451474743",
    "Number of deleted lines": 1,
    "Deleted lines": "-  // TODO(mihaimaruseac): Handle windows when changing its filesystem",
    "Added lines": "+  // TODO(b/139060984): Handle windows when changing its filesystem",
    "Label": "clean"
},
{
    "Id": 231,
    "Library": "tensorflow",
    "Date": "2022/05/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2f006a44cfa1fd9d14f6b55bf594d2369bffec7e",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Fix a bug where filtered elements in `ParallelFilter` buffers are not recorded in `Model` when popped from the buffers.\n\nPiperOrigin-RevId: 451034896",
    "Number of deleted lines": 3,
    "Deleted lines": "-        while (ShouldWait(&result)) {\n-        RecordBufferDequeue(ctx, *out_tensors);\n-    bool ShouldWait(std::shared_ptr<InvocationResult>* result)",
    "Added lines": "+        while (ShouldWait(ctx, &result)) {\n+    bool ShouldWait(IteratorContext* ctx,\n+                    std::shared_ptr<InvocationResult>* result)\n+        RecordBufferDequeue(ctx, invocation_results_.front()->return_values);\n+          // End of input result is not recorded in the model proto when the\n+          // invocation result was created. It should not be recorded when it is\n+          // popped either.\n+          if (!(*result)->end_of_input) {\n+            RecordBufferDequeue(ctx, (*result)->return_values);\n+          }",
    "Label": "clean"
},
{
    "Id": 232,
    "Library": "tensorflow",
    "Date": "2022/05/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b0137228c3920977ee0fecbaa7a56f8629711f52",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Remove debugging statement accidentally left in.\n\nPiperOrigin-RevId: 450835000",
    "Number of deleted lines": 1,
    "Deleted lines": "-    LOG(ERROR) << \"HAS CONFIG \" << this->name();",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 233,
    "Library": "tensorflow",
    "Date": "2022/05/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/15c26be8f79834ae5a6123939b4885e4d6a112af",
    "Root Cause": "N.A",
    "Bug report": "[xla:jitrt] Work around the canonicalization bug for mhlo copy operation\n\nmhlo.copy operation canonicalizer can break downstream gpu code generation, for now just make sure that the first pass only removes memref.get_global ops\n\nPiperOrigin-RevId: 450593603",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // TODO(ezhulenev): By adding MHLO and LMHLO to a set of legal dialects, we\n+  // suppress any rewrites for these dialects (there are canonicalization\n+  // patterns that interact badly with downstream Gpu binary code generation).\n+  target.addLegalDialect<mhlo::MhloDialect, lmhlo::LmhloDialect>();\n+",
    "Label": "clean"
},
{
    "Id": 234,
    "Library": "tensorflow",
    "Date": "2022/05/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/28d943b7d40a6a59ad0c888c1b3859902aab1688",
    "Root Cause": "N.A",
    "Bug report": "Remove debug logs from saver.py",
    "Number of deleted lines": 2,
    "Deleted lines": "-    logging.info(file)\n-    logging.info(size)",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 235,
    "Library": "tensorflow",
    "Date": "2022/05/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dc00a5331fee9c5f2d7e12bd1ae27b102b788959",
    "Root Cause": "N.A",
    "Bug report": "[xla:jitrt] Fix a bug in GemmBias arguments order\n\nPiperOrigin-RevId: 449671949",
    "Number of deleted lines": 16,
    "Deleted lines": "-    int64_t algorithm, double alpha_imag, double alpha_real,\n-                           int64_t algorithm, double alpha_imag,\n-                           double alpha_real, ArrayRef<int64_t> lhs_batch,\n-    int64_t algorithm, double alpha_imag, double alpha_real,\n-                             alpha_imag, alpha_real, lhs_batch, lhs_contract,\n-          .Attr<double>(\"alpha_imag\")\n-                           jitrt::MemrefView rhs, jitrt::MemrefView out,\n-                           jitrt::MemrefView bias, int64_t algorithm,\n-                           double alpha_imag, double alpha_real, double beta,\n-    jitrt::MemrefView lhs, jitrt::MemrefView rhs, jitrt::MemrefView out,\n-    jitrt::MemrefView bias, int64_t algorithm, double alpha_imag,\n-    double alpha_real, double beta, ArrayRef<int64_t> lhs_batch,\n-  se::DeviceMemoryBase output_data = GetDeviceAddress(out);\n-                             alpha_imag, alpha_real, lhs_batch, lhs_contract,\n-          .Arg<jitrt::MemrefView>()  // out\n-          .Attr<double>(\"alpha_imag\")",
    "Added lines": "+    int64_t algorithm, double alpha_real, double alpha_imag,\n+                           int64_t algorithm, double alpha_real,\n+                           double alpha_imag, ArrayRef<int64_t> lhs_batch,\n+    int64_t algorithm, double alpha_real, double alpha_imag,\n+                             alpha_real, alpha_imag, lhs_batch, lhs_contract,\n+          .Attr<double>(\"alpha_imag\")\n+                           jitrt::MemrefView rhs, jitrt::MemrefView bias,\n+                           jitrt::MemrefView out, int64_t algorithm,\n+                           double alpha_real, double alpha_imag, double beta,\n+    jitrt::MemrefView lhs, jitrt::MemrefView rhs, jitrt::MemrefView bias,\n+    jitrt::MemrefView out, int64_t algorithm, double alpha_real,\n+    double alpha_imag, double beta, ArrayRef<int64_t> lhs_batch,\n+  se::DeviceMemoryBase output_data = GetDeviceAddress(out);\n+                             alpha_real, alpha_imag, lhs_batch, lhs_contract,\n+          .Arg<jitrt::MemrefView>()  // out\n+          .Attr<double>(\"alpha_imag\")",
    "Label": "clean"
},
{
    "Id": 236,
    "Library": "tensorflow",
    "Date": "2022/05/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ce0396ecd0267b5a85623c9354347d9de0464b23",
    "Root Cause": "N.A",
    "Bug report": "Fix error when compiling in debug mode.\n\nA constant was declared but not defined.\n\nPiperOrigin-RevId: 448507865",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+/* static */ constexpr int64_t DerivedXLineBuilder::kInvalidGroupId;\n+",
    "Label": "clean"
},
{
    "Id": 237,
    "Library": "tensorflow",
    "Date": "2022/05/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/34257b0fd57205059cc561f059e26a9e35974bbe",
    "Root Cause": "N.A",
    "Bug report": "Trusted Partner PR Workflow testing.  Remove debug statements.\n\nPiperOrigin-RevId: 446459711",
    "Number of deleted lines": 9,
    "Deleted lines": "-# TODO: Test permissions issue. Remove after test and give targetted permission\n-permissions: write-all\n-\n-      - name: Print Dummy Variables\n-        env:\n-          DUMMY_SECRET: ${{ secrets.DUMMY_SECRET }}\n-        run: |\n-          echo \"Dummy Secret: ${DUMMY_SECRET}\"\n-          echo ${{ secrets.DUMMY_SECRET }}",
    "Added lines": "+    permissions:\n+      # Needed to attach tags into the PR\n+      issues: write\n+      contents: write\n+      pull-requests: write",
    "Label": "clean"
},
{
    "Id": 238,
    "Library": "tensorflow",
    "Date": "2022/04/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b748413a803229ef668bac8061a49d7cde46ed58",
    "Root Cause": "N.A",
    "Bug report": "fix bug",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    compatible_devices = gpu_devices",
    "Label": "clean"
},
{
    "Id": 239,
    "Library": "tensorflow",
    "Date": "2022/04/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ffea1aae91cd7dd471f3c2f61e6e74e8fc4215fc",
    "Root Cause": "N.A",
    "Bug report": "Fix device index unordered bug brought by set",
    "Number of deleted lines": 1,
    "Deleted lines": "-    compatible_devices = list(set(gpu_devices + pluggable_devices))",
    "Added lines": "+    for dev in pluggable_devices:\n+      if dev not in gpu_devices:\n+        compatible_devices.append(dev)",
    "Label": "clean"
},
{
    "Id": 240,
    "Library": "tensorflow",
    "Date": "2022/04/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/10d8770a80a72e53b1eb40d22a24616b40ab2c48",
    "Root Cause": "N.A",
    "Bug report": "Add a bug number to a TODO. NFC\n\nPiperOrigin-RevId: 439793523",
    "Number of deleted lines": 1,
    "Deleted lines": "-// TODO(kramerb): Assert that $x and $y have the same shape.",
    "Added lines": "+// TODO(b/228291745): Assert that $x and $y have the same shape.",
    "Label": "clean"
},
{
    "Id": 241,
    "Library": "tensorflow",
    "Date": "2022/03/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a5e670f37f90753dab9a67507e8752372f2b308c",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in Benchmark tool to release dynamic tensors based on input flags.\n\nPiperOrigin-RevId: 438110859",
    "Number of deleted lines": 1,
    "Deleted lines": "-  options.SetPreserveAllTensors(params_.Get<bool>(\"release_dynamic_tensors\"));",
    "Added lines": "+  options.SetEnsureDynamicTensorsAreReleased(\n+      params_.Get<bool>(\"release_dynamic_tensors\"));",
    "Label": "clean"
},
{
    "Id": 242,
    "Library": "tensorflow",
    "Date": "2022/03/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/01e5a2db0230f003cb069bb584b35669eade4ba7",
    "Root Cause": "N.A",
    "Bug report": "[XLA:GPU] Fix buffer overflow in GpuTransferManager::ReadDynamicShapes().\n\nWe were allocating one buffer of size k bytes and then dividing it up into n\nbuffers of size k.\n\nUnfortunately we don't have tools that could catch this very bad bug.  Even\ncuda_asan won't catch it because the OOB write occurs in the CUDA driver.\n\nPiperOrigin-RevId: 437014284",
    "Number of deleted lines": 4,
    "Deleted lines": "-        (*executor)->HostMemoryAllocate(kPinnedBufferBytes));\n-  // Check out pinned memory for each buffer we want to copy.  If by some\n-  // there aren't enough pinned buffers available, or if one of our buffers is\n-  // so big it doesn't fit, allocate an entry for it in fallback_buffers.",
    "Added lines": "+        (*executor)->HostMemoryAllocate(kPinnedChunkBytes));\n+  // Check out pinned memory for each buffer we want to copy.  If there aren't\n+  // enough pinned buffers available, or if one of our buffers is so big it\n+  // doesn't fit, allocate an entry for it in fallback_buffers.",
    "Label": "clean"
},
{
    "Id": 243,
    "Library": "tensorflow",
    "Date": "2022/03/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f62c8863b3e19b81d2484a95e96ddb854ea66b2f",
    "Root Cause": "N.A",
    "Bug report": "Rollback of previous change:\n\n[PJRT:CPU] Ensure TfrtCpuBuffer is defined and ready before letting caller acquire an ExternalReference for zero-copy raw pointer read.\n\nTFRT CPU backend in theory allows CPU computations to be dispatched asynchronously, yet still allow PJRT callers to acquire an ExternalReference and read the buffer contents in a zero-copy way (see PyBuffer::AsNumpyArray implementation).\n\nThis CL fixes a bug where we did not synchronize and wait for the buffer to be ready before giving PJRT users a ExternalReference and allowing them to read via raw pointer.\n\nThis problem is however masked by a different bug in async dispatch. Due to a bug in retrieving flop count from HloCostAnalysis, all CPU computations are currently dispatched synchronously. This meant that the buffer content is always ready, and no users at HEAD is affected by the lack of synchronization that this CL fixes.\n\nHowever, we do intend to allow async dispatch, in which case, the synchronization before handing out an ExternalReference is necessary.\n\nPiperOrigin-RevId: 436799582",
    "Number of deleted lines": 7,
    "Deleted lines": "-  // Before allow caller to zero-copy read the buffer content via the\n-  // ExternalReference, we must ensure buffer is defined and ready.\n-  TF_RETURN_IF_ERROR(GetReadyFuture().Await());\n-\n-  // Must wait for buffer to be defined and ready to be read.\n-  TF_RETURN_IF_ERROR(GetReadyFuture().Await());\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 244,
    "Library": "tensorflow",
    "Date": "2022/03/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6218689b9ad8686794a8d70e3a0054ca19e5c544",
    "Root Cause": "N.A",
    "Bug report": "[PJRT:CPU] Ensure TfrtCpuBuffer is defined and ready before letting caller acquire an ExternalReference for zero-copy raw pointer read.\n\nTFRT CPU backend in theory allows CPU computations to be dispatched asynchronously, yet still allow PJRT callers to acquire an ExternalReference and read the buffer contents in a zero-copy way (see PyBuffer::AsNumpyArray implementation).\n\nThis CL fixes a bug where we did not synchronize and wait for the buffer to be ready before giving PJRT users a ExternalReference and allowing them to read via raw pointer.\n\nThis problem is however masked by a different bug in async dispatch. Due to a bug in retrieving flop count from HloCostAnalysis, all CPU computations are currently dispatched synchronously. This meant that the buffer content is always ready, and no users at HEAD is affected by the lack of synchronization that this CL fixes.\n\nHowever, we do intend to allow async dispatch, in which case, the synchronization before handing out an ExternalReference is necessary.\n\nPiperOrigin-RevId: 436327791",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // Before allow caller to zero-copy read the buffer content via the\n+  // ExternalReference, we must ensure buffer is defined and ready.\n+  TF_RETURN_IF_ERROR(GetReadyFuture().Await());\n+\n+  // Must wait for buffer to be defined and ready to be read.\n+  TF_RETURN_IF_ERROR(GetReadyFuture().Await());\n+",
    "Label": "clean"
},
{
    "Id": 245,
    "Library": "tensorflow",
    "Date": "2022/03/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2181645d7289d08ab67af114bd234f1eb7621bc6",
    "Root Cause": "N.A",
    "Bug report": "bug fix",
    "Number of deleted lines": 4,
    "Deleted lines": "-    if bias is not None:\n-      # Adjust the mean of the batchnorm based on the add op in-between the conv\n-      # and the batchnorm.\n-      mean_value = mean_value - values_from_const(bias)",
    "Added lines": "+    if bias is not None:\n+      # Adjust the mean of the batchnorm based on the add op in-between the conv\n+      # and the batchnorm.\n+      mean_value = mean_value - values_from_const(bias)",
    "Label": "clean"
},
{
    "Id": 246,
    "Library": "tensorflow",
    "Date": "2022/03/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4915b1eed0c914d5482e1175c88faeda94e057a8",
    "Root Cause": "N.A",
    "Bug report": "Make the sed command more readable and fix ^M bug\n\nSed has the same effect, but should be easier for later devs to read.\nCurl on GitHub Actions seems to be putting extra whitespace at the end of the\nline.\n\nPiperOrigin-RevId: 435716763",
    "Number of deleted lines": 3,
    "Deleted lines": "-          # metadata. gcr.io helpfully includes it in the header of the response.\n-          digest=$(curl -s --head \"https://gcr.io/v2/tensorflow-sigs/build/manifests/$2\" | awk '/docker-content-digest/ {print $2}')\n-          sed -i\"\" \"s/\\(\\\"$1\\\".*build@\\)sha256:.*\\(\\\",\\)/\\1$digest\\2/\" tensorflow/tools/toolchains/remote_config/configs.bzl",
    "Added lines": "+          # metadata. gcr.io helpfully includes it in the header of the response\n+          # as docker-content-digest: sha256:[digest]. Note we use egrep to\n+          # match exactly sha256:<hash> because curl may include a ^M symbol at\n+          # the end of the line.\n+          digest=$(curl -s --head \"https://gcr.io/v2/tensorflow-sigs/build/manifests/$2\" | egrep -o \"sha256:[[:alnum:]]*\")\n+          # Find the line matching the regex \"sigbuild-r2.9\" (with quotes) and\n+          # replace just the digest portion in it\n+          sed -i\"\" \"/\\\"$1\\\"/ s/sha256:[[:alnum:]]*/$digest/g\" tensorflow/tools/toolchains/remote_config/configs.bzl",
    "Label": "clean"
},
{
    "Id": 247,
    "Library": "tensorflow",
    "Date": "2022/03/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8fba6855865268d62111ea59c9e1e69583f93add",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Add `LessThanByKey` utility function.\n\nMakes it possible to avoid the duplication (and silly bugs!) required by C++ comparison functions.\n\nPiperOrigin-RevId: 435315261",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+// Returns a comparison function using the provided key function on each value,\n+// i.e. `key_fn(a) < key_fn(b)`.\n+template <typename KeyFn>\n+auto LessThanByKey(KeyFn&& key_fn) {\n+  return [=](const auto& a, const auto& b) { return key_fn(a) < key_fn(b); };\n+}\n+",
    "Label": "clean"
},
{
    "Id": 248,
    "Library": "tensorflow",
    "Date": "2022/03/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/608afba608e643f848ef5c9bcd9d4338c067b037",
    "Root Cause": "N.A",
    "Bug report": "Einsum debug message moved to VLOG(2)",
    "Number of deleted lines": 2,
    "Deleted lines": "-  LOG(INFO) << \"Condition operand. Need reshape \" << need_reshape\n-            << \" nned transpose \" << need_transpose;",
    "Added lines": "+\n+  VLOG(2) << \"Condition operand. Need reshape: \" << need_reshape\n+          << \". Need transpose: \" << need_transpose;",
    "Label": "clean"
},
{
    "Id": 249,
    "Library": "tensorflow",
    "Date": "2022/03/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c63e869e5eb9f89627551e0360dd48f1df2babe0",
    "Root Cause": "N.A",
    "Bug report": "Removes extra call to native.genrule() because underlying bugs requiring the call are now resolved.  This cleans up the code and reduces Forge impact a tiny bit.\n\nPiperOrigin-RevId: 433021563",
    "Number of deleted lines": 39,
    "Deleted lines": "-            \" \" + flags + \" \" + profiling_flag + \" \" + mlir_flag + \" \" + traceme_flag\n-        ),\n-        tools = [tfcompile_tool],\n-        visibility = visibility,\n-        testonly = testonly,\n-        # Run tfcompile on the build host since it's typically faster on the\n-        # local machine.\n-        #\n-        # Note that setting the local=1 attribute on a *test target* causes the\n-        # test infrastructure to skip that test.  However this is a genrule, not\n-        # a test target, and runs with --strategy=Genrule=forced_forge, meaning\n-        # the local=1 attribute is ignored, and the genrule is still run.\n-        #\n-        # https://www.bazel.io/versions/master/docs/be/general.html#genrule\n-        local = 1,\n-        tags = tags,\n-    )\n-\n-    # Rule that runs tfcompile to produce the SessionModule proto, useful for\n-    # debugging.  TODO(b/64813587): Once the SessionModule proto is\n-    # deterministic, move this into the main rule above.\n-    session_module_pb = name + \"_session_module.pb\"\n-    native.genrule(\n-        name = (name + \"_session_module\"),\n-        srcs = srcs,\n-        outs = [\n-            session_module_pb,\n-        ],\n-        cmd = (\n-            default_fast_math_xla_flags +\n-            \"CUDA_VISIBLE_DEVICES='' \" +\n-            \"$(location \" + tfcompile_tool + \")\" +\n-            \" --graph=$(location \" + tfcompile_graph + \")\" +\n-            debug_info_flag +\n-            \" --config=$(location \" + config + \")\" +\n-            \" --entry_point=\" + ep +\n-            \" --cpp_class=\" + cpp_class +\n-            \" --target_triple=\" + target_llvm_triple() +\n-            \" \" + flags",
    "Added lines": "+    session_module_pb = name + \"_session_module.pb\"\n+            session_module_pb,\n+            \" \" + flags + \" \" + profiling_flag + \" \" + mlir_flag + \" \" + traceme_flag",
    "Label": "clean"
},
{
    "Id": 250,
    "Library": "tensorflow",
    "Date": "2022/03/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a7873fa24f25d8188560a746203382a8f0ce4b80",
    "Root Cause": "N.A",
    "Bug report": "Fix a few minor formatting issues in the new bug template for TF Lite in Play Services.\n\nPiperOrigin-RevId: 432183111",
    "Number of deleted lines": 10,
    "Deleted lines": "-about: Use this template for issues with TensorFlow Lite in Play Services issues\n-- Android Device information (use adb shell getprop ro.build.fingerprint if\n-possible):\n-- TensorFlow Lite in Play Services SDK version (found in build.gradle):\n-- Google Play Services version (Settings > Apps > Google Play Services > App\n-details):\n-\n-the problem. If possible, please share a link to or attach code demonstating the\n-problem.\n-",
    "Added lines": "+about: Use this template for issues with TensorFlow Lite in Google Play Services\n+- Android Device information (use `adb shell getprop ro.build.fingerprint`\n+  if possible):\n+- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):\n+- Google Play Services version\n+  (`Settings` > `Apps` > `Google Play Services` > `App details`):\n+the problem. If possible, please share a link to or attach code demonstrating\n+the problem.",
    "Label": "clean"
},
{
    "Id": 251,
    "Library": "tensorflow",
    "Date": "2022/02/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8d4cb00f0fde4baeb0b11bdef09ef3aa7cb55b93",
    "Root Cause": "N.A",
    "Bug report": "Fix bug with live range end position.\n\nPiperOrigin-RevId: 431443850",
    "Number of deleted lines": 2,
    "Deleted lines": "-      if (instruction_schedule_[position.instruction] >= max_end_time) {\n-        max_end_time = instruction_schedule_[value->instruction()];",
    "Added lines": "+      int64_t position_time = instruction_schedule_[position.instruction];\n+      if (position_time >= max_end_time) {\n+        max_end_time = position_time;\n+",
    "Label": "clean"
},
{
    "Id": 252,
    "Library": "tensorflow",
    "Date": "2022/02/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b5c8968ff145227ccf03061b582b06663fca55f1",
    "Root Cause": "N.A",
    "Bug report": "Append the path when the metadata was not found to the error message for easier debugging.\n\nPiperOrigin-RevId: 430997637",
    "Number of deleted lines": 1,
    "Deleted lines": "-              errors::NotFound(\"Could not find metadata file.\"));",
    "Added lines": "+              errors::NotFound(\"Could not find metadata file [\", path, \"]\"));",
    "Label": "clean"
},
{
    "Id": 253,
    "Library": "tensorflow",
    "Date": "2022/02/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c652bed5aef256df19e44c6a796b9318b5bf9496",
    "Root Cause": "N.A",
    "Bug report": "Assign a bug to a todo.\n\nPiperOrigin-RevId: 430705559\nChange-Id: Ifbf08831679605d6fe7834c86d24d644dad94b90",
    "Number of deleted lines": 1,
    "Deleted lines": "-# TODO(mdan): Get rid of if_static, it's nonstandard.",
    "Added lines": "+# TODO(b/221223841): Get rid of if_static, it's nonstandard.",
    "Label": "clean"
},
{
    "Id": 254,
    "Library": "tensorflow",
    "Date": "2022/01/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/39bd65bf8d4c382c6217c52425ddf0fa83f3b2fa",
    "Root Cause": "N.A",
    "Bug report": "remove debug lines",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+",
    "Label": "clean"
},
{
    "Id": 255,
    "Library": "tensorflow",
    "Date": "2022/02/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/32af2a3229c8f1bdde5103bf783114eba12c1bce",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Compile time opt: don't explore prefetch start times earlier than what we have already discovered.\n\nAlso fix a bug where we weren't clearing pending chunks after allocating scoped\nmemor.\n\nPiperOrigin-RevId: 428277117\nChange-Id: I9c5bc8aef8cffc4349e70e0fb1934fdea4adde6e",
    "Number of deleted lines": 1,
    "Deleted lines": "-      VLOG(2) << \"This would violate asynchronous copy resource = \"",
    "Added lines": "+  ClearPendingChunks();\n+  // As a compilation time optimization, store the prefetch start time where we\n+  // have first seen out of memory. There is no point of exploring prefetch\n+  // start times earlier than this point.\n+  absl::optional<int64_t> out_of_mem_start;\n+    if (out_of_mem_start.has_value() &&\n+        alternate_mem_interval.start <= *out_of_mem_start) {\n+      VLOG(4) << \"This would OOM (cached).\";\n+      result_mark(Result::kFailOutOfMemory, result);\n+      continue;\n+    }\n+      VLOG(4) << \"This would violate asynchronous copy resource = \"\n+    } else {\n+      // Mark the out of memory start with the prefetch start time so that we\n+      // don't explore prefetch start times earlier than this point.\n+      out_of_mem_start =\n+          std::max(out_of_mem_start.has_value() ? *out_of_mem_start : -1,\n+                   alternate_mem_interval.start);",
    "Label": "clean"
},
{
    "Id": 256,
    "Library": "tensorflow",
    "Date": "2022/02/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/814883f04962dd4df85d9244e5ef86ed46b9d8ca",
    "Root Cause": "N.A",
    "Bug report": "Fix the MSVC bug in another file.\n\n```\nerror C2039: 'copysign': is not a member of '`global namespace''\n```\n\nPiperOrigin-RevId: 427293506\nChange-Id: I65648c7ee4f766059fe070dc1559d7cfb58b9184",
    "Number of deleted lines": 4,
    "Deleted lines": "-#include <Python.h>\n-#include \"pybind11/pybind11.h\"\n-#include \"pybind11/stl.h\"\n-#include \"pybind11/stl_bind.h\"",
    "Added lines": "+// clang-format off\n+// These headers must be at the top, before including Python.h header\n+// Otherwise, we get C2039 on MSVC due to 'copysign'\n+#include \"pybind11/complex.h\"\n+#include \"pybind11/pybind11.h\"\n+#include \"pybind11/stl.h\"\n+#include \"pybind11/stl_bind.h\"\n+// clang-format on\n+\n+#include \"Python.h\"",
    "Label": "clean"
},
{
    "Id": 257,
    "Library": "tensorflow",
    "Date": "2022/02/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7867014df1ce2b7c31ff59ce46a08b4ce115d94e",
    "Root Cause": "N.A",
    "Bug report": "Add missing imports that should fix the MSVC bug\n\n```\nerror C2039: 'copysign': is not a member of '`global namespace''\n```\n\nPiperOrigin-RevId: 427269276\nChange-Id: Ife6b888f30e0ebf20a35c8ebfdce4e38575979f7",
    "Number of deleted lines": 1,
    "Deleted lines": "-#include \"pybind11/pybind11.h\"",
    "Added lines": "+// clang-format off\n+// These headers must be at the top, before including Python.h header\n+// Otherwise, we get C2039 on MSVC due to 'copysign'\n+#include \"pybind11/complex.h\"\n+#include \"pybind11/pybind11.h\"\n+// clang-format on\n+",
    "Label": "clean"
},
{
    "Id": 258,
    "Library": "tensorflow",
    "Date": "2022/02/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3edc862f922b43bedbafad0f6b091a9d71055795",
    "Root Cause": "N.A",
    "Bug report": "Switch from `<Python.h>` to `\"Python.h\"` to fix MSVC bug\n\nShould probably fix the\n\n```\nerror C2039: 'copysign': is not a member of '`global namespace''\n```\n\nerror we're seeing? Testing with one file before doing a larger change.\n\nPiperOrigin-RevId: 427246946\nChange-Id: Iae9100cd9a43bb5ecff90008a35a331f3a94c94e",
    "Number of deleted lines": 2,
    "Deleted lines": "-#include <Python.h>\n-",
    "Added lines": "+#include \"Python.h\"",
    "Label": "clean"
},
{
    "Id": 259,
    "Library": "tensorflow",
    "Date": "2022/02/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ecda9b3d753eff967c7f88eef84c7c9f3dec7fef",
    "Root Cause": "N.A",
    "Bug report": "This is necessary if the user has instantiated multiple sessions for a single cluster/device and invokes TF_OptimizeGraph without acquiring a python lock. The reason: PyErr_SetObject (called in SetRegisteredErrFromStatus) assumes that the GIL is acquired. More specifically, if the GIL is not acquired and we execute PyThreadState_GET(), which itself is a wrapper around the macro GET_TSTATE which does a relaxed atomic load of &_PyThreadState_Current, we will likely be reading the wrong thread's ThreadState and later in PyErr_SetObject try to dereference it by accessing one of its members resulting in a segfault.\n\nSee the following for a very similar issue: https://bugs.python.org/issue17706\n\nPiperOrigin-RevId: 425648156\nChange-Id: I0d8c11d34468c6b318ca836eeddb641182a54d6e",
    "Number of deleted lines": 1,
    "Deleted lines": "-            MaybeRaiseRegisteredFromStatus(",
    "Added lines": "+            MaybeRaiseRegisteredFromStatusWithGIL(",
    "Label": "clean"
},
{
    "Id": 260,
    "Library": "tensorflow",
    "Date": "2022/01/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e6a59c2da9976b603a2d90e9a7a7766d18e55803",
    "Root Cause": "N.A",
    "Bug report": "Add a VLOG to ease debugging CollectiveReduceV2 with legacy bridge on XLA path.\n\nPiperOrigin-RevId: 424711406\nChange-Id: Ib6b90ddbd01a168df71b98d2014d47ecff1efba1",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    VLOG(2) << \"Emitting xla::AllReduce on channel \" << *channel_id\n+            << \" for Op \" << ctx->op_kernel().name()\n+            << \" group_size=\" << group_size << \" group_key=\" << group_key;",
    "Label": "clean"
},
{
    "Id": 261,
    "Library": "tensorflow",
    "Date": "2022/01/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e4b62da3a61a1b43bda8eb3aad85de5e83cf354d",
    "Root Cause": "N.A",
    "Bug report": "Change LOG(INFO) to VLOG(1) to give more fine grained control of logging.\n\nThis also brings the debug logging inline with other vmodule logging used in the file.\n\nPiperOrigin-RevId: 421871339\nChange-Id: I48dabfabd8b836368c74c63d608028b66f4fc927",
    "Number of deleted lines": 1,
    "Deleted lines": "-void MetaOptimizer::PrintResult() { LOG(INFO) << GetResultString(); }",
    "Added lines": "+void MetaOptimizer::PrintResult() { VLOG(1) << GetResultString(); }",
    "Label": "clean"
},
{
    "Id": 262,
    "Library": "tensorflow",
    "Date": "2022/01/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/93d09ee4ac3a44925595884390be6eb48103e840",
    "Root Cause": "N.A",
    "Bug report": "Disable Test: (tensorflow/python/debug/cli:readline_ui_test) on Windows\n\nPiperOrigin-RevId: 421654465\nChange-Id: I6d825c8389aa2c956a7c880e6f37a04617a162d7",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    tags = [\"no_windows\"],  # TODO(b/214427155)",
    "Label": "clean"
},
{
    "Id": 263,
    "Library": "tensorflow",
    "Date": "2022/01/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/913aed00db90cf812a77a9f043ef961b57c0db83",
    "Root Cause": "N.A",
    "Bug report": "Remove some of the remote_calls from multideviceiterator implementation\n\nNot all remote_calls can be removed due to a bug/feature in our\ndevice placement logic. I added reference to the bug so we know where to look\nnext time visiting this.\n\nPiperOrigin-RevId: 421351829\nChange-Id: I19206a1d55d7ceb8338470b138da1e56d89b831d",
    "Number of deleted lines": 28,
    "Deleted lines": "-    init_func_concrete = _init_func.get_concrete_function()\n-\n-    # TODO(b/124254153): Enable autograph once the overhead is low enough.\n-    @function.defun(autograph=False)  # Pure graph code.\n-    def _remote_init_func():\n-      return functional_ops.remote_call(\n-          target=source_device,\n-          args=init_func_concrete.captured_inputs,\n-          Tout=[dtypes.string],\n-          f=init_func_concrete)\n-\n-    self._init_func = _remote_init_func.get_concrete_function()\n-    @function.defun(\n-    # TODO(b/124254153): Enable autograph once the overhead is low enough.\n-    finalize_func_concrete = _finalize_func.get_concrete_function()\n-\n-    # TODO(b/124254153): Enable autograph once the overhead is low enough.\n-    @function.defun(\n-        input_signature=[tensor_spec.TensorSpec([], dtypes.string)],\n-        autograph=False)  # Pure graph code.\n-    def _remote_finalize_func(string_handle):\n-      return functional_ops.remote_call(\n-          target=source_device,\n-          args=[string_handle] + finalize_func_concrete.captured_inputs,\n-          Tout=[dtypes.int64],\n-          f=finalize_func_concrete)\n-\n-    self._finalize_func = _remote_finalize_func.get_concrete_function()",
    "Added lines": "+    self._init_func = _init_func.get_concrete_function()\n+    @function.defun_with_attributes(\n+    # TODO(b/213621472): Remove this remote call.\n+    # TODO(b/211676070): Directly capture the iterator resource via the\n+    # function, removing the serialization Ops.\n+    self._finalize_func = _finalize_func.get_concrete_function()",
    "Label": "clean"
},
{
    "Id": 264,
    "Library": "tensorflow",
    "Date": "2022/01/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9d8f4e90ea5410c64c38abe696b18c05f8d762ba",
    "Root Cause": "N.A",
    "Bug report": "Remove some of the remote_calls from multideviceiterator implementation\n\nNot all remote_calls can be removed due to a bug/feature in our\ndevice placement logic. I added reference to the bug so we know where to look\nnext time visiting this.\n\nPiperOrigin-RevId: 421065035\nChange-Id: I1f897a1169ecc4bc182bf9aefd80692b010f5ffa",
    "Number of deleted lines": 28,
    "Deleted lines": "-    init_func_concrete = _init_func.get_concrete_function()\n-\n-    # TODO(b/124254153): Enable autograph once the overhead is low enough.\n-    @function.defun(autograph=False)  # Pure graph code.\n-    def _remote_init_func():\n-      return functional_ops.remote_call(\n-          target=source_device,\n-          args=init_func_concrete.captured_inputs,\n-          Tout=[dtypes.string],\n-          f=init_func_concrete)\n-\n-    self._init_func = _remote_init_func.get_concrete_function()\n-    @function.defun(\n-    # TODO(b/124254153): Enable autograph once the overhead is low enough.\n-    finalize_func_concrete = _finalize_func.get_concrete_function()\n-\n-    # TODO(b/124254153): Enable autograph once the overhead is low enough.\n-    @function.defun(\n-        input_signature=[tensor_spec.TensorSpec([], dtypes.string)],\n-        autograph=False)  # Pure graph code.\n-    def _remote_finalize_func(string_handle):\n-      return functional_ops.remote_call(\n-          target=source_device,\n-          args=[string_handle] + finalize_func_concrete.captured_inputs,\n-          Tout=[dtypes.int64],\n-          f=finalize_func_concrete)\n-\n-    self._finalize_func = _remote_finalize_func.get_concrete_function()",
    "Added lines": "+    self._init_func = _init_func.get_concrete_function()\n+    @function.defun_with_attributes(\n+    # TODO(b/213621472): Remove this remote call.\n+    # TODO(b/211676070): Directly capture the iterator resource via the\n+    # function, removing the serialization Ops.\n+    self._finalize_func = _finalize_func.get_concrete_function()",
    "Label": "clean"
},
{
    "Id": 265,
    "Library": "tensorflow",
    "Date": "2022/01/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5cf030785b78c672ac49a58fecf7911ccd8d9442",
    "Root Cause": "N.A",
    "Bug report": "remove debug string",
    "Number of deleted lines": 1,
    "Deleted lines": "-  printf(\"&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\");",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 266,
    "Library": "tensorflow",
    "Date": "2022/01/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5d2a37a1ca528d454fc33400cad1d3163f1672b2",
    "Root Cause": "N.A",
    "Bug report": "[tf:tfrt] Check the returned memref alignment\n\nFailures in downstream kernels are hard to debug and impossible to find the original source of misaligned tensor.\n\nPiperOrigin-RevId: 420368719\nChange-Id: I4b6f73e26ffbb37e49dafe77f1b798487311f744",
    "Number of deleted lines": 2,
    "Deleted lines": "-      DCHECK(tensor.IsAligned()) << \"global memref is not aligned\";\n-      DCHECK(tensor.IsAligned()) << \"allocated memref is not aligned\";",
    "Added lines": "+      CHECK(tensor.IsAligned()) << \"global memref is not aligned\";\n+      CHECK(tensor.IsAligned()) << \"allocated memref is not aligned\";",
    "Label": "clean"
},
{
    "Id": 267,
    "Library": "tensorflow",
    "Date": "2022/01/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1a28739b0ce1f6670f1d6d93badc5f3b1ecdfe7d",
    "Root Cause": "N.A",
    "Bug report": "Run only one job per step.\n\nTrying to debug timeouts/hangs\n\nPiperOrigin-RevId: 419668412\nChange-Id: I36c208d677cba59575d75cbe26b4342ef1bf0324",
    "Number of deleted lines": 1,
    "Deleted lines": "-bazel test \\",
    "Added lines": "+bazel test -j 1\\",
    "Label": "clean"
},
{
    "Id": 268,
    "Library": "tensorflow",
    "Date": "2022/01/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7e7a73b9de37b658b59d12be993271849cc02916",
    "Root Cause": "N.A",
    "Bug report": "Attaches the traceback of the source_error into the wrapped error in autograph's\n`create_exception` method, which could improve the debugging experience using `pdb.pm`.\n\nPiperOrigin-RevId: 419500371\nChange-Id: I8f6b83a1202b0228c7edc6fea860865cf7b6501d",
    "Number of deleted lines": 4,
    "Deleted lines": "-      return preferred_type(self.get_message())\n-      return preferred_type(self.get_message())\n-      return MultilineMessageKeyError(self.get_message(), self.cause_message)\n-    return None",
    "Added lines": "+    \"\"\"Creates exception from source_error.\"\"\"\n+    to_ret = None\n+      to_ret = preferred_type(self.get_message())\n+      to_ret = preferred_type(self.get_message())\n+      to_ret = MultilineMessageKeyError(self.get_message(), self.cause_message)\n+\n+    if to_ret is not None:\n+      return to_ret.with_traceback(source_error.__traceback__)",
    "Label": "clean"
},
{
    "Id": 269,
    "Library": "tensorflow",
    "Date": "2021/12/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c191a1083020eb598fb6e549010cf90b1ba04bb7",
    "Root Cause": "N.A",
    "Bug report": "update `is_numeric_tensor` with new dtypes\n\n`tf.debugging.is_numeric_tensor` is out-of-date. Some new dtypes are not there.",
    "Number of deleted lines": 3,
    "Deleted lines": "-    [dtypes.float32, dtypes.float64, dtypes.int8, dtypes.int16, dtypes.int32,\n-     dtypes.int64, dtypes.uint8, dtypes.qint8, dtypes.qint32, dtypes.quint8,\n-     dtypes.complex64])",
    "Added lines": "+    [dtypes.float16, dtypes.float32, dtypes.float64, dtypes.int8, dtypes.int16,\n+     dtypes.int32, dtypes.int64, dtypes.uint8, dtypes.uint16, dtypes.uint32,\n+     dtypes.uint64, dtypes.qint8, dtypes.qint16, dtypes.qint32, dtypes.quint8,\n+     dtypes.quint16, dtypes.complex64, dtypes.complex128, dtypes.bfloat16])\n+  * `tf.float16`\n+  * `tf.uint16`\n+  * `tf.uint32`\n+  * `tf.uint64`\n+  * `tf.qint16`\n+  * `tf.quint16`\n+  * `tf.complex128`\n+  * `tf.bfloat16`",
    "Label": "clean"
},
{
    "Id": 270,
    "Library": "tensorflow",
    "Date": "2021/12/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0d3be41279c77ae76ce26fd175c14b63d47ba8e7",
    "Root Cause": "N.A",
    "Bug report": "Debug the macos nightly hang.\n\nIf this works, will update other scripts too.\n\nPiperOrigin-RevId: 418982857\nChange-Id: I4502ccd7f4b08477893fbb29436186210b038466",
    "Number of deleted lines": 1,
    "Deleted lines": "-bazel test \\",
    "Added lines": "+# TODO(b/212470799): Figure out why this is extremely slow / hangs.\n+bazel test --rules -j 4\\",
    "Label": "clean"
},
{
    "Id": 271,
    "Library": "tensorflow",
    "Date": "2021/12/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c3510a0aa8949bbb5c4852994f6225fcb8b2d12c",
    "Root Cause": "N.A",
    "Bug report": "[TF-TRT] Fix bug in the convert summary method\n\nSigned-off-by: Serge Panev <spanev@nvidia.com>",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return [dtype.str() for dtype in node.attr[key].list.type]",
    "Added lines": "+  return [\n+    dtypes._TYPE_TO_STRING[dtype]\n+    for dtype in node.attr[key].list.type\n+  ]",
    "Label": "clean"
},
{
    "Id": 272,
    "Library": "tensorflow",
    "Date": "2021/12/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6005ff64bfd9fb47e1a7aa5475a53ed90fd91776",
    "Root Cause": "N.A",
    "Bug report": "Fix bugs introduced by graph summary tags.\n\nPiperOrigin-RevId: 417510329\nChange-Id: I2f0b5fce54143695e3ac1ff436c68b25a4490e74",
    "Number of deleted lines": 1,
    "Deleted lines": "-                graph_summary_tag + '/' + key, value, metadata=summary_metadata,",
    "Added lines": "+                _TT_SUMMARY_TAG + '/' + key + '#' + graph_summary_tag + '#',\n+                value, metadata=summary_metadata,",
    "Label": "clean"
},
{
    "Id": 273,
    "Library": "tensorflow",
    "Date": "2021/12/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/894bdd6a269fa0fa3b067febd79abfa9fff635d9",
    "Root Cause": "N.A",
    "Bug report": "MacOS Build - Nightly - Debug 10_14/10_15\n\nPiperOrigin-RevId: 416281151\nChange-Id: I04a21fa01c8be28285dc973c5fe44a61dab8cd9d",
    "Number of deleted lines": 3,
    "Deleted lines": "-# TODO(rameshsampath): Update MACOS Development Target\n-export MACOSX_DEPLOYMENT_TARGET=10.10\n-",
    "Added lines": "+  # Change 10_15 to 10_14\n+  NEW_WHL_PATH=${f/macosx_10_15/macosx_10_14}\n+  mv ${f} ${NEW_WHL_PATH}\n+  f=${NEW_WHL_PATH}",
    "Label": "clean"
},
{
    "Id": 274,
    "Library": "tensorflow",
    "Date": "2021/12/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3b956a58ac5eeaf87678fe903537b2e38aed7018",
    "Root Cause": "N.A",
    "Bug report": "Fix crash bug when dealing with scalar types.\n\nFor scalar types, we have 0 loops.\n\nPiperOrigin-RevId: 415237564\nChange-Id: Ie0d8bef3e9c88335c4b1eff6820bcf2a39d7c72f",
    "Number of deleted lines": 2,
    "Deleted lines": "-    if (failed(tiled_linalg_op)) return failure();\n-          tiles.back() = b.create<ConstantIndexOp>(op->getLoc(), 8);",
    "Added lines": "+    if (failed(tiled_linalg_op) || tiled_linalg_op.getValue().loops.empty())\n+      return failure();\n+          if (!tiles.empty())\n+            tiles.back() = b.create<ConstantIndexOp>(op->getLoc(), 8);",
    "Label": "clean"
},
{
    "Id": 275,
    "Library": "tensorflow",
    "Date": "2021/12/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4b262f5774f7e5c8db08cc44dea1f909bfb3d786",
    "Root Cause": "N.A",
    "Bug report": "Use Protobuf TextFormat instead of DebugString to create a text proto\n\nPiperOrigin-RevId: 414984815\nChange-Id: I530e0a42dcffff13f81d0adf83f862888a9ab595",
    "Number of deleted lines": 22,
    "Deleted lines": "-#include \"tensorflow/core/platform/protobuf.h\"\n-#if defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)\n-string PrintInTextFormat(const protobuf::MessageLite& message) {\n-  // Unfortunately proto2::TextFormat::Printer::PrintToString does not have\n-  // a overload for MessageLite so here we have to use\n-  // MessageLite::ShortDebugString.\n-  return message.ShortDebugString();\n-}\n-#else\n-string PrintInTextFormat(const protobuf::Message& message) {\n-  string message_text;\n-  ::tensorflow::protobuf::TextFormat::Printer printer;\n-  printer.SetSingleLineMode(true);\n-  printer.PrintToString(message, &message_text);\n-  if (!message_text.empty() && message_text[message_text.size() - 1] == ' ') {\n-    message_text.resize(message_text.size() - 1);\n-  }\n-  return message_text;\n-}\n-#endif  // defined(IS_MOBILE_PLATFORM) || defined(IS_SLIM_BUILD)\n-\n-                                         PrintInTextFormat(attr.second));",
    "Added lines": "+                                         attr.second.ShortDebugString());",
    "Label": "clean"
},
{
    "Id": 276,
    "Library": "tensorflow",
    "Date": "2021/12/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5c252f8c508ea30f7414378cb43571f58b503867",
    "Root Cause": "N.A",
    "Bug report": "Remove debug info to the other macos release script.\n\nPiperOrigin-RevId: 414874984\nChange-Id: I81f183aaed5112da9eb3ec3be8007ed0749e3f24",
    "Number of deleted lines": 3,
    "Deleted lines": "-# TODO(mihaimaruseac): Debug info, remove later\n-${PYTHON_BIN_PATH} -m pip debug --verbose\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 277,
    "Library": "tensorflow",
    "Date": "2021/12/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/005ddfe4a123d7471030065f73d4fade10cc2775",
    "Root Cause": "N.A",
    "Bug report": "Add debug info to the other macos release script.\n\nPiperOrigin-RevId: 414864914\nChange-Id: Ie96925327e8f91cd973751a599f18af66636adcd",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+# TODO(mihaimaruseac): Debug info, remove later\n+${PYTHON_BIN_PATH} -m pip debug --verbose\n+",
    "Label": "clean"
},
{
    "Id": 278,
    "Library": "tensorflow",
    "Date": "2021/12/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/80bcfe068776b227ef3d26318d58194ecbd26956",
    "Root Cause": "N.A",
    "Bug report": "Remove debug info for macos nightly as it is no longer needed\n\nPiperOrigin-RevId: 414864746\nChange-Id: I84d95a47d7e1064bf8e92b5a491f4a5442460be0",
    "Number of deleted lines": 3,
    "Deleted lines": "-# TODO(mihaimaruseac): Debug info, remove later\n-${PYTHON_BIN_PATH} -m pip debug --verbose\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 279,
    "Library": "tensorflow",
    "Date": "2021/12/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d68b03fae432f45dd24faed3889fcd167c7a6074",
    "Root Cause": "N.A",
    "Bug report": "Add debug info for macos nightly as they fail with the new xcode bump\n\nPiperOrigin-RevId: 414816991\nChange-Id: I49c5139332a275d7a3939ef529bb37b4b02393af",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+# TODO(mihaimaruseac): Debug info, remove later\n+${PYTHON_BIN_PATH} -m pip debug --verbose\n+",
    "Label": "clean"
},
{
    "Id": 280,
    "Library": "tensorflow",
    "Date": "2021/12/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/45d7d254a5736a1702c2378aa24ec7c4e26daaf2",
    "Root Cause": "N.A",
    "Bug report": "Use Protobuf TextFormat instead of DebugString to create a text proto\n\nPiperOrigin-RevId: 414749793\nChange-Id: Ib0e42421c3c904fecd12c759aaec95d731c12c82",
    "Number of deleted lines": 11,
    "Deleted lines": "-#include \"tensorflow/core/platform/protobuf.h\"\n-      std::string attr_second_text;\n-      ::tensorflow::protobuf::TextFormat::Printer printer;\n-      printer.SetSingleLineMode(true);\n-      printer.PrintToString(attr.second, &attr_second_text);\n-      if (!attr_second_text.empty() &&\n-          attr_second_text[attr_second_text.size() - 1] == ' ') {\n-        attr_second_text.resize(attr_second_text.size() - 1);\n-      }\n-      string attr_str =\n-          absl::Substitute(\"('$0', $1)\", attr.first, attr_second_text);",
    "Added lines": "+      string attr_str = absl::Substitute(\"('$0', $1)\", attr.first,\n+                                         attr.second.ShortDebugString());",
    "Label": "clean"
},
{
    "Id": 281,
    "Library": "tensorflow",
    "Date": "2021/12/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dcaa60290e85d186ebd9eae9df87b2bbc4756099",
    "Root Cause": "N.A",
    "Bug report": "Use Protobuf TextFormat instead of DebugString to create a text proto\n\nPiperOrigin-RevId: 414741319\nChange-Id: Ic12af92b36dbdc1346be0f75a2544de67f8e260a",
    "Number of deleted lines": 1,
    "Deleted lines": "-  string new_api_defs_str = api_defs.DebugString();",
    "Added lines": "+\n+#include \"tensorflow/core/platform/protobuf.h\"\n+  std::string new_api_defs_str;\n+  ::tensorflow::protobuf::TextFormat::PrintToString(api_defs,\n+                                                    &new_api_defs_str);",
    "Label": "clean"
},
{
    "Id": 282,
    "Library": "tensorflow",
    "Date": "2021/12/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4b8b9db1ac4f102a7abbe8dff96ad59a20bb2aac",
    "Root Cause": "N.A",
    "Bug report": "Use Protobuf TextFormat instead of DebugString to create a text proto\n\nPiperOrigin-RevId: 414595206\nChange-Id: Ia3a967a8392440771cd8558d3e010c89bb187f11",
    "Number of deleted lines": 2,
    "Deleted lines": "-      string attr_str = absl::Substitute(\"('$0', $1)\", attr.first,\n-                                         attr.second.ShortDebugString());",
    "Added lines": "+#include \"tensorflow/core/platform/protobuf.h\"\n+      std::string attr_second_text;\n+      ::tensorflow::protobuf::TextFormat::Printer printer;\n+      printer.SetSingleLineMode(true);\n+      printer.PrintToString(attr.second, &attr_second_text);\n+      if (!attr_second_text.empty() &&\n+          attr_second_text[attr_second_text.size() - 1] == ' ') {\n+        attr_second_text.resize(attr_second_text.size() - 1);\n+      }\n+      string attr_str =\n+          absl::Substitute(\"('$0', $1)\", attr.first, attr_second_text);",
    "Label": "clean"
},
{
    "Id": 283,
    "Library": "tensorflow",
    "Date": "2021/12/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/aba622c15efe76cfae043e2478a2b6c7c9277077",
    "Root Cause": "N.A",
    "Bug report": "[tf:tfrt] Fix cpurt benchmark errors\n\n- Make cpurt benchmarks runnable in debug/asan modes.\n- Use single threaded work queue in TFRT benchmarks for num_threads=0\n\nPiperOrigin-RevId: 414126025\nChange-Id: Id6ee6c1cc706324c30c63948b0bf6ededa0cc2a0",
    "Number of deleted lines": 1,
    "Deleted lines": "-  auto host = CreateMultiThreadedHostContext(num_threads);",
    "Added lines": "+#include <algorithm>\n+#include <functional>\n+#include <utility>\n+    call_frame.args[0] = nullptr;  // reset kernel context argument\n+  auto host = num_threads ? CreateMultiThreadedHostContext(num_threads)\n+                          : CreateSingleThreadedHostContext();",
    "Label": "clean"
},
{
    "Id": 284,
    "Library": "tensorflow",
    "Date": "2021/12/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a4fb02b15ec7692d1ec0ca72b376d7a8b94f281c",
    "Root Cause": "N.A",
    "Bug report": "Disable Test for Python 3.10 (tensorflow/python/debug/lib:source_utils_test)\n\nPiperOrigin-RevId: 414074861\nChange-Id: I04ec558c4bddc213fd31ea2a5b91765843439338",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"no_oss_py310\",  # b/209089624\n+        \"no_oss_py310\",  # b/209089616",
    "Label": "clean"
},
{
    "Id": 285,
    "Library": "tensorflow",
    "Date": "2021/12/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0c1c22071bdd35e83ed0163b45d4bcc9cc428457",
    "Root Cause": "N.A",
    "Bug report": "[XLA:TPU] Edge condition bug fix for ApproxTopK/PartialReduce\n\nPiperOrigin-RevId: 413972682\nChange-Id: I4b2b61de189251122db909021644bced0ca0d876",
    "Number of deleted lines": 2,
    "Deleted lines": "-      std::max(static_cast<uint64_t>((1.0 - top_k) / std::log(recall_target)),\n-               tpu_tiling),",
    "Added lines": "+  // Need to handle 1.0 explicitly, otherwise we would encounter division by\n+  // log(1.0) = 0 issue.\n+  if (recall_target == 1.0) {\n+    return std::pair<int64_t, int64_t>(input_size, 0);\n+  }\n+\n+      std::max(\n+          static_cast<uint64_t>((1.0 - top_k) /\n+                                std::log(static_cast<double>(recall_target))),\n+          tpu_tiling),",
    "Label": "clean"
},
{
    "Id": 286,
    "Library": "tensorflow",
    "Date": "2021/12/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/29d30e09cd6266a2ffc8240cf0e9d117970c94b5",
    "Root Cause": "N.A",
    "Bug report": "[Cleanup] Remove TODO references to bug b/111309333 (fixed)\n\nPiperOrigin-RevId: 413708134\nChange-Id: Ic232c302df4d584fe64f8f036bda024e8d2ddfad",
    "Number of deleted lines": 1,
    "Deleted lines": "-  // TODO(b/111309333): Handle optional seed input.",
    "Added lines": "+  // The seed/seed2 attributes are not handled in this custom op implementation.",
    "Label": "clean"
},
{
    "Id": 287,
    "Library": "tensorflow",
    "Date": "2021/11/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c3671002a364ff5bda8d23885ac0a7ebe8ad8eff",
    "Root Cause": "N.A",
    "Bug report": "Permanent fix for parallel compilation bug.\n\nPiperOrigin-RevId: 413286831\nChange-Id: I40063b79339c1d81418428b1be7a4d34fc8e3ff9",
    "Number of deleted lines": 16,
    "Deleted lines": "-#include <tuple>\n-\n-  const bool linking_supported = [] {\n-      return true;\n-    }\n-    auto version_or_status = gpu::Diagnostician::FindKernelDriverVersion();\n-    if (!version_or_status.ok()) {\n-      LOG(WARNING) << \"Couldn't read CUDA driver version.\";\n-      return false;\n-    std::tuple<int, int, int> version = *version_or_status;\n-    if (CUDA_VERSION < 11040) return version >= std::make_tuple(465, 19, 1);\n-    if (CUDA_VERSION < 11050) return version >= std::make_tuple(470, 82, 1);\n-    return version >= std::make_tuple(495, 29, 5);\n-  if (!linking_supported) {\n-    return tensorflow::errors::Unimplemented(\"Linking is unsupported\");\n-  }",
    "Added lines": "+  const port::Status linking_supported = [] {\n+      return port::Status::OK();\n+    int driver_cuda_version;\n+    // Get the highest version of CUDA supported by this driver.\n+    RETURN_IF_CUDA_ERROR(cuDriverGetVersion(&driver_cuda_version));\n+    return driver_cuda_version >= CUDA_VERSION\n+               ? port::Status::OK()\n+               : tensorflow::errors::Unimplemented(\n+                     \"CUDA version unsupported by NVIDIA driver version.\");\n+  TF_RETURN_IF_ERROR(linking_supported);",
    "Label": "clean"
},
{
    "Id": 288,
    "Library": "tensorflow",
    "Date": "2021/11/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1262377f39bbd734a9a8da65dba699f1e492d59a",
    "Root Cause": "N.A",
    "Bug report": "Spelling fix for PythonAPIDispatcher::DebugString.\n\nPiperOrigin-RevId: 412040419\nChange-Id: Ic7a1abe92802a3e94dd3c23a2a69bbcb7a9384ba",
    "Number of deleted lines": 1,
    "Deleted lines": "-  std::string out = absl::StrCat(\"<Disptach(\", api_name_, \"): \");",
    "Added lines": "+  std::string out = absl::StrCat(\"<Dispatch(\", api_name_, \"): \");",
    "Label": "clean"
},
{
    "Id": 289,
    "Library": "tensorflow",
    "Date": "2021/11/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/201345c2dfc40b842a1f9e3e731f4309b056ecb0",
    "Root Cause": "N.A",
    "Bug report": "Convert `DCHECK` for L1 into an `OP_REQUIRES`.\n\n`DCHECK` is a no-op in release builds and a `CHECK`-fail in debug builds. We actually need to validate L1 to prevent vulnerabilities.\n\nPiperOrigin-RevId: 411120531\nChange-Id: Ie19bb632154b03ab6d3d891274f937b851b13359",
    "Number of deleted lines": 2,
    "Deleted lines": "-    DCHECK_GE(l1, 0);\n-      DCHECK_EQ(l1, 0);",
    "Added lines": "+    OP_REQUIRES(context, l1 >= 0,\n+                errors::InvalidArgument(\"l1 = \", l1, \" but it should be >= 0\"));\n+      OP_REQUIRES(\n+          context, l1 == 0,\n+          errors::InvalidArgument(\n+              \"l1 != 0 is not yet supported for multi-class regularization\"));",
    "Label": "clean"
},
{
    "Id": 290,
    "Library": "tensorflow",
    "Date": "2021/11/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8afd8c4954ee6b3394b4a5a2af8bec1e921efea7",
    "Root Cause": "N.A",
    "Bug report": "Remove debug print.\n\nPiperOrigin-RevId: 410679370\nChange-Id: Ia723b4b4efe187d8d7a637b9dde2b2e3ccc9847f",
    "Number of deleted lines": 2,
    "Deleted lines": "-      fprintf(stdout, \"1111111\\n\");\n-      fflush(stdout);",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 291,
    "Library": "tensorflow",
    "Date": "2021/11/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dcc21c7bc972b10b6fb95c2fb0f4ab5a59680ec2",
    "Root Cause": "N.A",
    "Bug report": "Eliminate debug `CHECK`-fail from `function.cc`\n\nPiperOrigin-RevId: 409416119\nChange-Id: I8376ee464d434e9b970ff0ad49edfdaa2a273cfe",
    "Number of deleted lines": 1,
    "Deleted lines": "-      DCHECK_EQ(arg_index, result_.nodes.size());",
    "Added lines": "+      if (arg_index != result_.nodes.size()) {\n+        return errors::Internal(\n+            \"Expected arg_index to be equal to the number of nodes in result.\",\n+            \" Got \", arg_index, \" and \", result_.nodes.size());\n+      }",
    "Label": "clean"
},
{
    "Id": 292,
    "Library": "tensorflow",
    "Date": "2021/11/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/09dab4f0e0f3785c3b764b64b476831e529bb139",
    "Root Cause": "N.A",
    "Bug report": "Remove stray debugging statement.\n\nPiperOrigin-RevId: 409405707\nChange-Id: I195fd3c767da6b42b365d4f4cb8c80abaeb03433",
    "Number of deleted lines": 1,
    "Deleted lines": "-      std::cerr << \"Cleanup called...\\n\";",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 293,
    "Library": "tensorflow",
    "Date": "2021/11/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ef1d027be116f25e25bb94a60da491c2cf55bd0b",
    "Root Cause": "N.A",
    "Bug report": "Prevent copying uninitialized data in `AssignOp`.\n\nThis prevents harder to debug undefined behaviors that cannot be traced back to the original tensor after assignments occur earlier in the graph execution. Several of these undefined behaviors are just reference bindings to null pointers, which are caught when running under ubsan/asan.\n\nPiperOrigin-RevId: 408654780\nChange-Id: Iad2ec40d43f5fd7ea016c20283356c12d5ddeab1",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    // Prevent copying uninitialized data, to solve harder to debug undefined\n+    // behaviors that cannot be traced back to the original tensor.\n+    OP_REQUIRES(\n+        context, rhs.IsInitialized(),\n+        errors::Internal(\"Right hand side of AssignOp is not initialized\"));\n+",
    "Label": "clean"
},
{
    "Id": 294,
    "Library": "tensorflow",
    "Date": "2021/11/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2b31ff2d3151acb230edc3f5b1832d2c713a9e0",
    "Root Cause": "N.A",
    "Bug report": "Remove a `DCHECK`-fail, log an error instead.\n\n`DCHECK` in debug mode results in crashes. TensorFlow has had multiple vulnerabilities due to this.\n\nOutside of debug mode, `DCHECK` is a no-op.\n\nA better alternative is to report an error to the log buffer and continue. This should happen both in debug mode and in prod mode.\n\nPiperOrigin-RevId: 408375925\nChange-Id: Id5b3e19c73f3fbe0cc4bba26ca44ff9607bb6356",
    "Number of deleted lines": 3,
    "Deleted lines": "-    DCHECK(a1_set.find(def.name()) == a1_set.end())\n-        << \"AttrDef names must be unique, but '\" << def.name()\n-        << \"' appears more than once\";",
    "Added lines": "+    if (a1_set.find(def.name()) != a1_set.end()) {\n+      LOG(ERROR) << \"AttrDef names must be unique, but '\" << def.name()\n+                 << \"' appears more than once\";\n+    }",
    "Label": "clean"
},
{
    "Id": 295,
    "Library": "tensorflow",
    "Date": "2021/11/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/27419e9f339b11e590e80aa965598e52bcd371dc",
    "Root Cause": "N.A",
    "Bug report": "Debug MacOS build, add profiling info\n\nPiperOrigin-RevId: 407381067\nChange-Id: I9a0db979e4c26576b9991f0669b500648cfab42e",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    --profile=\"${KOKORO_ARTIFACTS_DIR}/profile.json\" \\",
    "Label": "clean"
},
{
    "Id": 296,
    "Library": "tensorflow",
    "Date": "2021/10/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3f507b4f8a39a2978d0a626b3128549bf45505aa",
    "Root Cause": "N.A",
    "Bug report": "Remove  duplicate freeze variable pass\n\nSeems like no test is failing when removing this. The associated bug mention the\nneed to support WhileRegion but the code in the pass seems to have such support now.\n\nPiperOrigin-RevId: 406608589\nChange-Id: Ia056e7cd9ec895a5c46178b8eff15e7c6ce04472",
    "Number of deleted lines": 7,
    "Deleted lines": "-  // TODO(b/149099381): Remove after handling WhileRegion in favor of later\n-  // instance.\n-  if (session.hasValue()) {\n-    pass_manager->addPass(\n-        mlir::tf_saved_model::CreateFreezeVariablesPass(session.getValue()));\n-  }\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 297,
    "Library": "tensorflow",
    "Date": "2021/10/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6ab00d035f162354cbb946b775b354e02dcd73dc",
    "Root Cause": "N.A",
    "Bug report": "[XLIR] Dump TFRT BEF binary if debug options say so.\n\nPiperOrigin-RevId: 406170157\nChange-Id: Icb30faf0972893372dacf6bd3450e3ba0a777300",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (DumpingEnabledForHloModule(*hlo_module)) {\n+    DumpToFileInDirOrStdout(*hlo_module, \"\", \"tfrt_bef\", bef);\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 298,
    "Library": "tensorflow",
    "Date": "2021/10/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e2d117929e6cc4fa4c7ae3ab965997aecba87bf6",
    "Root Cause": "N.A",
    "Bug report": "Include the instruction in the error message when failing autotuning.\n\nThis would've helped with debugging a recent issue where x32 conv failed and I\ncouldn't tell what type was involved.\n\nPiperOrigin-RevId: 405782641\nChange-Id: I2c7450a87cde5f4879ccad04b71f9ee8dfcd6b50",
    "Number of deleted lines": 5,
    "Deleted lines": "-        \"Failed to determine best cudnn convolution algorithm: \"\n-        \"%s\\n\\nConvolution performance may be suboptimal.\",\n-        best_algo_or.status().ToString());\n-          \"%s  To ignore this failure and try to use a fallback algorithm, use \"\n-    LOG(WARNING) << msg;",
    "Added lines": "+        \"Failed to determine best cudnn convolution algorithm for:\\n%s\\n\\n\"\n+        \"Original error: %s\",\n+        instr->ToString(), best_algo_or.status().ToString());\n+          \"%s\\n\\nTo ignore this failure and try to use a fallback algorithm \"\n+          \"(which may have suboptimal performance), use \"\n+    LOG(WARNING)\n+        << msg << \"\\n\\nAs a result, convolution performance may be suboptimal.\";",
    "Label": "clean"
},
{
    "Id": 299,
    "Library": "tensorflow",
    "Date": "2021/10/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d4fe9e38485c499d2993f99cb806b99ab35b1b86",
    "Root Cause": "N.A",
    "Bug report": "[XLA:TPU] Fix shape bug for k=1 in ApproxTopK\n\nPiperOrigin-RevId: 405745255\nChange-Id: I7e5174a65b33306a009475ac49ead6d0e782ce02",
    "Number of deleted lines": 2,
    "Deleted lines": "-    return Reduce(builder, operands, init_values, reduction_computation,\n-                  {reduction_dim});",
    "Added lines": "+    auto val_args = Reduce(builder, operands, init_values,\n+                           reduction_computation, {reduction_dim});\n+    Shape op_shape = operands_shapes[0];\n+    op_shape.mutable_dimensions()[reduction_dim] = 1;\n+    auto top1_vals =\n+        Reshape(GetTupleElement(val_args, 0), op_shape.dimensions());\n+    auto top1_args =\n+        Reshape(GetTupleElement(val_args, 1), op_shape.dimensions());\n+    return Tuple(builder, {top1_vals, top1_args});",
    "Label": "clean"
},
{
    "Id": 300,
    "Library": "tensorflow",
    "Date": "2021/10/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9fed570e077126998a93743f096479735b7ee198",
    "Root Cause": "N.A",
    "Bug report": "Check for invalid TraceMeEncode character in debug mode\n\nPiperOrigin-RevId: 405442945\nChange-Id: I4c125cd3068f16dcb0e0ce20a909ec2758a99f6e",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include \"absl/strings/match.h\"\n+  DCHECK(!absl::StrContains(str, '#'))\n+      << \"'#' is not a valid character in TraceMeEncode\";",
    "Label": "clean"
},
{
    "Id": 301,
    "Library": "tensorflow",
    "Date": "2021/10/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e4045d49f7c1342dbbac0474cba8679a136ae972",
    "Root Cause": "N.A",
    "Bug report": "Update TODO to refer to bug.\n\nPiperOrigin-RevId: 404828456\nChange-Id: Ie9b9481283d415e3bd7e23ef44afcb50e1c968f0",
    "Number of deleted lines": 7,
    "Deleted lines": "-// TODO(hinsu): Generalize this pass to handle all the ops with regions. Any\n-// value used within the region that is defined outside of op's region should be\n-// sank to the regions and not just the constants. Ops such as If and While\n-// whose computations doesn't require fixed signature like Sort or Reduce have\n-// an option to pass outside values as operands of the op to avoid recomputing\n-// those within internally. Note that doing so is the only option in case of\n-// values defined outside that are BlockArguments of any of the parent region.",
    "Added lines": "+// TODO(b/203775547): Generalize this pass to handle all the ops with regions.\n+// Any value used within the region that is defined outside of op's region\n+// should be sank to the regions and not just the constants. Ops such as If and\n+// While whose computations doesn't require fixed signature like Sort or Reduce\n+// have an option to pass outside values as operands of the op to avoid\n+// recomputing those within internally. Note that doing so is the only option in\n+// case of values defined outside that are BlockArguments of any of the parent\n+// region.",
    "Label": "clean"
},
{
    "Id": 302,
    "Library": "tensorflow",
    "Date": "2021/10/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/839c78433c883f8f0f78839b4953db941c896a7f",
    "Root Cause": "N.A",
    "Bug report": "Reland: Use variadic reduce on GPU for argmax/argmin\n\nFixed underlying bug\n\nPiperOrigin-RevId: 404713314\nChange-Id: I28c4ae3c22b45a3de1e128992503160ee4d18a6f",
    "Number of deleted lines": 1,
    "Deleted lines": "-_version = 40",
    "Added lines": "+_version = 41",
    "Label": "clean"
},
{
    "Id": 303,
    "Library": "tensorflow",
    "Date": "2021/10/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b7c807287d23254e04da48033cabd63e7380628f",
    "Root Cause": "N.A",
    "Bug report": "[XLIR] Dump TFRT MLIR text if debug options say so.\n\nPiperOrigin-RevId: 404645361\nChange-Id: Ic5608d7b2588dec121446b9d205fbe8a940e4636",
    "Number of deleted lines": 3,
    "Deleted lines": "-                                           std::string entry_function_name) {\n-  TF_ASSIGN_OR_RETURN(results->thunks_or_bef,\n-                      LowerToBef(*mlir_module, entry_function.getName().str()));",
    "Added lines": "+                                           std::string entry_function_name,\n+                                           HloModule* hlo_module) {\n+  if (DumpingEnabledForHloModule(*hlo_module)) {\n+    std::string tfrt_mlir;\n+    llvm::raw_string_ostream tfrt_mlir_ostream(tfrt_mlir);\n+    mlir_module.print(tfrt_mlir_ostream);\n+    DumpToFileInDirOrStdout(*hlo_module, \"\", \"tfrt_mlir\", tfrt_mlir);\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(\n+      results->thunks_or_bef,\n+      LowerToBef(*mlir_module, entry_function.getName().str(), hlo_module));",
    "Label": "clean"
},
{
    "Id": 304,
    "Library": "tensorflow",
    "Date": "2021/10/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b84cc9a4abd7d087653b430d26579d2d1fe3c6e9",
    "Root Cause": "N.A",
    "Bug report": "Update bug ID to the bug focusing on remaining work\n\nPiperOrigin-RevId: 404641365\nChange-Id: Iaddf4caa4f5a8648eedcb375565312fe1c184c25",
    "Number of deleted lines": 1,
    "Deleted lines": "-  // TODO(b/178423010): Add more functional ops.",
    "Added lines": "+  // TODO(b/203689805): Add more functional ops.",
    "Label": "clean"
},
{
    "Id": 305,
    "Library": "tensorflow",
    "Date": "2021/09/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/af4115fb4dc1adc8cd49c729f4331dedf95797f3",
    "Root Cause": "N.A",
    "Bug report": "Add tfds update command to quantization debugger colab.\n\nThe default tensorflow_datasets version for current colab runtime is 4.0.1,and it has known issue with imagenet_v2 dataset which has been resolved. (https://github.com/tensorflow/datasets/issues/2888)\n\nUpdated accuracy numbers and added note that this numbers might change in the future.\n\nPiperOrigin-RevId: 400101225\nChange-Id: I2fadd6a02fc0cb5856ce358ec032fcf07a193311",
    "Number of deleted lines": 12,
    "Deleted lines": "-        \"!pip install tf-nightly\"\n-        \"model = tf.keras.Sequential([hub.KerasLayer(MODEL_URI)])\\n\",\n-        \"We can see that the original model has 82% top-5 accuracy for our small dataset,\\n\",\n-        \"while the quantized model shows 54% top-5 accuracy, which indicates a\\n\",\n-        \"significant accuracy loss.\"\n-        \"The accuracy is still lower compared to 84% of the original float model, but we\\n\",\n-        \"have 12pp improvement from the whole quantized model by skipping quantization\\n\",\n-        \"for ~10 layers out of 111 layers.\\n\",\n-        \"With these techniques, we were able to improve the quantized MobileNet V3 model\\n\",\n-        \"accuracy easily by 12pp. Next, we'll explore the advanced techniques to further\\n\",\n-        \"debug and improve the model accuracy\"\n-      \"collapsed_sections\": [],",
    "Added lines": "+        \"!pip install tf-nightly\\n\",\n+        \"!pip install tensorflow_datasets --upgrade  # imagenet_v2 needs latest checksum\"\n+        \"model = tf.keras.Sequential([\\n\",\n+        \"  tf.keras.layers.Input(shape=(224, 224, 3), batch_size=1),\\n\",\n+        \"  hub.KerasLayer(MODEL_URI)\\n\",\n+        \"])\\n\",\n+        \"We can see that the original model has a much higher top-5 accuracy for our\\n\",\n+        \"small dataset, while the quantized model has a significant accuracy loss.\"\n+        \"The accuracy is still lower compared to the original float model, but we have\\n\",\n+        \"notable improvement from the whole quantized model by skipping quantization for\\n\",\n+        \"~10 layers out of 111 layers.\\n\",\n+        \"With these techniques, we are able to improve the quantized MobileNet V3 model\\n\",\n+        \"accuracy. Next we'll explore advanced techniques to improve the model accuracy\\n\",\n+        \"even more.\"\n+      \"collapsed_sections\": [\n+        \"Eq_8T2oauIED\"\n+      ],",
    "Label": "clean"
},
{
    "Id": 306,
    "Library": "tensorflow",
    "Date": "2021/09/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/63a4f82cf87cfd28b8533c25bcceaf12889c947f",
    "Root Cause": "N.A",
    "Bug report": "Workaround MSVC14 bug on nested class access scope:\n\nhttps://developercommunity.visualstudio.com/t/cannot-access-to-protected-member-in-base-class-fr/428946\n\nPiperOrigin-RevId: 399498017\nChange-Id: I9d7edbd79b49270d22067c180b5fd790ff3db774",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // MSVC14 workaround: access permission of a nested class member is not\n+  // treated as an ordinary member in MSVC14.\n+  friend struct WeakRefData;",
    "Label": "clean"
},
{
    "Id": 307,
    "Library": "tensorflow",
    "Date": "2021/09/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8eccfc17bfc6567c9d0c8472ff7919c1a9b62b94",
    "Root Cause": "N.A",
    "Bug report": "Adding the ODML library to the generated docs in TF-Lite's Java API\n\nThe ODML docs are bundled with TF Lite, and are referenced by the support library, so we need to provide their reference docs too (e.g. `com.google.android.odml.image.MlImage`)\n\nAs the current system expects a single top-level package, I've added support for multiple packages in the javadoc generator and the TOC/YAML post-processor. There was a weird bug generating federated API references due to using `0` as the API name that has been resolved too.\n\nPiperOrigin-RevId: 397917665\nChange-Id: Icdff7692430ce0881561f4f93f4c1cd9b5b7da52",
    "Number of deleted lines": 1,
    "Deleted lines": "-        package='org.tensorflow.lite',",
    "Added lines": "+SOURCE_PATH_ODML = REPO_ROOT / 'tensorflow_lite_support/odml/java/image/src'\n+# This (key) ordering is preserved in the TOC output.\n+    # If we ever need other ODML packages, drop the `.image` here.\n+    'com.google.android.odml.image': 'ODML',\n+    overlay(SOURCE_PATH_ODML, merged_temp_dir)\n+        package=['org.tensorflow.lite', 'com.google.android.odml'],",
    "Label": "clean"
},
{
    "Id": 308,
    "Library": "tensorflow",
    "Date": "2021/09/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2181bcd3e219e7c271776c7c1ec628fdc16f788",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in TF2XLA where non-tuple sharding may be applied to tuple ops\n\nPiperOrigin-RevId: 396403490\nChange-Id: Ib5654a4a57236e433014ebc19fc2fb0952fd1522",
    "Number of deleted lines": 6,
    "Deleted lines": "-  auto identity_op = [builder](xla::XlaOp op) {\n-    return xla::GetTupleElement(xla::Tuple(builder, {op}), 0);\n-          xla::XlaScopedShardingAssignment assign_sharding(builder, sharding);\n-          value = identity_op(value);\n-      xla::XlaScopedShardingAssignment assign_sharding(builder, sharding);\n-      handle = identity_op(handle);",
    "Added lines": "+  auto identity_op = [builder](\n+                         xla::XlaOp op,\n+                         const absl::optional<xla::OpSharding>& sharding) {\n+    xla::XlaScopedShardingAssignment assign_sharding(builder, sharding);\n+    return xla::Copy(op);\n+          value = identity_op(value, sharding);\n+      handle = identity_op(handle, sharding);",
    "Label": "clean"
},
{
    "Id": 309,
    "Library": "tensorflow",
    "Date": "2021/09/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/09ba5577d553146bac9d93844d741fdf58f08600",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in TF2XLA where non-tuple sharding may be applied to tuple ops\n\nPiperOrigin-RevId: 395819910\nChange-Id: Ib033a90dd31fa749f158a05e5fc8017c496c2112",
    "Number of deleted lines": 6,
    "Deleted lines": "-    xla::XlaOp tuple;\n-    {\n-      xla::XlaScopedShardingAssignment assign_sharding(builder, absl::nullopt);\n-      tuple = xla::Tuple(builder, {op});\n-    }\n-    return xla::GetTupleElement(tuple, 0);",
    "Added lines": "+    return xla::GetTupleElement(xla::Tuple(builder, {op}), 0);",
    "Label": "clean"
},
{
    "Id": 310,
    "Library": "tensorflow",
    "Date": "2021/09/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1a21073f082824116f6fb17274fc682201bd72b8",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in TF2XLA where non-tuple sharding may be applied to tuple ops\n\nPiperOrigin-RevId: 395799115\nChange-Id: Id7c0c29233d72123abec7dd894b30ef9afebef23",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return xla::GetTupleElement(xla::Tuple(builder, {op}), 0);",
    "Added lines": "+    xla::XlaOp tuple;\n+    {\n+      xla::XlaScopedShardingAssignment assign_sharding(builder, absl::nullopt);\n+      tuple = xla::Tuple(builder, {op});\n+    }\n+    return xla::GetTupleElement(tuple, 0);",
    "Label": "clean"
},
{
    "Id": 311,
    "Library": "tensorflow",
    "Date": "2021/09/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d2e0e7958f71ac841c2d4e6e2ec0c35276c80537",
    "Root Cause": "N.A",
    "Bug report": "Preserve converter's optimization value.\n\nCurrently quantization debugger can't set optimizations directly, as tf.lite can't be imported due to circular dependency.\n\nPiperOrigin-RevId: 395654613\nChange-Id: Ic01c73b7513da6c9295e2bb9832fdd918516379d",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        old_optimizations = converter.optimizations\n+        converter.optimizations = old_optimizations",
    "Label": "clean"
},
{
    "Id": 312,
    "Library": "tensorflow",
    "Date": "2021/09/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/64387f4e027a4e4980a4203cf78a87c3c402c659",
    "Root Cause": "N.A",
    "Bug report": "Fix buggy original implementation of the sizetracker git version workaround\n\nMy original version of this fix last week was a bit buggy; I forgot that the\ngit_pretty function can be given a range of commits, and that the CL would be\ndifferent for each result. This new fix is much simpler, and does not act\ndifferently for different versions of git, so I'm not going to miss it again.\n\nThe data is still correct, but results with the original fix do not correctly\ninclude a full range of CLs, and they instead repeat the same one many times.\n\nPiperOrigin-RevId: 395577412\nChange-Id: I6e014343761b2f7ee4f72682183ab46d5e2eccfd",
    "Number of deleted lines": 16,
    "Deleted lines": "-PRETTY_CL = \"%(trailers:key={},valueonly)\".format(CL_TRAILER)\n-  t = \"%(trailers:key=PiperOrigin-RevId,valueonly)\"\n-  if t in out:\n-    ret = subprocess.run([\n-        \"git\", \"log\", *n, \"--grep\", CL_TRAILER, commit_range,\n-        \"--pretty=%(trailers)\"\n-    ],\n-                         check=True,\n-                         universal_newlines=True,\n-                         stderr=subprocess.PIPE,\n-                         stdout=subprocess.PIPE)\n-    cl_number = re.search(\"PiperOrigin-RevId: (?P<cl>[0-9]+)\",\n-                          ret.stdout).group(\"cl\")\n-    out = out.replace(t, cl_number)\n-  # Split by \\0 and make list of text, extra whitespace and empty lines removed\n-  return list(filter(None, map(str.strip, out.split(\"\\0\"))))",
    "Added lines": "+# \\001 is a byte with value \"1\", in octal. We use this in git_pretty()\n+PRETTY_CL = \"\\001%(trailers)\\001\"\n+  cleaned = list(filter(None, map(str.strip, out.split(\"\\0\"))))\n+  trailers_removed = []\n+  for row in cleaned:\n+    # Find: a chunk of text surrounded by \\001, and extract the number after\n+    # PiperOrigin-RevId.\n+    row = re.sub(\"\\001.*PiperOrigin-RevId: (?P<cl>[0-9]+).*\\001\", r\"\\g<1>\", row)\n+    trailers_removed.append(row)\n+  return trailers_removed",
    "Label": "clean"
},
{
    "Id": 313,
    "Library": "tensorflow",
    "Date": "2021/08/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/755b71370e27d4710519a29920ce8df08737bad4",
    "Root Cause": "N.A",
    "Bug report": "Fail BiasAdd lowering to hlo on unranked tensors.\n\nCurrently, the lowering would produce garbage or crash in debug mode.\n\nPiperOrigin-RevId: 392674345\nChange-Id: Iff81cdb49dc1448e777a217a0afe86260c2a7971",
    "Number of deleted lines": 2,
    "Deleted lines": "-    auto feature_dim = GetFeatureDimension(\n-        data_format, op.value().getType().cast<RankedTensorType>());",
    "Added lines": "+    auto value_type = op.value().getType().dyn_cast<RankedTensorType>();\n+    if (!value_type) return failure();\n+    auto feature_dim = GetFeatureDimension(data_format, value_type);",
    "Label": "clean"
},
{
    "Id": 314,
    "Library": "tensorflow",
    "Date": "2021/08/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/58905c0a7a6bea06ec450505db97a1849fe8a671",
    "Root Cause": "N.A",
    "Bug report": "Fix metrics import order in quantization debugger.\n\nImport order should default to metrics_portable when metrics_nonportable is not available, not the reverse.\n\nPiperOrigin-RevId: 392626924\nChange-Id: Iac4f8a7f626cb2b74668511064a44505152b1086",
    "Number of deleted lines": 2,
    "Deleted lines": "-  from tensorflow.lite.python import metrics_portable as metrics_stub  # type: ignore\n-except ImportError:",
    "Added lines": "+except ImportError:\n+  from tensorflow.lite.python import metrics_portable as metrics_stub  # type: ignore",
    "Label": "clean"
},
{
    "Id": 315,
    "Library": "tensorflow",
    "Date": "2021/08/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8714a150aae0cc62703e5b7070747443296903d3",
    "Root Cause": "N.A",
    "Bug report": "Fix bug to enable function conversion with main graph disable flag.\n\nSetting the `minimum_segment_size` to -1 will disable the main graph conversion, but it currently also disables the function conversions. This change disables only the main graph from conversion, and runs function conversions.\n\nPiperOrigin-RevId: 392517438\nChange-Id: I32eb70f4016bd111391cef72d3fb81d34180b118",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (minimum_segment_size_ == -1 ||",
    "Added lines": "+  // Optimizing the main graph(identified with `item.id == \"tf_graph\"`) with\n+  // `minimim_segment_size == -1` indicates skipping main graph conversion.\n+  if ((minimum_segment_size_ == -1 && item.id == \"tf_graph\") ||\n+    assert(cp.minimum_segment_size > 0);",
    "Label": "clean"
},
{
    "Id": 316,
    "Library": "tensorflow",
    "Date": "2021/08/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0e91ea00780443833bef2d2a55189c6fef939379",
    "Root Cause": "N.A",
    "Bug report": "[MLIR][KernelGen] Remove default values for `GenerateKernelForTfCode` function\n\nThe default values are not used and are likely to introduce bugs when a\nparameter is added, as happened recently.\n\nPiperOrigin-RevId: 388221719\nChange-Id: I3f224b12d3b2aaa228a611459065a482d893a167",
    "Number of deleted lines": 3,
    "Deleted lines": "-    int64_t max_supported_rank, bool embed_memref_prints = false,\n-    bool print_ptx = false, bool print_llvmir = false, bool enable_ftz = false,\n-    bool cpu_codegen = false, bool jit_compile = false);",
    "Added lines": "+    int64_t max_supported_rank, bool embed_memref_prints, bool print_ptx,\n+    bool print_llvmir, bool enable_ftz, bool cpu_codegen, bool jit_compile);",
    "Label": "clean"
},
{
    "Id": 317,
    "Library": "tensorflow",
    "Date": "2021/07/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e07213384d16f3d26eb98934fcf8902a017deeb5",
    "Root Cause": "N.A",
    "Bug report": "Fix an ordering bug.\n\nThe Python `shard_args` returns a list of of size `len(args)` of list for the per-device buffers.\n\nPiperOrigin-RevId: 387337225\nChange-Id: I678037eb2fc1bec29dfe378fca0213d7a2d9eacb",
    "Number of deleted lines": 3,
    "Deleted lines": "-  // Should be `[num_devices x num_args]`.\n-  arg_buffers.reserve(list_of_list_of_buffers.size());\n-  for (int i = 0; i < list_of_list_of_buffers.size(); ++i) {",
    "Added lines": "+#include <algorithm>\n+#include <utility>\n+  // The Python shard_args returns `[num_args, num_devices]`.\n+  // arg_buffers is `[num_args x num_devices]`.\n+  const int num_args = list_of_list_of_buffers.size();\n+  arg_buffers.reserve(num_args);\n+  for (int i = 0; i < num_args; ++i) {\n+  // TODO(jblespiau): `ExecuteShardedOnLocalDevices` performs an inversion of\n+  // the [args, num_devices] axis. When moving shard_args to C++, we can prevent\n+  // this by calling Execute directly.\n+  // A vector of [num_outputs, num_devices].",
    "Label": "clean"
},
{
    "Id": 318,
    "Library": "tensorflow",
    "Date": "2021/07/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/59c07f3010b43304f4c8601594bf98b14cc35597",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug that Split doesn't recognize the last part if it's empty.\n\nPiperOrigin-RevId: 387226609\nChange-Id: Ib8bb639103af799ad78a0d6b21c456a19da6ae82",
    "Number of deleted lines": 4,
    "Deleted lines": "-  std::vector<std::string> results;\n-  if (!util::SplitAndParse(str, delim, &results)) {\n-    results.clear();\n-  return results;",
    "Added lines": "+#include \"absl/strings/str_split.h\"\n+  if (str.empty()) {\n+    return {};\n+  return absl::StrSplit(str, delim);",
    "Label": "clean"
},
{
    "Id": 319,
    "Library": "tensorflow",
    "Date": "2021/07/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c9d6f6f031f51c92949ede7c44b74a43a6f0c2a4",
    "Root Cause": "N.A",
    "Bug report": "Add additional debugging information to fused conv2d bias-activation ops.\n\nNo functional change.\n\nPiperOrigin-RevId: 387000797\nChange-Id: Idc82b595011470e8d886231248ee65018a3ecedf",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+          << \", tensor_ops_enabled=\" << algo_desc.tensor_ops_enabled()\n+            << \", tensor_ops_enabled=\" << algo_desc.tensor_ops_enabled()",
    "Label": "clean"
},
{
    "Id": 320,
    "Library": "tensorflow",
    "Date": "2021/07/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f6ca173b12eaa9c212d1935586e88f04bbff8403",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in `WhileOp` kernel.\n\nPiperOrigin-RevId: 386902445\nChange-Id: I998bc3712871818e7313deb355db5d6bce5c0537",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      done();",
    "Label": "clean"
},
{
    "Id": 321,
    "Library": "tensorflow",
    "Date": "2021/07/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fd639d1ac1191818ff79a7a7698f0234d25d7a39",
    "Root Cause": "N.A",
    "Bug report": "Remove debug logging\n\nPiperOrigin-RevId: 386798155\nChange-Id: I62f0cf4139dc6be31664834216dfa324c42df6ee",
    "Number of deleted lines": 1,
    "Deleted lines": "-  LOG(INFO) << \"[DEBUG] RegisterFingerprints\";",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 322,
    "Library": "tensorflow",
    "Date": "2021/07/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/973d0896504bc573b5c3b12a101b43852205edfb",
    "Root Cause": "N.A",
    "Bug report": "[PJRT] Fix a bug in the TransposePlanCache.\nThe output_tiling was not being copied to the plan cache key, causing incorrect cache hits.\n\nPiperOrigin-RevId: 386544106\nChange-Id: I960cd82a615fcd33dbedceb3a8bf25634b5bd6c9",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  key.output_tiling.resize(output_tiling.tiling.size());\n+  absl::c_copy(output_tiling.tiling, key.output_tiling.begin());",
    "Label": "clean"
},
{
    "Id": 323,
    "Library": "tensorflow",
    "Date": "2021/07/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/caf62f37a9518eeabc777f30e119934fc74dfda9",
    "Root Cause": "N.A",
    "Bug report": "[kernel_gen] Fix bug in JIT kernel building.\n\nThe JIT parameter was not piped through in one place and thus had no effect.\n\nPiperOrigin-RevId: 386495709\nChange-Id: Id3412df77d8d5c4ae88bc7b2c49ac630ddbd871c",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            \"--jit=%s\" % ctx.attr.jit,",
    "Label": "clean"
},
{
    "Id": 324,
    "Library": "tensorflow",
    "Date": "2021/07/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/65627b449f1d4ec19aa7b7ae86154b936049aab7",
    "Root Cause": "N.A",
    "Bug report": "Fix two bugs in int8 fused convs with the frontend API.\n\n - We were using the output pointer as the side-input pointer (copy-paste bug).\n - We need to say that the bias buffer is \"vectorized\", otherwise the int8x4\n   conv is rejected by cudnn.\n\nWith this change we can now enable the new API for fused int8 convolutions.\n\nPiperOrigin-RevId: 386040593\nChange-Id: I3b163eba3d5c387f1d934eb0b79fad2ca0e6b579",
    "Number of deleted lines": 4,
    "Deleted lines": "-    const dnn::BatchDescriptor& bias_descriptor,\n-      const_cast<void*>(conv_input_data.opaque()), output_data.opaque(),\n-      const_cast<void*>(filter_data.opaque()), output_data.opaque(),\n-      const_cast<void*>(biases.opaque())};",
    "Added lines": "+    dnn::BatchDescriptor bias_descriptor,\n+  // For the purposes of the cudnn graph, say that the bias tensor has the same\n+  // layout as the output tensor.  It doesn't actually matter, because bias is a\n+  // 1D array.  But we need to get the correct vectorization, otherwise the\n+  // cudnn graph API rejects this tensor.\n+  bias_descriptor.set_layout(output_descriptor.layout());\n+\n+      conv_input_data.opaque(), output_data.opaque(), filter_data.opaque(),\n+      side_input_data.opaque(), biases.opaque(),\n+  };",
    "Label": "clean"
},
{
    "Id": 325,
    "Library": "tensorflow",
    "Date": "2021/07/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1c11769caff68dfe4172399c338b48367c1558a3",
    "Root Cause": "N.A",
    "Bug report": "[XLA][NFC] Remove leftover debug print\n\nPiperOrigin-RevId: 384953072\nChange-Id: I6b6bc3310556d1ed6f0f28ef1a6cc817032ce923",
    "Number of deleted lines": 2,
    "Deleted lines": "-      std::cerr << CollectiveOpGroupModeToString(group_mode) << \"\\n\";\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 326,
    "Library": "tensorflow",
    "Date": "2021/07/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6f7fd1b82f613959779f678e827abd958c397a05",
    "Root Cause": "N.A",
    "Bug report": "Resolve a custom device placement TODO after the bug was fixed\n\nCustom devices see ops with resource inputs placed on them after cl/351603218.\n\nPiperOrigin-RevId: 384786315\nChange-Id: Ic6931c3049bb6fda068676c20a7875158f61417e",
    "Number of deleted lines": 7,
    "Deleted lines": "-    # TODO(b/169792703): The current device placement logic never overrides an\n-    # explicit placement with a custom device, causing `v.is_initalized()` to\n-    # fail under a non-custom device context if `v` is in a custom device. The\n-    # explicit placement below makes this work, but should not be necessary once\n-    # the logic is updated to handle cases like this.\n-    with ops.device(self.device):\n-      return gen_resource_variable_ops.var_is_initialized_op(self.handle, name)",
    "Added lines": "+    return gen_resource_variable_ops.var_is_initialized_op(self.handle, name)",
    "Label": "clean"
},
{
    "Id": 327,
    "Library": "tensorflow",
    "Date": "2021/07/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9175b89e51e5aa55a81e51ebc1d4be8a83d395a3",
    "Root Cause": "N.A",
    "Bug report": "Don't suppress exceptions from other delegates after a successful\napplication of the Flex delegate.  Previously this was required due\nto b/142678372.  But that bug was fixed back in September last year,\nand now that that bug is fixed, the exception suppression should\nnot be needed anymore, and is undesirable because it is a confusing\ninconsistency with the behavior when the flex delegate isn't enabled\n(e.g. because the model doesn't contain flex ops).\n\nPiperOrigin-RevId: 384705026\nChange-Id: Ie077cec1edac516a768106bc6b5d119111d9747f",
    "Number of deleted lines": 22,
    "Deleted lines": "-    try {\n-      for (Delegate delegate : options.delegates) {\n-        applyDelegate(interpreterHandle, errorHandle, delegate.getNativeHandle());\n-        delegates.add(delegate);\n-      }\n-      if (options.useNNAPI != null && options.useNNAPI.booleanValue()) {\n-        NnApiDelegate optionalNnApiDelegate = new NnApiDelegate();\n-        ownedDelegates.add(optionalNnApiDelegate);\n-        applyDelegate(interpreterHandle, errorHandle, optionalNnApiDelegate.getNativeHandle());\n-      }\n-    } catch (IllegalArgumentException e) {\n-      // Suppress exceptions where a delegate fails to apply after the flex delegate is successfuly\n-      // applied. This can be a common occurrence, as the flex delegate makes the graph dynamic,\n-      // which is typically unsupported by most delegates (e.g., NNAPI, GPU delegates). We should\n-      // still log an error to indicate that the delegate application was a no-op.\n-      // TODO(b/142678372): Fix the flex delegate to not unconditionally mark graphs as dynamic.\n-      boolean shouldSuppressException =\n-          originalGraphHasUnresolvedFlexOp && !hasUnresolvedFlexOp(interpreterHandle);\n-      if (!shouldSuppressException) {\n-        throw e;\n-      }\n-      System.err.println(\"Ignoring failed delegate application: \" + e);",
    "Added lines": "+    for (Delegate delegate : options.delegates) {\n+      applyDelegate(interpreterHandle, errorHandle, delegate.getNativeHandle());\n+      delegates.add(delegate);\n+    }\n+    if (options.useNNAPI != null && options.useNNAPI.booleanValue()) {\n+      NnApiDelegate optionalNnApiDelegate = new NnApiDelegate();\n+      ownedDelegates.add(optionalNnApiDelegate);\n+      applyDelegate(interpreterHandle, errorHandle, optionalNnApiDelegate.getNativeHandle());",
    "Label": "clean"
},
{
    "Id": 328,
    "Library": "tensorflow",
    "Date": "2021/07/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/456af48684a1bf1d549e244340afc639e804051d",
    "Root Cause": "N.A",
    "Bug report": "Fixed a bug in checkReuseCompatibility.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    // Compare the element Types of both shapes.\n+    if (shapedA.getElementType() != shapedB.getElementType())\n+      return false;\n+\n+    // If the shapes have different ranks, we cannot reuse them.\n+    if (shapedA.getRank() != shapedB.getRank())\n+      return false;\n+\n+    // Compare each dimension. If the dimensions are not equal no reuse is\n+    // possible.\n+    for (unsigned idx = 0, e = shapedA.getRank(); idx < e; ++idx) {\n+      if (shapedA.getDimSize(idx) != shapedB.getDimSize(idx))\n+        return false;\n+    }\n+",
    "Label": "clean"
},
{
    "Id": 329,
    "Library": "tensorflow",
    "Date": "2021/07/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e245af12765996de30bf97fe7a40163ebc3e51d4",
    "Root Cause": "N.A",
    "Bug report": "Disable asan for tensorflow/python/debug/examples/v1:examples_v1_debug_mnist_test\n\nPiperOrigin-RevId: 383686575\nChange-Id: I1493ff607b45d1aadbcd026394f1ba64cd42f6ca",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"noasan\",  # TODO(b/193153560)",
    "Label": "clean"
},
{
    "Id": 330,
    "Library": "tensorflow",
    "Date": "2021/07/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9fd013d10567bc7d591daf9118e0cc7ce1ba578b",
    "Root Cause": "N.A",
    "Bug report": "[PJRT:TFRT:CPU] Fix use-after-free bug in TfrtCpuClient::BufferFromHostBuffer.\n\nPiperOrigin-RevId: 383489038\nChange-Id: Ic877d3fa85414a7d16900ed1592e8f17887417d8",
    "Number of deleted lines": 2,
    "Deleted lines": "-    buffers.push_back(std::move(device_buffer));\n-            [dst_data_ptr, data, byte_size, copy_event = std::move(copy_event),",
    "Added lines": "+    buffers.push_back(device_buffer);\n+            [device_buffer = std::move(device_buffer), dst_data_ptr, data,\n+             byte_size, copy_event = std::move(copy_event),",
    "Label": "clean"
},
{
    "Id": 331,
    "Library": "tensorflow",
    "Date": "2021/07/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3fb83ce583d50c365432c7a3cfffc08fb3fedc2c",
    "Root Cause": "N.A",
    "Bug report": "Fix recently-introduced bug where the 'tensorflowlite' target depended\non both libtensorflowlite_jni.so AND libtensorflowlite_jni_stable.so,\nwhich was causing a significant increase in APK size for Android targets\nthat depend on tensorflow/lite/java:tensorflowlite.\n\nPiperOrigin-RevId: 382763859\nChange-Id: I5d16f4829d3e8f8fb7bf0834cd06d1caa40abfae",
    "Number of deleted lines": 5,
    "Deleted lines": "-        \":tensorflowlite_stable\",\n-        \":tensorflowlite_experimental_java_with_stable_native\",\n-    tflite_exports = [\n-        \":tensorflowlite_experimental_java_with_stable_native\",\n-        \":tensorflowlite_stable\",",
    "Added lines": "+    exports = [\n+        \":tensorflowlite_javalib\",\n+    ],\n+    deps = [\n+        \":tensorflowlite_javalib\",",
    "Label": "clean"
},
{
    "Id": 332,
    "Library": "tensorflow",
    "Date": "2021/07/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/35223865b9bcfba69bef8ca4bffdef7f812d0cd3",
    "Root Cause": "N.A",
    "Bug report": "Fix bug of referring to a Thunk's original mlir::Operation* during Thunk execution.\n\nPiperOrigin-RevId: 382546941\nChange-Id: I16fcf6d640fbbd00137b2fcb74b58101fee0a9e2",
    "Number of deleted lines": 18,
    "Deleted lines": "-        bef_file_(std::move(bef_file)),\n-        op_(op) {}\n-  mlir::Operation* op_;\n-// TODO(hanbinyoon): Templatize to handle other collective ops.\n-static Status CreateCclContext(\n-    const Thunk::ExecuteParams& params, mlir::lmhlo::AllReduceOp* src_op,\n-  std::vector<ReplicaGroup> replica_groups =\n-      ConvertReplicaGroups(src_op->replica_groups()).ValueOrDie();\n-  CollectiveOpGroupMode group_mode =\n-      GetCollectiveOpGroupMode(src_op->channel_id().hasValue(),\n-                               src_op->use_global_device_ids())\n-          .ValueOrDie();\n-  TF_ASSIGN_OR_RETURN(\n-      std::vector<GlobalDeviceId> participants,\n-      GetParticipatingDevices(global_device_id, *params.device_assn,\n-                              replica_groups, group_mode));\n-  if (auto all_reduce = mlir::dyn_cast<mlir::lmhlo::AllReduceOp>(op_)) {\n-        CreateCclContext(params, &all_reduce, &request_context_builder));",
    "Added lines": "+#include \"tensorflow/compiler/xla/service/gpu/nccl_collective_thunk.h\"\n+        bef_file_(std::move(bef_file)) {\n+    // TODO(hanbinyoon): Also handle other collective ops.\n+    if (auto all_reduce_op = mlir::dyn_cast<mlir::lmhlo::AllReduceOp>(*op)) {\n+      xccl_config_ = GetNcclCollectiveConfigForMlir(\n+          all_reduce_op, all_reduce_op.use_global_device_ids());\n+    }\n+  }\n+  absl::optional<NcclCollectiveConfig> xccl_config_;\n+static Status CreateXcclContext(\n+    const Thunk::ExecuteParams& params, const NcclCollectiveConfig& xccl_config,\n+  TF_ASSIGN_OR_RETURN(std::vector<GlobalDeviceId> participants,\n+                      GetParticipatingDevices(\n+                          global_device_id, *params.device_assn,\n+                          xccl_config.replica_groups, xccl_config.group_mode));\n+  if (xccl_config_.has_value()) {\n+        CreateXcclContext(params, *xccl_config_, &request_context_builder));",
    "Label": "clean"
},
{
    "Id": 333,
    "Library": "tensorflow",
    "Date": "2021/06/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2c3bdb6eab5f8e3abd4f9c0b8fb18132698b349b",
    "Root Cause": "N.A",
    "Bug report": "Remove leftover debugging dump\n\nFollow up from cl/381992050\n\nPiperOrigin-RevId: 382318052\nChange-Id: I4d85d564e5f24358879fc7f1630f152abc681702",
    "Number of deleted lines": 1,
    "Deleted lines": "-      slices.back().dump();",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 334,
    "Library": "tensorflow",
    "Date": "2021/06/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/897e6d2c0803485af9159b40dfe0fb7a37951688",
    "Root Cause": "N.A",
    "Bug report": "Enable buffer hoisting and buffer deallocation in kernel generator pipeline.\n\nThese were previously disabled due to two LLVM bugs.\n\nPiperOrigin-RevId: 382285426\nChange-Id: I8c39b5277d071538edb0104c9b1d1ce684bccb29",
    "Number of deleted lines": 10,
    "Deleted lines": "-      // Drop all tensor_to_memref that have no more users. Currently this will\n-      // not happen, as tensor_to_memref has a side-effect. See\n-  // This depends on https://bugs.llvm.org/show_bug.cgi?id=49142 being fixed.\n-  // pm.addNestedPass<mlir::FuncOp>(::mlir::createBufferHoistingPass());\n-  // TODO(herhut): Depends on https://bugs.llvm.org/show_bug.cgi?id=48385.\n-  // We also cannot properly free temporaries until\n-  // https://llvm.discourse.group/t/remove-tight-coupling-of-the-bufferdeallocation-pass-to-std-and-linalg-operations/2162\n-  // is resolved.\n-  // pm.addNestedPass<mlir::FuncOp>(::mlir::createBufferDeallocationPass());\n-  // pm.addNestedPass<mlir::FuncOp>(mlir::createCopyRemovalPass());",
    "Added lines": "+      // Drop all buffercast that have no more users. Currently this will\n+      // not happen, as buffercast has a side-effect. See\n+  pm.addNestedPass<mlir::FuncOp>(::mlir::createBufferHoistingPass());\n+  // Free all temporaries,\n+  pm.addNestedPass<mlir::FuncOp>(::mlir::createBufferDeallocationPass());\n+  pm.addPass(mlir::createCanonicalizerPass());\n+",
    "Label": "clean"
},
{
    "Id": 335,
    "Library": "tensorflow",
    "Date": "2021/06/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c5a0cf79f465d97e48c067d028856957641239fb",
    "Root Cause": "N.A",
    "Bug report": "[NFC] More debug cleanup\n\nPiperOrigin-RevId: 382123799\nChange-Id: I6aecce8d9871cd6ccc3eb13854fdb204a458784b",
    "Number of deleted lines": 2,
    "Deleted lines": "-  int c = 0;\n-      c++;",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 336,
    "Library": "tensorflow",
    "Date": "2021/06/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ab06e14e06befed3eb3c83dfe6cc2c7fdeaa2f74",
    "Root Cause": "N.A",
    "Bug report": "[NFC] Remove debug print accidentally left over\n\nPiperOrigin-RevId: 381995070\nChange-Id: Ic3eac75ad152a28df17df7b925f58df3e524e366",
    "Number of deleted lines": 1,
    "Deleted lines": "-  std::cerr << \"Created \" << c << \" ars\\n\";",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 337,
    "Library": "tensorflow",
    "Date": "2021/06/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1ee1a8073e01badcbcf186aebaa7a2428e04606e",
    "Root Cause": "N.A",
    "Bug report": "Disable asan for tfdbg debug_keras.\n\nPreviously the combined examples_v1_test in tfdbg had asan disabled.  This was changed once\nthe test was split up.  The debug_keras part seems to *sometimes* fail asan, so disabling\nagain for now.\n\nPiperOrigin-RevId: 379719192\nChange-Id: I7d7de42d92d478d886537a8146d3143e81134c24",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"noasan\",  # TODO(b/190625515)",
    "Label": "clean"
},
{
    "Id": 338,
    "Library": "tensorflow",
    "Date": "2021/06/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9d3c37e872c3775715dae15664ce3b4fbd3854fa",
    "Root Cause": "N.A",
    "Bug report": "Fix for a bug in the ROCMFftPlan::UpdateScratchAllocator routine.\n\nThis was the cause of some unit-test failures in JAX...see the following github issue for more details\n\nhttps://github.com/ROCmSoftwarePlatform/frameworks-internal/issues/240\n\nmany thanks to @ekuznetsov139 for coming up with this fix.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  scratch_allocator_ = scratch_allocator;",
    "Label": "clean"
},
{
    "Id": 339,
    "Library": "tensorflow",
    "Date": "2021/06/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c405c59212c764485818fad8b34294c0b553e6fb",
    "Root Cause": "N.A",
    "Bug report": "Use -fmerge-all-constants with clang.\n\nThe code base still has memory access bugs that go away when we use\n-fmerge-all-constants.\n\nPiperOrigin-RevId: 379466615\nChange-Id: Ic10ee5df60b916405478a2625ad4325fd21013d2",
    "Number of deleted lines": 1,
    "Deleted lines": "-                            flag_group(flags = [\"-fexperimental-new-pass-manager\"]),",
    "Added lines": "+                            flag_group(flags = [\n+                                \"-fexperimental-new-pass-manager\",\n+                                \"-fmerge-all-constants\",\n+                            ]),",
    "Label": "clean"
},
{
    "Id": 340,
    "Library": "tensorflow",
    "Date": "2021/06/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e60741121151ead703ea659dc7ccce5e60afff00",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug that makes infinite loop when `num_cols` exceeds `2^31-1`.\n\nPiperOrigin-RevId: 379029747\nChange-Id: Ic5cdb4bcfd0e4276d860b73988811c160865376b",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    OP_REQUIRES(\n+        context, num_rows <= std::numeric_limits<int32>::max(),\n+        errors::InvalidArgument(\n+            \"First dimension of flattened input must be <= INT_MAX, got \",\n+            num_rows));\n+    OP_REQUIRES(\n+        context, num_cols <= std::numeric_limits<int32>::max(),\n+        errors::InvalidArgument(\n+            \"Second dimension of flattened input must be <= INT_MAX, got \",\n+            num_cols));",
    "Label": "clean"
},
{
    "Id": 341,
    "Library": "tensorflow",
    "Date": "2021/06/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d3bbd2ffc930220455e12a9269057f0e200147f9",
    "Root Cause": "N.A",
    "Bug report": "Fix build failure with --config=dbg.\n\nBefore, the build would fail with errors such as: \"relocation truncated to fit: R_X86_64_32 against .debug_info'\". The issue was the debug info was too large. I believe the issue was occurring because offsets into the .debug_info section are stored as 32-bit integers, and so that section cannot exceed 4GiB. To fix, debug info is only included for files under tensorflow/, excluding kernels. This brings the size of the .debug_info section down to about 1.4GiB, well under the 4GiB limit.\n\nUnfortunately, TF kernels and TF dependencies do not have debugging info anymore, but I suspect these are rarely debugged. Debugging info for specific kernels/dependencies can still be explicitly included by the user, e.g. by passing the bazel flags: --config=dbg --per_file_copt=+tensorflow/core/kernels/identity_op.*@-g\n\nSee https://github.com/tensorflow/tensorflow/issues/48919 for more context.\n\nPiperOrigin-RevId: 378910826\nChange-Id: I4b94e3d53bb3ca00c30d5c83d2a57e4bd390c5a8",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+# Only include debug info for files under tensorflow/, excluding kernels, to\n+# reduce the size of the debug info in the binary. This is because if the debug\n+# sections in the ELF binary are too large, errors can occur. See\n+# https://github.com/tensorflow/tensorflow/issues/48919.\n+# Users can still include debug info for a specific kernel, e.g. with:\n+#     --config=dbg --per_file_copt=+tensorflow/core/kernels/identity_op.*@-g\n+build:dbg --per_file_copt=+.*,-tensorflow.*@-g0\n+build:dbg --per_file_copt=+tensorflow/core/kernels.*@-g0",
    "Label": "clean"
},
{
    "Id": 342,
    "Library": "tensorflow",
    "Date": "2021/06/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d20eb1de0902100f218707fd2ea9a5812fc9c005",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in ApplyFtrlV2 that flipped the meaning of multiply_linear_by_lr.\n\nPiperOrigin-RevId: 378904908\nChange-Id: I47f372a344222504d998d00b3f6b6a1c89e88808",
    "Number of deleted lines": 2,
    "Deleted lines": "-        functor::ApplyFtrlV2<Device, T>()(\n-        functor::ApplyFtrlV2MultiplyLinearByLr<Device, T>()(",
    "Added lines": "+        functor::ApplyFtrlV2MultiplyLinearByLr<Device, T>()(\n+        functor::ApplyFtrlV2<Device, T>()(",
    "Label": "clean"
},
{
    "Id": 343,
    "Library": "tensorflow",
    "Date": "2021/06/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/61b97bb772c96dea93ad1e3a9016093b34cc6c55",
    "Root Cause": "N.A",
    "Bug report": "Print the IR after each pass in CompileGraphSetup pipeline logging.\n\nThe logging is intended to help with debugging, therefore save the entire module\n(instead of single functions) and disable multi-threading if logging is\nactivated which helps with reproducibility.\n\nPiperOrigin-RevId: 378404376\nChange-Id: If70d58c6f4961cd7bb930515f6c3094f7d482ad2",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (VLOG_IS_ON(2)) {\n+    // Print the whole module after each pass which requires disabling\n+    // multi-threading as well.\n+    module_op.getContext()->disableMultithreading();\n+    pm.enableIRPrinting(std::make_unique<tensorflow::BridgeLoggerConfig>(\n+        /*print_module_scope=*/true));\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 344,
    "Library": "tensorflow",
    "Date": "2021/06/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3e6eb263935b2a23aa4af90751755e44263a57d7",
    "Root Cause": "N.A",
    "Bug report": "Disable buggy pylint that runs in ci_sanity\n\nPlease refer to the TensorFlow style guide for instructions on running Pylint\nwhile we work on an alternative solution to automated pylinting.\n\nYour changes will still be checked for style by our code import process, but\nyou can speed up the import process by running pylint on your code before\ncreating a PR.\n\nPiperOrigin-RevId: 378010513\nChange-Id: I3310e69e7b917197387111cf5f8bb5c0ea0956d7",
    "Number of deleted lines": 2,
    "Deleted lines": "-SANITY_STEPS=(\"do_configure_test\" \"do_pylint\" \"do_buildifier\" \"do_bazel_nobuild\" \"do_bazel_deps_query\" \"do_pip_package_licenses_check\" \"do_lib_package_licenses_check\" \"do_java_package_licenses_check\" \"do_pip_smoke_test\" \"do_check_load_py_test\" \"do_code_link_check\" \"do_check_file_name_test\" \"do_pip_no_cuda_deps_check_ubuntu\" \"do_pip_no_cuda_deps_check_windows\")\n-SANITY_STEPS_DESC=(\"Run ./configure\" \"Python 3 pylint\" \"buildifier check\" \"bazel nobuild\" \"bazel query\" \"pip: license check for external dependencies\" \"C library: license check for external dependencies\" \"Java Native Library: license check for external dependencies\" \"Pip Smoke Test: Checking py_test dependencies exist in pip package\" \"Check load py_test: Check that BUILD files with py_test target properly load py_test\" \"Code Link Check: Check there are no broken links\" \"Check file names for cases\" \"Check Ubuntu gpu pip package does not depend on cuda shared libraries\" \"Check Windows gpu pip package does not depend on cuda shared libraries\")",
    "Added lines": "+SANITY_STEPS=(\"do_configure_test\" \"do_buildifier\" \"do_bazel_nobuild\" \"do_bazel_deps_query\" \"do_pip_package_licenses_check\" \"do_lib_package_licenses_check\" \"do_java_package_licenses_check\" \"do_pip_smoke_test\" \"do_check_load_py_test\" \"do_code_link_check\" \"do_check_file_name_test\" \"do_pip_no_cuda_deps_check_ubuntu\" \"do_pip_no_cuda_deps_check_windows\")\n+SANITY_STEPS_DESC=(\"Run ./configure\" \"buildifier check\" \"bazel nobuild\" \"bazel query\" \"pip: license check for external dependencies\" \"C library: license check for external dependencies\" \"Java Native Library: license check for external dependencies\" \"Pip Smoke Test: Checking py_test dependencies exist in pip package\" \"Check load py_test: Check that BUILD files with py_test target properly load py_test\" \"Code Link Check: Check there are no broken links\" \"Check file names for cases\" \"Check Ubuntu gpu pip package does not depend on cuda shared libraries\" \"Check Windows gpu pip package does not depend on cuda shared libraries\")",
    "Label": "clean"
},
{
    "Id": 345,
    "Library": "tensorflow",
    "Date": "2020/10/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d5e6a3c310ad34893c86f8a4eb5cae9b823da748",
    "Root Cause": "N.A",
    "Bug report": "fix bug",
    "Number of deleted lines": 1,
    "Deleted lines": "-__global__ __launch_bounds__(1024) ResourceSparseApplyAdadeltaKernel(",
    "Added lines": "+__global__ __launch_bounds__(1024) void ResourceSparseApplyAdadeltaKernel(",
    "Label": "clean"
},
{
    "Id": 346,
    "Library": "tensorflow",
    "Date": "2021/06/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/44265f6b49d4ab3008e667c59ce23f5a7f2f0a51",
    "Root Cause": "N.A",
    "Bug report": "Lowercase argument support without bugs.",
    "Number of deleted lines": 3,
    "Deleted lines": "-        precision_mode == TrtPrecisionMode.INT8 and use_calibration)\n-        conversion_params.precision_mode == TrtPrecisionMode.INT8 and\n-        conversion_params.use_calibration)",
    "Added lines": "+        ((precision_mode == TrtPrecisionMode.INT8) or \n+         (precision_mode == TrtPrecisionMode.INT8.lower())) \n+        and use_calibration)\n+        ((conversion_params.precision_mode == TrtPrecisionMode.INT8) or\n+         (conversion_params.precision_mode == TrtPrecisionMode.INT8.lower()))\n+        and conversion_params.use_calibration)",
    "Label": "clean"
},
{
    "Id": 347,
    "Library": "tensorflow",
    "Date": "2021/05/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ed81d0ef4592e19c36f4b593ea88fc86bbf13263",
    "Root Cause": "N.A",
    "Bug report": "Fix a couple of space-to-batch propagation bugs\n\nPiperOrigin-RevId: 376766341\nChange-Id: Iad1b311eb99d114e96a360c72d032653f5b1671c",
    "Number of deleted lines": 10,
    "Deleted lines": "-  CHECK_LE(std::abs(halo_size - low_padding), spatial_split_size);\n-      if (window.dimensions(old_space_dim).size() != 1) {\n-        if (new_operand->shape().dimensions(new_space_dim) %\n-                window.dimensions(old_space_dim).stride() !=\n-            0) {\n-          return false;\n-        }\n-  while (spatial_split_size * num_splits + c.halo_size - c.spatial_size < 0) {\n-  // is passed on to the new convolutions. Halo does not have to account for it.\n-      std::max(kernel_spatial_dim_size - stride - (base_dilation_factor - 1),",
    "Added lines": "+  CHECK_LE(std::abs(halo_size - low_padding), spatial_split_size);\n+\n+      if (new_operand->shape().dimensions(new_space_dim) %\n+              window.dimensions(old_space_dim).stride() !=\n+          0) {\n+        return false;\n+\n+  // If the spatial size is less than the halo size required, we need to\n+  // increase the spatial size.\n+  while (spatial_split_size * num_splits + c.halo_size - c.spatial_size < 0 ||\n+         spatial_split_size < c.halo_size) {\n+\n+  // passed on to the new convolutions. Halo does not have to account for it.\n+      std::max(kernel_spatial_dim_size - 1 - (base_dilation_factor - 1),",
    "Label": "clean"
},
{
    "Id": 348,
    "Library": "tensorflow",
    "Date": "2021/06/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6e523fa723f0462343d3fb55b3ac24750d3b323c",
    "Root Cause": "N.A",
    "Bug report": "fix keras v1 bug",
    "Number of deleted lines": 1,
    "Deleted lines": "-            outputs = self(inputs, training=False)",
    "Added lines": "+            if (base_layer_utils.is_subclassed(self) and\n+                      not base_layer_utils.from_saved_model(self)):\n+              call_fn = autograph.tf_convert(self.call,\n+                                             ag_ctx.control_status_ctx())\n+            else:\n+              call_fn = self.call\n+            if self._expects_training_arg:\n+              outputs = call_fn(inputs, training=False)\n+            else:\n+              outputs = call_fn(inputs)",
    "Label": "clean"
},
{
    "Id": 349,
    "Library": "tensorflow",
    "Date": "2021/05/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9b510a99704f1e4fcbb82d38bf294e0f8dc48d7d",
    "Root Cause": "N.A",
    "Bug report": "Fix typo on line 3918, \"tf.data.experimental.enable_debug_mode()\"",
    "Number of deleted lines": 1,
    "Deleted lines": "-              \"`tf.data.experimental.enable.debug_mode()`.\")",
    "Added lines": "+              \"`tf.data.experimental.enable_debug_mode()`.\")",
    "Label": "clean"
},
{
    "Id": 350,
    "Library": "tensorflow",
    "Date": "2021/02/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6d6a1d272c73977dd2a1824adf6aff9644190779",
    "Root Cause": "N.A",
    "Bug report": "This bug was introduced in 15275d3 - because TF libs got loaded the\nsecond time somewhere in its life cycle (probably dynamically when there\nare multiple saved_model models in the same inference environment,\nplease see https://github.com/tensorflow/tensorflow/issues/44428 for\nexamples and fixes), RTTI changes for every class for every new dynamic\nlibrary loading, thus TensorList got different RTTI ID and decoder in\nvariant->get<VariantTensorDataProto>() refuses to decode protobuf.\n\nYou have fixed this for MacOS by reverting to the old behaviour (and for\nWindows in master branch), now my patch forces this for everyone else.",
    "Number of deleted lines": 9,
    "Deleted lines": "-\n-#if defined(MACOS) || defined(TARGET_OS_MAC) || defined(PLATFORM_WINDOWS)\n-#endif  // defined(MACOS) || defined(TARGET_OS_MAC) || defined(PLATFORM_WINDOWS)\n-#if defined(MACOS) || defined(TARGET_OS_MAC) || defined(PLATFORM_WINDOWS)\n-#else\n-    // Use the real type name if we have RTTI.\n-    return TypeIndex(static_cast<uint64>(reinterpret_cast<intptr_t>(hash_bit)),\n-                     typeid(T).name());\n-#endif  // defined(MACOS) || defined(TARGET_OS_MAC) || defined(PLATFORM_WINDOWS)",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 351,
    "Library": "tensorflow",
    "Date": "2021/05/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/863af33dd172043502fcb9e4f63e09449a9b29f3",
    "Root Cause": "N.A",
    "Bug report": "Update verbosity of tensor tracer debug message.\n\nPiperOrigin-RevId: 375128382\nChange-Id: I2fbf0d518d3b0312200966ad00b3f31fb7ff6d12",
    "Number of deleted lines": 2,
    "Deleted lines": "-      logging.info('Tensor Tracer is enabled with flags %s.' %\n-                   self._env.get(FLAGS_ENV_VAR))",
    "Added lines": "+      logging.debug('Tensor Tracer is enabled with flags %s.',\n+                    self._env.get(FLAGS_ENV_VAR))",
    "Label": "clean"
},
{
    "Id": 352,
    "Library": "tensorflow",
    "Date": "2021/05/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/adefa1cb7bafb9fa481e2ebb4d80e30a4cfcfa6e",
    "Root Cause": "N.A",
    "Bug report": "Print both assigned and requested device in `Node::DebugString()`\n\nAt the moment the requested device is not present in the debug output\n\nPiperOrigin-RevId: 374304054\nChange-Id: Ia435036b6b0e4242b31133c188545d451cb56673",
    "Number of deleted lines": 3,
    "Deleted lines": "-    strings::StrAppend(&ret, \" op device:\");\n-    strings::StrAppend(&ret, \"{\", assigned_device_name(), \"}\");\n-    strings::StrAppend(&ret, \" def:{\", SummarizeNode(*this), \"}}\");",
    "Added lines": "+    strings::StrAppend(&ret, \" op device:\", \"{requested: '\", requested_device(),\n+                       \"', assigned: '\", assigned_device_name(), \"'}\", \" def:{\",\n+                       SummarizeNode(*this), \"}}\");",
    "Label": "clean"
},
{
    "Id": 353,
    "Library": "tensorflow",
    "Date": "2021/05/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/50034ad2d55b10eb9d4593374546710b12f134e1",
    "Root Cause": "N.A",
    "Bug report": "Disabling test for windows. Recently added changes are breaking the test. Details in bug.\n\nPiperOrigin-RevId: 373239513\nChange-Id: Id40d069cbd8968ac9c9f88b3a5bcb80614f259b9",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    tags = [\n+        \"no_windows\",  # TODO(b/187863739): Fix failure and remove.\n+    ],",
    "Label": "clean"
},
{
    "Id": 354,
    "Library": "tensorflow",
    "Date": "2021/05/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6c42e4ce29b4ae46382931bb01709d342a34d220",
    "Root Cause": "N.A",
    "Bug report": "Fix bug with `deprecated_arg_value` when the value is None.\n\nPiperOrigin-RevId: 372156308\nChange-Id: I3fe64722276c1a20c69bc76a0f7420a856718529",
    "Number of deleted lines": 1,
    "Deleted lines": "-  f = tf_inspect.currentframe().f_back.f_back",
    "Added lines": "+import inspect\n+  f = inspect.currentframe().f_back.f_back",
    "Label": "clean"
},
{
    "Id": 355,
    "Library": "tensorflow",
    "Date": "2021/05/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ce901649beec92845480b6bcdf6851baeff83daa",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Making LsbMaskU32 support any integer type and fix a bug.\n\nLsbMaskU32 was limited to uint32 type, also there was a bug where\nif a mask of 32-bits (all bits set) was required it would have caused\nundefined behavior.\n\nRenamed from LsbMaskU32 to LsbMask.\n\nPiperOrigin-RevId: 371831407\nChange-Id: I67c75de4d09e3a1b8ef670e0cf65b4cbbe5a7d33",
    "Number of deleted lines": 4,
    "Deleted lines": "-// Returns a mask with \"bits\" number of least significant bits set.\n-inline uint32 LsbMaskU32(int bits) {\n-  CHECK_GE(bits, 0);\n-  return (1U << bits) - 1;",
    "Added lines": "+// Returns a mask with \"width\" number of least significant bits set.\n+template <typename T>\n+inline T LsbMask(int width) {\n+  static_assert(std::is_unsigned<T>::value,\n+                \"T should be an unsigned integer type\");\n+  CHECK_GE(width, 0) << \"Unsupported width \" << width;\n+  CHECK_LE(width, std::numeric_limits<T>::digits)\n+      << \"Unsupported width \" << width;\n+  return static_cast<T>(-1) >> (std::numeric_limits<T>::digits - width);",
    "Label": "clean"
},
{
    "Id": 356,
    "Library": "tensorflow",
    "Date": "2021/04/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b1a3085fda2b731c4e991bfe13c76225be5db9c8",
    "Root Cause": "N.A",
    "Bug report": "Enable warning diagnostics for all log levels.\n\nThis fixes a bug in the previous code where the warning diagnostics is issued\nwhen vlog is disabled but suppressed when vlog is disabled.\n\nThe ideal behavior is to only issue warning diagnostics when vlog is enabled,\nbut we don't have a way to enable vlog in the OSS code that is consistent with\nthe google internal code, causing some test cases to fail.\n\nPiperOrigin-RevId: 371398589\nChange-Id: If725d21bd626743e045c24e011deb9f182a1fad5",
    "Number of deleted lines": 4,
    "Deleted lines": "-  // Non-error diagnostic are ignored when VLOG isn't enabled.\n-  if (diag->getSeverity() != DiagnosticSeverity::Error && VLOG_IS_ON(1))\n-    return success();\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 357,
    "Library": "tensorflow",
    "Date": "2021/04/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3e47a822d95b7847fd7cf84e9ef22bd85f8cf9c6",
    "Root Cause": "N.A",
    "Bug report": "Add --xla_gpu_cuda_data_dir to the set of XLA debug options that can be set from Python.\n\nSort flags alphabetically.\n\nPiperOrigin-RevId: 370738784\nChange-Id: I89e0fd1b2056b193525cca9cc8b2092bae80608c",
    "Number of deleted lines": 3,
    "Deleted lines": "-      .def_property(\"xla_cpu_enable_xprof_traceme\",\n-                    &DebugOptions::xla_cpu_enable_xprof_traceme,\n-                    &DebugOptions::set_xla_cpu_enable_xprof_traceme)",
    "Added lines": "+      .def_property(\"xla_cpu_enable_xprof_traceme\",\n+                    &DebugOptions::xla_cpu_enable_xprof_traceme,\n+                    &DebugOptions::set_xla_cpu_enable_xprof_traceme)\n+      .def_property(\"xla_gpu_cuda_data_dir\",\n+                    &DebugOptions::xla_gpu_cuda_data_dir,\n+                    [](DebugOptions* self, std::string value) {\n+                      self->set_xla_gpu_cuda_data_dir(value);\n+                    })",
    "Label": "clean"
},
{
    "Id": 358,
    "Library": "tensorflow",
    "Date": "2021/04/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2ef26e8d4fe86ed7d4463a421756b43a0841ff76",
    "Root Cause": "N.A",
    "Bug report": "Fix a minor bug where the wrong value was being logged.\n\nPiperOrigin-RevId: 370553443\nChange-Id: Id8dfc6749d895b53b05a6653009135f2aa921b92",
    "Number of deleted lines": 1,
    "Deleted lines": "-            \"%d, group_size = %d, implementation = %s\", len(dense_values),",
    "Added lines": "+            \"%d, group_size = %d, implementation = %s\", len(sparse_values),",
    "Label": "clean"
},
{
    "Id": 359,
    "Library": "tensorflow",
    "Date": "2021/04/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/03225c65dea1ef468c6b20ee649c7fbafb2a93dc",
    "Root Cause": "N.A",
    "Bug report": "Minor bug fix: When constructing a StructSpec from a Struct, do not include private cached variables.\n\n(This wasn't really hurting anything, but there's no need for the Spec to have copies of these variables.)\n\nPiperOrigin-RevId: 370530434\nChange-Id: I867ae0e5add2d15a9b7b6fdbc0f6dfd983328ad3",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    spec_fields.pop('_tf_struct_cached_type_spec', None)\n+    spec_fields.pop('_tf_struct_cached_fields', None)",
    "Label": "clean"
},
{
    "Id": 360,
    "Library": "tensorflow",
    "Date": "2021/04/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9cc98f9e73a8a4aeb7a1ed9fc5b60487ec4670d6",
    "Root Cause": "N.A",
    "Bug report": "[tf:tfrt:jit] Reorder tf_to_corert passes to support compilaton of operations with custom fused kernels.\n\n+ fix alignment bug in the tf_cpurt_passes\n\nPiperOrigin-RevId: 370138309\nChange-Id: Ie4b38c38374ecc64d79b43d785dfbc707322df51",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+    // We do support fusion only if the contraction operation is inside one of\n+    // the expected operations with regions. Other operations can have semantics\n+    // that is not compatible with fusion (e.g. region compilation).\n+    if (!isa<FuncOp, IfOp, WhileOp>(contraction->getParentOp())) {\n+      return rewriter.notifyMatchFailure(\n+          contraction,\n+          \"fused operation must be nested inside a function, If or While\");\n+    }\n+",
    "Label": "clean"
},
{
    "Id": 361,
    "Library": "tensorflow",
    "Date": "2021/04/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/80dc954cfc37d1dac18c4121fbcaef7fbe481291",
    "Root Cause": "N.A",
    "Bug report": "style(log) : pretty up a debug log string\n\nHere is a example, it changes from\n\"Available kernels for Castare   device='CPU'\"\nto\n\"Available kernels for Cast are  device='CPU'\"",
    "Number of deleted lines": 1,
    "Deleted lines": "-      DVLOG(4) << \"Available kernels for \" << op->Name() << \"are \"",
    "Added lines": "+      DVLOG(4) << \"Available kernels for \" << op->Name() << \" are\"",
    "Label": "clean"
},
{
    "Id": 362,
    "Library": "tensorflow",
    "Date": "2021/04/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1615440b17b364b875eb06f43d087381f1460a65",
    "Root Cause": "N.A",
    "Bug report": "Forward all `kwargs` from `InterpreterWithCustomOps` to `Interpreter`.\n\nThis allows users to set all parameters available in `Interpreter`, e.g. `experimental_preserve_all_tensors=True` for debugging.\n\nPiperOrigin-RevId: 369955785\nChange-Id: I72a44bd4f595f84ffe9c75ca9ab68d8c6e59bcee",
    "Number of deleted lines": 14,
    "Deleted lines": "-  def __init__(self,\n-               model_path=None,\n-               model_content=None,\n-               experimental_delegates=None,\n-               custom_op_registerers=None):\n-      model_path: Path to TF-Lite Flatbuffer file.\n-      model_content: Content of model.\n-      experimental_delegates: Experimental. Subject to change. List of\n-        [TfLiteDelegate](https://www.tensorflow.org/lite/performance/delegates)\n-          objects returned by lite.load_delegate().\n-    super(InterpreterWithCustomOps, self).__init__(\n-        model_path=model_path,\n-        model_content=model_content,\n-        experimental_delegates=experimental_delegates)",
    "Added lines": "+  def __init__(self, custom_op_registerers=None, **kwargs):\n+      **kwargs: Additional arguments passed to Interpreter.\n+    super(InterpreterWithCustomOps, self).__init__(**kwargs)",
    "Label": "clean"
},
{
    "Id": 363,
    "Library": "tensorflow",
    "Date": "2021/04/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e6e654de881adcadc1aac0e1da98ee959840cb4c",
    "Root Cause": "N.A",
    "Bug report": "Work around an Android 4.4 (KitKat) dlsym bug.\n\nThe underlying bug is already fixed in the platform long ago\n<https://android-review.googlesource.com/c/platform/bionic/+/69033>\nbut since there are still many existing Android 4.4 systems that don't\nhave the fix, it's worth working around.\n\nPiperOrigin-RevId: 369943443\nChange-Id: I2c78ebc742acd84e16ea52deea92ab4c9d71f7d2",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // TF_AcquireFlexDelegate isn't defined on Android, and the following block of\n+  // code would have no effect if TF_AcquireFlexDelegate isn't defined, so we\n+  // only enable that block for non-Android platforms.  Also, on Android 4.4\n+  // (Kitkat), the dlsym() implementation has a bug where dlsym() of an unknown\n+  // name will result in a SIGFPE, which would crash the process, so it's\n+  // important that on Android 4.4 we *don't* call SharedLibrary::GetSymbol\n+  // unless the symbol is sure to exist.\n+#if !defined(__ANDROID__)\n+#endif",
    "Label": "clean"
},
{
    "Id": 364,
    "Library": "tensorflow",
    "Date": "2021/04/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cadeb9fa1ae87845c5692af3f301f38ee9d119e1",
    "Root Cause": "N.A",
    "Bug report": "Ensure TODO has an appropriate bug number\n\nPiperOrigin-RevId: 369876276\nChange-Id: I747782b8f5e86a05b4358e5dcdc3c4f5490c6329",
    "Number of deleted lines": 1,
    "Deleted lines": "-  // TODO(ahentz): we assume this is a new tensor and allocate a new buffer",
    "Added lines": "+  // TODO(b/152916533): We assume this is a new tensor and allocate a new buffer",
    "Label": "clean"
},
{
    "Id": 365,
    "Library": "tensorflow",
    "Date": "2021/04/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/45ff8a2fb72172aaa9d704cbd32fd18c363101e1",
    "Root Cause": "N.A",
    "Bug report": "Replace TODO(ldap) with TODO(bug number)\n\nPiperOrigin-RevId: 369534413\nChange-Id: Ifb21df1f5c71e94bc4433b649e1222b1e27840f1",
    "Number of deleted lines": 1,
    "Deleted lines": "-// TODO(ycling): Support non-zero default values.",
    "Added lines": "+// TODO(b/115961645): Support non-zero default values.",
    "Label": "clean"
},
{
    "Id": 366,
    "Library": "tensorflow",
    "Date": "2021/04/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f5ead6f8e4de470fcf140360b304a4d788198090",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in SyncBatchNormalization, when reduction_axes[i] != i.\n\nPiperOrigin-RevId: 368429682\nChange-Id: Idf335961fc1674b36739eed097f9aaa9b49bb00f",
    "Number of deleted lines": 2,
    "Deleted lines": "-        batch_size = math_ops.cast(array_ops.shape_v2(y)[0], dtypes.float32)\n-        axes_vals = [(array_ops.shape_v2(y))[i] for i in range(1, len(axes))]",
    "Added lines": "+        batch_size = math_ops.cast(array_ops.shape_v2(y)[axes[0]],\n+                                   dtypes.float32)\n+        axes_vals = [(array_ops.shape_v2(y))[axes[i]]\n+                     for i in range(1, len(axes))]",
    "Label": "clean"
},
{
    "Id": 367,
    "Library": "tensorflow",
    "Date": "2021/04/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5b26d09370b9125b84fd298185f8ff3b9eb66588",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Update the documentation for `enable_debug_mode`.\n\nPiperOrigin-RevId: 368318497\nChange-Id: I630d2eba55d541ae645e04fd45e44d777af24230",
    "Number of deleted lines": 2,
    "Deleted lines": "-  Example usage:\n-  ds = ... # input pipeline definition",
    "Added lines": "+  Example usage with pdb module:\n+  import pdb\n+\n+  def func(x):\n+    # Python 3.7 and older requires `pdb.Pdb(nosigint=True).set_trace()`\n+    pdb.set_trace()\n+    x = x + 1\n+    return x\n+\n+  dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n+  dataset = dataset.map(func)\n+\n+  for item in dataset:\n+    print(item)",
    "Label": "clean"
},
{
    "Id": 368,
    "Library": "tensorflow",
    "Date": "2021/04/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/27f95264d54c159be51a28e3c719b7ec1579f988",
    "Root Cause": "N.A",
    "Bug report": "Update tensorflow/python/keras/callbacks.py\n\nSmall bug in CSVLogger. Change the `open` function to `file_io.FileIO` to make it work with files stored on the cloud and not only locally.",
    "Number of deleted lines": 4,
    "Deleted lines": "-        with open(self.filename, 'r' + self.file_flags) as f:\n-    self.csv_file = io.open(self.filename,\n-                            mode + self.file_flags,\n-                            **self._open_args)",
    "Added lines": "+        with file_io.FileIO(self.filename, 'r' + self.file_flags) as f:\n+    self.csv_file = file_io.FileIO(self.filename,\n+                            mode + self.file_flags)",
    "Label": "clean"
},
{
    "Id": 369,
    "Library": "tensorflow",
    "Date": "2021/04/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c5a3f4b0a14dcdea908ac62d5bffc35b5a529465",
    "Root Cause": "N.A",
    "Bug report": "debug exit",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  exit 1",
    "Label": "clean"
},
{
    "Id": 370,
    "Library": "tensorflow",
    "Date": "2021/04/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3fcf9f3cb72001e2e441c02f7180260e34549b9d",
    "Root Cause": "N.A",
    "Bug report": "[XLA:Python] Add xla_detailed_logging_and_dumping debug option to XLA Python bindings.\n\nPiperOrigin-RevId: 367128101\nChange-Id: I9783b86ea889f3e15a3b560a1de15d788cd3f7e1",
    "Number of deleted lines": 3,
    "Deleted lines": "-      .def_property(\"xla_backend_optimization_level\",\n-                    &DebugOptions::xla_backend_optimization_level,\n-                    &DebugOptions::set_xla_backend_optimization_level)",
    "Added lines": "+      .def_property(\"xla_backend_optimization_level\",\n+                    &DebugOptions::xla_backend_optimization_level,\n+                    &DebugOptions::set_xla_backend_optimization_level)\n+      .def_property(\"xla_detailed_logging_and_dumping\",\n+                    &DebugOptions::xla_detailed_logging_and_dumping,\n+                    &DebugOptions::set_xla_detailed_logging_and_dumping)",
    "Label": "clean"
},
{
    "Id": 371,
    "Library": "tensorflow",
    "Date": "2021/04/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0b0b30e8b8bcbb078f00a635d8a72c1fe45ce7ab",
    "Root Cause": "N.A",
    "Bug report": "Add the op name to the log line, for seeded stateful random ops in XLA. This can provide useful debug context.\n\nPiperOrigin-RevId: 366793825\nChange-Id: I8293e3a2a7adbe8de1e5a69a77ef3c7d90705e5b",
    "Number of deleted lines": 5,
    "Deleted lines": "-           \"reproducible behavior is desired.\";\n-           \"will ignore seeds.\";\n-           \"reproducible behavior is desired.\";\n-           \"reproducible behavior is desired.\";\n-           \"reproducible behavior is desired.\";",
    "Added lines": "+           \"reproducible behavior is desired. \"\n+        << name();\n+           \"will ignore seeds. \"\n+        << name();\n+           \"reproducible behavior is desired. \"\n+        << name();\n+           \"reproducible behavior is desired. \"\n+        << name();\n+           \"reproducible behavior is desired. \"\n+        << name();",
    "Label": "clean"
},
{
    "Id": 372,
    "Library": "tensorflow",
    "Date": "2021/04/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7fa7874186238ee7f1d2082f25b523e9c01cd4db",
    "Root Cause": "N.A",
    "Bug report": "Fixes a bug in example code\n\n    The sample implementation of representative_dataset() in the full integer quantization section has a faulty cast to tf.float32.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    yield [data.astype(tf.float32)]",
    "Added lines": "+    yield [tf.dtypes.cast(data, tf.float32)]",
    "Label": "clean"
},
{
    "Id": 373,
    "Library": "tensorflow",
    "Date": "2021/03/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5047cad268c7a6bd3a56f6849d463b67e36274c4",
    "Root Cause": "N.A",
    "Bug report": "MultiProcessRunner: Add logging of TF_CONFIG at the start of subprocess for debugging.\n\nPiperOrigin-RevId: 366182566\nChange-Id: I161b3cc19ec5ddcb6a21d618bbefb36699087aca",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    logging.info('TF_CONFIG: %r', os.environ['TF_CONFIG'])",
    "Label": "clean"
},
{
    "Id": 374,
    "Library": "tensorflow",
    "Date": "2021/03/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f8aee66b8c227da41273a8d3cef558d9dcf431de",
    "Root Cause": "N.A",
    "Bug report": "Fix typo/logic bug in modular plugins.\n\nWe want to enable plugins only if the environment variable exists!\n\nCC @yongtang, @kvignesh1420, @vnvo2409.\n\nPiperOrigin-RevId: 366056903\nChange-Id: Ia6f0918c97cbd6e064e19a02ce3a7d46f48b2799",
    "Number of deleted lines": 1,
    "Deleted lines": "-      if (!(load_plugin == \"true\" || load_plugin == \"1\")) {",
    "Added lines": "+      if (load_plugin == \"true\" || load_plugin == \"1\") {",
    "Label": "clean"
},
{
    "Id": 375,
    "Library": "tensorflow",
    "Date": "2021/03/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7c2d94fb7f68af8b84a43ff2d4f13d0f902c2022",
    "Root Cause": "N.A",
    "Bug report": "Revert logic to infer profiler_outdir from SummaryWriter\n\nThis reverts https://github.com/tensorflow/tensorflow/pull/35340 from Jan 2020. As pointed out in https://github.com/tensorflow/tensorboard/issues/4468, this change never worked (it has no tests). The bug is that that stashing the logdir on the SummaryWriter metadata results in an EagerTensor, when we need a Python str.\n\nThis is a revert rather than a fix forward because the current bug is symptomatic of larger issues with this approach. While in pure eager mode we can always evaluate the EagerTensor to get the string value, once tf.function is involved it's all more complicated. For example, if we were to allow building writers inside tf.functions, passing them out of the function would now require also passing the logdir together with the writer's resource tensor, which is a design constraint we'd prefer to avoid.\n\nA better approach would be something avoids duplicating the logdir tracking in Python state alongside the actual underlying writer. This could be either A) more deeply integrating the profiler code with the summary system (so that it writes data via the writer resource itself), or alternatively B) adding a private API to retrieve the logdir of the current default writer (if it's logdir-based).\n\nPiperOrigin-RevId: 365862669\nChange-Id: Ida5d854cde4b0e4805aa77a8c992ac094f74cb60",
    "Number of deleted lines": 20,
    "Deleted lines": "-  def __init__(self,\n-               shared_name,\n-               init_op_fn,\n-               name=None,\n-               v2=False,\n-               metadata=None):\n-    self._metadata = {} if metadata is None else metadata\n-          v2=True,\n-          metadata={\"logdir\": logdir})\n-    profiler_outdir: Output directory for profiler. This is only used when the\n-      profiler was enabled when the trace was started. In that case, if there is\n-      a logdir-based default SummaryWriter, this defaults to the same directory,\n-      but otherwise the argument must be passed.\n-    if profiler_outdir is None \\\n-        and isinstance(_summary_state.writer, ResourceSummaryWriter):\n-      logdir = _summary_state.writer._metadata.get(\"logdir\")  # pylint: disable=protected-access\n-      if logdir is not None:\n-        profiler_outdir = logdir\n-      raise ValueError(\"Must set profiler_outdir or \"\n-                       \"enable summary writer with logdir.\")",
    "Added lines": "+  def  __init__(self, shared_name, init_op_fn, name=None, v2=False):\n+          v2=True)\n+    profiler_outdir: Output directory for profiler. It is required when profiler\n+      is enabled when trace was started. Otherwise, it is ignored.\n+  # TODO(stephanlee): See if we can remove profiler_outdir and infer it from\n+  # the SummaryWriter's logdir.\n+      raise ValueError(\"Required profiler_outdir is not specified\")",
    "Label": "clean"
},
{
    "Id": 376,
    "Library": "tensorflow",
    "Date": "2021/03/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/505032f2b7499f08b45fbd51145548dfed75a8ad",
    "Root Cause": "N.A",
    "Bug report": "Enable XLA debug options flags in replay computation.\n\nPiperOrigin-RevId: 365850224\nChange-Id: Ib6d35b0d211bc663bd7f17fc64861a2f9431bf69",
    "Number of deleted lines": 1,
    "Deleted lines": "-  const std::vector<tensorflow::Flag> flag_list = {",
    "Added lines": "+  std::vector<tensorflow::Flag> flag_list = {\n+  xla::AppendDebugOptionsFlags(&flag_list);",
    "Label": "clean"
},
{
    "Id": 377,
    "Library": "tensorflow",
    "Date": "2021/03/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f541e46212636fd77075d746f25a54037751fdaa",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in RESHAPE's output tensor dimension calculation that can sometimes occur on Intel Celeron N4020 compiled with WAsm SIMD.\n\nPiperOrigin-RevId: 365731903\nChange-Id: I85fc1ceef3dcfefe12e4fda6c7a422d936d4a63d",
    "Number of deleted lines": 4,
    "Deleted lines": "-#include <stdint.h>\n-#include <string.h>\n-  int num_input_elements = NumElements(input);\n-  int num_output_elements = 1;",
    "Added lines": "+#include <cstdint>\n+#include <cstring>\n+  int64_t num_input_elements = NumElements(input);\n+  int64_t num_output_elements = 1;",
    "Label": "clean"
},
{
    "Id": 378,
    "Library": "tensorflow",
    "Date": "2021/03/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/343cb03f21c73cfe84e47ab2974a6faa1ad80972",
    "Root Cause": "N.A",
    "Bug report": "Fix tpu_strategy.py's corner bug which may add list with tuple b/c `DistributedValues.values` actually returns tuple.\n\nPiperOrigin-RevId: 365159809\nChange-Id: Ib118106eaa9ad804683564f0b3df347f0ccb388a",
    "Number of deleted lines": 1,
    "Deleted lines": "-    value_list = value.values",
    "Added lines": "+    value_list = list(value.values)",
    "Label": "clean"
},
{
    "Id": 379,
    "Library": "tensorflow",
    "Date": "2021/03/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7f2db2bbe1928f6b6ade68c45b7079e2f346e639",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in replay computation\n\nPiperOrigin-RevId: 365042972\nChange-Id: I399ff8323695cb50df5ca1d58e8f8c35835f43b2",
    "Number of deleted lines": 7,
    "Deleted lines": "-    const HloSnapshot& module, LocalClient* client) {\n-  for (const ShapeProto& param :\n-       computation.proto().host_program_shape().parameters()) {\n-    argument_layouts.push_back(Shape(param));\n-    argument_layout_ptrs.push_back(&argument_layouts.back());\n-      thread_pool.Schedule([&snapshots, &executables, client, i] {\n-        executables[i] = CompileExecutable(snapshots[i], client);",
    "Added lines": "+    const HloSnapshot& module, LocalClient* client, const Options& opts) {\n+  if (opts.use_fake_data) {\n+    for (const ShapeProto& param :\n+         computation.proto().host_program_shape().parameters()) {\n+      argument_layouts.push_back(Shape(param));\n+      argument_layout_ptrs.push_back(&argument_layouts.back());\n+    }\n+  } else {\n+    for (const auto& proto : module.arguments()) {\n+      if (!proto.has_shape()) {\n+        return InvalidArgument(\"LiteralProto has no shape\");\n+      }\n+      Shape shape(proto.shape());\n+      argument_layouts.push_back(shape);\n+      argument_layout_ptrs.push_back(&argument_layouts.back());\n+    }\n+      thread_pool.Schedule([&snapshots, &executables, client, i, &opts] {\n+        executables[i] = CompileExecutable(snapshots[i], client, opts);",
    "Label": "clean"
},
{
    "Id": 380,
    "Library": "tensorflow",
    "Date": "2021/03/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/19d1ff21a63f5a4a898afa51ded9aac64026f758",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in propagations on base dilated convolutions\n\nPiperOrigin-RevId: 364805070\nChange-Id: Ie2d070d6889748c9c7ff2bdcbec0235322fa63d7",
    "Number of deleted lines": 27,
    "Deleted lines": "-  VLOG(1) << \"spatial size \" << c.spatial_size;\n-\n-\n-    TF_ASSIGN_OR_RETURN(\n-        activations_new,\n-        HaloDuplicateWithSlice(activations_new, c.spatial_dimension_to_split,\n-                               activations_batch_dim, old_batch_size,\n-                               /*low_padding=*/c.base_dilation_factor != 1 &&\n-                                       c.inherent_low_padding != 0\n-                                   ? c.base_dilation_factor - 1\n-                                   : c.inherent_low_padding,\n-                               c.inherent_high_padding,\n-                               slice_size - spatial_split_size,\n-                               old_split_dim_size));\n-    VLOG(3) << \"Decreasing the spatial size while propagating\";\n-\n-    TF_ASSIGN_OR_RETURN(\n-        activations_new,\n-        HaloDuplicateWithSlice(activations_new, c.spatial_dimension_to_split,\n-                               activations_batch_dim, old_batch_size,\n-                               /*low_padding=*/c.base_dilation_factor != 1 &&\n-                                       c.inherent_low_padding != 0\n-                                   ? c.base_dilation_factor - 1\n-                                   : c.inherent_low_padding,\n-                               c.inherent_high_padding,\n-                               slice_size - spatial_split_size,\n-                               old_split_dim_size));",
    "Added lines": "+  VLOG(1) << \"spatial size \" << c.spatial_size << \" halo size \" << c.halo_size\n+          << \" spatial_split_size \" << spatial_split_size;\n+  VLOG(1) << \"Modified spatial_split_size \" << spatial_split_size;\n+    VLOG(3)\n+        << \"Decreasing the spatial size while propagating spatial_split_size \"\n+        << spatial_split_size << \" new_space_size \" << new_space_size;\n+  // For space-to-batch supported base-dilated convolutions, the low padding is\n+  // is passed on to the new convolutions. Halo does not have to account for it.\n+  TF_ASSIGN_OR_RETURN(activations_new,\n+                      HaloDuplicateWithSlice(\n+                          activations_new, c.spatial_dimension_to_split,\n+                          activations_batch_dim, old_batch_size,\n+                          /*low_padding=*/c.base_dilation_factor != 1 &&\n+                                  c.inherent_low_padding != 0\n+                              ? 0\n+                              : c.inherent_low_padding,\n+                          c.inherent_high_padding,\n+                          slice_size - spatial_split_size, old_split_dim_size));\n+",
    "Label": "clean"
},
{
    "Id": 381,
    "Library": "tensorflow",
    "Date": "2021/03/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2a21421f01df1f3cc43f2cff42f62afec24247dd",
    "Root Cause": "N.A",
    "Bug report": "Trying to fix MacOS nightly pip build.\n\nBash versions < 4.0 have a bug that will exit when 'source' fails and -e is set, even in an || true expression. MacOS is still on bash 3.2.\n\nPiperOrigin-RevId: 364503011\nChange-Id: I9213fddfa8d7381dd3ab8937cb44fcdc0aefc289",
    "Number of deleted lines": 2,
    "Deleted lines": "-# Configure python. Obtain the path to python binary as written by ./configure.\n-source tools/python_bin_path.sh || true",
    "Added lines": "+# Obtain the path to python binary as written by ./configure if it was run.\n+if [[ -e tools/python_bin_path.sh ]]; then\n+  source tools/python_bin_path.sh\n+fi",
    "Label": "clean"
},
{
    "Id": 382,
    "Library": "tensorflow",
    "Date": "2021/03/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c1ce3541fcd31141a2372c9ad285e5912781e3d9",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Minor fix for debug mode.\n\nPiperOrigin-RevId: 363807230\nChange-Id: I0262ead723e1add585c8aa91885fd335b61cb3a5",
    "Number of deleted lines": 1,
    "Deleted lines": "-      if DEBUG_MODE or def_function.functions_run_eagerly():",
    "Added lines": "+      if DEBUG_MODE:\n+        if def_function.functions_run_eagerly():\n+          warnings.warn(\n+              \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n+              \"option is set, this option does not apply to tf.data functions. \"\n+              \"To force eager execution of tf.data functions, please use \"\n+              \"`tf.data.experimental.enable.debug_mode()`.\")",
    "Label": "clean"
},
{
    "Id": 383,
    "Library": "tensorflow",
    "Date": "2021/03/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/63e2cf222bc82f2c92ebeb05c0b9843514877dca",
    "Root Cause": "N.A",
    "Bug report": "Fix tpu_strategy.py's bug which may add list with tuple when _packed_variable is not None.\n\nPiperOrigin-RevId: 363346090\nChange-Id: I65bf81cb41b57735db1dda4721a8ada09ca15eab",
    "Number of deleted lines": 1,
    "Deleted lines": "-      value_list = tuple(",
    "Added lines": "+      value_list = list(",
    "Label": "clean"
},
{
    "Id": 384,
    "Library": "tensorflow",
    "Date": "2021/03/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/583814b2d7c63e879d4a80c080f409c163ee793a",
    "Root Cause": "N.A",
    "Bug report": "Add VLOG debug statement for Save and Restore in DatasetBaseIterator\n\nPiperOrigin-RevId: 363096436\nChange-Id: Iebfa0d716571349cc5a8b84e6c5bc67470ed0ec3",
    "Number of deleted lines": 3,
    "Deleted lines": "-  Status Restore(IteratorContext* ctx, IteratorStateReader* reader) {\n-    TF_RETURN_IF_ERROR(input->SaveInternal(ctx, writer));\n-    TF_RETURN_IF_ERROR(input->RestoreInternal(ctx, reader));",
    "Added lines": "+  virtual Status Restore(IteratorContext* ctx, IteratorStateReader* reader) {\n+    TF_RETURN_IF_ERROR(input->Save(ctx, writer));\n+    TF_RETURN_IF_ERROR(input->Restore(ctx, reader));\n+    VLOG(2) << \"Attempting to save checkpoints on iterator (prefix: \"\n+            << prefix() << \") from \" << dataset()->DebugString();\n+  Status Restore(IteratorContext* ctx, IteratorStateReader* reader) final {\n+    VLOG(2) << \"Attempting to restore checkpoints on iterator (prefix: \"\n+            << prefix() << \") from \" << dataset()->DebugString();\n+    return IteratorBase::Restore(ctx, reader);\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 385,
    "Library": "tensorflow",
    "Date": "2021/03/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/75e2ebf490a0d7dea0000aca777be6ff3c126aab",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in BatchFunction for num_batch_threads=0\n\nPiperOrigin-RevId: 361601558\nChange-Id: I77664827643fca3d4c5368d685ca87af7750904e",
    "Number of deleted lines": 9,
    "Deleted lines": "-\n-      // `shared_name_` and `container_` is used to look up an instantiated\n-      // scheduler instance in `ComputeAsync`.\n-      //\n-      // Rewrite `container_` and `shared_name_` to a pre-defined constant so\n-      // that a shared shared pool across all models if adaptive shared batch\n-      // scheduler is used.\n-      container_ = \"__adapative_container\";\n-      shared_name_ = \"__adaptive_global_shared_thread_pool\";",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 386,
    "Library": "tensorflow",
    "Date": "2021/03/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c7776771ec08a590fb74bfb5657df2bdecc4bbc4",
    "Root Cause": "N.A",
    "Bug report": "[PJRT:TPU] Refactor platform initialization loop to avoid Initialized() bug.\n\nTpuPlatform::Initialized() always return true, which was causing the\ncurrent logic to segfault when initialization failed instead of\nreturning the error status.\nPiperOrigin-RevId: 361042987\nChange-Id: I7096e8d6bdb1ee45d576491d8954ff87f77a39ae",
    "Number of deleted lines": 9,
    "Deleted lines": "-  // TODO(b/165870356): TpuPlatform::Initialized() always returns true!\n-  auto status = platform->Initialize({});\n-  while (!platform->Initialized()) {\n-    status = platform->Initialize({});\n-    if (!status.ok()) {\n-      LOG(ERROR) << \"Platform initialization failed: \" << status;\n-      if ((absl::Now() - start) >= init_retry_timeout) {\n-        return status;\n-      }",
    "Added lines": "+  while (true) {\n+    Status status = platform->Initialize({});\n+    if (status.ok()) {\n+      break;\n+    LOG(INFO) << \"TPU platform initialization failed: \" << status;\n+    if ((absl::Now() - start) >= init_retry_timeout) {\n+      return status;\n+    }\n+    absl::SleepFor(absl::Microseconds(10));\n+  CHECK(platform->Initialized());",
    "Label": "clean"
},
{
    "Id": 387,
    "Library": "tensorflow",
    "Date": "2021/03/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d36adfa1640f7b2a865ba13f4f4623e464964d94",
    "Root Cause": "N.A",
    "Bug report": "[libtpu] Fix memory management bug in GetLibTpuInitArguments().\n\nWe save the raw pointers from 'args' into 'arg_ptrs' and return both vectors, so we should return 'std::move(args)' instead of making an implicit copy. I thought it would do this automatically but it empirically doesn't; I guess the std::pair screws up copy elision.\n\nI also 'std::move(arg_ptrs)' for good measure, but I don't think this is necessary for correctness.\n\nPiperOrigin-RevId: 360813200\nChange-Id: Icfdf043883239d33777f2330f981a19b89c426ce",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return {args, arg_ptrs};",
    "Added lines": "+  return {std::move(args), std::move(arg_ptrs)};",
    "Label": "clean"
},
{
    "Id": 388,
    "Library": "tensorflow",
    "Date": "2021/02/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/01c0eee21cf901e603cecd42527d22a65d1ed5be",
    "Root Cause": "N.A",
    "Bug report": "debug",
    "Number of deleted lines": 1,
    "Deleted lines": "-RUN apt install g++-7 -y",
    "Added lines": "+RUN add-apt-repository ppa:ubuntu-toolchain-r/test\n+RUN apt update\n+RUN apt -y install gcc-7 g++-7",
    "Label": "clean"
},
{
    "Id": 389,
    "Library": "tensorflow",
    "Date": "2021/02/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7bd2b4ccdc1c8214e744d6e38d94c0b40fbb3571",
    "Root Cause": "N.A",
    "Bug report": "debug",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+RUN apt install g++-7 -y",
    "Label": "clean"
},
{
    "Id": 390,
    "Library": "tensorflow",
    "Date": "2021/02/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a297cd670d58149763d3eba82b626a4e9d5c55da",
    "Root Cause": "N.A",
    "Bug report": "Disable MLIR bridge in the second phase for non entry functions\n\nCurrently, the MLIR bridge is enabled based on the graph analysis for each function individually so it may happen that a parent function is not compiled with MLIR but function attached to any op like WhileOp in the function is compiled with MLIR. This makes the logic complex and can expose new bugs if we disable MLIR bridge in the first phase.\n\nThe downside of this is that second phase of the bridge will be rolled out slowly and with coarser grained forks. This is what we want in the beginning to limit bridge rollout to the first phase and later this can be tweaked, if required.\n\nPiperOrigin-RevId: 359640757\nChange-Id: I768e6cff5e38dcef781eab3305692dd73d599f94",
    "Number of deleted lines": 3,
    "Deleted lines": "-  MlirBridgeRolloutPolicy policy = GetMlirBridgeRolloutPolicy(\n-      *graph, config_proto,\n-      /*uses_uninitialized_resource_args=*/AnyUninitializedResourceArg(args));",
    "Added lines": "+  MlirBridgeRolloutPolicy policy = MlirBridgeRolloutPolicy::kDisabledByUser;\n+  if (options.is_entry_computation) {\n+    policy = GetMlirBridgeRolloutPolicy(\n+        *graph, config_proto,\n+        /*uses_uninitialized_resource_args=*/AnyUninitializedResourceArg(args));\n+  }",
    "Label": "clean"
},
{
    "Id": 391,
    "Library": "tensorflow",
    "Date": "2021/02/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d54a254ccf8aeca81f3082af4a8dbada57d51368",
    "Root Cause": "N.A",
    "Bug report": "fix Windows debug build\nPDB file format has internal 32-bit limits, which don't allow PDB files to grow beyond 4GB even on x64 builds\nthus use reduce debug symbols set in order not to exceed 4GB limit",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+config_setting(\n+    name = \"msvc_cl_debug\",\n+    values = {\n+        \"compiler\": \"msvc-cl\",\n+        \"compilation_mode\": \"dbg\",\n+    },\n+    visibility = [\"//visibility:public\"],\n+)\n+\n+    }) + select({\n+        \"//tensorflow:msvc_cl_debug\": [\n+            \"/DEBUG:FASTLINK\",\n+        ],\n+        \"//conditions:default\": [],",
    "Label": "clean"
},
{
    "Id": 392,
    "Library": "tensorflow",
    "Date": "2021/02/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/48f7f6530a1685124d24baa2f87e2283e95ae1aa",
    "Root Cause": "N.A",
    "Bug report": "Fix asan bug in space-to-batch transformation.\n\nPiperOrigin-RevId: 358526294\nChange-Id: I1915c2629d51cf761ea46bfb6a7b2d01b7e9c125",
    "Number of deleted lines": 8,
    "Deleted lines": "-\n-          TF_ASSIGN_OR_RETURN(HloInstruction * batch_to_space,\n-                              BatchToSpace(node));\n-          for (int64 i = 0; i < user->operand_count(); ++i) {\n-            if (user->operand(i) == node) {\n-              TF_CHECK_OK(user->ReplaceOperandWith(i, batch_to_space));\n-            }\n-          }",
    "Added lines": "+\n+      HloInstructionSet unsupported_users;\n+          unsupported_users.insert(user);\n+\n+      if (!unsupported_users.empty()) {\n+        TF_ASSIGN_OR_RETURN(HloInstruction * batch_to_space,\n+                            BatchToSpace(node));\n+        for (auto user : unsupported_users) {\n+          for (int64 i = 0; i < user->operand_count(); ++i) {\n+            if (user->operand(i) == node) {\n+              TF_CHECK_OK(user->ReplaceOperandWith(i, batch_to_space));\n+            }\n+          }\n+        }\n+      }",
    "Label": "clean"
},
{
    "Id": 393,
    "Library": "tensorflow",
    "Date": "2021/02/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f6695e73fc8e47b35f37b1153ae0e404c58a074d",
    "Root Cause": "N.A",
    "Bug report": "Update bazel command to `bazel run` instead of `bazel test`.\n\n`bazel test` puts the error logs into a file instead of on the terminal,\nwhereas `bazel run` will show all the logs on the terminal, which is\nnicer when debugging locally.",
    "Number of deleted lines": 3,
    "Deleted lines": "-    CC=clang bazel test ---config=asan tensorflow/lite/micro:micro_interpreter_test\n-    CC=clang bazel test ---config=msan tensorflow/lite/micro:micro_interpreter_test\n-    CC=clang bazel test ---config=ubsan tensorflow/lite/micro:micro_interpreter_test",
    "Added lines": "+    CC=clang bazel run --config=asan tensorflow/lite/micro:micro_interpreter_test\n+    CC=clang bazel run --config=msan tensorflow/lite/micro:micro_interpreter_test\n+    CC=clang bazel run --config=ubsan tensorflow/lite/micro:micro_interpreter_test",
    "Label": "clean"
},
{
    "Id": 394,
    "Library": "tensorflow",
    "Date": "2021/02/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4c01f5d82f0eb3f8f585d87676d759c78a7fa5cf",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Fix the bug when restoring the iterator, the buffered elements are not correctly recorded for ParallelInterleaveDataset.\n\nPiperOrigin-RevId: 356627548\nChange-Id: I8bb0ea293d468453790bfdb996fbf428ba7cf71d",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+          RecordBufferEnqueue(ctx, result->return_values);\n+      for (auto& element : current_elements_) {\n+        DCHECK(element == nullptr);\n+      }\n+      for (auto& element : future_elements_) {\n+        DCHECK(element == nullptr);\n+      }",
    "Label": "clean"
},
{
    "Id": 395,
    "Library": "tensorflow",
    "Date": "2021/02/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/718f7a4a9e4767c0c1b9ed523952b2ad15e895f5",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in space to batch conversion\n\nPiperOrigin-RevId: 355976086\nChange-Id: Ie95cca053bd95640f5a31e6785fa675098294c03",
    "Number of deleted lines": 5,
    "Deleted lines": "-    if (!last_try) {\n-    const int64 rhs_dilation = consumer->window()\n-                                   .dimensions(get_chosen_spatial_dim(consumer))\n-                                   .window_dilation();\n-",
    "Added lines": "+    auto win_dims =\n+        consumer->window().dimensions(get_chosen_spatial_dim(consumer));\n+    const int64 rhs_dilation = win_dims.window_dilation();\n+\n+    // If the rhs_dilation is absent, we want both LHS and RHS to be space-to-\n+    // batched for propagating on backprop convolutions.\n+    if (!last_try || rhs_dilation == 1) {",
    "Label": "clean"
},
{
    "Id": 396,
    "Library": "tensorflow",
    "Date": "2021/02/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ea2477d56c002f144997a40284e0be96bf1a162a",
    "Root Cause": "N.A",
    "Bug report": "Fix quantization debugger's `tf_export`\n\nThis should be usable with TF v2, too.\n\nPiperOrigin-RevId: 355786139\nChange-Id: I0725913ab3b410c23588cc59b5847f351a0eeb5a",
    "Number of deleted lines": 2,
    "Deleted lines": "-@tf_export.tf_export(v1=['lite.experimental.QuantizationDebugOptions'])\n-@tf_export.tf_export(v1=['lite.experimental.QuantizationDebugger'])",
    "Added lines": "+@tf_export.tf_export('lite.experimental.QuantizationDebugOptions')\n+@tf_export.tf_export('lite.experimental.QuantizationDebugger')",
    "Label": "clean"
},
{
    "Id": 397,
    "Library": "tensorflow",
    "Date": "2021/02/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a8b2a3d48c1db0c47a9f2dcc6c46d31717147f2c",
    "Root Cause": "N.A",
    "Bug report": "[XLA/GPU] Remove unintended debug line.\n\nPiperOrigin-RevId: 355716319\nChange-Id: I88e3fd38706e17aab3234b0135f9aedcbaa4e825",
    "Number of deleted lines": 1,
    "Deleted lines": "-  LOG(ERROR) << \"TIM: \";",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 398,
    "Library": "tensorflow",
    "Date": "2021/02/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/868276741aa0a0521fd00953df914b60bbc2661a",
    "Root Cause": "N.A",
    "Bug report": "Fix small bug in space to batch conversion\n\nPiperOrigin-RevId: 355644976\nChange-Id: I8f75c3a6638e218a640e34bc13ad87e32bfad8c2",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+           old_producer->opcode() == HloOpcode::kBroadcast &&\n+                 consumer->mutable_operand(i)->opcode() ==\n+                     HloOpcode::kBroadcast &&",
    "Label": "clean"
},
{
    "Id": 399,
    "Library": "tensorflow",
    "Date": "2021/01/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9304b8af0ea04da71795d5cd83accd5cf66ed2a3",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in tpu_on_demand_compiler.cc\n\nDon't free the result on the failure path.\n\nPiperOrigin-RevId: 353349148\nChange-Id: I26308f70662d3cc56b6350f904195b752c81799e",
    "Number of deleted lines": 3,
    "Deleted lines": "-    XLA_HloModule result;\n-    auto cleanup = xla::MakeCleanup([&hlo_module, &result]() {\n-      stream_executor::tpu::SerializedProto_Free(result.proto);",
    "Added lines": "+    auto cleanup = xla::MakeCleanup([&hlo_module]() {\n+    XLA_HloModule result;\n+    stream_executor::tpu::SerializedProto_Free(result.proto);",
    "Label": "clean"
},
{
    "Id": 400,
    "Library": "tensorflow",
    "Date": "2021/01/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4d5197c63c7c625fed31b66ed03f086feee0b430",
    "Root Cause": "N.A",
    "Bug report": "[BugFix] Fix bug when reduce dimension becomes emptry in reduction_degenerate_dim_remover pass",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (updated_reduced_dimensions.empty()) {\n+      std::unique_ptr<HloInstruction> reshape = HloInstruction::CreateBitcast(\n+          reduce_shape, reduced_op);\n+      return ReplaceWithNewInstruction(instr, std::move(reshape));\n+    }\n+",
    "Label": "clean"
},
{
    "Id": 401,
    "Library": "tensorflow",
    "Date": "2021/01/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b1f3a521990f051554a2cd889419168a80fceed5",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in dense shapes calculated from output size rather than node_def\n\nPiperOrigin-RevId: 351519984\nChange-Id: Ieceef64fefe9c34027bd869d06b43444fec34918",
    "Number of deleted lines": 2,
    "Deleted lines": "-\n-    if (data->dense_size > 0 && data->dense_shapes.empty()) {",
    "Added lines": "+  const bool missing_shape_info = data->dense_shapes.empty();\n+    if (missing_shape_info) {",
    "Label": "clean"
},
{
    "Id": 402,
    "Library": "tensorflow",
    "Date": "2021/01/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/24efa2683fb576af767258115cbc0e58b03cc954",
    "Root Cause": "N.A",
    "Bug report": "Bugfix: TFLite `If` with dynamically allocated tensors.\n\nFixes a bug when branch subgraphs have dynamically allocated\ninputs/outputs.\n\nThe newly added tests are failing without the fix and passing\nwith the fix.\n\nPiperOrigin-RevId: 351181346\nChange-Id: I5bdec6179c03157df9fd012018983384a6a2cb81",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+    if (IsDynamicTensor(subgraph_input)) {\n+      TfLiteTensorRealloc(input->bytes, subgraph_input);\n+    }\n+\n+\n+    if (IsDynamicTensor(output)) {\n+      TfLiteTensorRealloc(subgraph_output->bytes, output);\n+    }\n+",
    "Label": "clean"
},
{
    "Id": 403,
    "Library": "tensorflow",
    "Date": "2021/01/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/95a80c02469e294c5d196337d906b18e288a6c35",
    "Root Cause": "N.A",
    "Bug report": "Adds debugging info to a CHECK in ShapeUtil::ReshapeIsBitcast().\n\nPiperOrigin-RevId: 350695371\nChange-Id: I66df99001ad4fe54a703fdc6d12d1968849d42f7",
    "Number of deleted lines": 1,
    "Deleted lines": "-  CHECK_EQ(ElementsIn(input_shape), ElementsIn(output_shape));",
    "Added lines": "+  CHECK_EQ(ElementsIn(input_shape), ElementsIn(output_shape))\n+      << \"input_shape=\" << input_shape.ShortDebugString()\n+      << \", output_shape=\" << output_shape.ShortDebugString();",
    "Label": "clean"
},
{
    "Id": 404,
    "Library": "tensorflow",
    "Date": "2020/12/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d712b970c99943510165fd238abfd463a2603eff",
    "Root Cause": "N.A",
    "Bug report": "Improve debuggability of XLine/XPlane by allowing name to be printed while building.\n\nPiperOrigin-RevId: 348566881\nChange-Id: Ia5f347bd35ea0f4f8700223288b85e6d09fe2342",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  absl::string_view Name() const { return line_->name(); }\n+  absl::string_view Name() const { return plane_->name(); }",
    "Label": "clean"
},
{
    "Id": 405,
    "Library": "tensorflow",
    "Date": "2020/12/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ca42aa829cc15d947e102ab144bde8834fa6d7da",
    "Root Cause": "N.A",
    "Bug report": "Set xla_gpu_ftz to true in xla DebugOptions.\n\nThis is necessary to be compatible with Tensorflow.\n\nPiperOrigin-RevId: 347829756\nChange-Id: I8a933de74634181d32bc35c70a8053d2c92909eb",
    "Number of deleted lines": 6,
    "Deleted lines": "-    xla::DebugOptions options = xla::GetDebugOptionsFromFlags();\n-    options.set_xla_gpu_ftz(true);\n-    config.set_debug_options(options);\n-    xla::DebugOptions options = xla::GetDebugOptionsFromFlags();\n-    options.set_xla_gpu_ftz(true);\n-    config.set_debug_options(options);",
    "Added lines": "+    config.set_debug_options(xla::GetDebugOptionsFromFlags());\n+    config.set_debug_options(xla::GetDebugOptionsFromFlags());",
    "Label": "clean"
},
{
    "Id": 406,
    "Library": "tensorflow",
    "Date": "2020/12/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2492f10eca1f91864d7238ac646c5848834d6629",
    "Root Cause": "N.A",
    "Bug report": "Set xla_gpu_ftz to true in xla DebugOptions.\n\nThis is necessary to be compatible with Tensorflow.\n\nPiperOrigin-RevId: 347813508\nChange-Id: I815a7d36de99b3fd0d972bf5f9798771df15a329",
    "Number of deleted lines": 2,
    "Deleted lines": "-    config.set_debug_options(xla::GetDebugOptionsFromFlags());\n-    config.set_debug_options(xla::GetDebugOptionsFromFlags());",
    "Added lines": "+    xla::DebugOptions options = xla::GetDebugOptionsFromFlags();\n+    options.set_xla_gpu_ftz(true);\n+    config.set_debug_options(options);\n+    xla::DebugOptions options = xla::GetDebugOptionsFromFlags();\n+    options.set_xla_gpu_ftz(true);\n+    config.set_debug_options(options);",
    "Label": "clean"
},
{
    "Id": 407,
    "Library": "tensorflow",
    "Date": "2020/12/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c6228dd44ac84445e701b2a83c100ecfe9cf5c70",
    "Root Cause": "N.A",
    "Bug report": "Add bug number for TODO\n\nPiperOrigin-RevId: 347642991\nChange-Id: Ic8535c2ebcf80e4ed265d0e99914c4507d915298",
    "Number of deleted lines": 3,
    "Deleted lines": "-  // TODO(soroosh): add kGenericOptimized\n-  // TODO(soroosh) add support for INT64\n-// TODO(soroosh): add optimized",
    "Added lines": "+  // TODO(b/175642009): add kGenericOptimized\n+  // TODO(b/175642009): add support for INT64",
    "Label": "clean"
},
{
    "Id": 408,
    "Library": "tensorflow",
    "Date": "2020/12/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6df0071c32203a1c20a2ae4104b6db0a1976a410",
    "Root Cause": "N.A",
    "Bug report": "[MLIR] Add minor fix for filter dimensions.\nThis commit addresses a bug where incorrect filter dimensions were being read and used. Corrected the bug and the relevant test cases were also checked.\n\nSigned-off-by: Prateek Gupta <prateek@polymagelabs.com>",
    "Number of deleted lines": 3,
    "Deleted lines": "-      filter_ty.getShape()[GetFilterTensorInnerInputChannelsDimIndex(\n-          num_dims, tensorflow::FilterTensorFormat::FORMAT_HWIO)];\n-",
    "Added lines": "+      filter_ty.getShape()[GetFilterTensorOutputChannelsDimIndex(\n+          num_dims, tensorflow::FORMAT_HWIO)];\n+          ",
    "Label": "clean"
},
{
    "Id": 409,
    "Library": "tensorflow",
    "Date": "2020/12/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4b4893e25b954eecef0cac1cebf4d3d21ba0679f",
    "Root Cause": "N.A",
    "Bug report": "Fix API initializer bug: request TPU library initialize itself during loading\n\nPiperOrigin-RevId: 347112603\nChange-Id: I3951680b9c7873e507363dd40da49da9e9764b4c",
    "Number of deleted lines": 1,
    "Deleted lines": "-    (*initialize_fn)(/*init_library=*/false, /*argc=*/0, /*argv=*/nullptr);",
    "Added lines": "+    (*initialize_fn)(/*init_library=*/true, /*argc=*/0, /*argv=*/nullptr);",
    "Label": "clean"
},
{
    "Id": 410,
    "Library": "tensorflow",
    "Date": "2020/12/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bd1fd58c37b53426d0efca83bda64e49817da80d",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Fix a bug in automatic space to batch transformation.\n\nPiperOrigin-RevId: 347019389\nChange-Id: Id58efbbc42fe31f8a4821e7f416f20fa2a339b96",
    "Number of deleted lines": 1,
    "Deleted lines": "-          CHECK_GT(pivot_space_size, new_dimensions[space_dim]);",
    "Added lines": "+    bool is_pivot_producer_modified = false;\n+      is_pivot_producer_modified = true;\n+          CHECK(pivot_space_size > new_dimensions[space_dim] ||\n+                !is_pivot_producer_modified);",
    "Label": "clean"
},
{
    "Id": 411,
    "Library": "tensorflow",
    "Date": "2020/12/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5c61a49f35620fbdb2f849d0d48badec196f0e94",
    "Root Cause": "N.A",
    "Bug report": "Fix registration for Neg kernel.\n\nThere was a copy/paste bug here which was only caught with the python-based\ntests. Now tensorflow/python/kernel_tests:cwise_ops_unary_test_gpu passes with\nunranked kernels enabled.\n\nPiperOrigin-RevId: 346947580\nChange-Id: I5195c6b5f44deb11469d50e925cf460c3944014d",
    "Number of deleted lines": 2,
    "Deleted lines": "-GENERATE_AND_REGISTER_UNARY_KERNEL(Neg, i16, DT_DOUBLE, int16);\n-GENERATE_AND_REGISTER_UNARY_KERNEL(Neg, i64, DT_DOUBLE, int64);",
    "Added lines": "+GENERATE_AND_REGISTER_UNARY_KERNEL(Neg, i16, DT_INT16, int16);\n+GENERATE_AND_REGISTER_UNARY_KERNEL(Neg, i64, DT_INT64, int64);",
    "Label": "clean"
},
{
    "Id": 412,
    "Library": "tensorflow",
    "Date": "2020/12/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/928116bb70f07e9197e87db0d22866bc43ad8ef9",
    "Root Cause": "N.A",
    "Bug report": "Remove debug print",
    "Number of deleted lines": 4,
    "Deleted lines": "-  print(saved_model_path)\n-  print(str(saved_model_path).encode('utf-8'))\n-  print(exported_names)\n-  print(str(exported_names).encode('utf-8'))",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 413,
    "Library": "tensorflow",
    "Date": "2020/12/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2fc826104470db79eeed92f994105f2bd820c508",
    "Root Cause": "N.A",
    "Bug report": "Add bug number for Transpose3D refactor suggestion\n\nPiperOrigin-RevId: 346388751\nChange-Id: I6de757cf96ded9d36ee839d8891f037f8dcba078",
    "Number of deleted lines": 1,
    "Deleted lines": "-// TODO(alanchiao): see if we can reduce the number",
    "Added lines": "+// TODO(b/173718660): see if we can reduce the number",
    "Label": "clean"
},
{
    "Id": 414,
    "Library": "tensorflow",
    "Date": "2020/12/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4425be2b7b3ea85cd538f766802664cb6dfd8983",
    "Root Cause": "N.A",
    "Bug report": "TFLu: Fix bug in PPD op\n\nChange-Id: I58da0d92491d264d6ba90fb1b3a5673c0058047f",
    "Number of deleted lines": 2,
    "Deleted lines": "-      keep_values[counter++] = values[i];\n-      keep_indices[i] = i;",
    "Added lines": "+      keep_values[counter] = values[i];\n+      keep_indices[counter] = i;\n+      counter++;",
    "Label": "clean"
},
{
    "Id": 415,
    "Library": "tensorflow",
    "Date": "2020/12/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3e40e27e454cd8d8d27f757f4942b0bb43da4b03",
    "Root Cause": "N.A",
    "Bug report": "[XLA/GPU] Fix debug mode crash.\n\nPiperOrigin-RevId: 345622289\nChange-Id: I7aa15eabf8df9ed3072f32e87a921c6494e94df2",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (init_type.getElementTypeBitWidth() == 1) {",
    "Added lines": "+    if (init_shape.element_type() == PRED) {\n+      TF_RET_CHECK(init_type.getElementTypeBitWidth() == 1);",
    "Label": "clean"
},
{
    "Id": 416,
    "Library": "tensorflow",
    "Date": "2020/11/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/87b03eeb92f5088b317cc0e503e497f8d415cb76",
    "Root Cause": "N.A",
    "Bug report": "Push and pop gcc diagnostic pragma's in the flexbuffer patching script.\n\nThe change from PR #45040 made null-dereference an error for any code\nthat was compiled after flexbuffers.h was included. This resulted in\nbuild errors that were hard to debug.\n\nSee [this comment](https://github.com/tensorflow/tensorflow/issues/44971#issuecomment-736130061) for additional context.",
    "Number of deleted lines": 2,
    "Deleted lines": "-  echo \"#pragma GCC diagnostic ignored \\\"-Wnull-dereference\\\"\" >> ${temp_flexbuffers_path}\n-  echo \"#pragma GCC diagnostic error \\\"-Wnull-dereference\\\"\" >> ${temp_flexbuffers_path}",
    "Added lines": "+  echo \"#pragma GCC diagnostic push\" >> ${temp_flexbuffers_path}\n+  echo \"#pragma GCC diagnostic ignored \\\"-Wnull-dereference\\\"\" >> ${temp_flexbuffers_path}\n+  echo \"#pragma GCC diagnostic pop\" >> ${temp_flexbuffers_path}",
    "Label": "clean"
},
{
    "Id": 417,
    "Library": "tensorflow",
    "Date": "2020/11/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/06fb72020d3af8c2b27fd4f80993ff3f6b110d36",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in delegating partial subgraphs to XNNPACK\n\nDelegation of partial subgraphs with DENSIFY or FP16 DEQUANTIZE nodes could be\nbroken due to using wrong index (index within the execution plan rather than\na global node index) throughout the delegate.\n\nPiperOrigin-RevId: 344863679\nChange-Id: Ifbdc9395ca8650fb2a8cafad8a5a1a23839c351a",
    "Number of deleted lines": 4,
    "Deleted lines": "-        static_unpack_nodes_.insert(i);\n-        quasi_static_tensors_producers[node->outputs->data[0]] = i;\n-        static_unpack_nodes_.insert(i);\n-        quasi_static_tensors_producers[node->outputs->data[0]] = i;",
    "Added lines": "+        static_unpack_nodes_.insert(node_index);\n+        quasi_static_tensors_producers[node->outputs->data[0]] = node_index;\n+        static_unpack_nodes_.insert(node_index);\n+        quasi_static_tensors_producers[node->outputs->data[0]] = node_index;",
    "Label": "clean"
},
{
    "Id": 418,
    "Library": "tensorflow",
    "Date": "2020/11/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ccb98fd368fd2a0e6054390be74da59b22b41880",
    "Root Cause": "N.A",
    "Bug report": "[MLIR][KernelGen] Generate assertion message in `transform_unranked_hlo` pass\n\nUse constant to generate the correct assertion message. This avoids\nconfusion when lowering the max rank specialization for debugging.\n\nPiperOrigin-RevId: 344769021\nChange-Id: Ie445863f014fc810cda0423d3209d88d5aae4e36",
    "Number of deleted lines": 4,
    "Deleted lines": "-    // specializations from 1-6.\n-    // the ranks was greater than 6).\n-        \"Input for dynamic binary op lowering was of a rank greater than \"\n-        \"6\");",
    "Added lines": "+    // specializations from 1 to `kMaxRankSpecialization`.\n+    // the ranks was greater than `kMaxRankSpecialization`).\n+        \"Input for dynamic binary op lowering was of a rank greater than \" +\n+            std::to_string(kMaxRankSpecialization));",
    "Label": "clean"
},
{
    "Id": 419,
    "Library": "tensorflow",
    "Date": "2020/11/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/175187b78f6032390d9dc7f91788788777415492",
    "Root Cause": "N.A",
    "Bug report": "Add bug for TODO in svdf about unrolling loop optimization.\n\nPiperOrigin-RevId: 344377656\nChange-Id: I2a06439b6bd9aa663ae5aa09766bffc7bed03b26",
    "Number of deleted lines": 2,
    "Deleted lines": "-  // TODO(alanchiao): can optimize hybrid case ~5% by unrolling loop in applying\n-  // time weights so that the inner loop multiplies eight elements at a time.",
    "Added lines": "+  // TODO(b/174275776): can optimize hybrid case ~5% by unrolling loop in\n+  // applying time weights so that the inner loop multiplies eight elements at\n+  // a time.",
    "Label": "clean"
},
{
    "Id": 420,
    "Library": "tensorflow",
    "Date": "2020/11/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5d5e0037ec66a6fe0923143e77ef16ec0a3a1351",
    "Root Cause": "N.A",
    "Bug report": "Add bug for TODO in concatenation.\nFirst TODO is removed due to duplicated code in optimized is already removed before.\n\nPiperOrigin-RevId: 344350119\nChange-Id: I8ea87cae8a3c605fdb7ab733b52cd97c090834c2",
    "Number of deleted lines": 2,
    "Deleted lines": "-// TODO(prabhumk): This is the same as the optimized implementation.\n-// TODO(prabhumk): The quantized implementation of concatentation isn't fully",
    "Added lines": "+// TODO(b/174275780): The quantized implementation of concatentation isn't fully",
    "Label": "clean"
},
{
    "Id": 421,
    "Library": "tensorflow",
    "Date": "2020/11/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4f8460e92f3e0867f18f4970077577907ac84456",
    "Root Cause": "N.A",
    "Bug report": "Add bug for TODO in slice function about larger than 4D support.\n\nPiperOrigin-RevId: 344350048\nChange-Id: I987db6159fa0e0ac5f71954749cf619691413e19",
    "Number of deleted lines": 1,
    "Deleted lines": "-  // TODO(dkalenichenko): This op only supports 4D tensors or smaller.",
    "Added lines": "+  // TODO(b/174275841): This op only supports 4D tensors or smaller.",
    "Label": "clean"
},
{
    "Id": 422,
    "Library": "tensorflow",
    "Date": "2020/11/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ace0c15a22f7f054abcc1f53eabbcb0a1239a9e2",
    "Root Cause": "N.A",
    "Bug report": "Default initialize fixed point Eigen types.\n\nIn certain cases, tensors are filled with default values of the type. But, for these fixed point types, these values were uninitialized. Thus, we would have uninitialized memory access bugs, some of which were caught by MSAN.\n\nPiperOrigin-RevId: 344101137\nChange-Id: I14555fda74dca3b5f1582da9008901937e3f14e2",
    "Number of deleted lines": 5,
    "Deleted lines": "-  QInt8() {}\n-  QUInt8() {}\n-  QInt16() {}\n-  QUInt16() {}\n-  QInt32() {}",
    "Added lines": "+  QInt8() : value(0) {}\n+  QUInt8() : value(0) {}\n+  QInt16() : value(0) {}\n+  QUInt16() : value(0) {}\n+  QInt32() : value(0) {}",
    "Label": "clean"
},
{
    "Id": 423,
    "Library": "tensorflow",
    "Date": "2020/11/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/daa73375550249b0069145f6ea27df0e9603a5e4",
    "Root Cause": "N.A",
    "Bug report": "Add bug for TODO on Quantized PortableApplyLayerNorm reference ops.\n\nPiperOrigin-RevId: 343905283\nChange-Id: I8e34db09c23bb97b29453f0aa63e77f3d73f01d3",
    "Number of deleted lines": 1,
    "Deleted lines": "-    // TODO(jianlijianli): Avoids overflow but only works for POT n_input.",
    "Added lines": "+    // TODO(b/173994730): Avoids overflow but only works for POT n_input.",
    "Label": "clean"
},
{
    "Id": 424,
    "Library": "tensorflow",
    "Date": "2020/11/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6e2de42a1d149e1d6902e29e6f88c734c77c9b9d",
    "Root Cause": "N.A",
    "Bug report": "Add bug for TODO on legacy toco flag\n\nPiperOrigin-RevId: 343362472\nChange-Id: Iaaaeed63399f538546461c2152ababe79cc6a5be",
    "Number of deleted lines": 3,
    "Deleted lines": "-  // TODO(benoitjacob): This really should be:\n-  // TODO(benoitjacob): This really should be:\n-  // TODO(benoitjacob): This really should be:",
    "Added lines": "+  // TODO(b/62193649): This really should be:\n+  // TODO(b/62193649): This really should be:\n+  // TODO(b/62193649): This really should be:",
    "Label": "clean"
},
{
    "Id": 425,
    "Library": "tensorflow",
    "Date": "2020/11/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/98ed5c580a755432c9ed171844f6db29b5f54b76",
    "Root Cause": "N.A",
    "Bug report": "Add debug logging to SavedModelSignatureDefImporter to print input signatures.\n\nPiperOrigin-RevId: 343319320\nChange-Id: Idbb8d502913953a8d806d48a8fa18243ef6d0c19",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  VLOG(1) << \"Importing Signature: \" << name;\n+\n+    VLOG(1) << \"Importing Signature Input: input_name = \" << iter.first\n+            << \", tensor_info = \" << tensor_info.DebugString();\n+",
    "Label": "clean"
},
{
    "Id": 426,
    "Library": "tensorflow",
    "Date": "2020/11/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1e5c3bf7727f7f0ec85fd9ad8285c705a788bda6",
    "Root Cause": "N.A",
    "Bug report": "Add TODO with bug for refactor suggestion.\n\nPiperOrigin-RevId: 343308507\nChange-Id: I630e4ea8eacd4601aa69777fd4ad6e2014cfc3bc",
    "Number of deleted lines": 1,
    "Deleted lines": "-// TODO(renjieliu): Refactor this to merge with other",
    "Added lines": "+// TODO(b/173708994): Refactor this to merge with other",
    "Label": "clean"
},
{
    "Id": 427,
    "Library": "tensorflow",
    "Date": "2020/11/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c58c88b23122576fc99ecde988aab6041593809b",
    "Root Cause": "N.A",
    "Bug report": "Use proper bug for deprecated tfmini flag removal\n\nPiperOrigin-RevId: 343190641\nChange-Id: I6f1d8e521cbe30b0ffc8d7cb7e1c0e70c5219eeb",
    "Number of deleted lines": 4,
    "Deleted lines": "-  // TODO(benoitjacob): This really should be:\n-  // TODO(benoitjacob): This really should be:\n-  // TODO(benoitjacob): This really should be:\n-  // TODO(benoitjacob): This really should be:",
    "Added lines": "+  // TODO(b/62193649): This really should be:\n+  // TODO(b/62193649): This really should be:\n+  // TODO(b/62193649): This really should be:\n+  // TODO(b/62193649): This really should be:",
    "Label": "clean"
},
{
    "Id": 428,
    "Library": "tensorflow",
    "Date": "2020/11/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f8d00d4ce6b822bfec5836883a15bba92d9e9306",
    "Root Cause": "N.A",
    "Bug report": "Mark TODO with proper bug\n\nPiperOrigin-RevId: 342729558\nChange-Id: I7976d10edd244f411e8a0137b3fb9a763e5f003a",
    "Number of deleted lines": 2,
    "Deleted lines": "-  // TODO(ahentz): Our usage of VectorOfTensors could be optimized by\n-  // TODO(ahentz): We can improve the optimized_ops version to handle other",
    "Added lines": "+  // TODO(b/173221795): Our usage of VectorOfTensors could be optimized by\n+  // We can improve the optimized_ops version to handle other",
    "Label": "clean"
},
{
    "Id": 429,
    "Library": "tensorflow",
    "Date": "2020/11/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c7bee6875e32c2708026d2a7820def0ad785e94a",
    "Root Cause": "N.A",
    "Bug report": "Update shape debug string to use explicit tile notation in tiled_layout.md\n\nPiperOrigin-RevId: 342289679\nChange-Id: Iad6e687d053eae8f842d1f7e16c7cf934fd27c68",
    "Number of deleted lines": 1,
    "Deleted lines": "-shape with this layout is written as F32[3,5]{1,0:(2,2)}, where 1,0 relates to",
    "Added lines": "+shape with this layout is written as F32[3,5]{1,0:T(2,2)}, where 1,0 relates to",
    "Label": "clean"
},
{
    "Id": 430,
    "Library": "tensorflow",
    "Date": "2020/11/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c4bfcaf61cfeb637c3d5aaf64f251bcef69330ea",
    "Root Cause": "N.A",
    "Bug report": "Fix single pip package renaming bug\n\nThere is a bug in the single pip package renaming code: we do a replacement over the entire contents of `METADATA` and this causes the `tensorflow_estimator` dependency to be replaced with `tensorflow_gpu_estimator` (on 1.15 it was `tensorflow_cpu_estimator`). These packages don't exist by themseleves, Estimator has no CPU/GPU split. Previously this required a manual alteration of the Estimator package to fake it being the CPU/GPU version and a manual upload for that, but we should move away from this manual step as it always causes issues with new releases.\n\nSee for example #44775 (there are a few more similar issues, both internally and externally, but this is the most recent one).\n\nWe should build each pip package instead of doing the renaming. We do that on Linux/Mac already but Windows builds take too long so rather than rebuilding we just fake the new package via this renaming function. Future work in this area is needed to get rid of the renaming function, eventually removing it completely from both TF and TF ecosystem packages.\n\nPiperOrigin-RevId: 341915841\nChange-Id: I2bd4c3621e581ccf31e7bdd52958937b93971b90",
    "Number of deleted lines": 1,
    "Deleted lines": "-  sed -i.bak \"s/${ORIGINAL_PROJECT_NAME_DASH}/${NEW_PROJECT_NAME_DASH}/g\" \"${NEW_WHL_DIR_PREFIX}.dist-info/METADATA\"",
    "Added lines": "+\n+  # We need to change the name in the METADATA file, but we need to ensure that\n+  # all other occurences of the name stay the same, otherwise things such as\n+  # URLs and depedencies might be broken (for example, replacing without care\n+  # might transform a `tensorflow_estimator` dependency into\n+  # `tensorflow_gpu_estimator`, which of course does not exist -- except by\n+  # manual upload of a manually altered `tensorflow_estimator` package)\n+  sed -i.bak \"s/Name: ${ORIGINAL_PROJECT_NAME_DASH}/Name: ${NEW_PROJECT_NAME_DASH}/g\" \"${NEW_WHL_DIR_PREFIX}.dist-info/METADATA\"",
    "Label": "clean"
},
{
    "Id": 431,
    "Library": "tensorflow",
    "Date": "2020/11/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0c3501e9afc2f08ff26aa92c82da60f0cbdd023a",
    "Root Cause": "N.A",
    "Bug report": "Fix a legacy bug in ParameterizedBenchmark.\nIt was registering the wrong benchmark method under the target name.\n\nPiperOrigin-RevId: 341067872\nChange-Id: Ib9fec1a4aedebde91a2592ad021c07e26fb628ac",
    "Number of deleted lines": 4,
    "Deleted lines": "-        def create_benchmark_function(params):\n-          return lambda self: original_benchmark(self, *params)\n-\n-        benchmark = create_benchmark_function(params)",
    "Added lines": "+    def create_benchmark_function(original_benchmark, params):\n+      return lambda self: original_benchmark(self, *params)\n+\n+        benchmark = create_benchmark_function(original_benchmark, params)",
    "Label": "clean"
},
{
    "Id": 432,
    "Library": "tensorflow",
    "Date": "2020/11/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b54a2ddf6839d31f9ba65be7e50f0513e397d89d",
    "Root Cause": "N.A",
    "Bug report": "Fix compile bug in cuda_blas.cc in dbg mode.\n\nThis is reproducible with gcc 7.5 with the command:\n\n    bazel build --per_file_copt=+tensorflow/stream_executor/cuda.cuda_blas.cc@-O0,-g -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\nThe error was:\n\n    /usr/bin/ld: bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/libcublas_plugin.pic.lo(cuda_blas.pic.o): relocation R_X86_64_PC32 against undefined symbol `_ZN15stream_executor3gpu12_GLOBAL__N_120CUDABlasLtMatmulPlan14kMaxBatchCountE' can not be used when making a shared object; recompile with -fPIC\n\n@timshen91 helped me debug. We don't know why the error was occurring but this fixes it.\n\nPiperOrigin-RevId: 340989390\nChange-Id: I860e1ac8ee8aecd1543c068b66f281d0e0c56acd",
    "Number of deleted lines": 1,
    "Deleted lines": "-  static constexpr const int kMaxBatchCount = 65535;",
    "Added lines": "+  // TODO(reedwm): Making this static or constexpr causes a link error with gcc\n+  // in debug mode for unknown reasons. Investigate why.\n+  const int kMaxBatchCount = 65535;",
    "Label": "clean"
},
{
    "Id": 433,
    "Library": "tensorflow",
    "Date": "2020/10/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3f55e126804b0a0aec5ed6c90eb5ad85263ba563",
    "Root Cause": "N.A",
    "Bug report": "Use replaceUsesWithIf instead of iterating and replacing.\n\nThis fixes a bug where the getUses iterator was iterated and mutated without using an early_inc_iterator.\n\nPiperOrigin-RevId: 339539562\nChange-Id: Ib1a73fe9adff3aa2201e65278877bbc8be9284f4",
    "Number of deleted lines": 7,
    "Deleted lines": "-    for (auto& result_use : std::get<0>(result).getUses()) {\n-      Operation* result_using_op = result_use.getOwner();\n-      const bool inside_device_cluster =\n-          tpu_cluster.body().isAncestor(result_using_op->getParentRegion());\n-      if (inside_device_cluster) result_use.set(std::get<1>(result));\n-    }\n-        compilation_key, cluster_section_ops, controlflow_stack,",
    "Added lines": "+    llvm::ArrayRef<Operation*> cluster_ops,\n+  auto operand_inside_device_cluster = [&](OpOperand& operand) {\n+    return tpu_cluster.body().isAncestor(\n+               operand.getOwner()->getParentRegion()) &&\n+           llvm::none_of(cluster_ops, [&](Operation* cluster_op) {\n+             return operand.getOwner() == cluster_op;\n+           });\n+  };\n+\n+    Value external_output = std::get<0>(result);\n+    external_output.replaceUsesWithIf(std::get<1>(result),\n+                                      operand_inside_device_cluster);\n+        compilation_key, cluster_section_ops, controlflow_stack, cluster_ops,",
    "Label": "clean"
},
{
    "Id": 434,
    "Library": "tensorflow",
    "Date": "2020/10/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3bed57e8794be0af0b7f50aaef110fef6211a3c4",
    "Root Cause": "N.A",
    "Bug report": "Add string formatter for StackSummary. This allows printing a user friendly trace when using `Operation.traceback` which is often useful for debugging.\n\nPiperOrigin-RevId: 339375074\nChange-Id: I3b54110791bd0cd5bf42b6c63002bd4ac07db157",
    "Number of deleted lines": 6,
    "Deleted lines": "-      .def(\"__repr__\",\n-           [](const FrameSummary& self) {\n-             return py::str(\"<FrameSummary file {}, line {} in {}>\")\n-                 .format(self.filename, self.lineno, self.name);\n-           })\n-          py::return_value_policy::reference_internal);",
    "Added lines": "+\n+  py::str toString() const {\n+    return py::str(\"<FrameSummary file {}, line {} in {}>\")\n+        .format(filename, lineno, name);\n+  }\n+      .def(\"__repr__\", [](const FrameSummary& self) { return self.toString(); })\n+          py::return_value_policy::reference_internal)\n+      .def(\"__repr__\", [](const std::vector<FrameSummary>& self) {\n+        py::list frames;\n+        for (const auto& frame : self) {\n+          frames.append(frame.toString());\n+        }\n+        // \"\\n\".join(frames)\n+        return py::cast(\"\\n\").attr(\"join\")(frames);\n+      });",
    "Label": "clean"
},
{
    "Id": 435,
    "Library": "tensorflow",
    "Date": "2020/10/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7b5c80ed9e0606d4edbcd8fae005229422e9d4e1",
    "Root Cause": "N.A",
    "Bug report": "Remove 'size=small' to avoid test timeout issues for direct_session_with_debug_test and re-enable asan and tsan tests.\n\nPiperOrigin-RevId: 338717195\nChange-Id: I8ae260d1a3d619308f0c0ccaf842c81610fd5343",
    "Number of deleted lines": 5,
    "Deleted lines": "-    size = \"small\",\n-    tags = [\n-        \"noasan\",  #b/168811551\n-        \"notsan\",  #b/168811551\n-    ],",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 436,
    "Library": "tensorflow",
    "Date": "2020/10/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/eb68be4a42717c601732c928fec588760062d4a5",
    "Root Cause": "N.A",
    "Bug report": "Move TensorArray decomposition pass before region to functional control flow conversion.\n\nSome unit tests fail if we remove tf-region-control-flow-to-functional pass completely. So we still need this conversion before control flow legalization. Created a new bug to track it.\n\nPiperOrigin-RevId: 338355165\nChange-Id: I31bed4d9bd98bfa483cbce3f595aed6cf19c6ea9",
    "Number of deleted lines": 4,
    "Deleted lines": "-\n-  // TODO(b/159127949): TensorArray decomposition passes does not handle region\n-  // based control flow yet. So convert back to functional control flow.\n-  pm.addPass(mlir::TF::CreateTFRegionControlFlowToFunctional());",
    "Added lines": "+  // TODO(b/171426148): We cannot completely remove region to functional control\n+  // flow conversion from this pipeline yet as it causes some unit tests to\n+  // fail.\n+  pm.addPass(mlir::TF::CreateTFRegionControlFlowToFunctional());",
    "Label": "clean"
},
{
    "Id": 437,
    "Library": "tensorflow",
    "Date": "2020/10/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fec830a3c84480870bbb6e63a5532ffb6c41f747",
    "Root Cause": "N.A",
    "Bug report": "TFLM: Fix hard crash on Arduino.\n\nLong debug log caused the program to crash.\n\nAlso this debug log is not much in use anyway.\n\nPiperOrigin-RevId: 338264544\nChange-Id: I940208380074e74a40e21cf22960fdc8f1c00c4d",
    "Number of deleted lines": 7,
    "Deleted lines": "-  if (aligned_arena != tensor_arena) {\n-    TF_LITE_REPORT_ERROR(\n-        error_reporter,\n-        \"%d bytes lost due to alignment. To avoid this loss, please make sure \"\n-        \"the tensor_arena is 16 bytes aligned.\",\n-        aligned_arena - tensor_arena);\n-  }",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 438,
    "Library": "tensorflow",
    "Date": "2020/10/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e203dad0298792bbb5b3aa21fb6bc1a5629cda67",
    "Root Cause": "N.A",
    "Bug report": "Fix bug where tf32 wasn't properly being disabled.\n\nBefore, calling tf.config.experimental.enable_tensor_float_32_execution(False) did not actually disable TensorFloat-32 execution.\n\nPiperOrigin-RevId: 338190077\nChange-Id: I8613f168f5a0af1b8789e7b78bdd06616fbaafa2",
    "Number of deleted lines": 1,
    "Deleted lines": "-        \"//tensorflow/core/platform:tensor_float_32_utils\",",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 439,
    "Library": "tensorflow",
    "Date": "2020/10/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/833b3a49a9d77cf5b04990d003c91cc9810b40d8",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Solve the bug when output status not OK, for `PrefetchDataset`, `RecordBufferEnqueue` function may be called more times than `RecordBufferDequeue`.\n\nPiperOrigin-RevId: 338149161\nChange-Id: I8a7df4e21364814c6097447465d26f145219b776",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      } else {\n+        // If status not ok, we still record the dequeue event to make sure each\n+        // enqueue event is paired with a dequeue event even in the presence of\n+        // errors.\n+        RecordBufferDequeue(ctx, buffer_.front().value);",
    "Label": "clean"
},
{
    "Id": 440,
    "Library": "tensorflow",
    "Date": "2020/10/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/28db548cfd2d238e63c3fb049119fca8369abdbe",
    "Root Cause": "N.A",
    "Bug report": "Fixing a bug in conv+add fusion",
    "Number of deleted lines": 10,
    "Deleted lines": "-#include \"mkldnn.hpp\"\n-        if (!context->forward_input_to_output_with_shape(\n-          AllocateOutputSetMklShape(context, kOutputIndex_Dst, output_tensor,\n-                                    output_tf_shape, *output_mkl_shape,\n-                                    native_format);\n-          bool result =\n-              (*output_tensor)->CopyFrom(add_tensor, add_tensor.shape());\n-          DCHECK(result);\n-        return;\n-                                              add_mkl_shape, false)) {",
    "Added lines": "+#include \"mkldnn.hpp\"\n+        if (context->forward_input_to_output_with_shape(\n+          return;\n+                                              add_mkl_shape, false) &&\n+          !native_format) {\n+        if (native_format) {\n+          // We are simply deep copying the add_tensor to output_tensor without\n+          // changing memory layout, hence using same memory descriptor.\n+          ADD_MD = DST_MD =\n+              memory::desc({add_tensor.NumElements()}, MklDnnType<Toutput>(),\n+                           mkldnn::memory::format_tag::x);\n+        }",
    "Label": "clean"
},
{
    "Id": 441,
    "Library": "tensorflow",
    "Date": "2020/10/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3d21c6393dff611c920020680bf970fa6bbb78c0",
    "Root Cause": "N.A",
    "Bug report": "Remove debug print.\n\nPiperOrigin-RevId: 337166681\nChange-Id: I449ead5c3a92eca2084286cec377467313c207f9",
    "Number of deleted lines": 1,
    "Deleted lines": "-  LOG(ERROR) << instr->GetModule()->ToString();",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 442,
    "Library": "tensorflow",
    "Date": "2020/10/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3fdc2307c5dc58eaec48b4159c9ccfffe5940552",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in AddCallbackForEvent where we are using invalid events\n\nPiperOrigin-RevId: 337163503\nChange-Id: I60fc2d54a6758aa26b9c200390dd155e270628f0",
    "Number of deleted lines": 6,
    "Deleted lines": "-    }\n-\n-    if (event->second->underlying_event != nullptr &&\n-        event->second->underlying_event.use_count() != 0) {\n-      event->second->underlying_event->AddCallback(fn);\n-      event->second->callbacks.push_back(std::move(fn));",
    "Added lines": "+      if (event->second->underlying_event != nullptr &&\n+          event->second->underlying_event.use_count() != 0) {\n+        event->second->underlying_event->AddCallback(fn);\n+      } else {\n+        event->second->callbacks.push_back(std::move(fn));\n+      }",
    "Label": "clean"
},
{
    "Id": 443,
    "Library": "tensorflow",
    "Date": "2020/10/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c346633dd41e3793f71d6b61a130838c448e0005",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug where vector::reserve was being called on config.sparse instead of config.ragged.\n\nPiperOrigin-RevId: 337133816\nChange-Id: I4b712c2e141201f21f19fb92627981c53bb4280d",
    "Number of deleted lines": 1,
    "Deleted lines": "-    config.sparse.reserve(attrs_.num_ragged);",
    "Added lines": "+    config.ragged.reserve(attrs_.num_ragged);",
    "Label": "clean"
},
{
    "Id": 444,
    "Library": "tensorflow",
    "Date": "2020/10/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e485bb64c5b51e422833ef9ed6f32d985a09bc0f",
    "Root Cause": "N.A",
    "Bug report": "Disable direct_session_with_debug_test on asan TAP.\n\nPiperOrigin-RevId: 336999668\nChange-Id: I6103f2ebdf0907e1d4dd59a5a54020ef9584ee8a",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"noasan\",  #b/168811551",
    "Label": "clean"
},
{
    "Id": 445,
    "Library": "tensorflow",
    "Date": "2020/10/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b071d687adf2fb6383bb409eaca92d5ddee711c4",
    "Root Cause": "N.A",
    "Bug report": "Fixed bug where Tensorflow misreports sizes when sizes are mismatched.\n\nPiperOrigin-RevId: 336185233\nChange-Id: I484b326ad9917495ea147448895eb72c8fc9caeb",
    "Number of deleted lines": 5,
    "Deleted lines": "-    const size_t expected_max_elements = feature.length;\n-        expected_max_elements != example_shape.num_elements()) {\n-          \"Inconsistent number of elements for feature \", c.feature_name, \": \",\n-          expected_max_elements, \" vs \", dense_shape.num_elements());\n-      if (num_elements != expected_max_elements) {",
    "Added lines": "+    const size_t data_max_elements = feature.length;\n+        data_max_elements != example_shape.num_elements()) {\n+          \"Inconsistent max number of elements for feature \", c.feature_name,\n+          \": expected \", example_shape.num_elements(), \", but found \",\n+          data_max_elements);\n+      if (num_elements != data_max_elements) {",
    "Label": "clean"
},
{
    "Id": 446,
    "Library": "tensorflow",
    "Date": "2020/10/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dc666bf0f4588f4c2c51f0c42b2421b35d45c3a4",
    "Root Cause": "N.A",
    "Bug report": "Fixing a couple bugs in the xtensa_hifimini conv implementation:\n  1. inner loop counters were not incremented correctly\n  2. quantization multiplier scaling was wrong\n\nPiperOrigin-RevId: 336117753\nChange-Id: I65bae033097436ed1ed58a342ac28ac731f4296c",
    "Number of deleted lines": 8,
    "Deleted lines": "-  const int input_depth_iters = input_depth / 2;\n-            for (int filter_x = 0; filter_x < filter_width; ++filter_x) {\n-                        input_depth -\n-                for (int i = 0; i < input_depth_iters; ++i) {\n-                      (i * 2) - 2;\n-          // alignment:\n-          acc_56 = ops::micro::xtensa::hifimini::MultiplyByQuantizedMultiplier(\n-              acc_24x2, output_multiplier[out_channel],",
    "Added lines": "+            for (int filter_x = 0; filter_x < filter_width; filter_x += 2) {\n+                        input_depth * 2 -\n+                for (int i = 0; i < input_depth; i += 2) {\n+                      i - 2;\n+          // alignment. Convert the (unsigned) 32-bit multiplier down to a\n+          // 24-bit multiplier.\n+          acc_56 = micro::xtensa::hifimini::MultiplyByQuantizedMultiplier(\n+              acc_24x2, output_multiplier[out_channel] >> 8,",
    "Label": "clean"
},
{
    "Id": 447,
    "Library": "tensorflow",
    "Date": "2020/10/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bde6a9cfeee634889e8e844cf8e30cf012a734ae",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Change the code to solve the bug that ParallelMap records more buffered elements enqueue than dequeue.\n\nPiperOrigin-RevId: 335742261\nChange-Id: I5f43c880f19022b59addc02dd9a93da74ad96a74",
    "Number of deleted lines": 1,
    "Deleted lines": "-      RecordBufferEnqueue(ctx.get(), result->return_values);",
    "Added lines": "+        RecordBufferEnqueue(ctx.get(), result->return_values);",
    "Label": "clean"
},
{
    "Id": 448,
    "Library": "tensorflow",
    "Date": "2020/09/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/351a2eeec340be78de572b1bf76890c6cd8d8a27",
    "Root Cause": "N.A",
    "Bug report": "internal changes\nUse NLClassifier in movie reiview ref app, split the app into two, one with interpreter, another with NLClassifier\n\nbuild with gradle:\n  Using support lib:\n  ./gradlew installInterpreterDebug\n\n  Using task API lib:\n  ./gradlew installTaskApiDebug\n\nPiperOrigin-RevId: 333539487\nChange-Id: I49debe8c492cf70f0ec9d4e35bd78d8126913310",
    "Number of deleted lines": 1,
    "Deleted lines": "-        \"The TensorFlow Lite model file and label file can be used in the [text classification](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification) reference app by adding `model.tflite`, `text_label.txt` and `vocab.txt` to the [assets directory](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification/android/app/src/main/assets). Do not forget to also change the filenames in the [code](https://github.com/tensorflow/examples/blob/master/lite/examples/text_classification/android/app/src/main/java/org/tensorflow/lite/examples/textclassification/TextClassificationClient.java#L43).\"",
    "Added lines": "+        \"The TensorFlow Lite model file can be used in the [text classification](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification) reference app by adding `model.tflite` to the [assets directory](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification/android/lib_task_api/src/main/assets). Do not forget to also change the filenames in the [code](https://github.com/tensorflow/examples/blob/master/lite/examples/text_classification/android/lib_task_api/src/main/java/org/tensorflow/lite/examples/textclassification/TextClassificationClient.java#L31).\"",
    "Label": "clean"
},
{
    "Id": 449,
    "Library": "tensorflow",
    "Date": "2020/09/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e38e718a08182ccda45905ed390ef7f7bf1b2e0d",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Add the `kBufferSizeMin` to .cc file to solve some bug.\n\nPiperOrigin-RevId: 333311804\nChange-Id: I8b9c6e13c8c40612ab4c08760920ef59a92ce15a",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+/* static */ constexpr const char* const PrefetchDatasetOp::kBufferSizeMin;",
    "Label": "clean"
},
{
    "Id": 450,
    "Library": "tensorflow",
    "Date": "2020/09/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d6adf94ff3cbf4b5ee77681b44a0b0000bdb8014",
    "Root Cause": "N.A",
    "Bug report": "Add documentation of how to debug test failures.\n\nPiperOrigin-RevId: 333248272\nChange-Id: Ie42f981b26c4b22709e72df01c2461cb592b6995",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+# If this test fails, use the build flag --verbose_failures to see a full\n+# command line of the failing build command, then copy the command line and run\n+# it for further debugging.",
    "Label": "clean"
},
{
    "Id": 451,
    "Library": "tensorflow",
    "Date": "2020/09/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/60c4c3e2105afe28fb7a11cb8d440e0caabbf735",
    "Root Cause": "N.A",
    "Bug report": "Fix memory bug\n\nPiperOrigin-RevId: 333009744\nChange-Id: Id0c4d896fef5c30dc3508f7cc777007ad8721163",
    "Number of deleted lines": 3,
    "Deleted lines": "-        // tflite_tensor from the outer scope is invalidated due to calling\n-        // CreateNewTensorWithDifferentType\n-        tflite_tensor = &context->tensors[tensor_idx];",
    "Added lines": "+        // `tflite_tensor` value could be invalid when the `context->tensors`\n+        // is reallocated. Thus reassigning `tflite_tensor` with a fresh value.\n+        tflite_tensor = &context->tensors[tensor_idx];",
    "Label": "clean"
},
{
    "Id": 452,
    "Library": "tensorflow",
    "Date": "2020/09/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9240cef2e6d7bfda47823be96f3e45e617fd9150",
    "Root Cause": "N.A",
    "Bug report": "Fix some memory management bugs.\n\nPiperOrigin-RevId: 332919172\nChange-Id: Id490d94a38c35ceece6b7659f5f55d5e6fd5cb67",
    "Number of deleted lines": 5,
    "Deleted lines": "-    SE_StreamExecutorList* se_lists =\n-        new SE_StreamExecutorList[stream_exec.size()];\n-      se_lists[i].exec = new SE_StreamExecutor*[stream_exec[i].size()];\n-        compiler_, &se_module_group, se_lists, stream_exec.size(), &allocator,\n-        se_executables, status.c_status);",
    "Added lines": "+    std::vector<SE_StreamExecutorList> se_lists(stream_exec.size());\n+    std::vector<std::vector<SE_StreamExecutor*>> se_lists_storage;\n+      se_lists[i].count = stream_exec[i].size();\n+      se_lists_storage.emplace_back(stream_exec[i].size());\n+      se_lists[i].exec = se_lists_storage.back().data();\n+        compiler_, &se_module_group, se_lists.data(), stream_exec.size(),\n+        &allocator, se_executables, status.c_status);",
    "Label": "clean"
},
{
    "Id": 453,
    "Library": "tensorflow",
    "Date": "2020/09/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6e9da4b8c14ac894e3c270e2fa55c43084f6c7c0",
    "Root Cause": "N.A",
    "Bug report": "Clear device_parsing_cache in context._reset_context().\n\nThis fixes a bug where context.device(\"cpu:0\") still resolves to a device str from an older context after resetting the context.\n\nPiperOrigin-RevId: 332369820\nChange-Id: I020588abe700c33c7750bc7ece6d1cc21628b314",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  global _device_parsing_cache\n+  _device_parsing_cache = {}",
    "Label": "clean"
},
{
    "Id": 454,
    "Library": "tensorflow",
    "Date": "2020/09/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8a773b71258e627ee51bee30b017c27d790a1b3f",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug that the file copied by TF from HDFS to local may be wrong, when HDFS file is being overwritten #42597",
    "Number of deleted lines": 3,
    "Deleted lines": "-        file_(file) {}\n-    const char* disable_eof_retried = getenv(\"HDFS_DISABLE_READ_EOF_RETRIED\");\n-    if (disable_eof_retried && disable_eof_retried[0] == '1') {",
    "Added lines": "+        file_(file) {\n+          const char* disable_eof_retried = getenv(\"HDFS_DISABLE_READ_EOF_RETRIED\");\n+          if (disable_eof_retried && disable_eof_retried[0] == '1') {\n+            disable_eof_retried_ = true;\n+          } else {\n+            disable_eof_retried_ = false;\n+          }\n+        }\n+    if (disable_eof_retried_) {\n+  bool disable_eof_retried_;",
    "Label": "clean"
},
{
    "Id": 455,
    "Library": "tensorflow",
    "Date": "2020/09/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/770b38be96abb970e296050cf69b19caacb5fa3e",
    "Root Cause": "N.A",
    "Bug report": "Don't prune the __nv_relfatbin section\n\nPruning seems to strip PTX for NCCL as well, which defeats the purpose of\nshipping PTX with TF at all.\n\nNote: it is expect that this will increase the TF pip size by ~ 33MB.  There may\nbe a way to claw this back but fixing the bug takes higher priority.\nPiperOrigin-RevId: 330657787\nChange-Id: I3fac11e504326e08428ae2ac4dd899783519c2bc",
    "Number of deleted lines": 1,
    "Deleted lines": "-        srcs = [pruned, dlink],",
    "Added lines": "+        srcs = [lib, dlink],",
    "Label": "clean"
},
{
    "Id": 456,
    "Library": "tensorflow",
    "Date": "2020/09/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/28a2d0a1a83b3dfa3c903ce645aa9d319a4fd5ef",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in strides to layout conversion.\n\nDo not rely on implicit bool conversion of iterator.\n\nPiperOrigin-RevId: 330584856\nChange-Id: I42e2128542e11c6704e24a9319358462c780490f",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (dlmt->dl_tensor.strides && !absl::c_find(dimensions, 0)) {",
    "Added lines": "+  if (dlmt->dl_tensor.strides &&\n+      absl::c_find(dimensions, 0) == dimensions.end()) {",
    "Label": "clean"
},
{
    "Id": 457,
    "Library": "tensorflow",
    "Date": "2020/09/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5dc4cac97cd433331d4a5cef14a5004c8c5e25d8",
    "Root Cause": "N.A",
    "Bug report": "Add support for passing extra args to the cubin creator.\n\nThis is particulalry useful when debugging to enable IR printing with\n\n    extra_args = [\"--print-ir-before-all\"]\n\nin a gen_kernel_library rule.\n\nPiperOrigin-RevId: 330532324\nChange-Id: I68744689c8f075aac58a59b06696c8f0f1f3b47a",
    "Number of deleted lines": 2,
    "Deleted lines": "-def _gen_kernel_image_hdr(name, mlir_op, gpu_archs, tile_size, same_shape = None, unroll_factors = None):\n-def gen_kernel_library(name, types, tile_size, tags = [], same_shape = None, unroll_factors = None):",
    "Added lines": "+    if ctx.attr.extra_args:\n+        cmd_args.extend(ctx.attr.extra_args)\n+\n+        \"extra_args\": attr.string_list(),\n+def _gen_kernel_image_hdr(name, mlir_op, gpu_archs, tile_size, same_shape = None, unroll_factors = None, extra_args = []):\n+        extra_args = extra_args,\n+def gen_kernel_library(name, types, tile_size, tags = [], same_shape = None, unroll_factors = None, extra_args = []):\n+      extra_args: Extra arguments to pass to the generator tool.\n+                extra_args = extra_args,",
    "Label": "clean"
},
{
    "Id": 458,
    "Library": "tensorflow",
    "Date": "2020/09/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c0af29905c1c48e4e59a8741d807a0cd3c924065",
    "Root Cause": "N.A",
    "Bug report": "Use a new list to store the captured inputs, to avoid modifying the original\ninputs passed in by the caller. This can make debugging easier under some\ncases.\n\nPiperOrigin-RevId: 329767428\nChange-Id: Id5647aaf41a591708dc2bfd89f651b5492f7dcd6",
    "Number of deleted lines": 3,
    "Deleted lines": "-    for i, inp in enumerate(inputs):\n-      inputs[i] = inp\n-        op_type, inputs, dtypes, input_types, name, attrs, op_def,",
    "Added lines": "+    # Use a different list to avoid modifying the original inputs list.\n+    captured_inputs = []\n+    for inp in inputs:\n+      captured_inputs.append(inp)\n+        op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,",
    "Label": "clean"
},
{
    "Id": 459,
    "Library": "tensorflow",
    "Date": "2020/09/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3e4222c02e2aff4a0ff8997a55455d304b6c28c3",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug that the file copied by TF from HDFS to local may be wrong, when HDFS file is being overwritten #42597",
    "Number of deleted lines": 1,
    "Deleted lines": "-    const char* disable_eof_retried = getenv(\"DISABLE_HDFS_READ_EOF_RETRIED\");",
    "Added lines": "+    const char* disable_eof_retried = getenv(\"HDFS_DISABLE_READ_EOF_RETRIED\");",
    "Label": "clean"
},
{
    "Id": 460,
    "Library": "tensorflow",
    "Date": "2020/09/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/43641031eaf781fb12c8c18d1bc320bd9108de62",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug that the file copied by TF from HDFS to local may be wrong, when HDFS file is being overwritten #42597",
    "Number of deleted lines": 1,
    "Deleted lines": "-    const char* disable_eof_retried = getenv(\"DISABLE_HDFS_READ_EOF_RETRIED\")",
    "Added lines": "+    const char* disable_eof_retried = getenv(\"DISABLE_HDFS_READ_EOF_RETRIED\");",
    "Label": "clean"
},
{
    "Id": 461,
    "Library": "tensorflow",
    "Date": "2020/09/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bdf940adf6eb2e0eff1655dfaae3eab36580fac6",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug that the file copied by TF from HDFS to local may be wrong, when HDFS file is being overwritten #42597",
    "Number of deleted lines": 1,
    "Deleted lines": "-      //eof_retried = true, avoid calling hdfsOpenFile in Read, Fixes #42597",
    "Added lines": "+      // eof_retried = true, avoid calling hdfsOpenFile in Read, Fixes #42597",
    "Label": "clean"
},
{
    "Id": 462,
    "Library": "tensorflow",
    "Date": "2020/09/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/316c4d68cdb79c6b11d9662907ec8afdbd0432e2",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug that the file copied by TF from HDFS to local may be wrong, when HDFS file is being overwritten #42597",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    const char* disable_eof_retried = getenv(\"DISABLE_HDFS_READ_EOF_RETRIED\")\n+    if (disable_eof_retried && disable_eof_retried[0] == '1') {\n+      //eof_retried = true, avoid calling hdfsOpenFile in Read, Fixes #42597\n+      eof_retried = true;\n+    }",
    "Label": "clean"
},
{
    "Id": 463,
    "Library": "tensorflow",
    "Date": "2020/08/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/71d09fe58c93dcff3cbcdd75426e033d13b97488",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Update the `OutputTimeLocked` function for `AsyncKnownRatio` node. It is to fix the bug of current modeling for `MapAndBatchDatasetOp`(`AsyncKnownRatio` node with `ratio_`>1) auto-tuning. To be precise, when element in the buffer corresponds to `ratio_` elements from the input node(one batch), the `buffer_size` also needs to be adjusted by dividing `ratio_`.\n\nPiperOrigin-RevId: 329330931\nChange-Id: Ic8e4e5b18e6e09ab09e03deeebc9487bff21f80d",
    "Number of deleted lines": 5,
    "Deleted lines": "-      buffer_size = parallelism;\n-        (*gradients)[long_name()] =\n-            buffer_size_der - (1.0L + consumer_time_der +\n-                               producer_time_der * inputs_time_der_sum) *\n-                                  self_processing_time / Square(parallelism);",
    "Added lines": "+      if (ratio_ == 0) {\n+        buffer_size = parallelism;\n+      } else {\n+        // Currently, MapAndBatch is the only transformation creates\n+        // AsyncKnownRatio nodes with ratio >= 1. For MapAndBatch, we create\n+        // `parallelism` threads to apply the function on elements from input\n+        // dataset, while one element in the buffer actually corresponds to\n+        // `ratio_` elements from input dataset. So we adjust the `buffer_size`\n+        // by dividing `ratio_`.\n+        buffer_size = parallelism / ratio_;\n+      }\n+        (*gradients)[long_name()] = buffer_size_der / ratio_ -\n+                                    (1.0L + consumer_time_der +\n+                                     producer_time_der * inputs_time_der_sum) *\n+                                        self_processing_time /\n+                                        Square(parallelism);",
    "Label": "clean"
},
{
    "Id": 464,
    "Library": "tensorflow",
    "Date": "2020/08/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/11a48e7f4948d2c2fbfce1d9c4d68fa19fbbe185",
    "Root Cause": "N.A",
    "Bug report": "Update TPU rewrite pass to serialize computation for compilation in generic form with debug information (NFC).\n\nPiperOrigin-RevId: 328348895\nChange-Id: I9f27d6b6147a120756b0201418b5498d6d3ed2f7",
    "Number of deleted lines": 1,
    "Deleted lines": "-    module_for_func.get().print(os);",
    "Added lines": "+#include \"mlir/IR/OperationSupport.h\"  // from @llvm-project\n+    OpPrintingFlags print_flags;\n+    print_flags.enableDebugInfo();\n+    module_for_func.get().print(os, print_flags);",
    "Label": "clean"
},
{
    "Id": 465,
    "Library": "tensorflow",
    "Date": "2020/08/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/576ed353aa85c8e1f534fa1650eafec1030c4ae5",
    "Root Cause": "N.A",
    "Bug report": "Only warn about duplicate flags in debug builds when parsing the tflite tooling commandline flags.\n\nPiperOrigin-RevId: 327382108\nChange-Id: I4c582b0ed1c32a8c72e27704e802fadfcf214915",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#ifndef NDEBUG\n+      // Only log this in debug builds.\n+#endif",
    "Label": "clean"
},
{
    "Id": 466,
    "Library": "tensorflow",
    "Date": "2020/08/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ecca4dc3c7f14b940a6f58cb002ab3a205bb5010",
    "Root Cause": "N.A",
    "Bug report": "Use fmt.Errorf() instead of bug()",
    "Number of deleted lines": 1,
    "Deleted lines": "-\t\treturn bug(\"unable to convert shape %v (num_elements: %d) into shape %v (num_elements: %d)\", t.shape, old_shape_size, new_shape, new_shape_size)",
    "Added lines": "+\t\treturn fmt.Errorf(\"unable to convert shape %v (num_elements: %d) into shape %v (num_elements: %d)\", t.shape, old_shape_size, new_shape, new_shape_size)",
    "Label": "clean"
},
{
    "Id": 467,
    "Library": "tensorflow",
    "Date": "2020/08/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/99720318dff80146ccfb18ca184bdb73c0785f5d",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in TPUExtractOutsideCompilation where a builder is setting an insertion position in a region block when a block has not been added to the region.\n\nPiperOrigin-RevId: 326267964\nChange-Id: Ibc6dfcfbd656c1f6282153b571c4bf49d7804872",
    "Number of deleted lines": 4,
    "Deleted lines": "-  builder->setInsertionPoint(&then_branch.front(), then_branch.front().begin());\n-  builder->createBlock(&then_branch);\n-  builder->setInsertionPoint(&else_branch.front(), else_branch.front().begin());\n-  builder->createBlock(&else_branch);",
    "Added lines": "+  then_branch.push_back(new Block);\n+  builder->setInsertionPointToEnd(&then_branch.front());\n+  else_branch.push_back(new Block);\n+  builder->setInsertionPointToEnd(&else_branch.front());",
    "Label": "clean"
},
{
    "Id": 468,
    "Library": "tensorflow",
    "Date": "2020/08/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0c944514c1f933a420f6f35e96c66ad4714d989d",
    "Root Cause": "N.A",
    "Bug report": "Remove left-over debugging in bufferize pass.\n\nPiperOrigin-RevId: 325035330\nChange-Id: I25aa3f413106a74a76847ecfc32810b61b1ed3e4",
    "Number of deleted lines": 1,
    "Deleted lines": "-    module.dump();",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 469,
    "Library": "tensorflow",
    "Date": "2020/08/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0b84561324672ee3045ee23112dbc68540b39cdc",
    "Root Cause": "N.A",
    "Bug report": "[MLIR:TF] Fix a bug in binary out of concat hoisting\n\nPiperOrigin-RevId: 324962240\nChange-Id: I77aa3559a3a7244ee92e952c384a86a655bf4e4b",
    "Number of deleted lines": 5,
    "Deleted lines": "-      auto lhs = arg.getDefiningOp()->getOperand(operand_idx);\n-      auto ranked = lhs.getType().dyn_cast<RankedTensorType>();\n-      auto lhs = arg.getDefiningOp()->getOperand(operand_idx);\n-      auto ranked = lhs.getType().dyn_cast<RankedTensorType>();\n-  auto ranked = op.getType().cast<RankedTensorType>();",
    "Added lines": "+      auto operand = arg.getDefiningOp()->getOperand(operand_idx);\n+      auto ranked = operand.getType().dyn_cast<RankedTensorType>();\n+      auto operand = arg.getDefiningOp()->getOperand(operand_idx);\n+      auto ranked = operand.getType().dyn_cast<RankedTensorType>();\n+  // Concat result type must be a ranked tensor.\n+  auto ranked = op.getType().dyn_cast<RankedTensorType>();",
    "Label": "clean"
},
{
    "Id": 470,
    "Library": "tensorflow",
    "Date": "2020/08/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c96f601e0caabe294c906aa0355b18e970713091",
    "Root Cause": "N.A",
    "Bug report": "remove obsolete comment, the bug is already fixed\n\nPiperOrigin-RevId: 324906751\nChange-Id: I625e5f9509044c40130733168d7c6a92c25d57bd",
    "Number of deleted lines": 3,
    "Deleted lines": "-    # TODO(b/124370185): Use all elements as inputs to throw an error if there\n-    # are ignored arguments. Calling with arguments that are not part of the\n-    # signature should throw an error.",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 471,
    "Library": "tensorflow",
    "Date": "2020/08/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7972c284ac705b06066dffc85b7a36ef9662fb74",
    "Root Cause": "N.A",
    "Bug report": "fixed bug that doesn't return on TF_InvalidArugment",
    "Number of deleted lines": 8,
    "Deleted lines": "-      break;\n-      break; \n-  // Must use new status for TF_AllocateOutput if previous status is set \n-  // because of an invalid values argument.  \n-  StatusWrapper allocation_status_wrapper;\n-      sizeof(tensorflow::tstring), allocation_status_wrapper.s);\n-  if (TF_GetCode(allocation_status_wrapper.s) != TF_OK){ \n-    TF_OpKernelContext_Failure(ctx, allocation_status_wrapper.s);",
    "Added lines": "+      return;\n+      return; \n+      sizeof(tensorflow::tstring), status_wrapper.s);\n+  if (TF_GetCode(status_wrapper.s) != TF_OK){ \n+    TF_OpKernelContext_Failure(ctx, status_wrapper.s);",
    "Label": "clean"
},
{
    "Id": 472,
    "Library": "tensorflow",
    "Date": "2020/08/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b6c6e28076d67b6b2026460dfb36ac2ec856b84e",
    "Root Cause": "N.A",
    "Bug report": "TFL java/BUILD: remove no_mac tag from a few targets.\n\nThe bug has been resolved.\n\nPiperOrigin-RevId: 324449997\nChange-Id: Id849e7ce3cbb1ea610c1a748abc985d40ee1bc52",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n+    tags = [\n+        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n+    ],\n+    tags = [\n+        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n+    ],\n+    tags = [\n+        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n+    ],\n+    tags = [\n+        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n+    ],\n+        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n+    tags = [\n+        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n+    ],",
    "Label": "clean"
},
{
    "Id": 473,
    "Library": "tensorflow",
    "Date": "2020/08/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/af197c251c1c4251dee82f8dd9a2f10fc7b28b24",
    "Root Cause": "N.A",
    "Bug report": "TFL java/BUILD: remove no_mac tag from a few targets.\n\nThe bug has been resolved.\n\nPiperOrigin-RevId: 324395469\nChange-Id: I776c5ca54864dc94a9669bdf110408a23883ef82",
    "Number of deleted lines": 17,
    "Deleted lines": "-        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n-    tags = [\n-        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n-    ],\n-    tags = [\n-        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n-    ],\n-    tags = [\n-        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n-    ],\n-    tags = [\n-        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n-    ],\n-        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n-    tags = [\n-        \"no_mac\",  # TODO(b/122888913): libtensorflowlite_test_jni broke on mac.\n-    ],",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 474,
    "Library": "tensorflow",
    "Date": "2020/07/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f6cc28bda0824751ef26c96f8c0ffbe3cc7a5c2d",
    "Root Cause": "N.A",
    "Bug report": "Fix variable shadowing bug in dispatcher_impl.cc\n\nPiperOrigin-RevId: 324150733\nChange-Id: I9128b6462643b103ad313f649d03f366a2208f45",
    "Number of deleted lines": 2,
    "Deleted lines": "-      for (const auto& task : tasks_by_job_[task->job_id]) {\n-        if (!task->finished) {",
    "Added lines": "+      for (const auto& job_task : tasks_by_job_[task->job_id]) {\n+        if (!job_task->finished) {",
    "Label": "clean"
},
{
    "Id": 475,
    "Library": "tensorflow",
    "Date": "2020/07/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/091eebc2ac26e3fb6b532cca2c28d34558765393",
    "Root Cause": "N.A",
    "Bug report": "Fixed a bug in log message queue.",
    "Number of deleted lines": 2,
    "Deleted lines": "-      SendToSink(*sink, entry);\n-      queue_.pop();",
    "Added lines": "+      SendToSink(*sink, queue_.front());\n+    queue_.pop();",
    "Label": "clean"
},
{
    "Id": 476,
    "Library": "tensorflow",
    "Date": "2020/07/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8d4711c52d6d5788f93f68b539e316b1ff83a2aa",
    "Root Cause": "N.A",
    "Bug report": "Ensure each call to ASharedMemory_create produces a unique memory space.\n\nThe non-android ASharedMemory_create has a bug, where if two calls to create memory regions use the same 'name', they will be mapped to the to same /dev/shm file and trip over each other (particularly if they're different sizes).\n\nPiperOrigin-RevId: 322945710\nChange-Id: I103e385a2a82addf46f19188dd63baa6818db96d",
    "Number of deleted lines": 2,
    "Deleted lines": "-int ASharedMemory_create(const char* name, size_t size) {\n-  int fd = shm_open(name, O_RDWR | O_CREAT, 0644);",
    "Added lines": "+#include <algorithm>\n+int ASharedMemory_create(const char* /* name */, size_t size) {\n+  // Each call to ASharedMemory_create produces a unique memory space, hence\n+  // name should not be used to create the shared memory file, otherwise\n+  // two calls to create memory regions using the same 'name', will collide.\n+  char shm_name_buffer[L_tmpnam];\n+  if (tmpnam(shm_name_buffer) == nullptr) {\n+    return -1;\n+  }\n+\n+  // tmpnam will produce a string containing with slashes, but shm_open\n+  // won't like that.\n+  std::string shm_region_name = std::string(shm_name_buffer);\n+  std::replace(shm_region_name.begin(), shm_region_name.end(), '/', '-');\n+\n+  int fd = shm_open(shm_region_name.c_str(), O_RDWR | O_CREAT, 0644);",
    "Label": "clean"
},
{
    "Id": 477,
    "Library": "tensorflow",
    "Date": "2020/07/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b1c32123aafd991d1c812ca87db8f78712901dfd",
    "Root Cause": "N.A",
    "Bug report": "Fix bug for node with attr output_shape empty.\n\nPiperOrigin-RevId: 322825760\nChange-Id: I280fab9cc2670895df4325404734afc9b3f7dafb",
    "Number of deleted lines": 6,
    "Deleted lines": "-    node.node.attr[\"output_shapes\"].list.shape[\n-        incoming_edge.destination.index].CopyFrom(\n-            tensor_shape_pb2.TensorShapeProto(dim=[\n-                tensor_shape_pb2.TensorShapeProto.Dim(size=dim)\n-                for dim in tensor_data.numpy.shape\n-            ]))",
    "Added lines": "+    if node.node.attr[\"output_shapes\"].list.shape:\n+      node.node.attr[\"output_shapes\"].list.shape[\n+          incoming_edge.destination.index].CopyFrom(\n+              tensor_shape_pb2.TensorShapeProto(dim=[\n+                  tensor_shape_pb2.TensorShapeProto.Dim(size=dim)\n+                  for dim in tensor_data.numpy.shape\n+              ]))\n+",
    "Label": "clean"
},
{
    "Id": 478,
    "Library": "tensorflow",
    "Date": "2020/07/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/803ce808dd349df10c5b639f929a75036efa4c4b",
    "Root Cause": "N.A",
    "Bug report": "The non-android ASharedMemory_create has a bug, where if two calls to create memory regions use the same 'name', they will be mapped to the to same /dev/shm file and trip over each other (particularly if they're different sizes).\n\nThis CL ensures that each created memory region is unique.\n\nPiperOrigin-RevId: 322628204\nChange-Id: I13b3b59cd87107844dcdbb26ed86f337c761d94f",
    "Number of deleted lines": 15,
    "Deleted lines": "-#include <algorithm>\n-int ASharedMemory_create(const char* /* name */, size_t size) {\n-  // name should not be used to identify the memory region (hence\n-  // 'anonymous' shared memory). Generate a unique name for every create call.\n-  char _tmpname[L_tmpnam];\n-  if (tmpnam_r(_tmpname) == nullptr) {\n-    return -1;\n-  }\n-\n-  // tmpnam will produce a string containing with slashes, but shm_open\n-  // won't like that.\n-  std::string _name = std::string(_tmpname);\n-  std::replace(_name.begin(), _name.end(), '/', '-');\n-\n-  int fd = shm_open(_name.c_str(), O_RDWR | O_CREAT, 0644);",
    "Added lines": "+int ASharedMemory_create(const char* name, size_t size) {\n+  int fd = shm_open(name, O_RDWR | O_CREAT, 0644);",
    "Label": "clean"
},
{
    "Id": 479,
    "Library": "tensorflow",
    "Date": "2020/07/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/23f6e460f37d474d2224255789b22da153ac446a",
    "Root Cause": "N.A",
    "Bug report": "The non-android ASharedMemory_create has a bug, where if two calls to create memory regions use the same 'name', they will be mapped to the to same /dev/shm file and trip over each other (particularly if they're different sizes).\n\nThis CL ensures that each created memory region is unique.\n\nPiperOrigin-RevId: 322593774\nChange-Id: Ib1137045604955871dd2e33aae8205275201d4b1",
    "Number of deleted lines": 2,
    "Deleted lines": "-int ASharedMemory_create(const char* name, size_t size) {\n-  int fd = shm_open(name, O_RDWR | O_CREAT, 0644);",
    "Added lines": "+#include <algorithm>\n+int ASharedMemory_create(const char* /* name */, size_t size) {\n+  // name should not be used to identify the memory region (hence\n+  // 'anonymous' shared memory). Generate a unique name for every create call.\n+  char _tmpname[L_tmpnam];\n+  if (tmpnam_r(_tmpname) == nullptr) {\n+    return -1;\n+  }\n+\n+  // tmpnam will produce a string containing with slashes, but shm_open\n+  // won't like that.\n+  std::string _name = std::string(_tmpname);\n+  std::replace(_name.begin(), _name.end(), '/', '-');\n+\n+  int fd = shm_open(_name.c_str(), O_RDWR | O_CREAT, 0644);",
    "Label": "clean"
},
{
    "Id": 480,
    "Library": "tensorflow",
    "Date": "2020/07/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a80239d1573f8b56b9d570e7bf19d00bcb59bede",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in Return op construction while lifting resource ops.\n\nWhile constructing the return op, the code assumes that the key-value pairs in SmallDenseMap `resource_arg_to_new_output` are in the same order as they are inserted. This is incorrect as SmallDenseMap container does not guarantee the ordering of key-value pairs. Instead we use the `resource_arg_to_new_output` index mapping itself to construct the return op.\n\nPiperOrigin-RevId: 321723375\nChange-Id: I85a244fcaf7d0339a1ce773b153cd4cb1af48a71",
    "Number of deleted lines": 1,
    "Deleted lines": "-      new_retvals.push_back(branch.getArgument(entry.getFirst()));",
    "Added lines": "+    new_retvals.resize(new_retvals.size() + resource_arg_to_new_output.size());\n+      int64_t resource_arg_index = entry.getFirst();\n+      int64_t output_index = entry.getSecond();\n+      new_retvals[output_index] = branch.getArgument(resource_arg_index);",
    "Label": "clean"
},
{
    "Id": 481,
    "Library": "tensorflow",
    "Date": "2020/07/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5f1b79f00f04cf26292a5a2d75eb8360ecaee090",
    "Root Cause": "N.A",
    "Bug report": "[TF:TRT] Avoid null pointer accesses in TRTOptimizationPass::PrintDebugInfo.\n\nCluster::GetDeviceSet may return a null pointer. Skip printing the DeviceSet\nwhen the routine returns nullptr. Move the printing of DeviceSet to closer to\nthe other information for the cluster and before the information for the\ngrappler item.\nPiperOrigin-RevId: 321368504\nChange-Id: Ic59c9f79fe759a40558fbf2377818e3d8999d752",
    "Number of deleted lines": 7,
    "Deleted lines": "-  for (const auto dev : cluster->GetDeviceSet()->devices()) {\n-    const auto& pname = dev->parsed_name();\n-    LOG(INFO) << \"Device name= \" << dev->name()\n-              << \" parsedname job= \" << pname.job << \" id= \" << pname.id\n-              << \" has_id: \" << pname.has_id << \" has_job: \" << pname.has_job\n-              << \"has_type: \" << pname.has_type << \" type =\" << pname.type;\n-  }",
    "Added lines": "+\n+\n+    if (cluster->GetDeviceSet()) {\n+      for (const auto dev : cluster->GetDeviceSet()->devices()) {\n+        LOG(INFO) << \"Device name= \" << dev->name() << \"Pased name= \"\n+                  << DeviceNameUtils::ParsedNameToString(dev->parsed_name());\n+      }\n+    }\n+",
    "Label": "clean"
},
{
    "Id": 482,
    "Library": "tensorflow",
    "Date": "2020/07/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/03cb974777d00ddf05957d41aa3d44d7c76af924",
    "Root Cause": "N.A",
    "Bug report": "Work around the bazel bug around /showIncludes logic on windows GPU builds.\n\nPiperOrigin-RevId: 321310704\nChange-Id: Ie5182dd706696f8c5f425cec02919ca998bb15e8",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"C:\\\\\\\\botcode\\\\\\\\w\",",
    "Label": "clean"
},
{
    "Id": 483,
    "Library": "tensorflow",
    "Date": "2020/07/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/01635a48b125ea465fe5c37bcfe3d33484355299",
    "Root Cause": "N.A",
    "Bug report": "Enable info about no MLIR passes scheduled by default.\n\nThis is really important for debugging to know what runs and does not run.\n\nPiperOrigin-RevId: 321290297\nChange-Id: I38b9b9dbb28f75751e0aa6b30490af234afbef4d",
    "Number of deleted lines": 8,
    "Deleted lines": "-    VLOG(0) << \"None of the MLIR optimization passes are enabled \"\n-            << \"(registered \" << registry_->passes().size() << \")\";\n-  VLOG(0) << \"Running MLIR Graph Optimization Passes \"\n-          << \"(registered \" << registry_->passes().size() << \" passes)\";\n-    VLOG(0) << \"None of the MLIR optimization passes are enabled \"\n-            << \"(registered \" << registry_->passes().size() << \" passes)\";\n-  VLOG(0) << \"Running MLIR Graph Optimization V1 Compat Passes \"\n-          << \"(registered \" << registry_->passes().size() << \" passes)\";",
    "Added lines": "+    LOG_FIRST_N(INFO, 1)\n+        << \"None of the MLIR optimization passes are enabled \"\n+        << \"(registered \" << registry_->passes().size() << \")\";\n+  LOG_FIRST_N(INFO, 1) << \"Running MLIR Graph Optimization Passes \"\n+                          << \"(registered \" << registry_->passes().size()\n+                          << \" passes)\";\n+    LOG_FIRST_N(INFO, 1)\n+        << \"None of the MLIR optimization passes are enabled \"\n+        << \"(registered \" << registry_->passes().size() << \" passes)\";\n+  LOG_FIRST_N(INFO, 1) << \"Running MLIR Graph Optimization V1 Compat Passes \"\n+                          << \"(registered \" << registry_->passes().size()\n+                          << \" passes)\";",
    "Label": "clean"
},
{
    "Id": 484,
    "Library": "tensorflow",
    "Date": "2020/07/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e9516a8b0e980b1b2478a683bd233d5ac2c9f2e9",
    "Root Cause": "N.A",
    "Bug report": "Fix size computation logic in TransposeSimple\n\nUse the right type when computing `num_bytes`.  This caused the crash observed\nin the bug, but I could not reproduce in a unit test (even with cuda_asan) since\nthe `InlinedVector` always uses stack storage.\n\nPiperOrigin-RevId: 321199018\nChange-Id: I339307a2d2d098d4ad73b363b5f96c19ed65ea52",
    "Number of deleted lines": 1,
    "Deleted lines": "-  auto num_bytes = sizeof(int64) * host_buf.size();",
    "Added lines": "+  auto num_bytes = sizeof(int32) * host_buf.size();",
    "Label": "clean"
},
{
    "Id": 485,
    "Library": "tensorflow",
    "Date": "2020/07/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0a00a296358e3f2ed4bab82f3e22bb1d800c3150",
    "Root Cause": "N.A",
    "Bug report": "Enable info about no MLIR passes scheduled by default.\n\nThis is really important for debugging to know what runs and does not run.\n\nPiperOrigin-RevId: 320097145\nChange-Id: I6ce34bd45dd2d09a8155953380eeee3a04b3921b",
    "Number of deleted lines": 4,
    "Deleted lines": "-    VLOG(1) << \"None of the MLIR optimization passes are enabled \"\n-  VLOG(1) << \"Running MLIR Graph Optimization Passes \"\n-    VLOG(1) << \"None of the MLIR optimization passes are enabled \"\n-  VLOG(1) << \"Running MLIR Graph Optimization V1 Compat Passes \"",
    "Added lines": "+    VLOG(0) << \"None of the MLIR optimization passes are enabled \"\n+  VLOG(0) << \"Running MLIR Graph Optimization Passes \"\n+    VLOG(0) << \"None of the MLIR optimization passes are enabled \"\n+  VLOG(0) << \"Running MLIR Graph Optimization V1 Compat Passes \"",
    "Label": "clean"
},
{
    "Id": 486,
    "Library": "tensorflow",
    "Date": "2020/07/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9448a8c0676dc82b9c01b7e3e14307804cc3d53c",
    "Root Cause": "N.A",
    "Bug report": "[TF:TRT] Relocate the code block that sets verbose network name for debugging.\n\ncr/286899513 introduced a verbose network name to help debugging for TensorRT\n6+. However, the code block was added to the wrong branch for the older\nTensorRT versions as dead code. This CL relocates the code block to make it\nuseful.\n\nPiperOrigin-RevId: 319919276\nChange-Id: I16b8fdb376652df94ed82ee1b4b455535c958223",
    "Number of deleted lines": 13,
    "Deleted lines": "-#if IS_TRT_VERSION_GE(6, 0, 0, 0)\n-  string precision_mode_str;\n-  TF_RETURN_IF_ERROR(\n-      TrtPrecisionModeToName(precision_mode_, &precision_mode_str));\n-  string trt_network_name = StrCat(\n-      \"TF:\", TF_VERSION_STRING, \", \", \"TRT:\", GetLoadedTensorRTVersion(), \"-\",\n-      \"Precision:\", precision_mode_str, \", \", \"Calibration:\", use_calibration_,\n-      \", \", \"Max-Batch-Size:\", max_batch_size, \", \",\n-      \"Max-Workspace-Size:\", max_workspace_size_bytes);\n-  VLOG(1) << \"Setting TensorRT network name to \" << trt_network_name;\n-  network()->setName(trt_network_name.c_str());\n-#endif  // #if IS_TRT_VERSION_GE(6, 0, 0, 0)\n-",
    "Added lines": "+\n+  string precision_mode_str;\n+  TF_RETURN_IF_ERROR(\n+      TrtPrecisionModeToName(precision_mode_, &precision_mode_str));\n+  string trt_network_name = StrCat(\n+      \"TF:\", TF_VERSION_STRING, \", \", \"TRT:\", GetLoadedTensorRTVersion(), \"-\",\n+      \"Precision:\", precision_mode_str, \", \", \"Calibration:\", use_calibration_,\n+      \", \", \"Max-Batch-Size:\", max_batch_size, \", \",\n+      \"Max-Workspace-Size:\", max_workspace_size_bytes);\n+  VLOG(1) << \"Setting TensorRT network name to \" << trt_network_name;\n+  network()->setName(trt_network_name.c_str());\n+",
    "Label": "clean"
},
{
    "Id": 487,
    "Library": "tensorflow",
    "Date": "2020/07/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ea6b83192106bdc145624c73193802cc872ed5d8",
    "Root Cause": "N.A",
    "Bug report": "tests passing for summary op, added shape debug string",
    "Number of deleted lines": 14,
    "Deleted lines": "-template <typename T> \n-float get_float_value(T* element){ \n-  return static_cast<float>(*element); \n-}\n-\n-template<>\n-float get_float_value(Eigen::half* element){\n-  return Eigen::half_impl::half_to_float(*element);  \n-}\n-\n-      v->set_simple_value(get_float_value(&values_array[i]));\n-    TF_Tensor* summary_tensor = TF_AllocateOutput(ctx, 0 \n-      *(TF_TensorData(summary_tensor)) = &summary_tstring; \n-      TF_SetOutput(ctx, 0, summary_tensor, status); ",
    "Added lines": "+      v->set_simple_value(float(values_array[i]));\n+    TF_Tensor* summary_tensor = TF_AllocateOutput(ctx, 0,\n+      TF_TString* output_tf_tstring = reinterpret_cast<TF_TString*>(TF_TensorData(summary_tensor)); \n+      TF_TString_Init(output_tf_tstring); \n+      tensorflow::tstring* output_tstring = reinterpret_cast<tensorflow::tstring*>(output_tf_tstring); \n+      *output_tstring = summary_tstring; // may want to use std::move \n+",
    "Label": "clean"
},
{
    "Id": 488,
    "Library": "tensorflow",
    "Date": "2020/07/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d69076f6e6575886055845125003472e18882916",
    "Root Cause": "N.A",
    "Bug report": "Fix the function name in debugging.md",
    "Number of deleted lines": 3,
    "Deleted lines": "-### Debugging `tf.function`: `tf.config.experimental_execute_functions_eagerly`\n-by using `tf.config.experimental_execute_functions_eagerly`. This will\n-Adding a call to `tf.config.experimental_execute_functions_eagerly` before",
    "Added lines": "+### Debugging `tf.function`: `tf.config.experimental_run_functions_eagerly`\n+by using `tf.config.experimental_run_functions_eagerly`. This will\n+Adding a call to `tf.config.experimental_run_functions_eagerly` before",
    "Label": "clean"
},
{
    "Id": 489,
    "Library": "tensorflow",
    "Date": "2020/07/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a326299119bb12b177960f9c1a407663a8d7223",
    "Root Cause": "N.A",
    "Bug report": "Fix the function name in debugging.md",
    "Number of deleted lines": 3,
    "Deleted lines": "-### Debugging `tf.function`: `tf.config.experimental_execute_functions_eagerly`\n-by using `tf.config.experimental_execute_functions_eagerly`. This will\n-Adding a call to `tf.config.experimental_execute_functions_eagerly` before",
    "Added lines": "+### Debugging `tf.function`: `tf.config.experimental_run_functions_eagerly`\n+by using `tf.config.experimental_run_functions_eagerly`. This will\n+Adding a call to `tf.config.experimental_run_functions_eagerly` before",
    "Label": "clean"
},
{
    "Id": 490,
    "Library": "tensorflow",
    "Date": "2020/07/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/adbf01fac1a980913acdf8724cfba3cdd7d60e33",
    "Root Cause": "N.A",
    "Bug report": "Sort the memory_profile snapshot by timestamp.\nTo fix a bug that timeline is incorrectly rendered in the memory profile tool.\n\nPiperOrigin-RevId: 319334723\nChange-Id: I9d80fd9a85669cac4bff3ffa408449889a1edaab",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // Sort the memory_profile_snapshots by time_offset_ps (ascending) after\n+  // sampling.\n+  absl::c_sort(*snapshots, [](const MemoryProfileSnapshot& a,\n+                              const MemoryProfileSnapshot& b) {\n+    return a.time_offset_ps() < b.time_offset_ps();\n+  });",
    "Label": "clean"
},
{
    "Id": 491,
    "Library": "tensorflow",
    "Date": "2020/06/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/35b3d903617ea8993555e59e627501f960698219",
    "Root Cause": "N.A",
    "Bug report": "Ensure that we remove all debug information during kernel lowering.\n\nThe MLIR to LLVM translation takes the presence of location information as a signal as to whether it should produce a \"debug\" module. This degrades performance. To be on the safe side, we now globally strip all debug information and not just for the kernels themselves.\n\nPiperOrigin-RevId: 318975928\nChange-Id: If1d871850ee4dcb66db1d4692a89890727ad8f80",
    "Number of deleted lines": 1,
    "Deleted lines": "-  kernelPm.addPass(::mlir::createStripDebugInfoPass());",
    "Added lines": "+  // Remove all location information to prevent a debug build.\n+  pm.addPass(::mlir::createStripDebugInfoPass());",
    "Label": "clean"
},
{
    "Id": 492,
    "Library": "tensorflow",
    "Date": "2020/06/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1603b2053c8514d84c1214e5f4421a8a8db4d77b",
    "Root Cause": "N.A",
    "Bug report": "zeroPadding3D shape bug",
    "Number of deleted lines": 6,
    "Deleted lines": "-        dim1 = input_shape[2] + 2 * self.padding[0][0]\n-        dim2 = input_shape[3] + 2 * self.padding[1][0]\n-        dim3 = input_shape[4] + 2 * self.padding[2][0]\n-        dim1 = input_shape[1] + 2 * self.padding[0][1]\n-        dim2 = input_shape[2] + 2 * self.padding[1][1]\n-        dim3 = input_shape[3] + 2 * self.padding[2][1]",
    "Added lines": "+        dim1 = input_shape[2] + self.padding[0][0] + self.padding[0][1]\n+        dim2 = input_shape[3] + self.padding[1][0] + self.padding[1][1]\n+        dim3 = input_shape[4] + self.padding[2][0] + self.padding[2][1]\n+        dim1 = input_shape[1] + self.padding[0][0] + self.padding[0][1]\n+        dim2 = input_shape[2] + self.padding[1][0] + self.padding[1][1]\n+        dim3 = input_shape[3] + self.padding[2][0] + self.padding[2][1]",
    "Label": "clean"
},
{
    "Id": 493,
    "Library": "tensorflow",
    "Date": "2020/06/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f24487e6190eb4637f5e4988737cbe17e91a231a",
    "Root Cause": "N.A",
    "Bug report": "Disabling test until bug fix lands.\n\nPiperOrigin-RevId: 316930923\nChange-Id: I8aa57e5627fa649474c469bd5c92a713c4d9bd75",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    tags = [\n+        \"no_windows\",  # TODO(b/159210739): Remove this tag after fixing the bug.\n+    ],",
    "Label": "clean"
},
{
    "Id": 494,
    "Library": "tensorflow",
    "Date": "2020/06/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0292d384f84f55a78312ae0132f700caec245315",
    "Root Cause": "N.A",
    "Bug report": "Suppress a deprecation warning for Object.finalize\n\nThe API is deprecated starting in JDK 9:\nhttps://bugs.openjdk.java.net/browse/JDK-8165641\n\nPiperOrigin-RevId: 316587198\nChange-Id: Icafed5879fd50973e955522a176aa11a8773090b",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // for Object.finalize, see https://bugs.openjdk.java.net/browse/JDK-8165641\n+  @SuppressWarnings(\"deprecation\")",
    "Label": "clean"
},
{
    "Id": 495,
    "Library": "tensorflow",
    "Date": "2020/06/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fe6201c57b7fd78e344b4e5ee7fe6c7f7151b08a",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in data flow analysis for asynchronous collective-permute.\n\nPiperOrigin-RevId: 316540956\nChange-Id: Icb1fb9d1d445d5aa3cf7afa580eed06607c4ecb3",
    "Number of deleted lines": 1,
    "Deleted lines": "-  // CollectivePermuteDone forwards the operand value at {0} to its output.",
    "Added lines": "+  // CollectivePermuteDone forwards the operand value at {1} to its output.\n+        case HloOpcode::kCollectivePermuteStart:\n+          // CollectivePermuteStart produces a tuple of\n+          // {aliased operand, destination buffer, U32 context, U32 context}.\n+          define_value_at(/*index=*/{});\n+          define_value_at(/*index=*/{1});\n+          define_value_at(/*index=*/{2});\n+          define_value_at(/*index=*/{3});\n+          break;\n+        case HloOpcode::kCollectivePermuteDone:\n+          // CollectivePermuteDone's output aliases its input tuple element {1}.\n+          break;",
    "Label": "clean"
},
{
    "Id": 496,
    "Library": "tensorflow",
    "Date": "2020/06/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3efb46044d96f0064ca032102d39e11f904ef7dc",
    "Root Cause": "N.A",
    "Bug report": "[tfdbg2] Doc: add to list of supported tensor_debug_modes\n\nPiperOrigin-RevId: 316110652\nChange-Id: I3ba412107ac63641d63d8b7233501d062574521f",
    "Number of deleted lines": 3,
    "Deleted lines": "-        - \"NO_TENSOR\": (Default) Only traces the execution of ops' output\n-          tensors, while not dumping the value of the ops' output tensors\n-          or any form of concise summary of them.",
    "Added lines": "+      - \"NO_TENSOR\": (Default) Only traces the output tensors of all executed\n+        ops (including those executed eagerly at the Python level or as a part\n+        of a TensorFlow graph) and functions, while not extracting any\n+        information from the values of the tensors.\n+      - \"CURT_HEALTH\": For each floating-dtype tensor (e.g., tensors of dtypes\n+        such as `float32`, `float64` and `bfloat16`), extracts a binary bit\n+        indicating whether it contains any -infinity, +infinity or NaN.\n+      - \"CONCISE_HEALTH\": For each floating-dtype tensor, extract total\n+        element count, and counts of -infinity, +infinity and NaN elements.\n+      - \"FULL_HEALTH\": For each floating-dtype tensor, extracts the dtype,\n+        rank (number of dimensions), total element count, and counts of\n+        -infinity, +infinity and NaN elements.\n+      - \"SHAPE\": For each tensor (regardless of dtype), extracts its dtype,\n+        rank, total element count and shape.",
    "Label": "clean"
},
{
    "Id": 497,
    "Library": "tensorflow",
    "Date": "2020/06/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fa58bd5e942ae7c87ff9193804ca07d645fb4c4c",
    "Root Cause": "N.A",
    "Bug report": "Fix the bug in recording a hexagon profiling event.\n\nPiperOrigin-RevId: 316049143\nChange-Id: I7da5a6c0bd00d0b3a35fdbcb1925197adc5d51ef",
    "Number of deleted lines": 2,
    "Deleted lines": "-                         Profiler::EventType::OPERATOR_INVOKE_EVENT, node_id, 0,\n-                         counter);",
    "Added lines": "+                         Profiler::EventType::OPERATOR_INVOKE_EVENT, 0, counter,\n+                         node_id);",
    "Label": "clean"
},
{
    "Id": 498,
    "Library": "tensorflow",
    "Date": "2020/06/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c3067cb52138d991cd4d448f72a2895a61adb26d",
    "Root Cause": "N.A",
    "Bug report": "Fix bug that causes parameterized type annotations to lose their arguments. This prevents errors in Python 3.5 when combined with #40132.\n\nPiperOrigin-RevId: 315961338\nChange-Id: I01604b87bcee4b8e687055f727b8bdfc5da53376",
    "Number of deleted lines": 1,
    "Deleted lines": "-  copy_tx = type(tx.__name__, tx.__bases__, dict(tx.__dict__))",
    "Added lines": "+  # Prefer using __orig_bases__, which preserve generic type arguments.\n+  bases = getattr(tx, '__orig_bases__', tx.__bases__)\n+  copy_tx = type(tx.__name__, bases, dict(tx.__dict__))",
    "Label": "clean"
},
{
    "Id": 499,
    "Library": "tensorflow",
    "Date": "2020/06/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4cd4fcfd890123428ff86ed894f4dc48f3ccba5f",
    "Root Cause": "N.A",
    "Bug report": "Remove accidental debug leftover in crosstool wrapper.\n\nPiperOrigin-RevId: 314596277\nChange-Id: Idea72b9952a5387a7ffd4beb441ad6b6d09f3ea0",
    "Number of deleted lines": 1,
    "Deleted lines": "-    args.cuda_log = True",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 500,
    "Library": "tensorflow",
    "Date": "2020/06/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4f96acb5fdaf5d2e09e125d2d76874bebd52650c",
    "Root Cause": "N.A",
    "Bug report": "bugfix",
    "Number of deleted lines": 1,
    "Deleted lines": "-def _check_penalty_number(self, x):",
    "Added lines": "+def _check_penalty_number(x):",
    "Label": "clean"
},
{
    "Id": 501,
    "Library": "tensorflow",
    "Date": "2020/06/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/56ff49b74aab14c609975dc2f4a4af09d48070a0",
    "Root Cause": "N.A",
    "Bug report": "bugfix: missing format string part",
    "Number of deleted lines": 1,
    "Deleted lines": "-        )",
    "Added lines": "+        ).format(x)",
    "Label": "clean"
},
{
    "Id": 502,
    "Library": "tensorflow",
    "Date": "2020/06/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/61a0c3bccd0bf236f469203cf6e641d466e26c61",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Fix performance modeling bug in `group_by_window`.\n\nThe use of `::Reduce` suffix would result in iterator nodes created for iterating through the contents of a group not being properly attached to the parent iterator.\n\nPiperOrigin-RevId: 314218939\nChange-Id: Ic38851d0f9740fc3731bb93050c5e386368bdec4",
    "Number of deleted lines": 11,
    "Deleted lines": "-            // TODO(b/154341936): Explicitly stopping and starting this iterator\n-            // should not be necessary, but the `::Reduce` added to the prefix\n-            // passed to `current_group_iterator_` when it was created prevents\n-            // the model from identifying this iterator as the output of\n-            // `current_group_iterator_`.\n-            RecordStop(ctx);\n-            RecordStart(ctx);\n-\n-        // Restoring groups\n-        // Restoring Windows\n-            ctx, this, strings::StrCat(prefix(), \"::Reduce\"),",
    "Added lines": "+        TF_RETURN_IF_ERROR(writer->WriteScalar(full_name(\"group_counter\"),\n+                                               group_counter_ - 1));\n+        // Restoring groups_\n+        // Restoring window_sizes_\n+        // Group counter needs to be restored before current group iterator.\n+        TF_RETURN_IF_ERROR(\n+            reader->ReadScalar(full_name(\"group_counter\"), &group_counter_));\n+\n+            ctx, this, strings::StrCat(prefix(), \"[\", group_counter_++, \"]\"),\n+      int64 group_counter_ TF_GUARDED_BY(mu_) = 0;",
    "Label": "clean"
},
{
    "Id": 503,
    "Library": "tensorflow",
    "Date": "2020/05/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/44a649cc7a00540eb256488152aaf11f54000f3c",
    "Root Cause": "N.A",
    "Bug report": "Update Eigen to 8719b9c5bc1a97e62d675c02495ed72dda6fae73 to fix compiling error\n\nThis PR tries to update Eigen to 8719b9c5bc1a97e62d675c02495ed72dda6fae73.\nThe reason to update Eigen is to fix the build error for custom ops (See error below).\nThe issue is that in Eigen there was a bug that uses `if defined(EIGEN_ARCH_PPC)` incorrectly (should be `if EIGEN_ARCH_PPC`).\n\nI have created a PR in Eigen https://gitlab.com/libeigen/eigen/-/merge_requests/131 and the PR has already been merged.\n\nThis PR is a follow up in tensorflow repo to bump the Eigen to the latest version.\n\nThe error before this PR, when building a custom ops:\n```\nExecution platform: @local_config_platform//:host\n\nUse --sandbox_debug to see verbose messages from the sandbox\nIn file included from tensorflow_io/core/kernels/io_optimization.cc:22:\nIn file included from bazel-out/darwin-fastbuild/bin/external/local_config_tf/include/tensorflow/compiler/mlir/mlir_graph_optimization_pass.h:20:\nIn file included from bazel-out/darwin-fastbuild/bin/external/local_config_tf/include/tensorflow/core/common_runtime/function_optimization_registry.h:23:\nIn file included from bazel-out/darwin-fastbuild/bin/external/local_config_tf/include/tensorflow/core/common_runtime/device_set.h:23:\nIn file included from bazel-out/darwin-fastbuild/bin/external/local_config_tf/include/tensorflow/core/common_runtime/device.h:35:\nIn file included from bazel-out/darwin-fastbuild/bin/external/local_config_tf/include/tensorflow/core/framework/allocator.h:26:\nIn file included from bazel-out/darwin-fastbuild/bin/external/local_config_tf/include/tensorflow/core/framework/numeric_types.h:20:\nIn file included from bazel-out/darwin-fastbuild/bin/external/local_config_tf/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\nIn file included from bazel-out/darwin-fastbuild/bin/external/local_config_tf/include/unsupported/Eigen/CXX11/Tensor:14:\nbazel-out/darwin-fastbuild/bin/external/local_config_tf/include/unsupported/Eigen/CXX11/../../../Eigen/Core:334:10: fatal error: 'src/Core/arch/AltiVec/MatrixProduct.h' file not found\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n1 error generated.\nTarget //tensorflow_io/core:optimization failed to build\nINFO: Elapsed time: 4.778s, Critical Path: 4.54s\n```\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Number of deleted lines": 4,
    "Deleted lines": "-        sha256 = \"854eabe6817e38d7738fde6ec39c3dfc55fd5e68b2523de8cae936f391a38a69\",  # SHARED_EIGEN_SHA\n-        strip_prefix = \"eigen-cc86a31e20b48b0f03d714b4d1b1f50d52848d36\",\n-            \"https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/cc86a31e20b48b0f03d714b4d1b1f50d52848d36/eigen-cc86a31e20b48b0f03d714b4d1b1f50d52848d36.tar.gz\",\n-            \"https://gitlab.com/libeigen/eigen/-/archive/cc86a31e20b48b0f03d714b4d1b1f50d52848d36/eigen-cc86a31e20b48b0f03d714b4d1b1f50d52848d36.tar.gz\",",
    "Added lines": "+        sha256 = \"615be1295290c13039b0c980a4a55933be26b1e06194d86c6014876fa85c7c6b\",  # SHARED_EIGEN_SHA\n+        strip_prefix = \"eigen-8719b9c5bc1a97e62d675c02495ed72dda6fae73\",\n+            \"https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/8719b9c5bc1a97e62d675c02495ed72dda6fae73/eigen-8719b9c5bc1a97e62d675c02495ed72dda6fae73.tar.gz\",\n+            \"https://gitlab.com/libeigen/eigen/-/archive/8719b9c5bc1a97e62d675c02495ed72dda6fae73/eigen-8719b9c5bc1a97e62d675c02495ed72dda6fae73.tar.gz\",",
    "Label": "clean"
},
{
    "Id": 504,
    "Library": "tensorflow",
    "Date": "2020/05/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f6bf10607fc0bd00e94704e1ae20f06f34b81df3",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Fix a bug in prefetch dataset serialization logic.\n\nPiperOrigin-RevId: 313453820\nChange-Id: I573d4288fbb10b7491778ce4edf24241f5e35fa1",
    "Number of deleted lines": 3,
    "Deleted lines": "-    TF_RETURN_IF_ERROR(b->AddDataset(\n-        this, {input_graph_node, buffer_size},\n-        {std::make_pair(kSlackPeriod, slack_period_attr)}, output));",
    "Added lines": "+    AttrValue legacy_autotune_attr;\n+    b->BuildAttrValue(legacy_autotune_, &legacy_autotune_attr);\n+    TF_RETURN_IF_ERROR(\n+        b->AddDataset(this, {input_graph_node, buffer_size},\n+                      {std::make_pair(kSlackPeriod, slack_period_attr),\n+                       std::make_pair(kLegacyAutotune, legacy_autotune_attr)},\n+                      output));",
    "Label": "clean"
},
{
    "Id": 505,
    "Library": "tensorflow",
    "Date": "2020/05/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0a1449a983e6e88e62e175d0f34564414d26c4dd",
    "Root Cause": "N.A",
    "Bug report": "Fix bugs in docstring of conv3d_transpose.\n\nSeems like depth was missing and NCHW and NHWC were mentioned\ninstead of NCDHW and NDHWC.\n\nPiperOrigin-RevId: 313320657\nChange-Id: Id599e6e7b91a18f193b952153c50c27839c5693f",
    "Number of deleted lines": 6,
    "Deleted lines": "-    input: A 5-D `Tensor` of type `float` and shape `[batch, height, width,\n-      in_channels]` for `NHWC` data format or `[batch, in_channels, height,\n-      width]` for `NCHW` data format.\n-    filters: A 5-D `Tensor` with the same type as `value` and shape `[height,\n-      width, output_channels, in_channels]`.  `filter`'s `in_channels` dimension\n-      must match that of `value`.",
    "Added lines": "+    input: A 5-D `Tensor` of type `float` and shape `[batch, depth, height,\n+      width, in_channels]` for `NDHWC` data format or `[batch, in_channels,\n+      depth, height, width]` for `NCDHW` data format.\n+    filters: A 5-D `Tensor` with the same type as `value` and shape `[depth,\n+      height, width, output_channels, in_channels]`.  `filter`'s `in_channels`\n+      dimension must match that of `value`.",
    "Label": "clean"
},
{
    "Id": 506,
    "Library": "tensorflow",
    "Date": "2020/05/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b6f542de704c4f1b8897f2a8c7c359cddb9bd043",
    "Root Cause": "N.A",
    "Bug report": "[Profiler} Add link to a doc that describes how to use the profiler to debug tf.data performance.\n\nPiperOrigin-RevId: 313298827\nChange-Id: Idb1378b1efcb4f09225af5d23044e94737dd92ce",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  *re->add_documentation_tips() = MakeOverviewPageTipDocLink(\n+      \"https://www.tensorflow.org/guide/data_performance_analysis\",\n+      \"Analyze tf.data performance with the TF Profiler\");",
    "Label": "clean"
},
{
    "Id": 507,
    "Library": "tensorflow",
    "Date": "2020/05/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fd98070b2daece57e96d41f211e97fb16cf431e9",
    "Root Cause": "N.A",
    "Bug report": "Expose more XLA debug settings in Python.\n\nPiperOrigin-RevId: 313178400\nChange-Id: I236f4f2180f4efd334cb58b445bc6f1ba47401d4",
    "Number of deleted lines": 1,
    "Deleted lines": "-                    &DebugOptions::set_xla_gpu_enable_fast_min_max);",
    "Added lines": "+                    &DebugOptions::set_xla_gpu_enable_fast_min_max)\n+      .def_property(\"xla_backend_optimization_level\",\n+                    &DebugOptions::xla_backend_optimization_level,\n+                    &DebugOptions::set_xla_backend_optimization_level)\n+      .def_property(\"xla_cpu_enable_xprof_traceme\",\n+                    &DebugOptions::xla_cpu_enable_xprof_traceme,\n+                    &DebugOptions::set_xla_cpu_enable_xprof_traceme)\n+      .def_property(\"xla_llvm_disable_expensive_passes\",\n+                    &DebugOptions::xla_llvm_disable_expensive_passes,\n+                    &DebugOptions::set_xla_llvm_disable_expensive_passes)\n+      .def_property(\"xla_test_all_input_layouts\",\n+                    &DebugOptions::xla_test_all_input_layouts,\n+                    &DebugOptions::set_xla_test_all_input_layouts);",
    "Label": "clean"
},
{
    "Id": 508,
    "Library": "tensorflow",
    "Date": "2020/05/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b9f941a53fa9490fee3306c8f448aeb56bed9ce3",
    "Root Cause": "N.A",
    "Bug report": "Fix incorrect reference of np.assert_allclose (should be np.testing.assert_allclose)\n\nIn the docstring of tf.debugging.assert_near, the numpy compatibility\npart incorrectly uses np.assert_allclose.\n\nThis should be np.testing.assert_allclose instead.\n\nThis PR fixes the incorrect docstring.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "Number of deleted lines": 6,
    "Deleted lines": "-  Similar to `numpy.assert_allclose`, except tolerance depends on data type.\n-  This is due to the fact that `TensorFlow` is often used with `32bit`, `64bit`,\n-  and even `16bit` data.\n-  Similar to `numpy.assert_allclose`, except tolerance depends on data type.\n-  This is due to the fact that `TensorFlow` is often used with `32bit`, `64bit`,\n-  and even `16bit` data.",
    "Added lines": "+  Similar to `numpy.testing.assert_allclose`, except tolerance depends on data\n+  type. This is due to the fact that `TensorFlow` is often used with `32bit`,\n+  `64bit`, and even `16bit` data.\n+  Similar to `numpy.testing.assert_allclose`, except tolerance depends on data\n+  type. This is due to the fact that `TensorFlow` is often used with `32bit`,\n+  `64bit`, and even `16bit` data.",
    "Label": "clean"
},
{
    "Id": 509,
    "Library": "tensorflow",
    "Date": "2020/05/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1c73e5f20a7b69169e69e14d835c257b6c4e355b",
    "Root Cause": "N.A",
    "Bug report": "Add judgment for whether the sp_input is ordered\n\nFix a bug that the output will not be ordered when  the sp_input is unordered and all_rows_full is true.",
    "Number of deleted lines": 2,
    "Deleted lines": "-      ++csr_offset[indices(i, 0)];\n-    if (all_rows_full) {",
    "Added lines": "+    bool indices_is_order = true;\n+    int64 last_indices_row = 0;\n+      ++csr_offset[row];\n+      indices_is_order = indices_is_order & (row >= last_indices_row);\n+      last_indices_row = row;\n+    if (all_rows_full && indices_is_order) {",
    "Label": "clean"
},
{
    "Id": 510,
    "Library": "tensorflow",
    "Date": "2020/05/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8f1e715482accc94f9859954ed8b334c88c2b0cb",
    "Root Cause": "N.A",
    "Bug report": "Remove debug message\n\nPiperOrigin-RevId: 311808620\nChange-Id: I3c1ded522e5e2a9487ee9b1c2307d5e72820c9e6",
    "Number of deleted lines": 1,
    "Deleted lines": "-      LOG(INFO) << dtype_attr << \" \" << shape_attr;",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 511,
    "Library": "tensorflow",
    "Date": "2020/05/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/43691f9b891045f41b59ec1afbe06637c19d2377",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Add use_names column to buffer info debug string to help with debugging.\n\nPiperOrigin-RevId: 311440576\nChange-Id: I060aed0171625c79bfa7d8ae821f098670d6c84f",
    "Number of deleted lines": 5,
    "Deleted lines": "-                    \"buffer_id,buffer_name,alt_mem_benefit,size,definition_\"\n-                    \"time,use_times\\n\");\n-  std::set<int64> use_times;\n-      use_times.insert(instruction_schedule.at(use.instruction));\n-  absl::StrAppend(debug_str, \"\\\"\", absl::StrJoin(use_times, \";\"), \"\\\"\");",
    "Added lines": "+  // use_names: string. This is a semicolon-separated list of string\n+  // representation of uses.\n+                    \"buffer_id,buffer_name,alt_mem_benefit,size,\"\n+                    \"definition_time,use_times,use_names\\n\");\n+  std::vector<std::pair<int64, std::string>> uses;\n+      uses.push_back(\n+          {instruction_schedule.at(use.instruction), use.ToString()});\n+  absl::c_sort(uses);\n+  std::vector<int64> use_times;\n+  std::vector<std::string> use_names;\n+  use_times.reserve(uses.size());\n+  use_names.reserve(uses.size());\n+  for (auto use : uses) {\n+    use_times.push_back(use.first);\n+    use_names.push_back(use.second);\n+  }\n+  absl::StrAppend(debug_str, \"\\\"\", absl::StrJoin(use_times, \";\"), \"\\\",\");\n+  absl::StrAppend(debug_str, \"\\\"\", absl::StrJoin(use_names, \";\"), \"\\\"\");",
    "Label": "clean"
},
{
    "Id": 512,
    "Library": "tensorflow",
    "Date": "2020/05/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ed400dea16540c75d4e659494adc25a5ffc8bc9c",
    "Root Cause": "N.A",
    "Bug report": "Log the rendezvous address for debugging purpose. This is very helpful for\ndebugging blocking issues.\n\nPiperOrigin-RevId: 310836009\nChange-Id: I68f7b349de0c07b5e09518f2e5a2983eb4d16302",
    "Number of deleted lines": 4,
    "Deleted lines": "-    VLOG(2) << \"Send \" << parsed_key_.buf_;\n-    VLOG(2) << \"Send \" << in_loop_parsed.buf_;\n-    VLOG(2) << \"Recv \" << parsed_key_.buf_;\n-    VLOG(2) << \"Recv \" << in_loop_parsed.buf_;",
    "Added lines": "+    VLOG(2) << \"Send \" << parsed_key_.buf_ << \" using \"\n+            << reinterpret_cast<uintptr_t>(ctx->rendezvous());\n+    VLOG(2) << \"Send \" << in_loop_parsed.buf_ << \" using \"\n+            << reinterpret_cast<uintptr_t>(ctx->rendezvous());\n+    VLOG(2) << \"Recv \" << parsed_key_.buf_ << \" using \"\n+            << reinterpret_cast<uintptr_t>(ctx->rendezvous());\n+    VLOG(2) << \"Recv \" << in_loop_parsed.buf_ << \" using \"\n+            << reinterpret_cast<uintptr_t>(ctx->rendezvous());",
    "Label": "clean"
},
{
    "Id": 513,
    "Library": "tensorflow",
    "Date": "2020/05/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cd8b64f7fbdc2bc8700b71dadb4c51744a752095",
    "Root Cause": "N.A",
    "Bug report": "add -DDEBUG_BUILD to dbg profile\n\nthis prevents issue https://github.com/tensorflow/tensorflow/issues/37498.\nThe optimized AWS SDK can only be built in release mode. Fall back in debug mode\nSee: https://github.com/TileDB-Inc/TileDB/issues/1351",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+# AWS SDK must be compiled in release mode. see: https://github.com/tensorflow/tensorflow/issues/37498\n+build:dbg --copt -DDEBUG_BUILD",
    "Label": "clean"
},
{
    "Id": 514,
    "Library": "tensorflow",
    "Date": "2020/05/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dfc190e78833f4d619657b908f9aeec8c4f09b0a",
    "Root Cause": "N.A",
    "Bug report": "Re-enabling optimizer_v2 test on OSS as the dependent bug has been closed.\n\nPiperOrigin-RevId: 309963007\nChange-Id: I00ade07663bfa639a9193a1efbb003aedd192239",
    "Number of deleted lines": 1,
    "Deleted lines": "-        \"no_oss\",  # http://b/119349471",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 515,
    "Library": "tensorflow",
    "Date": "2020/05/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bf687433298f29af2ee7fc1068329b50ed310693",
    "Root Cause": "N.A",
    "Bug report": "Remove extra debug print statement",
    "Number of deleted lines": 1,
    "Deleted lines": "-  print(args.key_value)",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 516,
    "Library": "tensorflow",
    "Date": "2020/05/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6bcfc5e26aabedba3ea71cf5fe5f9f834b16cd54",
    "Root Cause": "N.A",
    "Bug report": "Disable quantize_and_dequantize_op_test test on Windows\n\nCulprit is 7fc0c18fec64c856790936442e6630b283ad0178 which removed the manual tag from several BUILD targets, thus causing this Windows failure (#38553)\n\n```\n# git bisect bad\n7fc0c18fec64c856790936442e6630b283ad0178 is the first bad commit\ncommit 7fc0c18fec64c856790936442e6630b283ad0178\nAuthor: ag.ramesh <ag.ramesh@intel.com>\nDate:   Tue Apr 14 17:30:51 2020 -0700\n\n    Make tf_gpu_cc_test always execute the test in CPU only build, and fixed a bug exposed in c_api_test by this change.\n\n tensorflow/c/BUILD        | 2 +-\n tensorflow/tensorflow.bzl | 2 +-\n 2 files changed, 2 insertions(+), 2 deletions(-)\n```\n\nPiperOrigin-RevId: 309415698\nChange-Id: I2415ff123a973c3c2078fdf5e63d31afcf8e8021",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    tags = [\n+        \"no_windows\",  # test uses rand_r which does not exist on Windows\n+    ],",
    "Label": "clean"
},
{
    "Id": 517,
    "Library": "tensorflow",
    "Date": "2020/04/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/336c431b3baab30fa6c76d93f55408ed51b4bd1f",
    "Root Cause": "N.A",
    "Bug report": "Add missing bug to disabled test target.\n\nPiperOrigin-RevId: 309297392\nChange-Id: I24ae812dd9b79c19238d1d627590a30acfa3b6f9",
    "Number of deleted lines": 1,
    "Deleted lines": "-        \"no_oss\",  # Target too big to run serially reliably.",
    "Added lines": "+        \"no_oss\",  # b/150954621 Target too big to run serially reliably.",
    "Label": "clean"
},
{
    "Id": 518,
    "Library": "tensorflow",
    "Date": "2020/04/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6f008417c18d6377048b13a74c79af56c5cb6e05",
    "Root Cause": "N.A",
    "Bug report": "Disable split_v_op test on Windows\n\nCulprit is 7fc0c18fec64c856790936442e6630b283ad0178 which removed the manual tag from several BUILD targets, thus causing this Windows failure (#38553)\n\n```\n# git bisect bad\n7fc0c18fec64c856790936442e6630b283ad0178 is the first bad commit\ncommit 7fc0c18fec64c856790936442e6630b283ad0178\nAuthor: ag.ramesh <ag.ramesh@intel.com>\nDate:   Tue Apr 14 17:30:51 2020 -0700\n\n    Make tf_gpu_cc_test always execute the test in CPU only build, and fixed a bug exposed in c_api_test by this change.\n\n tensorflow/c/BUILD        | 2 +-\n tensorflow/tensorflow.bzl | 2 +-\n 2 files changed, 2 insertions(+), 2 deletions(-)\n```\n\nPiperOrigin-RevId: 309119180\nChange-Id: I81b3250c418ff0690023bedb389735d3cd7e6cf7",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    tags = [\n+        \"no_windows\",  # split_v_op uses lrand48 which does not exist on Windows\n+    ],",
    "Label": "clean"
},
{
    "Id": 519,
    "Library": "tensorflow",
    "Date": "2020/04/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/51d23ffe3b111ed3b04c6d6363d1efe996173f1e",
    "Root Cause": "N.A",
    "Bug report": "Add tracking bug for broken DistStrat test.\n\nPiperOrigin-RevId: 309089042\nChange-Id: I50f1331c36b37cd815c184c0a7b6074d73e9875f",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    # b/155301154 broken with XLA:GPU",
    "Label": "clean"
},
{
    "Id": 520,
    "Library": "tensorflow",
    "Date": "2020/04/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6044647af404b492a23328a1380295db34e16d23",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Fix a bug in input time computation.\n\nPiperOrigin-RevId: 308740851\nChange-Id: I0ab84bd02c06492a5f08a9bd797727ef7630b676",
    "Number of deleted lines": 1,
    "Deleted lines": "-    input_times->back() +=",
    "Added lines": "+    input_times->back() =",
    "Label": "clean"
},
{
    "Id": 521,
    "Library": "tensorflow",
    "Date": "2020/04/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7218bcd4c04081850075dfe4e3f07b97d99a2549",
    "Root Cause": "N.A",
    "Bug report": "Fix incorrect usage of os.system().\n\nThe code wrongly assumed that os.system(cmd) returns the exit code of the\nprocess. Instead, it returns a concatenation of the exit code and signal\nnumber that terminated the process (if any). This change uses the proper\naccessor methods to extract the exit code and signal number from the\nreturn value. If a process terminated with exit(N) we return N. Else, we\nreturn the negative signal number. This is in line with the behaviour of\nsubprocess.run.\n\nSpecfically, this change fixes a bug where nvcc would exit with 1 but\nthe wrapper script would exit with 0. This is because os.system() would\nreturn 0x0100 which got truncated to 0x00 due to exit codes being\nlimited to 8-bit on Linux.",
    "Number of deleted lines": 3,
    "Deleted lines": "-    The return value of calling os.system('nvcc ' + args)\n-    exit_status = os.system(cmd)\n-  return os.system(cmd)",
    "Added lines": "+def system(cmd):\n+  \"\"\"Invokes cmd with os.system().\n+\n+  Args:\n+    cmd: The command.\n+\n+  Returns:\n+    The exit code if the process exited with exit() or -signal\n+    if the process was terminated by a signal.\n+  \"\"\"\n+  retv = os.system(cmd)\n+  if os.WIFEXITED(retv):\n+    return os.WEXITSTATUS(retv)\n+  else:\n+    return -os.WTERMSIG(retv)\n+    The return value of calling system('nvcc ' + args)\n+    exit_status = system(cmd)\n+  return system(cmd)",
    "Label": "clean"
},
{
    "Id": 522,
    "Library": "tensorflow",
    "Date": "2020/04/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/036b75a818493a30cd25caef1761931a3bc2b074",
    "Root Cause": "N.A",
    "Bug report": "Fix a potential overflow bug.\n\nAlso remove an unnecessary capture.\n\nPiperOrigin-RevId: 308582689\nChange-Id: Ifa9734cd77fe41e79bbf89e10e6a1bd0b0021ddf",
    "Number of deleted lines": 2,
    "Deleted lines": "-                          [&](const HloInstruction* instr) {\n-      int linear_index = j * vector_size + i;",
    "Added lines": "+                          [](const HloInstruction* instr) {\n+      int64 linear_index = j * vector_size + i;",
    "Label": "clean"
},
{
    "Id": 523,
    "Library": "tensorflow",
    "Date": "2020/04/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3b1545a555e14a94a1394d93327d1e6ad64e8319",
    "Root Cause": "N.A",
    "Bug report": "[Grappler] Fix a bug in folding multiply into add arithmetic optimization\n\nPiperOrigin-RevId: 308348068\nChange-Id: I61ca637344519f9effee313ea3f911c462d9bfe6",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    // And that Mul is not in the preserve set.\n+    TF_RETURN_IF_TRUE(IsInPreserveSet(*source));",
    "Label": "clean"
},
{
    "Id": 524,
    "Library": "tensorflow",
    "Date": "2020/04/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d9192b8cdc418267c3665d9e70f621eec0d4faac",
    "Root Cause": "N.A",
    "Bug report": "Don't pass invalid args to tp_new, it triggers an assertion in Python 3.7+.\n\nPy_None has never been valid there.  But most people don't run on a Python\ninterpreter built with NDEBUG asserts enabled so it goes unnoticed and is\nharmless by sheer luck (SWIG generated code included an equivalent bug for\n8+ years so it was widespread luck).\n\nPiperOrigin-RevId: 308265309\nChange-Id: Ie0d8259e20e39a29a72946419ae9424458c0bb83",
    "Number of deleted lines": 1,
    "Deleted lines": "-      EagerTensorType->tp_new(EagerTensorType, Py_None, Py_None));",
    "Added lines": "+  PyObject* empty_args = PyTuple_New(0);\n+  if (empty_args == nullptr) {\n+    return nullptr;\n+  }\n+  PyObject* empty_kwargs = PyDict_New();\n+  if (empty_kwargs == nullptr) {\n+    Py_DECREF(empty_args);\n+    return nullptr;\n+  }\n+      EagerTensorType->tp_new(EagerTensorType, empty_args, empty_kwargs));\n+  Py_DECREF(empty_kwargs);\n+  Py_DECREF(empty_args);",
    "Label": "clean"
},
{
    "Id": 525,
    "Library": "tensorflow",
    "Date": "2020/04/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f0ff56e9e240bc5758a4fd206256fbdfe0ce9cfd",
    "Root Cause": "N.A",
    "Bug report": "Update TFLite converter bug template to add a note about RNN conversion\n\nPiperOrigin-RevId: 307648781\nChange-Id: I686965788cc35bc2c40bd76062902f6bdd84f605",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+**RNN conversion support**\n+If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\n+",
    "Label": "clean"
},
{
    "Id": 526,
    "Library": "tensorflow",
    "Date": "2020/04/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/51d480498b07346b8b6e2ee3fbd3dc486f60ed96",
    "Root Cause": "N.A",
    "Bug report": "[TF2XLA] Do not promote autoclustering on compilation failure\n\nWhen the explicit compilation scope fails, the right solution is to try to compile less (or to file a bug).\nSuggesting autoclustering in these cases is rarely helpful.\n\nPiperOrigin-RevId: 307089449\nChange-Id: I082e9303cd3cff438ab6d56beaf39179e8632cc6",
    "Number of deleted lines": 7,
    "Deleted lines": "-    if (!s.ok() && (platform_info_.device_type().type_string() == DEVICE_CPU ||\n-                    platform_info_.device_type().type_string() == DEVICE_GPU)) {\n-      // Suggest auto jit if the failure was with GPU or CPU.\n-      errors::AppendToMessage(&s,\n-                              xla::status_macros::kPossibleAutoJitAlternative);\n-    }\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 527,
    "Library": "tensorflow",
    "Date": "2020/04/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/041dd6eac4f4abc69fb064bf3d2d0f7a48091eae",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Fix async copy ordering / outstanding async copy limit accounting.\n\nPrefetches to alternate memory has four different times in schedule:\n\n(1)-------(2)---------(3)-------(4)\n\n(1) Time the value is produced in default mem.\n(2) Time the value starts to be copied from defalt memory to alternate memory (CopyStart).\n(3) Time the value must be copied to the alternate memory (CopyDone).\n(4) Time the value is used from alternate memory.\n\nIn accounting for asynchronous copy ordering (maintaining pipeline order) and\nlimiting the outstanding number of asynchronous copies at a given time, we were\nerroneously using (2) and (4) instead of (2) and (3). This CL fixes this issue.\n\nThis bug was preventing a lot of prefetches when cross-program prefetch was\nenabled. This was because cross-program prefetches use (2)=beginning of program\nand (4)=end of program.\n\nPiperOrigin-RevId: 307063888\nChange-Id: I767b3adf9f1fbe5dfaba061972939ab47c20bd22",
    "Number of deleted lines": 4,
    "Deleted lines": "-  pending_async_copies_.push_back({start_time, end_time, memory_space});\n-  async_copy_interval_tree_.Add(start_time, end_time, kDummyChunk);\n-                                              request.end_time)) {\n-                                  request.end_time)) {",
    "Added lines": "+  pending_async_copies_.push_back(\n+      {start_time, copy_done_schedule_before_time, memory_space});\n+  async_copy_interval_tree_.Add(start_time, copy_done_schedule_before_time,\n+                                kDummyChunk);\n+                                              request.latest_prefetch_time)) {\n+                                  request.latest_prefetch_time)) {",
    "Label": "clean"
},
{
    "Id": 528,
    "Library": "tensorflow",
    "Date": "2020/04/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/faa58d6e94cfee5c7b64a8a7b5309985696cca78",
    "Root Cause": "N.A",
    "Bug report": "Preserve dict ordering after regroup() with best effort\n\ndict ordering may or may not be guaranteed, depending on Python version and\nwhich dict implementation it is. But we still would like to preverve the\nordering on a best effort basis, so that problems caused by incorrect\nassumption on the ordering doesn't look that it works without distributed\nstrategy, but breaks with strategy, which is very hard to debug.\n\nPiperOrigin-RevId: 306898958\nChange-Id: I660f0cd19e23f7991f5d0420e7232a94701a076f",
    "Number of deleted lines": 3,
    "Deleted lines": "-    v0keys = set(v0.keys())\n-      assert set(v.keys()) == v0keys, (\"v[0].keys: %s  v[i].keys: %s\" %\n-                                       (v0keys, set(v.keys())))",
    "Added lines": "+    v0keys = v0.keys()\n+      assert set(v.keys()) == set(v0keys), (\"v[0].keys: %s  v[i].keys: %s\" %\n+                                            (set(v0keys), set(v.keys())))",
    "Label": "clean"
},
{
    "Id": 529,
    "Library": "tensorflow",
    "Date": "2020/04/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/84d9e0a32f7d0d6d1ba157eda1b53be605ad04a6",
    "Root Cause": "N.A",
    "Bug report": "fix a bug in conv_ops_test.py by handling ndims==2 case for conv bwd input",
    "Number of deleted lines": 4,
    "Deleted lines": "-      TensorShape src_tf_shape = MakeInputTfShape(context, src_tensor);\n-      string error_msg = \"Status: \" + std::to_string(e.status) +\n-                         \", message: \" + string(e.message) + \", in file \" +\n-                         string(__FILE__) + \":\" + std::to_string(__LINE__);",
    "Added lines": "+#include \"tensorflow/core/kernels/conv_grad_shape_utils.h\"\n+      TensorShape src_tf_shape;\n+      if (src_tensor.dim_size(0) == 2) {\n+        Conv2DBackpropComputeInputShape(src_tensor, filter_tensor.shape(),\n+                                        diff_dst_tensor.shape(),\n+                                        this->data_format_, &src_tf_shape);\n+      } else {\n+        src_tf_shape = MakeInputTfShape(context, src_tensor);\n+      }\n+\n+      string error_msg = \"Status: \" + std::to_string(e.status) + \", message: \" +\n+                         string(e.message) + \", in file \" + string(__FILE__) +\n+                         \":\" + std::to_string(__LINE__);",
    "Label": "clean"
},
{
    "Id": 530,
    "Library": "tensorflow",
    "Date": "2020/04/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/62ae2478670ff9b8f0c5b553afab9444b5dc59ae",
    "Root Cause": "N.A",
    "Bug report": "fix bug in keras when only pass run options will trigger segmentation fault",
    "Number of deleted lines": 8,
    "Deleted lines": "-  // Allocate a RunMetadata protobuf object to receive the metadata,\n-  // if the caller is expecting any.\n-  std::unique_ptr<RunMetadata> run_metadata_proto;\n-  if (run_metadata != nullptr) {\n-    run_metadata_proto.reset(new RunMetadata);\n-  }\n-                           run_metadata_proto.get());\n-    s = MessageToBuffer(*run_metadata_proto, run_metadata);",
    "Added lines": "+  RunMetadata run_metadata_proto;\n+                           &run_metadata_proto);\n+    s = MessageToBuffer(run_metadata_proto, run_metadata);",
    "Label": "clean"
},
{
    "Id": 531,
    "Library": "tensorflow",
    "Date": "2020/04/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a257b75fd33931fc838bf773bccf1ce31417b96",
    "Root Cause": "N.A",
    "Bug report": "Update XNNPACK to support armhf CPU\n\nThe following changes are applied.\n\n68eef3f Update XNNPACK and cpuinfo build rules for armhf\nd0cf9bd Replace VLAs with direct alloca calls\n0183625 Increase error tolerance in IBilinearMicrokernelTester\n1f4e461 F16 1x8 GEMM ld64 microkernel\nbcbae1e Minor fix in CMake configuration\n9f240d1 Fix typo in f32-vscaleextexp-test target in CMakeLists\n3fd4b29 Work around \"too many sections\" error in f32-igemm-minmax-test with MinGW\nc2cfb97 Port AlignedAllocator to Windows\nef25e75 Use generator expression for Release/Debug flags\n6ae741a Specify Windows 7+ API target in CMake config\n666c271 Specify -msse2 for PSIMD micro-kernels\n0952f41 Update FP16 dependency\n8fc8776 Suppress type narrowing warning in X8-ZIP SSE2 micro-kernel\nc5ee9ff Include missing <numeric> header in BinaryElementwiseOperatorTester\nbdc8099 Avoid arithmetics on void* in indirection buffer setup\nef3e7dc Replace __builtin_lrintf with lrintf\n504f594 Port xnn_aligned_deallocate to Windows\n57133c0 Port xnn_initialize to Windows\n2acf108 Avoid arithetics on void* in NCHW Convolution\n8ac2b3a Include immintrin.h in sources using _mm_undefined_ps\n\nPiperOrigin-RevId: 306545874\nChange-Id: Ic3f520005fea790d02ed01c38484da61e16cd30e",
    "Number of deleted lines": 4,
    "Deleted lines": "-        sha256 = \"246aa56afc5263f1d41fc4a3437ecd51b56f78e16421818961cf79e39431c1df\",\n-        strip_prefix = \"XNNPACK-b9d07cfa38af15c2abf564c980e00c965857ba21\",\n-            \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/b9d07cfa38af15c2abf564c980e00c965857ba21.zip\",\n-            \"https://github.com/google/XNNPACK/archive/b9d07cfa38af15c2abf564c980e00c965857ba21.zip\",",
    "Added lines": "+        sha256 = \"7019a386752afaa5dd941d17201c5ff863b6ff1e77539e8cfcff0d13647a9f4a\",\n+        strip_prefix = \"XNNPACK-68eef3f15735c07774b3722f7b1b1142cebc9fed\",\n+            \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/68eef3f15735c07774b3722f7b1b1142cebc9fed.zip\",\n+            \"https://github.com/google/XNNPACK/archive/68eef3f15735c07774b3722f7b1b1142cebc9fed.zip\",",
    "Label": "clean"
},
{
    "Id": 532,
    "Library": "tensorflow",
    "Date": "2020/04/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0538c9c9d481ecc9cdb248d351de4c816cea7c64",
    "Root Cause": "N.A",
    "Bug report": "Run \"xla-legalize-tf-with-tf2xla\" pass after native MLIR legalization pass\n\nThis way native legalization patterns are preferred in most cases if an op can be lowered by both the passes.\n\nEarlier, I was thinking of having \"xla-legalize-tf-with-tf2xla\" first to expose bugs and whitelisting only ops that are not natively supported. There would be some maintenance overhead in this approach so seems easier to run this pass only on ops that didn't get lowered natively.\n\nPiperOrigin-RevId: 306309276\nChange-Id: Ia7ccd147ab9923afba7b7b7e9bd4eeef2fa0c021",
    "Number of deleted lines": 5,
    "Deleted lines": "-  tf2xla.addPass(mlir::xla_hlo::createLegalizeTfWithTf2XlaPass(device_type));\n-  // We need to run LegalizeTFPass 2 times because first\n-  // LegalizeTFPass(allow_partial_conversion=true) can expose more graph pruning\n-  // and canonicalization opportunities that are necessary for the second\n-  // LegalizeTFPass(allow_partial_conversion=false) invocation.",
    "Added lines": "+\n+\n+  // Leverage tf2xla kernels for ops that didn't get lowered in the previous\n+  // legalization pass.\n+  tf2xla.addPass(mlir::xla_hlo::createLegalizeTfWithTf2XlaPass(device_type));\n+  tf2xla.addNestedPass<mlir::FuncOp>(mlir::createCanonicalizerPass());\n+\n+  // Run LegalizeTFPass again because the previous legalization passes can\n+  // expose more graph pruning and canonicalization opportunities that are\n+  // necessary for the second LegalizeTFPass(allow_partial_conversion=false)\n+  // invocation.",
    "Label": "clean"
},
{
    "Id": 533,
    "Library": "tensorflow",
    "Date": "2020/04/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5e4ce4f0776772798cbe0036b3b42a4aa416fabe",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug related to build TF Lite on RPI Zero.\n\nWhy:\r\n\r\n* Enable to build TF Lite on RPI Zero.\r\n\r\nThis change addresses the need by:\r\n\r\n* Changing compiler from arm-linux-gnueabi- to arm-linux-gnueabihf-.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    TARGET_TOOLCHAIN_PREFIX := arm-linux-gnueabi-",
    "Added lines": "+    TARGET_TOOLCHAIN_PREFIX := arm-linux-gnueabihf-",
    "Label": "clean"
},
{
    "Id": 534,
    "Library": "tensorflow",
    "Date": "2020/04/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7d5d21afd194e5bc39cf87cb03cc68e15c3b7750",
    "Root Cause": "N.A",
    "Bug report": "Disable failing test third_party/tensorflow/python/debug:dumping_callback_test_gpu\n\nPiperOrigin-RevId: 305755196\nChange-Id: I488ca8a0498793640b7a302d6c968f12e39d1e28",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"manual\",\n+        \"no_oss\",\n+        \"notap\",  # TODO(b/153671240)",
    "Label": "clean"
},
{
    "Id": 535,
    "Library": "tensorflow",
    "Date": "2020/04/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4a9bdc5193db51e17ec5b431e68bf16978b7d5da",
    "Root Cause": "N.A",
    "Bug report": "Fix compile for some -c dbg configurations\n\nWe don't have a consistent debug string for std::unique_ptr. We could add one, although for now this just avoids using one in a DCHECK.\n\nPiperOrigin-RevId: 304647384\nChange-Id: If012b1bb9cb80a75001cfa8f4eb3e9ebd42af184",
    "Number of deleted lines": 1,
    "Deleted lines": "-    DCHECK_EQ(finfo->pending_counts, nullptr);",
    "Added lines": "+    DCHECK_EQ(finfo->pending_counts.get(), nullptr);",
    "Label": "clean"
},
{
    "Id": 536,
    "Library": "tensorflow",
    "Date": "2020/04/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/50433098af6951f8559e2cb5f5bcf25d2c7eb922",
    "Root Cause": "N.A",
    "Bug report": "Clarify the error message a bit. That should hopefully make cases like #36792 easier to debug.\n\nPiperOrigin-RevId: 304620870\nChange-Id: I2b1c34438b188c2f43a077c27285d94aed45728c",
    "Number of deleted lines": 2,
    "Deleted lines": "-    raise NotImplementedError(\"Cannot convert a symbolic Tensor ({}) to a numpy\"\n-                              \" array.\".format(self.name))",
    "Added lines": "+    raise NotImplementedError(\n+        \"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\n+        \" This error may indicate that you're trying to pass a Tensor to\"\n+        \" a NumPy call, which is not supported\".format(self.name))",
    "Label": "clean"
},
{
    "Id": 537,
    "Library": "tensorflow",
    "Date": "2020/04/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/394b6bb7111df0b76deec019d46b68484372f154",
    "Root Cause": "N.A",
    "Bug report": "[tfls.util.image] Fix bug in Buffer->Bitmap conversion.\n\nPiperOrigin-RevId: 304529665\nChange-Id: Id844bd6822a2f60556963a9373b0066fc5f3184d",
    "Number of deleted lines": 4,
    "Deleted lines": "-      byte r = (byte) rgbValues[j++];\n-      byte g = (byte) rgbValues[j++];\n-      byte b = (byte) rgbValues[j++];\n-      intValues[i] = ((r << 16) | (g << 8) | b);",
    "Added lines": "+import android.graphics.Color;\n+      int r = rgbValues[j++];\n+      int g = rgbValues[j++];\n+      int b = rgbValues[j++];\n+      intValues[i] = Color.rgb(r, g, b);",
    "Label": "clean"
},
{
    "Id": 538,
    "Library": "tensorflow",
    "Date": "2020/04/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0ebf54a8dd0f8cb1a8a0f295b4f6d0a8a2bb00be",
    "Root Cause": "N.A",
    "Bug report": "Use tf_grpc_cc_dependency in tensorflow/core/debug\n\nPiperOrigin-RevId: 304481760\nChange-Id: I5c4e7158606546e7280cab6cd9eb470d5d437a99",
    "Number of deleted lines": 2,
    "Deleted lines": "-        \"//tensorflow:grpc++\",\n-        \"//tensorflow:grpc++\",",
    "Added lines": "+load(\"//tensorflow:tensorflow.bzl\", \"tf_grpc_cc_dependency\")\n+        tf_grpc_cc_dependency(),\n+        tf_grpc_cc_dependency(),",
    "Label": "clean"
},
{
    "Id": 539,
    "Library": "tensorflow",
    "Date": "2020/02/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f0fa15e9466f747bda563e7cd85143707d86e628",
    "Root Cause": "N.A",
    "Bug report": "Fix new latent bug.",
    "Number of deleted lines": 1,
    "Deleted lines": "-                return emit_elem_function(source_idx_x, y_loc, x_loc, j);",
    "Added lines": "+                return emit_elem_function(source_idx_x, y_loc, x_loc, old_j);",
    "Label": "clean"
},
{
    "Id": 540,
    "Library": "tensorflow",
    "Date": "2020/03/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/983a5e245e703ba8b6dd84007fed6e641f741ab0",
    "Root Cause": "N.A",
    "Bug report": "Change DistributedVarOp to be a non-tuple class\n\nSome functions takes both update_op and (old_value, new_value) and uses istuple\nto dispatch them. We need var.assign().op not to be a tuple.\n\nCleaning up DistributedVarOp turns out more complicated than anticipated.\nThere're a lot of mysterious test failures. Let's not block fixing variable\nassign bug on them.\n\nPiperOrigin-RevId: 302828354\nChange-Id: I924cb5ee117b2c67f4161df6eb695a7c54bd7cfe",
    "Number of deleted lines": 3,
    "Deleted lines": "-import collections\n-DistributedVarOp = collections.namedtuple(\n-    \"DistributedVarOp\", [\"name\", \"graph\", \"traceback\", \"type\"])",
    "Added lines": "+class DistributedVarOp(object):\n+  \"\"\"A class that looks like `tf.Operation`.\"\"\"\n+\n+  def __init__(self, name, graph, traceback, typ):\n+    self.name = name\n+    self.graph = graph\n+    self.traceback = traceback\n+    self.type = typ\n+\n+  def __eq__(self, o):\n+    if not isinstance(o, self.__class__):\n+      raise NotImplementedError\n+    return (self.name == o.name and self.graph == o.graph and\n+            self.traceback == o.traceback and self.type == o.type)\n+\n+  def __hash__(self):\n+    return hash((self.name, self.graph, self.traceback, self.type))",
    "Label": "clean"
},
{
    "Id": 541,
    "Library": "tensorflow",
    "Date": "2020/03/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dab07567118ff04e7865edb3ca3f1835e6e6bb87",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in lift_to_graph.\n\nPiperOrigin-RevId: 301919704\nChange-Id: I35c08facb9523101681a57587d207bd10d0f4441",
    "Number of deleted lines": 2,
    "Deleted lines": "-        if inp.name == \"TPUReplicateMetadata\":\n-        if mutation.old_graph_op.name == \"TPUReplicateMetadata\":",
    "Added lines": "+        if inp.type == \"TPUReplicateMetadata\":\n+        if mutation.old_graph_op.type == \"TPUReplicateMetadata\":",
    "Label": "clean"
},
{
    "Id": 542,
    "Library": "tensorflow",
    "Date": "2020/03/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a7ad6fe1231291fb139f353b7a596a431c1b1b9",
    "Root Cause": "N.A",
    "Bug report": "[MLIR][XLA] Fix ops erase bug in DeadTempBufferRemoval",
    "Number of deleted lines": 2,
    "Deleted lines": "-      // TODO(herhut): There should be a generic helper for this.\n-      recursiveErase(allocOp);",
    "Added lines": "+    llvm::SmallVector<mlir::Operation *, 8> opsToErase;\n+      opsToErase.push_back(allocOp);\n+\n+    for (auto *op : opsToErase) {\n+      // TODO(herhut): There should be a generic helper for this.\n+      recursiveErase(op);\n+    }",
    "Label": "clean"
},
{
    "Id": 543,
    "Library": "tensorflow",
    "Date": "2020/03/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2f7bee60493424147af553d965a26aada5bf426",
    "Root Cause": "N.A",
    "Bug report": "fix a bug in AddSymbolicGradients",
    "Number of deleted lines": 3,
    "Deleted lines": "-    size_t dx_index = 0;\n-      if (dx_index == dx.size()) {\n-          BackpropAlongEdge(dx[dx_index++], {e->src(), e->src_output()}));",
    "Added lines": "+      int dx_index = e->dst_input();\n+      if (dx_index >= dx.size()) {\n+          BackpropAlongEdge(dx[dx_index], {e->src(), e->src_output()}));",
    "Label": "clean"
},
{
    "Id": 544,
    "Library": "tensorflow",
    "Date": "2020/03/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e25955350c1da44497fa9fb6c96999bc9cf4fd61",
    "Root Cause": "N.A",
    "Bug report": "[tfdbg2] Instrument the usage of tf.debugging.enable_check_numerics()\n\nPiperOrigin-RevId: 301193111\nChange-Id: I0e05de729dfa0633dad91e09c05a77bb765798cc",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+from tensorflow.python.eager import monitoring\n+_check_numerics_callback_create_counter = monitoring.Counter(\n+    \"/tensorflow/api/python/debugging/check_numerics_callback_create_counter\",\n+    \"Counter for number of times the check_numerics op callback is created.\")\n+\n+  _check_numerics_callback_create_counter.get_cell().increase_by(1)",
    "Label": "clean"
},
{
    "Id": 545,
    "Library": "tensorflow",
    "Date": "2020/03/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/febcaebba76a141348ea3a8d021e37380ac4c1cf",
    "Root Cause": "N.A",
    "Bug report": "Fix bug which resulted in windows py3.8 pips not being uploaded.\n\nPart of the solution for #33374.\n\nPiperOrigin-RevId: 300937663\nChange-Id: If134272240c160e3c6a739d0dc4a80b782789901",
    "Number of deleted lines": 2,
    "Deleted lines": "-for f in $(ls \"${TF_FILE_DIR}\"/tf_nightly_cpu*dev*cp3*-cp3*m-win_amd64.whl); do\n-for f in $(ls \"${TF_FILE_DIR}\"/tf_nightly*dev*cp3*-cp3*m-win_amd64.whl); do",
    "Added lines": "+for f in $(ls \"${TF_FILE_DIR}\"/tf_nightly_cpu*dev*cp3*-cp3*-win_amd64.whl); do\n+for f in $(ls \"${TF_FILE_DIR}\"/tf_nightly*dev*cp3*-cp3*-win_amd64.whl); do",
    "Label": "clean"
},
{
    "Id": 546,
    "Library": "tensorflow",
    "Date": "2020/03/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a9c3a9f30e738d5e63fa2ed4b22ec69880341150",
    "Root Cause": "N.A",
    "Bug report": "Fixing a bug in concat",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#ifdef ENABLE_MKLDNN_V1\n+      // Temporally call Eigen if number of input dimensions is 2.\n+      // That is due to an incorrect output results in DNNL 1.2 path.\n+      if (expected_dims == 2) invoke_eigen = true;\n+#endif  // ENABLE_MKLDNN_V1\n+",
    "Label": "clean"
},
{
    "Id": 547,
    "Library": "tensorflow",
    "Date": "2020/03/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a389e90cccf9c3758f8dd39facedb7edadf42f64",
    "Root Cause": "N.A",
    "Bug report": "Disabling tsan test until bug fix lands.\n\nPiperOrigin-RevId: 300400426\nChange-Id: Ic1190e056c2a56cd18fa54996892bdc570aec0fd",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"notsan\",  # TODO(b/151240519) Re-enable.",
    "Label": "clean"
},
{
    "Id": 548,
    "Library": "tensorflow",
    "Date": "2020/03/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/20b3b6b5bb516b6cc64ff791a9526d327b66bf99",
    "Root Cause": "N.A",
    "Bug report": "remove fPIC, remove debug echo",
    "Number of deleted lines": 4,
    "Deleted lines": "-\techo $(THIRD_PARTY_TARGETS)\n-\t$(CXX) $(CXXFLAGS) $(INCLUDES) -fPIC -c $< -o $@\n-\t$(CC) $(CCFLAGS) $(INCLUDES) -fPIC  -c $< -o $@\n-\t$(CC) $(CCFLAGS) $(INCLUDES) -fPIC -c $< -o $@",
    "Added lines": "+\t$(CXX) $(CXXFLAGS) $(INCLUDES) -c $< -o $@\n+\t$(CC) $(CCFLAGS) $(INCLUDES) -c $< -o $@\n+\t$(CC) $(CCFLAGS) $(INCLUDES) -c $< -o $@",
    "Label": "clean"
},
{
    "Id": 549,
    "Library": "tensorflow",
    "Date": "2020/03/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/de960f4090270d02a7f077f13a6788d68f469981",
    "Root Cause": "N.A",
    "Bug report": "Print the whole module in TF2XLA logging\n\nThe logging is intended to help with debugging, therefore save the entire module\n(instead of single functions) and disable multi-threading if logging is\nactivated which helps with reproducibility.\n\ncl/298430951 and cl/298000569 added the same behavior in phase 1 and 2 of the\nbridge.\n\nPiperOrigin-RevId: 299458773\nChange-Id: I8b752723253d0870b4597f605bc2e026992c9f75",
    "Number of deleted lines": 2,
    "Deleted lines": "-  if (VLOG_IS_ON(1))\n-    tf2xla.enableIRPrinting(std::make_unique<tensorflow::BridgeLoggerConfig>());",
    "Added lines": "+  if (VLOG_IS_ON(1)) {\n+    // Print the whole module after each pass which requires disabling\n+    // multi-threading as well.\n+    tf2xla.disableMultithreading();\n+    tf2xla.enableIRPrinting(std::make_unique<tensorflow::BridgeLoggerConfig>(\n+        /*print_module_scope=*/true));\n+  }",
    "Label": "clean"
},
{
    "Id": 550,
    "Library": "tensorflow",
    "Date": "2020/03/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/17e9b7adf5c333900c1d6b49259b2e774832ed7e",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug which may cause segmentation fault",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      if (end > node.input_size()) {\n+        return errors::FailedPrecondition(\n+              \"Got attr N=\", n, \" without enough inputs.\");\n+      }",
    "Label": "clean"
},
{
    "Id": 551,
    "Library": "tensorflow",
    "Date": "2020/03/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5db7460fa8dddcc139684b404f37cc20725a1419",
    "Root Cause": "N.A",
    "Bug report": "Add the kSnappyReaderInputBufferSizeBytes and kSnappyReaderoutputBufferSizeBytes defined in .h file to the .cc file as well. Currently build in debug mode will fail because .cc file can not find consts defined in .h file in build time.\n\nPiperOrigin-RevId: 299412527\nChange-Id: I933b9d9f56c0626f7f4134a2afc483dee159daf9",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+/* static */ constexpr const int64 Reader::kSnappyReaderInputBufferSizeBytes;\n+/* static */ constexpr const int64 Reader::kSnappyReaderOutputBufferSizeBytes;\n+",
    "Label": "clean"
},
{
    "Id": 552,
    "Library": "tensorflow",
    "Date": "2020/03/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8a3be2333442fc9405b00c8afbb2dcef39cf412d",
    "Root Cause": "N.A",
    "Bug report": "Update activations.cc\n\nDelete a debugging line.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    std::cout << \"Quantized Q alpha value is: \" << int(data->q_alpha) << '\\n';",
    "Added lines": "+",
    "Label": "clean"
},
{
    "Id": 553,
    "Library": "tensorflow",
    "Date": "2020/02/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/09af0ebd6bf20a67731c23eaa7930db10bdaf823",
    "Root Cause": "N.A",
    "Bug report": "Fix new latent bug.",
    "Number of deleted lines": 1,
    "Deleted lines": "-                return emit_elem_function(source_idx_x, y_loc, x_loc, j);",
    "Added lines": "+                return emit_elem_function(source_idx_x, y_loc, x_loc, old_j);",
    "Label": "clean"
},
{
    "Id": 554,
    "Library": "tensorflow",
    "Date": "2020/03/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f6ff46ce76ac83c3e305e30c6f7a199dd5f3e1eb",
    "Root Cause": "N.A",
    "Bug report": "Fix bugs in tf.ragged.cross that impacts windows:\n* Explicitly convert hash key from uint64 -> int64 before passing to generated op.\n* Pass DType to generated op as an enum (not a DType object).\n\nPiperOrigin-RevId: 298870119\nChange-Id: Ie25040509e8b666e8be673fa9422ee6e2afc2eb0",
    "Number of deleted lines": 2,
    "Deleted lines": "-        out_values_type=out_values_type,\n-        out_row_splits_type=out_row_splits_type,",
    "Added lines": "+    # Convert hash_key from uint64 -> int64, since we need to pass it via\n+    # an int64 attr.\n+    if hash_key > 2**63:\n+      hash_key -= 2**64\n+\n+        out_values_type=out_values_type.as_datatype_enum,\n+        out_row_splits_type=out_row_splits_type.as_datatype_enum,",
    "Label": "clean"
},
{
    "Id": 555,
    "Library": "tensorflow",
    "Date": "2020/03/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6df2a7e47210ac0b2d55b15c4e9f20a3424b5779",
    "Root Cause": "N.A",
    "Bug report": "Print the whole module in the bridge logging\n\nThe logging is intended to help debugging, however printing only single function\ndoes not help with reproducibility. Instead we will save the entire module, which\nrequires also to disable multi-threading for obvious reasons.\n\ncl/298000569 added the same behavior in phase2 of the bridge, this is the same for\nphase1.\n\nPiperOrigin-RevId: 298430951\nChange-Id: I14c86772920bcfd0c603fee6c7fdc21afc40b768",
    "Number of deleted lines": 13,
    "Deleted lines": "-\n-  // Add logger to bridge passmanager.\n-  if (enable_logging)\n-    bridge.enableIRPrinting(std::make_unique<tensorflow::BridgeLoggerConfig>());\n-\n-  // Add logger to bridge passmanager.\n-  if (enable_logging) {\n-    // Print the whole module after each pass, which requires disabling\n-    // multi-threading as well.\n-    bridge.disableMultithreading();\n-    bridge.enableIRPrinting(std::make_unique<tensorflow::BridgeLoggerConfig>(\n-        /* print_module_scope=*/true));\n-  }",
    "Added lines": "+namespace {\n+// Add logger to bridge passmanager.\n+void EnableLogging(PassManager *pm) {\n+  // Print the whole module after each pass, which requires disabling\n+  // multi-threading as well.\n+  pm->disableMultithreading();\n+  pm->enableIRPrinting(std::make_unique<tensorflow::BridgeLoggerConfig>(\n+      /*print_module_scope=*/true));\n+}\n+}  // namespace\n+\n+  if (enable_logging) EnableLogging(&bridge);\n+  if (enable_logging) EnableLogging(&bridge);",
    "Label": "clean"
},
{
    "Id": 556,
    "Library": "tensorflow",
    "Date": "2020/02/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6d54799412e4c4dbde82af54ee523452000bd82b",
    "Root Cause": "N.A",
    "Bug report": "Print the whole module in the bridge logging\n\nThe logging is intended to help debugging, however printing only single function\ndoes not help with reproducibility. Instead we will save the entire module, which\nrequires also to disable multi-threading for obvious reasons.\n\nPiperOrigin-RevId: 298068695\nChange-Id: I5af69edf20e7028c4551b12915537b8389bf65e1",
    "Number of deleted lines": 2,
    "Deleted lines": "-  if (enable_logging)\n-    bridge.enableIRPrinting(std::make_unique<tensorflow::BridgeLoggerConfig>());",
    "Added lines": "+  if (enable_logging) {\n+    // Print the whole module after each pass, which requires disabling\n+    // multi-threading as well.\n+    bridge.disableMultithreading();\n+    bridge.enableIRPrinting(std::make_unique<tensorflow::BridgeLoggerConfig>(\n+        /* print_module_scope=*/true));\n+  }",
    "Label": "clean"
},
{
    "Id": 557,
    "Library": "tensorflow",
    "Date": "2020/02/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3a8c575c4efe57560f36670ddc028f6fc889060a",
    "Root Cause": "N.A",
    "Bug report": "Disable failing tf debugger tests on windows\n\nPiperOrigin-RevId: 297949337\nChange-Id: I93ccec356d5ad785e24495cde0a7c9e323fdc314",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    tags = [\"no_windows\"],\n+    tags = [\"no_windows\"],",
    "Label": "clean"
},
{
    "Id": 558,
    "Library": "tensorflow",
    "Date": "2020/02/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9a4a5569e43f2ac6e14d2dafcf75d35f87c6d976",
    "Root Cause": "N.A",
    "Bug report": "Add temporary vlog messages for debugging.\n\nPiperOrigin-RevId: 297894920\nChange-Id: I46cb40e46276a98b40616e7bb89eacfd635f8940",
    "Number of deleted lines": 4,
    "Deleted lines": "-  VLOG(1) << \"ConvertXSpaceToTraceEvents\";\n-    VLOG(1) << \"  XPlane id=\" << xplane.Id() << \" name=\" << xplane.Name();\n-      VLOG(1) << \"    XLine id=\" << xline.Id() << \" name=\" << xline.Name()\n-              << \" display_id=\" << xline.DisplayId();",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 559,
    "Library": "tensorflow",
    "Date": "2020/02/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9c6cab4d2f57bd0328a9bee51a29c0992afd944e",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in deserializing tensorflow_addons optimizers\n\nPiperOrigin-RevId: 297848348\nChange-Id: I73b113ca1039faf68d2905ab3ef34125f04db2a2",
    "Number of deleted lines": 2,
    "Deleted lines": "-            'class_name': model.optimizer.__class__.__name__,\n-            'config': model.optimizer.get_config()",
    "Added lines": "+            'class_name':\n+                generic_utils.get_registered_name(model.optimizer.__class__),\n+            'config':\n+                model.optimizer.get_config()",
    "Label": "clean"
},
{
    "Id": 560,
    "Library": "tensorflow",
    "Date": "2020/02/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a1bec16fd83e35f50f681c20d7d2a43947cfd35b",
    "Root Cause": "N.A",
    "Bug report": "Add temporary vlog messages for debugging.\n\nPiperOrigin-RevId: 297758655\nChange-Id: Ie7a118557672a95e7700c0ed499f0e61af176cae",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  VLOG(1) << \"ConvertXSpaceToTraceEvents\";\n+    VLOG(1) << \"  XPlane id=\" << xplane.Id() << \" name=\" << xplane.Name();\n+      VLOG(1) << \"    XLine id=\" << xline.Id() << \" name=\" << xline.Name()\n+              << \" display_id=\" << xline.DisplayId();",
    "Label": "clean"
},
{
    "Id": 561,
    "Library": "tensorflow",
    "Date": "2020/02/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e571da0e46fd95023e21cf140c240bf8bb60654f",
    "Root Cause": "N.A",
    "Bug report": "fix resize image bug",
    "Number of deleted lines": 8,
    "Deleted lines": "-    size_const_as_shape = tensor_util.constant_value_as_shape(size)\n-    new_height_const = size_const_as_shape.dims[0].value\n-    new_width_const = size_const_as_shape.dims[1].value\n-          math_ops.cast(new_height_const, dtypes.float32) /\n-          math_ops.cast(new_width_const, dtypes.float32) /\n-      size_const_as_shape = tensor_util.constant_value_as_shape(size)\n-      new_height_const = size_const_as_shape.dims[0].value\n-      new_width_const = size_const_as_shape.dims[1].value",
    "Added lines": "+          math_ops.cast(size[0], dtypes.float32) /\n+          math_ops.cast(size[1], dtypes.float32) /\n+\n+    size_const_as_shape = tensor_util.constant_value_as_shape(size)\n+    new_height_const = size_const_as_shape.dims[0].value\n+    new_width_const = size_const_as_shape.dims[1].value",
    "Label": "clean"
},
{
    "Id": 562,
    "Library": "tensorflow",
    "Date": "2020/02/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6ea53a1cfe2dfa6b26f56c7ce4c5fb946b95c847",
    "Root Cause": "N.A",
    "Bug report": "fix bug",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      self._step_stats = self._origin_step_stats",
    "Label": "clean"
},
{
    "Id": 563,
    "Library": "tensorflow",
    "Date": "2020/02/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0e4f3a99d439c41add66513aa065e5c0656167d7",
    "Root Cause": "N.A",
    "Bug report": "Add a TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE debug flag\n\nIf set, TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE lets the TensorRT bridge lower\ninefficient transpose operations.\n\nPiperOrigin-RevId: 296773482\nChange-Id: Ifee5ce2d24de996336e753c48a7229fccff749ab",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (tensor_size > kMaxEfficientTranspose) {",
    "Added lines": "+#include \"tensorflow/core/util/env_var.h\"\n+bool AllowInefficientTranspose() {\n+  static bool result = [] {\n+    bool value;\n+    Status status =\n+        ReadBoolFromEnvVar(\"TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE\",\n+                           /*default_value=*/false, &value);\n+    if (!status.ok()) {\n+      LOG(ERROR) << status;\n+    }\n+    return value;\n+  }();\n+\n+  return result;\n+}\n+\n+  if (!AllowInefficientTranspose() && tensor_size > kMaxEfficientTranspose) {",
    "Label": "clean"
},
{
    "Id": 564,
    "Library": "tensorflow",
    "Date": "2020/02/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ab963689c3b3f0399304dcd9af33cd84505e2358",
    "Root Cause": "N.A",
    "Bug report": "Toggle legalize while default while debugging a failing case.\n\nPiperOrigin-RevId: 296470543\nChange-Id: I05b85a55deaf1733fedb4432e93fe9fbd854e05a",
    "Number of deleted lines": 1,
    "Deleted lines": "-        legalize_tf_while(true) {}",
    "Added lines": "+        legalize_tf_while(false) {}",
    "Label": "clean"
},
{
    "Id": 565,
    "Library": "tensorflow",
    "Date": "2020/02/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9b53275852f37fb1f48e22ed99237f1baef761c5",
    "Root Cause": "N.A",
    "Bug report": "Remove left-over debugging.\n\nPiperOrigin-RevId: 296396471\nChange-Id: I5478bef24e53a2eb80f4d84774c8bfa58273fc6f",
    "Number of deleted lines": 1,
    "Deleted lines": "-      sml_dim.dump();",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 566,
    "Library": "tensorflow",
    "Date": "2019/12/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/520533a52bd067a5c785e5e021981eb96eddf6dc",
    "Root Cause": "N.A",
    "Bug report": "Fix debug path",
    "Number of deleted lines": 1,
    "Deleted lines": "-#include \"tensorflow/lite/experimental/micro/debug_log.h\"",
    "Added lines": "+#include \"tensorflow/lite/micro/debug_log.h\"",
    "Label": "clean"
},
{
    "Id": 567,
    "Library": "tensorflow",
    "Date": "2019/11/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9903797166938a3f30e9cfb73d669b969f990eed",
    "Root Cause": "N.A",
    "Bug report": "Update debug log",
    "Number of deleted lines": 4,
    "Deleted lines": "-<<<<<<< HEAD\n-=======\n-  puts(s);\n->>>>>>> Change printf to puts",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 568,
    "Library": "tensorflow",
    "Date": "2020/02/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e85f354bba3e20224a1bd3df91b47161c8218592",
    "Root Cause": "N.A",
    "Bug report": "Remove spurious std:cerr debugging statement.\n\nPiperOrigin-RevId: 296234228\nChange-Id: I4817dbcaf48ab37fe68df18bad5f030746099341",
    "Number of deleted lines": 1,
    "Deleted lines": "-    std::cerr << \"WOWZA: \" << pair.first << std::endl;",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 569,
    "Library": "tensorflow",
    "Date": "2020/02/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/59840cf101741aac00070a066259bf0b6d4d17ec",
    "Root Cause": "N.A",
    "Bug report": "Return \"argX\" for BlockArgument in OpOrArgLocNameMapper::GetName.\n\nThis is for printing better debugging information.\n\nPiperOrigin-RevId: 295825438\nChange-Id: I9b049656aa11a20692d328bcea9a8adf2a5bf1fd",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // Use the ASM syntax for BloackArgument\n+  if (auto arg = val.dyn_cast<mlir::BlockArgument>()) {\n+    return \"arg\" + std::to_string(arg.getArgNumber());\n+  }",
    "Label": "clean"
},
{
    "Id": 570,
    "Library": "tensorflow",
    "Date": "2020/02/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/27a9123a7f00b00f84957c0d09c83ddaa7585878",
    "Root Cause": "N.A",
    "Bug report": "Temporarily disabling a test until the bug fix lands.\n\nPiperOrigin-RevId: 295206645\nChange-Id: Iadcb0e02ed45ecde1ab3f6cbd43b98c15e1c72b4",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    tags = [\n+        \"no_oss\",  # TODO(b/149575105): Reenable when fix lands.\n+        \"notap\",  # TODO(b/149575105): Reenable when fix lands.\n+    ],",
    "Label": "clean"
},
{
    "Id": 571,
    "Library": "tensorflow",
    "Date": "2020/02/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c7ce71f168bb2be59ec7a22117cecb8466872960",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Fix a latent bug. Currently I think the code is fine, but the function doesn't do what its name say.",
    "Number of deleted lines": 1,
    "Deleted lines": "-          EmitCallToTargetIntrinsic(TargetIntrinsicID::kThreadIdx, {}, {}, b)));",
    "Added lines": "+          EmitCallToTargetIntrinsic(TargetIntrinsicID::kBlockIdx, {}, {}, b)));",
    "Label": "clean"
},
{
    "Id": 572,
    "Library": "tensorflow",
    "Date": "2020/02/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d7dfa9f16030d6d7a73f646fda4c4803ad451887",
    "Root Cause": "N.A",
    "Bug report": "Updated bug-issue template. Earlier I did update the template through GitHub PR but copybara is formatting the template and the template became unusable.\n\nPiperOrigin-RevId: 294949577\nChange-Id: I210efd9b519fb13f59fc9d664368ca1b07370de1",
    "Number of deleted lines": 2,
    "Deleted lines": "-\n-name: Bug Issue about: Use this template for reporting an issue related to bug.",
    "Added lines": "+---\n+name: Bug Issue\n+about: Use this template for reporting an issue related to bug\n+---",
    "Label": "clean"
},
{
    "Id": 573,
    "Library": "tensorflow",
    "Date": "2020/02/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4de896db9f9307c5db28e97a1fa210b5fee13143",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in _basename. I accidentally used a backslash on Linux and forwardslash on Windows.\n\nPiperOrigin-RevId: 294929053\nChange-Id: Ic7e77b415406e1429e970006c3b14ef5ccd586a8",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if (is_win and path_str[r_i] == \"/\") or path_str[r_i] == \"\\\\\":",
    "Added lines": "+        if (is_win and path_str[r_i] == \"\\\\\") or path_str[r_i] == \"/\":",
    "Label": "clean"
},
{
    "Id": 574,
    "Library": "tensorflow",
    "Date": "2020/02/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b5a3eda2dc4b16b61467ef4afd5f8211681133b3",
    "Root Cause": "N.A",
    "Bug report": "[ROCm] Fix for a bug in the ROCm implementation for matrix_triangular_solve op",
    "Number of deleted lines": 1,
    "Deleted lines": "-    uplo = lower ? rocblas_fill_upper : rocblas_fill_upper;",
    "Added lines": "+    uplo = lower ? rocblas_fill_upper : rocblas_fill_lower;",
    "Label": "clean"
},
{
    "Id": 575,
    "Library": "tensorflow",
    "Date": "2020/02/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/0b11e00b312be85da7aad1ad145ca7a8d32da4dc",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in padding segment for online yogi. The accumulator should be non-zero\nsince we are taking rsqrt.\n\nPiperOrigin-RevId: 294558421\nChange-Id: I4cc9639c4a53743920e80ae2281a08847d1f8d0e",
    "Number of deleted lines": 1,
    "Deleted lines": "-          MakeStandardStateVariableSpecification(\"vs\", 0.0));",
    "Added lines": "+          MakeStandardStateVariableSpecification(\"vs\", 0.1));",
    "Label": "clean"
},
{
    "Id": 576,
    "Library": "tensorflow",
    "Date": "2020/02/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/391b21e9c2bee4164f888c222978d35c666fedde",
    "Root Cause": "N.A",
    "Bug report": "Fix gpu test when running the ./configure script\n\nToday if you run the ./configure script without setting the\nvariable TF_NEED_CUDA, in the resulting .tf_configure.bazelrc\nyou get:\n\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\n\nThis is incorrect because -gpu means exclude the gpu test. It should be\n-no_gpu.\n\nDebugging the problem I found when the code was switched from using\nos.env to using environ_cp, that the method system_specific_test_config\nwas never updated. So unless the environment variable TF_NEED_CUDA was\nset before running ./configure then answering yes for CUDA would not\nselect the correct test filter for gpus.\n\nWith this change the test filters are correct:\n\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only",
    "Number of deleted lines": 4,
    "Deleted lines": "-def system_specific_test_config(env):\n-    if env.get('TF_NEED_CUDA', None) == '1':\n-    if env.get('TF_NEED_CUDA', None) == '1':\n-  system_specific_test_config(os.environ)",
    "Added lines": "+def system_specific_test_config(environ_cp):\n+    if environ_cp.get('TF_NEED_CUDA', None) == '1':\n+    if environ_cp.get('TF_NEED_CUDA', None) == '1':\n+  system_specific_test_config(environ_cp)",
    "Label": "clean"
},
{
    "Id": 577,
    "Library": "tensorflow",
    "Date": "2020/02/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/92671a470be3e2a6c799bc8fe68b25b322b4e98e",
    "Root Cause": "N.A",
    "Bug report": "Update tflite runtime with bug fix\n\nPiperOrigin-RevId: 294484091\nChange-Id: I73e7cd56dc2ea1a0ebcae96c9357a42eb8d76c34",
    "Number of deleted lines": 16,
    "Deleted lines": "-pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp37-cp37m-linux_armv7l.whl\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp35-cp35m-linux_armv7l.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp36-cp36m-linux_armv7l.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp37-cp37m-linux_armv7l.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp35-cp35m-linux_aarch64.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp36-cp36m-linux_aarch64.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp37-cp37m-linux_aarch64.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp35-cp35m-linux_x86_64.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp36-cp36m-linux_x86_64.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp37-cp37m-linux_x86_64.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp35-cp35m-macosx_10_14_x86_64.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp36-cp36m-macosx_10_14_x86_64.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp37-cp37m-macosx_10_14_x86_64.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp35-cp35m-win_amd64.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp36-cp36m-win_amd64.whl</td>\n-  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp37-cp37m-win_amd64.whl</td>",
    "Added lines": "+pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp35-cp35m-linux_armv7l.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp36-cp36m-linux_armv7l.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp35-cp35m-linux_aarch64.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp36-cp36m-linux_aarch64.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_aarch64.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp35-cp35m-linux_x86_64.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp36-cp36m-linux_x86_64.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_x86_64.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp35-cp35m-macosx_10_14_x86_64.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp36-cp36m-macosx_10_14_x86_64.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-macosx_10_14_x86_64.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp35-cp35m-win_amd64.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp36-cp36m-win_amd64.whl</td>\n+  <td>https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-win_amd64.whl</td>",
    "Label": "clean"
},
{
    "Id": 578,
    "Library": "tensorflow",
    "Date": "2020/02/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/376007fad4f675b9c530197b95a66313775f28d9",
    "Root Cause": "N.A",
    "Bug report": "Update and rename 00-bug-performance-issue.md to 00-bug-issue.md\n\nUpdate and rename 00-bug-performance-issue.md to 00-bug-issue.md",
    "Number of deleted lines": 2,
    "Deleted lines": "-name: Bug/Performance Issue\n-about: Use this template for reporting a bug or a performance issue.",
    "Added lines": "+name: Bug Issue\n+about: Use this template for reporting an issue related to bug.\n+labels: 'type:bug'",
    "Label": "clean"
},
{
    "Id": 579,
    "Library": "tensorflow",
    "Date": "2020/02/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d26ee9801c8117f7fd6297a05a82eab98023a2c3",
    "Root Cause": "N.A",
    "Bug report": "bug fix in the ROCm python implementation for gpu_lstm op",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    # full_bias is a tensor of shape (8*n,)\n+    full_bias = array_ops.split(full_bias, 8, axis=0)",
    "Label": "clean"
},
{
    "Id": 580,
    "Library": "tensorflow",
    "Date": "2020/02/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b41270fccce0e2bb5ca97aedd2a31dd191f35803",
    "Root Cause": "N.A",
    "Bug report": "TFLM: Memory map tensor name from the flatbuffer model data.\n\nThe current code sets tensor name to <no name> if it's missing from the model. This is slightly wasteful. We could instead display <no name> on the debugging side.\n\nPiperOrigin-RevId: 294165766\nChange-Id: I7e77b61a0437abc4d29098111b1c65fdef0b73eb",
    "Number of deleted lines": 6,
    "Deleted lines": "-  // Copy the name, if there is one.\n-  if (flatbuffer_tensor.name()->c_str() != nullptr) {\n-    result->name = flatbuffer_tensor.name()->c_str();\n-  } else {\n-    result->name = \"<No name>\";\n-  }",
    "Added lines": "+  result->name = flatbuffer_tensor.name()->c_str();",
    "Label": "clean"
},
{
    "Id": 581,
    "Library": "tensorflow",
    "Date": "2020/02/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cd964065fb03e0b74f338dd7d9a499d1e7544ffb",
    "Root Cause": "N.A",
    "Bug report": "Fix a subtle bug where we unsafely modify the list while iterating it.\n\nPiperOrigin-RevId: 293933459\nChange-Id: I0230df64b5dbfd03e941a0d19bd5d339b414cfff",
    "Number of deleted lines": 1,
    "Deleted lines": "-    for (auto& use : from.getUses()) {",
    "Added lines": "+    // TODO(jingpu): move this to RegionUtils.h in MLIR core.\n+    for (auto& use : llvm::make_early_inc_range(from.getUses())) {",
    "Label": "clean"
},
{
    "Id": 582,
    "Library": "tensorflow",
    "Date": "2020/02/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bcc5af092da9903b0ac476f9b03fa5d0c32d2fdd",
    "Root Cause": "N.A",
    "Bug report": "Keep loading functions in case of error.\n\nThat way the log will show all the functions that cannot be loaded, making\ndebugging easier.\nAlso removed redundant error message when function cannot be loaded.\n\nPiperOrigin-RevId: 293839634\nChange-Id: I0d3f39841ed7f0ffced80de37f361bd2919308c5",
    "Number of deleted lines": 7,
    "Deleted lines": "-#define LOAD_FUNCTION(dl_handle, method_name, hexagon_obj)           \\\n-  hexagon_obj.method_name = reinterpret_cast<method_name##_fn*>(     \\\n-      LoadFunction(dl_handle, #method_name));                        \\\n-  if ((hexagon_obj.method_name) == nullptr) {                        \\\n-    TFLITE_LOG_PROD(TFLITE_LOG_ERROR, \"%s is NULL\", (#method_name)); \\\n-    return hexagon_obj;                                              \\\n-  hexagon_nn.interface_loaded = true;",
    "Added lines": "+#define LOAD_FUNCTION(dl_handle, method_name, hexagon_obj)       \\\n+  hexagon_obj.method_name = reinterpret_cast<method_name##_fn*>( \\\n+      LoadFunction(dl_handle, #method_name));                    \\\n+  if ((hexagon_obj.method_name) == nullptr) {                    \\\n+    successfully_loaded = false;                                 \\\n+  // The flag will be set to false if a function cannot be loaded.\n+  bool successfully_loaded = true;\n+  hexagon_nn.interface_loaded = successfully_loaded;",
    "Label": "clean"
},
{
    "Id": 583,
    "Library": "tensorflow",
    "Date": "2020/02/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/dc94b3a69e8988e5db1994ce607b893f59d6f2d9",
    "Root Cause": "N.A",
    "Bug report": "Fix the bug of putting an expression that should be executed in a DCHECK call.\n\nPiperOrigin-RevId: 293463048\nChange-Id: I966cf8f0f0e991bdcafdb069489c804af057d9b9",
    "Number of deleted lines": 1,
    "Deleted lines": "-    DCHECK_EQ(tensor::MakeShape(input_tensor, &input_tf_shape).ok(), true);",
    "Added lines": "+    TF_CHECK_OK(tensor::MakeShape(input_tensor, &input_tf_shape));",
    "Label": "clean"
},
{
    "Id": 584,
    "Library": "tensorflow",
    "Date": "2020/02/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/86c3de4eff9192d2ef44baad7aab996020e92ef6",
    "Root Cause": "N.A",
    "Bug report": "Simplify DebugString functions",
    "Number of deleted lines": 18,
    "Deleted lines": "-  string out = \"[\";\n-  for (auto dims: dimvec) {\n-    StrAppend(&out, DebugString(dims));\n-  }\n-  StrAppend(&out, \"]\");\n-  return out;\n-  string out = \"[\";\n-  for (auto shape: shapes) {\n-    StrAppend(&out, shape.DebugString());\n-  }\n-  StrAppend(&out, \"]\");\n-  return out;\n-  string out = \"[\";\n-  for (auto shape: shapes) {\n-    StrAppend(&out, shape.DebugString());\n-  }\n-  StrAppend(&out, \"]\");\n-  return out;",
    "Added lines": "+  return absl::StrCat(\"[\",\n+                      absl::StrJoin(dimvec, \",\",\n+                                    [](std::string* out, nvinfer1::Dims in)\n+                                    {out->append(DebugString(in));}),\n+                      \"]\");\n+  return TensorShapeUtils::ShapeListString(shapes);\n+  return PartialTensorShapeUtils::PartialShapeListString(shapes);",
    "Label": "clean"
},
{
    "Id": 585,
    "Library": "tensorflow",
    "Date": "2020/02/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/63d610da2b0319e425feb584896bafd7d8cce656",
    "Root Cause": "N.A",
    "Bug report": "Fix a small bug in Windows def file filter. We cannot rename `cc_lib` as we use it as a key later. We need to create a new variable `cc_target`.\n\nPiperOrigin-RevId: 292990734\nChange-Id: Ifb5288d444c7782b292d1ed346e5b26c06d16791",
    "Number of deleted lines": 2,
    "Deleted lines": "-        cc_lib = path_to_lib[-1]\n-        if cc_lib in lib and parent_dir in lib:",
    "Added lines": "+        cc_target = path_to_lib[-1]\n+        if cc_target in lib and parent_dir in lib:",
    "Label": "clean"
},
{
    "Id": 586,
    "Library": "tensorflow",
    "Date": "2020/02/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c12da07ee88a68c77ca06ca058de029d2c78bb9e",
    "Root Cause": "N.A",
    "Bug report": "Disable debug_ops_test on windows.\n\nIt uses dirent.h, which does not exist on windows.\n\nPiperOrigin-RevId: 292710227\nChange-Id: I3ad236b6bb482fb110b0ff6d78b4ed727f7bfdbf",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    tags = [\"no_windows\"],",
    "Label": "clean"
},
{
    "Id": 587,
    "Library": "tensorflow",
    "Date": "2020/01/29",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a34091e538540aad57a7a941575538944f38db24",
    "Root Cause": "N.A",
    "Bug report": "Disable //tensorflow/python/debug:check_numerics_callback_test since it is flaky.\n\nPiperOrigin-RevId: 292235166\nChange-Id: Iaa6ec79f4699cd43e424b6c97f441f243a761a6a",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    tags = [\n+        \"notap\",  # b/148484328\n+    ],",
    "Label": "clean"
},
{
    "Id": 588,
    "Library": "tensorflow",
    "Date": "2020/01/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/057595bf7711b98e2655249f76462b55c1ce5ed3",
    "Root Cause": "N.A",
    "Bug report": "[XLA/GPU] Fix a bug in EmitPrintf\n\nThe bug causes us to use the input argument values while we should use the new\nvalues with the needed type promotion.\n\nPiperOrigin-RevId: 291193450\nChange-Id: I5f8a1d7013b2a1f434901214cc927eefc17ca8d6",
    "Number of deleted lines": 3,
    "Deleted lines": "-        arguments[i],\n-        builder->CreateGEP(arguments_ptr,\n-                           {builder->getInt64(0), builder->getInt32(i)}));",
    "Added lines": "+        value, builder->CreateGEP(arguments_ptr, {builder->getInt64(0),\n+                                                  builder->getInt32(i)}));",
    "Label": "clean"
},
{
    "Id": 589,
    "Library": "tensorflow",
    "Date": "2020/01/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d74dd1d4043d2238e3020817fe898b1346830649",
    "Root Cause": "N.A",
    "Bug report": "[XLA/GPU] Document EmitPrintf bug\n\nPiperOrigin-RevId: 290308836\nChange-Id: I7aa0762fd62bf59786784bbb71a28850e58b1fc8",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+// TODO(b/147893680): %f format specifier produces incorrect output, use %d.",
    "Label": "clean"
},
{
    "Id": 590,
    "Library": "tensorflow",
    "Date": "2020/01/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c6fa2dc9e4c3330aa3e21014efdba3a18108d51b",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug that kernel launch events are over calculated.\n\nPiperOrigin-RevId: 290185357\nChange-Id: I87ae6f8b43caf4678273e4d6e2f1b4c3c1d53a6f",
    "Number of deleted lines": 2,
    "Deleted lines": "-  int64 correlation_id = -1;\n-  int64 group_id = -1;",
    "Added lines": "+    int64 correlation_id = -1;\n+    int64 group_id = -1;",
    "Label": "clean"
},
{
    "Id": 591,
    "Library": "tensorflow",
    "Date": "2020/01/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/49e4e94f207938f3009175360467f0c8cb3f3a9a",
    "Root Cause": "N.A",
    "Bug report": "Minor bugfix after a change in DeviceAssignment types\n\nPiperOrigin-RevId: 290178875\nChange-Id: I2e4b215668b14d158c4011e2f42a47305f3bed2e",
    "Number of deleted lines": 1,
    "Deleted lines": "-  DeviceAssignment device_assignment = {1, 1};",
    "Added lines": "+  DeviceAssignment device_assignment = {NULL, 0};",
    "Label": "clean"
},
{
    "Id": 592,
    "Library": "tensorflow",
    "Date": "2020/01/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/34b72b3120e94224fe81dfa134fe7d99fc0fa29e",
    "Root Cause": "N.A",
    "Bug report": "Fix bug causing tf.sparse.expand_dims to crash for arguments of dynamic dense rank.\n\nPiperOrigin-RevId: 290049840\nChange-Id: I0a99bbf41e21f75511edefb75c49994a3323f963",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if rank is None:\n+    rank = array_ops.shape(sp_input.dense_shape)[0]",
    "Label": "clean"
},
{
    "Id": 593,
    "Library": "tensorflow",
    "Date": "2020/01/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a1e9c5a1454c35598080040da743a11b1cdc76c7",
    "Root Cause": "N.A",
    "Bug report": "Blacklist algorithms described in nvbugs/2774617.\n\nPiperOrigin-RevId: 289979133\nChange-Id: Iefcf78590b3abc56143068e597e46888d1683f44",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  entries {\n+    hlo: \"(f32[4,32,32,32]{2,1,3,0}, u8[0]{0}) custom-call(f32[4,32,32,32]{2,1,3,0}, f32[5,5,32,32]{1,0,2,3}), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_01io->b01f, custom_call_target=\\\"__cudnn$convForward\\\", backend_config=\\\"{conv_result_scale:1}\\\"\"\n+    cc { major: 7 }\n+    cudnn_version { major: 7 minor: 6 patch: 4 }\n+    algos { id: 7 }\n+    blas_version: \"10201\"\n+  }\n+  entries {\n+    hlo: \"(f32[4,32,32,32]{2,1,3,0}, u8[0]{0}) custom-call(f32[4,32,32,32]{2,1,3,0}, f32[5,5,32,32]{1,0,2,3}), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_01io->b01f, custom_call_target=\\\"__cudnn$convForward\\\", backend_config=\\\"{conv_result_scale:1}\\\"\"\n+    cc { major: 7 }\n+    cudnn_version { major: 7 minor: 6 patch: 4 }\n+    algos { id: 7 tensor_ops: true }\n+    blas_version: \"10201\"\n+  }",
    "Label": "clean"
},
{
    "Id": 594,
    "Library": "tensorflow",
    "Date": "2020/01/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6fdcd7e975c36baf99a0a071135aeaf5b4d6896e",
    "Root Cause": "N.A",
    "Bug report": "Fix dequantize xla kernel bug.\n\nPiperOrigin-RevId: 289936604\nChange-Id: I7796cd5908ca137a818aec91d777dc4d600af1d7",
    "Number of deleted lines": 1,
    "Deleted lines": "-      output = xla::ConvertElementType(input, xla::BF16);",
    "Added lines": "+      output = xla::ConvertElementType(output, xla::BF16);",
    "Label": "clean"
},
{
    "Id": 595,
    "Library": "tensorflow",
    "Date": "2020/01/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b213a59e16db7e9b9c943d19539fda05856e08d6",
    "Root Cause": "N.A",
    "Bug report": "Fix a subtle bug that the MLIR function name for the entry point is not \"main\".\n\nAccording to the function comment, the entry point in TFLite is always the first subgraph and the entry point in MLIR is always the \"main\" function. Thus, update the logic accordingly.\n\nPiperOrigin-RevId: 289797744\nChange-Id: Icf578d3068f4258b16d5fff52523d1473bf23b3b",
    "Number of deleted lines": 7,
    "Deleted lines": "-    if (index == 0) {\n-      return \"main\";\n-    } else {\n-      return llvm::formatv(\"fn_{0}\", index).str();\n-    }\n-  } else {\n-    return subgraph.name;",
    "Added lines": "+  if (index == 0) {\n+    return \"main\";\n+  }\n+    return llvm::formatv(\"fn_{0}\", index).str();\n+  return subgraph.name;",
    "Label": "clean"
},
{
    "Id": 596,
    "Library": "tensorflow",
    "Date": "2020/01/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b55f48a63b8f02c18f31cfe43b150c9172052abb",
    "Root Cause": "N.A",
    "Bug report": "[Grappler] Fix bug in node signature hash in arithmetic_optimizer. The order of (input_node, port) and (attribute name, attribute value) matters.\n\nPiperOrigin-RevId: 289522768\nChange-Id: I94f4f90a5c681faa172ee926999c72c9f0eff6ac",
    "Number of deleted lines": 5,
    "Deleted lines": "-    h = Hash64CombineUnordered(\n-        Hash64(input_tensor.node().data(), input_tensor.node().size()), h);\n-    h = Hash64CombineUnordered(std::hash<int>()(input_tensor.index()), h);\n-    h = Hash64CombineUnordered(Hash64(attr.first), h);\n-    h = Hash64CombineUnordered(FastAttrValueHash(attr.second), h);",
    "Added lines": "+    uint64 input_hash = Hash64Combine(\n+        Hash64(input_tensor.node().data(), input_tensor.node().size()),\n+        std::hash<int>()(input_tensor.index()));\n+    h = Hash64CombineUnordered(input_hash, h);\n+    uint64 attr_hash =\n+        Hash64Combine(Hash64(attr.first), FastAttrValueHash(attr.second));\n+    h = Hash64CombineUnordered(attr_hash, h);",
    "Label": "clean"
},
{
    "Id": 597,
    "Library": "tensorflow",
    "Date": "2020/01/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bed506e3a160402f4f93aa8fdfc4bb8b270a3953",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Fix race condition in RefcountingHashMap\n\nQuoting the bug from jlebar@:\n\n> Suppose the refcount of entry for key K goes to 0.  Then before the deleter is run, someone touches map[K], thus causing the refcount of this entry to go back to 1.  Then the deleter runs, deleting the object.  Boom.\n\nPiperOrigin-RevId: 289194684\nChange-Id: I3a1d9a8294d45eb1c554ee511328fc5a9d0b1e20",
    "Number of deleted lines": 12,
    "Deleted lines": "-    if (it == map_.end()) {\n-      // Create entry in the map and then set its value, so the value can\n-      // contain a pointer back into the map.\n-      it = map_.emplace(key, std::weak_ptr<V>()).first;\n-      std::shared_ptr<V> value(value_factory_(key).release(),\n-                               Deleter{&it->first, this});\n-      it->second = value;  // Set the weak ptr to the shared ptr.\n-      return value;\n-    return it->second.lock();\n-      CHECK(it != parent->map_.end());\n-      CHECK(it->second.expired());\n-      parent->map_.erase(it);",
    "Added lines": "+    // We ensure that the entry has not expired in case deleter was running when\n+    // we have entered this block.\n+    if (it != map_.end()) {\n+      if (std::shared_ptr<V> value = it->second.lock()) {\n+        return value;\n+      }\n+      map_.erase(it);\n+\n+    // Create entry in the map and then set its value, so the value can\n+    // contain a pointer back into the map.\n+    it = map_.emplace(key, std::weak_ptr<V>()).first;\n+    std::shared_ptr<V> value(value_factory_(key).release(),\n+                             Deleter{&it->first, this});\n+    it->second = value;  // Set the weak ptr to the shared ptr.\n+    return value;\n+      if (it != parent->map_.end() && it->second.expired()) {\n+        parent->map_.erase(it);\n+      }",
    "Label": "clean"
},
{
    "Id": 598,
    "Library": "tensorflow",
    "Date": "2020/01/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fcc4ff55c3dcb8602b5b8ac578dded48168197a3",
    "Root Cause": "N.A",
    "Bug report": "Fix build breakage due to missing static member definitions\n\nThe static constexpr datamember declarations in the various ToDataType\nspecialiations (in stream_executor/dnn.h) do not have a corresponding\ndefinition outside of the class.\nThis results in compilation failures in debug mode",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+constexpr DataType ToDataType<float>::value;\n+constexpr DataType ToDataType<double>::value;\n+constexpr DataType ToDataType<Eigen::half>::value;\n+constexpr DataType ToDataType<int8>::value;\n+constexpr DataType ToDataType<int32>::value;\n+",
    "Label": "clean"
},
{
    "Id": 599,
    "Library": "tensorflow",
    "Date": "2020/01/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/31abd783e6ff567e2a85b3a154a87780fa880a74",
    "Root Cause": "N.A",
    "Bug report": "[TF/XLA] Remove an accidentally introduced race condition\n\ncl/288939060 has introduced a race condition: previously, the lambda was only run once. This is the intend of the code.\n\nThanks to hyeontaek@ for discovering the bug.\n\nPiperOrigin-RevId: 289163903\nChange-Id: I35939dd84e5e789c5b72da0a8f2f28a0eaec0891",
    "Number of deleted lines": 2,
    "Deleted lines": "-  {\n-  }",
    "Added lines": "+  static void* registration_init = [&registry]() {\n+    return nullptr;\n+  }();\n+  (void)registration_init;",
    "Label": "clean"
},
{
    "Id": 600,
    "Library": "tensorflow",
    "Date": "2020/01/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4a310fb72cf4613448a3cc9ee4c2c717161b1812",
    "Root Cause": "N.A",
    "Bug report": "Temporary workaround until upstream integrate\n\nThis avoids a bug at an integrate and we can revert this once LLVM version\nreferenced is post it.\n\nPiperOrigin-RevId: 288503242\nChange-Id: Ia4b99bcbf7523503269405b09e576df7cd9deeda",
    "Number of deleted lines": 1,
    "Deleted lines": "-    rewriter.replaceOp(op, {op.GetYield().getOperand(0), nullptr});",
    "Added lines": "+    // TODO(jpienaar): Revert this, this accounts for an intermediate bug that\n+    // has already been fixed upstream but has not been integrated yet. The\n+    // second result is unused here and so should be removed, but just using\n+    // the same result in both places (which should not matter as unused).\n+    rewriter.replaceOp(\n+        op, {op.GetYield().getOperand(0), op.GetYield().getOperand(0)});",
    "Label": "clean"
},
{
    "Id": 601,
    "Library": "tensorflow",
    "Date": "2020/01/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/02bf45625f488682344c24789898735f86da85a4",
    "Root Cause": "N.A",
    "Bug report": "[Intel MKL] reverted the commit 782e12b7aa42015263370c7593df780dd917c776 which fixes for Fixed a bug in mkl_conv2d constant filter caching because it causes performance regression in a few models",
    "Number of deleted lines": 9,
    "Deleted lines": "-        bool do_cache_filter = src_dims[MklDnnDims::Dim_N] > kSmallBatchSize;\n-        if (is_filter_const_ && do_cache_filter) {\n-          if (filter_out_tensor != nullptr) {\n-            Tfilter* filter_out_tensor_buf =\n-                static_cast<Tfilter*>(const_cast<Tfilter*>(\n-                    filter_out_tensor->flat<Tfilter>().data()));\n-            memcpy(filter_out_tensor_buf, filter_data,\n-                   filter_out_tensor->AllocatedBytes());\n-          }",
    "Added lines": "+        if (is_filter_const_) {",
    "Label": "clean"
},
{
    "Id": 602,
    "Library": "tensorflow",
    "Date": "2020/01/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/728967326e45ae3896f17b27fb2cb6b9bce257c4",
    "Root Cause": "N.A",
    "Bug report": "Disabling tsan test for //tensorflow/core/kernels/data:parallel_interleave_dataset_op_test until bug fix lands.\n\nPiperOrigin-RevId: 288043424\nChange-Id: I5dc510862196545ee9bd12a32ace7610d34c7ea4",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    tags = [\"notsan\"],  # TODO(b/147147071): Remove this tag once bug fix lands.",
    "Label": "clean"
},
{
    "Id": 603,
    "Library": "tensorflow",
    "Date": "2019/05/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4ce40d837152b89dcf8ae01d201861d364e73e30",
    "Root Cause": "N.A",
    "Bug report": "ResolveDilatedConv bug fix",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+//   Pad -> SpaceToBatchND -> Expand -> Conv2D -> Squeeze -> BatchToSpaceND ->\n+//   BiasAdd\n+//\n+  //   If still Pad Op is not present, there might be possiblity it is added\n+  //   before STB Op like below Pad -> SpaceToBatchND -> Expand -> Conv2D ->\n+  //   Squeeze -> BatchToSpaceND -> BiasAdd So eliminate this Pad Op as well\n+  if (!has_pad_op) {\n+    auto* pre_stb_pad_op = GetOpWithOutput(*model, stb_op->inputs[0]);\n+    // If it is a Pad Op then just rewire the Input of Pad Op with Input of STB\n+    if (pre_stb_pad_op->type == OperatorType::kPad) {\n+      stb_op->inputs[0] = pre_stb_pad_op->inputs[0];\n+      has_pad_op = true;\n+      pad_op = pre_stb_pad_op;\n+    }\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 604,
    "Library": "tensorflow",
    "Date": "2019/12/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3ee3ab9e44fdd46b928661dfdfc732e0736d976c",
    "Root Cause": "N.A",
    "Bug report": "Lower function def debug string to VLOG(4) (consistent with other appearance).\n\nPiperOrigin-RevId: 287562801\nChange-Id: Ia5b1fce29db2b6572e1f01c6b5ca751e1580fcb7",
    "Number of deleted lines": 2,
    "Deleted lines": "-  VLOG(2) << \"Main function graph to be partitioned:\";\n-  VLOG(2) << DebugString(graph->ToGraphDefDebug());",
    "Added lines": "+  VLOG(4) << \"Main function graph to be partitioned:\";\n+  VLOG(4) << DebugString(graph->ToGraphDefDebug());",
    "Label": "clean"
},
{
    "Id": 605,
    "Library": "tensorflow",
    "Date": "2019/12/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/87f69493d2553ceb9fdb94ca7f203dc1c8e417ea",
    "Root Cause": "N.A",
    "Bug report": "Cleanup for closed bugs: remove associated optimization patches. They no longer seem to have any effect. V2 GPU control flow remains ~20% slower than V1.\n\nPiperOrigin-RevId: 287200087\nChange-Id: Ia35f1bdaf9a0ff8b0081f78f853209f86acb010f",
    "Number of deleted lines": 29,
    "Deleted lines": "-from tensorflow.python.ops import control_flow_util\n-  # TODO(b/134181679): Let the op itself handle optimizations.\n-  if control_flow_util.GraphOrParentsInXlaContext(ops.get_default_graph()):\n-    opts['maximum_iterations'] = n\n-\n-    def build_main_test():\n-      \"\"\"Main iteration condition.\"\"\"\n-      # TODO(b/138857806): The optimizer should handle this.\n-      # LogicalAnd is slow on GPU so we avoid adding it if `delta` is a\n-      # compile time constant.\n-      delta_const = tensor_util.constant_value(delta)\n-      if delta_const is not None:\n-        # Support single element arrays.\n-        delta_const = np.asscalar(delta_const)\n-        if delta_const >= 0:\n-          return iterate < limit\n-        else:\n-          return iterate > limit\n-      else:\n-        return math_ops.logical_or(\n-            math_ops.logical_and(delta >= 0, iterate < limit),\n-            math_ops.logical_and(delta < 0, iterate > limit))\n-\n-    main_test = build_main_test()\n-  # TODO(b/134181679): The op should handle this optimizations.\n-  if control_flow_util.GraphOrParentsInXlaContext(ops.get_default_graph()):\n-    # This specific dtype is required by while_loop.\n-    opts['maximum_iterations'] = math_ops.cast(\n-        misc.get_range_len(start, limit, delta), dtypes.int32)",
    "Added lines": "+  opts['maximum_iterations'] = n\n+    main_test = math_ops.logical_or(\n+        math_ops.logical_and(delta >= 0, iterate < limit),\n+        math_ops.logical_and(delta < 0, iterate > limit))\n+  opts['maximum_iterations'] = math_ops.cast(\n+      misc.get_range_len(start, limit, delta), dtypes.int32)",
    "Label": "clean"
},
{
    "Id": 606,
    "Library": "tensorflow",
    "Date": "2019/12/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/446ee0118980a784c0b43bcded9ba9205bb00293",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in documentation of  tf.while_loop.parallel_iterations",
    "Number of deleted lines": 1,
    "Deleted lines": "-  b = lambda i: tf.add(i, 1)",
    "Added lines": "+  b = lambda i: (tf.add(i, 1), )",
    "Label": "clean"
},
{
    "Id": 607,
    "Library": "tensorflow",
    "Date": "2019/12/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/09c50ba082abaa1d828fdd3c40c894fe7641d4e7",
    "Root Cause": "N.A",
    "Bug report": "Avoid descending into Pandas source code. This is triggering a bug that will be fixed separately.\n\nPiperOrigin-RevId: 286638570\nChange-Id: I49de18592cc325a07278656ba4b7566adaed6ced",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    DoNotConvert('pandas'),",
    "Label": "clean"
},
{
    "Id": 608,
    "Library": "tensorflow",
    "Date": "2019/12/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/69d62a99f09bdca2e7118f7600652c2d4ba77202",
    "Root Cause": "N.A",
    "Bug report": "Re-enable collective GPU test on XLA\n\nThe original bug was due to python3 range returning iterators, not due to XLA,\nand was fixed in 285282691.\n\nPiperOrigin-RevId: 286593074\nChange-Id: I9cd7f03a67a875ba03ec33156351f645f6180244",
    "Number of deleted lines": 1,
    "Deleted lines": "-    xla_enable_strict_auto_jit = False,",
    "Added lines": "+    xla_enable_strict_auto_jit = True,",
    "Label": "clean"
},
{
    "Id": 609,
    "Library": "tensorflow",
    "Date": "2019/12/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f079d31f778d5955d109bd5e8ad61298acac0ba9",
    "Root Cause": "N.A",
    "Bug report": "[tfdbg] Fix DebugNumericSummaryV2OP kernel mode=CURT_HEALTH breakage on Windows GPU builds\n\nPiperOrigin-RevId: 286585970\nChange-Id: I2db43f37def35fb0f58a15f09f1d9c026b27a536",
    "Number of deleted lines": 8,
    "Deleted lines": "-\n-    if (isinf(data[offset]) || isnan(data[offset])) {\n-    if (isinf(data[offset])) {\n-    if (isnan(data[offset])) {\n-    if (isinf(data[offset])) {\n-    } else if (isnan(data[offset])) {\n-    if (isinf(data[offset])) {\n-    if (isnan(data[offset])) {",
    "Added lines": "+    if (Eigen::numext::isinf(data[offset]) ||\n+        Eigen::numext::isnan(data[offset])) {\n+    if (Eigen::numext::isinf(data[offset])) {\n+    if (Eigen::numext::isnan(data[offset])) {\n+    if (Eigen::numext::isinf(data[offset])) {\n+    } else if (Eigen::numext::isnan(data[offset])) {\n+    if (Eigen::numext::isinf(data[offset])) {\n+    if (Eigen::numext::isnan(data[offset])) {",
    "Label": "clean"
},
{
    "Id": 610,
    "Library": "tensorflow",
    "Date": "2019/12/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6ba3d4160cbd61ea3719256e5dc2d9ce2174f833",
    "Root Cause": "N.A",
    "Bug report": "Remove namespace convert from DebugString",
    "Number of deleted lines": 1,
    "Deleted lines": "-                    + \": \" + convert::DebugString(cuda_engine->getBindingDataType(i)) + \"\\n\";",
    "Added lines": "+                    + \": \" + DebugString(cuda_engine->getBindingDataType(i)) + \"\\n\";",
    "Label": "clean"
},
{
    "Id": 611,
    "Library": "tensorflow",
    "Date": "2019/12/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/50db00b5cd3093cfaaa50f7d8d03a884441e41bd",
    "Root Cause": "N.A",
    "Bug report": "Fix bug where Normalization layer would be broken if `adapt` isn't called\n\nPiperOrigin-RevId: 286287077\nChange-Id: Iea055499b8900fdb4903b7a22ca6a091e548bffe",
    "Number of deleted lines": 1,
    "Deleted lines": "-        initializer=init_ops.zeros_initializer)",
    "Added lines": "+        initializer=init_ops.ones_initializer)",
    "Label": "clean"
},
{
    "Id": 612,
    "Library": "tensorflow",
    "Date": "2019/12/18",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/41fe01bf47ca2d3c17b693ec1c71dbdec6333825",
    "Root Cause": "N.A",
    "Bug report": "remove workaround for 2.0.0 bug",
    "Number of deleted lines": 3,
    "Deleted lines": "-  # Workaround for bug in 2.0.0 SDK, remove once that's fixed.\n-  sed -i -e $'s/#ifndef AM_HAL_GPIO_H/#ifdef __cplusplus\\\\\\nextern \"C\" {\\\\\\n#endif\\\\\\n#ifndef AM_HAL_GPIO_H/g' ${am_dir}/mcu/apollo3/hal/am_hal_gpio.h\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 613,
    "Library": "tensorflow",
    "Date": "2019/12/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/435e32140fe1c4fc0eefa2084aaf54d807dbe223",
    "Root Cause": "N.A",
    "Bug report": "NFC: Use this-> to appease GCC bug related to template lambda.\n\nGCC is unable to properly implicitly capture 'this' in generic lambdas. This bug is not fixed until 7.1.0:\nhttps://gcc.gnu.org/bugzilla/show_bug.cgi?id=67274\n\nPiperOrigin-RevId: 286083427\nChange-Id: Id52437119a123a0b8f20d7af4b8f7067fdab0e86",
    "Number of deleted lines": 1,
    "Deleted lines": "-      .Default([&](auto *op) { return dispatchToAutogenSerialization(op); });",
    "Added lines": "+      .Default(\n+          [&](Operation *op) { return dispatchToAutogenSerialization(op); });",
    "Label": "clean"
},
{
    "Id": 614,
    "Library": "tensorflow",
    "Date": "2019/12/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/96805b75cd8d76f73a12a0e5d76067f593510812",
    "Root Cause": "N.A",
    "Bug report": "[Intel MKL] Fixed a bug in mkl_conv2d constant filter caching which caused the filter output tensor not get updated",
    "Number of deleted lines": 8,
    "Deleted lines": "-      OP_REQUIRES(context,\n-                  (GetTensorDim(dilations_, data_format_, 'N') == 1 &&\n-                   GetTensorDim(dilations_, data_format_, 'C') == 1),\n-          context,\n-          (GetTensorDim(dilations_, data_format_, '0') > 0 &&\n-           GetTensorDim(dilations_, data_format_, '1') > 0 &&\n-           GetTensorDim(dilations_, data_format_, '2') > 0),\n-        if (is_filter_const_) {",
    "Added lines": "+      OP_REQUIRES(context, (GetTensorDim(dilations_, data_format_, 'N') == 1 &&\n+                            GetTensorDim(dilations_, data_format_, 'C') == 1),\n+          context, (GetTensorDim(dilations_, data_format_, '0') > 0 &&\n+                    GetTensorDim(dilations_, data_format_, '1') > 0 &&\n+                    GetTensorDim(dilations_, data_format_, '2') > 0),\n+        bool do_cache_filter =\n+            src_dims[MklDnnDims::Dim_N] > kSmallBatchSize ? true : false;\n+        if (is_filter_const_ && do_cache_filter) {\n+          if (filter_out_tensor != nullptr) {\n+            Tfilter* filter_out_tensor_buf =\n+                static_cast<Tfilter*>(const_cast<Tfilter*>(\n+                    filter_out_tensor->flat<Tfilter>().data()));\n+            memcpy(filter_out_tensor_buf, filter_data,\n+                   filter_out_tensor->AllocatedBytes());\n+          }",
    "Label": "clean"
},
{
    "Id": 615,
    "Library": "tensorflow",
    "Date": "2019/12/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/26ed25516e81c63eb237c7ae2d29750fbd5e53c3",
    "Root Cause": "N.A",
    "Bug report": "TFLite GPU OpenGL: Make object type \"unknown\" more specific to make debugging easier.\n\nPiperOrigin-RevId: 285434171\nChange-Id: I838f65765c84fd24971cad810722eceb62ae2f61",
    "Number of deleted lines": 6,
    "Deleted lines": "-    default:\n-      return \"unknown\";\n-        return \"unknown\";\n-        return \"unknown\";\n-      return \"unknown\";\n-      return \"unknown\";",
    "Added lines": "+    case DataType::UINT64:\n+      return \"u64vec4_not_available_in_glsl\";\n+    case DataType::INT64:\n+      return \"i64vec4_not_available_in_glsl\";\n+    case DataType::FLOAT64:\n+      return \"dvec4\";\n+    case DataType::UNKNOWN:\n+      return \"unknown_buffer_type\";\n+      // Do NOT add `default:'; we want build failure for new enum values.\n+        return \"unknown_image_2d\";\n+        return \"unknown_image_2d_array\";\n+      return \"unknown_image_layout\";\n+      return \"unknown_image_precision\";",
    "Label": "clean"
},
{
    "Id": 616,
    "Library": "tensorflow",
    "Date": "2019/12/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f03ff9f3169d3391a5e1f6b55b98d1e3fb98a0a6",
    "Root Cause": "N.A",
    "Bug report": "Rolling forward \"In SparseTensor::Create and constructor, forward Tensor arguments using std::move.\"\n\nThe previous version had a bug: the moved-from Tensor arguments were used (i) in a subsequent initializer for SparseTensor::dims_, and (ii) in the DCHECK invocations in the constructor.\n\nPiperOrigin-RevId: 285132681\nChange-Id: Ib26386156a3d11a69f3a83b660016beb2ca32811",
    "Number of deleted lines": 15,
    "Deleted lines": "-    *result = SparseTensor(ix, vals, shape, order);\n-    return Create(ix, vals, TensorShapeToVector(shape),\n-    return Create(ix, vals, shape, UndefinedOrder(shape), result);\n-    return Create(ix, vals, TensorShapeToVector(shape), order, result);\n-      : SparseTensor(ix, vals, TensorShapeToVector(shape),\n-      : SparseTensor(ix, vals, shape, UndefinedOrder(shape)) {}\n-      : SparseTensor(ix, vals, TensorShapeToVector(shape), order) {}\n-      : ix_(ix),\n-        vals_(vals),\n-        dims_(UnsafeGetDimsFromIx(ix)) {\n-    DCHECK_EQ(ix.dtype(), DT_INT64)\n-        << \"indices must be type int64 but got: \" << ix.dtype();\n-    DCHECK(TensorShapeUtils::IsVector(vals.shape()))\n-        << \"vals must be a vec, but got: \" << vals.shape().DebugString();\n-    DCHECK_EQ(ix.shape().dim_size(0), vals.shape().dim_size(0))",
    "Added lines": "+    *result = SparseTensor(std::move(ix), std::move(vals), shape, order);\n+    return Create(std::move(ix), std::move(vals), TensorShapeToVector(shape),\n+    return Create(std::move(ix), std::move(vals), shape, UndefinedOrder(shape),\n+                  result);\n+    return Create(std::move(ix), std::move(vals), TensorShapeToVector(shape),\n+                  order, result);\n+      : SparseTensor(std::move(ix), std::move(vals), TensorShapeToVector(shape),\n+      : SparseTensor(std::move(ix), std::move(vals), shape,\n+                     UndefinedOrder(shape)) {}\n+      : SparseTensor(std::move(ix), std::move(vals), TensorShapeToVector(shape),\n+                     order) {}\n+      : ix_(std::move(ix)),\n+        vals_(std::move(vals)),\n+        dims_(UnsafeGetDimsFromIx(ix_)) {\n+    DCHECK_EQ(ix_.dtype(), DT_INT64)\n+        << \"indices must be type int64 but got: \" << ix_.dtype();\n+    DCHECK(TensorShapeUtils::IsVector(vals_.shape()))\n+        << \"vals must be a vec, but got: \" << vals_.shape().DebugString();\n+    DCHECK_EQ(ix_.shape().dim_size(0), vals_.shape().dim_size(0))",
    "Label": "clean"
},
{
    "Id": 617,
    "Library": "tensorflow",
    "Date": "2019/12/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e7904f5442b5bbad8c4be93fea0afebeac709dc7",
    "Root Cause": "N.A",
    "Bug report": "Fix modification in previous change that was not intended.\n\nA previous change was intended to be a non-functional change, but due to a bug\nin the transformation it changed in which cases dilated_x is set to true.\nThis changes it back to something equivalent to the old logic.\nThe previous logic was:\nby default, set dilated_x to true.\nSet it to false if reduction_dimensions.is_row_reduction is false (else block)\nand if IsUnrollingColumnReductionBeneficial returns true\n\nPiperOrigin-RevId: 284962122\nChange-Id: I2423070467786cf03e6d98d1e104fc1e1aced24e",
    "Number of deleted lines": 1,
    "Deleted lines": "-      !reduction_dimensions.is_row_reduction &&",
    "Added lines": "+      reduction_dimensions.is_row_reduction ||",
    "Label": "clean"
},
{
    "Id": 618,
    "Library": "tensorflow",
    "Date": "2019/12/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4aea9f74550173856d70abf925c37de7fe74dd8e",
    "Root Cause": "N.A",
    "Bug report": "Remove debug printing in file_system.cc\n\nPiperOrigin-RevId: 284658461\nChange-Id: I203fc36dea69614a4cef617b9168b7ac1873d87d",
    "Number of deleted lines": 5,
    "Deleted lines": "-  std::cerr << \"MM: RecursivelyCreateDir(\" << dirname << \")\\n\";\n-  std::cerr << \"MM: scheme=\\\"\" << scheme << \"\\\", host=\\\"\" << host\n-            << \"\\\" remaining_dir=\\\"\" << remaining_dir << \"\\\"\\n\";\n-    std::cerr << \"MM: current_entry=\\\"\" << current_entry << \"\\\"\\n\";\n-    std::cerr << \"MM: exists_status=\" << exists_status << \"\\n\";",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 619,
    "Library": "tensorflow",
    "Date": "2019/12/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c70fe93b765ecfc566dc79b55cdb51260df07f6c",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in dynamic kernel loader.\nif an environment variable is not set, getenv returns nullptr. Check that\nbefore running strcmp.\n\nPiperOrigin-RevId: 284646216\nChange-Id: Icfc1c3f299e912e845e6a297fdf64055bfabf49c",
    "Number of deleted lines": 2,
    "Deleted lines": "-  bool override_abi_check =\n-      strcmp(getenv(\"TF_REALLY_LOAD_UNSAFE_PACKAGES\"), \"1\") == 0;",
    "Added lines": "+  char* _abi_check_env_var = getenv(\"TF_REALLY_LOAD_UNSAFE_PACKAGES\");\n+  bool override_abi_check = false;\n+  if (_abi_check_env_var != nullptr) {\n+    override_abi_check = strcmp(_abi_check_env_var, \"1\") == 0;\n+  }",
    "Label": "clean"
},
{
    "Id": 620,
    "Library": "tensorflow",
    "Date": "2019/12/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c28ca27b96b3a141922523c005c71af51cc61906",
    "Root Cause": "N.A",
    "Bug report": "Fixing a TensorFlow control flow bug.\n\nCalling `nest.map_structure` with a lambda that does not return (i.e. only for its side-effect) will fail on structures that contain composite tensors because the `map_structure` implementation will try to reconstruct the composite tensors from the return values of the lambda, which will be None.\n\nPiperOrigin-RevId: 284197904\nChange-Id: I9b3e43bbd28712281839eaf77b2e4280db7c585c",
    "Number of deleted lines": 4,
    "Deleted lines": "-      nest.map_structure(\n-          lambda x: self._outer_context.AddName(x.name),\n-          result,\n-          expand_composites=True)",
    "Added lines": "+      def fn(x):\n+        self._outer_context.AddName(x.name)\n+        return x\n+      nest.map_structure(fn, result, expand_composites=True)",
    "Label": "clean"
},
{
    "Id": 621,
    "Library": "tensorflow",
    "Date": "2019/12/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fda7ab091fa547aa3d021c22156ce8938b1076ef",
    "Root Cause": "N.A",
    "Bug report": "Move the debug string from TraceMe name to argument.\n\nPiperOrigin-RevId: 283865965\nChange-Id: I996b10a3501c57c674bc5fe7d674d36e5d16ea3f",
    "Number of deleted lines": 1,
    "Deleted lines": "-        return absl::StrCat(\"EagerService:Enqueue:\", request->DebugString());",
    "Added lines": "+        return absl::StrCat(\n+            \"EagerService:Enqueue#debug_str=\", request->DebugString(), \"#\");",
    "Label": "clean"
},
{
    "Id": 622,
    "Library": "tensorflow",
    "Date": "2019/11/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b4b07944785f699d3de854635fdedfc2729a1aa3",
    "Root Cause": "N.A",
    "Bug report": "Fix bug: use_calibration --> use_calibration_",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (use_calibration) {",
    "Added lines": "+    if (use_calibration_) {",
    "Label": "clean"
},
{
    "Id": 623,
    "Library": "tensorflow",
    "Date": "2019/12/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/97b36b0d4a9389178e510b2f59f98c0c75e0b882",
    "Root Cause": "N.A",
    "Bug report": "Updated bug link now that it is filed properly upstream.\n\nPiperOrigin-RevId: 283594085\nChange-Id: If02fbabc9e6f91c196185cf5aa312c21b7f8e924",
    "Number of deleted lines": 1,
    "Deleted lines": "-    # See: https://github.com/google/iree/issues/114",
    "Added lines": "+    # See: https://bugs.llvm.org/show_bug.cgi?id=44211",
    "Label": "clean"
},
{
    "Id": 624,
    "Library": "tensorflow",
    "Date": "2019/12/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2cfa79f98497cd7d9ce9d26de38ec046323d1d31",
    "Root Cause": "N.A",
    "Bug report": "AffineLoopFusion: Prevent fusion of multi-out-edge producer loops\n\nhttps://github.com/tensorflow/mlir/pull/162 introduced a bug that\nincorrectly allowed fusion of producer loops with multiple outgoing\nedges. This commit fixes that problem. It also introduces a new flag to\ndisable sibling loop fusion so that we can test producer-consumer fusion\nin isolation.\n\nCloses #259\n\nCOPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/mlir/pull/259 from dcaballe:dcaballe/fix_multi_out_edge_producer_fusion 578d5661705fd5c56c555832d5e0528df88c5282\nPiperOrigin-RevId: 283531105\nChange-Id: I3a6173463ea20bd35555c24fa451bfbf2dfac098",
    "Number of deleted lines": 3,
    "Deleted lines": "-// 'srcLiveOutStoreOp', have an output edge.\n-// 'srcNode's write region to 'memref'.\n-  assert(mdg->getOutEdgeCount(srcId) == 1 && \"Expected only one output edge\");",
    "Added lines": "+// 'srcLiveOutStoreOp', has output edges.\n+// 'srcNode's write region to 'memref' and 'srcId' has only one output edge.\n+  // Return false if 'srcNode' has more than one output edge on 'memref'.\n+  if (mdg->getOutEdgeCount(srcId, memref) > 1)\n+    return false;",
    "Label": "clean"
},
{
    "Id": 625,
    "Library": "tensorflow",
    "Date": "2019/11/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4b736990a70bad67c0307e8aeeaa397f10744d77",
    "Root Cause": "N.A",
    "Bug report": "[Grappler] Workaround for bug in HoistCWiseUnaryChainsStage where duplicate inputs to Concat can cause the rewrite to create cycles in the graphs.\n\nPiperOrigin-RevId: 282949141\nChange-Id: Ib8fa6571399331b9b5c58aa5bfbdda955a510603",
    "Number of deleted lines": 2,
    "Deleted lines": "-      return n > 1;\n-            << \" root=\" << root_node->name()",
    "Added lines": "+      return n > 1 && FirstNInputsAreUnique(*node, n);\n+  bool FirstNInputsAreUnique(const NodeDef& node, int n) const {\n+    if (n > node.input_size()) return false;\n+    absl::flat_hash_set<string> unique_inputs;\n+    const int start = node.op() == \"Concat\" ? 1 : 0;\n+    const int end = start + n;\n+    for (int i = start; i < end; ++i) {\n+      unique_inputs.insert(node.input(i));\n+    }\n+    return unique_inputs.size() == n;\n+  }\n+\n+            << \" root=\" << root_node->DebugString()",
    "Label": "clean"
},
{
    "Id": 626,
    "Library": "tensorflow",
    "Date": "2019/11/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bbd8a3e0e489f7ac138075588cebc3011e43d675",
    "Root Cause": "N.A",
    "Bug report": "Fixed an internal bug where width/height were consistently (and incorrectly) mapped to y/x vs the correct x/y.\n\nPiperOrigin-RevId: 282812176\nChange-Id: I5df8fa023a8cc873a3cd6af8e012a0460baec8ee",
    "Number of deleted lines": 15,
    "Deleted lines": "-    x_index = 2;\n-    y_index = 3;\n-    x_index = 1;\n-    y_index = 2;\n-    filter_x_index = 0;\n-    filter_y_index = 1;\n-    filter_x_index = 2;\n-    filter_y_index = 3;\n-    in_channel_index = 1;\n-    output = DescribeTensor(DT_FLOAT, {dims.batch, dims.oz, dims.ox, dims.oy});\n-    output = DescribeTensor(DT_FLOAT, {dims.batch, dims.ox, dims.oy, dims.oz});\n-    x_index = 2;\n-    y_index = 3;\n-  } else {\n-    x_index = 1;",
    "Added lines": "+    y_index = 2;\n+    x_index = 3;\n+    y_index = 1;\n+    x_index = 2;\n+    filter_y_index = 0;\n+    filter_x_index = 1;\n+    in_channel_index = 1;\n+    filter_y_index = 2;\n+    filter_x_index = 3;\n+    output = DescribeTensor(DT_FLOAT, {dims.batch, dims.oz, dims.oy, dims.ox});\n+    output = DescribeTensor(DT_FLOAT, {dims.batch, dims.oy, dims.ox, dims.oz});\n+    x_index = 3;\n+  } else {\n+    y_index = 1;\n+    x_index = 2;",
    "Label": "clean"
},
{
    "Id": 627,
    "Library": "tensorflow",
    "Date": "2019/11/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8ef8c28a21690f31de3ed48f0e269526a36dee9d",
    "Root Cause": "N.A",
    "Bug report": "Add temp/intermediate tensors to interpreter state dump\n\nThis will make it easier to debug memory-related issues with intermediate tensors.\n\nPiperOrigin-RevId: 282810295\nChange-Id: Ibc3797884ffeccd23feb16ed22ed4fd760a6355f",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (node.intermediates && node.intermediates->size) {\n+      printf(\"  Intermediates:\");\n+      PrintTfLiteIntVector(node.intermediates);\n+    }\n+    if (node.temporaries && node.temporaries->size) {\n+      printf(\"  Temporaries:\");\n+      PrintTfLiteIntVector(node.temporaries);\n+    }",
    "Label": "clean"
},
{
    "Id": 628,
    "Library": "tensorflow",
    "Date": "2019/11/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/df9d9ea15958a4751c3be539d185f42ea8302dfe",
    "Root Cause": "N.A",
    "Bug report": "Only use sendfile implementation on linux.\n\nThat is, use copy_file_linux.cc only if we are on linux. This fixes a bug where BSD sendfile is not the same as Linux sendfile so Mac builds fail.\n\nPiperOrigin-RevId: 282396307\nChange-Id: I202ab4e0e33d4e0c45296f7b693953aac8cf4fb0",
    "Number of deleted lines": 2,
    "Deleted lines": "-        \"//tensorflow:android\": [\"copy_file_portable.cc\"],\n-        \"//conditions:default\": [\"copy_file_linux.cc\"],",
    "Added lines": "+        \"//tensorflow:linux_x86_64\": [\"copy_file_linux.cc\"],\n+        \"//conditions:default\": [\"copy_file_portable.cc\"],",
    "Label": "clean"
},
{
    "Id": 629,
    "Library": "tensorflow",
    "Date": "2019/11/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c974f7ed5f63278c24165d626e9e5dd63f18f7ae",
    "Root Cause": "N.A",
    "Bug report": "[tfdbg] Support enable_check_numerics() and enable_dump_debug_info() callback on TPUs\n\n- Skip a set of TPU compilation-specific ops from tfdbg's op callbacks.\n\nPiperOrigin-RevId: 281836861\nChange-Id: Ic7ff59a32eba26d5bb3ee2ac4f5f9166c78928c8",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    b\"LoopCond\",\n+    # TPU-specific ops begin.\n+    b\"TPUReplicatedInput\",\n+    b\"TPUReplicateMetadata\",\n+    b\"TPUCompilationResult\",\n+    b\"TPUReplicatedOutput\",\n+    b\"ConfigureDistributedTPU\",",
    "Label": "clean"
},
{
    "Id": 630,
    "Library": "tensorflow",
    "Date": "2019/11/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/13ffea151cc01d180d06274e326e76f938a91c05",
    "Root Cause": "N.A",
    "Bug report": "Fix one DecodeImageOp bug that could produce OOM.\n\nPiperOrigin-RevId: 281551737\nChange-Id: Ib6989ae4eb98a2abf403ae860eb2b744c0d90cc4",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+// TODO(b/141645641): Separate concerns here: constructors uses name to\n+// determine type of parsing, compute uses file magic to parse and these might\n+// not match.",
    "Label": "clean"
},
{
    "Id": 631,
    "Library": "tensorflow",
    "Date": "2019/11/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6e24888c44452c7c970b052e928fbea865c2450b",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in NCCL broadcast wrapper\n\n- The recvbuf passed to ncclBroadcast must not be nullptr. This issue\n  arises if the source node of the broadcast has no output.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        } else {\n+          // Operate in-place if no output (for the src node).\n+          recvbuff = const_cast<void*>(sendbuff);",
    "Label": "clean"
},
{
    "Id": 632,
    "Library": "tensorflow",
    "Date": "2019/11/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/80b02cd5f552241f6f8a3ff7c233119745da01cd",
    "Root Cause": "N.A",
    "Bug report": "Fix mismatch between declaration and definition for `CreateTPUMergeVariablesWithExecutePass`\n\nThis broke the Windows link step, the Linux/Max platform didn't catch the bug.\n\nPiperOrigin-RevId: 281419968\nChange-Id: If2e8117cf6d1db76ed761d0b50acf8e63551f925",
    "Number of deleted lines": 1,
    "Deleted lines": "-std::unique_ptr<OpPassBase<ModuleOp>> CreateTPUMergeVariablesWithExecutePass();",
    "Added lines": "+std::unique_ptr<OpPassBase<FuncOp>> CreateTPUMergeVariablesWithExecutePass();",
    "Label": "clean"
},
{
    "Id": 633,
    "Library": "tensorflow",
    "Date": "2019/11/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/41d8aaf0f4de184638134278e238cae3062c85e8",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug that if all the input shapes to TPUStrategy.experimental_run_v2 is static, it always uses the shape from first replica to pad. But it may fail if the first replica shape is not the maximum shape.\n\nPiperOrigin-RevId: 280717072\nChange-Id: I010da29d1d13345313f4bb2db2cc6bb43ba00e41",
    "Number of deleted lines": 2,
    "Deleted lines": "-            maximum_shape = input_tensor.get_shape()\n-            maximum_shape = tensor_shape.TensorShape(np.shape(input_tensor))",
    "Added lines": "+            rank = input_tensor.get_shape().rank\n+            rank = np.rank(input_tensor)\n+          maximum_shape = tensor_shape.TensorShape([None] * rank)",
    "Label": "clean"
},
{
    "Id": 634,
    "Library": "tensorflow",
    "Date": "2019/11/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a29f106161bee066cee1bce3e3c4f3c5a5fc4293",
    "Root Cause": "N.A",
    "Bug report": "Fix build warnings\n\nDelete unused constexpr ints in LowerToLLVMDialect.\nAdd (void)toStringRef for non-debug builds.\n\nFixes #232.\n\nPiperOrigin-RevId: 280671014\nChange-Id: I3f052630a2f8047b531c3b970be68919af63ca1f",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  (void)toStringRef;",
    "Label": "clean"
},
{
    "Id": 635,
    "Library": "tensorflow",
    "Date": "2019/11/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/946bd11412d641682d17b68e9437b888e4ec5619",
    "Root Cause": "N.A",
    "Bug report": "Fix build warnings\n\nDelete unused constexpr ints in LowerToLLVMDialect.\nAdd (void)toStringRef for non-debug builds.\n\nFixes #232.\n\nPiperOrigin-RevId: 280671014",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  (void)toStringRef;",
    "Label": "clean"
},
{
    "Id": 636,
    "Library": "tensorflow",
    "Date": "2019/11/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e0bef5bba19e4b38244fb7a0b729d17bf31f703a",
    "Root Cause": "N.A",
    "Bug report": "fix debug_events_writer: avoid possible NULL pointer dereference",
    "Number of deleted lines": 1,
    "Deleted lines": "-      break;",
    "Added lines": "+      return;",
    "Label": "clean"
},
{
    "Id": 637,
    "Library": "tensorflow",
    "Date": "2019/11/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/00b9ea941eed50d2219a1b4ad4a16fe8530d3ef8",
    "Root Cause": "N.A",
    "Bug report": "Add source, destination name to _send traceme to allow easier debugging\n\nPiperOrigin-RevId: 279761696\nChange-Id: I3f1ed987df91ffa746861696de58211764f369b6",
    "Number of deleted lines": 1,
    "Deleted lines": "-  profiler::TraceMe activity(\"_Send\", profiler::TraceMeLevel::kInfo);",
    "Added lines": "+  profiler::TraceMe activity(\n+      absl::StrCat(\"_Send input \", i, \" from \", handle_device->name(), \" to \",\n+                   expected_input_device->name()),\n+      profiler::TraceMeLevel::kInfo);",
    "Label": "clean"
},
{
    "Id": 638,
    "Library": "tensorflow",
    "Date": "2019/11/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/51296ec43c69c2a4311bb92dcd4f10c905b07537",
    "Root Cause": "N.A",
    "Bug report": "[TF:XLA] Fix off-by-one error in Cluster::DebugString.\n\nPiperOrigin-RevId: 278623066\nChange-Id: I5316eb455e134338e1651db889747f09a10af719",
    "Number of deleted lines": 2,
    "Deleted lines": "-      return absl::StrCat(\"<\", node->name(), \" + \", cluster_size(), \" others #\",\n-                          cycles_graph_node_id(), \">\");",
    "Added lines": "+      if (cluster_size() == 1) {\n+        return absl::StrCat(\"<\", node->name(), \" #\", cycles_graph_node_id(),\n+                            \">\");\n+      }\n+\n+      return absl::StrCat(\"<\", node->name(), \" + \", cluster_size() - 1,\n+                          \" others #\", cycles_graph_node_id(), \">\");",
    "Label": "clean"
},
{
    "Id": 639,
    "Library": "tensorflow",
    "Date": "2019/11/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/512c467ff5292f742d74bd538819ab8f0980ac42",
    "Root Cause": "N.A",
    "Bug report": "TFLM: Add additional information to help debugging memory planner.\n\nIt also gives more information on what are the tensors, otherwise, it just show a number in single digit hex.\n\nPiperOrigin-RevId: 278046130\nChange-Id: Iaea6cf432ceccc23a92b6f7c8c3e46a2b1d649b2",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+  for (int i = 0; i < buffer_count_; ++i) {\n+    error_reporter->Report(\n+        \"Planner buffer ID: %d, calculated offset: %d, size required: %d, \"\n+        \"first_time_created: %d, \"\n+        \"last_time_used: %d\",\n+        i, buffer_offsets_[i], requirements_[i].size,\n+        requirements_[i].first_time_used, requirements_[i].last_time_used);\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 640,
    "Library": "tensorflow",
    "Date": "2019/11/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f7d2d9f716c749728fb95dbf712db12aa2749580",
    "Root Cause": "N.A",
    "Bug report": "Drop spurious debug spew.\n\nPiperOrigin-RevId: 278023371\nChange-Id: I74455db88b81cc78896f1c4a13eb8f90d1381a9f",
    "Number of deleted lines": 2,
    "Deleted lines": "-  tileRes->op.getParentOfType<FuncOp>().dump();\n-  fusionRes->fusedProducer.getParentOfType<FuncOp>().dump();",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 641,
    "Library": "tensorflow",
    "Date": "2019/11/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6fbcb80d239053ad86ba18348fd116996691d7f4",
    "Root Cause": "N.A",
    "Bug report": "Drop spurious debug spew.\n\nPiperOrigin-RevId: 278023371",
    "Number of deleted lines": 2,
    "Deleted lines": "-  tileRes->op.getParentOfType<FuncOp>().dump();\n-  fusionRes->fusedProducer.getParentOfType<FuncOp>().dump();",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 642,
    "Library": "tensorflow",
    "Date": "2019/11/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2f40a93fdf9a2dd98c7b3e7d715fafb926539ddf",
    "Root Cause": "N.A",
    "Bug report": "[Grappler]: Fix bug in transitive reduction introduced in cl/277844506.\n\nPiperOrigin-RevId: 278003661\nChange-Id: Id67a427283ae8a6225560e6c4d7b527b1d364907",
    "Number of deleted lines": 4,
    "Deleted lines": "-void LongestPathsLowerBounds(int source, std::pair<int, int> target_range,\n-      // If the input node is before source in the topo order, no path\n-      // source -> input -> target can exits and we can skip it.\n-      if (input < target_range.first) break;",
    "Added lines": "+void LongestPathsLowerBounds(int source,\n+                             const std::pair<int, int>& target_range,\n+      // If the input node is before target_range.first in the topo order,\n+      // no path source -> input -> target can exist, unless input is the\n+      // source itself, so we can skip it.\n+      if (input != source && input < target_range.first) break;",
    "Label": "clean"
},
{
    "Id": 643,
    "Library": "tensorflow",
    "Date": "2019/10/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8bcc99578597d3a02e46bcc231c1ac76a14fc0a3",
    "Root Cause": "N.A",
    "Bug report": "Dump op location in createPrintOpGraphPass for easier debugging.\n\nPiperOrigin-RevId: 277546527\nChange-Id: Ied27ad9c715f26d02ac51a870fc343ae6ba691e7",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (!op->getLoc().isa<mlir::UnknownLoc>()) {\n+    os << op->getLoc() << \"\\n\";\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 644,
    "Library": "tensorflow",
    "Date": "2019/10/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3ef96bcf2710e34ce7e8d5b4aa3ab12ef5bd11a0",
    "Root Cause": "N.A",
    "Bug report": "Dump op location in createPrintOpGraphPass for easier debugging.\n\nPiperOrigin-RevId: 277546527",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (!op->getLoc().isa<mlir::UnknownLoc>()) {\n+    os << op->getLoc() << \"\\n\";\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 645,
    "Library": "tensorflow",
    "Date": "2019/10/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c6b2a2a87b05406a75d29641e625fd0eaa92a781",
    "Root Cause": "N.A",
    "Bug report": "`{` was left unescaped.\n\nBuilding genrules in debug mode (asserts turned on) results in this crashing:\n\nERROR: tensorflow/compiler/mlir/lite/BUILD:401:1: Couldn't build file tensorflow/compiler/mlir/lite/operator_converters.inc: Executing genrule //tensorflow/compiler/mlir/lite:operator_converter_inc failed (Aborted): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\noperator-converter-gen: external/llvm/lib/Support/FormatVariadic.cpp:117: static std::pair<llvm::ReplacementItem, llvm::StringRef> llvm::formatv_object_base::splitLiteralAndReplacement(llvm::StringRef): Assertion `false && \"Unterminated brace sequence.  Escape with {{ for a literal brace.\"' failed.                                                                                                                                                                                   Stack dump:\n0.      Program arguments: bazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen -I external/local_config_mlir/include -I external/org_tensorflow tensorflow/compiler/mlir/lite/ir/tfl_ops.td -o bazel-out/host/bin/tensorflo\nw/compiler/mlir/lite/operator_converters.inc                                                                                                                                                                                                   bazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0xe6bc4)[0x56141b511bc4]\nbazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0xe6c57)[0x56141b511c57]\nbazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0xe4a73)[0x56141b50fa73]                                                                                                                                              bazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0xe656f)[0x56141b51156f]\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x123a0)[0x7ff68d5733a0]                                                                                                                                                                                /lib/x86_64-linux-gnu/libc.so.6(gsignal+0x10b)[0x7ff68d1d8cfb]                                                                                                                                                                                 /lib/x86_64-linux-gnu/libc.so.6(abort+0x129)[0x7ff68d1c38ad]\n/lib/x86_64-linux-gnu/libc.so.6(+0x2177f)[0x7ff68d1c377f]                                                                                                                                                                                      /lib/x86_64-linux-gnu/libc.so.6(+0x2f542)[0x7ff68d1d1542]                                                                                                                                                                                      bazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0xa992d)[0x56141b4d492d]\nbazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0xa9b7b)[0x56141b4d4b7b]                                                                                                                                              bazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0x1abc6)[0x56141b445bc6]                                                                                                                                              bazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0x1d36d)[0x56141b44836d]\nbazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0x1c009)[0x56141b447009]\nbazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0x198e3)[0x56141b4448e3]                                                                                                                                              bazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0x19de6)[0x56141b444de6]\nbazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0x2bd57)[0x56141b456d57]                                                                                                                                              bazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0x19ea7)[0x56141b444ea7]                                                                                                                                              /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xeb)[0x7ff68d1c552b]\nbazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen(+0x1827a)[0x56141b44327a]                                                                                                                                              /bin/bash: line 1: 155940 Aborted                 bazel-out/host/bin/tensorflow/compiler/mlir/lite/operator-converter-gen -I external/local_config_mlir/include -I external/org_tensorflow tensorflow/compiler/mlir/lite/ir/tfl_ops.td -o bazel-out/host/bin/tensorflow/compiler/mlir/lite/operator_converters.inc\nPiperOrigin-RevId: 277145188\nChange-Id: Ib3a6ad2ff79092ccd9f7a02ec901254c8b9b87ee",
    "Number of deleted lines": 1,
    "Deleted lines": "-    os << formatv(\"  if(const auto *op = op_union.As{0}()) {\\n\", option_name);",
    "Added lines": "+    os << formatv(\"  if(const auto *op = op_union.As{0}()) {{\\n\", option_name);",
    "Label": "clean"
},
{
    "Id": 646,
    "Library": "tensorflow",
    "Date": "2019/10/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5f55cc82a9f86576129adea9ee2d27e146b8c0a8",
    "Root Cause": "N.A",
    "Bug report": "Fix bug where module passes were nested under function pass manager.\n\nPiperOrigin-RevId: 276792147\nChange-Id: I50de42f33802d5d9ed70a38577475db3dda022fd",
    "Number of deleted lines": 10,
    "Deleted lines": "-  OpPassManager &bridge = pm.nest<FuncOp>();\n-\n-  bridge.addPass(tf_executor::CreateTFExecutorIslandCoarseningPass());\n-  bridge.addPass(createCanonicalizerPass());\n-  bridge.addPass(CreateTPUClusterFormationPass());\n-  bridge.addPass(tf_executor::CreateTFExecutorConstantSinkingPass());\n-  bridge.addPass(TFDevice::CreateResourceOpLiftingPass());\n-  bridge.addPass(TFDevice::CreateClusterOutliningPass());\n-  bridge.addPass(CreateTPURewritePass());\n-  bridge.addPass(createCanonicalizerPass());",
    "Added lines": "+  OpPassManager &func_pm = pm.nest<FuncOp>();\n+  func_pm.addPass(tf_executor::CreateTFExecutorIslandCoarseningPass());\n+  func_pm.addPass(createCanonicalizerPass());\n+  func_pm.addPass(CreateTPUClusterFormationPass());\n+  func_pm.addPass(tf_executor::CreateTFExecutorConstantSinkingPass());\n+  func_pm.addPass(TFDevice::CreateResourceOpLiftingPass());\n+\n+  pm.addPass(TFDevice::CreateClusterOutliningPass());\n+  pm.addPass(CreateTPURewritePass());\n+  pm.addNestedPass<FuncOp>(createCanonicalizerPass());",
    "Label": "clean"
},
{
    "Id": 647,
    "Library": "tensorflow",
    "Date": "2019/10/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1c2fa3dd267d38cb37e795c7c887452ccd79a940",
    "Root Cause": "N.A",
    "Bug report": "Disable //third_party/tensorflow/python/debug:dumping_callback_test on TAP as it's flaky.\n\nPiperOrigin-RevId: 276108889\nChange-Id: I8c2f5e8ea9da393e2608c9441434c53db801c373",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"notap\",  # TODO(b/142879897)",
    "Label": "clean"
},
{
    "Id": 648,
    "Library": "tensorflow",
    "Date": "2019/10/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a894398b839e8b538210bc8b668c177d7ef78e36",
    "Root Cause": "N.A",
    "Bug report": "Disable //third_party/tensorflow/python/debug:examples_v1_test on msan TAP as it's failing.\n\nPiperOrigin-RevId: 276104472\nChange-Id: I67a038f63edcf25372bfb5d58292080cfecdd0a7",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        \"nomsan\",  # TODO(b/143150907)",
    "Label": "clean"
},
{
    "Id": 649,
    "Library": "tensorflow",
    "Date": "2019/10/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/60e1179aa777c09f9e51cbf8895882fef1f87d7f",
    "Root Cause": "N.A",
    "Bug report": "fix training dataset steps_per_epoch bug",
    "Number of deleted lines": 2,
    "Deleted lines": "-      training_utils.infer_steps_for_dataset(\n-",
    "Added lines": "+      inferred_steps = training_utils.infer_steps_for_dataset(\n+      \n+      steps_per_epoch = inferred_steps if steps_per_epoch is None else steps_per_epoch\n+        ",
    "Label": "clean"
},
{
    "Id": 650,
    "Library": "tensorflow",
    "Date": "2019/10/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3d7dd10f2e0d7b1a719d9fa1df61e9ae15e89cef",
    "Root Cause": "N.A",
    "Bug report": "Fix MLIR TensorFlow Shape Inference pass to actually iterate until fix point\n\nAlso fix a bug related to every new iteration inserting a new tf.Cast operation.\nThis is covered by the existing tests that are failing with the fix for the\niterative fix point convergence.\n\nPiperOrigin-RevId: 273516793",
    "Number of deleted lines": 4,
    "Deleted lines": "-  while (changed) {\n-        changed = InferShapeForSingleOperation(op, tf_dialect, graph_version);\n-    if (max_iteration--) return failure();\n-  return success();",
    "Added lines": "+  // tf.Cast are only inferred if they have at least one user in the tf dialect.\n+  // This is necessary to avoid reprocessing the tf.Cast that are inserted at\n+  // the end of this function.\n+  if (isa<CastOp>(op) &&\n+      llvm::all_of(op->getResult(0)->getUsers(), [&](Operation* user) {\n+        return user->getDialect() != tf_dialect;\n+      })) {\n+    LLVM_DEBUG(llvm::dbgs() << \"Skipping inference for tf.Cast with no TF \"\n+                               \"dialect operation users '\"\n+                            << *op << \"'.\\n\";);\n+    return false;\n+  }\n+\n+  for (int iteration = 0; iteration < max_iteration && changed; ++iteration) {\n+    changed = false;\n+    LLVM_DEBUG(llvm::dbgs()\n+               << \"Shape inference, iteration \" << iteration << \"\\n\");\n+        changed |= InferShapeForSingleOperation(op, tf_dialect, graph_version);\n+  return success(!changed);",
    "Label": "clean"
},
{
    "Id": 651,
    "Library": "tensorflow",
    "Date": "2019/10/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f23e6832c0d6c75290c93adc13ebc0bd34d9d527",
    "Root Cause": "N.A",
    "Bug report": "fix simplify-affine-structures bug\n\nSigned-off-by: Uday Bondhugula <uday@polymagelabs.com>\n\nCloses #157\n\nCOPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/mlir/pull/157 from bondhugula:quickfix bd1fcd79825fc0bd5b4a3e688153fa0993ab703d\nPiperOrigin-RevId: 273316498",
    "Number of deleted lines": 3,
    "Deleted lines": "-  // Turn memrefs' non-identity layouts maps into ones with identity.\n-  func.walk([](AllocOp op) { normalizeMemRef(op); });\n-    pass(\"simplify-affine-structures\", \"Simplify affine expressions\");",
    "Added lines": "+  // Turn memrefs' non-identity layouts maps into ones with identity. Collect\n+  // alloc ops first and then process since normalizeMemRef replaces/erases ops\n+  // during memref rewriting.\n+  SmallVector<AllocOp, 4> allocOps;\n+  func.walk([&](AllocOp op) { allocOps.push_back(op); });\n+  for (auto allocOp : allocOps) {\n+    normalizeMemRef(allocOp);\n+  }\n+    pass(\"simplify-affine-structures\",\n+         \"Simplify affine expressions in maps/sets and normalize memrefs\");",
    "Label": "clean"
},
{
    "Id": 652,
    "Library": "tensorflow",
    "Date": "2019/10/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8c8afe88694ac40d6c64f3d999dc29e8d7a23da7",
    "Root Cause": "N.A",
    "Bug report": "Remove debug leftover",
    "Number of deleted lines": 2,
    "Deleted lines": "-  nvinfer1::INetworkDefinition* network = params->converter->network();\n-  nvinfer1::IPoolingLayer* layer = network->addPoolingNd(*tensor, type, ksize);",
    "Added lines": "+  nvinfer1::IPoolingLayer* layer =\n+      params->converter->network()->addPoolingNd(*tensor, type, ksize);",
    "Label": "clean"
},
{
    "Id": 653,
    "Library": "tensorflow",
    "Date": "2019/10/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3f7f8ef9223f6902fa76aa2cfaf409d5ed185076",
    "Root Cause": "N.A",
    "Bug report": "Show type even if elementsattr is elided in graph\n\nThe type is quite useful for debugging and shouldn't be too large.\n\nPiperOrigin-RevId: 272390311",
    "Number of deleted lines": 2,
    "Deleted lines": "-// NOLINTNEXTLINE\n-      os << \"...\";",
    "Added lines": "+#include \"mlir/IR/StandardTypes.h\"\n+      os << std::string(elements.getType().getRank(), '[') << \"...\"\n+         << std::string(elements.getType().getRank(), ']') << \" : \"\n+         << elements.getType();",
    "Label": "clean"
},
{
    "Id": 654,
    "Library": "tensorflow",
    "Date": "2019/10/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2f1a73851905e14275b298e8f646cf1fe0e148c2",
    "Root Cause": "N.A",
    "Bug report": "Show type even if elementsattr is elided in graph\n\nThe type is quite useful for debugging and shouldn't be too large.\n\nPiperOrigin-RevId: 272390311",
    "Number of deleted lines": 2,
    "Deleted lines": "-// NOLINTNEXTLINE\n-      os << \"...\";",
    "Added lines": "+#include \"mlir/IR/StandardTypes.h\"\n+      os << std::string(elements.getType().getRank(), '[') << \"...\"\n+         << std::string(elements.getType().getRank(), ']') << \" : \"\n+         << elements.getType();",
    "Label": "clean"
},
{
    "Id": 655,
    "Library": "tensorflow",
    "Date": "2019/10/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7bee8ad08d5a1bdef84b314956d580cc20d82439",
    "Root Cause": "N.A",
    "Bug report": "Push date for broadcast_to in math_grad forward while we debug\n\nPiperOrigin-RevId: 272218609",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if compat.forward_compatible(2019, 9, 23):",
    "Added lines": "+  if compat.forward_compatible(2019, 10, 23):",
    "Label": "clean"
},
{
    "Id": 656,
    "Library": "tensorflow",
    "Date": "2019/09/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d590b643f18d27afe2e1abb388e7bfadf7c2763b",
    "Root Cause": "N.A",
    "Bug report": "Inspecting tensor shapes and types from within a checkpoint is a useful function\nof inspect_checkpoint tool. Unfortunately, the tool only displays tensor shapes\nand types when none of the --all_tensors, --all_tensor_names or --tensor_names\nflags are used (internally, this triggers using CheckpointReader::DebugString()).\n\nThis change modifies the tool to also display tensor shapes and types when using\n\"--all_tensors\", \"--all_tensor_names\" and \"--tensor_name <name>\" options,\nemulating the results from reader.debug_string() for the appropriate tensors.\n\nPiperOrigin-RevId: 271560612",
    "Number of deleted lines": 4,
    "Deleted lines": "-      for key in sorted(var_to_shape_map):\n-        print(\"tensor_name: \", key)\n-      print(reader.debug_string().decode(\"utf-8\"))\n-      print(\"tensor_name: \", tensor_name)",
    "Added lines": "+      var_to_dtype_map = reader.get_variable_to_dtype_map()\n+      for key, value in sorted(var_to_shape_map.items()):\n+        print(\"tensor: %s (%s) %s\" % (key, var_to_dtype_map[key].name, value))\n+      print(reader.debug_string().decode(\"utf-8\", errors=\"ignore\"))\n+      if not reader.has_tensor(tensor_name):\n+        print(\"Tensor %s not found in checkpoint\" % tensor_name)\n+        return\n+\n+      var_to_shape_map = reader.get_variable_to_shape_map()\n+      var_to_dtype_map = reader.get_variable_to_dtype_map()\n+      print(\"tensor: %s (%s) %s\" %\n+            (tensor_name, var_to_dtype_map[tensor_name].name,\n+             var_to_shape_map[tensor_name]))",
    "Label": "clean"
},
{
    "Id": 657,
    "Library": "tensorflow",
    "Date": "2019/09/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/38f9aa0025a7bf3bd6ba30cbec4cb291d7455d14",
    "Root Cause": "N.A",
    "Bug report": "[Grappler] Fix bug in RemoveStackStridedSliceSameAxis optimizer\n\nPiperOrigin-RevId: 271359941",
    "Number of deleted lines": 19,
    "Deleted lines": "-  bool GetConstantAsInt64(const NodeDef& node, DataType dtype,\n-                          std::vector<int64>* values) {\n-    if (dtype == DT_INT32) {\n-      std::vector<int32> values_int32;\n-      if (!ValuesFromConstNode(node, &values_int32)) {\n-        return false;\n-      }\n-      std::copy(values_int32.begin(), values_int32.end(),\n-                std::inserter(*values, values->begin()));\n-      return true;\n-    } else {\n-      return ValuesFromConstNode(node, values);\n-    }\n-  }\n-\n-    const int pack_input_rank = pack_output_shape->dims() - 1;\n-      // The ndims of any input into Pack op is its output ndims - 1.\n-      *pack_axis += pack_input_rank;\n-    if (*pack_axis < 0 || *pack_axis >= pack_input_rank) {",
    "Added lines": "+    const int pack_output_rank = pack_output_shape->dims();\n+      *pack_axis += pack_output_rank;\n+    if (*pack_axis < 0 || *pack_axis >= pack_output_rank) {",
    "Label": "clean"
},
{
    "Id": 658,
    "Library": "tensorflow",
    "Date": "2019/09/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cf0db0345bb6a200909faf78a655d499e32b7b1e",
    "Root Cause": "N.A",
    "Bug report": "Remove a debug log.\n\nPiperOrigin-RevId: 270971382",
    "Number of deleted lines": 1,
    "Deleted lines": "-  LOG(ERROR) << \"EmitHlo021Tile\";",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 659,
    "Library": "tensorflow",
    "Date": "2019/09/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e119e976b77030ea51629c4c300edf0085296f8f",
    "Root Cause": "N.A",
    "Bug report": "Remove unnecessary debug statement in flatbuffer_translate.cc.\n\nPiperOrigin-RevId: 270813041",
    "Number of deleted lines": 2,
    "Deleted lines": "-        fprintf(stderr, \"HERE: %s\\n\",\n-                output_names[it.index()].trim().str().c_str());",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 660,
    "Library": "tensorflow",
    "Date": "2019/09/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/70a20bb034b26d08d7cff82542510a81ccc1a5a3",
    "Root Cause": "N.A",
    "Bug report": "[Grappler] Fix bug in layout optimizer introduced by cl/247704284. Restore the original graph if Tune() fails.\n\nPiperOrigin-RevId: 270737190",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return Tune(item, graph_properties, config, output);",
    "Added lines": "+  Status status = Tune(item, graph_properties, config, output);\n+  if (!status.ok()) {\n+    *output = item.graph;\n+  }\n+  return status;",
    "Label": "clean"
},
{
    "Id": 661,
    "Library": "tensorflow",
    "Date": "2019/09/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/05876b2df8ecb10ee5c3439993cc6caeb8aee343",
    "Root Cause": "N.A",
    "Bug report": "Log name of the generated illegal operation name in DialectConversion debug mode\n\nPiperOrigin-RevId: 268859399",
    "Number of deleted lines": 2,
    "Deleted lines": "-    if (failed(legalize(rewriterImpl.createdOps[i], rewriter))) {\n-      LLVM_DEBUG(llvm::dbgs() << \"-- FAIL: Generated operation was illegal.\\n\");",
    "Added lines": "+    Operation *op = rewriterImpl.createdOps[i];\n+    if (failed(legalize(op, rewriter))) {\n+      LLVM_DEBUG(llvm::dbgs() << \"-- FAIL: Generated operation '\"\n+                              << op->getName() << \"' was illegal.\\n\");",
    "Label": "clean"
},
{
    "Id": 662,
    "Library": "tensorflow",
    "Date": "2019/09/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3aab403f8da84331436bc9ca17f767b5121fa358",
    "Root Cause": "N.A",
    "Bug report": "Log name of the generated illegal operation name in DialectConversion debug mode\n\nPiperOrigin-RevId: 268859399",
    "Number of deleted lines": 2,
    "Deleted lines": "-    if (failed(legalize(rewriterImpl.createdOps[i], rewriter))) {\n-      LLVM_DEBUG(llvm::dbgs() << \"-- FAIL: Generated operation was illegal.\\n\");",
    "Added lines": "+    Operation *op = rewriterImpl.createdOps[i];\n+    if (failed(legalize(op, rewriter))) {\n+      LLVM_DEBUG(llvm::dbgs() << \"-- FAIL: Generated operation '\"\n+                              << op->getName() << \"' was illegal.\\n\");",
    "Label": "clean"
},
{
    "Id": 663,
    "Library": "tensorflow",
    "Date": "2019/09/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/89af99d969edaaa7b3fe0b1e21062ae6a4261656",
    "Root Cause": "N.A",
    "Bug report": "fix a bug, populate node name before collector::save.\n\nPiperOrigin-RevId: 268556975",
    "Number of deleted lines": 1,
    "Deleted lines": "-          ns->set_node_name(std::move(activity_name));",
    "Added lines": "+          ns->set_node_name(activity_name);",
    "Label": "clean"
},
{
    "Id": 664,
    "Library": "tensorflow",
    "Date": "2019/09/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d59a63081a0287fc22e20827dd4855b4a3488867",
    "Root Cause": "N.A",
    "Bug report": "Add BuiltinOperatorName print in debug log info",
    "Number of deleted lines": 3,
    "Deleted lines": "-\n-      printf(\"Node %3zu Operator Builtin Code %3d\\n\", node_index,\n-             reg.builtin_code);",
    "Added lines": "+#include \"tensorflow/lite/schema/schema_generated.h\"\n+      printf(\"Node %3zu Operator Builtin Code %3d %s\\n\", node_index,\n+             reg.builtin_code, EnumNamesBuiltinOperator()[reg.builtin_code]);",
    "Label": "clean"
},
{
    "Id": 665,
    "Library": "tensorflow",
    "Date": "2019/09/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c782a538b0b90d93c6070ac177cb1f542272bcce",
    "Root Cause": "N.A",
    "Bug report": "Workaround for the bug in OpenGL Qualcomm driver. It appeared first time on Android 10. It's about sporadic overflowing of integer \"+\" and \"-\" operations.\n\nPiperOrigin-RevId: 268302540",
    "Number of deleted lines": 2,
    "Deleted lines": "-        int i = y * $kernel_size.x$ + x;\n-        ivec2 idx = gid.xy + ivec2(x, y) - $padding$;",
    "Added lines": "+      \n+        int i = int(float(y * $kernel_size.x$) + float(x));        \n+        ivec2 idx = ivec2(vec2(gid.xy + ivec2(x, y)) - vec2($padding$));\n+        ",
    "Label": "clean"
},
{
    "Id": 666,
    "Library": "tensorflow",
    "Date": "2019/09/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c45537230d52fcfd69cbe5757a46949d9a77bb7e",
    "Root Cause": "N.A",
    "Bug report": "Make debug message more human-readable.\n\nPiperOrigin-RevId: 268257613",
    "Number of deleted lines": 5,
    "Deleted lines": "-            << \"Edge \" << edge->src()->DebugString() << \":\"\n-            << edge->dst()->DebugString() << \" with dst_input \"\n-            << edge->dst_input() << \" and had pre-existing input edge \"\n-            << inputs[edge->dst_input()]->src()->DebugString() << \":\"\n-            << inputs[edge->dst_input()]->dst()->DebugString();",
    "Added lines": "+            << \"Edge \" << edge->src()->name() << \"->\" << edge->dst()->name()\n+            << \" conflicts with pre-existing input edge \"\n+            << inputs[edge->dst_input()]->src()->name() << \"->\"\n+            << inputs[edge->dst_input()]->dst()->name();",
    "Label": "clean"
},
{
    "Id": 667,
    "Library": "tensorflow",
    "Date": "2019/09/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8bbc0392bc2e843df07de38d8ab66414fbfa3e82",
    "Root Cause": "N.A",
    "Bug report": "[tf.contrib.seq2seq] Fix handling of sequence_length w/ beam search decoder.\n\nChange by Ran Tian; I'm just copying it here with minor cleanup.\n\nFix a bug of tf.contrib.seq2seq.dynamic_decode.\nWhen used with a tf.contrib.seq2seq.BeamSearchDecoder, the BeamSearchDecoder\nshuffles its beams at each step, which conflicts with the `dynamic_decode`\nfunction's tracking of finished states. Therefore, the flag\ndecoder.tracks_own_finished is turned on, and the finished states are correctly\nhandled. However, `sequence_lenghts` is also shuffled by the BeamSearchDecoder,\nbut this was not correctly handled previously. As a result,\nthe `final_sequence_lengths` returned by `dynamic_decode` was not correct. This\nfix updates the `sequence_lengths` from the returned decoder_state of\nBeamSearchDecoder.\n\nPiperOrigin-RevId: 268056568",
    "Number of deleted lines": 4,
    "Deleted lines": "-      next_sequence_lengths = array_ops.where(\n-          math_ops.logical_not(finished),\n-          array_ops.fill(array_ops.shape(sequence_lengths), time + 1),\n-          sequence_lengths)",
    "Added lines": "+      decoder_state_sequence_lengths = False\n+        lengths = getattr(decoder_state, \"lengths\", None)\n+        if lengths is not None:\n+          # sequence lengths are provided by decoder_state.lengths; overwrite\n+          # our sequence lengths.\n+          decoder_state_sequence_lengths = True\n+          sequence_lengths = math_ops.cast(lengths, dtypes.int32)\n+\n+      if decoder_state_sequence_lengths:\n+        # Just pass something through the loop; at the next iteration we'll pull\n+        # the sequence lengths from the decoder_state again.\n+        next_sequence_lengths = sequence_lengths\n+      else:\n+        next_sequence_lengths = array_ops.where(\n+            math_ops.logical_not(finished),\n+            array_ops.fill(array_ops.shape(sequence_lengths), time + 1),\n+            sequence_lengths)",
    "Label": "clean"
},
{
    "Id": 668,
    "Library": "tensorflow",
    "Date": "2019/09/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6713c594da837f1d031b865eed4369e622207aa1",
    "Root Cause": "N.A",
    "Bug report": "Wrap debug dump in LLVM_DEBUG\n\nPiperOrigin-RevId: 267774506",
    "Number of deleted lines": 4,
    "Deleted lines": "-    dbgs() << \"No object for \" << M->getModuleIdentifier()\n-           << \" in cache. Compiling.\\n\";\n-  dbgs() << \"Object for \" << M->getModuleIdentifier()\n-         << \" loaded from cache.\\n\";",
    "Added lines": "+#include \"llvm/Support/Debug.h\"\n+#define DEBUG_TYPE \"execution-engine\"\n+\n+    LLVM_DEBUG(dbgs() << \"No object for \" << M->getModuleIdentifier()\n+                      << \" in cache. Compiling.\\n\");\n+  LLVM_DEBUG(dbgs() << \"Object for \" << M->getModuleIdentifier()\n+                    << \" loaded from cache.\\n\");",
    "Label": "clean"
},
{
    "Id": 669,
    "Library": "tensorflow",
    "Date": "2019/09/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f48ab96e78715390c3f79cb9ff74b80e23897f25",
    "Root Cause": "N.A",
    "Bug report": "Wrap debug dump in LLVM_DEBUG\n\nPiperOrigin-RevId: 267774506",
    "Number of deleted lines": 4,
    "Deleted lines": "-    dbgs() << \"No object for \" << M->getModuleIdentifier()\n-           << \" in cache. Compiling.\\n\";\n-  dbgs() << \"Object for \" << M->getModuleIdentifier()\n-         << \" loaded from cache.\\n\";",
    "Added lines": "+#include \"llvm/Support/Debug.h\"\n+#define DEBUG_TYPE \"execution-engine\"\n+\n+    LLVM_DEBUG(dbgs() << \"No object for \" << M->getModuleIdentifier()\n+                      << \" in cache. Compiling.\\n\");\n+  LLVM_DEBUG(dbgs() << \"Object for \" << M->getModuleIdentifier()\n+                    << \" loaded from cache.\\n\");",
    "Label": "clean"
},
{
    "Id": 670,
    "Library": "tensorflow",
    "Date": "2019/09/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d42ac0e35e89222b6c3fba29edcf9e2403d50fda",
    "Root Cause": "N.A",
    "Bug report": "Fix backwards compatibility bug for new \"axis\" param in Dequantize and QuantizeV2.\n\nPiperOrigin-RevId: 267718155",
    "Number of deleted lines": 4,
    "Deleted lines": "-      int axis;\n-      TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", &axis));\n-      int axis;\n-      TF_RETURN_IF_ERROR(c->GetAttr(\"axis\", &axis));",
    "Added lines": "+      int axis = -1;\n+      Status s = c->GetAttr(\"axis\", &axis);\n+      if (!s.ok() && s.code() != error::NOT_FOUND) {\n+        return s;\n+      }\n+      int axis = -1;\n+      Status s = c->GetAttr(\"axis\", &axis);\n+      if (!s.ok() && s.code() != error::NOT_FOUND) {\n+        return s;\n+      }",
    "Label": "clean"
},
{
    "Id": 671,
    "Library": "tensorflow",
    "Date": "2019/09/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/90f35c4c87e7d76e7830c23847c55953ff54d33d",
    "Root Cause": "N.A",
    "Bug report": "it seems that callback can be triggered after unsubscribe (bug in cupti). Defend ourselves from that.\nprobably due to not calling cuptiFinalize in CUDA 10.1. We will evaluate that.\n\nPiperOrigin-RevId: 267419048",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (!api_tracing_enabled_) return Status::OK();  // already unsubscribed.",
    "Label": "clean"
},
{
    "Id": 672,
    "Library": "tensorflow",
    "Date": "2019/09/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/132bebc67527c6fc9c1cd342785630789e8df83c",
    "Root Cause": "N.A",
    "Bug report": "Remove debug string information from gen files\n\nPiperOrigin-RevId: 267286898",
    "Number of deleted lines": 5,
    "Deleted lines": "-  result.append(\"# \");\n-  auto ops_text = cleaned_ops.DebugString();\n-  absl::StripTrailingAsciiWhitespace(&ops_text);\n-  result.append(str_util::StringReplace(ops_text, \"\\n\", \"\\n# \", true));\n-  result.append(\"\\n\");",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 673,
    "Library": "tensorflow",
    "Date": "2019/09/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b50852ccac3d9e90af0568391ace90fc1da440e1",
    "Root Cause": "N.A",
    "Bug report": "Remove debugging output left behind\n\nPiperOrigin-RevId: 267135736",
    "Number of deleted lines": 1,
    "Deleted lines": "-    VLOG(1) << \"Roundtripping: \" << it.first << \" \" << it.second;",
    "Added lines": "+    VLOG(1) << \"Roundtripping: \" << it.first;",
    "Label": "clean"
},
{
    "Id": 674,
    "Library": "tensorflow",
    "Date": "2019/08/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b76fbd5d4d3ed92209e124746850004099687219",
    "Root Cause": "N.A",
    "Bug report": "remove left-over debug printf statement\n\nPiperOrigin-RevId: 266378933",
    "Number of deleted lines": 1,
    "Deleted lines": "-    printf(\"%d \\n\", data->output_multiplier);",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 675,
    "Library": "tensorflow",
    "Date": "2019/08/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4a3f6ccbc2f90099a5a6dd03d5fa88a6d5c12e63",
    "Root Cause": "N.A",
    "Bug report": "Fix a small bug. The guard should be to make sure 1 is the minimal value, instead of the max value. This only applies to where layer norm has identical input, which is extremely rare.\n\nPiperOrigin-RevId: 266047446",
    "Number of deleted lines": 4,
    "Deleted lines": "-      std::max(1, static_cast<int32_t>(10000 * layer_norm_input_scale));\n-      std::max(1, static_cast<int32_t>(10000 * layer_norm_forget_scale));\n-      std::max(1, static_cast<int32_t>(10000 * layer_norm_cell_scale));\n-      std::max(1, static_cast<int32_t>(10000 * layer_norm_output_scale));",
    "Added lines": "+      std::min(1, static_cast<int32_t>(10000 * layer_norm_input_scale));\n+      std::min(1, static_cast<int32_t>(10000 * layer_norm_forget_scale));\n+      std::min(1, static_cast<int32_t>(10000 * layer_norm_cell_scale));\n+      std::min(1, static_cast<int32_t>(10000 * layer_norm_output_scale));",
    "Label": "clean"
},
{
    "Id": 676,
    "Library": "tensorflow",
    "Date": "2019/07/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cd63197f6190176f45dad47b8b63ff0b625c0b50",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in the previous commit by returning Unimplemented.",
    "Number of deleted lines": 3,
    "Deleted lines": "-  HloInstruction* custom_call = [&]() -> HloInstruction* {\n-        Unimplemented(\n-  }();",
    "Added lines": "+  HloInstruction* custom_call = [&]() -> StatusOr<HloInstruction*> {\n+        return Unimplemented(\n+  }().ValueOrDie();",
    "Label": "clean"
},
{
    "Id": 677,
    "Library": "tensorflow",
    "Date": "2019/08/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/69a588b52aed9f08bb7f654c45c2f353dfbbba5e",
    "Root Cause": "N.A",
    "Bug report": "visualize.py bugfix: Scrolling on any subgraph will always zoom the last subgraph\n\nPiperOrigin-RevId: 264755031",
    "Number of deleted lines": 16,
    "Deleted lines": "-    // Build graph data\n-    var graph = %s;\n-\n-    var svg = d3.select(\"#subgraph%d\")\n-    var width = svg.attr(\"width\");\n-    var height = svg.attr(\"height\");\n-    // Make the graph scrollable.\n-    svg = svg.call(d3.zoom().on(\"zoom\", function() {\n-      svg.attr(\"transform\", d3.event.transform);\n-    })).append(\"g\");\n-    var color = d3.scaleOrdinal(d3.schemeDark2);\n-    var simulation = d3.forceSimulation()\n-        .force(\"link\", d3.forceLink().id(function(d) {return d.id;}))\n-        .force(\"charge\", d3.forceManyBody())\n-        .force(\"center\", d3.forceCenter(0.5 * width, 0.5 * height));\n-    function buildGraph() {",
    "Added lines": "+    function buildGraph() {\n+      // Build graph data\n+      var graph = %s;\n+      var svg = d3.select(\"#subgraph%d\")\n+      var width = svg.attr(\"width\");\n+      var height = svg.attr(\"height\");\n+      // Make the graph scrollable.\n+      svg = svg.call(d3.zoom().on(\"zoom\", function() {\n+        svg.attr(\"transform\", d3.event.transform);\n+      })).append(\"g\");\n+      var color = d3.scaleOrdinal(d3.schemeDark2);\n+      var simulation = d3.forceSimulation()\n+          .force(\"link\", d3.forceLink().id(function(d) {return d.id;}))\n+          .force(\"charge\", d3.forceManyBody())\n+          .force(\"center\", d3.forceCenter(0.5 * width, 0.5 * height));",
    "Label": "clean"
},
{
    "Id": 678,
    "Library": "tensorflow",
    "Date": "2019/08/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/8d8a3a2c952c2b7c67faf9eb2d49262c83732c90",
    "Root Cause": "N.A",
    "Bug report": "Include all registered kernels as a debug message.\n\nI find it useful when debugging kernel registration.\n\nPiperOrigin-RevId: 264306797",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include \"tensorflow/core/framework/op_kernel.h\"\n+  VLOG(4) << \"Available kernels for \" << op->Name() << \"are \"\n+          << KernelsRegisteredForOp(op->Name());",
    "Label": "clean"
},
{
    "Id": 679,
    "Library": "tensorflow",
    "Date": "2019/08/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b5ea5c9300b417a505726a77f5cb1d68e3267ab5",
    "Root Cause": "N.A",
    "Bug report": "Add shape debug output to Reshape errors.\n\nIt's often difficult to know which tensor failed to reshape, and knowing the input and output shapes can help track it down.\n\nPiperOrigin-RevId: 264198171",
    "Number of deleted lines": 2,
    "Deleted lines": "-            product));\n-                              output_shape.num_elements()));",
    "Added lines": "+            product, \". input_shape=\", input_shape.DebugString(),\n+            \" output_shape=\", output_shape.DebugString()));\n+                              output_shape.num_elements(),\n+                              \". input_shape=\", input_shape.DebugString(),\n+                              \" output_shape=\", output_shape.DebugString()));",
    "Label": "clean"
},
{
    "Id": 680,
    "Library": "tensorflow",
    "Date": "2019/08/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a913689fddb70729dbce45a2cad44f4bd0f03935",
    "Root Cause": "N.A",
    "Bug report": "Update the warning message in implementation_selector to VLOG.\n\nThis warning is most likely to be thrown when the tf function has been updated by the function optimizer, and then reaching implementation_selector with a new signature. At that time, the optimization is ready done, but the warning message gives user a wrong impression.\n\nChange it VLOG so that we can still see the information when we debug.\n\nPiperOrigin-RevId: 263786180",
    "Number of deleted lines": 2,
    "Deleted lines": "-    LOG(WARNING) << \"Skipping optimization due to error while loading function \"\n-                 << \"libraries: \" << status;",
    "Added lines": "+    VLOG(2) << \"Skipping optimization due to error while loading function \"\n+            << \"libraries: \" << status;",
    "Label": "clean"
},
{
    "Id": 681,
    "Library": "tensorflow",
    "Date": "2019/08/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/07cac23b3d333076f276f1fd4f2a83a5c0020ed3",
    "Root Cause": "N.A",
    "Bug report": "Include strcat.h in debug_node_key.cc\n\nIt was being transitively depended on. it should have been properly depending\non it.\n\nPiperOrigin-RevId: 263601875",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include \"tensorflow/core/lib/strings/strcat.h\"",
    "Label": "clean"
},
{
    "Id": 682,
    "Library": "tensorflow",
    "Date": "2019/08/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/52828d8c856d26a4396576b0c62ef7095f735281",
    "Root Cause": "N.A",
    "Bug report": "Fix Tensor equality bug in _run_internal_graph.\n\nWe can no longer simply say `t in collection` when `t` is a Tensor. We need to look up by id explicitly.\n\nPiperOrigin-RevId: 263598715",
    "Number of deleted lines": 4,
    "Deleted lines": "-                [\n-                    kwargs['training'] is t\n-                    for t in backend._GRAPH_LEARNING_PHASES.values()\n-                ]):",
    "Added lines": "+                any([kwargs['training'] is x\n+                     for x in backend._GRAPH_LEARNING_PHASES.values()])):",
    "Label": "clean"
},
{
    "Id": 683,
    "Library": "tensorflow",
    "Date": "2019/08/13",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/20180d323cdbca87b0bdf9fdad51d71ad7c73d37",
    "Root Cause": "N.A",
    "Bug report": "Make debug output VLOG(1).\n\nPiperOrigin-RevId: 263267773",
    "Number of deleted lines": 3,
    "Deleted lines": "-  LOG(INFO) << \"\\n++++++++ Reordering node \" << node->name() << \": \"\n-            << node->op() << \"(\" << left_child->op() << \", \"\n-            << right_child->op() << \")\\n\";",
    "Added lines": "+  VLOG(1) << \"\\n++++++++ Reordering node \" << node->name() << \": \" << node->op()\n+          << \"(\" << left_child->op() << \", \" << right_child->op() << \")\\n\";",
    "Label": "clean"
},
{
    "Id": 684,
    "Library": "tensorflow",
    "Date": "2019/08/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a28f6768043259fe6fd6b8fc8c62bed5b16aab16",
    "Root Cause": "N.A",
    "Bug report": "Add an Clif proto library for GraphDebugInfo\n\nThis build target is very useful for prototyping tools with GraphDebugInfo.\n\nPiperOrigin-RevId: 262955134",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+tf_pyclif_proto_library(\n+    name = \"protobuf/graph_debug_info_pyclif\",\n+    proto_lib = \":protos_all_cc\",\n+    proto_srcfile = \"protobuf/graph_debug_info.proto\",\n+    visibility = [\"//visibility:public\"],\n+)\n+",
    "Label": "clean"
},
{
    "Id": 685,
    "Library": "tensorflow",
    "Date": "2019/08/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/d1225113eb7ec3a7af9826c358a1290dfcc84c08",
    "Root Cause": "N.A",
    "Bug report": "Ruy: Fix bug in AVX-512 quant packing.\nPiperOrigin-RevId: 262926213",
    "Number of deleted lines": 1,
    "Deleted lines": "-  RUY_DCHECK_EQ(kHalfBlockOffset * 2, Layout::kRows * Layout::kRows);",
    "Added lines": "+  RUY_DCHECK_EQ(kHalfBlockOffset * 2, Layout::kRows * Layout::kCols);",
    "Label": "clean"
},
{
    "Id": 686,
    "Library": "tensorflow",
    "Date": "2019/08/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3afc0e4ecf947ccf7299f65cfe7a8c27dc9ca441",
    "Root Cause": "N.A",
    "Bug report": "The following PR/commit breaks the --config=rocm build\n\nhttps://github.com/tensorflow/tensorflow/commit/ddd77ee043ac720793d8dfb887b0eab3cfcb0adb\n\nIt introduces references to se::cuda::RedzoneAllocator (which is only visible in the CUDA build) within code that is common to both the ROCm and CUDA builds. This \"fix\" moves those reference to code that is visible only in the CUDA build\n\nThis is essentially the same bug + fix as in PR #31393 (but in a different file)",
    "Number of deleted lines": 6,
    "Deleted lines": "-  se::TfAllocatorAdapter tf_allocator_adapter(stream->parent()->platform(),\n-                                              ctx->device()->GetAllocator({}));\n-  se::cuda::RedzoneAllocator rz_allocator(stream, &tf_allocator_adapter,\n-                                          se::cuda::PtxCompilationOptions());\n-  se::DeviceMemory<T> in_backprop_ptr_rz(\n-      WrapRedzoneBestEffort(&rz_allocator, in_backprop_ptr));",
    "Added lines": "+\n+    se::TfAllocatorAdapter tf_allocator_adapter(\n+        stream->parent()->platform(), ctx->device()->GetAllocator({}));\n+\n+    se::cuda::RedzoneAllocator rz_allocator(stream, &tf_allocator_adapter,\n+                                            se::cuda::PtxCompilationOptions());\n+\n+    se::DeviceMemory<T> in_backprop_ptr_rz(\n+        WrapRedzoneBestEffort(&rz_allocator, in_backprop_ptr));\n+",
    "Label": "clean"
},
{
    "Id": 687,
    "Library": "tensorflow",
    "Date": "2019/08/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6b4b4d417db40595d3593802c560420591c3f2bc",
    "Root Cause": "N.A",
    "Bug report": "Disallow mixed precision to be used with TPUStrategy.\n\nThis is untested and probably very buggy. AutoCastVariables are known not to work properly with TPUMirroredVariables in certain cases.\n\nPiperOrigin-RevId: 261492089",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if self._dtype_policy.should_cast_variables and backend.is_tpu_strategy(\n+        ds_context.get_strategy()):\n+      # TODO(b/137859335): Supoprt this. AutoCastVariables currently do not work\n+      # properly when wrapping TPUMirroredVariables.\n+      raise ValueError('DType Policies ending in \"_with_float32_vars\" are '\n+                       'not yet supported with TPUStrategy. Got policy: %s' %\n+                       self._dtype_policy.name)\n+",
    "Label": "clean"
},
{
    "Id": 688,
    "Library": "tensorflow",
    "Date": "2019/08/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2d2bd4231ba98e8cc04a98675a7088838c671e1b",
    "Root Cause": "N.A",
    "Bug report": "Fixing bug that was trying to call append on a tuple. Fixed by converting to a list.\n\nPiperOrigin-RevId: 261336294",
    "Number of deleted lines": 1,
    "Deleted lines": "-      defaults = arg_spec.defaults or []",
    "Added lines": "+      defaults = list(arg_spec.defaults or [])",
    "Label": "clean"
},
{
    "Id": 689,
    "Library": "tensorflow",
    "Date": "2019/08/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/21488b7bca8313abe12fbdd4fc7cf4387a26d7bc",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in FoldTransposeIntoMatMul arithmetic optimization.\n\nPreviously, the optimization would leave the node map in an inconsistent state:\na non-folded input would continue to consider the pre-optimized node as its\noutput. If the non-folded input was subsequently optimized in the same pass of\nthe ArithmeticOptimizer, we could end up with an incorrect graph. To fix, we\nensure that the non-folded input (if any) is rewired to the new node.\n\nPiperOrigin-RevId: 261222411",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    } else {\n+      ctx().node_map->UpdateOutput(a->name(), node->name(), new_op->name());\n+    } else {\n+      ctx().node_map->UpdateOutput(b->name(), node->name(), new_op->name());",
    "Label": "clean"
},
{
    "Id": 690,
    "Library": "tensorflow",
    "Date": "2019/07/31",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/caa237ddd99cd6af9b8a562bd64ad2868e63c189",
    "Root Cause": "N.A",
    "Bug report": "Prevent to decompress the speech dataset every time \n\nIt's only necessary to decompress the speech dataset once after downloading, and repeated decompression slows down the debug progress.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    tarfile.open(filepath, 'r:gz').extractall(dest_directory)",
    "Added lines": "+      tarfile.open(filepath, 'r:gz').extractall(dest_directory)",
    "Label": "clean"
},
{
    "Id": 691,
    "Library": "tensorflow",
    "Date": "2019/07/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9ebead98618f310ddfeb8bcaa40c67147282e93f",
    "Root Cause": "N.A",
    "Bug report": "Fix an error in constant_folding.cc.\n\nImagine we have a ShapeN node with N=2 which can be constant folded. The current buggy logic is:\n1. 'tensors_to_fetch' will be: {{Shape 0 in constant graph, 0}, ShapeN node in original graph} and {{Shape 1 in constant graph, 0}, ShapeN node in original graph}.\n2. 'tensors_to_replace' will be {ShapeN node in original graph, 0} and {ShapeN node in original graph, 0}.\n3. when we replace tensors with constant values, we will replace ShapeN:0 twice, with ShapeN:0 value and ShapeN:1 value.\n\nFix is to add output index to 'tensors_to_fetch'. So now the logic becomes:\n1. 'tensors_to_fetch' will be: {{Shape 0 in constant graph, 0}, {ShapeN node in original graph, 0}} and {{Shape 1 in constant graph, 0}, {ShapeN node in original graph, 1}}.\n2. 'tensors_to_replace' will be {ShapeN node in original graph, 0} and {ShapeN node in original graph, 1}.\n3. tensors will be correctly replaced.\n\nPiperOrigin-RevId: 260069780",
    "Number of deleted lines": 9,
    "Deleted lines": "-    std::map<NodeAndOutput, Node*>* tensors_to_fetch,\n-               added_nodes.first});\n-               added_nodes.first});\n-  std::map<NodeAndOutput, Node*> tensors_to_fetch;\n-  std::vector<std::pair<NodeAndOutput, Node*>> tensors_to_fetch_sorted(\n-            [](const std::pair<NodeAndOutput, Node*>& n1,\n-               const std::pair<NodeAndOutput, Node*>& n2) {\n-              return n1.first.first->name() < n2.first.first->name();\n-    tensors_to_replace.push_back({n.second, n.first.second});",
    "Added lines": "+    std::map<NodeAndOutput, NodeAndOutput>* tensors_to_fetch,\n+               {added_nodes.first, out_edge->src_output()}});\n+               {added_nodes.first, out_edge->src_output()}});\n+  std::map<NodeAndOutput, NodeAndOutput> tensors_to_fetch;\n+  std::vector<std::pair<NodeAndOutput, NodeAndOutput>> tensors_to_fetch_sorted(\n+            [](const std::pair<NodeAndOutput, NodeAndOutput>& n1,\n+               const std::pair<NodeAndOutput, NodeAndOutput>& n2) {\n+              return std::tie(n1.first.first->name(), n1.first.second) <\n+                     std::tie(n2.first.first->name(), n2.first.second);\n+    tensors_to_replace.push_back(n.second);",
    "Label": "clean"
},
{
    "Id": 692,
    "Library": "tensorflow",
    "Date": "2019/07/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/3c180988a21626042da280865ad553ffdf145a13",
    "Root Cause": "N.A",
    "Bug report": "Surround device type in quotes\n\nThis makes it more obvious when the device type is (incorrectly) an empty string as it happened in a recent bug report.\n\nPiperOrigin-RevId: 259962563",
    "Number of deleted lines": 2,
    "Deleted lines": "-                              \"' OpKernel for \", DeviceTypeString(device_type),\n-                              \" devices compatible with node \",",
    "Added lines": "+                              \"' OpKernel for '\", DeviceTypeString(device_type),\n+                              \"' devices compatible with node \",",
    "Label": "clean"
},
{
    "Id": 693,
    "Library": "tensorflow",
    "Date": "2019/07/23",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b71bdb8980b3050bf7a147f060dcc23b352c6d7b",
    "Root Cause": "N.A",
    "Bug report": "Fix test for mod partitioning of embedding tables on CPU in TPUEstimator. Fix bug with sequence columns in a shared embedding.\n\nPiperOrigin-RevId: 259553876",
    "Number of deleted lines": 2,
    "Deleted lines": "-    tensor = fc_lib.SharedEmbeddingColumn._dense_tensor_internal(\n-        self, transformation_cache, state_manager)",
    "Added lines": "+    tensor = self._get_dense_tensor_internal(\n+        transformation_cache, state_manager)",
    "Label": "clean"
},
{
    "Id": 694,
    "Library": "tensorflow",
    "Date": "2019/07/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/710d3113bf63558aa8a0faccab9cdb562052692e",
    "Root Cause": "N.A",
    "Bug report": "Unwrap `initial_value` if it is a `CheckpointInitialValue` in collective_all_reduce_strategy's `initial_value_fn`. This fixes a bug where running keras_mnist_multi_worker with eager causes seg fault.\n\nPiperOrigin-RevId: 259393313",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+from tensorflow.python.training.tracking import base as trackable\n+              # Unwrap `initial_value` if it is a `CheckpointInitialValue`.\n+              # TODO(b/138130844): Revert the following check once\n+              # `CheckpointInitialValue` class is removed.\n+              if isinstance(initial_value, trackable.CheckpointInitialValue):\n+                initial_value = initial_value.wrapped_value",
    "Label": "clean"
},
{
    "Id": 695,
    "Library": "tensorflow",
    "Date": "2019/07/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f72f8bef216ed2712f1e69468e375c901b061ace",
    "Root Cause": "N.A",
    "Bug report": "Remove left over debug loggging.\n\nPiperOrigin-RevId: 259367050",
    "Number of deleted lines": 1,
    "Deleted lines": "-      LOG(INFO) << \"Got element = \" << element;",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 696,
    "Library": "tensorflow",
    "Date": "2019/07/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b3ccc749b87a7a6c298e10f66e25f6894c63e87d",
    "Root Cause": "N.A",
    "Bug report": "Fix large (>4GB) files reading on windows\n\nThere is a bug which prevents reading files larger than 4GB on Windows.\nTF uses ::ReadFile winapi function (see pread in windows_file_system.cc)\nThis function accepts requested bytes number as DWORD, which is 32 bit on both 32bit\nand 64bit systems. But WindowsRandomAccessFile::Read passes number of\nbytes as size_t which is 64 bit on 64 bit systems. Then there is a\nstaic_cast from 64 bit size_t to 32 bit DWORD, which causes the error.\nChanged to read such files in portions of no more than\nstd::numeric_limits<DWORD>::max() bytes.",
    "Number of deleted lines": 1,
    "Deleted lines": "-      SSIZE_T r = pread(hfile_, dst, n, offset);",
    "Added lines": "+      size_t requested_read_length;\n+      if (n > std::numeric_limits<DWORD>::max()) {\n+        requested_read_length = std::numeric_limits<DWORD>::max();\n+      } else {\n+        requested_read_length = n;\n+      }\n+      SSIZE_T r = pread(hfile_, dst, requested_read_length, offset);",
    "Label": "clean"
},
{
    "Id": 697,
    "Library": "tensorflow",
    "Date": "2019/07/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/02874a2e4272733cd2148ab9498f3ee4a06dc2da",
    "Root Cause": "N.A",
    "Bug report": "[tf.data] Fixing a bug in TFRecordWriter.\n\nThe problem was that the op kernel was not originally creating the `ResourceMgr` parameter of `IteratorContext`, which would cause any upstream dataset op that creates resources (such as `shuffle` or `cache`) to segfault.\n\nPiperOrigin-RevId: 259007273",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include \"tensorflow/core/framework/resource_mgr.h\"\n+      auto resource_mgr = absl::make_unique<ResourceMgr>();\n+      params.resource_mgr = resource_mgr.get();",
    "Label": "clean"
},
{
    "Id": 698,
    "Library": "tensorflow",
    "Date": "2019/07/16",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4cb97078f48d22f0f4c2455aaf5c8f192c688a5e",
    "Root Cause": "N.A",
    "Bug report": "Use the right lookup key when the graph debug info is imported and E2E tests\n\nWhen the graph debug info is generated by TensorFlow, the node stack trace is\nindexed by the string concatenation of function name and node name, so we have\nto use the same string to lookup the stack trace for the nodes.\n\nE2E tests for displaying stack traces in the error messages are added to verify\nthe stack traces for nodes from both graphs and functions can be displayed\ncorrectly.\n\nPiperOrigin-RevId: 258311493",
    "Number of deleted lines": 2,
    "Deleted lines": "-    for (const auto& node_name : original_nodes) {\n-      node_call_sites.push_back(node_name_to_call_site(node_name));",
    "Added lines": "+  auto original_funcs =\n+      node_def.experimental_debug_info().original_func_names();\n+    for (int i = 0, e = original_nodes.size(); i != e; ++i) {\n+      auto node_name = original_nodes[i];\n+      auto func_name = (i < original_funcs.size()) ? original_funcs[i] : \"\";\n+      // Use the catenation of function and node names as the lookup key. This\n+      // is to match the utility of generating the GraphDebugInfo.\n+      node_call_sites.push_back(node_name_to_call_site(func_name + node_name));",
    "Label": "clean"
},
{
    "Id": 699,
    "Library": "tensorflow",
    "Date": "2019/07/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/67436a5ff89461f7397fe756c7e5f69f611535fd",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in the canonicalizer when replacing constants via patterns.\n\nThe GreedyPatternRewriteDriver currently does not notify the OperationFolder when constants are removed as part of a pattern match. This materializes in a nasty bug where a different operation may be allocated to the same address. This causes an assertion in the OperationFolder when it gets notified of the new operations removal.\n\nPiperOrigin-RevId: 257817627",
    "Number of deleted lines": 3,
    "Deleted lines": "-  OperationFolder helper;\n-        helper.notifyRemoval(op);\n-      if (succeeded(helper.tryToFold(op, collectOps, collectOperandsAndUses))) {",
    "Added lines": "+    folder.notifyRemoval(op);\n+\n+  /// Non-pattern based folder for operations.\n+  OperationFolder folder;\n+        folder.notifyRemoval(op);\n+      if (succeeded(folder.tryToFold(op, collectOps, collectOperandsAndUses))) {",
    "Label": "clean"
},
{
    "Id": 700,
    "Library": "tensorflow",
    "Date": "2019/07/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/db6d5a4289c28a82fdec368e9a239d5757f542cd",
    "Root Cause": "N.A",
    "Bug report": "Disable using location for generating node names during export.\n\nb/137006652 prevents us from using location info (derived from experimental_debug_info) to generate node names. For now, first try using \"name\" attribute to generate node names.\n\nPiperOrigin-RevId: 257859924",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // TODO(prakalps): b/137006652 prevents us from using location info (derived\n+  // from experimental_debug_info) to generate node names. Until it is fixed,\n+  // first check for \"name\" attribute to get node name.\n+  if (auto attr = inst->getAttrOfType<mlir::StringAttr>(\"name\")) {\n+    return attr.getValue();\n+  }",
    "Label": "clean"
},
{
    "Id": 701,
    "Library": "tensorflow",
    "Date": "2019/07/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/95644131f481eee0356fe922b90e7ca08be2967e",
    "Root Cause": "N.A",
    "Bug report": "format image_ops_impl.py\n\nremoved debug print() statement",
    "Number of deleted lines": 1,
    "Deleted lines": "-  print(output.shape)  #=> (5, 24, 24, 3)",
    "Added lines": "+  output.shape  #=> (5, 24, 24, 3)",
    "Label": "clean"
},
{
    "Id": 702,
    "Library": "tensorflow",
    "Date": "2019/07/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/222ca11e0e28de021cf0b5514b2c4a0c334a5c76",
    "Root Cause": "N.A",
    "Bug report": "Add set -ex to ci_parameterized_build.sh\n\nThis helps in debugging build jobs\n\nPiperOrigin-RevId: 257472023",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+set -ex\n+",
    "Label": "clean"
},
{
    "Id": 703,
    "Library": "tensorflow",
    "Date": "2019/05/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9ab4b5983bf2b22b909bbca490970ec2de9f7c8d",
    "Root Cause": "N.A",
    "Bug report": "Better error message when we swap parameter to Session. I had this bug and search on the web do not bring good answer.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        if isinstance(target, config_pb2.ConfigProto):\n+          raise TypeError('target must be a string, but got %s.'\n+                          ' Did you do \"Session(config)\" instead of'\n+                          ' \"Session(config=config)\"?' % type(target))",
    "Label": "clean"
},
{
    "Id": 704,
    "Library": "tensorflow",
    "Date": "2019/07/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c88fd63cc599195cbf88885689e8630dd888bb6d",
    "Root Cause": "N.A",
    "Bug report": "[Grappler] Fix bug in arithmetic optimizer causing non-unique node names.\n\nPiperOrigin-RevId: 257124468",
    "Number of deleted lines": 1,
    "Deleted lines": "-           ctx().node_map->NodeExists(OuterNodeName(node, true));",
    "Added lines": "+           ctx().node_map->NodeExists(OuterNodeName(node, true)) ||\n+           ctx().node_map->NodeExists(InnerAddNodeName(node));",
    "Label": "clean"
},
{
    "Id": 705,
    "Library": "tensorflow",
    "Date": "2019/07/08",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/31ec5abe89d408183a62f7b2010a98fb70b72b08",
    "Root Cause": "N.A",
    "Bug report": "[Grappler] Fix a bug in RemoveStackStridedSliceSameAxis optimizer\n\nPiperOrigin-RevId: 257111618",
    "Number of deleted lines": 5,
    "Deleted lines": "-    NodeDef* input_slice;\n-    TF_RETURN_IF_ERROR(\n-        GetInputNode(pack->input(slice_start_value), &input_slice));\n-      output->add_input(input_slice->name());\n-      output->add_input(input_slice->name());",
    "Added lines": "+    const string& input_slice = pack->input(slice_start_value);\n+\n+      output->add_input(input_slice);\n+      output->add_input(input_slice);",
    "Label": "clean"
},
{
    "Id": 706,
    "Library": "tensorflow",
    "Date": "2019/07/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bd47c759176f0039026fd5cac8db247bf452de28",
    "Root Cause": "N.A",
    "Bug report": "Avoid leaking buffer when MessageToBuffer() fails on too-large (>2GB) messages.\n\nThis change also removes a workaround for a bug in protobuf that was fixed in https://github.com/protocolbuffers/protobuf/commit/36c5780a3d7f620d0d919f9e80b67780a443f4f7 .\n\nPiperOrigin-RevId: 256295194",
    "Number of deleted lines": 15,
    "Deleted lines": "-  void* buf = tensorflow::port::Malloc(proto_size);\n-  // SerializeToArray takes size as an int.\n-  // This next 'if' is a workaround till we update to depend on a version\n-  // of protocol buffers that includes\n-  // https://github.com/google/protobuf/pull/4739\n-  if (proto_size > std::numeric_limits<int>::max()) {\n-    return InvalidArgument(\"Cannot serialize protocol buffer of type \",\n-                           in.GetTypeName(), \" as the serialized size (\",\n-                           proto_size,\n-                           \"bytes) would be larger than the limit (\",\n-                           std::numeric_limits<int>::max(), \" bytes)\");\n-  }\n-  out->data_deallocator = [](void* data, size_t length) {\n-    tensorflow::port::Free(data);\n-  };",
    "Added lines": "+  void* buf = port::Malloc(proto_size);\n+    port::Free(buf);\n+  out->data_deallocator = [](void* data, size_t length) { port::Free(data); };",
    "Label": "clean"
},
{
    "Id": 707,
    "Library": "tensorflow",
    "Date": "2019/07/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/219fd8d727b0b1178a52a1d5d6627cf829a21f5e",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in ParameterServerStrategy that num_gpus_per_worker needs to be remembered for 1.x paths.\n\nPiperOrigin-RevId: 256211610",
    "Number of deleted lines": 3,
    "Deleted lines": "-        # Save the num_gpus_per_worker for configure method which is used by the\n-        # contrib version.\n-        self._num_gpus_per_worker = num_gpus",
    "Added lines": "+      # Save the num_gpus_per_worker for configure method which is used by the\n+      # contrib version.\n+      self._num_gpus_per_worker = num_gpus",
    "Label": "clean"
},
{
    "Id": 708,
    "Library": "tensorflow",
    "Date": "2019/07/02",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a96e718bd7716c47291b36a56a46cc6e43201ab8",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in GpuAtomicCasHelper",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return __float_as_int(",
    "Added lines": "+  return __int_as_float(",
    "Label": "clean"
},
{
    "Id": 709,
    "Library": "tensorflow",
    "Date": "2019/07/01",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f043da36052c648157059c2da4870095222006cc",
    "Root Cause": "N.A",
    "Bug report": "Point a few TODOs to bugs instead of people\n\nPiperOrigin-RevId: 255995613",
    "Number of deleted lines": 2,
    "Deleted lines": "-    // TODO(nareshmodi): This is a limitation of the current code base (but\n-  // TODO(nareshmodi): this is currently slow, but can be fixed by making",
    "Added lines": "+    // TODO(b/122851476): This is a limitation of the current code base (but\n+  // TODO(b/110044833): this is currently slow, but can be fixed by making",
    "Label": "clean"
},
{
    "Id": 710,
    "Library": "tensorflow",
    "Date": "2019/06/26",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/300590a1e39734809693b24ad0df2daa6cdf8482",
    "Root Cause": "N.A",
    "Bug report": "Add subgraph device into the dumped partitioned graph name for easier debugging.\n\nPiperOrigin-RevId: 255279496",
    "Number of deleted lines": 1,
    "Deleted lines": "-                          reinterpret_cast<uintptr_t>(optimized_subgraph)),",
    "Added lines": "+                          reinterpret_cast<uintptr_t>(optimized_subgraph), \"_\",\n+                          pair.first),",
    "Label": "clean"
},
{
    "Id": 711,
    "Library": "tensorflow",
    "Date": "2019/06/25",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1c14da0bc125b705c491bcd2a46a0dd00b74ab30",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in cuddn_fused_conv_rewriter.\n\nThe scale for the convolution result and for the side input must be scalar constants.\nBefore this change, any constant would be accepted.",
    "Number of deleted lines": 4,
    "Deleted lines": "-  using match::Constant;\n-  auto zero_pattern = Broadcast(match::ConstantScalar(0));\n-    auto alpha_pattern = Broadcast(Constant(&alpha_conv_instr));\n-    auto alpha_pattern = Broadcast(Constant(&alpha_side_input_instr));",
    "Added lines": "+  using match::ConstantScalar;\n+  auto zero_pattern = Broadcast(ConstantScalar(0));\n+    auto alpha_pattern = Broadcast(ConstantScalar(&alpha_conv_instr));\n+    auto alpha_pattern = Broadcast(ConstantScalar(&alpha_side_input_instr));",
    "Label": "clean"
},
{
    "Id": 712,
    "Library": "tensorflow",
    "Date": "2019/06/12",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bf19e4a398ffbead6f2552da5a2f0dffe5967d51",
    "Root Cause": "N.A",
    "Bug report": "bugfix:this file has been moved to another dir",
    "Number of deleted lines": 1,
    "Deleted lines": "-// third_party/tensorflow/compiler/tf2xla/functionalize_control_flow_pass_registration.cc",
    "Added lines": "+// tensorflow/compiler/tf2xla/functionalize_control_flow_pass_registration.cc",
    "Label": "clean"
},
{
    "Id": 713,
    "Library": "tensorflow",
    "Date": "2019/06/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/5228dca4f7d4fcb4fb04cbc93630756aee61ed98",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in TPUMirroredVariable that variable handle ids may collide if variables are created inside tf.function but outside is eager mode.\n\nPiperOrigin-RevId: 252727209",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if context.executing_eagerly():",
    "Added lines": "+    if ops.executing_eagerly_outside_functions():",
    "Label": "clean"
},
{
    "Id": 714,
    "Library": "tensorflow",
    "Date": "2019/06/11",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/f2adcc0bc1f27061d611f3382b752d3531a794c8",
    "Root Cause": "N.A",
    "Bug report": "Fix bugs exposed by last bug fix",
    "Number of deleted lines": 8,
    "Deleted lines": "-                        uint32* actual_crc32c) {\n-      if (!port::kLittleEndian) {\n-        // Checksum would have been computed on a little-endian value.\n-          *actual_crc32c, reinterpret_cast<const char*>(&string_lengths[i]),\n-  uint32 length_checksum = 0;\n-      sizeof(uint32), reinterpret_cast<char*>(&length_checksum),\n-      crc32c::Extend(*actual_crc32c, reinterpret_cast<char*>(&length_checksum),\n-        GetStringBackingBuffer(*ret), &actual_crc32c));",
    "Added lines": "+\n+                        uint32* actual_crc32c, bool need_to_swap_bytes) {\n+      if (need_to_swap_bytes) {\n+        // Checksum would have been computed on the source machine's byte order\n+      uint64 length = string_lengths[i];\n+      if (need_to_swap_bytes) {\n+        length = BYTE_SWAP_64(length);\n+      }\n+          *actual_crc32c, reinterpret_cast<const char*>(&length),\n+  uint32 raw_length_checksum = 0;   // Bytes in file\n+  uint32 length_checksum = 0;       // In-memory representation\n+      sizeof(uint32), reinterpret_cast<char*>(&raw_length_checksum),\n+  length_checksum = need_to_swap_bytes ? BYTE_SWAP_32(raw_length_checksum) :\n+    raw_length_checksum;\n+      crc32c::Extend(*actual_crc32c, reinterpret_cast<char*>(&raw_length_checksum),\n+        GetStringBackingBuffer(*ret), &actual_crc32c, need_to_swap_bytes_));",
    "Label": "clean"
},
{
    "Id": 715,
    "Library": "tensorflow",
    "Date": "2019/06/10",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/4b10957c79e3e694fe085770e63383734e79261b",
    "Root Cause": "N.A",
    "Bug report": "Re-enable distribute_lib_test in PIP integration build now that it is\nusing a less-buggy version of Python.\n\nPiperOrigin-RevId: 252469361",
    "Number of deleted lines": 1,
    "Deleted lines": "-      -//${PIP_TEST_ROOT}/tensorflow/python/distribute:distribute_lib_test \\",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 716,
    "Library": "tensorflow",
    "Date": "2019/06/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2ff8601fdf787baac4cc3a3f416cf13bac65c7d2",
    "Root Cause": "N.A",
    "Bug report": "Preserve temporary files when verbosity is above 2. This simplifies debugging.\n\nPiperOrigin-RevId: 252164200",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if delete_on_exit:",
    "Added lines": "+from tensorflow.python.autograph.utils import ag_logging\n+  if delete_on_exit and ag_logging.get_verbosity() < 3:",
    "Label": "clean"
},
{
    "Id": 717,
    "Library": "tensorflow",
    "Date": "2019/06/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6f4bc2c5157cd75bda73ea67a17e4aae92c8a78c",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Fix ConvertRandomBitsToUniformFloatingPoint to exclude the leading one\nbit in number of mantissa bits.\n\nWe use primitive_util::SignificandWidth to find the number of mantissa bits for\na floating point data type. This actually results in one more than the actual\nnumber of mantissa bits. Need to subtract one from this value. This was a bug\nwe introduce when extending the routine for double precision in cr/251649864.\n\nPiperOrigin-RevId: 252142995",
    "Number of deleted lines": 1,
    "Deleted lines": "-  int mantissa_bits = primitive_util::SignificandWidth(value_type);",
    "Added lines": "+  // Subtract one as SignificandWidth includes the leading 1 bit.\n+  int mantissa_bits = primitive_util::SignificandWidth(value_type) - 1;",
    "Label": "clean"
},
{
    "Id": 718,
    "Library": "tensorflow",
    "Date": "2019/06/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/14f0c841dc5e670e392621d3e2092b45600070ba",
    "Root Cause": "N.A",
    "Bug report": "fixed a bug in convert_image_dtype\n\nRemoved `tf.` from `dtype = tf.dtypes.as_dtype(dtype)`.",
    "Number of deleted lines": 1,
    "Deleted lines": "-  dtype = tf.dtypes.as_dtype(dtype)",
    "Added lines": "+  dtype = dtypes.as_dtype(dtype)",
    "Label": "clean"
},
{
    "Id": 719,
    "Library": "tensorflow",
    "Date": "2019/06/06",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/fcb8c37574eb7e8a3ff974a7c1a91d22f42568ff",
    "Root Cause": "N.A",
    "Bug report": "Fix reference-counting bug in nest.assert_same_structure when expand_composites=True\n\nPiperOrigin-RevId: 251898231",
    "Number of deleted lines": 9,
    "Deleted lines": "-    Safe_PyObjectPtr type_spec_1(\n-        IsTypeSpec(o1) ? o1 : PyObject_GetAttrString(o1, \"_type_spec\"));\n-    if (PyErr_Occurred() || type_spec_1 == nullptr) return false;\n-    Safe_PyObjectPtr type_spec_2(\n-        IsTypeSpec(o2) ? o2 : PyObject_GetAttrString(o2, \"_type_spec\"));\n-    if (PyErr_Occurred() || type_spec_2 == nullptr) return false;\n-        type_spec_1.get(), compatible_type, argspec, type_spec_2.get()));\n-          PyObjectToString(type_spec_1.get()), \" vs. \",\n-          PyObjectToString(type_spec_2.get()));",
    "Added lines": "+    Safe_PyObjectPtr owned_type_spec_1;\n+    PyObject* type_spec_1 = o1;\n+    if (IsCompositeTensor(o1)) {\n+      owned_type_spec_1.reset(PyObject_GetAttrString(o1, \"_type_spec\"));\n+      type_spec_1 = owned_type_spec_1.get();\n+    }\n+\n+    Safe_PyObjectPtr owned_type_spec_2;\n+    PyObject* type_spec_2 = o2;\n+    if (IsCompositeTensor(o2)) {\n+      owned_type_spec_2.reset(PyObject_GetAttrString(o2, \"_type_spec\"));\n+      type_spec_2 = owned_type_spec_2.get();\n+    }\n+        type_spec_1, compatible_type, argspec, type_spec_2));\n+          PyObjectToString(type_spec_1), \" vs. \",\n+          PyObjectToString(type_spec_2));",
    "Label": "clean"
},
{
    "Id": 720,
    "Library": "tensorflow",
    "Date": "2019/06/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c2c6fc9cd48b59b5d332409580bb3f939049b939",
    "Root Cause": "N.A",
    "Bug report": "Work around ClangTidy bug: it wrongly reports that assert(false)\nshould be replaced by static_assert(false, \"\").\n\nPiperOrigin-RevId: 251222995",
    "Number of deleted lines": 1,
    "Deleted lines": "-#define TFLITE_ASSERT_FALSE assert(false)",
    "Added lines": "+#ifdef NDEBUG\n+#define TFLITE_ASSERT_FALSE (static_cast<void>(0))\n+#else\n+#define TFLITE_ASSERT_FALSE TFLITE_ABORT\n+#endif\n+",
    "Label": "clean"
},
{
    "Id": 721,
    "Library": "tensorflow",
    "Date": "2019/05/24",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c1f6d72a7331921476f2e0ffa519a9b024abd747",
    "Root Cause": "N.A",
    "Bug report": "Fix incorrect result type inference for nested tuples & remove hacky const case.\n\n    Builders can be further refined but wanted to address bug where the result type was used beyond the first depth.\n\n--\n\nPiperOrigin-RevId: 249936004",
    "Number of deleted lines": 7,
    "Deleted lines": "-  // TODO: this is a hack to support various constant ops. We are assuming\n-  // all of them have no operands and one attribute here. Figure out a better\n-  // way to do this.\n-  bool isConstOp =\n-      resultOp.getNumOperands() == 0 && resultOp.getNumNativeAttributes() == 1;\n-\n-  if (isConstOp || isSameValueType || isBroadcastable || useFirstAttr) {",
    "Added lines": "+  if (isSameValueType || isBroadcastable || useFirstAttr || depth > 0) {\n+    // If depth == 0 we can use the equivalence of the source and target root\n+    // ops in the pattern to determine the return type.",
    "Label": "clean"
},
{
    "Id": 722,
    "Library": "tensorflow",
    "Date": "2019/05/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/9366340689fef873ddd269ca9059cfcb950be3bf",
    "Root Cause": "N.A",
    "Bug report": "Return nullptr on Region::getContainingOperation()/getContainingFunction() instead of asserting\n\n    This avoids crashing when trying to dump an operation nested in a region that isn't yet attached to an operation, which is quite useful when debugging.\n\n    This alone won't be enough to print an unlink Operation, it'll display `<<UNLINKED INSTRUCTION>>`.\n\n--\n\nPiperOrigin-RevId: 249496388",
    "Number of deleted lines": 2,
    "Deleted lines": "-  assert(!container.isNull() && \"no container\");\n-  assert(!container.isNull() && \"no container\");",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 723,
    "Library": "tensorflow",
    "Date": "2019/05/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/27a9d9b992ca8703b0e4310d7fb3935e1ee2eb10",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in toy LateLowering where a type conversion was using 'cast' instead of 'dyn_cast'.\n\n--\n\nPiperOrigin-RevId: 249325111",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (auto array = t.cast<toy::ToyArrayType>())",
    "Added lines": "+    if (auto array = t.dyn_cast<toy::ToyArrayType>())",
    "Label": "clean"
},
{
    "Id": 724,
    "Library": "tensorflow",
    "Date": "2019/05/30",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/606cfacbfc8ec629b9324bf293f9f81357bd6e6d",
    "Root Cause": "N.A",
    "Bug report": "Explicitly convert depset to list for Bazel compatibility\n\nWith this change, this command succeeds:\n  bazel build //tensorflow/core/debug:all --incompatible_depset_is_not_iterable --nobuild\n\nThe flag `incompatible_depset_is_not_iterable` will soon be\nenabled in Bazel by default.\n\nPiperOrigin-RevId: 250761030",
    "Number of deleted lines": 1,
    "Deleted lines": "-        for dep in input_dep.tf_collected_deps:",
    "Added lines": "+        for dep in input_dep.tf_collected_deps.to_list():",
    "Label": "clean"
},
{
    "Id": 725,
    "Library": "tensorflow",
    "Date": "2019/05/28",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/6ada4ab006525fae4e8fdc0d4b82ab415511836e",
    "Root Cause": "N.A",
    "Bug report": "Fix bugs where StrAppend() is used instead of StrAppendFormat() to print error messages.\n\nPiperOrigin-RevId: 250384180",
    "Number of deleted lines": 16,
    "Deleted lines": "-    absl::StrAppend(&spatial, \"%d \", spatial_size()[i]);\n-    absl::StrAppend(&spatial, \"%d \", spatial_size()[i]);\n-    absl::StrAppend(&desc, \"%d \", input_filter_dims()[i]);\n-    absl::StrAppend(&spatial, \"%d \", input_filter_dims()[i]);\n-    absl::StrAppend(&padding, \"%d \", this->padding()[i]);\n-    absl::StrAppend(&strides, \"%d \", this->strides()[i]);\n-    absl::StrAppend(&dilations, \"%d \", this->dilations()[i]);\n-    absl::StrAppend(&desc, \"p%d:%d\", i, padding()[i]);\n-    absl::StrAppend(&desc, \"_s%d:%d\", i, strides()[i]);\n-    absl::StrAppend(&desc, \"_d%d:%d\", i, dilations()[i]);\n-    absl::StrAppend(&window, \"%d \", window_[i]);\n-    absl::StrAppend(&strides, \"%d \", strides_[i]);\n-    absl::StrAppend(&padding, \"%d\", padding_[i]);\n-    absl::StrAppend(&window, \"_w%d:%d\", i, window_[i]);\n-    absl::StrAppend(&strides, \"_s%d:%d\", i, strides_[i]);\n-    absl::StrAppend(&padding, \"_p%d:%d\", i, padding_[i]);",
    "Added lines": "+    absl::StrAppendFormat(&spatial, \"%d \", spatial_size()[i]);\n+    absl::StrAppendFormat(&spatial, \"%d \", spatial_size()[i]);\n+    absl::StrAppendFormat(&desc, \"%d \", input_filter_dims()[i]);\n+    absl::StrAppendFormat(&spatial, \"%d \", input_filter_dims()[i]);\n+    absl::StrAppendFormat(&padding, \"%d \", this->padding()[i]);\n+    absl::StrAppendFormat(&strides, \"%d \", this->strides()[i]);\n+    absl::StrAppendFormat(&dilations, \"%d \", this->dilations()[i]);\n+    absl::StrAppendFormat(&desc, \"p%d:%d\", i, padding()[i]);\n+    absl::StrAppendFormat(&desc, \"_s%d:%d\", i, strides()[i]);\n+    absl::StrAppendFormat(&desc, \"_d%d:%d\", i, dilations()[i]);\n+    absl::StrAppendFormat(&window, \"%d \", window_[i]);\n+    absl::StrAppendFormat(&strides, \"%d \", strides_[i]);\n+    absl::StrAppendFormat(&padding, \"%d\", padding_[i]);\n+    absl::StrAppendFormat(&window, \"_w%d:%d\", i, window_[i]);\n+    absl::StrAppendFormat(&strides, \"_s%d:%d\", i, strides_[i]);\n+    absl::StrAppendFormat(&padding, \"_p%d:%d\", i, padding_[i]);",
    "Label": "clean"
},
{
    "Id": 726,
    "Library": "tensorflow",
    "Date": "2019/05/27",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e617d328529820f395180375282d42dfbad4dce3",
    "Root Cause": "N.A",
    "Bug report": "Add --announce_rc into bazelrc file\n\nThis will help understand and debug builds.\n\nPiperOrigin-RevId: 250106925",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+# Make Bazel print out all options from rc files.\n+build --announce_rc\n+",
    "Label": "clean"
},
{
    "Id": 727,
    "Library": "tensorflow",
    "Date": "2019/05/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/88e8960fb2968bac4fe8fa2fba860bfb5a26e064",
    "Root Cause": "N.A",
    "Bug report": "Fix an additional bug",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      if (!port::kLittleEndian) {\n+        // Checksum would have been computed on a little-endian value.\n+        elem_size_uint32 = BYTE_SWAP_32(elem_size_uint32)\n+      }",
    "Label": "clean"
},
{
    "Id": 728,
    "Library": "tensorflow",
    "Date": "2019/05/22",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cf5687d4b3f66fecbab4ac35f89be0b9edac17eb",
    "Root Cause": "N.A",
    "Bug report": "Update Eigen to https://bitbucket.org/eigen/eigen/commits/c99e30229a767a92368e81a5ca94d137013576e0\n\nUpdate script to preserve BLAS dynamic dispatch files, and fix a bug in patching stack limit for AVX512.\n\nPiperOrigin-RevId: 249492770",
    "Number of deleted lines": 4,
    "Deleted lines": "-        sha256 = \"0dde8fb87f5dad2e409c9f4ea1bebc54e694cf4f3b633081b0d51a55c00f9c9f\",\n-        strip_prefix = \"eigen-eigen-a0d250e79c79\",\n-            \"http://mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz\",\n-            \"https://bitbucket.org/eigen/eigen/get/a0d250e79c79.tar.gz\",",
    "Added lines": "+        sha256 = \"049b5d08adde8e806d43e862e9f0ce08ed3b5a5ff1b608a446bb76810f2a8d5d\",\n+        strip_prefix = \"eigen-eigen-c99e30229a76\",\n+            \"http://mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/c99e30229a76.tar.gz\",\n+            \"https://bitbucket.org/eigen/eigen/get/c99e30229a76.tar.gz\",",
    "Label": "clean"
},
{
    "Id": 729,
    "Library": "tensorflow",
    "Date": "2019/05/21",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ebc41609e27dcf0998d8970e77a2e1f53e13ac86",
    "Root Cause": "N.A",
    "Bug report": "Change `TF_FunctionDebugString` to return a compact function-like output.\n\nPiperOrigin-RevId: 249337428",
    "Number of deleted lines": 1,
    "Deleted lines": "-  const auto& debug_str = func->fdef.DebugString();",
    "Added lines": "+  const auto& debug_str = DebugString(func->fdef);",
    "Label": "clean"
},
{
    "Id": 730,
    "Library": "tensorflow",
    "Date": "2019/05/19",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/e637ca8bb8e40b5404b4397bc70361e86f299363",
    "Root Cause": "N.A",
    "Bug report": "Fix debug build: static constexpr data member must have a definition (until C++17)\n\n--\n\nPiperOrigin-RevId: 248990338",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+constexpr StringLiteral ProducerGenerator::kProducerName;\n+",
    "Label": "clean"
},
{
    "Id": 731,
    "Library": "tensorflow",
    "Date": "2019/05/14",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/a5ddd61215cf5d22faef5550310d4519e0480eaf",
    "Root Cause": "N.A",
    "Bug report": "When converting a location to an SMLoc, advance to the first non-whitespace if the column is unknown(zero). This also fixes a small bug with call stack printing.\n\n    Example:\n    /tmp/file_C.py:21:5: error: 'foo.bar' op attribute 'something'\n        raise app.UsageError('Too many command-line arguments.')\n        ^\n    /tmp/file_D.py:20:3: note: called from\n      if len(argv) > 1:\n      ^\n    /tmp/file_E.py:19:1: note: called from\n    def main(argv):\n    ^\n    /tmp/file_F.py:24:3: note: called from\n      app.run(main)\n      ^\n\n--\n\nPiperOrigin-RevId: 248151212",
    "Number of deleted lines": 3,
    "Deleted lines": "-    for (unsigned currentDepth = 0; currentDepth < callStackLimit && callLoc;\n-         ++currentDepth, callLoc = callerLoc.dyn_cast<CallSiteLoc>()) {\n-      callerLoc = callLoc->getCaller();",
    "Added lines": "+    for (unsigned curDepth = 0; curDepth < callStackLimit; ++curDepth) {\n+      if ((callLoc = callerLoc.dyn_cast<CallSiteLoc>()))\n+        callerLoc = callLoc->getCaller();\n+      else\n+        break;\n+  // If the column is zero, try to skip to the first non-whitespace character.\n+  if (columnNo == 0) {\n+    auto isNewline = [](char c) { return c == '\\n' || c == '\\r'; };\n+    auto isWhitespace = [](char c) { return c == ' ' || c == '\\t'; };\n+\n+    // Look for a valid non-whitespace character before the next line.\n+    for (auto *newPos = position; newPos < end && !isNewline(*newPos); ++newPos)\n+      if (!isWhitespace(*newPos))\n+        return llvm::SMLoc::getFromPointer(newPos);\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 732,
    "Library": "tensorflow",
    "Date": "2019/05/20",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ed0a8b6b21ba708b6b29e7b3717f8892ee8b2414",
    "Root Cause": "N.A",
    "Bug report": "Remove padding debug prints",
    "Number of deleted lines": 6,
    "Deleted lines": "-      VLOG(2) << \"Using SAME padding\";\n-    VLOG(2) << \"Set padding to: \" << DebugString(layer->getPadding());\n-      VLOG(2) << \"Using SAME padding\";\n-    VLOG(2) << \"Set padding to: \" << DebugString(layer->getPadding());\n-    VLOG(2) << \"Padding!!!: \" << padding[0].first << padding[0].second\n-            << padding[1].first << padding[1].second;",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 733,
    "Library": "tensorflow",
    "Date": "2019/05/17",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/cecf3f583ac9ff8f1d012e79ad3291b70f52371c",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in the docstring for spatial_softmax to say \"ij\" coordinate system.\n\nPiperOrigin-RevId: 248819137",
    "Number of deleted lines": 2,
    "Deleted lines": "-  feature keypoints [x1, y1, ... xN, yN] for all N channels.\n-      [x1, y1, ... xN, yN].",
    "Added lines": "+  feature keypoints [i1, j1, ... iN, jN] for all N channels.\n+      [i1, j1, ... iN, jN].",
    "Label": "clean"
},
{
    "Id": 734,
    "Library": "tensorflow",
    "Date": "2019/05/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/7d52c0c37c2326abdb1ab728081bf742852edda7",
    "Root Cause": "N.A",
    "Bug report": "Workaround of a bug in Dataflow Analysis\n\nA valid Dataflow Analysis should have a compact value set, where we\ncan use the id to index the value. This seems not true and breaks\nbuffer assignment.\n\nUse a workaround before we dig deeper into this issue. The code also\nmakes more sense and is more effective.\n\nPiperOrigin-RevId: 248347504",
    "Number of deleted lines": 4,
    "Deleted lines": "-      for (HloValue::Id id = 0;\n-           id < alias_analysis->dataflow_analysis().values().size(); id++) {\n-        auto& value = alias_analysis->dataflow_analysis().GetValue(id);\n-        value.set_color(BufferValue::Color(0));",
    "Added lines": "+      for (HloValue* value : alias_analysis->dataflow_analysis().values()) {\n+        value->set_color(BufferValue::Color(0));",
    "Label": "clean"
},
{
    "Id": 735,
    "Library": "tensorflow",
    "Date": "2019/05/15",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/ba63891c8b2eb4776a0a2cac65df844bb141c37b",
    "Root Cause": "N.A",
    "Bug report": "TensorHandle::OnHostCPU is no longer always true inside tf.py_function\n\nPrior to this change a TensorHandle was assumed to reside on CPU if\n\n  device_ == nullptr || (ctx_ == nullptr || ctx_->HostCPU() == device_)\n\nThis resulted in inconsistencies when using tf.py_function since it\nalways creates eager tensors with nullptr ctx_ (see MakeArgTuple in\ncore/py_func.cc).\n\nFor instance, the following snippet\n\n  with tf.device(\"gpu:0\"):\n    tf.py_function(np.sin, [tf.constant(42.0)], [tf.float32])\n\nproduces a TensorHandle with device \"[...]/device:GPU:0\" and nullptr\ncontext and therefore is OnHostCPU.\n\nNote that this \"bug\" is unnoticeable currently because NumPy converts\neager tensors to arrays via __array__ which always copies a tensor\nto the CPU.\n\nPiperOrigin-RevId: 248297672",
    "Number of deleted lines": 1,
    "Deleted lines": "-           (ctx_ == nullptr || ctx_->HostCPU() == device_);",
    "Added lines": "+           (ctx_ != nullptr && ctx_->HostCPU() == device_);",
    "Label": "clean"
},
{
    "Id": 736,
    "Library": "tensorflow",
    "Date": "2019/05/09",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/1751a9f117baebdaadd04cfaa90393964055ae05",
    "Root Cause": "N.A",
    "Bug report": "[XLA] Use .at() instead of operator[] in InversePermutation and ComposePermutations.\n\n.at() has the property that it will crash on out-of-bounds accesses.\n\nThis catches a bug in AlgebraicSimplifier, which suggests to me it's\nworthwhile.  Anyway the permutations we're dealing with are usually small.\n\nNo functional change.\n\nPiperOrigin-RevId: 247507797",
    "Number of deleted lines": 2,
    "Deleted lines": "-    output_permutation[input_permutation[i]] = i;\n-    output.push_back(p1[p2[i]]);",
    "Added lines": "+    output_permutation.at(input_permutation.at(i)) = i;\n+    output.push_back(p1.at(p2.at(i)));",
    "Label": "clean"
},
{
    "Id": 737,
    "Library": "tensorflow",
    "Date": "2019/05/07",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/b493244d7848fc9f720a9847f0b4a23f9b5b5c60",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in zlib_inputstream Inflate.\nIf no output is generated after call to Inflate, read data from input stream.\nEarlier we would wait for zlib to consume all data before reading more input data. That assumption was incorrect. zlib can leave unread data in the input buffer if it is unable to make any progress.\n\nPiperOrigin-RevId: 247083700",
    "Number of deleted lines": 13,
    "Deleted lines": "-    // Step 1. Fill up input buffer.\n-    // We read from stream only after the previously read contents have been\n-    // completely consumed. This is an optimization and can be removed if\n-    // it causes problems. `ReadFromStream` is capable of handling partially\n-    // filled up buffers.\n-    if (z_stream_def_->stream->avail_in == 0) {\n-      TF_RETURN_IF_ERROR(ReadFromStream());\n-    }\n-\n-    // Step 2. Setup output stream.\n-    // Step 3. Inflate Inflate Inflate!\n-    bytes_to_read -= ReadBytesFromCache(bytes_to_read, result);\n-  if (error != Z_OK && error != Z_STREAM_END) {",
    "Added lines": "+    // Step 1. Setup output stream.\n+    // Step 2. Try to inflate some input data.\n+    // Step 3. Read any data produced by inflate. If no progress was made by\n+    // inflate, read more compressed data from the input stream.\n+    if (NumUnreadBytes() == 0) {\n+      TF_RETURN_IF_ERROR(ReadFromStream());\n+    } else {\n+      bytes_to_read -= ReadBytesFromCache(bytes_to_read, result);\n+    }\n+  // Source: http://zlib.net/manual.html\n+  // Z_BUF_ERROR: `inflate` returns Z_BUF_ERROR if no progress was made. This is\n+  // not fatal and `inflate` can be called again with more input and output\n+  // space to continue inflating.\n+  if (error != Z_OK && error != Z_STREAM_END && error != Z_BUF_ERROR) {",
    "Label": "clean"
},
{
    "Id": 738,
    "Library": "tensorflow",
    "Date": "2019/05/03",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/bbc71ba46b03d2e06cfb9f7e845bdb289c4c3a83",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in LoopTiling where a loop with trip count of 1 caused a division by zero\n\n--\n\nPiperOrigin-RevId: 246480710",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (tSizeAdjusted > constTripCount / 2)",
    "Added lines": "+    if (constTripCount > 1 && tSizeAdjusted > constTripCount / 2)",
    "Label": "clean"
},
{
    "Id": 739,
    "Library": "tensorflow",
    "Date": "2019/05/05",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/2112f2e9d62019621948672996f0fe527e3a77c7",
    "Root Cause": "N.A",
    "Bug report": "fix nccl build bug",
    "Number of deleted lines": 1,
    "Deleted lines": "-        repository_ctx.template(\"BUILD\", _label(\"system.BUILD.tpl\"), config)",
    "Added lines": "+        config_wrap = {\n+            \"%{nccl_version}\": config[\"nccl_version\"],\n+            \"%{nccl_header_dir}\": config[\"nccl_include_dir\"],\n+            \"%{nccl_library_dir}\": config[\"nccl_library_dir\"]}\n+        repository_ctx.template(\"BUILD\", _label(\"system.BUILD.tpl\"), config_wrap)",
    "Label": "clean"
},
{
    "Id": 740,
    "Library": "tensorflow",
    "Date": "2019/05/04",
    "Commit Link": "https://github.com/tensorflow/tensorflow/commit/c0b8378f08028ad05b82af9c04a88a4499f343d7",
    "Root Cause": "N.A",
    "Bug report": "Minor fix. Take a statement out of DCHECK since DCHECK doesn't do anything in non-debug mode. Also remove a redundant if.\n\nPiperOrigin-RevId: 246667869",
    "Number of deleted lines": 4,
    "Deleted lines": "-          if (type != DT_INT32 && type != DT_INT64) return false;\n-          DCHECK(tensor.FromProto(*proto))\n-              << \"Could not construct Tensor from TensorProto in node: \"\n-              << node->name();",
    "Added lines": "+          if (!tensor.FromProto(*proto)) {\n+            TF_CHECK_OK(errors::InvalidArgument(\n+                \"Could not construct Tensor from TensorProto in node: \",\n+                node->name()));\n+            return false;\n+          }",
    "Label": "clean"
},
{
    "Id": 741,
    "Library": "pytorch",
    "Date": "2024/07/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/419b8df0b6fcf886f03863fcd728e037bcc19d50",
    "Root Cause": "N.A",
    "Bug report": "[inductor][easy] add debug logs for inlining constants (#130799)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130799\nApproved by: https://github.com/chenyang78",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                log.debug(\"Inlining constant: %s \", str(target))",
    "Label": "clean"
},
{
    "Id": 742,
    "Library": "pytorch",
    "Date": "2024/07/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a29052a0bf3eb218f18b03e3a330cf805d50d864",
    "Root Cause": "N.A",
    "Bug report": "[BE][Ez]: Update ruff to 0.5.2 (#130698)\n\nUpdate ruff to 0.5.2 which bugfixes and performance improvements\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130698\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 1,
    "Deleted lines": "-    'ruff==0.5.0',",
    "Added lines": "+    'ruff==0.5.2',",
    "Label": "clean"
},
{
    "Id": 743,
    "Library": "pytorch",
    "Date": "2024/07/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2c4303c1d1fc75bb96c0017a8aeb844e4134db60",
    "Root Cause": "N.A",
    "Bug report": "[ROCm] [BUGFIX] Re-enable rocm-specific tuning parameters (#130617)\n\nSmall bug fix - https://github.com/pytorch/pytorch/pull/124592 replaced the torch.version.hip with device_props but made a mistake in porting the original logic.\n\nThe original code was:\n```\nif torch.version.hip is not None:\n```\n\nWhich was incorrectly replaced by:\n```\nif self.device_props.type != \"hip\":\n```\n\nPerhaps we need to write some unit tests here in the future.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130617\nApproved by: https://github.com/masnesral",
    "Number of deleted lines": 1,
    "Deleted lines": "-            if self.device_props.type != \"hip\":",
    "Added lines": "+            if self.device_props.type == \"hip\":",
    "Label": "clean"
},
{
    "Id": 744,
    "Library": "pytorch",
    "Date": "2024/07/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/df83142131ff4b644ccc6c63afb304b15325b4af",
    "Root Cause": "N.A",
    "Bug report": "[CCA][Memory Snapshot] Stop duplicating annotations to all device_traces (#130315)\n\nSummary: This diff fixes a bug, where all record_annotations will save a TraceEntry to each of the device_traces. Instead, we should only save annotations to the current device_trace that is being called by the thread calling the native allocator's recordAnnotation.\n\nTest Plan: CI and ran workloads on MVAI WPR FBR.\n\nReviewed By: zdevito\n\nDifferential Revision: D59477339\n\nPulled By: aaronenyeshi\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130315\nApproved by: https://github.com/zdevito",
    "Number of deleted lines": 3,
    "Deleted lines": "-    for (auto& allocator : device_allocator) {\n-      allocator->recordAnnotation(name);\n-    }",
    "Added lines": "+    c10::DeviceIndex device = 0;\n+    C10_CUDA_CHECK(c10::cuda::GetDevice(&device));\n+    device_allocator[device]->recordAnnotation(name);",
    "Label": "clean"
},
{
    "Id": 745,
    "Library": "pytorch",
    "Date": "2024/07/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bffb278700dd1e4a85a02f843e3657b6d8481f57",
    "Root Cause": "N.A",
    "Bug report": "[ONNX] Add `artifacts_dir` to torch-onnx-patch in benchmark (#130069)\n\nAdd `artifacts_dir` to torch-onnx-patch to save error report for debugging.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/130069\nApproved by: https://github.com/justinchuby",
    "Number of deleted lines": 1,
    "Deleted lines": "-            torch_onnx.patch_torch(error_report=True, profile=True)",
    "Added lines": "+            torch_onnx.patch_torch(\n+                error_report=True,\n+                profile=True,\n+                dump_exported_program=False,\n+                artifacts_dir=os.path.dirname(output_path),\n+            )",
    "Label": "clean"
},
{
    "Id": 746,
    "Library": "pytorch",
    "Date": "2024/07/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/04a0d856207d83c2031e4b9cb6825ba3e0092850",
    "Root Cause": "N.A",
    "Bug report": "[BE] Print all pip packages installed on the system after TorchChat (#129809)\n\nTo make debugging regressions like ones happened last Wed when new version of torchao was released, that resulted in TorchBench downgrading pytorch version to 2.3.1\n\nTest plan: Look at the log output for example https://github.com/pytorch/pytorch/actions/runs/9720408234/job/26832794157?pr=129809#step:20:1158 contains\n```\n+ echo 'Print all dependencies after TorchBench is installed'\nPrint all dependencies after TorchBench is installed\n+ python -mpip freeze\nabsl-py==2.1.0\naccelerate==0.31.0\naiohttp==3.9.5\naiosignal==1.3.1\nastunparse==1.6.3\nasync-timeout==4.0.3\nattrs==23.2.0\naudioread==3.0.1\nbeautifulsoup4==4.12.3\nboto3==1.19.12\nbotocore==1.22.12\nbs4==0.0.2\ncachetools==5.3.3\ncertifi==2024.6.2\ncffi==1.16.0\ncharset-normalizer==3.3.2\nclick==8.1.7\n...\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129809\nApproved by: https://github.com/kit1980, https://github.com/atalman",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  echo \"Print all dependencies after TorchBench is installed\"\n+  python -mpip freeze",
    "Label": "clean"
},
{
    "Id": 747,
    "Library": "pytorch",
    "Date": "2024/06/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1865fe282f2dc6e2c1746e418b809dea8771843a",
    "Root Cause": "N.A",
    "Bug report": "Log whenever we sleep (#129197)\n\nSummary:\nLog whenever we sleep for heartbeatTimeout.\nUseful for debugging stuck jobs.\nThis will eventually turn into a metric.\n\nTest Plan:\nnone.\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129197\nApproved by: https://github.com/Skylion007, https://github.com/d4l3k, https://github.com/wconstab",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    LOG(INFO) << logPrefix() << \"slept for \" << heartbeatTimeoutInSec_\n+              << \" waiting for desync report or process group destroy.\";\n+            LOG(INFO) << logPrefix() << \"slept for \" << heartbeatTimeoutInSec_\n+                      << \" giving time for flight recorder dumps to finish.\";",
    "Label": "clean"
},
{
    "Id": 748,
    "Library": "pytorch",
    "Date": "2024/06/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/81de71fdc5b2562bc32d8d87f59918f10fa106bc",
    "Root Cause": "N.A",
    "Bug report": "[inductor] fix a double clone in coordesc tuning (#129399)\n\nIt's embarrassing that there is a hidden double clone bug in coordinate descent tuning.\n\nIn `CachingAutotuner.coordinate_descent_tuning`, we clone mutated args to make sure benchmarking does not cause numerical problems. But latter on in `CachingAutotuner.bench` we do that again.\n\nThis double clone is fine if\n- the tensor is small\n- the allocation of the tensor is not on the critical path for memory footprint.\n\nBut neither holds for quite common usage of cross entropy loss.\n\nThis is related to the memory usage debugging in https://github.com/pytorch/pytorch/pull/129043 . Note that the general issue that peak memory usage increasing due to autotuning still exists. This bug just makes it worse (since we double allocate).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129399\nApproved by: https://github.com/Chillee, https://github.com/jansel",
    "Number of deleted lines": 2,
    "Deleted lines": "-        cloned_args, _ = self.clone_args(*args)\n-            out = self.bench(launcher, *cloned_args, **kwargs)",
    "Added lines": "+            out = self.bench(launcher, *args, **kwargs)",
    "Label": "clean"
},
{
    "Id": 749,
    "Library": "pytorch",
    "Date": "2024/06/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/aee512cc9d1f0ae3eb251caa648aff09fa3aa0cc",
    "Root Cause": "N.A",
    "Bug report": "[dtensor][op] Fixed stack op strategy (#129018)\n\n**Summary**\nThe previous stack op strategy was causing the input to be resharded, resulting in list index out of range error. I delayed the resharding for after the input_specs were created so that the new dimension could be inserted, preventing the error above. I have also ran all the other test cases to ensure changes did not introduce any new bugs\n\n**Test Plan**\npytest test/distributed/_tensor/test_tensor_ops.py -s -k test_stack\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/129018\nApproved by: https://github.com/XilunWu",
    "Number of deleted lines": 1,
    "Deleted lines": "-    follow_placements = normalize_shard_for_stack(follow_placements, dim)",
    "Added lines": "+\n+    follow_placements = normalize_shard_for_stack(follow_placements, dim)\n+",
    "Label": "clean"
},
{
    "Id": 750,
    "Library": "pytorch",
    "Date": "2024/06/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ba19ed9a1a4efc7fa9ccdd6ace6a85fd1f43b49f",
    "Root Cause": "N.A",
    "Bug report": "FunctionalTensor: dispatch metadata directly to inner tensor (#127927)\n\nFixes https://github.com/pytorch/pytorch/issues/127374\n\nThe error in the linked repro is:\n```\nAssertionError: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode with 'allow_non_fake_inputs'. Found in aten.sym_storage_offset.default(_to_functional_tensor(FakeTensor(..., device='cuda:0', size=(16, 4), dtype=torch.uint8),\n       device='cuda:0'))\n```\n\nWhere we hit FakeTensor.__torch_dispatch__, but our input is a C++ `FunctionalTensorWrapper`.\n\nWhat should actually have happened is that the call to `aten.sym_storage_offset` hits the `Functionalize` dispatch key, which should remove the `FunctionalTensorWrapper`  and redispatch. I spent some time debugging and haven't actually figured out why this isn't happening. Instead, this PR just skips that step completely, and asks `FunctionalTensor` to directly unwrap the C++ `FunctionalTensorWrapper` when querying tensor metadata.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127927\nApproved by: https://github.com/tugsbayasgalan",
    "Number of deleted lines": 2,
    "Deleted lines": "-                return func(args[0].elem, args[1])\n-            return func(args[0].elem)",
    "Added lines": "+                return func(torch._from_functional_tensor(args[0].elem), args[1])\n+            return func(torch._from_functional_tensor(args[0].elem))",
    "Label": "clean"
},
{
    "Id": 751,
    "Library": "pytorch",
    "Date": "2024/06/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d5780396c70375458344bfe6950fbf4c391e8622",
    "Root Cause": "N.A",
    "Bug report": "Skip debug asserts for mixed dense, subclass views in autograd_not_implemented_fallback (#128057)\n\nFixes #125503\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128057\nApproved by: https://github.com/albanD, https://github.com/soulitzer\nghstack dependencies: #127007",
    "Number of deleted lines": 6,
    "Deleted lines": "-        TORCH_INTERNAL_ASSERT(\n-            aliased_input.storage().is_alias_of(aliased_output.storage()),\n-            op_name);\n-      } else {\n-        const auto aliased_output_vec = aliased_output_iv.toTensorVector();\n-        for (const auto& aliased_output : aliased_output_vec) {",
    "Added lines": "+        // for now, skip asserts for subclasses\n+        // TODO: Fix the aliasing situation involving subclasses\n+        if (!at::impl::dispatch_mode_enabled() &&\n+            !at::impl::tensor_has_dispatch(aliased_input) &&\n+            !at::impl::tensor_has_dispatch(aliased_output)) {\n+      } else {\n+        const auto aliased_output_vec = aliased_output_iv.toTensorVector();\n+        for (const auto& aliased_output : aliased_output_vec) {\n+          // for now, skip asserts for subclasses\n+          // TODO: Fix the aliasing situation involving subclasses\n+          if (!at::impl::dispatch_mode_enabled() &&\n+              !at::impl::tensor_has_dispatch(aliased_input) &&\n+              !at::impl::tensor_has_dispatch(aliased_output)) {\n+            TORCH_INTERNAL_ASSERT(\n+                aliased_input.storage().is_alias_of(aliased_output.storage()),\n+                op_name);\n+          }\n+        }",
    "Label": "clean"
},
{
    "Id": 752,
    "Library": "pytorch",
    "Date": "2024/06/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f4edd67fe7d5b4ab28535eb524fe509aa5f894b7",
    "Root Cause": "N.A",
    "Bug report": "[c10d] fix OSS commSplit bug (#128459)\n\nSummary:\nD56907877 modified OSS commSplit. However, commSplit requires every rank being called even though it is no-color. ncclCommSplit will not create a communicator for nocolor ranks hence this line of code will potentially throw error like `NCCL WARN CommUserRank : comm argument is NULL`\n\nRevert this change from D56907877\n\nTest Plan: CI\n\nDifferential Revision: D58436088\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128459\nApproved by: https://github.com/shuqiangzhang",
    "Number of deleted lines": 1,
    "Deleted lines": "-  ncclCommUserRank(comm->ncclComm_, &comm->rank_);",
    "Added lines": "+  comm->rank_ = rank;",
    "Label": "clean"
},
{
    "Id": 753,
    "Library": "pytorch",
    "Date": "2024/06/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8b3daf1768fdf8c1c54771fd738552c4fa984b12",
    "Root Cause": "N.A",
    "Bug report": "Add FloatTrueDiv and ToFloat to SYMPY_INTERP (#128418)\n\nSummary: I admit I'm not 100% sure what I'm doing here. I'm hitting a bug in the FX graph cache when we try to evaluate a guards expression. We're creating guards that look like this:\n```\nNe(CeilToInt(FloatTrueDiv(ToFloat(8*L['t0']) - 4.0, 8.0))*CeilToInt(FloatTrueDiv(ToFloat(8*L['t1']) - 4.0, 8.0)), CeilToInt(FloatTrueDiv(ToFloat(8*L['t1']) - 4.0, 8.0))) and ...\n```\nIt looks like we have a facility to define these operators in the SYMPY_INTERP map and we're just missing FloatTrueDiv and ToFloat. What's surprsing to me is that we're only hitting this problem with the FX graph enabled. We can create such guards, but we've never actually evaluated any?\n\nTest Plan:\n`TORCHINDUCTOR_FX_GRAPH_CACHE=1 python benchmarks/dynamo/torchbench.py --ci --accuracy --timing --explain --inductor --device cuda --inference --bfloat16 --only detectron2_fcos_r_50_fpn`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128418\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    'FloatTrueDiv': operator.truediv,\n+    'ToFloat': builtins.float,",
    "Label": "clean"
},
{
    "Id": 754,
    "Library": "pytorch",
    "Date": "2024/06/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3a620a0f653f26c638a72ce97abe557659b3a8c3",
    "Root Cause": "N.A",
    "Bug report": "bug fix of dynamo_timed in cprofile (#128203)\n\nFixes #ISSUE_NUMBER\n\nfb-only: \"Entire Frame\" was missing before this change.\n\nBefore: https://interncache-all.fbcdn.net/manifold/tlparse_reports/tree/logs/f565966006-TrainingApplication/20240527/rank_0/5_0_1/compilation_metrics_23.html\nAfter: https://interncache-all.fbcdn.net/manifold/tlparse_reports/tree/logs/f569854578-TrainingApplication/20240606/rank_0/0_0_0/compilation_metrics_16.html\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128203\nApproved by: https://github.com/Chillee",
    "Number of deleted lines": 3,
    "Deleted lines": "-        if config.cprofile:\n-            return func\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 755,
    "Library": "pytorch",
    "Date": "2024/06/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cafbcb63762e13d463fc173be411be4daa0c769d",
    "Root Cause": "N.A",
    "Bug report": "[BE]: Update ruff to 0.4.8 (#128214)\n\nUpdates ruff to 0.4.8. Some minor fixes, but noticably is 10% faster on microbenchmark and should further reduce local and CI runtime of the linter. Also includes a few bugfixes.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/128214\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 1,
    "Deleted lines": "-    'ruff==0.4.6',",
    "Added lines": "+    'ruff==0.4.8',",
    "Label": "clean"
},
{
    "Id": 756,
    "Library": "pytorch",
    "Date": "2024/06/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/91461601b6f262a60141061430b2066f42dd92ec",
    "Root Cause": "N.A",
    "Bug report": "[TORCH_FA2_flash_api] Update total_q to the reshaped query 0th dimension (#127524)\n\nThere is a difference (&bug) between the TORCH_FA2_flash_api:**mha_varlen_fwd** and FA2_flash_api:**mha_varlen_fwd** at the query transposition (GQA) step.\n\n```\nat::Tensor temp_q = q;\nif (seqlenq_ngroups_swapped) {\n        temp_q = q.reshape( ...\n ...\n}\nconst int total_q = q.sizes()[0];\nCHECK_SHAPE(temp_q, total_q, num_heads, head_size_og);\n```\n\nWhen doing query transposition we need to update total_q to the reshaped query 0th dimension, i.e:\n```\nconst int total_q = temp_q.sizes()[0];\n ```\n\nIn the original FA2_flash_api:**mha_varlen_fwd** they dont introduce a new variable temp_q but overwrite the q value directly.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127524\nApproved by: https://github.com/drisspg",
    "Number of deleted lines": 1,
    "Deleted lines": "-    const int total_q = q.sizes()[0];",
    "Added lines": "+    const int total_q = temp_q.sizes()[0];",
    "Label": "clean"
},
{
    "Id": 757,
    "Library": "pytorch",
    "Date": "2024/06/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f6ca822366e8ef19c2c02d33fc2a4629bd04c8ec",
    "Root Cause": "N.A",
    "Bug report": "Patch ARM Half use_gemv_fast_path gate to avoid kernel duplication (#127478)\n\nSummary: The existing code didn't gate the fast path, so the fast path had to duplicate the stock kernel. Now we gate it and delete the duplicate kernel.\n\nTest Plan: Existing tests. Flipped the TORCH_INTERNAL_ASSERT_DEBUG_ONLY to non-debug and forced to fail (locally) to make sure we had test coverage.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127478\nApproved by: https://github.com/malfet\nghstack dependencies: #127477",
    "Number of deleted lines": 25,
    "Deleted lines": "-                        C10_UNUSED int64_t lda, C10_UNUSED int64_t incx, C10_UNUSED int64_t incy) {\n-template bool gemv_use_fast_path<scalar_t>(int64_t m, int64_t n, int64_t lda, int64_t incx, int64_t incy);                                                                        \\\n-bool gemv_use_fast_path<float>(int64_t m, int64_t n, int64_t lda, int64_t incx, int64_t incy) {\n-bool gemv_use_fast_path<double>(int64_t m, int64_t n, int64_t lda, int64_t incx, int64_t incy) {\n-  return gemv_use_fast_path<float>(m, n, lda, incx, incy);\n-  return true;\n-  if (incx == 1 && alpha == 1.0 && beta == 0.0) {\n-    if (at::globalContext().allowFP16ReductionCPU()) {\n-      return fp16_gemv_trans_fp16_arith_by_dot_products(m, n, a, lda, x, y, incy);\n-    }\n-#endif\n-    return fp16_gemv_trans_fp32_arith_by_dot_products(m, n, a, lda, x, y, incy);\n-  }\n-  for (const auto i : c10::irange(n)) {\n-    float sum = 0;\n-    const auto row_ = a + lda * i;\n-    for (const auto j : c10::irange(m)) {\n-      sum += x[j * incx] * row_[j];\n-    }\n-    if (beta == 0.0) {\n-      y[i * incy] = alpha * sum;\n-    } else {\n-      y[i * incy] = beta * y[i * incy] + alpha * sum;\n-    }\n-  if (blas_impl::gemv_use_fast_path<scalar_t>(m, n, lda, incx, incy)) {",
    "Added lines": "+                        C10_UNUSED scalar_t alpha, C10_UNUSED int64_t lda,\n+                        C10_UNUSED int64_t incx, C10_UNUSED scalar_t beta,\n+                        C10_UNUSED int64_t incy) {\n+template bool gemv_use_fast_path<scalar_t>(int64_t m, int64_t n, scalar_t alpha, int64_t lda, int64_t incx, scalar_t beta, int64_t incy); \\\n+bool gemv_use_fast_path<float>(int64_t m, int64_t n, C10_UNUSED float alpha, int64_t lda, int64_t incx, C10_UNUSED float beta, int64_t incy) {\n+bool gemv_use_fast_path<double>(int64_t m, int64_t n, C10_UNUSED double alpha, int64_t lda, int64_t incx, C10_UNUSED double beta, int64_t incy) {\n+  return gemv_use_fast_path<float>(m, n, (float)alpha, lda, incx, (float)beta, incy);\n+    at::Half alpha,\n+    at::Half beta,\n+  return incx == 1 && c10::detail::fp16_from_bits(alpha.x) == 1.0f &&\n+    c10::detail::fp16_from_bits(beta.x) == 0.0f;\n+  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(incx == 1 && alpha == 1.0 && beta == 0.0);\n+  if (at::globalContext().allowFP16ReductionCPU()) {\n+    return fp16_gemv_trans_fp16_arith_by_dot_products(m, n, a, lda, x, y, incy);\n+#endif\n+  return fp16_gemv_trans_fp32_arith_by_dot_products(m, n, a, lda, x, y, incy);\n+  if (blas_impl::gemv_use_fast_path<scalar_t>(m, n, alpha, lda, incx, beta, incy)) {",
    "Label": "clean"
},
{
    "Id": 758,
    "Library": "pytorch",
    "Date": "2024/05/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8bf2c0a2030f56ab32b537fec79d4ac1a4f5f3a9",
    "Root Cause": "N.A",
    "Bug report": "[BE][Ez]: Update ruff to 0.4.6 (#127614)\n\nUpdate ruff linter to 0.4.6. Uneventful PR that fixes bugs and reduces false positives.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127614\nApproved by: https://github.com/albanD",
    "Number of deleted lines": 1,
    "Deleted lines": "-    'ruff==0.4.5',",
    "Added lines": "+    'ruff==0.4.6',",
    "Label": "clean"
},
{
    "Id": 759,
    "Library": "pytorch",
    "Date": "2024/05/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4105f91cfcecd101d8f3d143c85b4dbf7fdc7182",
    "Root Cause": "N.A",
    "Bug report": "[inductor] fix an assertion for node debug str (#127021)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/127021\nApproved by: https://github.com/aorenste",
    "Number of deleted lines": 1,
    "Deleted lines": "-        assert isinstance(backend, TritonScheduling)",
    "Added lines": "+        from torch._inductor.codegen.cuda_combined_scheduling import (\n+            CUDACombinedScheduling,\n+        )\n+        assert isinstance(backend, (TritonScheduling, CUDACombinedScheduling))",
    "Label": "clean"
},
{
    "Id": 760,
    "Library": "pytorch",
    "Date": "2024/05/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2db13633e746cf2212804dc9801f5112225d774f",
    "Root Cause": "N.A",
    "Bug report": "[export] disable forced specializations, even when solvable with single var (#126925)\n\nSummary:\nPreviously https://github.com/pytorch/pytorch/pull/124949 added the ability to disable forced specializations on dynamic shapes for export, keeping dynamism for complex guards instead of specializing, allowing unsoundness by having the user fail at runtime.\n\nIt avoided disabling one case: single-variable equality guards, where a variable is specified as dynamic but can be solvable for a concrete value, suggesting the correct behavior is specialization. For example, guard : Eq(s0 // 4, 400) suggests s0 should specialize to 1600.\n\nIn debugging, some users (e.g. APS) would like to keep this dynamic, and defer to failing at runtime instead. This PR adds this, so now all forced specializations should be turned off. Mostly this should be used for debugging, since it produces unsoundness, and lets the user proceed with (probably) incorrect dynamism.\n\nTest Plan: export tests\n\nDifferential Revision: D57698601\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126925\nApproved by: https://github.com/angelayi",
    "Number of deleted lines": 4,
    "Deleted lines": "-            # because this is univariate, the solution is a specialization\n-            self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n-            # add this as a substitution to simplify other constraints\n-            self._substitutions[s] = val",
    "Added lines": "+            # really don't force specializations here\n+            if not (_disable_forced_specializations and s in self._marked_dynamic):\n+                # because this is univariate, the solution is a specialization\n+                self._static_results.add(f\"{self._dcp.symbol_to_source[s][0].name()} == {val}\")\n+                # add this as a substitution to simplify other constraints\n+                self._substitutions[s] = val",
    "Label": "clean"
},
{
    "Id": 761,
    "Library": "pytorch",
    "Date": "2024/05/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5fa7aefb491e9266698e58182f9a055484553032",
    "Root Cause": "N.A",
    "Bug report": "[pipelining] Do not print loss (#126829)\n\n`loss` is a tensor, printing it would induce a GPU-CPU sync, which would slow down the program more than regular debug overhead.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126829\nApproved by: https://github.com/wconstab",
    "Number of deleted lines": 3,
    "Deleted lines": "-            logger.debug(\n-                f\"[{stage.stage_index}] Loss of microbatch {mb_index}: {loss}\"  # noqa: G004\n-            )",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 762,
    "Library": "pytorch",
    "Date": "2024/05/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/140ab89c027b019be9c89d17996294e7a1ea8577",
    "Root Cause": "N.A",
    "Bug report": "typing scheduler.py [1/2]: Bug fix (#126610)\n\nFound while getting scheduler.py to typecheck - split off to make reviewing easier.\n\n1. is_template: I'm pretty sure this is a bug.  Based on the definition of `is_template` I'm pretty sure we want to return the node's `get_template_node()`, not the node itself.\n\n2. can_free: It seems that this was intended to b a raise, not a return.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126610\nApproved by: https://github.com/eellison",
    "Number of deleted lines": 2,
    "Deleted lines": "-                return node\n-        return NotImplementedError",
    "Added lines": "+                return node.get_template_node()\n+        raise NotImplementedError",
    "Label": "clean"
},
{
    "Id": 763,
    "Library": "pytorch",
    "Date": "2024/05/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/93844a31b30c6b99fd18c37ee466637cca708d14",
    "Root Cause": "N.A",
    "Bug report": "Fix aarch64 debug build with GCC (#126290)\n\nBy working around GCCs quirks in instantiating templates that require immediate values.\nProvide alternative implementation for scaling the output if compiled without any optimizations (both GCC and clang define `__OPTIMIZE__` if invoked with anything but `-O0`)\n\nTest plan (after change was reverted): ssh into aarch64 runner and rebuild given file with `-O0`\n\nFixes https://github.com/pytorch/pytorch/issues/126283\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126290\nApproved by: https://github.com/atalman, https://github.com/seemethere",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#if __OPTIMIZE__\n+#else\n+    // Workaround GCCs inability to infer lane index at compile time\n+    // See https://github.com/pytorch/pytorch/issues/126283\n+    c10::ForcedUnroll<BLOCK_N>{}([&](auto i) {\n+      C[m * ldc + i] = reduce(c_val[i]) * float(scales[i]);\n+    });\n+#endif",
    "Label": "clean"
},
{
    "Id": 764,
    "Library": "pytorch",
    "Date": "2024/05/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/875221dedf9eeda567b2327336a2aedc681b9cdc",
    "Root Cause": "N.A",
    "Bug report": "Revert \"Fix aarch64 debug build with GCC (#126290)\"\n\nThis reverts commit 91bf952d10e9524a9b078900d9807efa5d252f5c.\n\nReverted https://github.com/pytorch/pytorch/pull/126290 on behalf of https://github.com/huydhn due to There seems to be a mis-match closing curly bracket here and it breaks some internal build in D57474505 ([comment](https://github.com/pytorch/pytorch/pull/126290#issuecomment-2118246756))",
    "Number of deleted lines": 8,
    "Deleted lines": "-#if __OPTIMIZE__\n-#else\n-    // Workaround GCCs inability to infer lane index at compile time\n-    // See https://github.com/pytorch/pytorch/issues/126283\n-    c10::ForcedUnroll<BLOCK_N>{}([&](auto i) {\n-      C[m * ldc + i] = reduce(c_val[i]) * float(scales[i]);\n-    });\n-#endif",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 765,
    "Library": "pytorch",
    "Date": "2024/05/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/55628624b872bd2ce51903841a65cc385ed7c526",
    "Root Cause": "N.A",
    "Bug report": "[c10d] add pg_name and pg_desc to logger (#126409)\n\nSummary:\nThis should further improve our debuggability\n\nTags:\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126409\nApproved by: https://github.com/XilunWu",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      data.strings[\"pg_name\"] = pg_name_;\n+      data.strings[\"pg_desc\"] = pg_desc_;",
    "Label": "clean"
},
{
    "Id": 766,
    "Library": "pytorch",
    "Date": "2024/05/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/91bf952d10e9524a9b078900d9807efa5d252f5c",
    "Root Cause": "N.A",
    "Bug report": "Fix aarch64 debug build with GCC (#126290)\n\nBy working around GCCs quirks in instantiating templates that require immediate values.\nProvide alternative implementation for scaling the output if compiled without any optimizations (both GCC and clang define `__OPTIMIZE__` if invoked with anything but `-O0`)\n\nFixes https://github.com/pytorch/pytorch/issues/126283\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126290\nApproved by: https://github.com/atalman, https://github.com/seemethere",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#if __OPTIMIZE__\n+#else\n+    // Workaround GCCs inability to infer lane index at compile time\n+    // See https://github.com/pytorch/pytorch/issues/126283\n+    c10::ForcedUnroll<BLOCK_N>{}([&](auto i) {\n+      C[m * ldc + i] = reduce(c_val[i]) * float(scales[i]);\n+    });\n+#endif",
    "Label": "clean"
},
{
    "Id": 767,
    "Library": "pytorch",
    "Date": "2024/05/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/31d22858e96ecfa0971e86e9f8d026a3d03c648f",
    "Root Cause": "N.A",
    "Bug report": "[onnx.export] Avoid unnecessary copy of debug_names (#123026)\n\nThis PR is part of an effort to speed up torch.onnx.export (#121422).\n\n- The `auto debug_names = ` infers a copy, where as `const auto& debug_names` does not.\n- However, this ones requires us to be careful, since calls to `setDebugName` changes `debug_names` and invalidates the `exist_name` iterator. So if we simply change `auto` to `const auto&`, then between that line and `find` we have corrupted the iterator by calling `output[i]->setDebugName`. This change aims to be functionally equivalent to the original, which is why we first get the Value pointer, then call `output[i]->setDebugName`, and finally call `setDebugName` on the found value. It is possible functionally it is OK to simply call `output[i]->setDebugName` first and then find and the second `setDebugName`, but this would not be identical to current behavior.\n- Resolves (2) in #121422.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123026\nApproved by: https://github.com/justinchuby",
    "Number of deleted lines": 7,
    "Deleted lines": "-          auto debug_names = new_block->owningGraph()->debugNames();\n-          auto exist_name = debug_names.find(new_name);\n-          if (exist_name != debug_names.end()) {\n-            // setDebugName changes name of existing value with same name.\n-            // Set again to revert the changes, but update name for new value\n-            // with suffix.\n-            exist_name->second->setDebugName(new_name);",
    "Added lines": "+          Value* found_value;\n+          bool exists;\n+          // In this scope, we fetch debug_names as a const reference and then\n+          // construct an iterator exist_name based on it. This iterator will\n+          // be corrupted if the underlying map of debug_names changes. This\n+          // will happen as a side-effect of setDebugName. For these reasons,\n+          // we make an explicit scope for exist_name and make sure that\n+          // setDebugName is never called with this scope.\n+          {\n+            const auto& debug_names = new_block->owningGraph()->debugNames();\n+            auto exist_name = debug_names.find(new_name);\n+            exists = exist_name != debug_names.end();\n+            if (exists) {\n+              found_value = exist_name->second;\n+            }\n+          }\n+          if (exists) {\n+            found_value->setDebugName(new_name);",
    "Label": "clean"
},
{
    "Id": 768,
    "Library": "pytorch",
    "Date": "2024/05/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2086f91c4cd250a66fb57fbc9c09901680a1476d",
    "Root Cause": "N.A",
    "Bug report": "Revert \"Fix aarch64 debug build with GCC (#126290)\"\n\nThis reverts commit a961e1ac83bf8831768c5a04eb7c4c18df8988d5.\n\nReverted https://github.com/pytorch/pytorch/pull/126290 on behalf of https://github.com/malfet due to Indeed lint is broken :/ ([comment](https://github.com/pytorch/pytorch/pull/126290#issuecomment-2113332757))",
    "Number of deleted lines": 8,
    "Deleted lines": "-#if __OPTIMIZE__      \n-#else\n-    // Workaround GCCs inability to infer lane index at compile time\n-    // See https://github.com/pytorch/pytorch/issues/126283\n-    c10::ForcedUnroll<BLOCK_N>{}([&](auto i) {\n-      C[m * ldc + i] = reduce(c_val[i]) * float(scales[i]);\n-    });    \n-#endif",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 769,
    "Library": "pytorch",
    "Date": "2024/05/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a961e1ac83bf8831768c5a04eb7c4c18df8988d5",
    "Root Cause": "N.A",
    "Bug report": "Fix aarch64 debug build with GCC (#126290)\n\nBy working around GCCs quirks in instantiating templates that require immediate values.\nProvide alternative implementation for scaling the output if compiled without any optimizations (both GCC and clang define __OPTIMIZE__ if invoked with anything but -O0)\n\nFixes https://github.com/pytorch/pytorch/issues/126283\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/126290\nApproved by: https://github.com/atalman, https://github.com/seemethere",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#if __OPTIMIZE__      \n+#else\n+    // Workaround GCCs inability to infer lane index at compile time\n+    // See https://github.com/pytorch/pytorch/issues/126283\n+    c10::ForcedUnroll<BLOCK_N>{}([&](auto i) {\n+      C[m * ldc + i] = reduce(c_val[i]) * float(scales[i]);\n+    });    \n+#endif",
    "Label": "clean"
},
{
    "Id": 770,
    "Library": "pytorch",
    "Date": "2024/05/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7dfd2949d74ab3a523a0abc6270cc426cdcf4a00",
    "Root Cause": "N.A",
    "Bug report": "Add missing type uint16, uint32, and uint64 to TensorHash in LTC. (#125972)\n\nIf I do:\n\n```\nxla_device = xm.xla_device()\nxla_tensor_0 = torch.tensor(42, dtype=torch.uint32).to(xla_device)\n```\n\nI got the error:\n\n```\nRuntimeError: false INTERNAL ASSERT FAILED at \"/ansible/pytorch/torch/csrc/lazy/core/hash.h\":139, please report a bug to PyTorch. Unsupported scalar type:UInt16\n```\n\nThis PR intends to fix this issue.\nThe data type can be found in pytorch/c10/core/ScalarType.h.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125972\nApproved by: https://github.com/JackCaoG",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    case at::ScalarType::UInt16:\n+      return DataHash(ctensor.const_data_ptr<uint16_t>(), size);\n+    case at::ScalarType::UInt32:\n+      return DataHash(ctensor.const_data_ptr<uint32_t>(), size);\n+    case at::ScalarType::UInt64:\n+      return DataHash(ctensor.const_data_ptr<uint64_t>(), size);",
    "Label": "clean"
},
{
    "Id": 771,
    "Library": "pytorch",
    "Date": "2024/05/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c61bfd24c1108716f0d4b91aa6d2da48a6ce184e",
    "Root Cause": "N.A",
    "Bug report": "[PT2] Register fake impl for quantized embedding bag ops (#125884)\n\nSummary: Register fake impl for quantized embedding bag ops (e.g. quantized::embedding_bag_4bit_rowwise_offsets) and bypass registration if it has been registered.\n\nTest Plan:\nBefore:\n```\nNotImplementedError: quantized::embedding_bag_4bit_rowwise_offsets: attempted to run this operator with Meta tensors, but there was no fake impl or Meta kernel registered\n```\nSee context here -\nhttps://fb.workplace.com/groups/1075192433118967/permalink/1423106614994212/\n\nAfter:\nSnapsoht was published successfully with PT2Archive.\n```\nAIMP_DISABLE_PRUNING=false  fdb buck2 run mode/opt-split-dwarf -c python.package_style=inplace -c fbcode.enable_gpu_sections=true  lego/scripts:lego_cli -- debug-locally --model_entity_id 545861329  --config_version 14 --publish_context OFFLINE_PUBLISH    --lego_pipeline aiplatform.modelstore.model_generation.lego.lego_pipeline_builder.gmpp_lego_pipeline --gmpp_config '{\"gmpp_pipeline_descriptor\": \"aiplatform.modelstore.model_generation.v1.ads_pipelines.aimp_pyper_pipeline.model_generation_pipeline\", \"worker_process_number\":24, \"worker_thread_per_process_number\": 12, \"use_work_assignment\": true}' --publish_config_overrides '{\"gpu_inference_options\": \"{\\\"submodules_to_lower\\\": []}\"}'  2>&1 | tee ./gmpp_lc_aimp.txt\n```\n\nReviewed By: ydwu4\n\nDifferential Revision: D57172944\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125884\nApproved by: https://github.com/ydwu4",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  m.set_python_module(\"caffe2.torch.fb.model_transform.splitting.split_dispatcher\");",
    "Label": "clean"
},
{
    "Id": 772,
    "Library": "pytorch",
    "Date": "2024/05/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5d97c22845ba09ea1dcbc23d2108ab90dd40446d",
    "Root Cause": "N.A",
    "Bug report": "AOTAutograd: use info not debug logging for ViewAndMutationMeta (#125676)\n\nBefore, the AOTAutograd metadata would not get logged when running with `TORCH_LOGS=\"aot\"`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125676\nApproved by: https://github.com/albanD",
    "Number of deleted lines": 1,
    "Deleted lines": "-    aot_graphs_log.debug(",
    "Added lines": "+    aot_graphs_log.info(",
    "Label": "clean"
},
{
    "Id": 773,
    "Library": "pytorch",
    "Date": "2024/05/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9a2375b6b7f113d2d3ee88152aa66aa352135756",
    "Root Cause": "N.A",
    "Bug report": "[dtensor] improve some pretty print in op schema (#125695)\n\nas titled, when I debugged\nhttps://github.com/pytorch/pytorch/pull/125369 I found this would be\nquality of life improvements\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125695\nApproved by: https://github.com/yifuwang, https://github.com/XilunWu\nghstack dependencies: #125693",
    "Number of deleted lines": 3,
    "Deleted lines": "-        input_specs_str = _pretty_print_spec(self.input_specs)\n-        return f\"{input_specs_str} -> {output_spec_str}\"\n-        return f\"OpStrategy:[{strategy_list_str}] @ mesh: {mesh_shape}\"",
    "Added lines": "+        if self.input_specs is not None:\n+            input_specs_str = f\"{_pretty_print_spec(self.input_specs)} -> \"\n+        else:\n+            input_specs_str = \"\"\n+        return f\"{input_specs_str}{output_spec_str}\"\n+        return f\"[{strategy_list_str}] @ mesh: {mesh_shape}\"",
    "Label": "clean"
},
{
    "Id": 774,
    "Library": "pytorch",
    "Date": "2024/05/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e5cc7ada6706f737959d8488d96028b3eb29aeea",
    "Root Cause": "N.A",
    "Bug report": "skip triton template precompilation in 311.0-3.11.7 to workaround 311 cpython bug (#125446)\n\nFix for https://github.com/pytorch/pytorch/issues/125374. We dont have CI for this specific versions, but I verified locally. THere is a cpython bug from 3.11.0->3.11.7 where the ast parsing state is global, and errors with multiple threads. when dust settles a little around the new process based compilation we can look into migrating.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125446\nApproved by: https://github.com/Chillee\nghstack dependencies: #125289",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            # https://github.com/python/cpython/issues/106905\n+            if (\n+                sys.version_info.major == 3\n+                and sys.version_info.minor == 11\n+                and sys.version_info.micro <= 8\n+            ):\n+                return no_op\n+",
    "Label": "clean"
},
{
    "Id": 775,
    "Library": "pytorch",
    "Date": "2024/04/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3946fa1c1247055d6b491e912b9fab02516d8d42",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in get_update_constraint (#125194)\n\nSummary: Title\n\nTest Plan: CI\n\nDifferential Revision: D56726321\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/125194\nApproved by: https://github.com/pianpwk",
    "Number of deleted lines": 4,
    "Deleted lines": "-            return fake_mode.shape_env, fake_mode\n-                return v.node.shape_env, fake_mode\n-        shape_env, _ = get_shape_env(gm)\n-    shape_env, fake_mode = get_shape_env(gm)",
    "Added lines": "+            return fake_mode.shape_env\n+                return v.node.shape_env\n+        shape_env = get_shape_env(gm)\n+    shape_env = get_shape_env(gm)",
    "Label": "clean"
},
{
    "Id": 776,
    "Library": "pytorch",
    "Date": "2024/04/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/96cc73dc13141351c4b0c6c9328aabdf7d4b4927",
    "Root Cause": "N.A",
    "Bug report": "[oss][torch.package] fix multiple error messages within PackageExporter (#124943)\n\nSummary:\nfixes two issues:\n- when exporting with debug=True, the list of error-causing modules and a dependency path to them is not printed correctly, there's a missing newline after the path, meaning the name of the module for the next error is on the wrong line, which makes the output a confusing mess to read\n- when a pickled object references more than one mocked module directly, the error message incorrectly repeats the same information, claiming the referenced attribute is present in several different libraries, because the if condition references the last seen module name while walking the pickle ops, not the module name from the enclosing block `for module_name in all_dependencies:`. this is confusing because one error will print as O(all_dependencies) errors, all with different module names but the same attribute name\n\nDifferential Revision: D56578035\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124943\nApproved by: https://github.com/JonAmazon, https://github.com/houseroad",
    "Number of deleted lines": 4,
    "Deleted lines": "-                        f\"      A path to {module_name}: {' -> '.join(module_path)}\"\n-                if module in mocked_modules:\n-                    assert isinstance(module, str)\n-                    fields = mocked_modules[module]",
    "Added lines": "+                        f\"      A path to {module_name}: {' -> '.join(module_path)}\\n\"\n+                if module_name in mocked_modules:\n+                    assert isinstance(module_name, str)\n+                    fields = mocked_modules[module_name]",
    "Label": "clean"
},
{
    "Id": 777,
    "Library": "pytorch",
    "Date": "2024/04/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2ea1e84d40ee9fc11b95fb4f0d6bc7d264b5eda4",
    "Root Cause": "N.A",
    "Bug report": "log pt2 config dict to signpost from inductor post grad (#124593)\n\nSummary:\nprevious attempts don't work eventually.  D49720297 causes online train SEV due to extra importing.  D56299408 mitigates a tricky bug from Distributed Shampoo constructor but unfortutenaly didn't correct the scuba logging either.\n\nsee f552546983\n\nTest Plan: {F1491621504}\n\nDifferential Revision: D56378270\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124593\nApproved by: https://github.com/anijain2305",
    "Number of deleted lines": 6,
    "Deleted lines": "-    def time_and_log(attr: str, extra_loggings: Optional[Dict[str, str]] = None):\n-@time_and_log(\n-    attr=\"compilation time (in seconds)\",\n-    extra_loggings={\"config_dict\": str(get_patched_config_dict())},\n-)\n-            log_optimus_to_scuba()",
    "Added lines": "+    def time_and_log(attr: str):\n+@time_and_log(attr=\"compilation time (in seconds)\")\n+            log_optimus_to_scuba(\n+                extra_logging={\"pt2_configs\": str(get_patched_config_dict())}\n+            )",
    "Label": "clean"
},
{
    "Id": 778,
    "Library": "pytorch",
    "Date": "2024/04/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7b6e354ecd0fbbbf2999cef19dc1947cdc9ee4a4",
    "Root Cause": "N.A",
    "Bug report": "[DDP][PT2D] Fix some tracing bugs of DDP (#124421)\n\n1. We need to clear the cache of get_legacy_mod_inlinelist to ensure the newly added rule will be captured.\n2. Don't add the hook if the parameter does not require gradient.\n\nDifferential Revision: [D56315534](https://our.internmc.facebook.com/intern/diff/D56315534/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124421\nApproved by: https://github.com/yf225",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            torch._dynamo.trace_rules.get_legacy_mod_inlinelist.cache_clear()\n+            if not param.requires_grad:\n+                continue",
    "Label": "clean"
},
{
    "Id": 779,
    "Library": "pytorch",
    "Date": "2024/04/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/660db767ef56a0512c5e94051c5058613fcaf1df",
    "Root Cause": "N.A",
    "Bug report": "Don't clean up fresh inductor cache on error (#124620)\n\nUseful for local debugging.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124620\nApproved by: https://github.com/oulgen, https://github.com/desertfire, https://github.com/jansel",
    "Number of deleted lines": 1,
    "Deleted lines": "-    with tempfile.TemporaryDirectory() as inductor_cache_dir:",
    "Added lines": "+    inductor_cache_dir = tempfile.mkdtemp()\n+    try:\n+        shutil.rmtree(inductor_cache_dir)\n+    except Exception:\n+        log.warning(\"on error, temporary cache dir kept at %s\", inductor_cache_dir)\n+        raise",
    "Label": "clean"
},
{
    "Id": 780,
    "Library": "pytorch",
    "Date": "2024/04/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a4c8002ee0714db7d19d98dc0c84108521c5ce6f",
    "Root Cause": "N.A",
    "Bug report": "MPS FFT implementation bug (#123274)\n\nCurrent implementation drops the negative frequency components even when the user doesn't ask for the one-sided transform. The tests for the negative frequency components seem to have worked by accident due to internal implementation details but the issue becomes evident in MacOs 14.4.\nCo-authored-by: Nikita Shulga <2453524+malfet@users.noreply.github.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123274\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 4,
    "Deleted lines": "-      auto outputTensor = [mpsGraph realToHermiteanFFTWithTensor:inputTensor\n-                                                            axes:IntArrayToNSArray(dim)\n-                                                      descriptor:descriptor\n-                                                            name:nil];",
    "Added lines": "+      MPSGraphTensor* outputTensor;\n+      if (onesided) {\n+        // Return only unique results:\n+        outputTensor = [mpsGraph realToHermiteanFFTWithTensor:inputTensor\n+                                                         axes:IntArrayToNSArray(dim)\n+                                                   descriptor:descriptor\n+                                                         name:nil];\n+      } else {\n+        // Return with Hermitean conjugate results:\n+        auto useDataType =\n+            (inputTensor.dataType == MPSDataTypeFloat16) ? MPSDataTypeComplexFloat16 : MPSDataTypeComplexFloat32;\n+        auto cTensor = [mpsGraph castTensor:inputTensor toType:useDataType name:nil];\n+        outputTensor = [mpsGraph fastFourierTransformWithTensor:cTensor\n+                                                           axes:IntArrayToNSArray(dim)\n+                                                     descriptor:descriptor\n+                                                           name:nil];\n+      }",
    "Label": "clean"
},
{
    "Id": 781,
    "Library": "pytorch",
    "Date": "2024/04/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/601112fdb4107d0c0c4d9a3766789d5075b9af10",
    "Root Cause": "N.A",
    "Bug report": "[dynamo][log] Print missing skipped frame info on debug (#124078)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/124078\nApproved by: https://github.com/yanboliang",
    "Number of deleted lines": 7,
    "Deleted lines": "-                if not is_skipfile or config.verbose:\n-                    log.debug(\n-                        \"skipping: %s (reason: %s, file: %s)\",\n-                        frame.f_code.co_name,\n-                        skip_reason,\n-                        frame.f_code.co_filename,\n-                    )",
    "Added lines": "+                log.debug(\n+                    \"skipping: %s (reason: %s, file: %s)\",\n+                    frame.f_code.co_name,\n+                    skip_reason,\n+                    frame.f_code.co_filename,\n+                )",
    "Label": "clean"
},
{
    "Id": 782,
    "Library": "pytorch",
    "Date": "2024/04/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e00282fecfcb53790aebfb24cc48a8703577778e",
    "Root Cause": "N.A",
    "Bug report": "[c10d] make monitorThread sleep when we try to dump (#123788)\n\nSummary:\nWe seperated the FR dump logic from the desync debug logic,\nso we no longer set collectiveDebugInfoMode_ to true when we just need FR\ndump. That's why monitor thread did not sleep and try to kill the\nprocess without waiting for the dump.\n\nThe fix is simple, we should sleep whenever shouldDump_ is true\nTest Plan:\nExisting unit tests\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123788\nApproved by: https://github.com/wconstab",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if ((terminateProcessGroup_.load() || collectiveDebugInfoMode_.load()) &&",
    "Added lines": "+          shouldDump_.store(true);\n+        shouldDump_.store(true);\n+  if ((terminateProcessGroup_.load() || collectiveDebugInfoMode_.load() ||\n+       shouldDump_.load()) &&",
    "Label": "clean"
},
{
    "Id": 783,
    "Library": "pytorch",
    "Date": "2024/04/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/01ab5a3104fc35f51b9e9218a9b6b5b3a8892dd7",
    "Root Cause": "N.A",
    "Bug report": "aot_eager and aot_eager_decomp_partition: include input mutations in graph (#123646)\n\nIn the next PR I force `set_()` input mutation to require always been in the graph.\n\nIt's a lot easier to do this if we make our other debugging backends allow input mutations in the graph. Input mutations are relatively hardened at this point, so I'd rather just have our debugging backends consistently allow input mutations.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123646\nApproved by: https://github.com/ezyang\nghstack dependencies: #122433",
    "Number of deleted lines": 2,
    "Deleted lines": "-    fw_compiler=boxed_nop, partition_fn=min_cut_rematerialization_partition\n-aot_eager_default_partitioner = aot_autograd(fw_compiler=boxed_nop)",
    "Added lines": "+    fw_compiler=boxed_nop,\n+    partition_fn=min_cut_rematerialization_partition,\n+    keep_inference_input_mutations=True,\n+aot_eager_default_partitioner = aot_autograd(\n+    fw_compiler=boxed_nop, keep_inference_input_mutations=True\n+)",
    "Label": "clean"
},
{
    "Id": 784,
    "Library": "pytorch",
    "Date": "2024/04/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f688d7a2f7dcb6caf5ded0e1eabedd5a7a6dc43b",
    "Root Cause": "N.A",
    "Bug report": "Only suggest debug envvar when debug is on (#123647)\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123647\nApproved by: https://github.com/Chillee",
    "Number of deleted lines": 1,
    "Deleted lines": "-        else:",
    "Added lines": "+        elif is_debug:",
    "Label": "clean"
},
{
    "Id": 785,
    "Library": "pytorch",
    "Date": "2024/04/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e8ad5460c0fc186457b1dc80aa219e1dc33eed8b",
    "Root Cause": "N.A",
    "Bug report": "Fix skip logic bug in dynamo benchmark runner (#123544)\n\nFix huggingface and timms_model did not uses TorchBenchmarksRunner class issue.\n![image](https://github.com/pytorch/pytorch/assets/84730719/358eed37-4d70-4034-85f9-58a922b5c532)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123544\nApproved by: https://github.com/chuanqi129, https://github.com/jgong5, https://github.com/desertfire",
    "Number of deleted lines": 5,
    "Deleted lines": "-    if device == \"cpu\":\n-        skip_tests.update(module.TorchBenchmarkRunner().skip_models_for_cpu)\n-    elif device == \"cuda\":\n-        skip_tests.update(module.TorchBenchmarkRunner().skip_models_for_cuda)\n-",
    "Added lines": "+        if device == \"cpu\":\n+            skip_tests.update(module.TorchBenchmarkRunner().skip_models_for_cpu)\n+        elif device == \"cuda\":\n+            skip_tests.update(module.TorchBenchmarkRunner().skip_models_for_cuda)",
    "Label": "clean"
},
{
    "Id": 786,
    "Library": "pytorch",
    "Date": "2024/04/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/07cecf4168503a5b3defef9b2ecaeb3e075f4761",
    "Root Cause": "N.A",
    "Bug report": "[dynamo][cpp-guards] Fix bug for slices (#123516)\n\nAutomatic testing as soon as we turn on cpp guards by default.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123516\nApproved by: https://github.com/jansel\nghstack dependencies: #123515",
    "Number of deleted lines": 2,
    "Deleted lines": "-            elif isinstance(base_example_value, list):\n-            elif isinstance(base_example_value, tuple):",
    "Added lines": "+            elif isinstance(base_example_value, list) and not source.index_is_slice:\n+            elif isinstance(base_example_value, tuple) and not source.index_is_slice:",
    "Label": "clean"
},
{
    "Id": 787,
    "Library": "pytorch",
    "Date": "2024/04/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0355f6e9549d4ba383112732f349c15b40169075",
    "Root Cause": "N.A",
    "Bug report": "[Bug Fix] Fix Cuda 12.4 compilation - Refactor SFINAE boxing logic  (#123377)\n\nSummary:\n\nPyTorch fails to compile from source using CUDA 12.4. The relevant log is extracted below. This was a recurring issue, which would cause the compilation to fail again on further objects if the first offending object was skipped.\n\nWhile searching for whether others had experienced this issue before attempting a fix myself, I found this suggested fix by @christian-heusel in https://github.com/pytorch/pytorch/issues/122169#issuecomment-2008455468 written by @lahwaacz. The code written by @lahwaacz at https://gitlab.archlinux.org/archlinux/packaging/packages/python-pytorch/-/commit/bb1f1a4c546c9692fb56db57172f14d25b95e645 fixes the issue.\n\nThe original issue (#122169) seems to have gone quiet, so I am submitting this PR. I made no substantive adjustments to @lahwaacz' code. My only adjustment was, for the sake of consistency, to remove the double underscores in the struct name, as double underscores are reserved to the implementation in C++ Standard. My change has no functional effect on the original code.\n\nThe ArchLinux package from which the original code was committed is licensed under the BSD license. https://archlinux.org/packages/extra/x86_64/python-pytorch/\n\n```\n[7900/8804] Building CUDA object caffe2/CMakeFiles/torch_cuda.dir/__/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu.o\nFAILED: caffe2/CMakeFiles/torch_cuda.dir/__/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu.o\n/usr/bin/ccache /usr/local/cuda-12.4/bin/nvcc -forward-unknown-to-host-compiler -DAT_PER_OPERATOR_HEADERS -DFLASHATTENTION_DISABLE_ALIBI -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DIDEEP_USE_MKL -DMINIZ_DISABLE_ZIP_READER_CRC32_CHECKS -DONNXIFI_ENABLE_EXT=1 -DONNX_ML=1 -DONNX_NAMESPACE=onnx_torch -DTORCH_CUDA_BUILD_MAIN_LIB -DUSE_C10D_GLOO -DUSE_C10D_NCCL -DUSE_CUDA -DUSE_CUSPARSELT -DUSE_DISTRIBUTED -DUSE_EXTERNAL_MZCRC -DUSE_FLASH_ATTENTION -DUSE_MEM_EFF_ATTENTION -DUSE_NCCL -DUSE_RPC -DUSE_TENSORPIPE -D_FILE_OFFSET_BITS=64 -Dtorch_cuda_EXPORTS -I/home/elliot/compile_test-pytorch/build/aten/src -I/home/elliot/compile_test-pytorch/aten/src -I/home/elliot/compile_test-pytorch/build -I/home/elliot/compile_test-pytorch -I/home/elliot/compile_test-pytorch/cmake/../third_party/benchmark/include -I/home/elliot/compile_test-pytorch/third_party/onnx -I/home/elliot/compile_test-pytorch/build/third_party/onnx -I/home/elliot/compile_test-pytorch/third_party/foxi -I/home/elliot/compile_test-pytorch/build/third_party/foxi -I/home/elliot/compile_test-pytorch/aten/src/THC -I/home/elliot/compile_test-pytorch/aten/src/ATen/cuda -I/home/elliot/compile_test-pytorch/aten/src/ATen/../../../third_party/cutlass/include -I/home/elliot/compile_test-pytorch/build/caffe2/aten/src -I/home/elliot/compile_test-pytorch/aten/src/ATen/.. -I/home/elliot/compile_test-pytorch/build/nccl/include -I/home/elliot/compile_test-pytorch/c10/cuda/../.. -I/home/elliot/compile_test-pytorch/c10/.. -I/home/elliot/compile_test-pytorch/third_party/tensorpipe -I/home/elliot/compile_test-pytorch/build/third_party/tensorpipe -I/home/elliot/compile_test-pytorch/third_party/tensorpipe/third_party/libnop/include -I/home/elliot/compile_test-pytorch/torch/csrc/api -I/home/elliot/compile_test-pytorch/torch/csrc/api/include -isystem /home/elliot/compile_test-pytorch/build/third_party/gloo -isystem /home/elliot/compile_test-pytorch/cmake/../third_party/gloo -isystem /home/elliot/compile_test-pytorch/cmake/../third_party/tensorpipe/third_party/libuv/include -isystem /home/elliot/compile_test-pytorch/cmake/../third_party/googletest/googlemock/include -isystem /home/elliot/compile_test-pytorch/cmake/../third_party/googletest/googletest/include -isystem /home/elliot/compile_test-pytorch/third_party/protobuf/src -isystem /home/elliot/miniforge3/envs/torchtest/include -isystem /home/elliot/compile_test-pytorch/third_party/gemmlowp -isystem /home/elliot/compile_test-pytorch/third_party/neon2sse -isystem /home/elliot/compile_test-pytorch/third_party/XNNPACK/include -isystem /home/elliot/compile_test-pytorch/third_party/ittapi/include -isystem /home/elliot/compile_test-pytorch/cmake/../third_party/eigen -isystem /usr/local/cuda-12.4/include -isystem /home/elliot/compile_test-pytorch/third_party/ideep/mkl-dnn/include/oneapi/dnnl -isystem /home/elliot/compile_test-pytorch/third_party/ideep/include -isystem /home/elliot/compile_test-pytorch/cmake/../third_party/cudnn_frontend/include -DLIBCUDACXX_ENABLE_SIMPLIFIED_COMPLEX_OPERATIONS -D_GLIBCXX_USE_CXX11_ABI=1 -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch -gencode arch=compute_86,code=sm_86 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda  -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUB_WRAPPED_NAMESPACE=at_cuda_detail -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -O3 -DNDEBUG -std=c++17 -Xcompiler=-fPIC -DMKL_HAS_SBGEMM -DMKL_HAS_SHGEMM -DTORCH_USE_LIBUV -DCAFFE2_USE_GLOO -Xcompiler=-Wall,-Wextra,-Wdeprecated,-Wno-unused-parameter,-Wno-unused-function,-Wno-missing-field-initializers,-Wno-unknown-pragmas,-Wno-type-limits,-Wno-array-bounds,-Wno-unknown-pragmas,-Wno-strict-overflow,-Wno-strict-aliasing,-Wno-maybe-uninitialized -Wno-deprecated-copy -MD -MT caffe2/CMakeFiles/torch_cuda.dir/__/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu.o -MF caffe2/CMakeFiles/torch_cuda.dir/__/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu.o.d -x cu -c /home/elliot/compile_test-pytorch/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu -o caffe2/CMakeFiles/torch_cuda.dir/__/torch/csrc/distributed/c10d/quantization/quantization_gpu.cu.o\n/home/elliot/compile_test-pytorch/aten/src/ATen/core/IListRef_inl.h: In static member function \u2018static c10::detail::IListRefConstRef<at::OptionalTensorRef> c10::detail::IListRefTagImpl<c10::IListRefTag::Boxed, at::OptionalTensorRef>::iterator_get(const c10::List<std::optional<at::Tensor> >::const_iterator&)\u2019:\n/home/elliot/compile_test-pytorch/aten/src/ATen/core/IListRef_inl.h:171:13: warning: possibly dangling reference to a temporary [-Wdangling-reference]\n  171 |     const auto& ivalue = (*it).get();\n      |             ^~~~~~\n/home/elliot/compile_test-pytorch/aten/src/ATen/core/IListRef_inl.h:171:33: note: the temporary was destroyed at the end of the full expression \u2018(& it)->c10::impl::ListIterator<std::optional<at::Tensor>, __gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue> > >::operator*().c10::impl::ListElementReference<std::optional<at::Tensor>, __gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue> > >::get()\u2019\n  171 |     const auto& ivalue = (*it).get();\n      |                      ~~~~~~~~~~~^~\n/home/elliot/compile_test-pytorch/aten/src/ATen/core/boxing/impl/boxing.h: At global scope:\n/home/elliot/compile_test-pytorch/aten/src/ATen/core/boxing/impl/boxing.h:42:103: error: expected primary-expression before \u2018>\u2019 token\n   42 | struct has_ivalue_to<T, std::void_t<decltype(std::declval<IValue>().to<T>())>>\n      |                                                                                                       ^\n/home/elliot/compile_test-pytorch/aten/src/ATen/core/boxing/impl/boxing.h:42:106: error: expected primary-expression before \u2018)\u2019 token\n   42 | struct has_ivalue_to<T, std::void_t<decltype(std::declval<IValue>().to<T>())>>\n      |                                                                                                          ^\n/home/elliot/compile_test-pytorch/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h: In lambda function:\n/home/elliot/compile_test-pytorch/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h:154:24: warning: possibly dangling reference to a temporary [-Wdangling-reference]\n  154 |         for (const at::Tensor& tensor : ivalue.toTensorList()) {\n      |                        ^~~~~~\n/home/elliot/compile_test-pytorch/aten/src/ATen/core/dispatch/DispatchKeyExtractor.h:154:53: note: the temporary was destroyed at the end of the full expression \u2018__for_begin .c10::impl::ListIterator<at::Tensor, __gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue> > >::operator*().c10::impl::ListElementReference<at::Tensor, __gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue> > >::operator std::conditional_t<true, const at::Tensor&, at::Tensor>()\u2019\n  154 |         for (const at::Tensor& tensor : ivalue.toTensorList()) {\n      |                                                     ^\n...\n\nninja: build stopped: subcommand failed.\n```\n```\nPyTorch version: 2.4.0a0+git595613d\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 23.10 (x86_64)\nGCC version: (Ubuntu 13.2.0-4ubuntu3) 13.2.0\nClang version: 16.0.6 (15)\nCMake version: version 3.29.0\nLibc version: glibc-2.38\n\nPython version: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:53:32) [GCC 12.3.0] (64-bit runtime)\nPython platform: Linux-6.5.0-26-generic-x86_64-with-glibc2.38\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Ti\nNvidia driver version: 550.67\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             24\nOn-line CPU(s) list:                0-23\nVendor ID:                          GenuineIntel\nModel name:                         13th Gen Intel(R) Core(TM) i7-13700K\nCPU family:                         6\nModel:                              183\nThread(s) per core:                 2\nCore(s) per socket:                 16\nSocket(s):                          1\nStepping:                           1\nCPU(s) scaling MHz:                 19%\nCPU max MHz:                        5400.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           6835.20\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          640 KiB (16 instances)\nL1i cache:                          768 KiB (16 instances)\nL2 cache:                           24 MiB (10 instances)\nL3 cache:                           30 MiB (1 instance)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-23\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] optree==0.11.0\n[pip3] pytorch-triton==3.0.0+989adb9a29\n[pip3] torch==2.4.0a0+git595613d\n[conda] magma-cuda124             2.6.1                         1    pytorch\n[conda] mkl-include               2024.1.0              intel_691    intel\n[conda] mkl-static                2024.1.0              intel_691    intel\n[conda] numpy                     1.26.4          py311h64a7726_0    conda-forge\n[conda] optree                    0.11.0          py311h9547e67_0    conda-forge\n[conda] pytorch-triton            3.0.0+989adb9a29          pypi_0    pypi\n[conda] torch                     2.4.0a0+git595613d          pypi_0    pypi\n```\n\nTagging @colesbury per https://github.com/pytorch/pytorch/issues/122169#issuecomment-2008232619\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123377\nApproved by: https://github.com/cyyever, https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-struct has_ivalue_to<T, std::void_t<decltype(std::declval<IValue>().to<T>())>>",
    "Added lines": "+struct ivalue_to_helper\n+{\n+    using type = decltype(std::declval<IValue>().template to<T>());\n+};\n+template <class T>\n+using ivalue_to_helper_t = typename ivalue_to_helper<T>::type;\n+\n+template <class T>\n+struct has_ivalue_to<T, std::void_t<ivalue_to_helper_t<T>>>",
    "Label": "clean"
},
{
    "Id": 788,
    "Library": "pytorch",
    "Date": "2024/04/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a4ef9cdd2807e7138e29d12ff03b48f60e1a5189",
    "Root Cause": "N.A",
    "Bug report": "benchmark: raise tolerance to unblock triton upgrade (#123484)\n\nDebugging is happening in https://github.com/pytorch/pytorch/issues/123126 .\n\nUpgrading triton cause accuracy failure for mixer_b16_224  and levit_128 .\n\nmixer_b16_224 is debugged specifically. It due to extra FMA instructions being used in a single kernel. That kernel itself only introduce small numerical difference. We conclude that this is not some 'real' accuracy issue and we should raise the tolerance to unblock the triton pin update.\n\nThe tolerance is picked such that the CI accuracy test can pass.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123484\nApproved by: https://github.com/jansel",
    "Number of deleted lines": 1,
    "Deleted lines": "-            if name in REQUIRE_HIGHER_TOLERANCE:",
    "Added lines": "+    \"mixer_b16_224\",\n+            if name in [\"levit_128\"]:\n+                tolerance = 8 * 1e-2\n+            elif name in REQUIRE_HIGHER_TOLERANCE:",
    "Label": "clean"
},
{
    "Id": 789,
    "Library": "pytorch",
    "Date": "2024/04/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dbeb21404364539b3b67cef861e16b7460766fd8",
    "Root Cause": "N.A",
    "Bug report": "[aot_inductor] Fix issues in pre_grad passes  (#123181)\n\nSummary:\n\nFixed a bug in `sink_cat_after_pointwise` pass for PT IR. The root cause is asumption of existence of input in kwargs or args\n\nDifferential Revision: D55617545\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123181\nApproved by: https://github.com/hl475, https://github.com/khabinov",
    "Number of deleted lines": 1,
    "Deleted lines": "-                    g.create_node(user.op, user.target, args=(arg,), kwargs=user.kwargs)",
    "Added lines": "+                new_kwargs = {\n+                    name: val for name, val in user.kwargs.items() if name != \"input\"\n+                }\n+                    g.create_node(user.op, user.target, args=(arg,), kwargs=new_kwargs)",
    "Label": "clean"
},
{
    "Id": 790,
    "Library": "pytorch",
    "Date": "2024/04/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8244ee00cfe49fc4a2e460e666a0dc5457e23c3e",
    "Root Cause": "N.A",
    "Bug report": "Add fuzzer instructions to pt2 bug template (#123156)\n\nAdds fuzzer instructions to our issue template\nPull Request resolved: https://github.com/pytorch/pytorch/pull/123156\nApproved by: https://github.com/eellison, https://github.com/anijain2305",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+        Note: if you're submitting an issue that you generated from a fuzzer. Please do the following:\n+\n+        - Ensure rtol/atol are at default tolerances\n+\n+        - Dont compare indices of max/min etc, because that avoids the above requirement\n+\n+        - If comparing eager and torch.compile at fp16/bf16, you should use fp32 as baseline\n+\n+        If the above requirements are met, add the label \"topic: fuzzer\" to your issue.\n+",
    "Label": "clean"
},
{
    "Id": 791,
    "Library": "pytorch",
    "Date": "2024/03/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e63e013c3b0d07f109cb70c5137ef575eee69798",
    "Root Cause": "N.A",
    "Bug report": "Skip use_count() debug assert for _nested_get_offsets() (#122917)\n\nThis broke [internal tests](https://www.internalfb.com/intern/test/844425064039866/) that run with unset `NDEBUG`. It wasn't initially caught because we don't test with unset `NDEBUG` in OSS CI.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122917\nApproved by: https://github.com/soulitzer\nghstack dependencies: #122902",
    "Number of deleted lines": 1,
    "Deleted lines": "-            at::impl::dispatch_mode_enabled())",
    "Added lines": "+            at::impl::dispatch_mode_enabled() ||\n+            // NJT offsets are expected to be reused; skip use_count() check\n+            op_name == \"aten::_nested_get_offsets\")",
    "Label": "clean"
},
{
    "Id": 792,
    "Library": "pytorch",
    "Date": "2024/03/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/235f24fc66078538249f978c275dc07c9e79d07e",
    "Root Cause": "N.A",
    "Bug report": "[inductor] Add FileLock around V.debug.copy (#122665)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122665\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 5,
    "Deleted lines": "-        if os.path.exists(new_path):\n-            shutil.rmtree(new_path)\n-            shutil.copytree(self._path, new_path)\n-            self._path = new_path\n-            pass",
    "Added lines": "+        from filelock import FileLock\n+\n+            with FileLock(f\"{new_path}.lock\"):\n+                if os.path.exists(new_path):\n+                    shutil.rmtree(new_path)\n+                shutil.copytree(self._path, new_path)",
    "Label": "clean"
},
{
    "Id": 793,
    "Library": "pytorch",
    "Date": "2024/03/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/06f22537ca751208cc6d8f5fc8ff600d563654b1",
    "Root Cause": "N.A",
    "Bug report": "[dynamo] Suppress warning about torch.autograd.Function() (#122566)\n\nPR #120577 got reverted due to issues in fbcode.  This hides warning\nthat PR was trying to fix until we can debug the fbcode issue.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122566\nApproved by: https://github.com/yanboliang",
    "Number of deleted lines": 1,
    "Deleted lines": "-            obj = torch.autograd.Function()",
    "Added lines": "+import warnings\n+            with warnings.catch_warnings(record=True):\n+                obj = torch.autograd.Function()",
    "Label": "clean"
},
{
    "Id": 794,
    "Library": "pytorch",
    "Date": "2024/03/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/19d27a13ea052230d9fb565a5b82e683e28d1697",
    "Root Cause": "N.A",
    "Bug report": "[CPUInductor] Fix out-of-bounds read/write in cvt_int64_to_[fp32|int32] (#122511)\n\nDiscovered while debugging regressions in enabling vectorization on ARM platform\n\nWithout this change `test_div2_cpu` will fail with invalid values on non-x86 CPU\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122511\nApproved by: https://github.com/peterbell10, https://github.com/jansel",
    "Number of deleted lines": 4,
    "Deleted lines": "-    src[i].store(src_buf + i * int64_vec_size);\n-      result[i * int64_vec_size + j] = static_cast<float>(src_buf[i * int64_vec_size + j]);\n-    src[i].store(src_buf + i * int64_vec_size);\n-      result[i * int64_vec_size + j] = static_cast<int32_t>(src_buf[i * int64_vec_size + j]);",
    "Added lines": "+    src[i].store(src_buf);\n+      result[i * int64_vec_size + j] = static_cast<float>(src_buf[j]);\n+    src[i].store(src_buf);\n+      result[i * int64_vec_size + j] = static_cast<int32_t>(src_buf[j]);",
    "Label": "clean"
},
{
    "Id": 795,
    "Library": "pytorch",
    "Date": "2024/03/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/17175cdbc70660595fa4ef9b66cb8466cdfd09ca",
    "Root Cause": "N.A",
    "Bug report": "[Docs] Add extended debugging options for troubleshooting (#122028)\n\nFixes #120889\n\nCo-authored-by: Nikita Shulga <2453524+malfet@users.noreply.github.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/122028\nApproved by: https://github.com/ezyang, https://github.com/malfet",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+Extended Debugging\n+~~~~~~~~~~~~~~~~~~\n+\n+Extended debugging can be enabled by using the following experimental flags.\n+\n+``TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED`` - provides extended debug information if the\n+string representation of a guard matches this flag value. For example, set it to\n+\"Ne(s0, 10)\" to generate full Python and C++ backtrace whenever guard was issued.\n+``TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL`` - provides extended debug information when\n+a particular symbol is allocated. For example, set this to \"u2\" to generate full Python\n+and C++ backtrace whenever this symbol was created.\n+``TORCHDYNAMO_EXTENDED_DEBUG_CPP`` - provides extended debug information (C++ backtrace)\n+for all extended debug settings as well as errors. For example, set this to \"1\". The C++\n+backtrace is slow and very spammy so it is not included by default with extended debugging.",
    "Label": "clean"
},
{
    "Id": 796,
    "Library": "pytorch",
    "Date": "2024/03/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/02f436da6db4e57b5fb456438189bc2f91def6d0",
    "Root Cause": "N.A",
    "Bug report": "[codemod][bugfix] Fix addressing bug in caffe2/caffe2/video/video_input_op.h (#121856)\n\nSummary:\n# Diff Specific\n\nThe signature of `copyFrom` is\n```\nvoid Tensor::CopyFrom(const Tensor& src, bool async) {\n```\nso the `&context` always evaluated to true.\n\nI could dig around to see if anyone cares about what the flag should actually be, but this is old code in caffe2, so I've just used `true` and we'll keep using whatever behaviour we've been using since 2019 or so when this was written.\n\n# General\n\nA bug in this code was identified by `-Waddress`, which we are working to enable globally.\n\nThis diff fixes the bug. There are a few types of fixes it might employ:\n\nThe bug could be `const_char_array == \"hello\"` which compares two addresses and therefore is almost always false. This is fixed with `const_char_array == std::string_view(\"hello\")` because `string_view` has an `==` operator that makes an appropriate comparison.\n\nThe bug could be `if(name_of_func)` which always returns true because the function always has an address. Likely you meant to call the function here!\n\n - If you approve of this diff, please use the \"Accept & Ship\" button :-)\n\nTest Plan: Sandcastle\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/121856\nApproved by: https://github.com/Skylion007",
    "Number of deleted lines": 15,
    "Deleted lines": "-            prefetched_clip_rgb_, &context_);\n-        prefetched_clip_of_on_device_.CopyFrom(prefetched_clip_of_, &context_);\n-      prefetched_label_on_device_.CopyFrom(prefetched_label_, &context_);\n-            prefetched_video_id_, &context_);\n-            prefetched_start_frame_, &context_);\n-        clip_rgb_output->CopyFrom(prefetched_clip_rgb_, &context_);\n-        clip_rgb_output->CopyFrom(prefetched_clip_rgb_on_device_, &context_);\n-        clip_of_output->CopyFrom(prefetched_clip_of_, &context_);\n-        clip_of_output->CopyFrom(prefetched_clip_of_on_device_, &context_);\n-      label_output->CopyFrom(prefetched_label_, &context_);\n-      label_output->CopyFrom(prefetched_label_on_device_, &context_);\n-        video_id_output->CopyFrom(prefetched_video_id_, &context_);\n-        video_id_output->CopyFrom(prefetched_video_id_on_device_, &context_);\n-        start_frame_output->CopyFrom(prefetched_start_frame_, &context_);\n-            prefetched_start_frame_on_device_, &context_);",
    "Added lines": "+            prefetched_clip_rgb_, true);\n+        prefetched_clip_of_on_device_.CopyFrom(prefetched_clip_of_, true);\n+      prefetched_label_on_device_.CopyFrom(prefetched_label_, true);\n+            prefetched_video_id_, true);\n+            prefetched_start_frame_, true);\n+        clip_rgb_output->CopyFrom(prefetched_clip_rgb_, true);\n+        clip_rgb_output->CopyFrom(prefetched_clip_rgb_on_device_, true);\n+        clip_of_output->CopyFrom(prefetched_clip_of_, true);\n+        clip_of_output->CopyFrom(prefetched_clip_of_on_device_, true);\n+      label_output->CopyFrom(prefetched_label_, true);\n+      label_output->CopyFrom(prefetched_label_on_device_, true);\n+        video_id_output->CopyFrom(prefetched_video_id_, true);\n+        video_id_output->CopyFrom(prefetched_video_id_on_device_, true);\n+        start_frame_output->CopyFrom(prefetched_start_frame_, true);\n+            prefetched_start_frame_on_device_, true);",
    "Label": "clean"
},
{
    "Id": 797,
    "Library": "pytorch",
    "Date": "2024/03/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0636c11811e15d1919cdd6cf20cb2d2bed2ee1da",
    "Root Cause": "N.A",
    "Bug report": "[AOTInductor] Include build cmds at the end of wrapper file (#121872)\n\nSummary:\nFor easier debugging, include build commands at the end of codegen wrapper.\n\n{F1468438991}\n\nTest Plan: CI\n\nDifferential Revision: D54882164\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/121872\nApproved by: https://github.com/chenyang78, https://github.com/desertfire",
    "Number of deleted lines": 8,
    "Deleted lines": "-            cmd = cpp_compile_command(\n-            log.debug(\"aot compilation command: %s\", cmd)\n-                compile_file(input_path, output_o, cmd.split())\n-                run_command_and_check(cmd)\n-            cmd = cpp_compile_command(\n-            log.debug(\"aot linkage command: %s\", cmd)\n-                compile_file([output_o, consts_o], output_so, cmd.split())\n-                run_command_and_check(cmd)",
    "Added lines": "+            compile_cmd = cpp_compile_command(\n+            log.debug(\"aot compilation command: %s\", compile_cmd)\n+                compile_file(input_path, output_o, compile_cmd.split())\n+                run_command_and_check(compile_cmd)\n+            link_cmd = cpp_compile_command(\n+            log.debug(\"aot linkage command: %s\", link_cmd)\n+                compile_file([output_o, consts_o], output_so, link_cmd.split())\n+                run_command_and_check(link_cmd)\n+\n+            # Append cmds to the end of codegen-ed wrapper file\n+            with open(input_path, \"a\") as f:\n+                f.write(\"\\n\")\n+                f.write(f\"// Compile cmd\\n// {compile_cmd}\\n\")\n+                f.write(f\"// Link cmd\\n// {link_cmd}\\n\")",
    "Label": "clean"
},
{
    "Id": 798,
    "Library": "pytorch",
    "Date": "2024/03/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/605c0a28aaf2ff6386b2f341a46fd0a0fd18b2ca",
    "Root Cause": "N.A",
    "Bug report": "[dtensor][debug] force visualize_sharding not to print for empty tensors (#121217)\n\n**Summary**\nCurrent `visualize_sharding` code cannot print for empty DTensor objects which leads to an exception. This PR skips the print logic if the DTensor passed in has 0 element.\n<img width=\"2165\" alt=\"Pasted Graphic\" src=\"https://github.com/pytorch/pytorch/assets/12968408/fa40b5e7-dad7-4d3a-a485-6a18067320ff\">\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/121217\nApproved by: https://github.com/wanchaol\nghstack dependencies: #121385, #121382",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+    note: no sharding info will be printed for empty tensors\n+    if dtensor.numel() == 0:  # we do not print for empty dtensors\n+        return\n+",
    "Label": "clean"
},
{
    "Id": 799,
    "Library": "pytorch",
    "Date": "2024/03/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3a5ab17bdc56470ab25291b2d72206253db79186",
    "Root Cause": "N.A",
    "Bug report": "[dtensor][debug] visualize_sharding skip if the current rank is not in mesh (#121382)\n\n**Summary**\nWe should skip the `visualize_sharding()` function on those ranks that are not a part of the DTensor's mesh. If not, exception will be thrown in current visualize logic.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/121382\nApproved by: https://github.com/wanchaol\nghstack dependencies: #121385",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if device_mesh.get_coordinate() is None:  # current rank is not in the mesh\n+        return\n+",
    "Label": "clean"
},
{
    "Id": 800,
    "Library": "pytorch",
    "Date": "2024/03/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b383123e3744689db3b728035c50e60b0cf743d0",
    "Root Cause": "N.A",
    "Bug report": "[dtensor][debug] visualize_sharding only compute offset on the first rank in mesh (#121385)\n\n**Summary**\navoid computing on ranks where we do not plan to visualize the DTensor.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/121385\nApproved by: https://github.com/wanchaol",
    "Number of deleted lines": 6,
    "Deleted lines": "-    local_rank_zero_on_all_dim = all(\n-        device_mesh.get_local_rank(mesh_dim=dim) == 0 for dim in range(device_mesh.ndim)\n-    )\n-    if local_rank_zero_on_all_dim:\n-        print(header)\n-        print(_create_table(blocks))",
    "Added lines": "+    # Only display the visualization once for each DTensor, on the rank whose\n+    # coordinate is 0 on all dimensions. For example, if the mesh is a full mesh,\n+    # we will only print on rank 0.\n+    local_rank_zero_on_all_dim = all(\n+        device_mesh.get_local_rank(mesh_dim=dim) == 0 for dim in range(device_mesh.ndim)\n+    )\n+    if not local_rank_zero_on_all_dim:\n+        return\n+\n+\n+    # Print the table\n+    print(header)\n+    print(_create_table(blocks))",
    "Label": "clean"
},
{
    "Id": 801,
    "Library": "pytorch",
    "Date": "2024/03/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e90cddb0d350df3b0447d6313da80d8676bdd915",
    "Root Cause": "N.A",
    "Bug report": "[inductor] Log triton kernel source and metadata on failure (#120494)\n\nIf Triton compilation fails it's much easier to debug when given the\nkernel source directly, versus a PyTorch repro.\n\nThis would have helped root cause\nhttps://github.com/pytorch/pytorch/issues/118589 almost immediately\n\nDifferential Revision: [D54119568](https://our.internmc.facebook.com/intern/diff/D54119568/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120494\nApproved by: https://github.com/peterbell10, https://github.com/eellison, https://github.com/jansel",
    "Number of deleted lines": 1,
    "Deleted lines": "-            binary = triton.compile(*compile_args, **compile_kwargs)",
    "Added lines": "+            try:\n+                binary = triton.compile(*compile_args, **compile_kwargs)\n+            except Exception:\n+                log.exception(\n+                    \"Triton compilation failed: %s\\n%s\\nmetadata: %s\",\n+                    self.inductor_meta.get(\"kernel_name\", \"triton_\"),\n+                    self.fn.src,\n+                    compile_meta,\n+                )\n+                raise",
    "Label": "clean"
},
{
    "Id": 802,
    "Library": "pytorch",
    "Date": "2024/03/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9cc0f23e5ccc122bcdeeb608ddcbf392c4e44802",
    "Root Cause": "N.A",
    "Bug report": "[dtensor][debug] allow visualize_sharding to print header (#121179)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/121179\nApproved by: https://github.com/wanchaol",
    "Number of deleted lines": 1,
    "Deleted lines": "-def visualize_sharding(dtensor):",
    "Added lines": "+def visualize_sharding(dtensor, header=\"\"):\n+        print(header)",
    "Label": "clean"
},
{
    "Id": 803,
    "Library": "pytorch",
    "Date": "2024/03/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fd2ab1f6131ca8ffa94c1b12b77ca8d84deeb61c",
    "Root Cause": "N.A",
    "Bug report": "[PT2][Inductor] Change the split cat log to debug (#120823)\n\nSummary: Address the report in https://github.com/pytorch/pytorch/issues/120771.\n\nTest Plan: see signal\n\nDifferential Revision: D54323475\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120823\nApproved by: https://github.com/jackiexu1992",
    "Number of deleted lines": 8,
    "Deleted lines": "-        log.info(\"couldn't find split args\")\n-        log.warning(\"example value absent for node: %s\", split_node)\n-        log.info(\"couldn't find unbind args\")\n-        log.warning(\"example value absent for node: %s\", input)\n-        log.info(\"couldn't find cat args\")\n-            log.warning(\"example value absent for node: %s\", tensor)\n-        log.info(\"couldn't find stack args\")\n-            log.warning(\"example value absent for node: %s\", tensor)",
    "Added lines": "+        log.debug(\"couldn't find split args\")\n+        log.debug(\"example value absent for node: %s\", split_node)\n+        log.debug(\"couldn't find unbind args\")\n+        log.debug(\"example value absent for node: %s\", input)\n+        log.debug(\"couldn't find cat args\")\n+            log.debug(\"example value absent for node: %s\", tensor)\n+        log.debug(\"couldn't find stack args\")\n+            log.debug(\"example value absent for node: %s\", tensor)",
    "Label": "clean"
},
{
    "Id": 804,
    "Library": "pytorch",
    "Date": "2024/02/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7881b95c73de586389c5b3a18ab343d578b10aee",
    "Root Cause": "N.A",
    "Bug report": "Don't suppress error codes in lint job, properly activate conda (#120769)\n\nBefore:\n\n```\n2024-02-28T02:38:24.3757573Z + conda activate /opt/conda/envs/py_3.9\n2024-02-28T02:38:24.3757872Z\n2024-02-28T02:38:24.3758116Z CondaError: Run 'conda init' before 'conda activate'\n```\n\nNow, this would actually fail the job, and I also fix the bug.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120769\nApproved by: https://github.com/albanD, https://github.com/janeyx99, https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-set -x",
    "Added lines": "+set -ex\n+eval \"$(command conda 'shell.bash' 'hook' 2> /dev/null)\"",
    "Label": "clean"
},
{
    "Id": 805,
    "Library": "pytorch",
    "Date": "2024/02/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f4cf25bb24be735b2502ae13f290017992c2fac8",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug where nn.functional._AllGather.backward produces wrong gradients (#120582)\n\nSummary:\nFixes #120386\n\n`_AllGather.backward` assumes that `_ReduceScatter` would always in-place update the output buffer. However, when the output buffer is non-contiguous, `_ReduceScatter` would allocate and return a different buffer, causing the gradient to be thrown away.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120582\nApproved by: https://github.com/XilunWu",
    "Number of deleted lines": 1,
    "Deleted lines": "-            _Reduce_Scatter.apply(ReduceOp.SUM, ctx.group, gx, *grad_outputs)",
    "Added lines": "+            gx = _Reduce_Scatter.apply(ReduceOp.SUM, ctx.group, gx, *grad_outputs)",
    "Label": "clean"
},
{
    "Id": 806,
    "Library": "pytorch",
    "Date": "2024/02/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c37d07a1bcddb89f2049e8ff833c57be3a76b8da",
    "Root Cause": "N.A",
    "Bug report": "[FSDP2] Removed `super().__setattr__` call (#120340)\n\n`nn.Module.__setattr__` does not actually call `super().__setattr__()`. If we make this call in our fast path, then we will inadvertently set the parameter as an actual attribute on the module, not just as an entry in the `_parameters` dict. This can lead to a bug where after replacing the parameters on the module (e.g. via `to_empty()` from meta device), we now have both an actual attribute (old) and a new entry in `_parameters` (new). Trying to access the parameter would give the old one since Python only resolves `__getattr__` if normal attribute lookup fails.\n\nThe bug was exercised in the following PR. I wanted to land this bug fix separately.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120340\nApproved by: https://github.com/yifuwang\nghstack dependencies: #120231",
    "Number of deleted lines": 1,
    "Deleted lines": "-        super(nn.Module, module).__setattr__(param_name, param)",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 807,
    "Library": "pytorch",
    "Date": "2024/02/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0e4bd25a33a5b4b4d84bd2bb3e6ea8a609eaf897",
    "Root Cause": "N.A",
    "Bug report": "[inductor] When generating debug logs don't fail if nvcc not found (#120346)\n\nIf nvcc isn't found subprocess throws a CalledProcessError\n\nDifferential Revision: [D54028438](https://our.internmc.facebook.com/intern/diff/D54028438/)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120346\nApproved by: https://github.com/Skylion007, https://github.com/shunting314",
    "Number of deleted lines": 1,
    "Deleted lines": "-    except FileNotFoundError:",
    "Added lines": "+    except (FileNotFoundError, subprocess.CalledProcessError):",
    "Label": "clean"
},
{
    "Id": 808,
    "Library": "pytorch",
    "Date": "2024/02/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/26fbbc3e844c8267aa8cf78f152c9612da40dc89",
    "Root Cause": "N.A",
    "Bug report": "DTensor + dynamo: fix is_shard/replicate always inlining to False (#118668)\n\nFixes an internal enablement bug. When dynamo traces `is_sharded`/`is_replicate`, it would unconditioanlly assume the result was False.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118668\nApproved by: https://github.com/wconstab, https://github.com/wanchaol\nghstack dependencies: #117667, #117666, #118209, #118191, #118667",
    "Number of deleted lines": 2,
    "Deleted lines": "-            method(self.value, *args, **kwargs)\n-            return self",
    "Added lines": "+            if name == \"__setattr__\":\n+                method(self.value, *args, **kwargs)\n+                return self\n+            constant_val = method(self.value, *args, **kwargs)\n+            return ConstantVariable.create(constant_val)",
    "Label": "clean"
},
{
    "Id": 809,
    "Library": "pytorch",
    "Date": "2024/02/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4f4629d5224e18d05e4fb952bef01a57f59654c2",
    "Root Cause": "N.A",
    "Bug report": "[Dynamo] Fix ListIteratorVariable repr to avoid log flooding (#120053)\n\nThis issue was found from Meta internal use case.\nBefore:\n```\nV0215 18:33:41.761000 140489262883968 torch/_dynamo/symbolic_convert.py:682 [0/0] TRACE starts_line /data/users/ybliang/debug/debug4.py:11 in <listcomp> (f) (inline depth: 1)\nV0215 18:33:41.761000 140489262883968 torch/_dynamo/symbolic_convert.py:682 [0/0]         a = [sum(x) for x in result]\nV0215 18:33:41.761000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE BUILD_LIST 0 []\nV0215 18:33:41.761000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LOAD_FAST .0 [ListVariable()]\nV0215 18:33:41.762000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE FOR_ITER 18 [ListVariable(), ListIteratorVariable([LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=0)]\nV0215 18:33:41.762000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE STORE_FAST x [ListVariable(), ListIteratorVariable([LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=1), LazyVariableTracker()]\nV0215 18:33:41.762000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LOAD_GLOBAL sum [ListVariable(), ListIteratorVariable([ListVariable(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=1)]\nV0215 18:33:41.763000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LOAD_FAST x [ListVariable(), ListIteratorVariable([ListVariable(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=1), BuiltinVariable(sum)]\nV0215 18:33:41.763000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE CALL_FUNCTION 1 [ListVariable(), ListIteratorVariable([ListVariable(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=1), BuiltinVariable(sum), ListVariable()]\nV0215 18:33:41.764000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LIST_APPEND 2 [ListVariable(), ListIteratorVariable([ListVariable(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=1), ConstantVariable(int: 50)]\nV0215 18:33:41.765000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE JUMP_ABSOLUTE 4 [ListVariable(), ListIteratorVariable([ListVariable(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=1)]\nV0215 18:33:41.765000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE FOR_ITER 18 [ListVariable(), ListIteratorVariable([ListVariable(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=1)]\nV0215 18:33:41.765000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE STORE_FAST x [ListVariable(), ListIteratorVariable([ListVariable(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=2), LazyVariableTracker()]\nV0215 18:33:41.765000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LOAD_GLOBAL sum [ListVariable(), ListIteratorVariable([ListVariable(), ListVariable(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=2)]\nV0215 18:33:41.765000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LOAD_FAST x [ListVariable(), ListIteratorVariable([ListVariable(), ListVariable(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=2), BuiltinVariable(sum)]\nV0215 18:33:41.766000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE CALL_FUNCTION 1 [ListVariable(), ListIteratorVariable([ListVariable(), ListVariable(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=2), BuiltinVariable(sum), ListVariable()]\nV0215 18:33:41.766000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LIST_APPEND 2 [ListVariable(), ListIteratorVariable([ListVariable(), ListVariable(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=2), ConstantVariable(int: 68)]\nV0215 18:33:41.767000 140489262883968 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE JUMP_ABSOLUTE 4 [ListVariable(), ListIteratorVariable([ListVariable(), ListVariable(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker(), LazyVariableTracker()], index=2)]\n```\nAfter:\n```\nV0215 18:27:57.901000 140556649206912 torch/_dynamo/symbolic_convert.py:682 [0/0] TRACE starts_line /data/users/ybliang/debug/debug4.py:11 in <listcomp> (f) (inline depth: 1)\nV0215 18:27:57.901000 140556649206912 torch/_dynamo/symbolic_convert.py:682 [0/0]         a = [sum(x) for x in result]\nV0215 18:27:57.901000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE BUILD_LIST 0 []\nV0215 18:27:57.901000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LOAD_FAST .0 [ListVariable()]\nV0215 18:27:57.901000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE FOR_ITER 18 [ListVariable(), ListIteratorVariable(length=10, index=0)]\nV0215 18:27:57.901000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE STORE_FAST x [ListVariable(), ListIteratorVariable(length=10, index=1), LazyVariableTracker()]\nV0215 18:27:57.902000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LOAD_GLOBAL sum [ListVariable(), ListIteratorVariable(length=10, index=1)]\nV0215 18:27:57.902000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LOAD_FAST x [ListVariable(), ListIteratorVariable(length=10, index=1), BuiltinVariable(sum)]\nV0215 18:27:57.903000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE CALL_FUNCTION 1 [ListVariable(), ListIteratorVariable(length=10, index=1), BuiltinVariable(sum), ListVariable()]\nV0215 18:27:57.903000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LIST_APPEND 2 [ListVariable(), ListIteratorVariable(length=10, index=1), ConstantVariable(int: 55)]\nV0215 18:27:57.904000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE JUMP_ABSOLUTE 4 [ListVariable(), ListIteratorVariable(length=10, index=1)]\nV0215 18:27:57.904000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE FOR_ITER 18 [ListVariable(), ListIteratorVariable(length=10, index=1)]\nV0215 18:27:57.904000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE STORE_FAST x [ListVariable(), ListIteratorVariable(length=10, index=2), LazyVariableTracker()]\nV0215 18:27:57.904000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LOAD_GLOBAL sum [ListVariable(), ListIteratorVariable(length=10, index=2)]\nV0215 18:27:57.904000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LOAD_FAST x [ListVariable(), ListIteratorVariable(length=10, index=2), BuiltinVariable(sum)]\nV0215 18:27:57.904000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE CALL_FUNCTION 1 [ListVariable(), ListIteratorVariable(length=10, index=2), BuiltinVariable(sum), ListVariable()]\nV0215 18:27:57.905000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LIST_APPEND 2 [ListVariable(), ListIteratorVariable(length=10, index=2), ConstantVariable(int: 64)]\nV0215 18:27:57.905000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE JUMP_ABSOLUTE 4 [ListVariable(), ListIteratorVariable(length=10, index=2)]\nV0215 18:27:57.905000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE FOR_ITER 18 [ListVariable(), ListIteratorVariable(length=10, index=2)]\nV0215 18:27:57.905000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE STORE_FAST x [ListVariable(), ListIteratorVariable(length=10, index=3), LazyVariableTracker()]\nV0215 18:27:57.906000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LOAD_GLOBAL sum [ListVariable(), ListIteratorVariable(length=10, index=3)]\nV0215 18:27:57.906000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LOAD_FAST x [ListVariable(), ListIteratorVariable(length=10, index=3), BuiltinVariable(sum)]\nV0215 18:27:57.906000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE CALL_FUNCTION 1 [ListVariable(), ListIteratorVariable(length=10, index=3), BuiltinVariable(sum), ListVariable()]\nV0215 18:27:57.907000 140556649206912 torch/_dynamo/symbolic_convert.py:708 [0/0] TRACE LIST_APPEND 2 [ListVariable(), ListIteratorVariable(length=10, index=3), ConstantVariable(int: 56)]\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120053\nApproved by: https://github.com/williamwen42",
    "Number of deleted lines": 3,
    "Deleted lines": "-        return (\n-            f\"{self.__class__.__name__}({repr(self.items)}, index={repr(self.index)})\"\n-        )",
    "Added lines": "+        return f\"{self.__class__.__name__}(length={len(self.items)}, index={repr(self.index)})\"",
    "Label": "clean"
},
{
    "Id": 810,
    "Library": "pytorch",
    "Date": "2024/02/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/24d5caba6eb8a4a0eff17a2266345712be6b1374",
    "Root Cause": "N.A",
    "Bug report": "[EZ] Fix argument parsing in build_with_debinfo (#120088)\n\n`nargs=\"?\"` accept 0 or 1 argument, but `nargs=\"*\"` accepts 0 or any number of arguments, which is the intended behavior of the tool\n\nTest plan: Run `python tools/build_with_debinfo.py aten/src/ATen/native/cpu/BlasKernel.cpp aten/src/ATen/native/BlasKernel.cpp` and observe that it generates torch_cpu with those two files containing debug information\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/120088\nApproved by: https://github.com/Skylion007",
    "Number of deleted lines": 1,
    "Deleted lines": "-    parser.add_argument(\"files\", nargs=\"?\", action=\"append\")",
    "Added lines": "+    parser.add_argument(\"files\", nargs=\"*\")",
    "Label": "clean"
},
{
    "Id": 811,
    "Library": "pytorch",
    "Date": "2024/01/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9c2b43cc5057cb840879224e3bdced58d6334c0a",
    "Root Cause": "N.A",
    "Bug report": "[inductor] Handle special values correctly in ir.Scan codegen (#118788)\n\nSpecial values (`NaN`/`+/-Inf`) are not correctly during codegen for `ir.Scan` nodes. This\nis a fairly minor bugfix that has not come up since the only two scan\nops with lowerings use \"normal\" values.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118788\nApproved by: https://github.com/peterbell10",
    "Number of deleted lines": 1,
    "Deleted lines": "-        default = init",
    "Added lines": "+        default = triton_constant(init)",
    "Label": "clean"
},
{
    "Id": 812,
    "Library": "pytorch",
    "Date": "2024/01/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9d5b950bdd303295eb7e3b5dc96e616ce029c28b",
    "Root Cause": "N.A",
    "Bug report": "[BE][Easy]: Update ruff to 0.1.14 (#118466)\n\nUpdates ruff to 0.1.14 which has some more autofixes, bugfixes, and fixes some false positives. Full changelog found here: https://github.com/astral-sh/ruff/releases/tag/v0.1.14\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118466\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 1,
    "Deleted lines": "-    'ruff==0.1.11',",
    "Added lines": "+    'ruff==0.1.14',",
    "Label": "clean"
},
{
    "Id": 813,
    "Library": "pytorch",
    "Date": "2024/01/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8d790abab9c91b2fa9e84515f00ffd6b47f9d900",
    "Root Cause": "N.A",
    "Bug report": "[NCCL][c10d] Log failing pointer if deregistration fails (#118455)\n\nFor debugging convenience\n\nCC @minsii @Aidyn-A @syed-ahmed @ptrblck\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118455\nApproved by: https://github.com/wconstab",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            \", with ptr \",\n+            ptr,",
    "Label": "clean"
},
{
    "Id": 814,
    "Library": "pytorch",
    "Date": "2024/01/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4d0b471389dd38865a2ff214ba7df185d02b18b1",
    "Root Cause": "N.A",
    "Bug report": "fix key error in pre_grad fx_passes_numeric_check (#118325)\n\nSummary:\n```\nI0125 121749.865 pyper_config_utils.py:8225] torchdynamo pyper config = TorchDynamoConfig(backend='inductor', optimize_ddp=False, log_compile_graph=False, inductor_config=TorchInductorConfig(enable_cudagraph=False, max_autotune=False, max_autotune_pointwise=True, max_autotune_gemm=False, search_autotune_cache=False, autotune_in_subproc=False, aggressive_fusion=False, shape_padding=True, permute_fusion=False, epilogue_fusion_first=False, debug=True, triton=None, trace_enabled=False, log_kernel_source=False, split_cat_fx_passes=False, group_fusion=False, batch_fusion=False, coordinate_descent_tuning=False, coordinate_descent_check_all_directions=False, coordinate_descent_search_radius=1, layout_optimization=True, pre_grad_fusion_options={}, post_grad_fusion_options={}, max_pointwise_cat_inputs=4, fx_passes_numeric_check={}), automatic_dynamic_shapes=True)\n```\nIn trainer\n```\nI0125 12:58:51.832000 4011.139732263132160 torchdynamo_wrapper.py:291  trainer:0:1 ] [pt2] creating torchdynamo backend wrapper with settings TorchDynamoConfig(backend='inductor', optimize_ddp=False, log_compile_graph=False, inductor_config=TorchInductorConfig(enable_cudagraph=False, max_autotune=False, max_autotune_pointwise=True, max_autotune_gemm=False, search_autotune_cache=False, autotune_in_subproc=False, aggressive_fusion=False, shape_padding=True, permute_fusion=False, epilogue_fusion_first=False, debug=True, triton=None, trace_enabled=False, log_kernel_source=False, split_cat_fx_passes=False, group_fusion=False, batch_fusion=False, coordinate_descent_tuning=False, coordinate_descent_check_all_directions=False, coordinate_descent_search_radius=1, layout_optimization=True, pre_grad_fusion_options={}, post_grad_fusion_options={}, max_pointwise_cat_inputs=4, fx_passes_numeric_check={}), automatic_dynamic_shapes=True) #ai_training_job_id=\"febe34d9-b2fb-493e-a5cc-6a0b1dc85ad4\" #ai_training_local_rank=\"1\" #ai_training_role_rank=\"1\" #mast_job_attempt=\"2\" #mast_job_name=\"f525072920-TrainingApplication\"\n...\nif config.fx_passes_numeric_check[\"pre_grad\"]:\n```\n\nhttps://www.internalfb.com/diff/D52826442?dst_version_fbid=1115735309429172&transaction_fbid=682438900759710\n\nhttps://www.internalfb.com/diff/D51838043?dst_version_fbid=336373395892373&transaction_fbid=349901787874069\n\nThis diff first fixes the key error to restore broken tests.  Its pyper changes can be addressed later.\n\nhttps://www.internalfb.com/code/fbsource/[72c19313ed73]/fbcode/caffe2/torch/_inductor/config.py?lines=142-147\n\nTest Plan: buck2 run //caffe2/torch/fb/training_toolkit/integration_tests/training_lifecycle/cogwheel_tests/pyper_release_v2:cogwheel_smallworld_mimo_cmf_deterministic_ne_pt2_training_platform__canary_offline_training-launcher -- --build-fbpkg --run-disabled --tests test\n\nReviewed By: yusuo\n\nDifferential Revision: D53102344\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118325\nApproved by: https://github.com/mengluy0125",
    "Number of deleted lines": 4,
    "Deleted lines": "-        if config.fx_passes_numeric_check[\"pre_grad\"]:\n-    if config.pattern_matcher and config.fx_passes_numeric_check[\"pre_grad\"]:\n-            config.fx_passes_numeric_check[\"num_iterations\"],\n-            config.fx_passes_numeric_check[\"precision\"],",
    "Added lines": "+        if config.fx_passes_numeric_check.get(\"pre_grad\", False):\n+    if config.pattern_matcher and config.fx_passes_numeric_check.get(\"pre_grad\", False):\n+            config.fx_passes_numeric_check.get(\"num_iterations\", 1),\n+            config.fx_passes_numeric_check.get(\"precision\", 1e-4),",
    "Label": "clean"
},
{
    "Id": 815,
    "Library": "pytorch",
    "Date": "2024/01/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3cdd4e236e2c58deeb39b620b5aa1c2bf70ec602",
    "Root Cause": "N.A",
    "Bug report": "[inductor][easy] dump triton kernel names in the log (#118313)\n\nThis may help debugging.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118313\nApproved by: https://github.com/desertfire",
    "Number of deleted lines": 2,
    "Deleted lines": "-            log.debug(\"CachingAutotuner gets %d configs\", len(self.configs))\n-            log.debug(\"Benchmark all input configs get:\")",
    "Added lines": "+            log.debug(\n+                \"CachingAutotuner gets %d configs for %s\",\n+                len(self.configs),\n+                self.fn.__name__,\n+            )\n+            log.debug(\"Benchmark all input configs for %s, get:\", self.fn.__name__)",
    "Label": "clean"
},
{
    "Id": 816,
    "Library": "pytorch",
    "Date": "2024/01/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/01388d07900ff9adea4de42cbd07ffca384d2dbb",
    "Root Cause": "N.A",
    "Bug report": "[dynamo] Slightly better error message if key not in dict (#117902)\n\nWas debugging an export issue, and currently when `key` does not exist in `self.items`, the error message is\n```\n  File \"/opt/pytorch/torch/_dynamo/variables/dicts.py\", line 208, in getitem_const\n    return self.items[key]\n           ~~~~~~~~~~^^^^^\ntorch._dynamo.exc.InternalTorchDynamoError: <torch._dynamo.variables.dicts.ConstDictVariable._HashableTracker object at 0x7fd7697cbf90>\n```\nThis PR changes it to be the following.\n```\nFile \"/data/users/angelayi/pytorch/torch/_dynamo/variables/dicts.py\", line 199, in getitem_const\n    raise KeyError(arg.value)\ntorch._dynamo.exc.InternalTorchDynamoError: shape\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117902\nApproved by: https://github.com/williamwen42",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        if key not in self.items:\n+            raise KeyError(arg.value)",
    "Label": "clean"
},
{
    "Id": 817,
    "Library": "pytorch",
    "Date": "2024/01/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/514159ddcbafff5d9b755c0fdcdf12eeac416735",
    "Root Cause": "N.A",
    "Bug report": "Add torch_dynamo to resume_in for ease of debugging (#118201)\n\nresume_in_* code objects show up in user backtraces when failures occur\nin code that has been Dynamo processed.  It is obvious to me, a PT2\ndeveloper, that these are generated by PT2, but it is NOT obvious to a\nnon-core dev that this is happened.  Add an extra torch_dynamo\nbreadcrumb to help get people to the right place.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118201\nApproved by: https://github.com/albanD",
    "Number of deleted lines": 2,
    "Deleted lines": "-            code_options[\"co_name\"] = f\"resume_in_{code_options['co_name']}_at_{lineno}\"\n-                    ] = f\"{module_name}.resume_in_{co_name}_at_{lineno}\"",
    "Added lines": "+            code_options[\n+                \"co_name\"\n+            ] = f\"torch_dynamo_resume_in_{code_options['co_name']}_at_{lineno}\"\n+                    ] = f\"{module_name}.torch_dynamo_resume_in_{co_name}_at_{lineno}\"",
    "Label": "clean"
},
{
    "Id": 818,
    "Library": "pytorch",
    "Date": "2024/01/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f9fca33baf8c6ffb0e2efeb6376400838659e2a1",
    "Root Cause": "N.A",
    "Bug report": "[codemod][highrisk] Fix shadowed variable in caffe2/caffe2/onnx/onnx_exporter.cc (#117996)\n\nSummary:\nOur upcoming compiler upgrade will require us not to have shadowed variables. Such variables have a _high_ bug rate and reduce readability, so we would like to avoid them even if the compiler was not forcing us to do so.\n\nThis codemod attempts to fix an instance of a shadowed variable. Please review with care: if it's failed the result will be a silent bug.\n\n**What's a shadowed variable?**\n\nShadowed variables are variables in an inner scope with the same name as another variable in an outer scope. Having the same name for both variables might be semantically correct, but it can make the code confusing to read! It can also hide subtle bugs.\n\nThis diff fixes such an issue by renaming the variable.\n\n - If you approve of this diff, please use the \"Accept & Ship\" button :-)\n\nTest Plan: Sandcastle\n\nReviewed By: igorsugak\n\nDifferential Revision: D52582853\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117996\nApproved by: https://github.com/PaliC, https://github.com/kit1980, https://github.com/malfet",
    "Number of deleted lines": 3,
    "Deleted lines": "-    const auto x_shape = dummy_->NewDummyName();\n-    nodes.emplace_back(MakeNode(\"Shape\", {x}, {x_shape}));\n-        {x_shape},",
    "Added lines": "+    const auto x_shape_2 = dummy_->NewDummyName();\n+    nodes.emplace_back(MakeNode(\"Shape\", {x}, {x_shape_2}));\n+        {x_shape_2},",
    "Label": "clean"
},
{
    "Id": 819,
    "Library": "pytorch",
    "Date": "2024/01/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0d1e7053ac7e88dc47fb8d1d31920af0ad94e805",
    "Root Cause": "N.A",
    "Bug report": "[easy] Log guard failure (#117639)\n\nFacilitates greatly debugging guard creation\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117639\nApproved by: https://github.com/Skylion007, https://github.com/jansel\nghstack dependencies: #112252, #117630, #110524, #108420",
    "Number of deleted lines": 2,
    "Deleted lines": "-            counter.visit(ast.parse(e))\n-        exec(pycode, builder.scope, out)",
    "Added lines": "+            try:\n+                counter.visit(ast.parse(e))\n+            except SyntaxError as ex:\n+                log.exception(\"Failed to visit expr at line %s.\\n%s\", ex.lineno, e)\n+                raise\n+        try:\n+            exec(pycode, builder.scope, out)\n+        except SyntaxError as ex:\n+            log.exception(\"Failed to exec guard at line %s.\\n%s\", ex.lineno, pycode)\n+            raise",
    "Label": "clean"
},
{
    "Id": 820,
    "Library": "pytorch",
    "Date": "2024/01/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4f2620ce56291c5116b9d9ca32fe56e035e57154",
    "Root Cause": "N.A",
    "Bug report": "[PT2][split_cat] fix a bug in merge_splits (#117707)\n\nSummary: Recently, we found merge splits (D45204109) is not working for AFOC model, thus patch a fix.\n\nTest Plan:\nThe error log: P1046934021\n# Flows used to local reproduce\n### non-first:\nf522317780\nafter the fix: P1047603217\n### first:\nf522253163\nafter the fix: P1047764917\n\nDifferential Revision: D52856359\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117707\nApproved by: https://github.com/jackiexu1992",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    # it is possible that the split has no users,\n+    # we check the corner case and skip the pattern\n+    if len(node.users.keys()) == 0:\n+        return",
    "Label": "clean"
},
{
    "Id": 821,
    "Library": "pytorch",
    "Date": "2024/01/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/84cfe6d8b214628b33771eff70c1876a67db5d10",
    "Root Cause": "N.A",
    "Bug report": "Drop all gather stats to debug not warning (#117669)\n\nLogger default level results in these all gather stats being spammed into every run which is very annoying\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117669\nApproved by: https://github.com/Skylion007, https://github.com/awgu",
    "Number of deleted lines": 1,
    "Deleted lines": "-        logger.warning(",
    "Added lines": "+        logger.debug(",
    "Label": "clean"
},
{
    "Id": 822,
    "Library": "pytorch",
    "Date": "2024/01/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d0fc268918236d8a56cc0ad82ace001160281a17",
    "Root Cause": "N.A",
    "Bug report": "Fixed issue in upsample_nearestnd lowering with scales (#117538)\n\nFixed #116848\n\nRelated to the bug introduced in my previous PR here: https://github.com/pytorch/pytorch/pull/113749/files#diff-a1b077971cddfabfa0071c5162265066e867bc07721816d95b9cbe58431c38e3R3264\n\nOriginally, the code was\n```python\ndef upsample_nearestnd(\n    x,\n    output_size,\n    scales_x: Tuple[Optional[float], ...],\n    n: int = 2,\n    exact: bool = False,\n):\n   # ...\n    scales = [i / o for i, o in zip(i_sizes, o_sizes)]\n    for i, scale in enumerate(scales):\n        if scale:\n            scales[i] = scale\n```\nwhich is wrong as `scales_x` is not used but can be provided by the user. The code was working for cases when user provided scale value can be recomputed using `input / output` sizes, e.g. scale=2.0. However, this would fail if input scale is a float value, e.g. 2.3, in this case recomputed scale is a bit different (e.g. 2.292682926829268, depending on input and output size) and can lead to an inconsistent output.\nThis problem was \"fixed\" to the following in my previous PR: https://github.com/pytorch/pytorch/pull/113749\n```python\ndef upsample_nearestnd(\n    x,\n    output_size,\n    scales_x: Tuple[Optional[float], ...],\n    n: int = 2,\n    exact: bool = False,\n):\n   # ...\n    scales = [i / o for i, o in zip(i_sizes, o_sizes)]\n    for i, scale in enumerate(scales_x):\n        if scale:\n            scales[i] = scale\n```\nhowever, this leads to a wrong scale value as it should be inverted as (1 / scale).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117538\nApproved by: https://github.com/peterbell10",
    "Number of deleted lines": 4,
    "Deleted lines": "-    scales = [i / o for i, o in zip(i_sizes, o_sizes)]\n-        if scale:\n-            scales[i] = scale\n-            [*b, *[scale_fn(i, s, size) for i, s, size in zip(x, scales, i_sizes)]]",
    "Added lines": "+    inv_scales = [i / o for i, o in zip(i_sizes, o_sizes)]\n+        if scale is not None:\n+            inv_scales[i] = 1.0 / scale\n+            [*b, *[scale_fn(i, s, size) for i, s, size in zip(x, inv_scales, i_sizes)]]",
    "Label": "clean"
},
{
    "Id": 823,
    "Library": "pytorch",
    "Date": "2024/01/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d78776e2e60ae0257847097dda3eb2fdc6f32476",
    "Root Cause": "N.A",
    "Bug report": "Stop unconditionally applying hermetic mode (#116996)\n\nWhen originally authored, it was not necessary to unconditionally apply\nhermetic mode, but I chose to apply it in eager mode to help catch bugs.\nWell, multipy is kind of dead, and hermetic mode is causing real\nimplementation problems for people who want to do fancy Python stuff\nfrom the dispatcher.  So let's yank this mode for now.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116996\nApproved by: https://github.com/jansel",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    // Jan 2024: We're slated to get rid of multipy, so stop forcing hermetic\n+    // mode unconditionally in all situations when you're using multipy.\n+    // Eventually just delete this entirely.  (Note that you may break multipy\n+    // anyway this way with dispatcher registered functions that require\n+    // hermetic to be off.)\n+#if defined(USE_DEPLOY)\n+#endif",
    "Label": "clean"
},
{
    "Id": 824,
    "Library": "pytorch",
    "Date": "2024/01/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/68b77311ad01cac56f133b2d29a992e502bd5f93",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in non-strict input processor (#116674)\n\nSummary: Title\n\nTest Plan: CI\n\nReviewed By: zhxchen17\n\nDifferential Revision: D52499932\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116674\nApproved by: https://github.com/tugsbayasgalan",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if isinstance(arg, list):",
    "Added lines": "+    if isinstance(arg, (tuple, list)):",
    "Label": "clean"
},
{
    "Id": 825,
    "Library": "pytorch",
    "Date": "2024/01/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cf618452d33022c391c240f7c28c196b6ebfded4",
    "Root Cause": "N.A",
    "Bug report": "[BE]: Fix F821 error in torch/fx/experimental (#116587)\n\nFix F821 error in torch/fx/experimental. Fixes a bug I did not fix in #116579\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116587\nApproved by: https://github.com/kit1980",
    "Number of deleted lines": 1,
    "Deleted lines": "-        return self._produce_dyn_sizes_from_int_tuple(tuple(ex.size()), source, symbolic_context)  # noqa: F821",
    "Added lines": "+        return self._produce_dyn_sizes_from_int_tuple(tuple(ex_size), source, symbolic_context)",
    "Label": "clean"
},
{
    "Id": 826,
    "Library": "pytorch",
    "Date": "2023/12/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/65c5eed01d3bf1b971f97c143beaa9dd8b0be0df",
    "Root Cause": "N.A",
    "Bug report": "[sigmoid] Remove workaround for constant output. (#116288)\n\nSummary: no more workaround_export_bug_constant_buffer_output\n\nTest Plan:\nbuck2 run mode/dev-nosan //scripts/ads_pt2_inference:pt2_cli -- --src_model manifold://ads_storage_fblearner/tree/user/facebook/fblearner/predictor/473164617/6/gpu_lowering/input.predictor.disagg.gpu.merge\n\nbuck2 run mode/opt caffe2/torch/fb/model_transform/fx2trt/packaging:generate_merge_net_file -- --action=generate --lower_backend=aot_inductor_ep --input_file=/data/users/zhxchen17/fbsource/fbcode/input.predictor.disagg.gpu.merge --output_file=/tmp/409501788_66.predictor.disagg.gpu.merge\n\nbuck2 run mode/opt -c fbcode.nvcc_arch=a100 caffe2/torch/fb/model_transform/fx2trt/packaging:load_merge_net_predictor -- --loadMode=Normal --inputMergeNetFile=/tmp/409501788_66.predictor.disagg.gpu.merge --pytorch_predictor_sigmoid_enabled=true\n\nReviewed By: khabinov, SherlockNoMad\n\nDifferential Revision: D52210429\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116288\nApproved by: https://github.com/tugsbayasgalan",
    "Number of deleted lines": 1,
    "Deleted lines": "-    exported_program._validate()",
    "Added lines": "+        if type(self) == ExportedProgramSerializer:\n+            exported_program._validate()\n+",
    "Label": "clean"
},
{
    "Id": 827,
    "Library": "pytorch",
    "Date": "2023/12/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0b9146bf5de4dbe273dfde101bc42a749e9db7c3",
    "Root Cause": "N.A",
    "Bug report": "[BE][Easy]: Update ruff to 0.1.9 (#116290)\n\nUpdates the ruff linter with lots of bugfixes, speed improvements, and fix improvements.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116290\nApproved by: https://github.com/janeyx99, https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-    'ruff==0.1.7',",
    "Added lines": "+    'ruff==0.1.9',",
    "Label": "clean"
},
{
    "Id": 828,
    "Library": "pytorch",
    "Date": "2023/12/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/127cae7ec80826a248d25803d94497601bb11468",
    "Root Cause": "N.A",
    "Bug report": "[C10D] Increase TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC (#116267)\n\nChange default from 2 min to 10 min.\n\nWhy? Many cases of heartbeat timeout were reported, but increasing\ntimeout led to the same job hanging in a different place, suggesting\nheartbeat kill was working well and not a false positive.  However, some\nothers reported jobs running fine with increased timeouts.  One such\ncase was investigated below, and suggests that indeed a 2 min timeout is\ntoo aggressive.  While we have not fully root caused the issue, it\nis better to avoid killing jobs that would otherwise complete.\n\nCurrent theory is that watchdog is not totally deadlocked, but is slowed\ndown in its processing of work objs due to some intermittent resource\ncontention.  Hence, allowing more time is more of a workaround than a\nfix.\n\nDebug/Analysis:\nhttps://docs.google.com/document/d/1NMNWoTB86ZpP9bqYLZ_EVA9byOlEfxw0wynMVEMlXwM\n\nDifferential Revision: [D52368791](https://our.internmc.facebook.com/intern/diff/D52368791)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116267\nApproved by: https://github.com/fduwjj",
    "Number of deleted lines": 1,
    "Deleted lines": "-      getCvarInt(TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC, 60 * 2 /*2 Mins*/);",
    "Added lines": "+      getCvarInt(TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC, 60 * 10 /*10 Mins*/);",
    "Label": "clean"
},
{
    "Id": 829,
    "Library": "pytorch",
    "Date": "2023/12/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/74e8cfc9a0d69cd64b27eb6a095dd5818b4e267d",
    "Root Cause": "N.A",
    "Bug report": "Forward fix torch package bug - dont depend on dynam in fsdp directly (#116229)\n\nDifferential Revision: [D52350752](https://our.internmc.facebook.com/intern/diff/D52350752)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116229\nApproved by: https://github.com/janeyx99, https://github.com/zou3519",
    "Number of deleted lines": 3,
    "Deleted lines": "-@torch._dynamo.allow_in_graph\n-@torch._dynamo.allow_in_graph\n-@torch._dynamo.allow_in_graph",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 830,
    "Library": "pytorch",
    "Date": "2023/12/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a734085a63f8b840c2ab36ab188cb842d731ddd8",
    "Root Cause": "N.A",
    "Bug report": "[ONNX][Dort] Fix bug preventing running with OrtValueVector (#116124)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116124\nApproved by: https://github.com/justinchuby, https://github.com/titaiwangms\nghstack dependencies: #115945",
    "Number of deleted lines": 1,
    "Deleted lines": "-            if hasattr(ORTC, \"push_back_batch\")",
    "Added lines": "+            if hasattr(ORTC.OrtValueVector, \"push_back_batch\")",
    "Label": "clean"
},
{
    "Id": 831,
    "Library": "pytorch",
    "Date": "2023/12/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7388d401655e3fa803d5a6e40e2c056da324fc63",
    "Root Cause": "N.A",
    "Bug report": "Make pytorch_qnnpack a shared library (#115570)\n\nSummary:\nThis library contains global state, e.g. pytorch_qnnp_params. If we make\nit a static library, different shared libraries linking that static\nlibrary can end up with their own copies of the global state, leading to\nbugs. Make it a shared library instead, to avoid this issue.\n\nTest Plan: buck2 test fbsource//fbandroid/javatests/com/facebook/playground/apps/fb4aplayground/scenarios/pytorchscenario:pytorchscenario -- --run-disabled --regex runBundledInputWithLocalAsset\n\nDifferential Revision: D51926024\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115570\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-        force_static = True,",
    "Added lines": "+        # FIXME(T172572183): This should be removed when fbcode no longer uses\n+        # produce_interface_from_stub_shared_library; it's needed to work around a bug\n+        # in that mode.\n+        supports_shlib_interfaces = select({\n+            \"ovr_config//os:linux\": False,\n+            \"DEFAULT\": True,\n+        }),\n+            third_party(\"pthreadpool\"),\n+            \":ukernels_asm\",\n+            \":ukernels_neon\",\n+            \":ukernels_psimd\",\n+            \":ukernels_scalar\",\n+            \":ukernels_sse2\",\n+            \":ukernels_sse41\",\n+            \":ukernels_ssse3\",",
    "Label": "clean"
},
{
    "Id": 832,
    "Library": "pytorch",
    "Date": "2023/12/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8739d1e3f9b08f4282fe79fc8dacd781d16913ff",
    "Root Cause": "N.A",
    "Bug report": "Fix a fast mode gradcheck bug where specified eps argument is ignored when switching to slow mode (#115634)\n\nAs in the title.\n\nThe reproducer for the bug is as follows:\n```python\n>>> import torch\n>>> dtype = torch.bfloat16\n>>> D1 = torch.tensor([[1, 2], [3, 4]], dtype=dtype, requires_grad=True)\n>>> D2 = torch.tensor([[1, 2], [3, 4]], dtype=dtype, requires_grad=True)\n>>> torch.autograd.gradcheck(torch.mm, (D1, D2), fast_mode=True)\n```\n\n<details>\n\n```\ntorch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor(0., dtype=torch.bfloat16)\nanalytical:tensor(4.9062, dtype=torch.bfloat16)\n\nThe above quantities relating the numerical and analytical jacobians are computed\nin fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background\nabout fast mode. Below, we recompute numerical and analytical jacobians in slow mode:\n\nNumerical:\n tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]], dtype=torch.bfloat16)\nAnalytical:\ntensor([[1., 2., 0., 0.],\n        [3., 4., 0., 0.],\n        [0., 0., 1., 2.],\n        [0., 0., 3., 4.]], dtype=torch.bfloat16)\n\n```\n</details>\n\n```python\nThe max per-element difference (slow mode) is: 4.0.\n>>> torch.autograd.gradcheck(torch.mm, (D1, D2), fast_mode=True, eps=1e-1)\n```\n\n<details>\n\n```\n<snip>\ntorch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor(5., dtype=torch.bfloat16)\nanalytical:tensor(4.9062, dtype=torch.bfloat16)\n\nThe above quantities relating the numerical and analytical jacobians are computed\nin fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background\nabout fast mode. Below, we recompute numerical and analytical jacobians in slow mode:\n\nNumerical:\n tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]], dtype=torch.bfloat16)\nAnalytical:\ntensor([[1., 2., 0., 0.],\n        [3., 4., 0., 0.],\n        [0., 0., 1., 2.],\n        [0., 0., 3., 4.]], dtype=torch.bfloat16)\n```\n\n</details>\n\n```\nThe max per-element difference (slow mode) is: 4.0.\n```\n\nNotice that changing `eps` value has no effect to max per-element difference.\n\nWith this PR, increasing `eps` value will lead to sensible results in numerical jacobian:\n```python\n>>> torch.autograd.gradcheck(torch.mm, (D1, D2), fast_mode=True, eps=1e-1)\n```\n\n<details>\n\n```\n<snip>\ntorch.autograd.gradcheck.GradcheckError: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor(5., dtype=torch.bfloat16)\nanalytical:tensor(4.9062, dtype=torch.bfloat16)\n\nThe above quantities relating the numerical and analytical jacobians are computed\nin fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background\nabout fast mode. Below, we recompute numerical and analytical jacobians in slow mode:\n\nNumerical:\n tensor([[0.9375, 1.8750, 0.0000, 0.0000],\n        [2.9688, 3.7500, 0.0000, 0.0000],\n        [0.0000, 0.0000, 1.2500, 2.5000],\n        [0.0000, 0.0000, 2.5000, 3.7500]], dtype=torch.bfloat16)\nAnalytical:\ntensor([[1., 2., 0., 0.],\n        [3., 4., 0., 0.],\n        [0., 0., 1., 2.],\n        [0., 0., 3., 4.]], dtype=torch.bfloat16)\n```\n\n</details>\n\n```\nThe max per-element difference (slow mode) is: 0.5.\n```\n\nFinally:\n```python\n>>> torch.autograd.gradcheck(torch.mm, (D1, D2), fast_mode=True, eps=1e-1, atol=1)\nTrue\n```\nthat would fail with the current main branch.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115634\nApproved by: https://github.com/lezcano, https://github.com/soulitzer, https://github.com/albanD\nghstack dependencies: #115536",
    "Number of deleted lines": 3,
    "Deleted lines": "-    func, tupled_inputs, outputs, input_idx, output_idx, rtol, atol, is_forward_ad\n-        func, tupled_inputs, outputs, is_forward_ad=is_forward_ad\n-                    func, tupled_inputs, outputs, i, j, rtol, atol, is_forward_ad",
    "Added lines": "+    func, tupled_inputs, outputs, input_idx, output_idx, rtol, atol, eps, is_forward_ad\n+        func, tupled_inputs, outputs, eps=eps, is_forward_ad=is_forward_ad\n+    eps,\n+                    func, tupled_inputs, outputs, i, j, rtol, atol, eps, is_forward_ad\n+        eps,",
    "Label": "clean"
},
{
    "Id": 833,
    "Library": "pytorch",
    "Date": "2023/12/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0379c11248d022faa19c2aace6c9f86a0cee5de2",
    "Root Cause": "N.A",
    "Bug report": "[c10d] Enable PG NCCL monitor thread by default (#115577)\n\nWe added a monitor thread in NCCL PG in https://github.com/pytorch/pytorch/pull/112518. To summarize what we are doing in monitor thread: it listens to the heartbeat from watchdog thread and detect unhealthy nccl watchdog hang (due to several reasons such as nccl/cuda API bugs or unexpected blocking behaviors). This is the last resort to ensure that we don't silently keep the training job run for hours.\n\nWe didn't open this feature as default, since we want to perform more due diligence and have some customers to try it out. So far, we didn't see any obstacle which blocks turning on this feature and received positive feedback from users. We now decided to turn in on by default in this PR.\n\nIf this feature turns out not to work as expected and disturb one's training process, one can set `TORCH_NCCL_ENABLE_MONITORING=0` to disable this feature. Please kindly file an issue with us so that we can see if we missed any corner cases during the design.\n\nDifferential Revision: [D52045911](https://our.internmc.facebook.com/intern/diff/D52045911)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115577\nApproved by: https://github.com/wconstab, https://github.com/kwen2501",
    "Number of deleted lines": 1,
    "Deleted lines": "-  monitorThreadEnabled_.store(getCvarBool(TORCH_NCCL_ENABLE_MONITORING, false));",
    "Added lines": "+  monitorThreadEnabled_.store(getCvarBool(TORCH_NCCL_ENABLE_MONITORING, true));",
    "Label": "clean"
},
{
    "Id": 834,
    "Library": "pytorch",
    "Date": "2023/12/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b9c4fb68c54e1dd72c9e1974da02eaa68a71153a",
    "Root Cause": "N.A",
    "Bug report": "[ONNX][Bench] Fix model name retrieval and remove unused argument (#115108)\n\nMight be some upstream updates, the previous hack starts to not pick up model names, updating to use the other more appropriate variable.\nAlso fix a bug with an unused argument that was supposed to be removed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115108\nApproved by: https://github.com/thiagocrepaldi",
    "Number of deleted lines": 5,
    "Deleted lines": "-    onnx_model_cls: Type[OnnxModelFromTorchScript],\n-        # Hack to get model name.\n-        from torch._functorch import aot_autograd\n-\n-        model_name = aot_autograd.model_name",
    "Added lines": "+        model_name = current_name",
    "Label": "clean"
},
{
    "Id": 835,
    "Library": "pytorch",
    "Date": "2023/12/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f09e8381b724d497b389ecadabcdc7fc938596d3",
    "Root Cause": "N.A",
    "Bug report": "[Inductor][fx pass] Fix a bug in batch linear fusion in the post grad (#115061) (#115131)\n\nSummary:\n\nTitled\n\nTest Plan:\n```\nbuck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/inductor:group_batch_fusion\n```\nBuck UI: https://www.internalfb.com/buck2/ab4b918c-9ffa-4d00-a747-880521a27851\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/16607023638890043\nNetwork: Up: 11MiB  Down: 117MiB  (reSessionID-079402d0-8fd7-4797-9ed5-dd0f778dce1a)\nJobs completed: 189430. Time elapsed: 2:02.5s.\nCache hits: 99%. Commands: 77000 (cached: 76995, remote: 5, local: 0)\nTests finished: Pass 7. Fail 0. Fatal 0. Skip 0. Build failure 0\n\nReviewed By: mengluy0125\n\nDifferential Revision: D51796899\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115131\nApproved by: https://github.com/mengluy0125",
    "Number of deleted lines": 3,
    "Deleted lines": "-        return len(input.meta[\"tensor_meta\"].shape) == 2\n-        # SymInt is not hashable, so we need to skip it\n-        if key is not None and not isinstance(key, torch.SymInt):",
    "Added lines": "+        input_shapes = input.meta[\"tensor_meta\"].shape\n+        return (\n+            len(input_shapes) == 2\n+            and isinstance(input_shapes[0], int)\n+            and isinstance(input_shapes[1], int)\n+        )\n+        if key is not None:",
    "Label": "clean"
},
{
    "Id": 836,
    "Library": "pytorch",
    "Date": "2023/11/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1f845d589885311447e6021def9da2463c8a989e",
    "Root Cause": "N.A",
    "Bug report": "[CI] Fix a REQUIRE_HIGHER_TOLERANCE comparison bug (#114870)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114870\nApproved by: https://github.com/jansel",
    "Number of deleted lines": 3,
    "Deleted lines": "-REQUIRE_HIGHER_TOLERANCE = set(\"sebotnet33ts_256\")\n-            if REQUIRE_HIGHER_TOLERANCE:\n-                tolerance = 2 * 1e-2",
    "Added lines": "+REQUIRE_HIGHER_TOLERANCE = {\n+    \"fbnetv3_b\",\n+    \"hrnet_w18\",\n+    \"inception_v3\",\n+    \"sebotnet33ts_256\",\n+    \"selecsls42b\",\n+}\n+            if name in REQUIRE_HIGHER_TOLERANCE:\n+                tolerance = 4 * 1e-2",
    "Label": "clean"
},
{
    "Id": 837,
    "Library": "pytorch",
    "Date": "2023/11/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e25b146b8c85a3da964986195c6f3f338db45428",
    "Root Cause": "N.A",
    "Bug report": "[BE][Easy]: Enable flake8-exe rules in ruff too. (#114521)\n\nEnable flake8-exe rules in ruff too. RUFF requires EXE rules to enabled separately from the E prefix. This fixes a parity bug between flake8 and ruff.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114521\nApproved by: https://github.com/kit1980",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    \"EXE\",",
    "Label": "clean"
},
{
    "Id": 838,
    "Library": "pytorch",
    "Date": "2023/11/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d37c4c69954ad7bdccca96854105c48e93d4587e",
    "Root Cause": "N.A",
    "Bug report": "Update `torch.compiler_troubleshooting.rst` (#114530)\n\nIf you copy and paste the env var in the docs:\n```console\nTORCHDYNAMO_REPRO_AFTER=\u201caot\u201d\n```\nit leads to this error:\n```python\n    @functools.wraps(unconfigured_compiler_fn)\n    def debug_wrapper(gm, example_inputs, **kwargs):\n        compiler_fn = functools.partial(unconfigured_compiler_fn, **kwargs)\n>       assert config.repro_after in (\"dynamo\", \"aot\", None)\nE       torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\nE       AssertionError:\n```\nbecause `config.repro_after` is being `'\u201caot\u201d'` but not `'aot'`.\n\n---\n\nIt would've saved a few minutes of my time \ud83d\ude04\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114530\nApproved by: https://github.com/Chillee",
    "Number of deleted lines": 2,
    "Deleted lines": "-environment variable ``TORCHDYNAMO_REPRO_AFTER=\u201caot\u201d`` (or setting\n-this program with ``TORCHDYNAMO_REPRO_AFTER=\u201cdynamo\u201d`` (or",
    "Added lines": "+environment variable ``TORCHDYNAMO_REPRO_AFTER=\"aot\"`` (or setting\n+this program with ``TORCHDYNAMO_REPRO_AFTER=\"dynamo\"`` (or",
    "Label": "clean"
},
{
    "Id": 839,
    "Library": "pytorch",
    "Date": "2023/11/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7d5e8c1d5131d3dfbcd47bb21748563346929b03",
    "Root Cause": "N.A",
    "Bug report": "[BE][easy]: Update ruff to 0.1.6 (#114125)\n\nUpdates ruff to 0.1.6 for more bugfixes, less false positives / false negatives, and support for more rules.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114125\nApproved by: https://github.com/albanD, https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-    'ruff==0.1.5',",
    "Added lines": "+    'ruff==0.1.6',",
    "Label": "clean"
},
{
    "Id": 840,
    "Library": "pytorch",
    "Date": "2023/11/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/99b89db174ce0359a8724409b2bf9df244d6761b",
    "Root Cause": "N.A",
    "Bug report": "[DTensor] Added `op_call` in no-mesh dispatch assert message (#113903)\n\nThis helps debug, e.g. when there is an unsupported op.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113903\nApproved by: https://github.com/wanchaol\nghstack dependencies: #113654",
    "Number of deleted lines": 1,
    "Deleted lines": "-        assert mesh is not None, \"found no DeviceMesh from dtensor args!\"",
    "Added lines": "+        assert mesh is not None, f\"found no DeviceMesh from dtensor args for {op_call}!\"",
    "Label": "clean"
},
{
    "Id": 841,
    "Library": "pytorch",
    "Date": "2023/11/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/de4fd3843c8746c64cd8bb36b5dd159a940c07d5",
    "Root Cause": "N.A",
    "Bug report": "[Inductor][fx pass] Fix a bug in the merge getitem cat pattern (#113822)\n\nSummary: The split cat pattern in D50100667 may change the sliced node returned by split node if the getitem to be merged is not consecutive indices.\n\nTest Plan:\n```\nbuck2 test 'fbcode//mode/opt' fbcode//pytorch/benchmark/fb/test_gpu:run_test_gpu -- --exact 'pytorch/benchmark/fb/test_gpu:run_test_gpu - test_train_mimo_cmf_30x_inductor_accuracy (pytorch.benchmark.fb.test_gpu.test_gpu.TestBenchmarkFbGpu)' --run-disabled\n```\nBuck UI: https://www.internalfb.com/buck2/1fd8fa6a-83d1-4cfd-bf33-c7ddb28de5b5\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/6473924659080211\nNetwork: Up: 1.3GiB  Down: 48MiB  (reSessionID-acaa2760-abff-442e-989f-3eefd1d1e034)\nJobs completed: 75. Time elapsed: 18:37.5s.\nCache hits: 0%. Commands: 68 (cached: 0, remote: 0, local: 68)\nTests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0\n\n```\nbuck2 test 'fbcode//mode/opt' fbcode//pytorch/benchmark/fb/test_gpu:run_test_gpu -- --exact 'pytorch/benchmark/fb/test_gpu:run_test_gpu - test_train_mimo_cmf_30x_inductor_speedup (pytorch.benchmark.fb.test_gpu.test_gpu.TestBenchmarkFbGpu)'\n```\nBuck UI: https://www.internalfb.com/buck2/7de122c6-23e0-4f13-b2b4-934cf780b60b\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/16888498613412388\nNetwork: Up: 90KiB  Down: 2.1MiB  (reSessionID-f75d6b7b-93ea-4d47-a52a-8d2429b30ad1)\nJobs completed: 6. Time elapsed: 17:28.0s.\nTests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0\n\nDifferential Revision: D51378532\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113822\nApproved by: https://github.com/xuzhao9",
    "Number of deleted lines": 1,
    "Deleted lines": "-#        stack (dim=0)  -> user=1",
    "Added lines": "+            # the gettitems to be merged must be consecutive, otherwise\n+            # returned sliced tensor could be wrong\n+            if indices[len(indices) - 1] - indices[0] + 1 != len(indices):\n+                continue\n+#        stack (dim=0)  -> user=1, getitems to be consecutive\n+            # the gettitems to be merged must be consecutive, otherwise\n+            # returned sliced tensor could be wrong\n+            if indices[len(indices) - 1] - indices[0] + 1 != len(indices):\n+                continue",
    "Label": "clean"
},
{
    "Id": 842,
    "Library": "pytorch",
    "Date": "2023/11/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/84ee7453ada1e7d9f0b705d4b623f5a5c21d516b",
    "Root Cause": "N.A",
    "Bug report": "ci: Add clickable PR link to trymerge (#113712)\n\nAdds a link to trymerge so that you can quickly click through the job to\nthe pull request for debugging.\n\nSigned-off-by: Eli Uriegas <eliuriegas@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113712\nApproved by: https://github.com/clee2000, https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-    print(f\"Attempting merge of {initial_commit_sha}\")",
    "Added lines": "+    pr_link = f\"https://github.com/{pr.org}/{pr.project}/pull/{pr.pr_num}\"\n+    print(f\"Attempting merge of {initial_commit_sha} ({pr_link})\")",
    "Label": "clean"
},
{
    "Id": 843,
    "Library": "pytorch",
    "Date": "2023/11/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c21320b3b176b999a5e9bf3b554534a6caa962b1",
    "Root Cause": "N.A",
    "Bug report": "CPU Publish: Fix Assign device error, when module has multiple devices (#109149) (#113509)\n\nSummary:\nnew version of this: https://www.internalfb.com/diff/D49110166?dst_version_fbid=252052334533986\n\nFix Assign device error, when module has multiple devices\nIf fc_fp16_quantization enabled for CPU model.\nAnd module REMOTE_OTHER has multiple devices: {device(type='meta'), device(type='cpu')}\nWe fail on this assertion:\nfbcode/caffe2/torch/ao/quantization/fx/utils.py\n232\n    assert len(devices) <= 1, (\nSince CPU models work on CPU devices, added a condition before the assertion.\nIn case, we have CPU in module list of devices. Set device as CPU.\nPlease see debug details:\nhttps://docs.google.com/document/d/1pMPCeJyMPA15NhFc2uAyNDkS9azR40uaNyOP0DIgHjU/edit\n\nTest Plan:\nAIMP_DISAGG_CPU=true buck run mode/opt -c python.package_style=inplace -c fbcode.enable_gpu_sections=true lego/scripts:lego_cli -- run-locally --model_entity_id 959168967 --config_version 28 --publish_context OFFLINE_PUBLISH --lego_pipeline aiplatform.modelstore.model_generation.lego.lego_pipeline_builder.gmpp_lego_pipeline --gmpp_config '{\"gmpp_pipeline_descriptor\": \"aiplatform.modelstore.model_generation.v1.ads_pipelines.aimp_pyper_pipeline.model_generation_pipeline\", \"worker_process_number\":12, \"worker_thread_per_process_number\": 6, \"use_work_assignment\": true}' 2>&1 | tee /tmp/gmpp_lc.txt\nSnapshot:\nhttps://www.internalfb.com/manifold/explorer/ads_storage_fblearner/tree/user/facebook/fblearner/predictor/959168967/47\n\nDifferential Revision: D51226114\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113509\nApproved by: https://github.com/jerryzh168",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    \"\"\"\n+    As a temp workaround for AIMP HHC publish we added CPU check.remove it later. T163614564\n+    \"\"\"\n+    if {torch.device(\"cpu\"), torch.device(\"meta\")} == devices:\n+        warnings.warn(\"Both 'meta' and 'cpu' are present in the list of devices. Module can have one device. We Select 'cpu'.\")\n+        devices = {torch.device(\"cpu\")}\n+    \"\"",
    "Label": "clean"
},
{
    "Id": 844,
    "Library": "pytorch",
    "Date": "2023/11/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e6f09607627f7f83c07f3a411b065b17a4519453",
    "Root Cause": "N.A",
    "Bug report": "[inductor] Make debug.py pass follow-imports typechecking (#113307)\n\npydot accepts both a str and a list of str for its `prog` parameter.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113307\nApproved by: https://github.com/Skylion007\nghstack dependencies: #113304, #113305, #113306",
    "Number of deleted lines": 2,
    "Deleted lines": "-from typing import Tuple\n-    prog: str = None,",
    "Added lines": "+from typing import List, Tuple, Union\n+    prog: Union[str, List[str]] = None,",
    "Label": "clean"
},
{
    "Id": 845,
    "Library": "pytorch",
    "Date": "2023/11/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d977f118ad5422bec0b57f5b98d7124a6e2366a3",
    "Root Cause": "N.A",
    "Bug report": "Update ruff linter to v0.1.5 (#113355)\n\nUpdate ruff linter to v0.1.5. Mainly bugfixes, primarily for autofixes, but good to include since there is at least one pydocstyle autofix update in their as people prepare their pydocstyle PRs.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113355\nApproved by: https://github.com/kit1980, https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-    'ruff==0.1.4',",
    "Added lines": "+    'ruff==0.1.5',",
    "Label": "clean"
},
{
    "Id": 846,
    "Library": "pytorch",
    "Date": "2023/11/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/82875e69fefd7d048b815c43609ac113ad5b0c72",
    "Root Cause": "N.A",
    "Bug report": "[inductor][fx pass] Fix a bug for the merge_stack_tahn_unbind pattern (#113101)\n\nSummary:\nContext:\nhttps://fb.workplace.com/groups/1075192433118967/permalink/1328366351134906/\n\nTest Plan:\nlocal reproduce igctr:\n```\nbuck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode split_batch-group\n```\nP874994427\n\nDifferential Revision: D51052304\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113101\nApproved by: https://github.com/jackiexu1992",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            # indices may not be necessarily sorted, we sort them first\n+            indices.sort()",
    "Label": "clean"
},
{
    "Id": 847,
    "Library": "pytorch",
    "Date": "2023/11/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/74c24d23675d72461fcc7797b731d84c5d978134",
    "Root Cause": "N.A",
    "Bug report": "Fixes a bug in inductor.triton.load (#113047)\n\nLettin CI/CD tell me if there is anything wrong with this\n\nOriginal bug:\n``` Shell\n        r1 = rindex\n        tmp37 = tl.load(out_ptr2 + (r1 + (8192*x0)), rmask, eviction_policy='evict_first', other=0)\n                                                     ^\nAssertionError('cannot cast int32[constexpr[1],constexpr[2048]] to <[1, 2048], fp8e4nv>')\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113047\nApproved by: https://github.com/Skylion007, https://github.com/ipiszy",
    "Number of deleted lines": 1,
    "Deleted lines": "-            other = \", other=0\"",
    "Added lines": "+            other = \", other=0.0\"",
    "Label": "clean"
},
{
    "Id": 848,
    "Library": "pytorch",
    "Date": "2023/11/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5248bc9c8ea3f40963417e6549bc550326256b29",
    "Root Cause": "N.A",
    "Bug report": "[LTC] Fix type inference for native_layer_norm_backward (#112948)\n\n## Description\nFix a bug in compute_shape_native_layer_norm_backward function.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112948\nApproved by: https://github.com/Skylion007",
    "Number of deleted lines": 1,
    "Deleted lines": "-      bias && weight->defined() ? bias->scalar_type() : input.scalar_type(),",
    "Added lines": "+      bias && bias->defined() ? bias->scalar_type() : input.scalar_type(),",
    "Label": "clean"
},
{
    "Id": 849,
    "Library": "pytorch",
    "Date": "2023/11/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/247b5bdbb5ec7a1e0d4ec907891fd5fe924c22b0",
    "Root Cause": "N.A",
    "Bug report": "[dynamo (easy)] Add skip reason to debug logs (#112869)\n\nFixes https://github.com/pytorch/pytorch/issues/112867\n\nExample logs\n```\n[2023-11-03 12:51:02,230] torch._dynamo.eval_frame: [DEBUG] skipping: helper (reason: in skipfiles, file: /usr/lib/python3.10/contextlib.py)\n[2023-11-03 12:51:02,230] torch._dynamo.eval_frame: [DEBUG] skipping: __init__ (reason: in skipfiles, file: /usr/lib/python3.10/contextlib.py)\n[2023-11-03 12:51:02,230] torch._dynamo.eval_frame: [DEBUG] skipping: __enter__ (reason: in skipfiles, file: /usr/lib/python3.10/contextlib.py)\n[2023-11-03 12:51:02,230] torch._dynamo.eval_frame: [DEBUG] skipping: backend_cache_wrapper (reason: in skipfiles, file: /home/jonch/Desktop/Programming/mlsys/pytorch/torch/_dynamo/eval_frame.py)\n[2023-11-03 12:51:02,230] torch._dynamo.eval_frame: [DEBUG] skipping: _maybe_init_guarded_backend_cache (reason: in skipfiles, file: /home/jonch/Desktop/Programming/mlsys/pytorch/torch/_dynamo/eval_frame.py)\n[2023-11-03 12:51:02,230] torch._dynamo.eval_frame: [DEBUG] skipping: innermost_fn (reason: in skipfiles, file: /home/jonch/Desktop/Programming/mlsys/pytorch/torch/_dynamo/eval_frame.py)\n[2023-11-03 12:51:02,230] torch._dynamo.eval_frame: [DEBUG] skipping: _set_current_backend (reason: in skipfiles, file: /home/jonch/Desktop/Programming/mlsys/pytorch/torch/_dynamo/eval_frame.py)\n[2023-11-03 12:51:02,230] torch._dynamo.eval_frame: [DEBUG] skipping: __init__ (reason: in skipfiles, file: /usr/lib/python3.10/contextlib.py)\n[2023-11-03 12:51:02,230] torch._dynamo.eval_frame: [DEBUG] skipping: __enter__ (reason: in skipfiles, file: /usr/lib/python3.10/contextlib.py)\n[2023-11-03 12:51:02,230] torch._dynamo.eval_frame: [DEBUG] skipping: enable_dynamic (reason: in skipfiles, file: /home/jonch/Desktop/Programming/mlsys/pytorch/torch/_dynamo/eval_frame.py)\n[2023-11-03 12:51:02,247] [0/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing fn /home/jonch/Desktop/sdpa.py:1635\n[2023-11-03 12:51:02,248] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /home/jonch/Desktop/sdpa.py:1635 in fn (fn)\n[2023-11-03 12:51:02,248] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]     def fn(x):\n[2023-11-03 12:51:02,313] [0/0] torch._dynamo.output_graph: [DEBUG] create_graph_input L_x_ L['x']\n[2023-11-03 12:51:02,314] [0/0] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['x'] (3,) [<DimDynamic.STATIC: 2>] [None]\n[2023-11-03 12:51:02,314] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /home/jonch/Desktop/sdpa.py:1636 in fn (fn)\n[2023-11-03 12:51:02,314] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]         x = x + 1\n[2023-11-03 12:51:02,314] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x []\n\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112869\nApproved by: https://github.com/jansel",
    "Number of deleted lines": 1,
    "Deleted lines": "-            log.debug(\"skipping %s %s\", frame.f_code.co_name, frame.f_code.co_filename)",
    "Added lines": "+            if log.isEnabledFor(logging.DEBUG):\n+                skip_reason = (\n+                    \"traced frame already\"\n+                    if frame.f_lasti >= first_real_inst_idx(frame.f_code)\n+                    else \"in skipfiles\"\n+                    if skipfiles.check(frame.f_code)\n+                    else \"dynamo tracing is disabled\"\n+                )\n+                log.debug(\n+                    \"skipping: %s (reason: %s, file: %s)\",\n+                    frame.f_code.co_name,\n+                    skip_reason,\n+                    frame.f_code.co_filename,\n+                )",
    "Label": "clean"
},
{
    "Id": 850,
    "Library": "pytorch",
    "Date": "2023/11/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/543a618ae83e955ee22abf742e1899e8a659c9c2",
    "Root Cause": "N.A",
    "Bug report": "[inductor][fx pass] Fix a split cat bug in the pre grad (#112667)\n\nSummary: blue reels vdd v3 has unit test failure, we fix the bug\n\nTest Plan:\n```\nbuck2 test 'fbcode//mode/opt' fbcode//pytorch/benchmark/fb/test_gpu:run_test_gpu -- --exact 'pytorch/benchmark/fb/test_gpu:run_test_gpu - test_train_blue_reels_vdd_v3_inductor_accuracy (pytorch.benchmark.fb.test_gpu.test_gpu.TestBenchmarkFbGpu)'\n```\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/13229323914259182\nNetwork: Up: 2.5MiB  Down: 8.3MiB  (reSessionID-b3362362-c80a-4ac2-8332-bc1321aaf0bd)\nJobs completed: 6. Time elapsed: 5:13.2s.\nTests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0\n\n```\nbuck2 test 'fbcode//mode/opt' fbcode//pytorch/benchmark/fb/test_gpu:run_test_gpu -- --exact 'pytorch/benchmark/fb/test_gpu:run_test_gpu - test_train_blue_reels_vdd_v3_inductor_speedup (pytorch.benchmark.fb.test_gpu.test_gpu.TestBenchmarkFbGpu)'\n```\nBuck UI: https://www.internalfb.com/buck2/aa3031a9-3f1b-4f42-a78c-decbf2beb14f\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/4785074810906355\nNetwork: Up: 1.3GiB  Down: 40MiB  (reSessionID-801ddf16-ff5d-4135-9758-ff286d1d59aa)\nJobs completed: 69. Time elapsed: 10:12.4s.\nCache hits: 10%. Commands: 61 (cached: 6, remote: 4, local: 51)\nTests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0\n\nDifferential Revision: D50901626\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112667\nApproved by: https://github.com/xuzhao9, https://github.com/Skylion007",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            # indices may not be necessarily sorted, we sort them first\n+            indices.sort()",
    "Label": "clean"
},
{
    "Id": 851,
    "Library": "pytorch",
    "Date": "2023/11/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/59869903b3a54acb3ac3a93470fc595d1de3fa7a",
    "Root Cause": "N.A",
    "Bug report": "Fix mem eff bias bug (#112673)\n\nThis fixes #112577\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112673\nApproved by: https://github.com/cpuhrsch",
    "Number of deleted lines": 2,
    "Deleted lines": "-  if (attn_mask.sym_stride(0) % 16 != 0 || attn_mask.sym_stride(1) % 16 != 0 ||\n-      attn_mask.sym_stride(2) % 16 != 0 || attn_mask.sym_stride(3) != 1) {",
    "Added lines": "+  auto needs_contig = [](const c10::SymInt& stride) {\n+    return (stride % 16 != 0) || (stride == 0);\n+  };\n+  if (needs_contig(attn_mask.sym_stride(0)) ||\n+      needs_contig(attn_mask.sym_stride(1)) ||\n+      needs_contig(attn_mask.sym_stride(2)) ||\n+      needs_contig(attn_mask.sym_stride(3))) {",
    "Label": "clean"
},
{
    "Id": 852,
    "Library": "pytorch",
    "Date": "2023/11/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0347b36b523a8c79b2ba65252236b6f9f23c67ba",
    "Root Cause": "N.A",
    "Bug report": "SummaryWriter.add_figure: add type hints (#110021)\n\nDiscovered a bug in our code that could have been prevented by type hints, so I added them \ud83d\ude04\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110021\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 6,
    "Deleted lines": "-    def add_figure(self, tag, figure, global_step=None, close=True, walltime=None):\n-            tag (str): Data identifier\n-            figure (matplotlib.pyplot.figure) or list of figures: Figure or a list of figures\n-            global_step (int): Global step value to record\n-            close (bool): Flag to automatically close the figure\n-            walltime (float): Optional override default walltime (time.time())",
    "Added lines": "+from typing import List, Optional, Union, TYPE_CHECKING\n+if TYPE_CHECKING:\n+    from matplotlib.figure import Figure\n+    def add_figure(\n+        self,\n+        tag: str,\n+        figure: Union[\"Figure\", List[\"Figure\"]],\n+        global_step: Optional[int] = None,\n+        close: bool = True,\n+        walltime: Optional[float] = None\n+    ) -> None:\n+            tag: Data identifier\n+            figure: Figure or a list of figures\n+            global_step: Global step value to record\n+            close: Flag to automatically close the figure\n+            walltime: Optional override default walltime (time.time())",
    "Label": "clean"
},
{
    "Id": 853,
    "Library": "pytorch",
    "Date": "2023/10/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4daf8afe8ebdbb4eec4f1e6c214c9f08f8404ba3",
    "Root Cause": "N.A",
    "Bug report": "Revert \"Fix bug: not creating empty tensor with correct sizes and device. (#106734)\" (#112170)\n\nThis reverts commit 528a2c0aa97d152b8004254040076b8ae605bf9f.\n\nThe PR is wrong, see #110941.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112170\nApproved by: https://github.com/albanD",
    "Number of deleted lines": 1,
    "Deleted lines": "-  Tensor result = at::empty(self.sizes(), self.options().dtype(kBool).device(self.device()));",
    "Added lines": "+  Tensor result = at::empty({0}, self.options().dtype(kBool));",
    "Label": "clean"
},
{
    "Id": 854,
    "Library": "pytorch",
    "Date": "2023/10/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/67638d4dadc120ed48a3f7d70cce05ca0483a0fc",
    "Root Cause": "N.A",
    "Bug report": "torch.compile: fix bug of fallback_randn when 'generator' is None (#112240)\n\nWhen I run Stable Diffusion in [Huggingface/Diffusers](https://github.com/huggingface/diffusers)\uff0can error occured:\n```\nLoweringException: AssertionError: should have been handled in replace_random.py.\n   target:  aten.randn.generator\n   args[0]:  [1, 4, 64, 64]\n   kwargs: {'generator': None, 'dtype': torch.float16, 'layout': torch.strided, 'device': device(type='cuda', index=0), 'pin_memory': False}\n```\nIt looks like some bug of dynamo, and you can reproduce this bug like this:\n```python\nimport torch\ndef model(shape, generator):\n      return torch.randn(shape, generator=generator, device=\"cuda:0\")\nmodel = torch.compile(model)\nx = model((1, 3, 64, 64), None)\nprint(x)\n```\nError occurs because 'None' is passed into \u2018generator' ,  and dynamo has to process `torch.randn` into fx node `torch.ops.aten.randn.generator`.\naten.randn.generator is not processed by decomposition and  it is processed by lowering in [torch/_inductor/lowering.py](https://github.com/pytorch/pytorch/blob/main/torch/_inductor/lowering.py#L1815), randn.generator is processed like this:\n```python\n@register_lowering(aten.randn)\ndef randn(*args, **kwargs):\n    if kwargs.get(\"generator\", None) is not None:\n        return fallback_randn_generator(*args, **kwargs)\n    elif config.fallback_random:\n        return fallback_randn_default(*args, **kwargs)\n    raise AssertionError(\"should have been handled in replace_random.py\")\n```\nAs you can see, because 'generator' is None, it will not step into `fallback_randn_generator`, and of course, if you don't open `config.fallback_random`, it will not step into `fallback_randn_default`, too. Actually, if 'generator' is None, it could also be processed as`aten.randn.default`.  And then, AssertionError will be throw, but in here, I will not disscuss too much about how to process this bug and will open an issue.\n\nActually, `config.fallback_random` offers a way to debug randn in [config.py](https://github.com/pytorch/pytorch/blob/main/torch/_inductor/config.py#L190), so I try to open `config.fallback_random` to debug my model. But when I open it by:\n```python\n# fallback to eager for random/dropout, this is slow but useful for debugging\nfallback_random = True\n```\nAnother error occurs!\n```python\nLoweringException: RuntimeError: Unknown keyword argument 'generator' for operator 'aten::randn'. Schema: aten::randn(SymInt[] size, *, ScalarType? dtype=None, Layouit? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\n```\nObviously, `aten::randn` does not support `kwargs:{generator: None}`, so it should be popped before kwargs is feeded into `fallback_randn_default`.\n\nThat's all I'm going to say. Thanks for reading carefully.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112240\nApproved by: https://github.com/jansel",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        kwargs.pop(\"generator\", None)\n+        kwargs.pop(\"generator\", None)",
    "Label": "clean"
},
{
    "Id": 855,
    "Library": "pytorch",
    "Date": "2023/10/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e5c8ac85448edba98654c898cf42ba43ce5e77cb",
    "Root Cause": "N.A",
    "Bug report": "Eliminate try-catch block around triton::_triton_bsr_dense_mm_out call. (#112154)\n\nAs in the title.\n\nCurrently, the try-catch block hides the failures from triton kernel launches that are not related to exceptions that the try-catch block is meant to ignore. When triton kernel launch fails (e.g. due to bugs in triton or lack of resources), ignoring such failures will lead to hard-to-explain/unrelated errors in subsequent code.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112154\nApproved by: https://github.com/amjames, https://github.com/cpuhrsch",
    "Number of deleted lines": 11,
    "Deleted lines": "-    try {\n-      const auto triton_kernel = c10::Dispatcher::singleton()\n-        .findSchemaOrThrow(\"triton::_triton_bsr_dense_mm_out\", \"\")\n-        .typed<Tensor&(const Tensor&, const Tensor&, Tensor&)>();\n-      // Call Triton only if dispatch key was overwritten.\n-      // This is not strictly necessary since the definition is done in Python,\n-      // but we leave it here for extra safety.\n-    } catch (const std::exception& e) {\n-      // The schema is not defined and/or the key is not overwritten,\n-      // so skip and execute the code below.\n-    }",
    "Added lines": "+    const auto triton_schema = c10::Dispatcher::singleton()\n+      .findSchema({\"triton::_triton_bsr_dense_mm_out\", \"\"});\n+    if (triton_schema.has_value()) {\n+      const auto triton_kernel = triton_schema.value().typed<Tensor&(const Tensor&, const Tensor&, Tensor&)>();\n+    } /* else the schema is not defined and/or the key is not\n+         overwritten, so skip and execute the code below. */",
    "Label": "clean"
},
{
    "Id": 856,
    "Library": "pytorch",
    "Date": "2023/10/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/37253c0cd5dc57a318d2024db896988700f2d747",
    "Root Cause": "N.A",
    "Bug report": "Update RUFF to 0.1.1 (#111618)\n\nUpdates ruff to the latest version with some bugfixes\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111618\nApproved by: https://github.com/colesbury",
    "Number of deleted lines": 1,
    "Deleted lines": "-    'ruff==0.1.0',",
    "Added lines": "+    'ruff==0.1.1',",
    "Label": "clean"
},
{
    "Id": 857,
    "Library": "pytorch",
    "Date": "2023/10/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/50b80185d62dfe19de58a7d31a534d42b1e5b68b",
    "Root Cause": "N.A",
    "Bug report": "fix bugs about traceback.walk_stack in python3.8.x (#110922)\n\nFixes #110769\n\nas stated.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110922\nApproved by: https://github.com/mikaylagawarecki",
    "Number of deleted lines": 2,
    "Deleted lines": "-            traceback.walk_stack(None), lookup_lines=False\n-            traceback.walk_stack(None), lookup_lines=False",
    "Added lines": "+import inspect\n+            traceback.walk_stack(inspect.currentframe()), lookup_lines=False\n+            traceback.walk_stack(inspect.currentframe()), lookup_lines=False",
    "Label": "clean"
},
{
    "Id": 858,
    "Library": "pytorch",
    "Date": "2023/10/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/898482f1bfd93f94d0be32d9201cbb6a093ec272",
    "Root Cause": "N.A",
    "Bug report": "[logging] log exceptions when provided (#111164)\n\nThis PR will cause logging.exception() to also dump the exception and stacktrace. Copied from https://github.com/python/cpython/blob/74723e11109a320e628898817ab449b3dad9ee96/Lib/logging/__init__.py#L707-L711\n\nrepro:\n\n<details>\n\n```python\nimport torch\nimport torch._inductor.config\n\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = \"runtime_error\"\n\ndef fn(x, y):\n    return (x @ y).relu()\n\nx, y = [torch.rand((16, 16), device='cuda') for _ in range (2)]\ntorch.compile(fn)(x, y)\n```\nrun with TORCHDYNAMO_REPRO_AFTER=aot TORCHDYNAMO_REPRO_LEVEL=4\n\n</details>\n\nbefore:\n```\n...\n[2023-10-12 14:18:52,902] torch._dynamo.debug_utils: [ERROR] While minifying the program in accuracy minification mode, ran into a runtime exception which is likely an unrelated issue. Skipping this graph.\n```\n\nnow:\n```\n...\n[2023-10-12 14:18:52,902] torch._dynamo.debug_utils: [ERROR] While minifying the program in accuracy minification mode, ran into a runtime exception which is likely an unrelated issue. Skipping this graph.\nTraceback (most recent call last):\n  File \"/data/users/dberard/scripts/relu_accuracy_issue.py\", line 10, in <module>\n    torch.compile(fn)(x, y)\n...\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111164\nApproved by: https://github.com/eellison",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        # exception handling - copied from logging.Formatter.format\n+        if record.exc_info:\n+            # Cache the traceback text to avoid converting it multiple times\n+            # (it's constant anyway)\n+            if not record.exc_text:\n+                record.exc_text = self.formatException(record.exc_info)\n+",
    "Label": "clean"
},
{
    "Id": 859,
    "Library": "pytorch",
    "Date": "2023/10/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/733368a82242f17e754be3f07cbacd782c4e64e9",
    "Root Cause": "N.A",
    "Bug report": "Change default NCCL_ASYNC_ERROR_HANDLING to 3:SkipCleanUp (#110723)\n\nSummary\n\nCurrently, when detecting a timeout/exception in the watchdog\nworkCleanupLoop, we call nccl APIs to abort all the active communicators\nbefore finally re-raising the exception and killing the process.  The\nnccl APIs may hang, causing additional problems. Instead, just re-raise.\n\n@kumpera proposed that changing this default should save us from a lot of commonly observed errors.\n\nNote: there are other cuda/nccl api calls in our watchdog, which also could hang. This change is not a substitute for a deeper refactor.\n\nDetail\n\nThe current default (NCCL_ASYNC_ERROR_HANDLING=1:TearDown) meant the following:\n\nSHOULD_TEAR_DOWN() evaluates to true\n  - This affects 'ProcessGroupNCCL::WorkNCCL::handleException`\n  - handleException is called from two places:\n     - work.wait() -> synchronizeInternal() -> handleException()\n     - workCleanupLoop() -> handleException()\n  - when true, the excpetion is logged and rethrown\n\nSHOULD_CLEAN_UP() evaluates to true\n  - This only impacts the workCleanupLoop()\n  - When true, it means all communicators will be aborted (ncclCommAbort())\n    upon work exception or timeout\n\nThe proposed new default is NCCL_ASYNC_ERROR_HANDLING3=3:SkipCleanUp.\n\nThis only changes SHOULD_CLEAN_UP() to false, impacting workCleanupLoop() behavior.\nCommunicators will no longer be aborted, which should avoid a class of bugs where the watchdog hangs due to calling nccl APIs which may block/hang.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110723\nApproved by: https://github.com/fduwjj, https://github.com/xw285cornell",
    "Number of deleted lines": 2,
    "Deleted lines": "-      parseEnvVarIntDefault(NCCL_ASYNC_ERROR_HANDLING, 1 /*TearDown*/));\n-      asyncErrorHandling_ = TearDown;",
    "Added lines": "+      parseEnvVarIntDefault(NCCL_ASYNC_ERROR_HANDLING, 3 /*SkipCleanUp*/));\n+      asyncErrorHandling_ = SkipCleanUp;",
    "Label": "clean"
},
{
    "Id": 860,
    "Library": "pytorch",
    "Date": "2023/10/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/16e3f158b947c2c14a98178670f22c047a40807c",
    "Root Cause": "N.A",
    "Bug report": "Add function to port FX minified graph to HLO via StableHLO (#109084)\n\nIf `XLA_HLO_DEBUG` flag is enabled, generated a minified HLO graph when using the minifier. This function enables HLO minification support by porting the minified FX graph to StableHLO via the `save_torch_model_as_stablehlo` function.\n\nThis allows users to port the minified graph to compilers that are not compatible with TorchDynamo/Inductor workflow and use XLA instead. The purpose of this PR is to help XLA users debug accuracy and compilation errors. It will also be helpful for existing TorchDynamo/XLA workflow on `torchxla_trace_once` backend as well.\n\nFixes [#5461](https://github.com/pytorch/xla/issues/5461) in Torch XLA repo. CC @GleasonK @qihqi\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109084\nApproved by: https://github.com/anijain2305",
    "Number of deleted lines": 1,
    "Deleted lines": "-import os.path",
    "Added lines": "+import os\n+def create_minified_hlo_graph(minified_fx_graph, inputs):\n+    \"\"\"\n+    Takes minified FX graph as primary input, and ports it to HLO via StableHLO\n+    Provides minified HLO graph as output, and archive them to local directory\n+    \"\"\"\n+    hlo_dir = f\"{os.getcwd()}/hlo_files\"\n+    os.makedirs(hlo_dir, exists_ok=True)\n+\n+    from torch_xla.stablehlo import save_torch_model_as_stablehlo\n+    save_torch_model_as_stablehlo(minified_fx_graph, inputs, hlo_dir)\n+\n+    # If XLA debugging environment is enabled, create minified HLO graph as well\n+    if \"XLA_HLO_DEBUG\" in os.environ:\n+        create_minified_hlo_graph(failing_fx, failing_state.inps)\n+",
    "Label": "clean"
},
{
    "Id": 861,
    "Library": "pytorch",
    "Date": "2023/09/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/40b83d98de1f5ebb72d081b04b253251caeee32a",
    "Root Cause": "N.A",
    "Bug report": "fix bugs in export docstrings (#110169)\n\nFirst error\n\n```\nTraceback (most recent call last):\n  File \"/home/ubuntu/exporty.py\", line 8, in <module>\n    ep = torch.export.export(MyModule(), torch.randn(5))\n  File \"/opt/conda/envs/sam/lib/python3.10/site-packages/torch/export/__init__.py\", line 509, in export\n    return export(f, args, kwargs, constraints)\n  File \"/opt/conda/envs/sam/lib/python3.10/site-packages/torch/_export/__init__.py\", line 314, in export\n    raise UserError(UserErrorType.INVALID_INPUT,\ntorch._dynamo.exc.UserError: Expecting `args` to be a tuple of example positional inputs, got <class 'torch.Tensor'>\n```\n\nSecond error\n\n```\n(sam) ubuntu@ip-172-31-9-217:~$ python exporty.py\nTraceback (most recent call last):\n  File \"/home/ubuntu/exporty.py\", line 13, in <module>\n    torch.export.save(ep, 'exported_program.pt2', extra_files=extra_files)\n  File \"/opt/conda/envs/sam/lib/python3.10/site-packages/torch/export/__init__.py\", line 566, in save\n    save(ep, f, extra_files=extra_files, opset_version=opset_version)\n  File \"/opt/conda/envs/sam/lib/python3.10/site-packages/torch/_export/__init__.py\", line 595, in save\n    encoded_content = content.encode('utf-8')\nAttributeError: 'bytes' object has no attribute 'encode'. Did you mean: 'decode'?\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110169\nApproved by: https://github.com/angelayi",
    "Number of deleted lines": 3,
    "Deleted lines": "-        ep = torch.export.export(MyModule(), torch.randn(5))\n-        extra_files = {'foo.txt': b'bar'}\n-",
    "Added lines": "+        ep = torch.export.export(MyModule(), (torch.randn(5),))\n+        extra_files = {'foo.txt': b'bar'.decode('utf-8')}\n+        print(ep(torch.randn(5)))",
    "Label": "clean"
},
{
    "Id": 862,
    "Library": "pytorch",
    "Date": "2023/09/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/213badf632371dabc955224db9dc91b2ed61a71c",
    "Root Cause": "N.A",
    "Bug report": "[dynamo][guards-log] Add debug msg for nn_module_guards only when log is enabled (#110167)\n\nI did not do any benchmarks, but there could be a small overhead of creating the debug_msg. Adding debug_msg only when guards log is enabled.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110167\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 3,
    "Deleted lines": "-        # debug_msg is only for debugging help and goes to kwargs of guard call,\n-        # which is ignored.\n-        self._produce_guard_code(guard, [f'{name}({ref}, debug_msg=\"{g}\")'])",
    "Added lines": "+        if guards_log.isEnabledFor(logging.DEBUG):\n+            # Avoid debug_msg related python bytecode overhead in the runtime.\n+\n+            # debug_msg is only for debugging help and goes to kwargs of guard call,\n+            # which is ignored.\n+            self._produce_guard_code(guard, [f'{name}({ref}, debug_msg=\"{g}\")'])\n+        else:\n+            self._produce_guard_code(guard, [f\"{name}({ref})\"])",
    "Label": "clean"
},
{
    "Id": 863,
    "Library": "pytorch",
    "Date": "2023/09/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/abd9b763caf4e56e7d5cd40894ac40f05c893682",
    "Root Cause": "N.A",
    "Bug report": "[RFC] Add debug log as we lower each FX node (#109602)\n\nI found this useful for orienting myself when I threw an error\nmid-lowering.  What do other people think?\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109602\nApproved by: https://github.com/malfet, https://github.com/voznesenskym",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+from torch._logging import LazyString\n+        log.debug(\"lowering %s\", LazyString(lambda: n.format_node()))",
    "Label": "clean"
},
{
    "Id": 864,
    "Library": "pytorch",
    "Date": "2023/09/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0ec9f59f70fc98ed8d71af3c89badc88f083872a",
    "Root Cause": "N.A",
    "Bug report": "Loudly Error in dynamo bench if eager fails (#109536)\n\nHelps debug https://github.com/pytorch/benchmark/issues/1901\n\nI will wait until the ONNX beartype sev is fixed before merging\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109536\nApproved by: https://github.com/xuzhao9",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            print(f\"Original Error: {str(e)}\")",
    "Label": "clean"
},
{
    "Id": 865,
    "Library": "pytorch",
    "Date": "2023/09/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6f4b9cc9abcceed95d4f9d2b6992c8d0eb9d6188",
    "Root Cause": "N.A",
    "Bug report": "[export] Skip noop runtime assertion pass. (#109395)\n\nSummary:\nIf there's no inline constraints added, just return the original graph.\nWe want to do this because sometimes this pass mess up the node names,\nbefore we actually fix this, we could make the behavior a bit less buggy\nby skipping noop passes.\n\nTest Plan:\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nFixes #ISSUE_NUMBER\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109395\nApproved by: https://github.com/angelayi",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        self.counter = 0\n+        self.counter += 1\n+        # Sometimes this pass would return a wrong graph where we have mismatched\n+        # node names in signature. Before we fix it, let's just skip it.\n+        if self.counter == 0 and type(self) is _AddRuntimeAssertionsForInlineConstraintsPass:\n+            return PassResult(graph_module, False)\n+",
    "Label": "clean"
},
{
    "Id": 866,
    "Library": "pytorch",
    "Date": "2023/09/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/afad0d074b5504c87aa1dc9ae352686a8dd3a8eb",
    "Root Cause": "N.A",
    "Bug report": "[inductor][Optimus]Improve logging for group batch fusion (#109314)\n\nSummary: Log graph with Everpaste for debug and find more patterns to fuse\n\nTest Plan: to add logs\n\nDifferential Revision: D49284640\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109314\nApproved by: https://github.com/yanboliang",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+if config.is_fbcode():\n+    from torch._inductor.fb.utils import get_everpaste_url\n+\n+def print_graph(graph: torch.fx.Graph, msg: str):\n+    if config.is_fbcode():\n+        log.info(\"%s Print graph: %s\", msg, get_everpaste_url(str(graph)))\n+\n+\n+    print_graph(graph, \"Before group_batch fusion in post grads pass.\")\n+        print_graph(graph, f\"Apply fusion {rule.__class__.__name__}.\")\n+    print_graph(graph, \"Before group_batch fusion in pre grads pass.\")\n+        print_graph(graph, f\"Apply fusion {rule.__class__.__name__}.\")",
    "Label": "clean"
},
{
    "Id": 867,
    "Library": "pytorch",
    "Date": "2023/09/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f6d8ecf9b38e970db4d72291c3f5ecdd67c43bb6",
    "Root Cause": "N.A",
    "Bug report": "Use the correct channel token when uploading nightly triton conda (#109073)\n\nThis fixes 2 bugs on triton build workflow:\n\n* Use the wrong conda credential when `UPLOAD_CHANNEL` is not set https://github.com/pytorch/pytorch/actions/runs/6129675580/job/16691419329#step:7:18\n* Upload wheel and conda packages when pushing to main in addition to nightly.  This is needed because the binary wheel build on trunk also looks for torchtriton package after the triton pin is updated.\n\n### Testing\n\nhttps://github.com/pytorch/pytorch/actions/runs/6152447684/job/16694843862?pr=109073#step:7:38 looks correct now.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109073\nApproved by: https://github.com/atalman",
    "Number of deleted lines": 3,
    "Deleted lines": "-    environment: ${{ (github.event_name == 'push' && (github.event.ref == 'refs/heads/nightly' || startsWith(github.event.ref, 'refs/tags/v'))) && 'conda-aws-upload' || '' }}\n-    environment: ${{ (github.event_name == 'push' && (github.event.ref == 'refs/heads/nightly' || startsWith(github.event.ref, 'refs/tags/v'))) && 'conda-aws-upload' || '' }}\n-          if [[ \"${UPLOAD_CHANNEL}\" = \"nightly\" ]]; then",
    "Added lines": "+    environment: ${{ (github.event_name == 'push' && (github.event.ref == 'refs/heads/nightly' || github.event.ref == 'refs/heads/main' || startsWith(github.event.ref, 'refs/tags/v'))) && 'conda-aws-upload' || '' }}\n+    environment: ${{ (github.event_name == 'push' && (github.event.ref == 'refs/heads/nightly' || github.event.ref == 'refs/heads/main' || startsWith(github.event.ref, 'refs/tags/v'))) && 'conda-aws-upload' || '' }}\n+          if [[ \"${UPLOAD_CHANNEL:-nightly}\" == \"nightly\" ]]; then",
    "Label": "clean"
},
{
    "Id": 868,
    "Library": "pytorch",
    "Date": "2023/09/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/20812d69e5ab45ac2a491f22581e69e467f72467",
    "Root Cause": "N.A",
    "Bug report": "Fix extension rebuilding on Linux (#108613)\n\nOn Linux, CUDA header dependencies are not correctly tracked. After you modify a CUDA header, affected CUDA files won't be rebuilt. This PR will fix this problem.\n\n```console\n$ ninja -t deps\nrep_penalty.o: #deps 2, deps mtime 1693956351892493247 (VALID)\n    /home/qc/Workspace/NotMe/exllama/exllama_ext/cpu_func/rep_penalty.cpp\n    /home/qc/Workspace/NotMe/exllama/exllama_ext/cpu_func/rep_penalty.h\n\nrms_norm.cuda.o: #deps 0, deps mtime 1693961188871054130 (VALID)\n\nrope.cuda.o: #deps 0, deps mtime 1693961188954388632 (VALID)\n\ncuda_buffers.cuda.o: #deps 0, deps mtime 1693961188797719768 (VALID)\n\n...\n```\n\nHistorically, this line of code has been changed twice. It was first implemented in #49344 and there's no `if IS_WINDOWS`, just like now. Then in #56015 someone added `if IS_WINDOWS` for unknown reason. That PR has no description so I don't know what bug he encountered. I don't think there's any bug with these flags on Linux, at least for today. CMake generates exactly the same flags for CUDA.\n\n```ninja\n#############################################\n# Rule for compiling CUDA files.\n\nrule CUDA_COMPILER__cpp_cuda_unscanned_Debug\n  depfile = $DEP_FILE\n  deps = gcc\n  command = ${LAUNCHER}${CODE_CHECK}/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler $DEFINES $INCLUDES $FLAGS -MD -MT $out -MF $DEP_FILE -x cu -c $in -o $out\n  description = Building CUDA object $out\n```\n\nwhere `-MD` is short for `--generate-dependencies-with-compile` and `-MF` is short for `--dependency-output`. My words can be verified by `nvcc --help`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108613\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 2,
    "Deleted lines": "-            if IS_WINDOWS:\n-                nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'",
    "Added lines": "+            nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'",
    "Label": "clean"
},
{
    "Id": 869,
    "Library": "pytorch",
    "Date": "2023/09/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/eab57145ab6dfcdda14d932f143faf59cb4ca683",
    "Root Cause": "N.A",
    "Bug report": "fix matrix_power documentation bug (#108585)\n\nThe torch.linalg.matrix_power documentation suggests using the formula\n`matrix_power(torch.linalg.solve(A, B), n) == matrix_power(A, -n)  @ B`\nto avoid negative matrix powers. But the ordering of the left side is not correct. This patch fixes it to:\n`torch.linalg.solve(matrix_power(A, n), B) == matrix_power(A, -n)  @ B`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108585\nApproved by: https://github.com/lezcano",
    "Number of deleted lines": 1,
    "Deleted lines": "-        matrix_power(torch.linalg.solve(A, B), n) == matrix_power(A, -n)  @ B",
    "Added lines": "+        torch.linalg.solve(matrix_power(A, n), B) == matrix_power(A, -n)  @ B",
    "Label": "clean"
},
{
    "Id": 870,
    "Library": "pytorch",
    "Date": "2023/09/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4a472d9e952f3a8b19d564bdf56d9e4fbc39486b",
    "Root Cause": "N.A",
    "Bug report": "[jit] Verify stack size and index to prevent off-by-one error (#108413)\n\nHi!\n\nI've been fuzzing different pytorch modules with with [sydr-fuzz](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch), and found a heap buffer overflow error that occurs by incorrect loop condition in torch::jit::unpickler.cpp. This bug can be triggered by `torch::distributed::rpc::deserializeRequest()` method in RPC module.\n\nDocker to reproduce found error: [Dockerfile](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch).\n\n### PoC for deserealizeRequest():\n[crash-001e49dcd3a3c439e2b1273d580049309e052bdd.txt](https://github.com/pytorch/pytorch/files/12498999/crash-001e49dcd3a3c439e2b1273d580049309e052bdd.txt)\n\n### ASAN report\n```\n==339982==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x619000086a88 at pc 0x000000996fa4 bp 0x7fffffff9c50 sp 0x7fffffff9c48\nREAD of size 4 at 0x619000086a88 thread T0\n    #0 0x996fa3 in c10::IValue::IValue(c10::IValue const&) /pytorch/aten/src/ATen/core/ivalue.h:226:33\n    #1 0xdf99a38 in std::pair<c10::impl::DictIterator<c10::IValue, c10::IValue, ska_ordered::detailv3::sherwood_v3_table<std::pair<c10::IValue, c10::IValue>, c10::IValue, c10::detail::DictKeyHash, ska_ordered::detailv3::KeyOrValueHasher<c10::IValue, std::pair<c10::IValue, c10::IValue>, c10::detail::DictKeyHash>, c10::detail::DictKeyEqualTo, ska_ordered::detailv3::KeyOrValueEquality<c10::IValue, std::pair<c10::IValue, c10::IValue>, c10::detail::DictKeyEqualTo>, std::allocator<std::pair<c10::IValue, c10::IValue> >, std::allocator<ska_ordered::detailv3::sherwood_v3_entry<std::pair<c10::IValue, c10::IValue> > > >::templated_iterator<std::pair<c10::IValue, c10::IValue> > >, bool> c10::Dict<c10::IValue, c10::IValue>::insert_or_assign<c10::IValue&, c10::IValue&>(c10::IValue&, c10::IValue&) const /pytorch/aten/src/ATen/core/Dict_inl.h:136:5\n    #2 0xed966c7 in torch::jit::Unpickler::readInstruction() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:490:14\n    #3 0xed94377 in torch::jit::Unpickler::run() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:253:27\n    #4 0xed93fd1 in torch::jit::Unpickler::parse_ivalue() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:206:3\n    #5 0xece09ee in torch::jit::unpickle(std::function<unsigned long (char*, unsigned long)>, std::function<c10::StrongTypePtr (c10::QualifiedName const&)>, c10::ArrayRef<at::Tensor>, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)) /pytorch/torch/csrc/jit/serialization/pickle.cpp:126:20\n    #6 0xece0dac in torch::jit::unpickle(char const*, unsigned long, std::function<c10::StrongTypePtr (c10::QualifiedName const&)>, c10::ArrayRef<at::Tensor>, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)) /pytorch/torch/csrc/jit/serialization/pickle.cpp:136:10\n    #7 0x1006a4e7 in torch::distributed::rpc::PythonRemoteCall::fromMessage(torch::distributed::rpc::Message const&) /pytorch/torch/csrc/distributed/rpc/python_remote_call.cpp:40:16\n    #8 0x101d02e1 in torch::distributed::rpc::deserializeRequest(torch::distributed::rpc::Message const&) /pytorch/torch/csrc/distributed/rpc/utils.cpp:111:14\n    #9 0x8db738 in LLVMFuzzerTestOneInput /message_deserialize.cc:192:27\n    #10 0x8d84cd in ExecuteFilesOnyByOne /AFLplusplus/utils/aflpp_driver/aflpp_driver.c:255:7\n    #11 0x8d82d8 in LLVMFuzzerRunDriver /AFLplusplus/utils/aflpp_driver/aflpp_driver.c\n    #12 0x8d7e98 in main /AFLplusplus/utils/aflpp_driver/aflpp_driver.c:300:10\n    #13 0x7ffff7a37082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082) (BuildId: 1878e6b475720c7c51969e69ab2d276fae6d1dee)\n    #14 0x817c4d in _start (/message_deserialize_afl+0x817c4d)\n\n0x619000086a88 is located 8 bytes to the right of 1024-byte region [0x619000086680,0x619000086a80)\nallocated by thread T0 here:\n    #0 0x8d54ca in operator new(unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/asan/asan_new_delete.cpp:95:3\n\nSUMMARY: AddressSanitizer: heap-buffer-overflow /pytorch/aten/src/ATen/core/ivalue.h:226:33 in c10::IValue::IValue(c10::IValue const&)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108413\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 2,
    "Deleted lines": "-          stack_.size() % 2 == 0 && start % 2 == 0,\n-          \", but stack_ expected to contain even number of elements\");",
    "Added lines": "+          (stack_.size() - start) % 2 == 0,\n+          \", but stack_ is iterated by two elements at a time\");\n+      TORCH_CHECK(\n+          (stack_.size() - start) % 2 == 0,\n+          \"Parsing error: stack_ is of size \",\n+          stack_.size(),\n+          \" and start index is \",\n+          start,\n+          \", but stack_ is iterated by two elemenst at a time\");",
    "Label": "clean"
},
{
    "Id": 871,
    "Library": "pytorch",
    "Date": "2023/09/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/aff7fdcb4cdd379d7dfaf9781d59f3f2f52c0fb3",
    "Root Cause": "N.A",
    "Bug report": "Add a missing argument (#108477)\n\nFix a tiny bug in string formatting.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108477\nApproved by: https://github.com/Skylion007",
    "Number of deleted lines": 1,
    "Deleted lines": "-          \"element %d of gradients tuple is None, but the corresponding Tensor requires grad\");",
    "Added lines": "+          \"element %d of gradients tuple is None, but the corresponding Tensor requires grad\",\n+          i);",
    "Label": "clean"
},
{
    "Id": 872,
    "Library": "pytorch",
    "Date": "2023/09/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1b3dc05c3e703841e64e0277d473a0baf3296671",
    "Root Cause": "N.A",
    "Bug report": "Use contiguous() to handle noncontiguous outputs during elementwise decomposition (#108140)\n\nFixes https://github.com/pytorch/pytorch/issues/108218\n\nUse contiguous() API to handle noncontiguous outputs during elementwise decomp\n\nWith this change, ops is decomposing properly (testcase from the bug):\n```\ngraph():\n    %arg0_1 : [#users=3] = placeholder[target=arg0_1]\n    %abs_1 : [#users=1] = call_function[target=torch.ops.aten.abs.default](args = (%arg0_1,), kwargs = {})\n    %floor : [#users=1] = call_function[target=torch.ops.aten.floor.default](args = (%abs_1,), kwargs = {})\n    %sign : [#users=1] = call_function[target=torch.ops.aten.sign.default](args = (%arg0_1,), kwargs = {})\n    %mul : [#users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%floor, %sign), kwargs = {})\n    %sub : [#users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%arg0_1, %mul), kwargs = {})\n    return (sub,)\n```\nOutput:\n```\ntensor([[ 0.2871,  0.7189,  0.7297],\n        [ 0.8782, -0.4899,  0.7055]], device='hpu:0')\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108140\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 1,
    "Deleted lines": "-        output = output.new_empty(output.shape)",
    "Added lines": "+        output = output.contiguous()",
    "Label": "clean"
},
{
    "Id": 873,
    "Library": "pytorch",
    "Date": "2023/08/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7fb131043c8deb59768dcd4a7ce2d9c51d4782ac",
    "Root Cause": "N.A",
    "Bug report": "[memory snapshots] _record_memory_history_legacy bug fix (#108260)\n\nThe argment order for the legacy path got swapped in a recent patch.\nBecause there is still a blog post documenting the legacy interface\npeople are hitting this pathway.\n\nThis patch fixes #108208\nI will also update the blog post to the new API so that people are\nmore likely to use the newer `_record_memory_history` API.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108260\nApproved by: https://github.com/awgu",
    "Number of deleted lines": 1,
    "Deleted lines": "-        record_context_cpp,",
    "Added lines": "+        record_context_cpp,",
    "Label": "clean"
},
{
    "Id": 874,
    "Library": "pytorch",
    "Date": "2023/08/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cd20a89ccc3a588f900acaeb8f9b3e42846a6479",
    "Root Cause": "N.A",
    "Bug report": "[ROCM] Add ROCm support to debug_dump and enable_debug_mode (#107845)\n\nenable_debug_mode and debug_dump are enabled in ROCM releases.  Add ROCM flags to #if defines so they can be accessed by PyTorch users.\n\nFixes #ISSUE_NUMBER\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/107845\nApproved by: https://github.com/pruthvistony, https://github.com/huydhn",
    "Number of deleted lines": 2,
    "Deleted lines": "-#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11030)\n-  TORCH_CHECK(false, \"CUDA graphs may only be used in Pytorch built with CUDA >= 11.3 and is not yet supported on ROCM\");",
    "Added lines": "+#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11030)|| (defined(USE_ROCM) && ROCM_VERSION >= 50600)\n+  TORCH_CHECK(false, \"CUDA graphs may only be used in Pytorch built with CUDA >= 11.3 or ROCM >= 5.6\");",
    "Label": "clean"
},
{
    "Id": 875,
    "Library": "pytorch",
    "Date": "2023/08/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c8f7f2659b8361077932d1b2d789e0b0453e6853",
    "Root Cause": "N.A",
    "Bug report": "Two small mem_eff bug fixes (#103201)\n\n# Summary\nUpstream two small bug fixes:\n* https://github.com/fairinternal/xformers/pull/679\n* https://github.com/fairinternal/xformers/pull/681\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103201\nApproved by: https://github.com/cpuhrsch",
    "Number of deleted lines": 12,
    "Deleted lines": "-      scale = warp_uniform(scale);\n-      head_dim = warp_uniform(head_dim);\n-      head_dim_value = warp_uniform(head_dim_value);\n-      num_heads = warp_uniform(num_heads);\n-      q_strideM = warp_uniform(q_strideM);\n-      k_strideM = warp_uniform(k_strideM);\n-      v_strideM = warp_uniform(v_strideM);\n-      bias_strideM = warp_uniform(bias_strideM);\n-      gO_strideM = warp_uniform(gO_strideM);\n-      gB_strideM = warp_uniform(gB_strideM);\n-      gQKV_strideM_multiplier = warp_uniform(gQKV_strideM_multiplier);\n-",
    "Added lines": "+      // Some values are modified above\n+      // Signal to the compiler that they are the same in all threads\n+      // and can be stored in warp-uniform registers (Sm75+)",
    "Label": "clean"
},
{
    "Id": 876,
    "Library": "pytorch",
    "Date": "2023/08/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d8dadb0f25debc98a5798ffd3f8554f061e9bd2d",
    "Root Cause": "N.A",
    "Bug report": "aot_inductor: fix compile returning None if cache hits (#107020)\n\nSummary:\nSeems like a bug in D47998435, where when cache hits it returns None\n\nRepro:\n\n```\nclass TestModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x + 1\n\nmod = TestModule()\ninp = torch.rand(1)\nout = mod(inp)\nmod2 = torch.fx.symbolic_trace(mod, concrete_args=[inp])\n\nso, _ = torch._export.aot_compile(mod2, tuple([inp]))\n# 2nd time, it will return None\nso, _ = torch._export.aot_compile(mod2, tuple([inp]))\nassert so is not None  # FAIL\n```\n\nTest Plan: Run the repro\n\nDifferential Revision: D48258375\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/107020\nApproved by: https://github.com/angelayi",
    "Number of deleted lines": 2,
    "Deleted lines": "-            return cls.cache[key]\n-        return None",
    "Added lines": "+        return cls.cache[key]",
    "Label": "clean"
},
{
    "Id": 877,
    "Library": "pytorch",
    "Date": "2023/08/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/528a2c0aa97d152b8004254040076b8ae605bf9f",
    "Root Cause": "N.A",
    "Bug report": "Fix bug: not creating empty tensor with correct sizes and device. (#106734)\n\nSummary:\nlogical_add and logical_add_ are reusing implementation of logical_add_out. But the `comparison_op` doesn't create an empty tensor with correct sizes and device type.\n```\nTensor& logical_and_out(const Tensor& self, const Tensor& other, Tensor& result) { return comparison_op_out(result, self, other, logical_and_stub); }\nTensor logical_and(const Tensor& self, const Tensor& other) { return comparison_op(self, other, static_cast<OutFunc>(at::logical_and_out)); }\nTensor& logical_and_(Tensor& self, const Tensor& other) { return comparison_op_(self, other, static_cast<OutFunc>(at::logical_and_out)); }\n```\n\nTest Plan: CI tests.\n\nDifferential Revision: D48134169\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/106734\nApproved by: https://github.com/jackm321",
    "Number of deleted lines": 1,
    "Deleted lines": "-  Tensor result = at::empty({0}, self.options().dtype(kBool));",
    "Added lines": "+  Tensor result = at::empty(self.sizes(), self.options().dtype(kBool).device(self.device()));",
    "Label": "clean"
},
{
    "Id": 878,
    "Library": "pytorch",
    "Date": "2023/08/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c9cdcb299a99ef260f854e09283eb4e22ff71a2d",
    "Root Cause": "N.A",
    "Bug report": "Remove ExclusivelyOwned from register_dispatch_key (#106791)\n\nThis fixes a bug that could occur with python decompositions.\n\nWhen an operation is intercepted in the c++ code in pytorch the outputs a created as `ExclusivelyOwned<at::Tensor>`s. Later on when it dispatches back to python for the decomposition these tensors have their ownership shared with python. In a normal use case the exclusively owned tensor is released and it's value returned as a non-exclusively owned tensor from the operation. However if the python decomposition throws an error the `ExclusivelyOwned` wrapper destroys the `at::Tensor` leading to a python reference to a tensor which isn't alive (and meaning pytorch falls over in debug mode).\n\nNote this will be a performance hit when handling errors.\n\nFixes #106790\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/106791\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 14,
    "Deleted lines": "-            maybe_star = \"*\" if k is SchemaKind.functional else \"\"\n-      namedinference::propagate_names({maybe_star}outputs_[output_idx], names);\n-    proxy_outputs_[output_idx] = c10::ExclusivelyOwned<Tensor>(std::move(maybe_proxy).value());\n-            output_type = \"c10::ExclusivelyOwned<Tensor>\"\n-            output_value = \"*outputs_[output_idx]\"\n-            output_value = \"proxy_outputs_[output_idx].has_value() ? **proxy_outputs_[output_idx] : outputs_[output_idx].get()\"\n-            proxy_field = f\"std::array<c10::optional<c10::ExclusivelyOwned<Tensor>>, {len(f.func.returns)}> proxy_outputs_;\"\n-            output_value = \"proxy_outputs_[output_idx].has_value() ? **proxy_outputs_[output_idx] : outputs_[output_idx].get()\"\n-            proxy_field = f\"std::array<c10::optional<c10::ExclusivelyOwned<Tensor>>, {len(f.func.returns)}> proxy_outputs_;\"\n-                    maybe_star = \"*\" if k is SchemaKind.functional else \"\"\n-                    expr = f\"{maybe_star}op.outputs_[{i}]\"\n-                        f\"if (op.proxy_outputs_[{i}].has_value()) op.outputs_[{i}].get().copy_(**op.proxy_outputs_[{i}]);\"\n-                    ret_expr = \"std::move(op.outputs_[0]).take()\"  # small optimization\n-                        f\"std::move(op.outputs_[{i}]).take()\"",
    "Added lines": "+      namedinference::propagate_names(outputs_[output_idx], names);\n+    proxy_outputs_[output_idx] = std::move(maybe_proxy).value();\n+            output_type = \"Tensor\"\n+            output_value = \"outputs_[output_idx]\"\n+            output_value = \"proxy_outputs_[output_idx].has_value() ? *proxy_outputs_[output_idx] : outputs_[output_idx].get()\"\n+            proxy_field = f\"std::array<c10::optional<Tensor>, {len(f.func.returns)}> proxy_outputs_;\"\n+            output_value = \"proxy_outputs_[output_idx].has_value() ? *proxy_outputs_[output_idx] : outputs_[output_idx].get()\"\n+            proxy_field = f\"std::array<c10::optional<Tensor>, {len(f.func.returns)}> proxy_outputs_;\"\n+                    expr = f\"op.outputs_[{i}]\"\n+                        f\"if (op.proxy_outputs_[{i}].has_value()) op.outputs_[{i}].get().copy_(*op.proxy_outputs_[{i}]);\"\n+                    ret_expr = \"std::move(op.outputs_[0])\"  # small optimization\n+                        f\"std::move(op.outputs_[{i}])\"",
    "Label": "clean"
},
{
    "Id": 879,
    "Library": "pytorch",
    "Date": "2023/08/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/787d5259fa3f4404b13d530f672fb4b96a14450b",
    "Root Cause": "N.A",
    "Bug report": "Include fused nodes' debug_str in FusedSchedulerNode::debug_str_extra (#106356)\n\nCurrently, there's no way to print the debug information of fused scheduler nodes. I'm adding this to inspect the individual nodes' ir type e.g. ComputedBuffer, but not sure if this would be useful for more use cases\n\nFusedSchedulerNode::debug_str_extra only prints its fused nodes' names\n```\n# calling .debug_str() on a FusedSchedulerNode\nbuf0_buf1: FusedSchedulerNode(NoneType)\nbuf0_buf1.writes = [MemoryDep('buf0', c0, {c0: 10}), MemoryDep('buf1', c0, {c0: 10})]\nbuf0_buf1.unmet_dependencies = []\nbuf0_buf1.met_dependencies = [MemoryDep('arg0_1', c0, {c0: 100}), MemoryDep('arg1_1', c0, {c0: 10})]\nbuf0_buf1.users = None\nbuf0_buf1.snodes = ['buf0', 'buf1']\n```\n\nThis PR adds support to print the fused nodes' debug_str\n```\nbuf0_buf1: FusedSchedulerNode(NoneType)\nbuf0_buf1.writes = [MemoryDep('buf0', c0, {c0: 10}), MemoryDep('buf1', c0, {c0: 10})]\nbuf0_buf1.unmet_dependencies = []\nbuf0_buf1.met_dependencies = [MemoryDep('arg0_1', c0, {c0: 100}), MemoryDep('arg1_1', c0, {c0: 10})]\nbuf0_buf1.users = None\n    buf0_buf1.snodes[0] =\n    buf0: SchedulerNode(ComputedBuffer)\n    buf0.writes = [MemoryDep('buf0', c0, {c0: 10})]\n    buf0.unmet_dependencies = []\n    buf0.met_dependencies = [MemoryDep('arg0_1', c0, {c0: 100})]\n    buf0.users = [NodeUser(node=SchedulerNode(name='buf1'), can_inplace=True)]\n    buf0.group.device = cuda:0\n    buf0.group.iteration = (10, 10)\n    buf0.sizes = ([10], [10])\n    class buf0_loop_body:\n        var_ranges = {z0: 10, z1: 10}\n        index0 = 10*z0 + z1\n        index1 = z0\n        def body(self, ops):\n            get_index = self.get_index('index0')\n            load = ops.load('arg0_1', get_index)\n            reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)\n            get_index_1 = self.get_index('index1')\n            store_reduction = ops.store_reduction('buf0', get_index_1, reduction)\n            return store_reduction\n    buf0_buf1.snodes[1] =\n    buf1: SchedulerNode(ComputedBuffer)\n    buf1.writes = [MemoryDep('buf1', c0, {c0: 10})]\n    buf1.unmet_dependencies = [MemoryDep('buf0', c0, {c0: 10})]\n    buf1.met_dependencies = [MemoryDep('arg1_1', c0, {c0: 10})]\n    buf1.users = [NodeUser(node=OUTPUT, can_inplace=False)]\n    buf1.group.device = cuda:0\n    buf1.group.iteration = (10, 1)\n    buf1.sizes = ([10], [])\n    class buf1_loop_body:\n        var_ranges = {z0: 10}\n        index0 = z0\n        def body(self, ops):\n            get_index = self.get_index('index0')\n            load = ops.load('arg1_1', get_index)\n            cos = ops.cos(load)\n            get_index_1 = self.get_index('index0')\n            load_1 = ops.load('buf0', get_index_1)\n            add = ops.add(cos, load_1)\n            get_index_2 = self.get_index('index0')\n            store = ops.store('buf1', get_index_2, add, None)\n            return store\n```\n\nI'm assuming that FusedSchedulerNode cannot be fused, i.e. can't have FusedSchedulerNode::snodes contain any FusedSchedulerNode.\n\n# Tests\nChanges were tested adhoc by printing debug_str in GraphLowering::count_bytes, and running `python3 test/inductor/test_perf.py -k test_fusion_choice3`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/106356\nApproved by: https://github.com/peterbell10",
    "Number of deleted lines": 8,
    "Deleted lines": "-    def debug_str(self):\n-    def debug_str_extra(self):\n-    def debug_str_extra(self):\n-    def debug_str_extra(self):\n-    def debug_str_extra(self):\n-        return (\n-            f\"{self.get_name()}.snodes = {pformat([x.get_name() for x in self.snodes])}\"\n-        )",
    "Added lines": "+    def debug_str(self) -> str:\n+\n+    def debug_str_extra(self) -> str:\n+    def debug_str_extra(self) -> str:\n+    def debug_str_extra(self) -> str:\n+    def debug_str_extra(self) -> str:\n+        lines = [\n+            f\"{self.get_name()}.snodes[{i}] =\\n{node.debug_str()}\"\n+            for i, node in enumerate(self.snodes)\n+        ]\n+        return textwrap.indent(\"\\n\".join(lines).rstrip(), \"    \")",
    "Label": "clean"
},
{
    "Id": 880,
    "Library": "pytorch",
    "Date": "2023/08/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a6f7dd4707ac116c0f5fb5f44f42429f38d23ab4",
    "Root Cause": "N.A",
    "Bug report": "Catch cuda driver shutdown error in NCCLWatchdog (#106503)\n\nThere is a design flaw in NCCLWatchdog, namely it spawns threads that\ntalk to the CUDA api, but the CUDA api may have been deinitialized,\nforming a race.\n\nThis is a known issue with widespread impact\n(https://github.com/pytorch/pytorch/issues/90848).\n\nI should point out that i tested this fix on the repro command for https://github.com/pytorch/pytorch/issues/82632 by running `NCCL_DESYNC_DEBUG=1 CUDA_LAUNCH_BLOCKING=1 python test/distributed/test_c10d_nccl.py -k test_find_unused_parameters_kwarg_debug_detail` and observing that instead of crashing, we observe log messages with the exception string about the cuda driver shutdown error.\n\nA partial fix was landed already, but it applied too narrowly:\nhttps://github.com/pytorch/pytorch/commit/ec071a0815adfd180ba1ab103be2f7d2227f07cc\n\nThis PR is a copy-paste of the previous fix, applying to one more case,\nplugging a hole.  We probably need to do a more thorough review and\neither plug all the holes, or design this differently.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/106503\nApproved by: https://github.com/kwen2501",
    "Number of deleted lines": 4,
    "Deleted lines": "-  for (const auto i : c10::irange(devices_.size())) {\n-    // Checking the work's corresponding CUDA events' status\n-    if (!(*ncclStartEvents_)[i].query()) {\n-      return false;",
    "Added lines": "+  try {\n+    for (const auto i : c10::irange(devices_.size())) {\n+      // Checking the work's corresponding CUDA events' status\n+      if (!(*ncclStartEvents_)[i].query()) {\n+        return false;\n+      }\n+  } catch (const std::exception& e) {\n+    if (std::string(e.what()).find(\"driver shutting down\") ==\n+        std::string::npos) {\n+      throw;\n+    }\n+    LOG(INFO) << \"[Rank \" << rank_\n+              << \"] Event query failed with exception: \" << e.what();\n+",
    "Label": "clean"
},
{
    "Id": 881,
    "Library": "pytorch",
    "Date": "2023/08/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/63b7be5a6f203338877a308ca10ed485290174a8",
    "Root Cause": "N.A",
    "Bug report": "Use ProcessPoolExecutor in the ufmt adapter (#106123)\n\nWhen running on a host with multiple CPUs, the ufmt linter was not able to use them very effectively. The biggest single culprit seems to be debug logging inside blib2to3 trying to acquire a lock, but disabling that doesn't help much - I suppose this must be GIL contention. Changing to a ProcessPoolExecutor makes it much faster.\n\nThe following timings are on a PaperSpace GPU+ instance with 8 vCPUs (the cores show up as Intel(R) Xeon(R) CPU E5-2623 v4 @ 2.60GHz but I'm not entirely clear if those are shared with other instances).\n\nOn main:\n\n```\n$ time lintrunner --all-files --take UFMT\nok No lint issues.\n\nreal    7m46.140s\nuser    8m0.828s\nsys     0m5.446s\n```\n\nOn this branch:\n\n```\n$ time lintrunner --all-files --take UFMT\nok No lint issues.\n\nreal    1m7.255s\nuser    8m13.388s\nsys     0m3.506s\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/106123\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 3,
    "Deleted lines": "-        format=\"<%(threadName)s:%(levelname)s> %(message)s\",\n-    with concurrent.futures.ThreadPoolExecutor(\n-        thread_name_prefix=\"Thread\",",
    "Added lines": "+        format=\"<%(processName)s:%(levelname)s> %(message)s\",\n+    with concurrent.futures.ProcessPoolExecutor(",
    "Label": "clean"
},
{
    "Id": 882,
    "Library": "pytorch",
    "Date": "2023/07/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f160a972aa7ef2db508dbd233246f6dc61643d88",
    "Root Cause": "N.A",
    "Bug report": "[inductor][easy] \"unhandled ValueRange op\" - log at debug level (#106215)\n\nSet this log line to debug level - it appears frequently for many ops that don't have implementations following https://github.com/pytorch/pytorch/pull/102611.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/106215\nApproved by: https://github.com/lezcano",
    "Number of deleted lines": 1,
    "Deleted lines": "-        log.warning(\"unhandled ValueRange op %s\", name)",
    "Added lines": "+        log.debug(\"unhandled ValueRange op %s\", name)",
    "Label": "clean"
},
{
    "Id": 883,
    "Library": "pytorch",
    "Date": "2023/07/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f725e6374def2cdb245b09c46944cef5512116d1",
    "Root Cause": "N.A",
    "Bug report": "doc: fix fake quantize per channel doc (#105955)\n\nanother doc bug for fake_quantize_per_channel\n\nfunction doc now matches https://github.com/pytorch/pytorch/blob/e7142700ede35bc0e8520b9fc9572f4a09672830/aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp#L32\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/105955\nApproved by: https://github.com/kit1980",
    "Number of deleted lines": 1,
    "Deleted lines": "-fake_quantize_per_channel_affine(input, scale, zero_point, quant_min, quant_max) -> Tensor",
    "Added lines": "+fake_quantize_per_channel_affine(input, scale, zero_point, axis, quant_min, quant_max) -> Tensor",
    "Label": "clean"
},
{
    "Id": 884,
    "Library": "pytorch",
    "Date": "2023/07/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/71d18f61059dbb617a8afc79b64598d2fbb89d63",
    "Root Cause": "N.A",
    "Bug report": "[DocString] Fix incorrect api Examples (#105911)\n\nFix incorrect Examples in `torch.linalg.tensorinv`.\n\n- before (bug) : `torch.linalg.inverse`\n- after: `torch.linalg.inv`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/105911\nApproved by: https://github.com/lezcano",
    "Number of deleted lines": 1,
    "Deleted lines": "-    >>> Ainv = torch.linalg.inverse(A)",
    "Added lines": "+    >>> Ainv = torch.linalg.inv(A)",
    "Label": "clean"
},
{
    "Id": 885,
    "Library": "pytorch",
    "Date": "2023/07/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b2b1f2194b2fe47fdf50678bc9ac888b3d62f523",
    "Root Cause": "N.A",
    "Bug report": "[inductor] Enable vectorization in fbcode (#105756)\n\nIn fbcode, to run the test python script (with its accompanying test DSO) we\nneed to invoke the correct python, with the correct PYTHONPATH, so we supply\nthose by reading the appropriate values out of `sys`.\n\nIt's an improvement for OSS too, since the user may not be running the default\npython.\n\nMy previous attempt of using `torch.backends.cpu.get_cpu_capability()` didn't work out, for two reasons:\n1. That function actually refuses to report AVX512 support; it's #ifdef-ed out, for some reason.\n2. In CI, we apparently are picking INVALID_VEC_ISA (at least when running\ninductor_timm_cpu_accuracy), whereas `get_cpu_capability` reports AVX2.  This\nis surprising, and probably indicates a bug (either in cpu capability or our\ntest binary), but I'd rather not go digging for it.\n\nDifferential Revision: [D47678649](https://our.internmc.facebook.com/intern/diff/D47678649/)\n\nDifferential Revision: [D47678649](https://our.internmc.facebook.com/intern/diff/D47678649)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/105756\nApproved by: https://github.com/jansel, https://github.com/mikekgfb",
    "Number of deleted lines": 4,
    "Deleted lines": "-                # TODO: get vectorization working in fbcode.\n-                # For now, this always fails, so we fall back to generating non-vectorized cpu code.\n-                        \"python\",\n-            macros = f\"-D{macros}\"",
    "Added lines": "+                        sys.executable,\n+                    env={**os.environ, \"PYTHONPATH\": \":\".join(sys.path)},\n+            if config.is_fbcode() and vec_isa != invalid_vec_isa:\n+                cap = str(vec_isa).upper()\n+                macros = \" \".join(\n+                    [\n+                        vec_isa.build_arch_flags(),\n+                        f\"-D CPU_CAPABILITY={cap}\",\n+                        f\"-D CPU_CAPABILITY_{cap}\",\n+                        f\"-D HAVE_{cap}_CPU_DEFINITION\",\n+                    ]\n+                )\n+            else:\n+                macros = f\"-D{macros}\"",
    "Label": "clean"
},
{
    "Id": 886,
    "Library": "pytorch",
    "Date": "2023/07/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/70b5264ec5912618a28e831f14571a01df7a63a9",
    "Root Cause": "N.A",
    "Bug report": "[EZ][BE] Fix the massively annoying strict-weak-ordering issue. (#105189)\n\nSummary:\nkip_fist_pump\n\nRunning any EgoOCR workflow in non-opt modes was breaking with https://fburl.com/strict-weak-ordering\n\nPainstakingly found out that the stable_sort comparator in the generate_proposals caffe2 op was the issue due to numerical imprecision. This was causing Word Detector model to barf with the error. Adding explicit handling for the [irreflexivity property](https://www.boost.org/sgi/stl/StrictWeakOrdering.html) fixes this annoying strict-weak-ordering issue that has bugged me and several others(https://fb.workplace.com/groups/1405155842844877/permalink/7079705785389826/) for a while.\n\nWe can finally run all OCR workflows in non-opt mode! :)\n\nTest Plan:\nDebugged this with `fdb --disable-auto-breakpoints --secondary-debugger=lldb buck2 run mode/dev-sand ai_demos/server_model_zoo/models/ego_ocr_e2e_prod:ego_ocr_e2e_prod_binary`\n\nand running `breakpoint set -E c++` in the lldb terminal.\n\nDifferential Revision: D47446816\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/105189\nApproved by: https://github.com/malfet, https://github.com/atalman",
    "Number of deleted lines": 1,
    "Deleted lines": "-    for (const auto i : c10::irange(m))q[i] += s;",
    "Added lines": "+        if (A == B) {\n+          // explicit irreflexivity handling to sate\n+          // https://fburl.com/strict-weak-ordering\n+          return false;\n+        }\n+    for (const auto i : c10::irange(m))\n+      q[i] += s;",
    "Label": "clean"
},
{
    "Id": 887,
    "Library": "pytorch",
    "Date": "2023/07/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d2fa3f608b5e4f582a8aaf752f10efe4ca72a7d0",
    "Root Cause": "N.A",
    "Bug report": "Produce more logs from TCPStore init (#105350)\n\nthis diff:\n1. adds debug logs to TCPStore initialization to better capture the \"connection reset by peer\" error.\n\nDifferential Revision: [D47454956](https://our.internmc.facebook.com/intern/diff/D47454956/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/105350\nApproved by: https://github.com/kumpera, https://github.com/fduwjj",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include <torch/csrc/distributed/c10d/logging.h>\n+    // server successfully started\n+    C10D_DEBUG(\"The server has started on port = {}.\", server_->port());\n+  // TCP connection established\n+  C10D_DEBUG(\"TCP client connected to host {}:{}\", addr_.host, addr_.port);\n+  // TCP CallbackClient connection established\n+  C10D_DEBUG(\n+      \"TCP callback client connected to host {}:{}\", addr_.host, addr_.port);",
    "Label": "clean"
},
{
    "Id": 888,
    "Library": "pytorch",
    "Date": "2023/07/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/398606e1c4af00429fdfdceb0ed35b2fb98647e8",
    "Root Cause": "N.A",
    "Bug report": "Fix bug when an index appears in two expressions (#104886)\n\nWe were not adding the bounds to `replacement_vars` in this case.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104886\nApproved by: https://github.com/eellison, https://github.com/Skylion007",
    "Number of deleted lines": 4,
    "Deleted lines": "-        if bound is not None:\n-            return bound\n-\n-        bound = bound_sympy(expr, self.replacement_vals)",
    "Added lines": "+        if bound is None:\n+            bound = bound_sympy(expr, self.replacement_vals)\n+        # The following assertion is true at the time of this writing\n+        # We don't assert is as to not execute bound_sympy when bound is not None\n+        # assert bound is None or bound == bound_sympy(expr, self.replacement_vals)",
    "Label": "clean"
},
{
    "Id": 889,
    "Library": "pytorch",
    "Date": "2023/07/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ed5ea15714361db57aff7f8b6dbf6f9d6cb3e95c",
    "Root Cause": "N.A",
    "Bug report": "[Easy] remove debug code (#104915)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104915\nApproved by: https://github.com/mlazos",
    "Number of deleted lines": 2,
    "Deleted lines": "-            if not len(node.tensor_weakrefs) == len(node.stack_traces):\n-                breakpoint()",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 890,
    "Library": "pytorch",
    "Date": "2023/07/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d184c811661eace999370fa4d833d7f91faae051",
    "Root Cause": "N.A",
    "Bug report": "Add -fstandalone-debug debug flag (#104475)\n\n# Summary\n\nWhile debugging something in lldb, I found that the formatter I wrote for c10::intarrayref was not working correctly producing:\n`(std::string) $6 = error: summary string parsing error`\n\nBased off of this thread: https://github.com/vadimcn/codelldb/issues/415\n\nI adde the standalone-debug information and fixed the std::string formatting issue.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104475\nApproved by: https://github.com/ezyang, https://github.com/malfet",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  append_cxx_flag_if_supported(\"-fstandalone-debug\" CMAKE_CXX_FLAGS_DEBUG)",
    "Label": "clean"
},
{
    "Id": 891,
    "Library": "pytorch",
    "Date": "2023/07/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/456ecefd525c3b6fa7d0854a57a3a659edc04fd1",
    "Root Cause": "N.A",
    "Bug report": "[BE] Fix warning in top-level CMakeLists.txt (#104726)\n\nFixes warning introduced by https://github.com/pytorch/pytorch/issues/102594:\n```\nCMake Warning (dev) in CMakeLists.txt:\n  A logical block opening on the line\n    /pytorch/CMakeLists.txt:726 (if)\n  closes on the line\n    /pytorch/CMakeLists.txt:735 (endif)\n  with mis-matching arguments.\n```\n\n<!--\ncopilot:poem\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at b7555d5</samp>\n\n> _`DEBUG_CUDA` on_\n> _No more CUDA in exe_\n> _Winter bug is fixed_\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104726\nApproved by: https://github.com/huydhn, https://github.com/atalman",
    "Number of deleted lines": 1,
    "Deleted lines": "-endif(NOT MSVC)",
    "Added lines": "+endif(DEBUG_CUDA)",
    "Label": "clean"
},
{
    "Id": 892,
    "Library": "pytorch",
    "Date": "2023/07/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b3e60ee05250a0b3289f4611d916e8ac4b0a0a78",
    "Root Cause": "N.A",
    "Bug report": "Fix broken torch._inductor.config import (#104477)\n\nThis fixes the bug in profiler code exposed by  https://github.com/pytorch/pytorch/pull/104368 that introduced on the fact that `import torch._dynamo` also imports `torch._inductor.config`:\n```\n$ python -c \"import torch._inductor;print(torch._inductor.config)\"\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nAttributeError: module 'torch._inductor' has no attribute 'config'\n(base) $ python -c \"import torch._dynamo;print(torch._inductor.config)\"\n<module 'torch._inductor.config' from '/home/nshulga/git/pytorch/pytorch/torch/_inductor/config.py'>\n```\n\n### Testing\nD47159397\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104477\nApproved by: https://github.com/aaronenyeshi, https://github.com/malfet",
    "Number of deleted lines": 3,
    "Deleted lines": "-                and torch._inductor.config.triton.cudagraphs\n-                os.environ[\"DISABLE_CUPTI_LAZY_REINIT\"] = \"1\"\n-                self.add_metadata_json(\"DISABLE_CUPTI_LAZY_REINIT\", \"1\")",
    "Added lines": "+                import torch._inductor.config as inductor_config\n+                if inductor_config.triton.cudagraphs:\n+                    os.environ[\"DISABLE_CUPTI_LAZY_REINIT\"] = \"1\"\n+                    self.add_metadata_json(\"DISABLE_CUPTI_LAZY_REINIT\", \"1\")",
    "Label": "clean"
},
{
    "Id": 893,
    "Library": "pytorch",
    "Date": "2023/06/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fd40abb70623f1f01427b80a4195e6dbbc40875f",
    "Root Cause": "N.A",
    "Bug report": "Minor bugfix for int inputs in minifier (#104100)\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104100\nApproved by: https://github.com/albanD",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if arg.is_cuda:",
    "Added lines": "+        if isinstance(arg, torch.Tensor) and arg.is_cuda:",
    "Label": "clean"
},
{
    "Id": 894,
    "Library": "pytorch",
    "Date": "2023/06/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4aee0fef117df1975200984a0e6c36eb8a078bf2",
    "Root Cause": "N.A",
    "Bug report": "Heap buffer overflow due to wrong loop condition in torch::jit::unpickler (#103667)\n\nHi!\n\nI've been fuzzing different pytorch modules with with [sydr-fuzz](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch), and found a heap buffer overflow error that occures by incorrect loop condition in torch::jit::unpickler.cpp. This bug was found in several fuzzing targets: it can be triggered by `torch::jit::load()` method when loading a .pt model and by `torch::distributed::rpc::deserializeRequest()` method in RPC module.\n\nAll found errors could be reproduced with provided docker: [Dockerfile](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch).\n\n### PoC for deserealizeRequest():\n[crash-0722408578cd2f26593b5a01e26d2a078d3dc5f6.zip](https://github.com/pytorch/pytorch/files/11756694/crash-0722408578cd2f26593b5a01e26d2a078d3dc5f6.zip)\n\n```\n=================================================================\n==29858==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x6020004ed808 at pc 0x000000680084 bp 0x7ffcbd8220d0 sp 0x7ffcbd8220c8\nREAD of size 4 at 0x6020004ed808 thread T0\n    #0 0x680083 in c10::IValue::IValue(c10::IValue const&) /pytorch/aten/src/ATen/core/ivalue.h:224:33\n    #1 0xdc4beb8 in std::pair<c10::impl::DictIterator<c10::IValue, c10::IValue, ska_ordered::detailv3::sherwood_v3_table<std::pair<c10::IValue, c10::IValue>, c10::IValue, c10::detail::DictKeyHash, ska_ordered::detailv3::KeyOrValueHasher<c10::IValue, std::pair<c10::IValue, c10::IValue>, c10::detail::DictKeyHash>, c10::detail::DictKeyEqualTo, ska_ordered::detailv3::KeyOrValueEquality<c10::IValue, std::pair<c10::IValue, c10::IValue>, c10::detail::DictKeyEqualTo>, std::allocator<std::pair<c10::IValue, c10::IValue> >, std::allocator<ska_ordered::detailv3::sherwood_v3_entry<std::pair<c10::IValue, c10::IValue> > > >::templated_iterator<std::pair<c10::IValue, c10::IValue> > >, bool> c10::Dict<c10::IValue, c10::IValue>::insert_or_assign<c10::IValue&, c10::IValue&>(c10::IValue&, c10::IValue&) const /pytorch/aten/src/ATen/core/Dict_inl.h:136:5\n    #2 0xea680a7 in torch::jit::Unpickler::readInstruction() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:452:14\n    #3 0xea64e07 in torch::jit::Unpickler::run() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:251:27\n    #4 0xea64a61 in torch::jit::Unpickler::parse_ivalue() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:204:3\n    #5 0xe9b13ce in torch::jit::unpickle(std::function<unsigned long (char*, unsigned long)>, std::function<c10::StrongTypePtr (c10::QualifiedName const&)>, c10::ArrayRef<at::Tensor>, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)) /pytorch/torch/csrc/jit/serialization/pickle.cpp:126:20\n    #6 0xe9b178c in torch::jit::unpickle(char const*, unsigned long, std::function<c10::StrongTypePtr (c10::QualifiedName const&)>, c10::ArrayRef<at::Tensor>, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)) /pytorch/torch/csrc/jit/serialization/pickle.cpp:136:10\n    #7 0xfdc8aa1 in torch::distributed::rpc::(anonymous namespace)::toIValues(torch::distributed::rpc::Message const&, torch::distributed::rpc::MessageType) /pytorch/torch/csrc/distributed/rpc/rref_proto.cpp:23:16\n    #8 0xfdca3ca in torch::distributed::rpc::PythonRRefFetchCall::fromMessage(torch::distributed::rpc::Message const&) /pytorch/torch/csrc/distributed/rpc/rref_proto.cpp:105:17\n    #9 0xfe7f347 in torch::distributed::rpc::deserializeRequest(torch::distributed::rpc::Message const&) /pytorch/torch/csrc/distributed/rpc/utils.cpp:117:14\n    #10 0x5c5d13 in LLVMFuzzerTestOneInput /message_deserialize.cc:192:27\n    #11 0x5c2bfd in ExecuteFilesOnyByOne /AFLplusplus/utils/aflpp_driver/aflpp_driver.c:255:7\n    #12 0x5c2a08 in LLVMFuzzerRunDriver /AFLplusplus/utils/aflpp_driver/aflpp_driver.c\n    #13 0x5c25c8 in main /AFLplusplus/utils/aflpp_driver/aflpp_driver.c:300:10\n    #14 0x7feb90908082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082) (BuildId: 1878e6b475720c7c51969e69ab2d276fae6d1dee)\n    #15 0x50237d in _start (/message_deserialize_afl+0x50237d)\n\n0x6020004ed808 is located 8 bytes to the right of 16-byte region [0x6020004ed7f0,0x6020004ed800)\nallocated by thread T0 here:\n    #0 0x5bfc1d in operator new(unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/asan/asan_new_delete.cpp:95:3\n    #1 0x32ad8d1 in std::_Vector_base<c10::IValue, std::allocator<c10::IValue> >::_M_allocate(unsigned long) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:346:20\n    #2 0x32ad8d1 in void std::vector<c10::IValue, std::allocator<c10::IValue> >::_M_realloc_insert<double>(__gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue, std::allocator<c10::IValue> > >, double&&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:440:33\n\nSUMMARY: AddressSanitizer: heap-buffer-overflow /pytorch/aten/src/ATen/core/ivalue.h:224:33 in c10::IValue::IValue(c10::IValue const&)\nShadow bytes around the buggy address:\n  0x0c0480095ab0: fa fa fd fd fa fa fd fd fa fa fd fd fa fa 00 00\n  0x0c0480095ac0: fa fa 00 00 fa fa 00 00 fa fa 04 fa fa fa 04 fa\n  0x0c0480095ad0: fa fa 00 fa fa fa fd fa fa fa 04 fa fa fa 00 fa\n  0x0c0480095ae0: fa fa 00 fa fa fa fd fa fa fa fd fa fa fa fd fa\n  0x0c0480095af0: fa fa fd fd fa fa 00 00 fa fa 00 fa fa fa 00 00\n=>0x0c0480095b00: fa[fa]fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0480095b10: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0480095b20: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0480095b30: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0480095b40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0480095b50: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\nShadow byte legend (one shadow byte represents 8 application bytes):\n  Addressable:           00\n  Partially addressable: 01 02 03 04 05 06 07\n  Heap left redzone:       fa\n  Freed heap region:       fd\n  Stack left redzone:      f1\n  Stack mid redzone:       f2\n  Stack right redzone:     f3\n  Stack after return:      f5\n  Stack use after scope:   f8\n  Global redzone:          f9\n  Global init order:       f6\n  Poisoned by user:        f7\n  Container overflow:      fc\n  Array cookie:            ac\n  Intra object redzone:    bb\n  ASan internal:           fe\n  Left alloca redzone:     ca\n  Right alloca redzone:    cb\n==29858==ABORTING\n```\n\n### PoC for load():\n[crash-2bd32e496811fb06de24a2bb720dc6490218009f.zip](/uploads/53d108cdd434ec4b11a2034bbca3cfd8/crash-2bd32e496811fb06de24a2bb720dc6490218009f.zip)\n\n```\n==29865==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60c00031f388 at pc 0x000000669984 bp 0x7ffd6c6de630 sp 0x7ffd6c6de628\nREAD of size 4 at 0x60c00031f388 thread T0\n    #0 0x669983 in c10::IValue::IValue(c10::IValue const&) /pytorch/aten/src/ATen/core/ivalue.h:224:33\n    #1 0xdc3de68 in std::pair<c10::impl::DictIterator<c10::IValue, c10::IValue, ska_ordered::detailv3::sherwood_v3_table<std::pair<c10::IValue, c10::IValue>, c10::IValue, c10::detail::DictKeyHash, ska_ordered::detailv3::KeyOrValueHasher<c10::IValue, std::pair<c10::IValue, c10::IValue>, c10::detail::DictKeyHash>, c10::detail::DictKeyEqualTo, ska_ordered::detailv3::KeyOrValueEquality<c10::IValue, std::pair<c10::IValue, c10::IValue>, c10::detail::DictKeyEqualTo>, std::allocator<std::pair<c10::IValue, c10::IValue> >, std::allocator<ska_ordered::detailv3::sherwood_v3_entry<std::pair<c10::IValue, c10::IValue> > > >::templated_iterator<std::pair<c10::IValue, c10::IValue> > >, bool> c10::Dict<c10::IValue, c10::IValue>::insert_or_assign<c10::IValue&, c10::IValue&>(c10::IValue&, c10::IValue&) const /pytorch/aten/src/ATen/core/Dict_inl.h:136:5\n    #2 0xea5a207 in torch::jit::Unpickler::readInstruction() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:452:14\n    #3 0xea56f67 in torch::jit::Unpickler::run() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:251:27\n    #4 0xea56bc1 in torch::jit::Unpickler::parse_ivalue() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:204:3\n    #5 0xe96db4e in torch::jit::readArchiveAndTensors(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<std::function<c10::StrongTypePtr (c10::QualifiedName const&)> >, c10::optional<std::function<c10::intrusive_ptr<c10::ivalue::Object, c10::detail::intrusive_target_default_null_type<c10::ivalue::Object> > (c10::StrongTypePtr, c10::IValue)> >, c10::optional<c10::Device>, caffe2::serialize::PyTorchStreamReader&, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&), std::shared_ptr<torch::jit::DeserializationStorageContext>) /pytorch/torch/csrc/jit/serialization/import_read.cpp:53:20\n    #6 0xe8fc648 in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/import.cpp:184:10\n    #7 0xe8f8935 in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::deserialize(c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:287:19\n    #8 0xe8f6d74 in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::istream&, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:386:25\n    #9 0xe90086e in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::istream&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:322:10\n    #10 0xe903209 in torch::jit::load(std::istream&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:482:10\n    #11 0x5c2d60 in LLVMFuzzerTestOneInput /load.cc:42:14\n    #12 0x5c2a8d in ExecuteFilesOnyByOne /AFLplusplus/utils/aflpp_driver/aflpp_driver.c:255:7\n    #13 0x5c2898 in LLVMFuzzerRunDriver /AFLplusplus/utils/aflpp_driver/aflpp_driver.c\n    #14 0x5c2458 in main /AFLplusplus/utils/aflpp_driver/aflpp_driver.c:300:10\n    #15 0x7f156ae33082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082) (BuildId: 1878e6b475720c7c51969e69ab2d276fae6d1dee)\n    #16 0x50220d in _start (/load_afl+0x50220d)\n\n0x60c00031f388 is located 8 bytes to the right of 128-byte region [0x60c00031f300,0x60c00031f380)\nallocated by thread T0 here:\n    #0 0x5bfaad in operator new(unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/asan/asan_new_delete.cpp:95:3\n    #1 0xa86231 in std::_Vector_base<c10::IValue, std::allocator<c10::IValue> >::_M_allocate(unsigned long) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:346:20\n    #2 0xa86231 in void std::vector<c10::IValue, std::allocator<c10::IValue> >::_M_realloc_insert<c10::IValue&>(__gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue, std::allocator<c10::IValue> > >, c10::IValue&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:440:33\n\nSUMMARY: AddressSanitizer: heap-buffer-overflow /pytorch/aten/src/ATen/core/ivalue.h:224:33 in c10::IValue::IValue(c10::IValue const&)\nShadow bytes around the buggy address:\n  0x0c188005be20: fd fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa\n  0x0c188005be30: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\n  0x0c188005be40: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd\n  0x0c188005be50: fd fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa\n  0x0c188005be60: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n=>0x0c188005be70: fa[fa]fa fa fa fa fa fa 00 00 00 00 00 00 00 00\n  0x0c188005be80: 00 00 00 00 00 00 00 00 fa fa fa fa fa fa fa fa\n  0x0c188005be90: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x0c188005bea0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c188005beb0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c188005bec0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\nShadow byte legend (one shadow byte represents 8 application bytes):\n  Addressable:           00\n  Partially addressable: 01 02 03 04 05 06 07\n  Heap left redzone:       fa\n  Freed heap region:       fd\n  Stack left redzone:      f1\n  Stack mid redzone:       f2\n  Stack right redzone:     f3\n  Stack after return:      f5\n  Stack use after scope:   f8\n  Global redzone:          f9\n  Global init order:       f6\n  Poisoned by user:        f7\n  Container overflow:      fc\n  Array cookie:            ac\n  Intra object redzone:    bb\n  ASan internal:           fe\n  Left alloca redzone:     ca\n  Right alloca redzone:    cb\n==29865==ABORTING\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103667\nApproved by: https://github.com/albanD",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      TORCH_CHECK(\n+          stack_.size() % 2 == 0 && start % 2 == 0,\n+          \"Parsing error: stack_ is of size \",\n+          stack_.size(),\n+          \" and start index is \",\n+          start,\n+          \", but stack_ expected to contain even number of elements\");",
    "Label": "clean"
},
{
    "Id": 895,
    "Library": "pytorch",
    "Date": "2023/06/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/750cbb299b4c0d04d5b785bdb4e28fd0aedefc2c",
    "Root Cause": "N.A",
    "Bug report": "[RPC] Check stack for emptiness in interpreter (#103327)\n\nHi! I found heap-buffer-overflow during PyTorch RPC-module fuzzing.\n\n[crash-9cc26b8da3b688a9c26614481239943b357c5636.zip](https://github.com/pytorch/pytorch/files/11707706/crash-9cc26b8da3b688a9c26614481239943b357c5636.zip)\n\n```\n    \"==10634==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x6060001b6a98 at pc 0x000000639a2e bp 0x7fffffff9100 sp 0x7fffffff90f8\",\n    \"READ of size 4 at 0x6060001b6a98 thread T0\",\n    \"    #0 0x639a2d in c10::IValue::isTensor() const /pytorch/aten/src/ATen/core/ivalue.h:432:27\",\n    \"    #1 0x639a2d in c10::IValue::toTensor() && /pytorch/aten/src/ATen/core/ivalue_inl.h:159:7\",\n    \"    #2 0xc5eb105 in at::Tensor c10::IValue::to<at::Tensor>() && /pytorch/aten/src/ATen/core/ivalue_inl.h:1690:1\",\n    \"    #3 0xc5eb105 in void torch::jit::pop<at::Tensor>(std::vector<c10::IValue, std::allocator<c10::IValue> >&, at::Tensor&) /pytorch/aten/src/ATen/core/stack.h:130:55\",\n    \"    #4 0xc5eaedb in torch::jit::dtype(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch/torch/csrc/jit/mobile/promoted_prim_ops.cpp:105:3\",\n    \"    #5 0xcc79600 in torch::jit::InterpreterStateImpl::runImpl(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch/torch/csrc/jit/runtime/interpreter.cpp:682:13\",\n    \"    #6 0xcc4158b in torch::jit::InterpreterStateImpl::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch/torch/csrc/jit/runtime/interpreter.cpp:1052:9\",\n    \"    #7 0x60f378 in runGraph(std::shared_ptr<torch::jit::Graph>, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) /jit_differential.cc:66:38\",\n    \"    #8 0x610bb9 in LLVMFuzzerTestOneInput /jit_differential.cc:107:25\",\n    \"    #9 0x535c91 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\",\n    \"    #10 0x51fb9c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\",\n    \"    #11 0x5258eb in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\",\n    \"    #12 0x54eea2 in main /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\",\n    \"    #13 0x7ffff7a37082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082) (BuildId: 1878e6b475720c7c51969e69ab2d276fae6d1dee)\",\n    \"    #14 0x51a4bd in _start (/jit_differential_fuzz+0x51a4bd)\",\n    \"\",\n    \"0x6060001b6a98 is located 8 bytes to the left of 64-byte region [0x6060001b6aa0,0x6060001b6ae0)\",\n    \"allocated by thread T0 here:\",\n    \"    #0 0x60c66d in operator new(unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/asan/asan_new_delete.cpp:95:3\",\n    \"    #1 0xa5a41b in std::_Vector_base<c10::IValue, std::allocator<c10::IValue> >::_M_allocate(unsigned long) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:346:20\",\n    \"    #2 0xa5a41b in void std::vector<c10::IValue, std::allocator<c10::IValue> >::_M_realloc_insert<c10::IValue&>(__gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue, std::allocator<c10::IValue> > >, c10::IValue&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:440:33\",\n    \"    #3 0xa5a241 in c10::IValue& std::vector<c10::IValue, std::allocator<c10::IValue> >::emplace_back<c10::IValue&>(c10::IValue&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:121:4\",\n    \"    #4 0xcc8209c in torch::jit::InterpreterStateImpl::runImpl(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch/torch/csrc/jit/runtime/interpreter.cpp:345:19\",\n    \"    #5 0xcc4158b in torch::jit::InterpreterStateImpl::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch/torch/csrc/jit/runtime/interpreter.cpp:1052:9\",\n    \"    #6 0x60f378 in runGraph(std::shared_ptr<torch::jit::Graph>, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) /jit_differential.cc:66:38\",\n    \"    #7 0x610bb9 in LLVMFuzzerTestOneInput /jit_differential.cc:107:25\",\n    \"    #8 0x535c91 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\",\n    \"    #9 0x51fb9c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\",\n    \"    #10 0x5258eb in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\",\n    \"    #11 0x54eea2 in main /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\",\n    \"    #12 0x7ffff7a37082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082) (BuildId: 1878e6b475720c7c51969e69ab2d276fae6d1dee)\",\n    \"\",\n    \"SUMMARY: AddressSanitizer: heap-buffer-overflow /pytorch/aten/src/ATen/core/ivalue.h:432:27 in c10::IValue::isTensor() const\",\n    \"Shadow bytes around the buggy address:\",\n    \"  0x0c0c8002ed00: 00 00 00 00 00 00 00 fa fa fa fa fa fd fd fd fd\",\n    \"  0x0c0c8002ed10: fd fd fd fd fa fa fa fa fd fd fd fd fd fd fd fd\",\n    \"  0x0c0c8002ed20: fa fa fa fa fd fd fd fd fd fd fd fd fa fa fa fa\",\n    \"  0x0c0c8002ed30: fd fd fd fd fd fd fd fd fa fa fa fa 00 00 00 00\",\n    \"  0x0c0c8002ed40: 00 00 00 00 fa fa fa fa fd fd fd fd fd fd fd fd\",\n    \"=>0x0c0c8002ed50: fa fa fa[fa]00 00 00 00 00 00 00 00 fa fa fa fa\",\n    \"  0x0c0c8002ed60: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\",\n    \"  0x0c0c8002ed70: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\",\n    \"  0x0c0c8002ed80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\",\n    \"  0x0c0c8002ed90: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\",\n    \"  0x0c0c8002eda0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\",\n    \"Shadow byte legend (one shadow byte represents 8 application bytes):\",\n    \"  Addressable:           00\",\n    \"  Partially addressable: 01 02 03 04 05 06 07\",\n    \"  Heap left redzone:       fa\",\n    \"  Freed heap region:       fd\",\n    \"  Stack left redzone:      f1\",\n    \"  Stack mid redzone:       f2\",\n    \"  Stack right redzone:     f3\",\n    \"  Stack after return:      f5\",\n    \"  Stack use after scope:   f8\",\n    \"  Global redzone:          f9\",\n    \"  Global init order:       f6\",\n    \"  Poisoned by user:        f7\",\n    \"  Container overflow:      fc\",\n    \"  Array cookie:            ac\",\n    \"  Intra object redzone:    bb\",\n    \"  ASan internal:           fe\",\n    \"  Left alloca redzone:     ca\",\n    \"  Right alloca redzone:    cb\",\n    \"==10634==ABORTING\"\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103327\nApproved by: https://github.com/Skylion007",
    "Number of deleted lines": 3,
    "Deleted lines": "-            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(stack.size() >= inst.N);\n-            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!stack.empty());\n-            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!stack.empty());",
    "Added lines": "+            TORCH_INTERNAL_ASSERT(stack.size() >= inst.N);\n+            TORCH_INTERNAL_ASSERT(!stack.empty());\n+            TORCH_INTERNAL_ASSERT(!stack.empty());",
    "Label": "clean"
},
{
    "Id": 896,
    "Library": "pytorch",
    "Date": "2023/06/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0ca3c6f7d7f4d4dd8cb51281036241ff8417bc83",
    "Root Cause": "N.A",
    "Bug report": "[_memory_viz.py] Fix bug when using profile_plot (#103384)\n\nWhen we updated plotting to add level of detail the Legend\ncode for profile_plot got broken. This patch fixes it.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103384\nApproved by: https://github.com/drisspg",
    "Number of deleted lines": 2,
    "Deleted lines": "-function Legend(svg, categories) {\n-        Legend(plot_svg.append('g'), alloc_data.categories)",
    "Added lines": "+function Legend(plot_svg, categories, width) {\n+        Legend(plot_svg.append('g'), alloc_data.categories, width)",
    "Label": "clean"
},
{
    "Id": 897,
    "Library": "pytorch",
    "Date": "2023/06/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3a0f37735c6935a46c3401195527e7197af8c905",
    "Root Cause": "N.A",
    "Bug report": "[pt2] bug fix: invert condition in `checkFloatingOrComplex` (#102944)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/102944\nApproved by: https://github.com/lezcano",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if allow_low_precision_dtypes:",
    "Added lines": "+    if not allow_low_precision_dtypes:",
    "Label": "clean"
},
{
    "Id": 898,
    "Library": "pytorch",
    "Date": "2023/06/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f3553c508c4a5187ba6cbc23a81ede8edcdef848",
    "Root Cause": "N.A",
    "Bug report": "ImportLib py3.10 bug in AOTInductor (#103277)\n\nOther projects have seen a similar issue https://github.com/quantumlib/Cirq/issues/4637\n\n## Before\n\n```\n(nightly) ubuntu@ip-172-31-2-131:~$ python /tmp/torchinductor_ubuntu/eq/ceqs7t4pesfhqllk6qf4k5spu2cm23l7quqdt2mkrp4rlcjl6kw5.py\nTraceback (most recent call last):\n  File \"/tmp/torchinductor_ubuntu/eq/ceqs7t4pesfhqllk6qf4k5spu2cm23l7quqdt2mkrp4rlcjl6kw5.py\", line 47, in <module>\n    module = CppWrapperCodeCache.load(cpp_wrapper_src, 'inductor_entry_cpp', 'czenwgemzbe2etzbh7hzhnwjhyamvwirgodyjlly75fayy4tp3rx', False)\n  File \"/opt/conda/envs/nightly/lib/python3.10/site-packages/torch/_inductor/codecache.py\", line 846, in load\n    assert isinstance(spec.loader, importlib.abc.Loader)\nAttributeError: module 'importlib' has no attribute 'abc'. Did you mean: '_abc'?\n```\n\n## After\n\n```sh\n(nightly) ubuntu@ip-172-31-2-131:~/test$ python /tmp/torchinductor_ubuntu/eq/ceqs7t4pesfhqllk6qf4k5spu2cm23l7quqdt2mkrp4rlcjl6kw5.py\n0.000272\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103277\nApproved by: https://github.com/desertfire",
    "Number of deleted lines": 1,
    "Deleted lines": "-                    assert isinstance(spec.loader, importlib.abc.Loader)",
    "Added lines": "+from importlib import abc\n+                    assert isinstance(spec.loader, abc.Loader)",
    "Label": "clean"
},
{
    "Id": 899,
    "Library": "pytorch",
    "Date": "2023/06/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/872fdb329b75c832c2cd1314b3613e28289d8fac",
    "Root Cause": "N.A",
    "Bug report": "This extra message would have helped with Wav2Vec2 debugging. (#103002)\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103002\nApproved by: https://github.com/janeyx99, https://github.com/anijain2305, https://github.com/voznesenskym, https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-                \"(graph leaves) support the deepcopy protocol at the moment\"",
    "Added lines": "+                \"(graph leaves) support the deepcopy protocol at the moment.  \"\n+                \"If you were attempting to deepcopy a module, this may be because \"\n+                \"of a torch.nn.utils.weight_norm usage, \"\n+                \"see https://github.com/pytorch/pytorch/pull/103001\"",
    "Label": "clean"
},
{
    "Id": 900,
    "Library": "pytorch",
    "Date": "2023/06/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cca7b3856451a48377b6e94efefc03ae79e4eb5d",
    "Root Cause": "N.A",
    "Bug report": "Don't allow skipping deepcopy (#102973)\n\nWe might mutate it afterwards!  This could lead to hard to understand\nbugs.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/102973\nApproved by: https://github.com/albanD",
    "Number of deleted lines": 4,
    "Deleted lines": "-        try:\n-            return copy.deepcopy(model)\n-        except TypeError:\n-            return model",
    "Added lines": "+        return copy.deepcopy(model)",
    "Label": "clean"
},
{
    "Id": 901,
    "Library": "pytorch",
    "Date": "2023/06/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e66c498d2df74c17c137d4bd3bb210c3976e28f4",
    "Root Cause": "N.A",
    "Bug report": "Log modules FSDP hooks fire for (#102508)\n\nUnder torch_distributed_debug >= INFO and use_orig_params=True, log post backward hook firing to debug things like FSDP + AC integration.\n\nDifferential Revision: [D46172916](https://our.internmc.facebook.com/intern/diff/D46172916/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/102508\nApproved by: https://github.com/awgu, https://github.com/fegin",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+import logging\n+import torch.distributed as dist\n+log = logging.getLogger(__name__)\n+    # Under TORCH_DISTRIBUTED_DEBUG=INFO, log the module names this hook fires for.\n+    # Below logging of module names this post-bwd hook fires for can help debug certain\n+    # cases where hooks don't fire, such as under certain activation checkpoint configs.\n+    if state._use_orig_params and handle._debug_level == dist.DebugLevel.INFO:\n+        param_to_fqn = state._exec_order_data.param_to_fqn\n+        handle_params = handle.flat_param._params  # only populated for use_orig_params\n+        param_fqns = [\n+            param\n+            for param_list in [param_to_fqn[p] for p in handle_params]\n+            for param in param_list\n+        ]\n+        log.warning(\"FSDP firing post-backward hooks for parameters %s\", param_fqns)\n+",
    "Label": "clean"
},
{
    "Id": 902,
    "Library": "pytorch",
    "Date": "2023/06/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a6a030a8eb708d0ced13fd2419636e1b24a8e668",
    "Root Cause": "N.A",
    "Bug report": "[data_loader] Enable overriding signal handler in DataLoader.cpp (#101816)\n\nSummary: Custom signal handlers (e.g. with more logging) can help in debugging crashes.\n\nTest Plan: builds\n\nReviewed By: drej82\n\nDifferential Revision: D45934625\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/101816\nApproved by: https://github.com/drej82",
    "Number of deleted lines": 4,
    "Deleted lines": "-static PyObject* THPModule_setWorkerSignalHandlers(\n-    PyObject* module,\n-    PyObject* arg) {\n-  HANDLE_TH_ERRORS",
    "Added lines": "+__attribute__((weak)) void setDataLoaderSignalHandlers() {\n+}\n+\n+static PyObject* THPModule_setWorkerSignalHandlers(\n+    PyObject* module,\n+    PyObject* arg) {\n+  HANDLE_TH_ERRORS\n+  setDataLoaderSignalHandlers();",
    "Label": "clean"
},
{
    "Id": 903,
    "Library": "pytorch",
    "Date": "2023/06/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/67792e175ccb8c58ca175e6d41b26c52ac06c3e8",
    "Root Cause": "N.A",
    "Bug report": "Add `-debug` suffix to trunk libtorch builds (#102764)\n\nCause that's what they are according to\nhttps://github.com/pytorch/pytorch/blob/30558c28966a31f9147d260176837f2804ab8d66/.ci/pytorch/build.sh#L307\n\n<!--\ncopilot:poem\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at 40cd88d</samp>\n\n> _`libtorch` debug_\n> _Build with symbols for Linux_\n> _Winter of errors_\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/102764\nApproved by: https://github.com/atalman",
    "Number of deleted lines": 2,
    "Deleted lines": "-  libtorch-linux-bionic-cuda11_8-py3_7-gcc9-build:\n-    name: libtorch-linux-bionic-cuda11.8-py3.7-gcc9",
    "Added lines": "+  libtorch-linux-bionic-cuda11_8-py3_7-gcc9-debug-build:\n+    name: libtorch-linux-bionic-cuda11.8-py3.7-gcc9-debug",
    "Label": "clean"
},
{
    "Id": 904,
    "Library": "pytorch",
    "Date": "2023/05/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5fa273c870516bbbfab620b7a5ac5048e36d6a4b",
    "Root Cause": "N.A",
    "Bug report": "ASAN: fix heap-buffer-overflow (#101970)\n\nPass size argument.\n\n<details>\n<summary>ASAN report</summary>\n\n```\n==1640574==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x609000022160 at pc 0x03ff31a04b42 bp 0x03ff69885dc0 sp 0x03ff69885db0\nREAD of size 16 at 0x609000022160 thread T1\n    #0 0x3ff31a04b41 in at::vec::ZVECTOR::Vectorized<unsigned char, void>::loadu(void const*, int) /home/user/pytorch/aten/src/ATen/cpu/vec/vec256/zarch/vec256_zarch.h:397\n    #1 0x3ff31a04b41 in at::vec::ZVECTOR::Vectorized<c10::quint8, void>::loadu(void const*, int) /home/user/pytorch/aten/src/ATen/cpu/vec/vec256/zarch/vec256_zarch.h:1574\n    #2 0x3ff31a04b41 in operator() /home/user/pytorch/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp:2668\n    #3 0x3ff31cefa5d in void at::internal::invoke_parallel<at::native::(anonymous namespace)::quantized_normalize_kernel(at::Tensor const&, at::Tensor const&, at::Tensor const&, bool, int, int, long, long\n, double, at::Tensor*)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(long, long)#1}>(long, long, long, at::native::(anonymous namespace)::quantized_normalize_kernel(at::Tens\nor const&, at::Tensor const&, at::Tensor const&, bool, int, int, long, long, double, at::Tensor*)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(long, long)#1} const&) [clone\n ._omp_fn.0] /home/user/pytorch/aten/src/ATen/ParallelOpenMP.h:42\n    #4 0x3ff6f31f52d in gomp_thread_start /var/tmp/portage/sys-devel/gcc-12.2.1_p20230304/work/gcc-12-20230304/libgomp/team.c:129\n    #5 0x3ff82218381 in start_thread /usr/src/debug/sys-libs/glibc-2.37-r1/glibc-2.37/nptl/pthread_create.c:444\n    #6 0x3ff822943f1  (/lib64/libc.so.6+0x1143f1)\n\n0x609000022160 is located 0 bytes to the right of 32-byte region [0x609000022140,0x609000022160)\nallocated by thread T0 here:\n    #0 0x3ff82a3663f in __interceptor_posix_memalign /usr/src/debug/sys-devel/gcc-11.3.1_p20230303/gcc-11-20230303/libsanitizer/asan/asan_malloc_linux.cpp:226\n    #1 0x3ff6f53ad95 in c10::alloc_cpu(unsigned long) /home/user/pytorch/c10/core/impl/alloc_cpu.cpp:74\n\nThread T1 created by T0 here:\n    #0 0x3ff829dc263 in __interceptor_pthread_create /usr/src/debug/sys-devel/gcc-11.3.1_p20230303/gcc-11-20230303/libsanitizer/asan/asan_interceptors.cpp:216\n    #1 0x3ff6f31fad5 in gomp_team_start /var/tmp/portage/sys-devel/gcc-12.2.1_p20230304/work/gcc-12-20230304/libgomp/team.c:858\n\nSUMMARY: AddressSanitizer: heap-buffer-overflow /home/user/pytorch/aten/src/ATen/cpu/vec/vec256/zarch/vec256_zarch.h:397 in at::vec::ZVECTOR::Vectorized<unsigned char, void>::loadu(void const*, int)\nShadow bytes around the buggy address:\n  0x100c12000043d0: 00 fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x100c12000043e0: fd fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x100c12000043f0: fd fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x100c1200004400: fd fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x100c1200004410: fa fa fa fa fa fa fa fa fd fa fa fa fa fa fa fa\n=>0x100c1200004420: fa fa fa fa fa fa fa fa 00 00 00 00[fa]fa fa fa\n  0x100c1200004430: fa fa fa fa fa fa fa fa fd fd fa fa fa fa fa fa\n  0x100c1200004440: fa fa fa fa fa fa fa fa fd fd fa fa fa fa fa fa\n  0x100c1200004450: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x100c1200004460: 00 00 fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x100c1200004470: 00 00 fa fa fa fa fa fa fa fa fa fa fa fa fa fa\nShadow byte legend (one shadow byte represents 8 application bytes):\n  Addressable:           00\n  Partially addressable: 01 02 03 04 05 06 07\n  Heap left redzone:       fa\n  Freed heap region:       fd\n  Stack left redzone:      f1\n  Stack mid redzone:       f2\n  Stack right redzone:     f3\n  Stack after return:      f5\n  Stack use after scope:   f8\n  Global redzone:          f9\n  Global init order:       f6\n  Poisoned by user:        f7\n  Container overflow:      fc\n  Array cookie:            ac\n  Intra object redzone:    bb\n  ASan internal:           fe\n  Left alloca redzone:     ca\n  Right alloca redzone:    cb\n  Shadow gap:              cc\n==1640574==ABORTING\n```\n</details>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/101970\nApproved by: https://github.com/Skylion007, https://github.com/jgong5",
    "Number of deleted lines": 2,
    "Deleted lines": "-    return Vectorized<T>{vinner_type::loadu(ptr)};\n-    _vec.store(ptr);",
    "Added lines": "+    return Vectorized<T>{vinner_type::loadu(ptr, count)};\n+    _vec.store(ptr, count);",
    "Label": "clean"
},
{
    "Id": 905,
    "Library": "pytorch",
    "Date": "2023/05/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bcaa93e80c03744e0c4235d6a0a88e6743f002cf",
    "Root Cause": "N.A",
    "Bug report": "s390x simd: disable functions with out-of-bounds reads (#102266)\n\n3 disabled functions are attempting out of bounds reads. Disable them until sleef library is fixed.\n\n<details>\n<summary>ASAN report</summary>\n\n```\n=================================================================\n==2030580==ERROR: AddressSanitizer: global-buffer-overflow on address 0x03ff70f54570 at pc 0x03ff6704e960 bp 0x03ffce128940 sp 0x03ffce128930\nREAD of size 4 at 0x03ff70f54570 thread T0\n    #0 0x3ff6704e95f in vgather_vf_p_vi2 /home/user/pytorch/third_party/sleef/src/arch/helpers390x_128.h:129\n    #1 0x3ff6704e95f in rempif /home/user/pytorch/third_party/sleef/src/libm/sleefsimdsp.c:550\n    #2 0x3ff6704e95f in Sleef_cosf4_u10vxe2 /home/user/pytorch/third_party/sleef/src/libm/sleefsimdsp.c:1021\n    #3 0x3ff67029cfb in Sleef_cosf4_u10 /home/user/pytorch/build/sleef/src/libm/disps390x_128.c:182\n    #4 0x3ff55d21941 in at::vec::ZVECTOR::Vectorized<float, void> at::vec::ZVECTOR::Vectorized<float, void>::mapSleef<float __vector(4) const (*)(float __vector(4)), double __vector(2) const (*)(double __\nvector(2)), float, 0>(float __vector(4) const (*)(float __vector(4)), double __vector(2) const (*)(double __vector(2))) const /home/user/pytorch/aten/src/ATen/cpu/vec/vec256/zarch/vec256_zarch.h:991\n    #5 0x3ff5689ad01 in at::vec::ZVECTOR::Vectorized<float, void>::cos() const /home/user/pytorch/aten/src/ATen/cpu/vec/vec256/zarch/vec256_zarch.h:1074\n    #6 0x3ff5685df97 in at::vml::ZVECTOR::vcos<float>(float*, float const*, long)::{lambda(at::vec::ZVECTOR::Vectorized<float, void>)#1}::operator()(at::vec::ZVECTOR::Vectorized<float, void>) const /home/\nuser/pytorch/aten/src/ATen/cpu/vml.h:71\n    #7 0x3ff5689b691 in void at::vec::map<float, at::vml::ZVECTOR::vcos<float>(float*, float const*, long)::{lambda(at::vec::ZVECTOR::Vectorized<float, void>)#1}, 0>(at::vml::ZVECTOR::vcos<float>(float*,\nfloat const*, long)::{lambda(at::vec::ZVECTOR::Vectorized<float, void>)#1} const&, float*, float const*, long) /home/user/pytorch/aten/src/ATen/cpu/vec/functional_base.h:239\n    #8 0x3ff5685e0df in void at::vml::ZVECTOR::vcos<float>(float*, float const*, long) /home/user/pytorch/aten/src/ATen/cpu/vml.h:71\n    #9 0x3ff563fdde3 in operator() /home/user/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:770\n    #10 0x3ff5648e4a3 in operator() /home/user/pytorch/aten/src/ATen/TensorIterator.h:406\n    #11 0x3ff5663cae1 in callback_fn<at::TensorIteratorBase::loop_2d_from_1d<at::native::ZVECTOR::cos_kernel(at::TensorIteratorBase&)::<lambda()>::<lambda()>::<lambda(char**, const int64_t*, int64_t)> >(c\nonst at::native::ZVECTOR::cos_kernel(at::TensorIteratorBase&)::<lambda()>::<lambda()>::<lambda(char**, const int64_t*, int64_t)>&)::<lambda(char**, const int64_t*, int64_t, int64_t)> > /home/user/pytorch/\nc10/util/FunctionRef.h:43\n    #12 0x3ff4d45a933 in c10::function_ref<void (char**, long const*, long, long)>::operator()(char**, long const*, long, long) const /home/user/pytorch/c10/util/FunctionRef.h:64\n    #13 0x3ff4d455133 in at::internal::serial_for_each(c10::ArrayRef<long>, c10::ArrayRef<long>, char**, unsigned long, c10::function_ref<void (char**, long const*, long, long)>, at::Range) /home/user/pyt\norch/aten/src/ATen/TensorIteratorInternal.h:52\n    #14 0x3ff4d43b703 in at::TensorIteratorBase::serial_for_each(c10::function_ref<void (char**, long const*, long, long)>, at::Range) const /home/user/pytorch/aten/src/ATen/TensorIterator.cpp:777\n    #15 0x3ff4d43ab59 in at::TensorIteratorBase::for_each(c10::function_ref<void (char**, long const*, long, long)>, long) /home/user/pytorch/aten/src/ATen/TensorIterator.cpp:749\n    #16 0x3ff5648e851 in for_each<at::native::ZVECTOR::cos_kernel(at::TensorIteratorBase&)::<lambda()>::<lambda()>::<lambda(char**, const int64_t*, int64_t)> > /home/user/pytorch/aten/src/ATen/TensorItera\ntor.h:421\n    #17 0x3ff563fe5f9 in operator() /home/user/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:770\n    #18 0x3ff56400915 in operator() /home/user/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:770\n    #19 0x3ff56400f1d in at::native::ZVECTOR::cos_kernel(at::TensorIteratorBase&) /home/user/pytorch/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp:770\n    #20 0x3ff4f303007 in void at::native::DispatchStub<void (*)(at::TensorIteratorBase&), at::native::cos_stub>::operator()<at::native::structured_cos_out&>(c10::DeviceType, at::native::structured_cos_out\n&) /home/user/pytorch/aten/src/ATen/native/DispatchStub.h:158\n    #21 0x3ff4f2edb3f in at::native::structured_cos_out::impl(at::Tensor const&, at::Tensor const&) /home/user/pytorch/aten/src/ATen/native/UnaryOps.cpp:330\n    #22 0x3ff526ef739 in wrapper_CPU_cos /home/user/pytorch/build/aten/src/ATen/RegisterCPU.cpp:4307\n    #23 0x3ff52c651d9 in operator() /home/user/pytorch/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13\n    #24 0x3ff52c651d9 in call /home/user/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:463\n    #25 0x3ff5076df2f in at::Tensor c10::callUnboxedKernelFunction<at::Tensor, at::Tensor const&>(void*, c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&) /home/user/pytorch/aten/src/ATen/core\n/boxing/KernelFunction_impl.h:50\n    #26 0x3ff5009a93f in at::Tensor c10::KernelFunction::call<at::Tensor, at::Tensor const&>(c10::OperatorHandle const&, c10::DispatchKeySet, at::Tensor const&) const /home/user/pytorch/aten/src/ATen/core\n/boxing/KernelFunction_impl.h:103\n    #27 0x3ff5009a93f in at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&)> const&, at::Tensor const&) const /home/user/pytorch/aten/s\nrc/ATen/core/dispatch/Dispatcher.h:639\n    #28 0x3ff5009a93f in c10::TypedOperatorHandle<at::Tensor (at::Tensor const&)>::call(at::Tensor const&) const /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:487\n    #29 0x3ff5009a93f in at::_ops::cos::call(at::Tensor const&) /home/user/pytorch/build/aten/src/ATen/Operators_0.cpp:2215\n    #30 0x3ff7d813741 in at::Tensor::cos() const /home/user/pytorch/build/aten/src/ATen/core/TensorBody.h:2107\n    #31 0x3ff7dc0f2b7 in operator() /home/user/pytorch/torch/csrc/autograd/generated/python_torch_functions_2.cpp:2953\n    #32 0x3ff7dc0faf7 in THPVariable_cos /home/user/pytorch/torch/csrc/autograd/generated/python_torch_functions_2.cpp:2955\n    #33 0x3ffa5ef5ae1 in cfunction_call Objects/methodobject.c:543\n    #34 0x3ffa5e843f3 in _PyObject_Call Objects/call.c:305\n    #35 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #36 0x3ffa5feb50d in do_call_core Python/ceval.c:5915\n    #37 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #38 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #39 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #40 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #41 0x3ffa5e841fb in PyVectorcall_Call Objects/call.c:255\n    #42 0x3ffa5e84347 in _PyObject_Call Objects/call.c:290\n    #43 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #44 0x3ff7f87a393 in torch::impl::dispatch::PythonKernelHolder::operator()(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) /home/user/pytorch/\ntorch/csrc/utils/python_dispatch.cpp:175\n    #45 0x3ff7f8871a7 in c10::BoxedKernel::makeFromFunctor<torch::impl::dispatch::PythonKernelHolder>(std::unique_ptr<torch::impl::dispatch::PythonKernelHolder, std::default_delete<torch::impl::dispatch::\nPythonKernelHolder> >)::{lambda(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)#1}::operator()(c10::OperatorKernel*, c10::Op\neratorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const /home/user/pytorch/aten/src/ATen/core/boxing/BoxedKernel_impl.h:87\n    #46 0x3ff7f887261 in c10::BoxedKernel::makeFromFunctor<torch::impl::dispatch::PythonKernelHolder>(std::unique_ptr<torch::impl::dispatch::PythonKernelHolder, std::default_delete<torch::impl::dispatch::\nPythonKernelHolder> >)::{lambda(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)#1}::_FUN(c10::OperatorKernel*, c10::Operator\nHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) /home/user/pytorch/aten/src/ATen/core/boxing/BoxedKernel_impl.h:86\n    #47 0x3ff7e0d10ab in c10::BoxedKernel::callBoxed(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const /home/user/pytorch/aten/src/ATen/core/b\noxing/BoxedKernel_impl.h:41\n    #48 0x3ff7e0d1459 in c10::KernelFunction::callBoxed(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const /home/user/pytorch/aten/src/ATen/cor\ne/boxing/KernelFunction_impl.h:43\n    #49 0x3ff7f876421 in c10::Dispatcher::callBoxed(c10::OperatorHandle const&, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:6\n91\n    #50 0x3ff4d22bcdd in c10::OperatorHandle::callBoxed(std::vector<c10::IValue, std::allocator<c10::IValue> >*) const /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:417\n    #51 0x3ff65a092d5 in c10::OperatorHandle::callBoxed(std::vector<c10::IValue, std::allocator<c10::IValue> >&) const /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:421\n    #52 0x3ff65a05641 in operator() /home/user/pytorch/torch/csrc/jit/runtime/register_c10_ops.cpp:15\n    #53 0x3ff65a08cb5 in __invoke_impl<void, torch::jit::(anonymous namespace)::createOperatorFromC10(const c10::OperatorHandle&)::<lambda(torch::jit::Stack&)>&, std::vector<c10::IValue, std::allocator<c1\n0::IValue> >&> /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/invoke.h:61\n    #54 0x3ff65a0897b in __invoke_r<void, torch::jit::(anonymous namespace)::createOperatorFromC10(const c10::OperatorHandle&)::<lambda(torch::jit::Stack&)>&, std::vector<c10::IValue, std::allocator<c10::\nIValue> >&> /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/invoke.h:111\n    #55 0x3ff65a084e1 in _M_invoke /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/std_function.h:290\n    #56 0x3ff7eb2cb21 in std::function<void (std::vector<c10::IValue, std::allocator<c10::IValue> >&)>::operator()(std::vector<c10::IValue, std::allocator<c10::IValue> >&) const /usr/lib/gcc/s390x-ibm-lin\nux-gnu/11/include/g++-v11/bits/std_function.h:590\n    #57 0x3ff7eb1b659 in torch::jit::Operation::operator()(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /home/user/pytorch/aten/src/ATen/core/stack.h:41\n    #58 0x3ff7eb08449 in torch::jit::invokeOperatorFromPython(std::vector<std::shared_ptr<torch::jit::Operator>, std::allocator<std::shared_ptr<torch::jit::Operator> > > const&, pybind11::args, pybind11::\nkwargs const&, c10::optional<c10::DispatchKey>) /home/user/pytorch/torch/csrc/jit/python/pybind_utils.cpp:764\n    #59 0x3ff7eb09d85 in torch::jit::_get_operation_for_overload_or_packet(std::vector<std::shared_ptr<torch::jit::Operator>, std::allocator<std::shared_ptr<torch::jit::Operator> > > const&, c10::Symbol,\npybind11::args, pybind11::kwargs const&, bool, c10::optional<c10::DispatchKey>) /home/user/pytorch/torch/csrc/jit/python/pybind_utils.cpp:829\n    #60 0x3ff7e573eb9 in operator() /home/user/pytorch/torch/csrc/jit/python/init.cpp:1549\n    #61 0x3ff7e6728dd in call_impl<pybind11::object, torch::jit::initJITBindings(PyObject*)::<lambda(const string&, const string&)>::<lambda(pybind11::args, pybind11::kwargs)>&, 0, 1, pybind11::detail::vo\nid_type> /home/user/pytorch/third_party/pybind11/include/pybind11/cast.h:1439\n    #62 0x3ff7e64312f in call<pybind11::object, pybind11::detail::void_type, torch::jit::initJITBindings(PyObject*)::<lambda(const string&, const string&)>::<lambda(pybind11::args, pybind11::kwargs)>&> /h\nome/user/pytorch/third_party/pybind11/include/pybind11/cast.h:1408\n    #63 0x3ff7e5da259 in operator() /home/user/pytorch/third_party/pybind11/include/pybind11/pybind11.h:249\n    #64 0x3ff7e5da441 in _FUN /home/user/pytorch/third_party/pybind11/include/pybind11/pybind11.h:224\n    #65 0x3ff7d317a1f in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) /home/user/pytorch/third_party/pybind11/include/pybind11/pybind11.h:929\n    #66 0x3ffa5ef5ae1 in cfunction_call Objects/methodobject.c:543\n    #67 0x3ffa5e843f3 in _PyObject_Call Objects/call.c:305\n    #68 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #69 0x3ffa5feb50d in do_call_core Python/ceval.c:5915\n    #70 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #71 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #72 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #73 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #74 0x3ffa5e83d1f in _PyObject_FastCallDictTstate Objects/call.c:142\n    #75 0x3ffa5e84937 in _PyObject_Call_Prepend Objects/call.c:431\n    #76 0x3ffa5f2f577 in slot_tp_call Objects/typeobject.c:7494\n    #77 0x3ffa5e843f3 in _PyObject_Call Objects/call.c:305\n    #78 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #79 0x3ffa5feb7cf in do_call_core Python/ceval.c:5943\n    #80 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #81 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #82 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #83 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #84 0x3ffa5fd76a3 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #85 0x3ffa5fd772f in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #86 0x3ffa5feb289 in call_function Python/ceval.c:5891\n    #87 0x3ffa5fe5c3b in _PyEval_EvalFrameDefault Python/ceval.c:4213\n    #88 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #89 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #90 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #91 0x3ffa5e841fb in PyVectorcall_Call Objects/call.c:255\n    #92 0x3ffa5e84347 in _PyObject_Call Objects/call.c:290\n    #93 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #94 0x3ffa5feb7cf in do_call_core Python/ceval.c:5943\n    #95 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #96 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #97 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #98 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #99 0x3ffa5e841fb in PyVectorcall_Call Objects/call.c:255\n    #100 0x3ffa5e84347 in _PyObject_Call Objects/call.c:290\n    #101 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #102 0x3ff7f87a393 in torch::impl::dispatch::PythonKernelHolder::operator()(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) /home/user/pytorch\n/torch/csrc/utils/python_dispatch.cpp:175\n    #103 0x3ff7f8871a7 in c10::BoxedKernel::makeFromFunctor<torch::impl::dispatch::PythonKernelHolder>(std::unique_ptr<torch::impl::dispatch::PythonKernelHolder, std::default_delete<torch::impl::dispatch:\n:PythonKernelHolder> >)::{lambda(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)#1}::operator()(c10::OperatorKernel*, c10::O\nperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const /home/user/pytorch/aten/src/ATen/core/boxing/BoxedKernel_impl.h:87\n    #104 0x3ff7f887261 in c10::BoxedKernel::makeFromFunctor<torch::impl::dispatch::PythonKernelHolder>(std::unique_ptr<torch::impl::dispatch::PythonKernelHolder, std::default_delete<torch::impl::dispatch:\n:PythonKernelHolder> >)::{lambda(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*)#1}::_FUN(c10::OperatorKernel*, c10::Operato\nrHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) /home/user/pytorch/aten/src/ATen/core/boxing/BoxedKernel_impl.h:86\n    #105 0x3ff7e0d10ab in c10::BoxedKernel::callBoxed(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const /home/user/pytorch/aten/src/ATen/core/\nboxing/BoxedKernel_impl.h:41\n    #106 0x3ff7e0d1459 in c10::KernelFunction::callBoxed(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const /home/user/pytorch/aten/src/ATen/co\nre/boxing/KernelFunction_impl.h:43\n    #107 0x3ff7f876421 in c10::Dispatcher::callBoxed(c10::OperatorHandle const&, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:\n691\n    #108 0x3ff4d22bcdd in c10::OperatorHandle::callBoxed(std::vector<c10::IValue, std::allocator<c10::IValue> >*) const /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:417\n    #109 0x3ff65a092d5 in c10::OperatorHandle::callBoxed(std::vector<c10::IValue, std::allocator<c10::IValue> >&) const /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:421\n    #110 0x3ff65a05641 in operator() /home/user/pytorch/torch/csrc/jit/runtime/register_c10_ops.cpp:15\n    #111 0x3ff65a08cb5 in __invoke_impl<void, torch::jit::(anonymous namespace)::createOperatorFromC10(const c10::OperatorHandle&)::<lambda(torch::jit::Stack&)>&, std::vector<c10::IValue, std::allocator<c\n10::IValue> >&> /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/invoke.h:61\n    #112 0x3ff65a0897b in __invoke_r<void, torch::jit::(anonymous namespace)::createOperatorFromC10(const c10::OperatorHandle&)::<lambda(torch::jit::Stack&)>&, std::vector<c10::IValue, std::allocator<c10:\n:IValue> >&> /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/invoke.h:111\n    #113 0x3ff65a084e1 in _M_invoke /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/std_function.h:290\n    #114 0x3ff7eb2cb21 in std::function<void (std::vector<c10::IValue, std::allocator<c10::IValue> >&)>::operator()(std::vector<c10::IValue, std::allocator<c10::IValue> >&) const /usr/lib/gcc/s390x-ibm-li\nnux-gnu/11/include/g++-v11/bits/std_function.h:590\n    #115 0x3ff7eb1b659 in torch::jit::Operation::operator()(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /home/user/pytorch/aten/src/ATen/core/stack.h:41\n    #116 0x3ff7eb08449 in torch::jit::invokeOperatorFromPython(std::vector<std::shared_ptr<torch::jit::Operator>, std::allocator<std::shared_ptr<torch::jit::Operator> > > const&, pybind11::args, pybind11:\n:kwargs const&, c10::optional<c10::DispatchKey>) /home/user/pytorch/torch/csrc/jit/python/pybind_utils.cpp:764\n    #117 0x3ff7eb09d85 in torch::jit::_get_operation_for_overload_or_packet(std::vector<std::shared_ptr<torch::jit::Operator>, std::allocator<std::shared_ptr<torch::jit::Operator> > > const&, c10::Symbol,\n pybind11::args, pybind11::kwargs const&, bool, c10::optional<c10::DispatchKey>) /home/user/pytorch/torch/csrc/jit/python/pybind_utils.cpp:829\n    #118 0x3ff7e573eb9 in operator() /home/user/pytorch/torch/csrc/jit/python/init.cpp:1549\n    #119 0x3ff7e6728dd in call_impl<pybind11::object, torch::jit::initJITBindings(PyObject*)::<lambda(const string&, const string&)>::<lambda(pybind11::args, pybind11::kwargs)>&, 0, 1, pybind11::detail::v\noid_type> /home/user/pytorch/third_party/pybind11/include/pybind11/cast.h:1439\n    #120 0x3ff7e64312f in call<pybind11::object, pybind11::detail::void_type, torch::jit::initJITBindings(PyObject*)::<lambda(const string&, const string&)>::<lambda(pybind11::args, pybind11::kwargs)>&> /\nhome/user/pytorch/third_party/pybind11/include/pybind11/cast.h:1408\n    #121 0x3ff7e5da259 in operator() /home/user/pytorch/third_party/pybind11/include/pybind11/pybind11.h:249\n    #122 0x3ff7e5da441 in _FUN /home/user/pytorch/third_party/pybind11/include/pybind11/pybind11.h:224\n    #123 0x3ff7d317a1f in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) /home/user/pytorch/third_party/pybind11/include/pybind11/pybind11.h:929\n    #124 0x3ffa5ef5ae1 in cfunction_call Objects/methodobject.c:543\n    #125 0x3ffa5e843f3 in _PyObject_Call Objects/call.c:305\n    #126 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #127 0x3ffa5feb50d in do_call_core Python/ceval.c:5915\n    #128 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #129 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #130 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #131 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #132 0x3ffa5e83d1f in _PyObject_FastCallDictTstate Objects/call.c:142\n    #133 0x3ffa5e84937 in _PyObject_Call_Prepend Objects/call.c:431\n    #134 0x3ffa5f2f577 in slot_tp_call Objects/typeobject.c:7494\n    #135 0x3ffa5e843f3 in _PyObject_Call Objects/call.c:305\n    #136 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #137 0x3ffa5feb7cf in do_call_core Python/ceval.c:5943\n    #138 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #139 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #140 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #141 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #142 0x3ffa5e87d2b in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #143 0x3ffa5e882dd in method_vectorcall Objects/classobject.c:83\n    #144 0x3ffa5e836d3 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #145 0x3ffa5e84b6f in _PyObject_CallFunctionVa Objects/call.c:485\n    #146 0x3ffa5e84f2d in callmethod Objects/call.c:557\n    #147 0x3ffa5e85039 in PyObject_CallMethod Objects/call.c:577\n    #148 0x3ff7f7efa05 in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<pybind11::handle>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) /home/user/py\ntorch/torch/csrc/utils/python_arg_parser.cpp:338\n    #149 0x3ff7eb09b67 in torch::jit::_get_operation_for_overload_or_packet(std::vector<std::shared_ptr<torch::jit::Operator>, std::allocator<std::shared_ptr<torch::jit::Operator> > > const&, c10::Symbol,\n pybind11::args, pybind11::kwargs const&, bool, c10::optional<c10::DispatchKey>) /home/user/pytorch/torch/csrc/jit/python/pybind_utils.cpp:827\n    #150 0x3ff7e573eb9 in operator() /home/user/pytorch/torch/csrc/jit/python/init.cpp:1549\n    #151 0x3ff7e6728dd in call_impl<pybind11::object, torch::jit::initJITBindings(PyObject*)::<lambda(const string&, const string&)>::<lambda(pybind11::args, pybind11::kwargs)>&, 0, 1, pybind11::detail::v\noid_type> /home/user/pytorch/third_party/pybind11/include/pybind11/cast.h:1439\n    #152 0x3ff7e64312f in call<pybind11::object, pybind11::detail::void_type, torch::jit::initJITBindings(PyObject*)::<lambda(const string&, const string&)>::<lambda(pybind11::args, pybind11::kwargs)>&> /\nhome/user/pytorch/third_party/pybind11/include/pybind11/cast.h:1408\n    #153 0x3ff7e5da259 in operator() /home/user/pytorch/third_party/pybind11/include/pybind11/pybind11.h:249\n    #154 0x3ff7e5da441 in _FUN /home/user/pytorch/third_party/pybind11/include/pybind11/pybind11.h:224\n    #155 0x3ff7d317a1f in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) /home/user/pytorch/third_party/pybind11/include/pybind11/pybind11.h:929\n    #156 0x3ffa5ef5ae1 in cfunction_call Objects/methodobject.c:543\n    #157 0x3ffa5e843f3 in _PyObject_Call Objects/call.c:305\n    #158 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #159 0x3ffa5feb50d in do_call_core Python/ceval.c:5915\n    #160 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #161 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #162 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #163 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #164 0x3ffa5e83d1f in _PyObject_FastCallDictTstate Objects/call.c:142\n    #165 0x3ffa5e84937 in _PyObject_Call_Prepend Objects/call.c:431\n    #166 0x3ffa5f2f577 in slot_tp_call Objects/typeobject.c:7494\n    #167 0x3ffa5e84027 in _PyObject_MakeTpCall Objects/call.c:215\n    #168 0x3ffa5fd767b in _PyObject_VectorcallTstate Include/cpython/abstract.h:112\n    #169 0x3ffa5fd772f in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #170 0x3ffa5feb289 in call_function Python/ceval.c:5891\n    #171 0x3ffa5fe5ad1 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #172 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #173 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #174 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #175 0x3ffa5fd76a3 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #176 0x3ffa5fd772f in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #177 0x3ffa5feb289 in call_function Python/ceval.c:5891\n    #178 0x3ffa5fe5c3b in _PyEval_EvalFrameDefault Python/ceval.c:4213\n    #179 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #180 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #181 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #182 0x3ffa5e8427f in PyVectorcall_Call Objects/call.c:267\n    #183 0x3ffa5e84347 in _PyObject_Call Objects/call.c:290\n    #184 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #185 0x3ffa5feb7cf in do_call_core Python/ceval.c:5943\n    #186 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #187 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #188 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #189 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #190 0x3ffa5e841fb in PyVectorcall_Call Objects/call.c:255\n    #191 0x3ffa5e84347 in _PyObject_Call Objects/call.c:290\n    #192 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #193 0x3ffa5feb7cf in do_call_core Python/ceval.c:5943\n    #194 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #195 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #196 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #197 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #198 0x3ffa5e841fb in PyVectorcall_Call Objects/call.c:255\n    #199 0x3ffa5e84347 in _PyObject_Call Objects/call.c:290\n    #200 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #201 0x3ffa5feb7cf in do_call_core Python/ceval.c:5943\n    #202 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #203 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #204 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #205 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #206 0x3ffa5e841fb in PyVectorcall_Call Objects/call.c:255\n    #207 0x3ffa5e84347 in _PyObject_Call Objects/call.c:290\n    #208 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #209 0x3ffa5feb7cf in do_call_core Python/ceval.c:5943\n    #210 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #211 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #212 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #213 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #214 0x3ffa5e83d1f in _PyObject_FastCallDictTstate Objects/call.c:142\n    #215 0x3ffa5e84937 in _PyObject_Call_Prepend Objects/call.c:431\n    #216 0x3ffa5f2f577 in slot_tp_call Objects/typeobject.c:7494\n    #217 0x3ffa5e843f3 in _PyObject_Call Objects/call.c:305\n    #218 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #219 0x3ffa5feb7cf in do_call_core Python/ceval.c:5943\n    #220 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #221 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #222 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #223 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #224 0x3ffa5fd76a3 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #225 0x3ffa5fd772f in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #226 0x3ffa5feb289 in call_function Python/ceval.c:5891\n    #227 0x3ffa5fe5b21 in _PyEval_EvalFrameDefault Python/ceval.c:4198\n    #228 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #229 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #230 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #231 0x3ffa5e8427f in PyVectorcall_Call Objects/call.c:267\n    #232 0x3ffa5e84347 in _PyObject_Call Objects/call.c:290\n    #233 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #234 0x3ffa5feb7cf in do_call_core Python/ceval.c:5943\n    #235 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #236 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #237 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #238 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #239 0x3ffa5e8427f in PyVectorcall_Call Objects/call.c:267\n    #240 0x3ffa5e84347 in _PyObject_Call Objects/call.c:290\n    #241 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #242 0x3ffa5feb7cf in do_call_core Python/ceval.c:5943\n    #243 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #244 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #245 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #246 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #247 0x3ffa5e8427f in PyVectorcall_Call Objects/call.c:267\n    #248 0x3ffa5e84347 in _PyObject_Call Objects/call.c:290\n    #249 0x3ffa5e84483 in PyObject_Call Objects/call.c:317\n    #250 0x3ffa5feb7cf in do_call_core Python/ceval.c:5943\n    #251 0x3ffa5fe6019 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #252 0x3ffa5fd7aed in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #253 0x3ffa5fe8ba9 in _PyEval_Vector Python/ceval.c:5065\n    #254 0x3ffa5e8459b in _PyFunction_Vectorcall Objects/call.c:342\n    #255 0x3ffa5e8427f in PyVectorcall_Call Objects/call.c:267\n\n0x03ff70f54570 is located 0 bytes to the right of global variable 'Sleef_rempitabsp' defined in '/home/user/pytorch/third_party/sleef/src/libm/rempitab.c:986:34' (0x3ff70f53f00) of size 1648\nSUMMARY: AddressSanitizer: global-buffer-overflow /home/user/pytorch/third_party/sleef/src/arch/helpers390x_128.h:129 in vgather_vf_p_vi2\nShadow bytes around the buggy address:\n  0x10007fee1ea850: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x10007fee1ea860: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x10007fee1ea870: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x10007fee1ea880: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x10007fee1ea890: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n=>0x10007fee1ea8a0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00[f9]f9\n  0x10007fee1ea8b0: f9 f9 f9 f9 00 00 00 00 00 00 00 00 00 00 00 00\n  0x10007fee1ea8c0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x10007fee1ea8d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x10007fee1ea8e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x10007fee1ea8f0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\nShadow byte legend (one shadow byte represents 8 application bytes):\n  Addressable:           00\n  Partially addressable: 01 02 03 04 05 06 07\n  Heap left redzone:       fa\n  Freed heap region:       fd\n  Stack left redzone:      f1\n  Stack mid redzone:       f2\n  Stack right redzone:     f3\n  Stack after return:      f5\n  Stack use after scope:   f8\n  Global redzone:          f9\n  Global init order:       f6\n  Poisoned by user:        f7\n  Container overflow:      fc\n  Array cookie:            ac\n  Intra object redzone:    bb\n  ASan internal:           fe\n  Left alloca redzone:     ca\n  Right alloca redzone:    cb\n  Shadow gap:              cc\n==2030580==ABORTING\n```\n</details>\n\nIt reproduces when running `pytest -v test/test_ops.py -k test_python_ref__refs_cos_cpu_bfloat16` under address sanitizer on s390x.\n\nSee also: https://github.com/shibatch/sleef/issues/464\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/102266\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#define SLEEF_MEMORY_WORKAROUND\n+\n+#ifndef SLEEF_MEMORY_WORKAROUND\n+#else\n+    return mapOrdinary(std::sin);\n+#endif\n+#ifndef SLEEF_MEMORY_WORKAROUND\n+#else\n+    return mapOrdinary(std::cos);\n+#endif\n+#ifndef SLEEF_MEMORY_WORKAROUND\n+#else\n+    return mapOrdinary(std::tan);\n+#endif",
    "Label": "clean"
},
{
    "Id": 906,
    "Library": "pytorch",
    "Date": "2023/05/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/124d812f38e5625bedc828a6b462a6295aeecdc9",
    "Root Cause": "N.A",
    "Bug report": "[BE] Fix rule not found error message (#101745)\n\nPrevent error message from becoming of single column of characters\n\nThanks @clee200 for explaining how it worked before\n\n<!--\ncopilot:poem\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at fef1e25</samp>\n\n> _`reject_reason` fixed_\n> _Syntax error caused trouble_\n> _Autumn of bugs ends_\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/101745\nApproved by: https://github.com/kit1980, https://github.com/osalpekar",
    "Number of deleted lines": 3,
    "Deleted lines": "-                        f\"Not all files match rule `{rule_name}`.\"\n-                        f\"{num_matching_files} files matched, but there are still non-matching files:\"\n-                        f\"{','.join(non_matching_files[:5])}{', ...' if len(non_matching_files) > 5 else ''}\"",
    "Added lines": "+                        f\"Not all files match rule `{rule_name}`.\",\n+                        f\"{num_matching_files} files matched, but there are still non-matching files:\",\n+                        f\"{','.join(non_matching_files[:5])}{', ...' if len(non_matching_files) > 5 else ''}\",",
    "Label": "clean"
},
{
    "Id": 907,
    "Library": "pytorch",
    "Date": "2023/05/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/effe1425dd698dda2497e9ce25f5c1655b33ecc4",
    "Root Cause": "N.A",
    "Bug report": "ASAN: fix use-after-free (#101400)\n\narguments() returns vector member of object returned by schema() call.\nWhen object returned by schema() call is destroyed, the vector is deallocated as well,\nit's lifetime isn't extended.\n\nThis issue detected while running `pytest -v test/mobile/test_lite_script_type.py -k test_nest_typing_namedtuple_custom_classtype` with ASAN.\n\n<details>\n<summary>ASAN output</summary>\n\n```\n==1134126==ERROR: AddressSanitizer: heap-use-after-free on address 0x60d0005a5790 at pc 0x03ff844488d8 bp 0x03fff584afe8 sp 0x03fff584afd8\nREAD of size 8 at 0x60d0005a5790 thread T0\n    #0 0x3ff844488d7 in __gnu_cxx::__normal_iterator<c10::Argument const*, std::vector<c10::Argument, std::allocator<c10::Argument> > >::__normal_iterator(c10::Argument const* const&) /usr/lib/gcc/s390x-i\nbm-linux-gnu/11/include/g++-v11/bits/stl_iterator.h:1028\n    #1 0x3ff8444293f in std::vector<c10::Argument, std::allocator<c10::Argument> >::begin() const /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/stl_vector.h:821\n    #2 0x3ff84d807d1 in torch::jit::toPyObject(c10::IValue) /home/user/pytorch/torch/csrc/jit/python/pybind_utils.cpp:617\n    #3 0x3ff84d80305 in torch::jit::toPyObject(c10::IValue) /home/user/pytorch/torch/csrc/jit/python/pybind_utils.cpp:604\n    #4 0x3ff84856871 in pybind11::detail::type_caster<c10::IValue, void>::cast(c10::IValue, pybind11::return_value_policy, pybind11::handle) /home/user/pytorch/torch/csrc/jit/python/pybind.h:138\n    #5 0x3ff85318191 in pybind11::cpp_function::initialize<torch::jit::initJitScriptBindings(_object*)::$_45, c10::IValue, torch::jit::mobile::Module&, pybind11::tuple const&, pybind11::name, pybind11::is\n_method, pybind11::sibling, pybind11::arg>(torch::jit::initJitScriptBindings(_object*)::$_45&&, c10::IValue (*)(torch::jit::mobile::Module&, pybind11::tuple const&), pybind11::name const&, pybind11::is_me\nthod const&, pybind11::sibling const&, pybind11::arg const&)::{lambda(pybind11::detail::function_call&)#1}::operator()(pybind11::detail::function_call&) const /home/user/pytorch/cmake/../third_party/pybin\nd11/include/pybind11/pybind11.h:249\n    #6 0x3ff85317cfd in pybind11::cpp_function::initialize<torch::jit::initJitScriptBindings(_object*)::$_45, c10::IValue, torch::jit::mobile::Module&, pybind11::tuple const&, pybind11::name, pybind11::is\n_method, pybind11::sibling, pybind11::arg>(torch::jit::initJitScriptBindings(_object*)::$_45&&, c10::IValue (*)(torch::jit::mobile::Module&, pybind11::tuple const&), pybind11::name const&, pybind11::is_me\nthod const&, pybind11::sibling const&, pybind11::arg const&)::{lambda(pybind11::detail::function_call&)#1}::__invoke(pybind11::detail::function_call&) /home/user/pytorch/cmake/../third_party/pybind11/incl\nude/pybind11/pybind11.h:224\n    #7 0x3ff82ee52e9 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) /home/user/pytorch/cmake/../third_party/pybind11/include/pybind11/pybind11.h:929\n    #8 0x3ffab002903 in cfunction_call Objects/methodobject.c:543\n    #9 0x3ffaaf8a933 in _PyObject_MakeTpCall Objects/call.c:215\n    #10 0x3ffaaf8e919 in _PyObject_VectorcallTstate Include/cpython/abstract.h:112\n    #11 0x3ffaaf8eddd in method_vectorcall Objects/classobject.c:53\n    #12 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #13 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #14 0x3ffab105447 in call_function Python/ceval.c:5891\n    #15 0x3ffab0ff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #16 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #17 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #18 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #19 0x3ffaaf8a615 in _PyObject_FastCallDictTstate Objects/call.c:142\n    #20 0x3ffaaf8b271 in _PyObject_Call_Prepend Objects/call.c:431\n    #21 0x3ffab03f307 in slot_tp_call Objects/typeobject.c:7494\n    #22 0x3ffaaf8a933 in _PyObject_MakeTpCall Objects/call.c:215\n    #23 0x3ffab0f0081 in _PyObject_VectorcallTstate Include/cpython/abstract.h:112\n    #24 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #25 0x3ffab105447 in call_function Python/ceval.c:5891\n    #26 0x3ffab0ff905 in _PyEval_EvalFrameDefault Python/ceval.c:4213\n    #27 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #28 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #29 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #30 0x3ffaaf8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #31 0x3ffaaf8eddd in method_vectorcall Objects/classobject.c:53\n    #32 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #33 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #34 0x3ffab105447 in call_function Python/ceval.c:5891\n    #35 0x3ffab0ff905 in _PyEval_EvalFrameDefault Python/ceval.c:4213\n    #36 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #37 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #38 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #39 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #40 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #41 0x3ffab105447 in call_function Python/ceval.c:5891\n    #42 0x3ffab0ff7d7 in _PyEval_EvalFrameDefault Python/ceval.c:4198\n    #43 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #44 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #45 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #46 0x3ffaaf8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #47 0x3ffaaf8eddd in method_vectorcall Objects/classobject.c:53\n    #48 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #49 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #50 0x3ffab105447 in call_function Python/ceval.c:5891\n    #51 0x3ffab0ffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #52 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #53 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #54 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #55 0x3ffaaf8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #56 0x3ffaaf8eddd in method_vectorcall Objects/classobject.c:53\n    #57 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #58 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #59 0x3ffab105447 in call_function Python/ceval.c:5891\n    #60 0x3ffab0ffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #61 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #62 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #63 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #64 0x3ffaaf8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #65 0x3ffaaf8eddd in method_vectorcall Objects/classobject.c:53\n    #66 0x3ffaaf8ab9b in PyVectorcall_Call Objects/call.c:267\n    #67 0x3ffaaf8ac65 in _PyObject_Call Objects/call.c:290\n    #68 0x3ffaaf8ada9 in PyObject_Call Objects/call.c:317\n    #69 0x3ffab1059c7 in do_call_core Python/ceval.c:5943\n    #70 0x3ffab0ffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #71 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #72 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #73 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #74 0x3ffaaf8a695 in _PyObject_FastCallDictTstate Objects/call.c:153\n    #75 0x3ffaaf8b271 in _PyObject_Call_Prepend Objects/call.c:431\n    #76 0x3ffab03f307 in slot_tp_call Objects/typeobject.c:7494\n    #77 0x3ffaaf8a933 in _PyObject_MakeTpCall Objects/call.c:215\n    #78 0x3ffab0f0081 in _PyObject_VectorcallTstate Include/cpython/abstract.h:112\n    #79 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #80 0x3ffab105447 in call_function Python/ceval.c:5891\n    #81 0x3ffab0ffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #82 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #83 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #84 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #85 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #86 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #87 0x3ffab105447 in call_function Python/ceval.c:5891\n    #88 0x3ffab0ff7d7 in _PyEval_EvalFrameDefault Python/ceval.c:4198\n    #89 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #90 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #91 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #92 0x3ffaaf8ab15 in PyVectorcall_Call Objects/call.c:255\n    #93 0x3ffaaf8ac65 in _PyObject_Call Objects/call.c:290\n    #94 0x3ffaaf8ada9 in PyObject_Call Objects/call.c:317\n    #95 0x3ffab1059c7 in do_call_core Python/ceval.c:5943\n    #96 0x3ffab0ffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #97 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #98 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #99 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #100 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #101 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #102 0x3ffab105447 in call_function Python/ceval.c:5891\n    #103 0x3ffab0ff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #104 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #105 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #106 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #107 0x3ffaaf8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #108 0x3ffaaf8eddd in method_vectorcall Objects/classobject.c:53\n    #109 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #110 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #111 0x3ffab105447 in call_function Python/ceval.c:5891\n    #112 0x3ffab0ff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #113 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #114 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #115 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #116 0x3ffaaf8a695 in _PyObject_FastCallDictTstate Objects/call.c:153\n    #117 0x3ffaaf8b271 in _PyObject_Call_Prepend Objects/call.c:431\n    #118 0x3ffab03f307 in slot_tp_call Objects/typeobject.c:7494\n    #119 0x3ffaaf8ad17 in _PyObject_Call Objects/call.c:305\n    #120 0x3ffaaf8ada9 in PyObject_Call Objects/call.c:317\n    #121 0x3ffab1059c7 in do_call_core Python/ceval.c:5943\n    #122 0x3ffab0ffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #123 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #124 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #125 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #126 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #127 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #128 0x3ffab105447 in call_function Python/ceval.c:5891\n    #129 0x3ffab0ff905 in _PyEval_EvalFrameDefault Python/ceval.c:4213\n    #130 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #131 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #132 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #133 0x3ffaaf8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #134 0x3ffaaf8eddd in method_vectorcall Objects/classobject.c:53\n    #135 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #136 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #137 0x3ffab105447 in call_function Python/ceval.c:5891\n    #138 0x3ffab0ffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #139 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #140 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #141 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #142 0x3ffaaf8ab15 in PyVectorcall_Call Objects/call.c:255\n    #143 0x3ffaaf8ac65 in _PyObject_Call Objects/call.c:290\n    #144 0x3ffaaf8ada9 in PyObject_Call Objects/call.c:317\n    #145 0x3ffab1059c7 in do_call_core Python/ceval.c:5943\n    #146 0x3ffab0ffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #147 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #148 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #149 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #150 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #151 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #152 0x3ffab105447 in call_function Python/ceval.c:5891\n    #153 0x3ffab0ff905 in _PyEval_EvalFrameDefault Python/ceval.c:4213\n    #154 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #155 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #156 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #157 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #158 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #159 0x3ffab105447 in call_function Python/ceval.c:5891\n    #160 0x3ffab0ffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #161 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #162 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #163 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #164 0x3ffaaf8ab15 in PyVectorcall_Call Objects/call.c:255\n    #165 0x3ffaaf8ac65 in _PyObject_Call Objects/call.c:290\n    #166 0x3ffaaf8ada9 in PyObject_Call Objects/call.c:317\n    #167 0x3ffab1059c7 in do_call_core Python/ceval.c:5943\n    #168 0x3ffab0ffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #169 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #170 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #171 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #172 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #173 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #174 0x3ffab105447 in call_function Python/ceval.c:5891\n    #175 0x3ffab0ff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #176 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #177 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #178 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #179 0x3ffaaf8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #180 0x3ffaaf8eddd in method_vectorcall Objects/classobject.c:53\n    #181 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #182 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #183 0x3ffab105447 in call_function Python/ceval.c:5891\n    #184 0x3ffab0ff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #185 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #186 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #187 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #188 0x3ffaaf8a695 in _PyObject_FastCallDictTstate Objects/call.c:153\n    #189 0x3ffaaf8b271 in _PyObject_Call_Prepend Objects/call.c:431\n    #190 0x3ffab03f307 in slot_tp_call Objects/typeobject.c:7494\n    #191 0x3ffaaf8a933 in _PyObject_MakeTpCall Objects/call.c:215\n    #192 0x3ffab0f0081 in _PyObject_VectorcallTstate Include/cpython/abstract.h:112\n    #193 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #194 0x3ffab105447 in call_function Python/ceval.c:5891\n    #195 0x3ffab0ffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #196 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #197 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #198 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #199 0x3ffaaf8ab15 in PyVectorcall_Call Objects/call.c:255\n    #200 0x3ffaaf8ac65 in _PyObject_Call Objects/call.c:290\n    #201 0x3ffaaf8ada9 in PyObject_Call Objects/call.c:317\n    #202 0x3ffab1059c7 in do_call_core Python/ceval.c:5943\n    #203 0x3ffab0ffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #204 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #205 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #206 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #207 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #208 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #209 0x3ffab105447 in call_function Python/ceval.c:5891\n    #210 0x3ffab0ff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #211 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #212 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #213 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #214 0x3ffaaf8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #215 0x3ffaaf8eddd in method_vectorcall Objects/classobject.c:53\n    #216 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #216 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #217 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #218 0x3ffab105447 in call_function Python/ceval.c:5891\n    #219 0x3ffab0ff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #220 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #221 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #222 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #223 0x3ffaaf8a695 in _PyObject_FastCallDictTstate Objects/call.c:153\n    #224 0x3ffaaf8b271 in _PyObject_Call_Prepend Objects/call.c:431\n    #225 0x3ffab03f307 in slot_tp_call Objects/typeobject.c:7494\n    #226 0x3ffaaf8a933 in _PyObject_MakeTpCall Objects/call.c:215\n    #227 0x3ffab0f0081 in _PyObject_VectorcallTstate Include/cpython/abstract.h:112\n    #228 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #229 0x3ffab105447 in call_function Python/ceval.c:5891\n    #230 0x3ffab0ffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #231 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #232 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #233 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #234 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #235 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #236 0x3ffab105447 in call_function Python/ceval.c:5891\n    #237 0x3ffab0ff905 in _PyEval_EvalFrameDefault Python/ceval.c:4213\n    #238 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #239 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #240 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #241 0x3ffab0f00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #242 0x3ffab0f013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #243 0x3ffab105447 in call_function Python/ceval.c:5891\n    #244 0x3ffab0ff905 in _PyEval_EvalFrameDefault Python/ceval.c:4213\n    #245 0x3ffab0f052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #246 0x3ffab102b67 in _PyEval_Vector Python/ceval.c:5065\n    #247 0x3ffaaf8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #248 0x3ffaaf8ab15 in PyVectorcall_Call Objects/call.c:255\n    #249 0x3ffaaf8ac65 in _PyObject_Call Objects/call.c:290\n\n0x60d0005a5790 is located 80 bytes inside of 136-byte region [0x60d0005a5740,0x60d0005a57c8)\nfreed by thread T0 here:\n    #0 0x3ffab537de5 in operator delete(void*) /var/tmp/portage/sys-devel/gcc-11.3.1_p20230303/work/gcc-11-20230303/libsanitizer/asan/asan_new_delete.cpp:160\n    #1 0x3ff55984fdb in __gnu_cxx::new_allocator<std::_Sp_counted_ptr_inplace<c10::FunctionSchema, std::allocator<c10::FunctionSchema>, (__gnu_cxx::_Lock_policy)2> >::deallocate(std::_Sp_counted_ptr_inplace<c10::FunctionSchema, std::allocator<c10::FunctionSchema>, (__gnu_cxx::_Lock_policy)2>*, unsigned long) /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/ext/new_allocator.h:145\n\npreviously allocated by thread T0 here:\n    #0 0x3ffab53734f in operator new(unsigned long) /var/tmp/portage/sys-devel/gcc-11.3.1_p20230303/work/gcc-11-20230303/libsanitizer/asan/asan_new_delete.cpp:99\n    #1 0x3ff5598443f in __gnu_cxx::new_allocator<std::_Sp_counted_ptr_inplace<c10::FunctionSchema, std::allocator<c10::FunctionSchema>, (__gnu_cxx::_Lock_policy)2> >::allocate(unsigned long, void const*) /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/ext/new_allocator.h:127\n    #2 0x3fff5849ecf  ([stack]+0xb2ecf)\n\nSUMMARY: AddressSanitizer: heap-use-after-free /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/stl_iterator.h:1028 in __gnu_cxx::__normal_iterator<c10::Argument const*, std::vector<c10::Argument, std::allocator<c10::Argument> > >::__normal_iterator(c10::Argument const* const&)\nShadow bytes around the buggy address:\n  0x100c1a000b4aa0: fd fd fd fd fd fd fd fd fd fd fd fa fa fa fa fa\n  0x100c1a000b4ab0: fa fa fa fa fd fd fd fd fd fd fd fd fd fd fd fd\n  0x100c1a000b4ac0: fd fd fd fd fd fa fa fa fa fa fa fa fa fa fd fd\n  0x100c1a000b4ad0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fa\n  0x100c1a000b4ae0: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd\n=>0x100c1a000b4af0: fd fd[fd]fd fd fd fd fd fd fa fa fa fa fa fa fa\n  0x100c1a000b4b00: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x100c1a000b4b10: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x100c1a000b4b20: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x100c1a000b4b30: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x100c1a000b4b40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\nShadow byte legend (one shadow byte represents 8 application bytes):\n  Addressable:           00\n  Partially addressable: 01 02 03 04 05 06 07\n  Heap left redzone:       fa\n  Freed heap region:       fd\n  Stack left redzone:      f1\n  Stack mid redzone:       f2\n  Stack right redzone:     f3\n  Stack after return:      f5\n  Stack use after scope:   f8\n  Global redzone:          f9\n  Global init order:       f6\n  Poisoned by user:        f7\n  Container overflow:      fc\n  Array cookie:            ac\n  Intra object redzone:    bb\n  ASan internal:           fe\n  Left alloca redzone:     ca\n  Right alloca redzone:    cb\n  Shadow gap:              cc\n==1134126==ABORTING\n```\n\nAdditional backtraces (not full):\nAllocation:\n```\n#0  __memset_z196 () at ../sysdeps/s390/memset-z900.S:144\n#1  0x000003ff96f3072a in __asan::Allocator::Allocate (this=this@entry=0x3ff97041eb8 <__asan::instance>, size=size@entry=136, alignment=8, alignment@entry=0, stack=<optimized out>,\n    stack@entry=0x3ffdbb45d78, alloc_type=<optimized out>, can_fill=true) at /var/tmp/portage/sys-devel/gcc-11.3.1_p20230303/work/gcc-11-20230303/libsanitizer/asan/asan_allocator.cpp:599\n#2  0x000003ff96f2c088 in __asan::asan_memalign (alignment=alignment@entry=0, size=size@entry=136, stack=stack@entry=0x3ffdbb45d78, alloc_type=alloc_type@entry=__asan::FROM_NEW)\n    at /var/tmp/portage/sys-devel/gcc-11.3.1_p20230303/work/gcc-11-20230303/libsanitizer/asan/asan_allocator.cpp:1039\n#3  0x000003ff96fb73b0 in operator new (size=136) at /var/tmp/portage/sys-devel/gcc-11.3.1_p20230303/work/gcc-11-20230303/libsanitizer/asan/asan_new_delete.cpp:99\n#4  0x000003ff41404440 in __gnu_cxx::new_allocator<std::_Sp_counted_ptr_inplace<c10::FunctionSchema, std::allocator<c10::FunctionSchema>, (__gnu_cxx::_Lock_policy)2> >::allocate (this=0x3ffdbb468c0,\n    __n=1) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/ext/new_allocator.h:127\n#5  0x000003ff414042a0 in std::allocator_traits<std::allocator<std::_Sp_counted_ptr_inplace<c10::FunctionSchema, std::allocator<c10::FunctionSchema>, (__gnu_cxx::_Lock_policy)2> > >::allocate (__a=...,\n    __n=1) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/alloc_traits.h:464\n#6  0x000003ff41403b66 in std::__allocate_guarded<std::allocator<std::_Sp_counted_ptr_inplace<c10::FunctionSchema, std::allocator<c10::FunctionSchema>, (__gnu_cxx::_Lock_policy)2> > > (__a=...)\n    at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/allocated_ptr.h:98\n#7  0x000003ff4140372a in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<c10::FunctionSchema, std::allocator<c10::FunctionSchema>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<c10::Argument, std::allocator<c10::Argument> >, std::vector<c10::Argument, std::allocator<c10::Argument> > > (this=0x3ffdbb47888, __p=@0x3ffdbb47880: 0x0, __a=..., __args=..., __args=..., __args=..., __args=...)\n    at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr_base.h:648\n#8  0x000003ff41403328 in std::__shared_ptr<c10::FunctionSchema, (__gnu_cxx::_Lock_policy)2>::__shared_ptr<std::allocator<c10::FunctionSchema>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<c10::Argument, std::allocator<c10::Argument> >, std::vector<c10::Argument, std::allocator<c10::Argument> > > (this=0x3ffdbb47880, __tag=..., __args=..., __args=..., __args=..., __args=...) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr_base.h:1342\n#9  0x000003ff41402f06 in std::shared_ptr<c10::FunctionSchema>::shared_ptr<std::allocator<c10::FunctionSchema>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<c10::Argument, std::allocator<c10::Argument> >, std::vector<c10::Argument, std::allocator<c10::Argument> > > (\n    this=0x3ffdbb47880, __tag=..., __args=..., __args=..., __args=..., __args=...) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr.h:409\n#10 0x000003ff41402b6e in std::allocate_shared<c10::FunctionSchema, std::allocator<c10::FunctionSchema>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<c10::Argument, std::allocator<c10::Argument> >, std::vector<c10::Argument, std::allocator<c10::Argument> > > (__a=...,\n    __args=..., __args=..., __args=..., __args=...) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr.h:862\n#11 0x000003ff4140215c in std::make_shared<c10::FunctionSchema, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<c10::Argument, std::allocator<c10::Argument> >, std::vector<c10::Argument, std::allocator<c10::Argument> > > (__args=..., __args=..., __args=..., __args=...)\n    at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr.h:878\n#12 0x000003ff413d180c in c10::TupleType::createWithSpec<c10::basic_string_view<char> > (qualName=..., field_names=std::vector of length 1, capacity 1 = {...},\n    field_types=std::vector of length 1, capacity 1 = {...}, field_defaults=std::vector of length 0, capacity 0) at /home/user/pytorch/aten/src/ATen/core/type.cpp:769\n#13 0x000003ff413b9ca6 in c10::TupleType::createNamed (qualName=..., field_names=std::vector of length 1, capacity 1 = {...}, field_types=std::vector of length 1, capacity 1 = {...})\n    at /home/user/pytorch/aten/src/ATen/core/type.cpp:725\n#14 0x000003ff4115fbac in c10::ivalue::TupleTypeFactory<c10::TupleType>::fallback (type=...) at /home/user/pytorch/aten/src/ATen/core/dynamic_type.cpp:383\n#15 0x000003ff708217fe in c10::ivalue::Tuple::type<c10::TupleType> (this=0x6080004b8520) at /home/user/pytorch/aten/src/ATen/core/ivalue_inl.h:781\n#16 0x000003ff70800740 in torch::jit::toPyObject (ivalue=...) at /home/user/pytorch/torch/csrc/jit/python/pybind_utils.cpp:613\n#17 0x000003ff70800306 in torch::jit::toPyObject (ivalue=...) at /home/user/pytorch/torch/csrc/jit/python/pybind_utils.cpp:604\n#18 0x000003ff702d6872 in pybind11::detail::type_caster<c10::IValue, void>::cast (src=...) at /home/user/pytorch/torch/csrc/jit/python/pybind.h:138\n#19 0x000003ff70d98192 in pybind11::cpp_function::initialize<torch::jit::initJitScriptBindings(_object*)::$_45, c10::IValue, torch::jit::mobile::Module&, pybind11::tuple const&, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg>(torch::jit::initJitScriptBindings(_object*)::$_45&&, c10::IValue (*)(torch::jit::mobile::Module&, pybind11::tuple const&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&)::{lambda(pybind11::detail::function_call&)#1}::operator()(pybind11::detail::function_call&) const (this=0x3ffdbb4ca20, call=...)\n    at /home/user/pytorch/cmake/../third_party/pybind11/include/pybind11/pybind11.h:249\n#20 0x000003ff70d97cfe in pybind11::cpp_function::initialize<torch::jit::initJitScriptBindings(_object*)::$_45, c10::IValue, torch::jit::mobile::Module&, pybind11::tuple const&, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg>(torch::jit::initJitScriptBindings(_object*)::$_45&&, c10::IValue (*)(torch::jit::mobile::Module&, pybind11::tuple const&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&)::{lambda(pybind11::detail::function_call&)#1}::__invoke(pybind11::detail::function_call&) (call=...)\n    at /home/user/pytorch/cmake/../third_party/pybind11/include/pybind11/pybind11.h:224\n#21 0x000003ff6e9652ea in pybind11::cpp_function::dispatcher (self=<PyCapsule at remote 0x3ff83e27720>,\n    args_in=(<torch._C.LiteScriptModule at remote 0x3ff811844b0>, (<Tensor at remote 0x3ff814efb00>,)), kwargs_in=0x0) at /home/user/pytorch/cmake/../third_party/pybind11/include/pybind11/pybind11.h:929\n```\n\nDeallocation:\n```\n#0  operator delete (ptr=0x60d0005a5740) at /var/tmp/portage/sys-devel/gcc-11.3.1_p20230303/work/gcc-11-20230303/libsanitizer/asan/asan_new_delete.cpp:160\n#1  0x000003ff44904fdc in __gnu_cxx::new_allocator<std::_Sp_counted_ptr_inplace<c10::FunctionSchema, std::allocator<c10::FunctionSchema>, (__gnu_cxx::_Lock_policy)2> >::deallocate (this=0x3ffc5dc8020,\n    __p=0x60d0005a5740, __t=1) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/ext/new_allocator.h:145\n#2  0x000003ff44904fa8 in std::allocator_traits<std::allocator<std::_Sp_counted_ptr_inplace<c10::FunctionSchema, std::allocator<c10::FunctionSchema>, (__gnu_cxx::_Lock_policy)2> > >::deallocate (\n    __a=..., __p=0x60d0005a5740, __n=1) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/alloc_traits.h:496\n#3  0x000003ff449041f2 in std::__allocated_ptr<std::allocator<std::_Sp_counted_ptr_inplace<c10::FunctionSchema, std::allocator<c10::FunctionSchema>, (__gnu_cxx::_Lock_policy)2> > >::~__allocated_ptr (\n    this=0x3ffc5dc8030) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/allocated_ptr.h:74\n#4  0x000003ff44904888 in std::_Sp_counted_ptr_inplace<c10::FunctionSchema, std::allocator<c10::FunctionSchema>, (__gnu_cxx::_Lock_policy)2>::_M_destroy (this=0x60d0005a5740)\n    at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr_base.h:538\n#5  0x000003ff43895a62 in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release (this=0x60d0005a5740) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr_base.h:184\n#6  0x000003ff43895420 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::~__shared_count (this=0x611000c40648) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr_base.h:705\n#7  0x000003ff4466e7f4 in std::__shared_ptr<c10::FunctionSchema, (__gnu_cxx::_Lock_policy)2>::~__shared_ptr (this=0x611000c40640)\n    at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr_base.h:1154\n#8  0x000003ff4466d820 in std::shared_ptr<c10::FunctionSchema>::~shared_ptr (this=0x611000c40640) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr.h:122\n#9  0x000003ff448d82f6 in c10::TupleType::~TupleType (this=0x611000c40580) at /home/user/pytorch/aten/src/ATen/core/jit_type.h:1142\n#10 0x000003ff448d8346 in c10::TupleType::~TupleType (this=0x611000c40580) at /home/user/pytorch/aten/src/ATen/core/jit_type.h:1142\n#11 0x000003ff731296a4 in std::_Sp_counted_ptr<c10::TupleType*, (__gnu_cxx::_Lock_policy)2>::_M_dispose (this=0x603000c43ae0)\n    at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr_base.h:348\n#12 0x000003ff71eaf666 in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release (this=0x603000c43ae0) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr_base.h:168\n#13 0x000003ff71eaf330 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::~__shared_count (this=0x3ffc5dc9368) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr_base.h:705\n#14 0x000003ff73129ee4 in std::__shared_ptr<c10::TupleType, (__gnu_cxx::_Lock_policy)2>::~__shared_ptr (this=0x3ffc5dc9360)\n    at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr_base.h:1154\n#15 0x000003ff73122390 in std::shared_ptr<c10::TupleType>::~shared_ptr (this=0x3ffc5dc9360) at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/shared_ptr.h:122\n#16 0x000003ff73d00788 in torch::jit::toPyObject (ivalue=...) at /home/user/pytorch/torch/csrc/jit/python/pybind_utils.cpp:613\n#17 0x000003ff73d00306 in torch::jit::toPyObject (ivalue=...) at /home/user/pytorch/torch/csrc/jit/python/pybind_utils.cpp:604\n```\n</details>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/101400\nApproved by: https://github.com/zou3519",
    "Number of deleted lines": 2,
    "Deleted lines": "-      const std::vector<Argument>& tuple_args =\n-          tuple->type()->schema()->arguments();",
    "Added lines": "+      std::vector<Argument> tuple_args = tuple->type()->schema()->arguments();",
    "Label": "clean"
},
{
    "Id": 908,
    "Library": "pytorch",
    "Date": "2023/05/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f0f700e8d257b1f08157aa86d9bfe0ffbeb32ec8",
    "Root Cause": "N.A",
    "Bug report": "ASAN: fix use-after-free (#101064)\n\nWhen tensor is resized, reference array to it's sizes may become invalid. Make a copy in advance.\n\n<details>\n<summary>ASAN report</summary>\n\n```\n=================================================================\n==1115867==ERROR: AddressSanitizer: heap-use-after-free on address 0x61000013d790 at pc 0x03ff8e7da360 bp 0x03fff53c83a0 sp 0x03fff53c8390\nREAD of size 8 at 0x61000013d790 thread T0\n    #0 0x3ff8e7da35f in c10::SymInt::is_heap_allocated() const /home/user/pytorch/c10/core/SymInt.h:154\n    #1 0x3ff8e7da35f in c10::SymInt::maybe_as_int() const /home/user/pytorch/c10/core/SymInt.h:215\n    #2 0x3ff8e7d0a6d in c10::SymInt::sym_eq(c10::SymInt const&) const /home/user/pytorch/c10/core/SymInt.cpp:69\n    #3 0x3ff7a9ab0bd in c10::SymInt::operator==(c10::SymInt const&) const /home/user/pytorch/c10/core/SymInt.h:177\n    #4 0x3ff7a9aaedd in bool std::__equal<false>::equal<c10::SymInt const*, c10::SymInt const*>(c10::SymInt const*, c10::SymInt const*, c10::SymInt const*) /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-\nv11/bits/stl_algobase.h:1162\n    #5 0x3ff7a9aae4b in bool std::__equal_aux1<c10::SymInt const*, c10::SymInt const*>(c10::SymInt const*, c10::SymInt const*, c10::SymInt const*) /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/\nstl_algobase.h:1211\n    #6 0x3ff7a9aae05 in bool std::__equal_aux<c10::SymInt const*, c10::SymInt const*>(c10::SymInt const*, c10::SymInt const*, c10::SymInt const*) /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/s\ntl_algobase.h:1219\n    #7 0x3ff7a9aad97 in bool std::equal<c10::SymInt const*, c10::SymInt const*>(c10::SymInt const*, c10::SymInt const*, c10::SymInt const*) /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/stl_alg\nobase.h:1556\n    #8 0x3ff4b23c771 in c10::ArrayRef<c10::SymInt>::equals(c10::ArrayRef<c10::SymInt>) const /home/user/pytorch/c10/util/ArrayRef.h:188\n    #9 0x3ff4cb91bc1 in bool c10::operator!=<c10::SymInt>(c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>) /home/user/pytorch/c10/util/ArrayRef.h:341\n    #10 0x3ff6d1b57ff in torch::ADInplaceOrView::resize_(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) /home/user/pytorch/torch/csrc/autograd/Variab\nleTypeManual.cpp:408\n    #11 0x3ff6d1e59c7 in c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c1\n0::MemoryFormat>), &torch::ADInplaceOrView::resize_>, at::Tensor const&, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>\n> >::operator()(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) /home/user/pytorch/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13\n    #12 0x3ff6d1e59c7 in c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10:\n:ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>), &torch::ADInplaceOrView::resize_>, at::Tensor const&, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::Sy\nmInt>, c10::optional<c10::MemoryFormat> > >, at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)>::call(c10::OperatorKernel*, c10::Disp\natchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) /home/user/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:480\n    #13 0x3ff51ca5129 in at::Tensor const& c10::callUnboxedKernelFunction<at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> >(void*, c10::OperatorKernel*,\nc10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>&&, c10::optional<c10::MemoryFormat>&&) /home/user/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:50\n    #14 0x3ff51ca6e8f in at::Tensor const& c10::KernelFunction::call<at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> >(c10::OperatorHandle const&, c10::D\nispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) const /home/user/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:90\n    #15 0x3ff51ca6e8f in at::Tensor const& c10::Dispatcher::redispatch<at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> >(c10::TypedOperatorHandle<at::Ten\nsor const& (at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)> const&, c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)\nconst /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:656\n    #16 0x3ff5182006b in c10::TypedOperatorHandle<at::Tensor const& (at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)>::redispatch(c10::DispatchKeySet, at::Tensor const&, c\n10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) const /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:492\n    #17 0x3ff5182006b in at::_ops::resize_::redispatch(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) aten/src/ATen/Operators_4.cpp:2144\n    #18 0x3ff6d1d5e07 in at::redispatch::resize__symint(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) aten/src/ATen/RedispatchFunctions.h:2847\n    #19 0x3ff6d1bbb67 in torch::autograd::VariableType::(anonymous namespace)::resize_(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) /home/user/pyto\nrch/torch/csrc/autograd/VariableTypeManual.cpp:243\n    #20 0x3ff6d1bd197 in c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c1\n0::MemoryFormat>), &torch::autograd::VariableType::(anonymous namespace)::resize_>, at::Tensor const&, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10\n::optional<c10::MemoryFormat> > >::operator()(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) /home/user/pytorch/aten/src/ATen/core/boxing/impl/WrapFu\nnctionIntoFunctor.h:13\n    #21 0x3ff6d1bd197 in c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10:\n:ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>), &torch::autograd::VariableType::(anonymous namespace)::resize_>, at::Tensor const&, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor\n const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> > >, at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)>::call(c\n10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) /home/user/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor\n.h:480\n    #22 0x3ff51ca5129 in at::Tensor const& c10::callUnboxedKernelFunction<at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> >(void*, c10::OperatorKernel*,\nc10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>&&, c10::optional<c10::MemoryFormat>&&) /home/user/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:50\n    #23 0x3ff5181ead1 in at::Tensor const& c10::KernelFunction::call<at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> >(c10::OperatorHandle const&, c10::D\nispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) const /home/user/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:90\n    #24 0x3ff5181ead1 in at::Tensor const& c10::Dispatcher::call<at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> >(c10::TypedOperatorHandle<at::Tensor co\nnst& (at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)> const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) const /home/user/pytorch/at\nen/src/ATen/core/dispatch/Dispatcher.h:639\n    #25 0x3ff5181ead1 in c10::TypedOperatorHandle<at::Tensor const& (at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)>::call(at::Tensor const&, c10::ArrayRef<c10::SymInt>,\nc10::optional<c10::MemoryFormat>) const /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:487\n    #26 0x3ff5181ead1 in at::_ops::resize_::call(at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) aten/src/ATen/Operators_4.cpp:2137\n    #27 0x3ff79b44fcf in at::Tensor::resize__symint(c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) const aten/src/ATen/core/TensorBody.h:2452\n    #28 0x3ff79a802db in torch::autograd::THPVariable_resize_(_object*, _object*, _object*)::$_0::operator()(at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) const /home/us\ner/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:13417\n    #29 0x3ff7999f1eb in torch::autograd::THPVariable_resize_(_object*, _object*, _object*) /home/user/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:13419\n    #30 0x3ffa2c9b009 in method_vectorcall_VARARGS_KEYWORDS Objects/descrobject.c:344\n    #31 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #32 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #33 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #34 0x3ffa2dff7d7 in _PyEval_EvalFrameDefault Python/ceval.c:4198\n    #35 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #36 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #37 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #38 0x3ffa2c8ab15 in PyVectorcall_Call Objects/call.c:255\n    #39 0x3ffa2c8ac65 in _PyObject_Call Objects/call.c:290\n    #40 0x3ffa2c8ada9 in PyObject_Call Objects/call.c:317\n    #41 0x3ffa2e059c7 in do_call_core Python/ceval.c:5943\n    #42 0x3ffa2dffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #43 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #44 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #45 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #46 0x3ffa2c8ab15 in PyVectorcall_Call Objects/call.c:255\n    #47 0x3ffa2c8ac65 in _PyObject_Call Objects/call.c:290\n    #48 0x3ffa2c8ada9 in PyObject_Call Objects/call.c:317\n    #49 0x3ffa2e059c7 in do_call_core Python/ceval.c:5943\n    #50 0x3ffa2dffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #51 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #52 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #53 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #54 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #55 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #56 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #57 0x3ffa2dff7d7 in _PyEval_EvalFrameDefault Python/ceval.c:4198\n    #58 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #59 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #60 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #61 0x3ffa2c8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #62 0x3ffa2c8eddd in method_vectorcall Objects/classobject.c:53\n    #63 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #64 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #65 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #66 0x3ffa2dff905 in _PyEval_EvalFrameDefault Python/ceval.c:4213\n    #67 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #68 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #69 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #70 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #71 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #72 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #73 0x3ffa2dff7d7 in _PyEval_EvalFrameDefault Python/ceval.c:4198\n    #74 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #75 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #76 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #77 0x3ffa2c8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #78 0x3ffa2c8eddd in method_vectorcall Objects/classobject.c:53\n    #79 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #80 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #81 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #82 0x3ffa2dffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #83 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #84 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #85 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #86 0x3ffa2c8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #87 0x3ffa2c8eddd in method_vectorcall Objects/classobject.c:53\n    #88 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #89 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #90 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #91 0x3ffa2dffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #92 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #93 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #94 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #95 0x3ffa2c8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #96 0x3ffa2c8eddd in method_vectorcall Objects/classobject.c:53\n    #97 0x3ffa2c8ab9b in PyVectorcall_Call Objects/call.c:267\n    #98 0x3ffa2c8ac65 in _PyObject_Call Objects/call.c:290\n    #99 0x3ffa2c8ada9 in PyObject_Call Objects/call.c:317\n    #100 0x3ffa2e059c7 in do_call_core Python/ceval.c:5943\n    #101 0x3ffa2dffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #102 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #103 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #104 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #105 0x3ffa2c8a695 in _PyObject_FastCallDictTstate Objects/call.c:153\n    #106 0x3ffa2c8b271 in _PyObject_Call_Prepend Objects/call.c:431\n    #107 0x3ffa2d3f307 in slot_tp_call Objects/typeobject.c:7494\n    #108 0x3ffa2c8a933 in _PyObject_MakeTpCall Objects/call.c:215\n    #109 0x3ffa2df0081 in _PyObject_VectorcallTstate Include/cpython/abstract.h:112\n    #110 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #111 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #112 0x3ffa2dffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #113 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #114 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #115 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #116 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #117 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #118 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #119 0x3ffa2dff7d7 in _PyEval_EvalFrameDefault Python/ceval.c:4198\n    #120 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #121 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #122 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #123 0x3ffa2c8ab15 in PyVectorcall_Call Objects/call.c:255\n    #124 0x3ffa2c8ac65 in _PyObject_Call Objects/call.c:290\n    #125 0x3ffa2c8ada9 in PyObject_Call Objects/call.c:317\n    #126 0x3ffa2e059c7 in do_call_core Python/ceval.c:5943\n    #127 0x3ffa2dffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #128 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #129 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #130 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #131 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #132 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #133 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #134 0x3ffa2dff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #135 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #136 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #137 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #138 0x3ffa2c8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #139 0x3ffa2c8eddd in method_vectorcall Objects/classobject.c:53\n    #140 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #141 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #142 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #143 0x3ffa2dff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #144 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #145 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #146 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #147 0x3ffa2c8a695 in _PyObject_FastCallDictTstate Objects/call.c:153\n    #148 0x3ffa2c8b271 in _PyObject_Call_Prepend Objects/call.c:431\n    #149 0x3ffa2d3f307 in slot_tp_call Objects/typeobject.c:7494\n    #150 0x3ffa2c8ad17 in _PyObject_Call Objects/call.c:305\n    #151 0x3ffa2c8ada9 in PyObject_Call Objects/call.c:317\n    #152 0x3ffa2e059c7 in do_call_core Python/ceval.c:5943\n    #153 0x3ffa2dffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #154 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #155 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #156 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #157 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #158 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #159 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #160 0x3ffa2dff905 in _PyEval_EvalFrameDefault Python/ceval.c:4213\n    #161 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #162 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #163 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #164 0x3ffa2c8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #165 0x3ffa2c8eddd in method_vectorcall Objects/classobject.c:53\n    #166 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #167 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #168 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #169 0x3ffa2dffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #170 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #171 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #172 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #173 0x3ffa2c8ab15 in PyVectorcall_Call Objects/call.c:255\n    #174 0x3ffa2c8ac65 in _PyObject_Call Objects/call.c:290\n    #175 0x3ffa2c8ada9 in PyObject_Call Objects/call.c:317\n    #176 0x3ffa2e059c7 in do_call_core Python/ceval.c:5943\n    #177 0x3ffa2dffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #178 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #179 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #180 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #181 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #182 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #183 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #184 0x3ffa2dff905 in _PyEval_EvalFrameDefault Python/ceval.c:4213\n    #185 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #186 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #187 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #188 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #189 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #190 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #191 0x3ffa2dffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #192 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #193 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #194 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #195 0x3ffa2c8ab15 in PyVectorcall_Call Objects/call.c:255\n    #196 0x3ffa2c8ac65 in _PyObject_Call Objects/call.c:290\n    #197 0x3ffa2c8ada9 in PyObject_Call Objects/call.c:317\n    #198 0x3ffa2e059c7 in do_call_core Python/ceval.c:5943\n    #199 0x3ffa2dffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #200 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #201 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #202 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #203 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #204 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #205 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #206 0x3ffa2dff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #207 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #208 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #209 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #210 0x3ffa2c8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #211 0x3ffa2c8eddd in method_vectorcall Objects/classobject.c:53\n    #212 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #213 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #214 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #215 0x3ffa2dff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #216 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #217 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #218 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #219 0x3ffa2c8a695 in _PyObject_FastCallDictTstate Objects/call.c:153\n    #220 0x3ffa2c8b271 in _PyObject_Call_Prepend Objects/call.c:431\n    #221 0x3ffa2d3f307 in slot_tp_call Objects/typeobject.c:7494\n    #222 0x3ffa2c8a933 in _PyObject_MakeTpCall Objects/call.c:215\n    #223 0x3ffa2df0081 in _PyObject_VectorcallTstate Include/cpython/abstract.h:112\n    #224 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #225 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #226 0x3ffa2dffa57 in _PyEval_EvalFrameDefault Python/ceval.c:4231\n    #227 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #228 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #229 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #230 0x3ffa2c8ab15 in PyVectorcall_Call Objects/call.c:255\n    #231 0x3ffa2c8ac65 in _PyObject_Call Objects/call.c:290\n    #232 0x3ffa2c8ada9 in PyObject_Call Objects/call.c:317\n    #233 0x3ffa2e059c7 in do_call_core Python/ceval.c:5943\n    #234 0x3ffa2dffd39 in _PyEval_EvalFrameDefault Python/ceval.c:4277\n    #235 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #236 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #237 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #238 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #239 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #240 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #241 0x3ffa2dff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #242 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #243 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #244 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #245 0x3ffa2c8e941 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #246 0x3ffa2c8eddd in method_vectorcall Objects/classobject.c:53\n    #247 0x3ffa2df00a9 in _PyObject_VectorcallTstate Include/cpython/abstract.h:114\n    #248 0x3ffa2df013d in PyObject_Vectorcall Include/cpython/abstract.h:123\n    #249 0x3ffa2e05447 in call_function Python/ceval.c:5891\n    #250 0x3ffa2dff779 in _PyEval_EvalFrameDefault Python/ceval.c:4181\n    #251 0x3ffa2df052b in _PyEval_EvalFrame Include/internal/pycore_ceval.h:46\n    #252 0x3ffa2e02b67 in _PyEval_Vector Python/ceval.c:5065\n    #253 0x3ffa2c8aec1 in _PyFunction_Vectorcall Objects/call.c:342\n    #254 0x3ffa2c8a695 in _PyObject_FastCallDictTstate Objects/call.c:153\n    #255 0x3ffa2c8b271 in _PyObject_Call_Prepend Objects/call.c:431\n    #256 0x3ffa2d3f307 in slot_tp_call Objects/typeobject.c:7494\n    #257 0x3ffa2c8a933 in _PyObject_MakeTpCall Objects/call.c:215\n\n0x61000013d790 is located 80 bytes inside of 192-byte region [0x61000013d740,0x61000013d800)\nfreed by thread T0 here:\n    #0 0x3ffa3237de5 in operator delete(void*) /var/tmp/portage/sys-devel/gcc-11.3.1_p20230303/work/gcc-11-20230303/libsanitizer/asan/asan_new_delete.cpp:160\n    #1 0x3ff8e7e3221 in c10::TensorImpl::~TensorImpl() /home/user/pytorch/c10/core/TensorImpl.cpp:75\n\npreviously allocated by thread T0 here:\n    #0 0x3ffa323734f in operator new(unsigned long) /var/tmp/portage/sys-devel/gcc-11.3.1_p20230303/work/gcc-11-20230303/libsanitizer/asan/asan_new_delete.cpp:99\n    #1 0x3ff4aeeb3d1 in c10::intrusive_ptr<c10::TensorImpl, c10::detail::intrusive_target_default_null_type<c10::TensorImpl> > c10::intrusive_ptr<c10::TensorImpl, c10::detail::intrusive_target_default_nul\nl_type<c10::TensorImpl> >::make<c10::intrusive_ptr<c10::StorageImpl, c10::detail::intrusive_target_default_null_type<c10::StorageImpl> >, c10::DispatchKeySet&, caffe2::TypeMeta&>(c10::intrusive_ptr<c10::S\ntorageImpl, c10::detail::intrusive_target_default_null_type<c10::StorageImpl> >&&, c10::DispatchKeySet&, caffe2::TypeMeta&) /home/user/pytorch/c10/util/intrusive_ptr.h:498\n    #2 0x3ff76f79e17  (/home/user/pytorch/build/lib.linux-s390x-cpython-310/torch/lib/libtorch_cpu.so+0x2fb79e17)\n\nSUMMARY: AddressSanitizer: heap-use-after-free /home/user/pytorch/c10/core/SymInt.h:154 in c10::SymInt::is_heap_allocated() const\nShadow bytes around the buggy address:\n  0x100c2000027aa0: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd\n  0x100c2000027ab0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\n  0x100c2000027ac0: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd\n  0x100c2000027ad0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\n  0x100c2000027ae0: fa fa fa fa fa fa fa fa fd fd fd fd fd fd fd fd\n=>0x100c2000027af0: fd fd[fd]fd fd fd fd fd fd fd fd fd fd fd fd fd\n  0x100c2000027b00: fa fa fa fa fa fa fa fa 00 00 00 00 00 00 00 00\n  0x100c2000027b10: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x100c2000027b20: fa fa fa fa fa fa fa fa 00 00 00 00 00 00 00 00\n  0x100c2000027b30: 00 00 00 00 04 fa fa fa fa fa fa fa fa fa fa fa\n  0x100c2000027b40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\nShadow byte legend (one shadow byte represents 8 application bytes):\n  Addressable:           00\n  Partially addressable: 01 02 03 04 05 06 07\n  Heap left redzone:       fa\n  Freed heap region:       fd\n  Stack left redzone:      f1\n  Stack mid redzone:       f2\n  Stack right redzone:     f3\n  Stack after return:      f5\n  Stack use after scope:   f8\n  Global redzone:          f9\n  Global init order:       f6\n  Poisoned by user:        f7\n  Container overflow:      fc\n  Array cookie:            ac\n  Intra object redzone:    bb\n  ASan internal:           fe\n  Left alloca redzone:     ca\n  Right alloca redzone:    cb\n  Shadow gap:              cc\n==1115867==ABORTING\n```\n</details>\n\n<details>\n<summary>Additional backtraces (not full)</summary>\n\nMemory deallocation:\n```\n#0  operator delete (ptr=0x61000013d740) at /var/tmp/portage/sys-devel/gcc-11.3.1_p20230303/work/gcc-11-20230303/libsanitizer/asan/asan_new_delete.cpp:160\n#1  0x000003ffa77e3222 in c10::TensorImpl::~TensorImpl (this=0x61000013d740) at /home/user/pytorch/c10/core/TensorImpl.cpp:75\n#2  0x000003ff63e76e8c in c10::intrusive_ptr<c10::TensorImpl, c10::UndefinedTensorImpl>::reset_ (this=0x3ffd7ec8230) at /home/user/pytorch/c10/util/intrusive_ptr.h:291\n#3  0x000003ff63e76910 in c10::intrusive_ptr<c10::TensorImpl, c10::UndefinedTensorImpl>::~intrusive_ptr (this=0x3ffd7ec8230) at /home/user/pytorch/c10/util/intrusive_ptr.h:370\n#4  0x000003ff63e67240 in at::TensorBase::~TensorBase (this=0x3ffd7ec8230) at /home/user/pytorch/aten/src/ATen/core/TensorBase.h:80\n#5  0x000003ff63e85ee0 in at::Tensor::~Tensor (this=0x3ffd7ec8230) at aten/src/ATen/core/TensorBody.h:90\n#6  0x000003ff63f67304 in resize__functionalization (dispatchKeySet=..., self=..., size=..., memory_format=...) at /home/user/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:173\n#7  0x000003ff63f89258 in c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>), &(resize__functionalization(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>))>, at::Tensor const&, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat> > >::operator()(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>) (\n    this=0x6030000390a0, args=..., args=..., args=..., args=...) at /home/user/pytorch/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13\n#8  c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>), &(resize__functionalization(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>))>, at::Tensor const&, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat> > >, at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>) (functor=0x6030000390a0, dispatchKeySet=..., args=..., args=...,\n    args=...) at /home/user/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:480\n#9  0x000003ff6aca560a in c10::callUnboxedKernelFunction<at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat> > (\n    unboxed_kernel_func=0x3ff63f88a80 <c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor const& (c10::DispatchKeySet, at::Tenso\nr const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>), &(resize__functionalization(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>))>, at::Tensor const&, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat> > >, at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>)>, functor=0x6030000390a0,\n    dispatchKeySet=..., args=..., args=..., args=...) at /home/user/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:50\n#10 0x000003ff6aca715c in c10::KernelFunction::call<at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> > (this=0x6210005e1b28, opHandle=...,\n    dispatchKeySet=..., args=..., args=..., args=...) at /home/user/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:96\n#11 c10::Dispatcher::redispatch<at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> >(c10::TypedOperatorHandle<at::Tensor const& (at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)> const&, c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) const (\n    this=0x3ff919400e0 <c10::Dispatcher::realSingleton()::_singleton>, op=..., currentDispatchKeySet=..., args=..., args=..., args=...) at /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:656\n#12 0x000003ff6a82006c in c10::TypedOperatorHandle<at::Tensor const& (at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)>::redispatch(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) const (\n    this=0x3ff919a07e0 <at::_ops::resize_::redispatch(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)::op>, currentDispatchKeySet=..., args=...,\n    args=..., args=...) at /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:492\n#13 at::_ops::resize_::redispatch (dispatchKeySet=..., self=..., size=..., memory_format=...) at /home/user/pytorch/build/aten/src/ATen/Operators_4.cpp:2144\n#14 0x000003ff861d5e08 in at::redispatch::resize__symint (dispatchKeySet=..., self=..., size=..., memory_format=...) at aten/src/ATen/RedispatchFunctions.h:2847\n#15 0x000003ff861b579e in torch::ADInplaceOrView::resize_ (ks=..., self=..., size=..., optional_memory_format=...) at /home/user/pytorch/torch/csrc/autograd/VariableTypeManual.cpp:401\n```\n\nMemory access:\n```\n#0  c10::SymInt::maybe_as_int (this=0x61000013d790) at /home/user/pytorch/c10/core/SymInt.h:215\n#1  0x000003ff734d0a6e in c10::SymInt::sym_eq (this=0x61000013d790, sci=...) at /home/user/pytorch/c10/core/SymInt.cpp:69\n#2  0x000003ff5f6ab0be in c10::SymInt::operator== (this=0x61000013d790, o=...) at /home/user/pytorch/c10/core/SymInt.h:177\n#3  0x000003ff5f6aaede in std::__equal<false>::equal<c10::SymInt const*, c10::SymInt const*> (__first1=0x61000013d790, __last1=0x61000013d7a0, __first2=0x602000015c30)\n    at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/stl_algobase.h:1162\n#4  0x000003ff5f6aae4c in std::__equal_aux1<c10::SymInt const*, c10::SymInt const*> (__first1=0x61000013d790, __last1=0x61000013d7a0, __first2=0x602000015c30)\n    at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/stl_algobase.h:1211\n#5  0x000003ff5f6aae06 in std::__equal_aux<c10::SymInt const*, c10::SymInt const*> (__first1=0x61000013d790, __last1=0x61000013d7a0, __first2=0x602000015c30)\n    at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/stl_algobase.h:1219\n#6  0x000003ff5f6aad98 in std::equal<c10::SymInt const*, c10::SymInt const*> (__first1=0x61000013d790, __last1=0x61000013d7a0, __first2=0x602000015c30)\n    at /usr/lib/gcc/s390x-ibm-linux-gnu/11/include/g++-v11/bits/stl_algobase.h:1556\n#7  0x000003ff2ff3c772 in c10::ArrayRef<c10::SymInt>::equals (this=0x3ffed7c9900, RHS=...) at /home/user/pytorch/c10/util/ArrayRef.h:188\n#8  0x000003ff31891bc2 in c10::operator!=<c10::SymInt> (a1=..., a2=...) at /home/user/pytorch/c10/util/ArrayRef.h:341\n#9  0x000003ff51eb5800 in torch::ADInplaceOrView::resize_ (ks=..., self=..., size=..., optional_memory_format=...) at /home/user/pytorch/torch/csrc/autograd/VariableTypeManual.cpp:408\n#10 0x000003ff51ee59c8 in c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c\n10::MemoryFormat>), &torch::ADInplaceOrView::resize_>, at::Tensor const&, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>\n > >::operator()(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) (this=0x6030007dca40, args=..., args=..., args=..., args=...)\n    at /home/user/pytorch/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13\n#11 c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt\n>, c10::optional<c10::MemoryFormat>), &torch::ADInplaceOrView::resize_>, at::Tensor const&, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<\nc10::MemoryFormat> > >, at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tenso\nr const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) (functor=0x6030007dca40, dispatchKeySet=..., args=..., args=..., args=...)\n    at /home/user/pytorch/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:480\n#12 0x000003ff369a512a in c10::callUnboxedKernelFunction<at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> > (\n    unboxed_kernel_func=0x3ff51ee51f0 <c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor const& (c10::DispatchKeySet, at::Tenso\nr const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>), &torch::ADInplaceOrView::resize_>, at::Tensor const&, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, c10::Ar\nrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> > >, at::Tensor const& (c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)>::call(c10::OperatorKern\nel*, c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)>, functor=0x6030007dca40, dispatchKeySet=..., args=..., args=..., args=...)\n    at /home/user/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:50\n#13 0x000003ff369a6e90 in c10::KernelFunction::call<at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> > (this=0x6210005e1bc8, opHandle=...,\n    dispatchKeySet=..., args=..., args=..., args=...) at /home/user/pytorch/aten/src/ATen/core/boxing/KernelFunction_impl.h:90\n#14 c10::Dispatcher::redispatch<at::Tensor const&, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat> >(c10::TypedOperatorHandle<at::Tensor const& (at::Tensor const&, c10::Arr\nayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)> const&, c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) const (\n    this=0x3ff5d6400e0 <c10::Dispatcher::realSingleton()::_singleton>, op=..., currentDispatchKeySet=..., args=..., args=..., args=...) at /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:656\n#15 0x000003ff3652006c in c10::TypedOperatorHandle<at::Tensor const& (at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)>::redispatch(c10::DispatchKeySet, at::Tensor const&,\nc10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>) const (\n    this=0x3ff5d6a07e0 <at::_ops::resize_::redispatch(c10::DispatchKeySet, at::Tensor const&, c10::ArrayRef<c10::SymInt>, c10::optional<c10::MemoryFormat>)::op>, currentDispatchKeySet=..., args=...,\n    args=..., args=...) at /home/user/pytorch/aten/src/ATen/core/dispatch/Dispatcher.h:492\n#16 at::_ops::resize_::redispatch (dispatchKeySet=..., self=..., size=..., memory_format=...) at /home/user/pytorch/build/aten/src/ATen/Operators_4.cpp:2144\n#17 0x000003ff51ed5e08 in at::redispatch::resize__symint (dispatchKeySet=..., self=..., size=..., memory_format=...) at aten/src/ATen/RedispatchFunctions.h:2847\n#18 0x000003ff51ebbb68 in torch::autograd::VariableType::(anonymous namespace)::resize_ (ks=..., self=..., size=..., optional_memory_format=...)\n    at /home/user/pytorch/torch/csrc/autograd/VariableTypeManual.cpp:243\n```\n</details>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/101064\nApproved by: https://github.com/Skylion007, https://github.com/albanD",
    "Number of deleted lines": 2,
    "Deleted lines": "-  auto org_size = self.sym_sizes();\n-  auto org_size = self.sym_sizes();",
    "Added lines": "+  // Explicitly copy data, since resizing can move original data\n+  // and make references invalid.\n+  auto org_size = self.sym_sizes().vec();\n+  // Explicitly copy data, since resizing can move original data\n+  // and make references invalid.\n+  auto org_size = self.sym_sizes().vec();",
    "Label": "clean"
},
{
    "Id": 909,
    "Library": "pytorch",
    "Date": "2023/05/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/116e04be2907c961c386b050872841732d0d2bb7",
    "Root Cause": "N.A",
    "Bug report": "Initialize view_replay_enabled_ in the AutogradState ctor (#100822)\n\nCruise uses [clang static analyzer](https://clang-analyzer.llvm.org/) internally.\nIn the v2.0.0 release of PyTorch it found this problem\n\n```\nIn file included from external/pytorch/aten/src/ATen/ATen.h:7:\nIn file included from external/pytorch/aten/src/ATen/Context.h:3:\nIn file included from external/pytorch/aten/src/ATen/CPUGeneratorImpl.h:3:\nIn file included from external/pytorch/aten/src/ATen/core/Generator.h:22:\nIn file included from external/pytorch/c10/core/GeneratorImpl.h:8:\nIn file included from external/pytorch/c10/core/TensorImpl.h:6:\nexternal/pytorch/c10/core/InferenceMode.h:58:5: warning: Passed-by-value struct argument contains uninitialized data (e.g., field: 'view_replay_enabled_')\n    AutogradState::set_tls_state(AutogradState(\n    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n1 warning generated.\n```\n\nIn other words, the value of `view_replay_enabled_` could be initialized which may lead to subtle bugs later on.\n\nThis PR addresses the warning by explicitly initializing it to `false`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/100822\nApproved by: https://github.com/Skylion007",
    "Number of deleted lines": 1,
    "Deleted lines": "-        multithreading_enabled_(multithreading_enabled) {}",
    "Added lines": "+        multithreading_enabled_(multithreading_enabled),\n+        view_replay_enabled_(false) {}",
    "Label": "clean"
},
{
    "Id": 910,
    "Library": "pytorch",
    "Date": "2023/05/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d5169e7141706f6ed6c1847d4447a6efaacd7fbe",
    "Root Cause": "N.A",
    "Bug report": "Use a stable ordering for saved values in functorch.default_partition (#100111)\n\nPreviously, due to the use of the Python set data structure, the ordering of saved values (and how they would appear in the graph) was unstable and changed across runs, making it hard to debug downstream applications. Here we use a dict (with insertion-ordering semantics) to deduplicate values in a way that preserves ordering\nPull Request resolved: https://github.com/pytorch/pytorch/pull/100111\nApproved by: https://github.com/Skylion007",
    "Number of deleted lines": 2,
    "Deleted lines": "-    saved_values = list(set(saved_values))\n-    saved_sym_nodes = list(set(saved_sym_nodes))",
    "Added lines": "+    saved_values = list({k: None for k in saved_values}.keys())\n+    saved_sym_nodes = list({k: None for k in saved_sym_nodes}.keys())",
    "Label": "clean"
},
{
    "Id": 911,
    "Library": "pytorch",
    "Date": "2023/04/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/baf092b82d791a908767e6d93b9e6f99e4b2867f",
    "Root Cause": "N.A",
    "Bug report": "Update pt2-bug-report.yml (#99928)\n\n<!--\ncopilot:summary\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at 9691a66</samp>\n\nUpdate the `pt2-bug-report.yml` template to use `curl` instead of `wget`, `main` instead of `master`, and `python3` instead of `python`. These changes improve the portability and reliability of the bug report process.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/99928\nApproved by: https://github.com/kit1980, https://github.com/msaroufim",
    "Number of deleted lines": 2,
    "Deleted lines": "-        wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n-        python collect_env.py",
    "Added lines": "+        curl -OL https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py\n+        python3 collect_env.py",
    "Label": "clean"
},
{
    "Id": 912,
    "Library": "pytorch",
    "Date": "2023/04/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bbfd577b7cdb703a4cfd3886269b4f056012fd7b",
    "Root Cause": "N.A",
    "Bug report": "bug-report.yml fix broken link (#99425)\n\nfix link in ISSUE_TEMPLATE\nPull Request resolved: https://github.com/pytorch/pytorch/pull/99425\nApproved by: https://github.com/kit1980",
    "Number of deleted lines": 1,
    "Deleted lines": "-      wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py",
    "Added lines": "+      wget https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py",
    "Label": "clean"
},
{
    "Id": 913,
    "Library": "pytorch",
    "Date": "2023/04/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7cb581d42f3cf952f2f3eb32b175167c88c72aac",
    "Root Cause": "N.A",
    "Bug report": "aot_autograd: more logging on metadata asserts (#99177)\n\nSummary: add better logging to aot autograd asserts to debug internal model issues\n\nTest Plan: let CI run\n\nDifferential Revision: D45006044\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/99177\nApproved by: https://github.com/bertmaher",
    "Number of deleted lines": 2,
    "Deleted lines": "-        assert ref_fw_metadata == updated_fw_metadata\n-        assert ref_fw_metadata == fw_metadata_updated",
    "Added lines": "+        assert ref_fw_metadata == updated_fw_metadata, \\\n+            f'ref_metadata={str(ref_fw_metadata)}, actual_metadata={str(updated_fw_metadata)}'\n+        assert ref_fw_metadata == fw_metadata_updated, \\\n+            f'ref_metadata={str(ref_fw_metadata)}, actual_metadata={str(fw_metadata_updated)}'",
    "Label": "clean"
},
{
    "Id": 914,
    "Library": "pytorch",
    "Date": "2023/04/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/20019f7c56d90ca88eaaf272b1577af0a615068a",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in symbolic shape messages (#99169)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/99169\nApproved by: https://github.com/anijain2305",
    "Number of deleted lines": 3,
    "Deleted lines": "-                    msg = f\"  {len(warn_msgs) + 1}. {msg}\"\n-                    msg = f\"  {len(error_msgs) + 1}. {msg}\"\n-            log.warning(\"Warning only constraints violated %s\", warn_msgs)",
    "Added lines": "+                    msg = f\"  {len(warn_msgs) + 1}. {msg()}\"\n+                    msg = f\"  {len(error_msgs) + 1}. {msg()}\"\n+                log.warning(\"Warning only constraints violated %s\", warn_msgs)\n+            elif len(warn_msgs) > 0:\n+                log.warning(\"%s Warning only constraints violated\", len(warn_msgs))",
    "Label": "clean"
},
{
    "Id": 915,
    "Library": "pytorch",
    "Date": "2023/04/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ee1f28cd15bbb6df234282c98e0c12dc8c77e4ba",
    "Root Cause": "N.A",
    "Bug report": "Fix the bug of comm headers. (#98658)\n\n`comm.hpp` is  exposed to the developer under `torch/include/torch/csrc/distributed/c10d/`.\nBut when I use it, the following error occurs : `undefined symbol:xxx`. So I want it can expose to developer.\n\n![image](https://user-images.githubusercontent.com/37650440/230697500-ec095103-3566-4415-88df-17491f01846e.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98658\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 1,
    "Deleted lines": "- at::Tensor parseCppCommHookResult(const c10::IValue& result);",
    "Added lines": "+TORCH_API at::Tensor parseCppCommHookResult(const c10::IValue& result);",
    "Label": "clean"
},
{
    "Id": 916,
    "Library": "pytorch",
    "Date": "2023/04/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/60ebb2f1163bc8e7df31514733063381c8959997",
    "Root Cause": "N.A",
    "Bug report": "[Gloo][BE] Print stacktrace on collectFullMesh (#98810)\n\nCatch error and torch_check it so full C++ stacktrace is printed for\nbetter debug\n\nDifferential Revision: [D44860626](https://our.internmc.facebook.com/intern/diff/D44860626/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98810\nApproved by: https://github.com/wanchaol",
    "Number of deleted lines": 1,
    "Deleted lines": "-    context->connectFullMesh(store, options->devices[i]);",
    "Added lines": "+    try {\n+      context->connectFullMesh(store, options->devices[i]);\n+    } catch (const std::runtime_error& e) {\n+      auto err = e.what();\n+      // TORCH_CHECK to print the cpp stacktrace.\n+      auto msg = c10::str(\"Gloo connectFullMesh failed with \", err);\n+      logAndThrow(msg, msg);\n+    }",
    "Label": "clean"
},
{
    "Id": 917,
    "Library": "pytorch",
    "Date": "2023/04/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/abafb1e6dc2c0dfd8897679076f9b0f632a989f1",
    "Root Cause": "N.A",
    "Bug report": "[fx] Minor bug fix for SubgraphMatcher when ignoring literals (#98458)\n\nFixes #ISSUE_NUMBER\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98458\nApproved by: https://github.com/andrewor14",
    "Number of deleted lines": 2,
    "Deleted lines": "-                    if not self.ignore_literals:\n-                        matched = self._match_literals(a1, a2, match)",
    "Added lines": "+                    matched = self.ignore_literals or self._match_literals(a1, a2, match)",
    "Label": "clean"
},
{
    "Id": 918,
    "Library": "pytorch",
    "Date": "2023/04/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3ed66f94b5fb1d30687f225aaf45314daafa9601",
    "Root Cause": "N.A",
    "Bug report": "Add more debug logs to evaluate_expr (#98344)\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98344\nApproved by: https://github.com/voznesenskym",
    "Number of deleted lines": 10,
    "Deleted lines": "-    def evaluate_expr(self, expr: \"sympy.Expr\", hint=None):\n-        if len(expr.free_symbols) == 0:\n-            return expr\n-        expr = self.simplify(expr)\n-            if concrete_val is sympy.true:\n-                self._add_guard(expr)\n-            elif concrete_val is sympy.false:\n-                self._add_guard(sympy.Not(expr))\n-            else:\n-                self._add_guard(sympy.Eq(expr, concrete_val))  # type: ignore[arg-type]",
    "Added lines": "+    def evaluate_expr(self, orig_expr: \"sympy.Expr\", hint=None):\n+        if len(orig_expr.free_symbols) == 0:\n+            log.debug(\"evaluate_expr %s [trivial]\", orig_expr)\n+            return orig_expr\n+\n+        expr = orig_expr\n+            log.debug(\"evaluate_expr %s == %s [statically known]\", orig_expr, static_expr)\n+        if concrete_val is sympy.true:\n+            g = expr\n+        elif concrete_val is sympy.false:\n+            g = sympy.Not(expr)\n+        else:\n+            g = sympy.Eq(expr, concrete_val)  # type: ignore[arg-type]\n+            self._add_guard(g)\n+            log.debug(\"evaluate_expr %s [guard added]\", g)\n+        else:\n+            log.debug(\"evaluate_expr %s [guard suppressed]\", g)\n+",
    "Label": "clean"
},
{
    "Id": 919,
    "Library": "pytorch",
    "Date": "2023/03/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a5b6f10c5d56dafba2aa9fd9e1949c2e88665df4",
    "Root Cause": "N.A",
    "Bug report": "Fix format bug in NT docs (#97998)\n\nFixes a formatting bug in the NT docs\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97998\nApproved by: https://github.com/jbschlosser",
    "Number of deleted lines": 4,
    "Deleted lines": "-  :func:`torch.add`; \"Supports elementwise addition of two nested tensors.\n-   Supports addition of a scalar to a nested tensor.\"\n-   :func:`torch.mul`; \"Supports elementwise multiplication of two nested tensors.\n-   Supports multiplication of a nested tensor by a scalar.\"",
    "Added lines": "+   :func:`torch.add`; \"Supports elementwise addition of two nested tensors. Supports addition of a scalar to a nested tensor.\"\n+   :func:`torch.mul`; \"Supports elementwise multiplication of two nested tensors. Supports multiplication of a nested tensor by a scalar.\"",
    "Label": "clean"
},
{
    "Id": 920,
    "Library": "pytorch",
    "Date": "2023/03/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c905251f9f0d23403facfb8b6485edf91df8507b",
    "Root Cause": "N.A",
    "Bug report": "[dynamo 3.11] fix eval_frame.c debug prints for 3.11 (#96500)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/96500\nApproved by: https://github.com/jansel, https://github.com/albanD",
    "Number of deleted lines": 5,
    "Deleted lines": "-#ifdef TORCHDYNAMO_DEBUG\n-#endif\n-      \"begin %s %s %i %i %i %i\",\n-      frame->f_iblock,\n-      frame->f_executing);",
    "Added lines": "+  #if IS_PYTHON_3_11_PLUS\n+  DEBUG_CHECK(ncells == frame->f_code->co_ncellvars);\n+  DEBUG_CHECK(nfrees == frame->f_code->co_nfreevars);\n+  #else\n+  #endif\n+  #if IS_PYTHON_3_11_PLUS\n+      \"begin %s %s %i %i\",\n+      name(frame),\n+      PyUnicode_AsUTF8(frame->f_code->co_filename),\n+      frame->f_code->co_firstlineno,\n+      _PyInterpreterFrame_LASTI(frame));\n+  #else\n+  DEBUG_TRACE(\n+      \"begin %s %s %i %i %i\",\n+      frame->f_iblock);\n+  #endif",
    "Label": "clean"
},
{
    "Id": 921,
    "Library": "pytorch",
    "Date": "2023/03/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5e6e98483508f60dd654fc86f55a09d6f3c7333f",
    "Root Cause": "N.A",
    "Bug report": "flake8 version reporting in collect_env (#94573)\n\nFixes #94571\n\n# Testing\n`[pip3] flake8==3.9.2` now appears under `Versions of relevant libraries:`\nwhen running: `python torch/utils/collect_env.py`\n### Output with this change\n```\nCollecting environment information...\nPyTorch version: N/A\nIs debug build: N/A\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: N/A\n\nOS: macOS 13.1 (x86_64)\nGCC version: Could not collect\nClang version: 14.0.0 (clang-1400.0.29.202)\nCMake version: Could not collect\nLibc version: N/A\n\nPython version: 3.9.12 (main, Apr  5 2022, 01:53:17)  [Clang 12.0.0 ] (64-bit runtime)\nPython platform: macOS-10.16-x86_64-i386-64bit\nIs CUDA available: N/A\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: Could not collect\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: N/A\n\nCPU:\nIntel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n\nVersions of relevant libraries:\n[pip3] flake8==3.9.2\n[pip3] mypy==0.971\n[pip3] mypy-extensions==0.4.3\n[pip3] numpy==1.21.5\n[pip3] numpydoc==1.2\n[conda] blas                      1.0                         mkl\n[conda] mkl                       2021.4.0           hecd8cb5_637\n[conda] mkl-service               2.4.0            py39h9ed2024_0\n[conda] mkl_fft                   1.3.1            py39h4ab4a9b_0\n[conda] mkl_random                1.2.2            py39hb2f4e1b_0\n[conda] numpy                     1.21.5           py39h2e5f0a9_1\n[conda] numpy-base                1.21.5           py39h3b1a694_1\n[conda] numpydoc                  1.2                pyhd3eb1b0_0\n```\n### Output before\n```\nCollecting environment information...\nPyTorch version: N/A\nIs debug build: N/A\nCUDA used to build PyTorch: N/A\nROCM used to build PyTorch: N/A\n\nOS: macOS 13.1 (x86_64)\nGCC version: Could not collect\nClang version: 14.0.0 (clang-1400.0.29.202)\nCMake version: Could not collect\nLibc version: N/A\n\nPython version: 3.9.12 (main, Apr  5 2022, 01:53:17)  [Clang 12.0.0 ] (64-bit runtime)\nPython platform: macOS-10.16-x86_64-i386-64bit\nIs CUDA available: N/A\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: Could not collect\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: N/A\n\nCPU:\nIntel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n\nVersions of relevant libraries:\n[pip3] mypy==0.971\n[pip3] mypy-extensions==0.4.3\n[pip3] numpy==1.21.5\n[pip3] numpydoc==1.2\n[conda] blas                      1.0                         mkl\n[conda] mkl                       2021.4.0           hecd8cb5_637\n[conda] mkl-service               2.4.0            py39h9ed2024_0\n[conda] mkl_fft                   1.3.1            py39h4ab4a9b_0\n[conda] mkl_random                1.2.2            py39hb2f4e1b_0\n[conda] numpy                     1.21.5           py39h2e5f0a9_1\n[conda] numpy-base                1.21.5           py39h3b1a694_1\n[conda] numpydoc                  1.2                pyhd3eb1b0_0\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94573\nApproved by: https://github.com/malfet, https://github.com/kit1980",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                    \"flake8\",",
    "Label": "clean"
},
{
    "Id": 922,
    "Library": "pytorch",
    "Date": "2023/03/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2ea097071a38f3f0bfdc1743fbaf4af418208fc8",
    "Root Cause": "N.A",
    "Bug report": "fix device type bug for custom device (#97213)\n\nFixes #ISSUE_NUMBER\nsupport  the custom renamed device ,@bdhirsh , please review my changes.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97213\nApproved by: https://github.com/bdhirsh, https://github.com/kit1980",
    "Number of deleted lines": 2,
    "Deleted lines": "-                    and self.device.type == \"privateuseone\"\n-            not torch._C._has_storage(self) and self.device.type == \"privateuseone\"",
    "Added lines": "+                    and self.device.type == torch._C._get_privateuse1_backend_name()\n+            not torch._C._has_storage(self)\n+            and self.device.type == torch._C._get_privateuse1_backend_name()",
    "Label": "clean"
},
{
    "Id": 923,
    "Library": "pytorch",
    "Date": "2023/03/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/be49d3b1702c629057560893db20f2a6b6901656",
    "Root Cause": "N.A",
    "Bug report": "[CI] Turn on debug logging for dla102 and gernet_l (#97307)\n\nSummary: Log the generated code for those two flaky tests to see if\nthere is any codegen difference when they fail.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97307\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        if (\n+            args.ci\n+            and args.accuracy\n+            and args.training\n+            and args.only in {\"dla102\", \"gernet_l\"}\n+        ):\n+            # Log generated code for flaky tests, to check if there is any codegen difference\n+            inductor_config.debug = True\n+",
    "Label": "clean"
},
{
    "Id": 924,
    "Library": "pytorch",
    "Date": "2023/03/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dbb31672b2381f2e9b782466c64b454f09f07349",
    "Root Cause": "N.A",
    "Bug report": "Fix the compatible issue of the Dynamo and the PyDev.Debugger. (#96721)\n\nThe PyDev.Debugger use the _PyFrameEvalFunction to debug the python script.\nFallback to the previous _PyFrameEvalFunction to fix the dynamo with PyDev.Debugger issue.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/96721\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 6,
    "Deleted lines": "-  return _PyEval_EvalFrameDefault(tstate, frame, throw_flag);\n-    _PyInterpreterState_SetEvalFrameFunc(\n-        tstate->interp, &custom_eval_frame_shim);\n-      &_PyEval_EvalFrameDefault) {\n-    _PyInterpreterState_SetEvalFrameFunc(\n-        tstate->interp, &_PyEval_EvalFrameDefault);",
    "Added lines": "+static PyObject *(*previous_eval_frame)(PyThreadState *tstate,\n+                                        PyFrameObject *frame, int throw_flag) = NULL;\n+\n+  if (previous_eval_frame) {\n+    return previous_eval_frame(tstate, frame, throw_flag);\n+  }\n+  else {\n+    return _PyEval_EvalFrameDefault(tstate, frame, throw_flag);\n+  }\n+    DEBUG_CHECK(previous_eval_frame == NULL);\n+    previous_eval_frame = _PyInterpreterState_GetEvalFrameFunc(tstate->interp);\n+    _PyInterpreterState_SetEvalFrameFunc(tstate->interp,\n+                                         &custom_eval_frame_shim);\n+      previous_eval_frame) {\n+    DEBUG_CHECK(previous_eval_frame != NULL);\n+    _PyInterpreterState_SetEvalFrameFunc(tstate->interp,\n+                                         previous_eval_frame);\n+    previous_eval_frame = NULL;",
    "Label": "clean"
},
{
    "Id": 925,
    "Library": "pytorch",
    "Date": "2023/03/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ea6113ea20e706891dd3ae93481dfd5545a00a4c",
    "Root Cause": "N.A",
    "Bug report": "Update loss.py (#95367)\n\nFix the dimension bug in the document\n\nFixes #ISSUE_NUMBER\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95367\nApproved by: https://github.com/albanD, https://github.com/kit1980",
    "Number of deleted lines": 1,
    "Deleted lines": "-        >>> input = torch.randn(T, C).log_softmax(2).detach().requires_grad_()",
    "Added lines": "+        >>> input = torch.randn(T, C).log_softmax(1).detach().requires_grad_()",
    "Label": "clean"
},
{
    "Id": 926,
    "Library": "pytorch",
    "Date": "2023/03/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/aeb3db8aa06d081735ae9c49fe6be8308e82d793",
    "Root Cause": "N.A",
    "Bug report": "Back out \"Fixing a bug where allocating a 4GB block results in using 8GB of memory (#95827)\" (#96796)\n\nSummary:\nOriginal commit changeset: a19273017a2a\n\nOriginal Phabricator Diff: D43969564\n\n-----------------------------------------------------------------------------------------------------------------------\n\nTest Plan: unlandayc\n\nReviewed By: terrycsy\n\nDifferential Revision: D44080273\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/96796\nApproved by: https://github.com/jianyuh, https://github.com/davidberard98",
    "Number of deleted lines": 35,
    "Deleted lines": "-// allocations above this size will not be rounded up to the next power of two\n-const size_t kMaxRoundThreshold = 32 * 1024 * 1024;\n-\n-// allocations above this size will not be cached\n-const size_t kMaxCachedSize = kMaxRoundThreshold;\n-\n-    size_t rounded_size = size;\n-    if (size < kMaxRoundThreshold) {\n-        rounded_size = c10::llvm::PowerOf2Ceil(size);\n-    }\n-\n-        &ptr, rounded_size, cudaHostAllocDefault));\n-    block->size_ = rounded_size;\n-      if (block->size_ <= kMaxCachedSize) {\n-        std::lock_guard<std::mutex> g(free_list_mutex_);\n-        free_list_.insert(block);\n-      } else {\n-        std::lock_guard<std::mutex> g(blocks_mutex_);\n-        delete_block(block);\n-      }\n-      delete_block(block);\n-  void delete_block(Block* block) {\n-    blocks_.erase(block);\n-    ptr_to_block_.erase(block->ptr_);\n-    AT_CUDA_CHECK(cudaFreeHost(block->ptr_));\n-    delete block;\n-  }\n-\n-        if (block->size_ <= kMaxCachedSize) {\n-          std::lock_guard<std::mutex> g(free_list_mutex_);\n-          free_list_.insert(block);\n-        } else {\n-          std::lock_guard<std::mutex> g(blocks_mutex_);\n-          delete_block(block);\n-        }",
    "Added lines": "+        &ptr, c10::llvm::PowerOf2Ceil(size), cudaHostAllocDefault));\n+    block->size_ = c10::llvm::PowerOf2Ceil(size);\n+      std::lock_guard<std::mutex> g(free_list_mutex_);\n+      free_list_.insert(block);\n+      blocks_.erase(block);\n+      ptr_to_block_.erase(block->ptr_);\n+      AT_CUDA_CHECK(cudaFreeHost(block->ptr_));\n+      delete block;\n+        std::lock_guard<std::mutex> g(free_list_mutex_);\n+        free_list_.insert(block);",
    "Label": "clean"
},
{
    "Id": 927,
    "Library": "pytorch",
    "Date": "2023/03/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/577d930c39e3db6eb7bab2c24be07b1e90d94244",
    "Root Cause": "N.A",
    "Bug report": "[CI] Revert https://github.com/pytorch/pytorch/pull/96195 (#96897)\n\nSummary: https://github.com/pytorch/pytorch/pull/96195 was an experiment\nfor debugging flaky failures on CI.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/96897\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 2,
    "Deleted lines": "-        if args.inductor and args.accuracy:\n-            torch._inductor.config.compile_threads = 1",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 928,
    "Library": "pytorch",
    "Date": "2023/03/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/707d8925646d2f42dedcde6be2f8662ba98d04b2",
    "Root Cause": "N.A",
    "Bug report": "Debug logging around DDP mixed precision copies (#96438)\n\nPer title\n\nDifferential Revision: [D43859976](https://our.internmc.facebook.com/intern/diff/D43859976/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/96438\nApproved by: https://github.com/zhaojuanmao",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            RECORD_FUNCTION(\n+                \"torch::distributed::reducer::mul_out\",\n+                std::vector<c10::IValue>({bucket_view}))\n+            RECORD_FUNCTION(\n+                \"torch::distributed::reducer::copy_\",\n+                std::vector<c10::IValue>({bucket_view}))\n+          RECORD_FUNCTION(\n+              \"torch::distributed::reducer::copy_\",\n+              std::vector<c10::IValue>({bucket_view}))",
    "Label": "clean"
},
{
    "Id": 929,
    "Library": "pytorch",
    "Date": "2023/03/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8bce88d9de5ee54e6d6136dddc13a7377c426814",
    "Root Cause": "N.A",
    "Bug report": "[caffe2] dont call cudnnDestroy on thread exit (crashes on windows with cuda 11/12) (#95382)\n\nSummary:\nMy team has been hitting a mysterious crash for a few months on a windows binary that uses Caffe2 inside a worker thread.\n\nWhen this thread gets destroyed, there is an error at this line in context_gpu.h where the state of this operation gives CUDNN_STATUS_INTERNAL_ERROR instead of CUDNN_STATUS_SUCCESS.\n\nWhen enabling cudnn debug logs (via the env variables nvidia specifies), I can see that the context is destroyed twice, even though this code only destroys it once, so something mysterious is causing a double free.\n\nThis seems very very similar to the issue/fix described here for pytorch:\nhttps://github.com/pytorch/pytorch/issues/17658\nhttps://github.com/apache/tvm/pull/8267\n\nAnd pytorch handles this in the same way, by just not calling cudnnDestroy\n\nThis seems to have become an issue with cuda11, but I tested cuda12 as well and found that the issue persists so this needs to be somehow fixed.\n\nTest Plan:\nCI\n\nI checked that the specific windows binary I am using is able to create and drestroy caffe2-invoking threads without causing the application to crash.\n\nbuck run arvr/mode/win/cuda11/opt //arvr/projects/nimble/prod/tools/MonoHandTrackingVis\n\nDifferential Revision: D43538017\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95382\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#ifdef _WIN32\n+        // this is because of something dumb in the ordering of\n+        // destruction. Sometimes at exit, the cuda context would already\n+        // be destroyed by the time this gets destroyed. This happens on\n+        // windows with cuda 11 and cuda 12.\n+        cudnnDestroy(element.second);\n+#else\n+#endif // _WIN32",
    "Label": "clean"
},
{
    "Id": 930,
    "Library": "pytorch",
    "Date": "2023/03/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e50ff3fcdb3890ce3bbab99e60b1c27ff49be2af",
    "Root Cause": "N.A",
    "Bug report": "Fix kernel name bug (#95739)\n\n[T146374491](https://www.internalfb.com/intern/tasks/?t=146374491): [Inductor] Descriptive kernel names not displaying in trace\n\nUse the descriptive kernel name for the triton function name if indicated in the config\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95739\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 1,
    "Deleted lines": "-            subs_name = kernel_name if config.triton.ordered_kernel_names else \"triton_\"",
    "Added lines": "+            subs_name = (\n+                kernel_name\n+                if config.triton.ordered_kernel_names\n+                or config.triton.descriptive_kernel_names\n+                else \"triton_\"\n+            )",
    "Label": "clean"
},
{
    "Id": 931,
    "Library": "pytorch",
    "Date": "2023/02/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c44a7330189fbbad944d08437433cd5248214052",
    "Root Cause": "N.A",
    "Bug report": "Fix split_module bug (#95493)\n\nSummary: Title, the mapping currently has lots of unused keys due to the condition or always return True, but it will not affect the correctness.\n\nTest Plan: N/A\n\nDifferential Revision: D43579510\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95493\nApproved by: https://github.com/Skylion007",
    "Number of deleted lines": 2,
    "Deleted lines": "-            if node.op == \"placeholder\" or \"get_attr\":\n-                org_mod_env[node.name] = node",
    "Added lines": "+            org_mod_env[node.name] = node",
    "Label": "clean"
},
{
    "Id": 932,
    "Library": "pytorch",
    "Date": "2023/02/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/407b0f321480d6ac41e26dc330853d68c18f028a",
    "Root Cause": "N.A",
    "Bug report": "fix for debug crash build (#95464)\n\nFixes https://github.com/pytorch/pytorch/issues/94376\n\n\u26a0\ufe0f Hacky fix\n\nDetails about use of `noop_vtable`:\nhttps://github.com/pytorch/pytorch/blob/d677432b706904f84b08bfee5d8bec7c4e220894/c10/core/impl/PyInterpreter.h#L92-L102\n\nCurrently, at destruction, `noop_vtable` goes out of scope first while there are dangling references to the object still present with other objects like `PythonKernelHolder` which is held by the singleton `Dispatcher`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95464\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 2,
    "Deleted lines": "-  // Intentionally leaked\n-  static NoopPyInterpreterVTable noop_vtable;",
    "Added lines": "+// Construct this in Global scope instead of within `disarm`\n+// where it will be only initialized first time `disarm` is called.\n+// This increases the likelihood `noop_vtable` lives longer than\n+// any object that refers to it.\n+\n+// If `noop_vtable` goes out of scope first, other objects will have dangling\n+// reference to it.\n+static NoopPyInterpreterVTable noop_vtable;\n+",
    "Label": "clean"
},
{
    "Id": 933,
    "Library": "pytorch",
    "Date": "2023/02/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cba8b12fa7ea6093a8be7605e1da6b9b8e78f31b",
    "Root Cause": "N.A",
    "Bug report": "[quant][bug fix] Fix qrange_len in `torch.ao.quantization.utils.py` (#95297)\n\nSummary:\n\nIt looks like there is a typo and qrange_len should be 2^32 instead of 2^31, as it is currently set.\n\nTest Plan:\n```\npython test/test_quantization.py TestObserver.test_per_tensor_observers\n\n```\n\nReviewers:\n\nSubscribers:\n\nTasks: https://github.com/pytorch/pytorch/issues/95295\n\nTags:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95297\nApproved by: https://github.com/vkuzo",
    "Number of deleted lines": 2,
    "Deleted lines": "-            initial_quant_min, initial_quant_max = 0, 2**31 - 1\n-                0 < qrange_len <= 2**31",
    "Added lines": "+            initial_quant_min, initial_quant_max = 0, 2**32 - 1\n+                0 < qrange_len <= 2**32",
    "Label": "clean"
},
{
    "Id": 934,
    "Library": "pytorch",
    "Date": "2023/02/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d4882a9445ea90cbd38e24ed204d00d1b02fd5e0",
    "Root Cause": "N.A",
    "Bug report": "Make the cuda device assert error message clearer (#95360)\n\nSummary: Easier to debug\n\nTest Plan: CI\n\nDifferential Revision: D43525303\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95360\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 3,
    "Deleted lines": "-  AT_ASSERT(device >= 0 && device < num_gpus);\n-  AT_ASSERT(device >= 0 && device < num_gpus);\n-  AT_ASSERT(peer_device >= 0 && peer_device < num_gpus);",
    "Added lines": "+  AT_ASSERT(device >= 0 && device < num_gpus, \"device=\", device, \", num_gpus=\", num_gpus);\n+  AT_ASSERT(device >= 0 && device < num_gpus, \"device=\", device, \", num_gpus=\", num_gpus);\n+  AT_ASSERT(peer_device >= 0 && peer_device < num_gpus, \"peer_device=\", peer_device, \", num_gpus=\", num_gpus);",
    "Label": "clean"
},
{
    "Id": 935,
    "Library": "pytorch",
    "Date": "2023/02/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8de4238a31b87707594dfe345a30bf561f673c27",
    "Root Cause": "N.A",
    "Bug report": "Add dynamo bench arg --per_process_memory_fraction (#95260)\n\nSimply pipes the arg to the existing torch.cuda API by the same name.\n\nUseful for locally debugging OOMs that happened on a smaller GPU.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95260\nApproved by: https://github.com/davidberard98",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    parser.add_argument(\n+        \"--per_process_memory_fraction\",\n+        type=float,\n+        default=1,\n+        help=\"Set per-process GPU memory fraction (limit) for reducing usable size and reproducing OOMs\",\n+    )\n+            if args.per_process_memory_fraction != 1:\n+                torch.cuda.set_per_process_memory_fraction(\n+                    args.per_process_memory_fraction\n+                )\n+",
    "Label": "clean"
},
{
    "Id": 936,
    "Library": "pytorch",
    "Date": "2023/02/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c137d3d688d2fae9cef235cfa26d9c556a77ee0f",
    "Root Cause": "N.A",
    "Bug report": "inductor: enable lowering for bitwise_right_shift (#94997)\n\ntriton pin has been moved past the relevant bug fix.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94997\nApproved by: https://github.com/Skylion007, https://github.com/jansel",
    "Number of deleted lines": 7,
    "Deleted lines": "-# TODO(fdrocha): this should be removed once the register_pointwise(aten.bitwise_right_shift) below is uncommented\n-make_fallback(aten.bitwise_right_shift, warn=False)\n-\n-\n-# TODO(fdrocha): once https://github.com/openai/triton/pull/1153 is merged and we advance the triton pin past it\n-# this should be uncommented\n-# register_pointwise(aten.bitwise_right_shift)",
    "Added lines": "+register_pointwise(aten.bitwise_right_shift)",
    "Label": "clean"
},
{
    "Id": 937,
    "Library": "pytorch",
    "Date": "2023/02/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/30d0112bf34971d8fcd447ce3b2c4b1630d1e7d6",
    "Root Cause": "N.A",
    "Bug report": "fix performance issue in torch.sparse.mm reduce mode (#94969)\n\nFix performance bug for `torch.sparse.mm()` with reduce flag.\n\nFound this bug within internal benchmarking.\nMade a mistake when updating previous patch which causes load imbalance between threads:\n\nTest on ogbn-products datasets on Xeon CLX with 24 cores:\n\n#### before\n```\nsparse.mm: mean: 1156.148 ms\nsparse.mm: sum: 1163.754 ms\nsparse.mm: (using mkl): 703.227 ms\n```\n\n#### after\n```\nsparse.mm: mean: 662.578 ms\nsparse.mm: sum: 662.301 ms\nsparse.mm: (using mkl): 700.178 ms\n```\n\nThe result also indicates that the current spmm kernel is no worse than MKL's sparse_mm .\n\nAlso update results on `pyg benchmark` with:\n```\npython gnn.py --use_sage --epochs=3 --runs=1 --inference\n```\n\n* Out of box: `13.32s`\n* Without the fix in this PR: `5.87s`\n* With the fix in this PR: `3.19s`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94969\nApproved by: https://github.com/jgong5",
    "Number of deleted lines": 1,
    "Deleted lines": "-  int64_t nnz = other_.numel();",
    "Added lines": "+  int64_t nnz = values.numel();",
    "Label": "clean"
},
{
    "Id": 938,
    "Library": "pytorch",
    "Date": "2023/02/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a863d5e37c65f325bb6ec08a1cf9467ae2a4ce96",
    "Root Cause": "N.A",
    "Bug report": "Hide failing merge rule's name in the internal debugging section (#94932)\n\nFixes https://github.com/pytorch/test-infra/issues/1081\n\nThe merge rule name is not helpful to most readers, and most of the time it's just \"superuser.\"  Move this to a less prominent place in the \"Details for Dev Infra team\" section\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94932\nApproved by: https://github.com/huydhn",
    "Number of deleted lines": 8,
    "Deleted lines": "-class MandatoryChecksMissingError(Exception):\n-                reject_reason = f\"PR #{pr.pr_num} has not been reviewed yet (Rule {rule_name})\"\n-                    f\"Approval needed from one of the following (Rule '{rule_name}'):\",\n-                    f\"{len(failed_checks)} mandatory check(s) failed (Rule `{rule_name}`).  The first few are:\",\n-                    f\"{len(pending_checks)} mandatory check(s) are pending/not yet run (Rule `{rule_name}`).  The first few are:\",\n-    raise RuntimeError(reject_reason)\n-            internal_debugging = \"\\n\".join((\n-            ))",
    "Added lines": "+class MergeRuleFailedError(RuntimeError):\n+class MandatoryChecksMissingError(MergeRuleFailedError):\n+    pass\n+\n+                reject_reason = f\"PR #{pr.pr_num} has not been reviewed yet\"\n+                    \"Approval needed from one of the following:\",\n+                    f\"{len(failed_checks)} mandatory check(s) failed.  The first few are:\",\n+                    f\"{len(pending_checks)} mandatory check(s) are pending/not yet run.  The first few are:\",\n+    raise MergeRuleFailedError(reject_reason, rule)\n+        failing_rule = None\n+        if (isinstance(e, MergeRuleFailedError)):\n+            failing_rule = e.rule.name if e.rule else None\n+\n+            internal_debugging = \"\\n\".join(line for line in (\n+                f\"Failing merge rule: {failing_rule}\" if failing_rule else \"\",\n+            ) if line)  # ignore empty lines during the join",
    "Label": "clean"
},
{
    "Id": 939,
    "Library": "pytorch",
    "Date": "2023/02/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8da776e3a716091aaa1bfab774badd1d1ea15f3c",
    "Root Cause": "N.A",
    "Bug report": "[FSDP] Fix \"use-after-free\" in reshard logic (#94859)\n\n**Overview**\nThis PR switches the order of freeing the unsharded `FlatParameter` (`self._free_unsharded_flat_param()`) and switching to use the sharded `FlatParameter` (`self._use_sharded_flat_param()`). This is to prevent \"use-after_free\"-type bugs where for `param.data = new_data`, `param` has its metadata intact but not its storage, causing an illegal memory access for any instrumentation that depends on its storage. (`param` is an original parameter and `new_data` is either a view into the sharded `FlatParameter` or `torch.empty(0)` depending on the sharding and rank.)\n\n**Details**\nTo see why simply switching the order of the two calls is safe, let us examine the calls themselves:\nhttps://github.com/pytorch/pytorch/blob/652457b1b738f710679b414fe4626d08c9a9e0db/torch/distributed/fsdp/flat_param.py#L1312-L1339\n\nhttps://github.com/pytorch/pytorch/blob/652457b1b738f710679b414fe4626d08c9a9e0db/torch/distributed/fsdp/flat_param.py#L1298-L1310\n\n- `_free_unsharded_flat_param()` does not make any assumption that `self.flat_param`'s data is the sharded `FlatParameter` (i.e. `_local_shard`).\n- The sharded `FlatParameter` (i.e. `_local_shard`) is always present in memory, which means that FSDP can use sharded views at any time, including before freeing the unsharded data.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94859\nApproved by: https://github.com/zhaojuanmao, https://github.com/fegin",
    "Number of deleted lines": 1,
    "Deleted lines": "-        self._use_sharded_flat_param()",
    "Added lines": "+        # Switch to the sharded `FlatParameter` before freeing to prevent\n+        # \"use-after-free\"-type bugs with external profiling tools, where for\n+        # `use_orig_params=True`, the `param` does not point to valid memory\n+        # when setting `param.data = ...` in `_use_sharded_views()`.\n+        self._use_sharded_flat_param()",
    "Label": "clean"
},
{
    "Id": 940,
    "Library": "pytorch",
    "Date": "2023/02/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/59e875667611054a5dfd76854140cd70de45051e",
    "Root Cause": "N.A",
    "Bug report": "[MPS] Fix the Channels last bug with GradientWithInput. (#94384)\n\n* Fix the Channels last bug with GradientWithInput.\nThe bug was mentioned in :\nhttps://github.com/pytorch/pytorch/issues/77764#issuecomment-1312241902\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94384\nApproved by: https://github.com/razarmehr",
    "Number of deleted lines": 14,
    "Deleted lines": "-    IntArrayRef input_size, const Tensor& grad_output_t, const Tensor& weight_t,\n-  TensorArg grad_output{ grad_output_t, \"grad_output\", 1 },\n-            weight{ weight_t, \"weight\", 2 };\n-  auto memory_format = grad_output_t.suggest_memory_format();\n-\n-  auto grad_input_t = at::empty(\n-                    input_size,\n-                    grad_output->scalar_type(),\n-                    c10::nullopt,\n-                    kMPS,\n-                    c10::nullopt,\n-                    c10::nullopt);\n-          MPSGraphTensor* weightTensor = native_mps::mpsGraphRankedPlaceHolder(mpsGraph, weight_t);\n-    auto weightsPlaceholder = Placeholder(cachedGraph->weightTensor_, weight_t);",
    "Added lines": "+    IntArrayRef input_size, const Tensor& grad_output_, const Tensor& weight_,\n+  TensorArg grad_output{ grad_output_, \"grad_output\", 1 },\n+            weight{ weight_, \"weight\", 2 };\n+  auto memory_format = grad_output_.suggest_memory_format();\n+  Tensor grad_output_t = grad_output_.contiguous(memory_format);\n+  Tensor weight_t = weight_.contiguous(memory_format);\n+  MPSShape* weightShape = getMPSShape(weight_);\n+  auto grad_input_t = at::empty( input_size, grad_output_t.options(), c10::nullopt);\n+          MPSGraphTensor* weightTensor = native_mps::mpsGraphRankedPlaceHolder(mpsGraph, native_mps::getMPSScalarType(weight_t.scalar_type()), weightShape);\n+    auto weightsPlaceholder = Placeholder(cachedGraph->weightTensor_, weight_t, weightShape);",
    "Label": "clean"
},
{
    "Id": 941,
    "Library": "pytorch",
    "Date": "2023/02/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3693039bb70828828b90b82191c16aa01547482f",
    "Root Cause": "N.A",
    "Bug report": "perf: fix missing noexcepts on minpybind in functorch (#94135)\n\nNoticed this performance bug in functorch. We got a pretty big perf in pybind11 improvement by explicitly marking at noexcept, see https://quuxplusone.github.io/blog/2022/08/26/vector-pessimization/\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94135\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 9,
    "Deleted lines": "-    handle()\n-    : ptr_(nullptr) {}\n-    PyObject * ptr_;\n-    hdl(obj<T> o)\n-    object(object&& other)\n-    object& operator=(object&& other) {\n-    obj(obj&& other)\n-    obj& operator=(obj&& other) {\n-        PyObject *k, *v;",
    "Added lines": "+    handle() = default;\n+    PyObject* ptr_ = nullptr;\n+    hdl(const obj<T>& o)\n+    object(object&& other) noexcept\n+    object& operator=(object&& other) noexcept {\n+    obj(obj&& other) noexcept\n+    obj& operator=(obj&& other) noexcept {\n+        PyObject *k = nullptr, *v = nullptr;",
    "Label": "clean"
},
{
    "Id": 942,
    "Library": "pytorch",
    "Date": "2023/02/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/68b06ee4d46b727c5378864f6a3db2a8a4256e8c",
    "Root Cause": "N.A",
    "Bug report": "Add `torch_compile_debug/` to .gitignore  (#93889)\n\n# Summary\nI have almost checked this in multiple times. Add to gitignore.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/93889\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+torch_compile_debug/",
    "Label": "clean"
},
{
    "Id": 943,
    "Library": "pytorch",
    "Date": "2023/01/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2a31c3589bb62ebaac3b3ec30c455668743f1893",
    "Root Cause": "N.A",
    "Bug report": "Report suppressed exception in minifier (#93368)\n\nSuppressing exceptions is bad!  If you're debugging PyTorch itself\nyou want to see the exception so you can do something about it.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/93368\nApproved by: https://github.com/Skylion007, https://github.com/mlazos, https://github.com/bdhirsh",
    "Number of deleted lines": 4,
    "Deleted lines": "-        log.warning(\n-                \"While minifying the program in accuracy minification mode,\"\n-        log.warning(\n-                \"While minifying the program in accuracy minification mode,\"",
    "Added lines": "+        log.exception(\n+                \"While minifying the program in accuracy minification mode, \"\n+        log.exception(\n+                \"While minifying the program in accuracy minification mode, \"",
    "Label": "clean"
},
{
    "Id": 944,
    "Library": "pytorch",
    "Date": "2023/01/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/44a948c82000dd7cf4cf23ff3aa0da92ff6812ef",
    "Root Cause": "N.A",
    "Bug report": "Fix MSVC compiler error in basic_ops.h (#93322)\n\nhttps://github.com/pytorch/pytorch/pull/93069 introduces a compiler error in some internal Windows builds using MSVC:\n\n```\nstderr: d:\\full-fbsource\\xplat\\caffe2\\torch\\csrc\\autograd\\functions\\basic_ops.h(43): fatal error C1001: An internal error has occurred in the compiler.\n```\nThis may be related to older versions of MSVC not recognizing the `[[maybe-unused]]` attribute: https://developercommunity.visualstudio.com/t/compiler-bug-on-parsing-maybe-unused-in-range-base/209488. This PR reverts the changes in `basic_ops.h` that resolves those errors.\n\nVerified this fixes the internal jobs, and landed as [D42854205](https://www.internalfb.com/diff/D42854205).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/93322\nApproved by: https://github.com/Skylion007, https://github.com/albanD",
    "Number of deleted lines": 1,
    "Deleted lines": "-    for ([[maybe_unused]] const auto _ : c10::irange(num_inputs)) {",
    "Added lines": "+    // NOLINTNEXTLINE(clang-analyzer-deadcode.DeadStores)\n+    for (const auto i : c10::irange(num_inputs)) {\n+      (void)i; // Suppress unused variable warning",
    "Label": "clean"
},
{
    "Id": 945,
    "Library": "pytorch",
    "Date": "2023/01/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c499e760f5f2ae79b364a6b13805364d2a49e425",
    "Root Cause": "N.A",
    "Bug report": "[XNNPACK] Enable Memopt for OSS (#93097)\n\nSummary:\nD38543798\n\nEnabled Memopt previously to fix a bug with memory planner\n\nMirroring the changes we made Internally to OSS\n\nTest Plan: OSS CI\n\nReviewed By: digantdesai\n\nDifferential Revision: D42782958\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/93097\nApproved by: https://github.com/digantdesai",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            \"-DXNN_ENABLE_MEMOPT\",",
    "Label": "clean"
},
{
    "Id": 946,
    "Library": "pytorch",
    "Date": "2023/01/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5105a8d3fccc80ee1e22fc8f1257e0a5f3d844f1",
    "Root Cause": "N.A",
    "Bug report": "Enable Kineto in OSS builds by fixing build condition (resubmit) (#93033)\n\nResubmit of https://github.com/pytorch/pytorch/pull/89174 . I think I fixed underlying issues back then, but only CI would tell.\n\nContext: This PR enables Kineto on OSS builds because of how the flags were misconfigured before. I think generally having global observer in OSS is nice. There's some work to release on demand profiling with dynolog, and right now its build instructions start with \"go change pytorch's CMake\": https://github.com/facebookincubator/dynolog/blob/main/docs/pytorch_profiler.md#pytorch-setup\n\nThe previous PR was reverted because of the bug in Kineto that got fixed in https://github.com/pytorch/kineto/pull/696 (and the submodule was updated since)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/93033\nApproved by: https://github.com/kimishpatel",
    "Number of deleted lines": 1,
    "Deleted lines": "-if(USE_LITE_INTERPRETER_PROFILER)",
    "Added lines": "+if(BUILD_LITE_INTERPRETER AND USE_LITE_INTERPRETER_PROFILER)",
    "Label": "clean"
},
{
    "Id": 947,
    "Library": "pytorch",
    "Date": "2023/01/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/020c0d589529bbb96714f582d551b90f37c4ea35",
    "Root Cause": "N.A",
    "Bug report": "Add debugability comments to DDPOptimizer (#89802)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89802\nApproved by: https://github.com/davidberard98",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    Debugging\n+     - Generally, it is easiest to debug DDPOptimizer in a single process program, using pdb.\n+     - In many cases, the log messages are helpful (they show bucket size assignments)-\n+       just configure torch._dynamo.config.log_level to info or debug.\n+     - See `benchmarks/dynamo/distributed.py` for a simple harness that will run a toy model or a torchbench model\n+       in a single process (or with torchrun, in multiple processes)\n+",
    "Label": "clean"
},
{
    "Id": 948,
    "Library": "pytorch",
    "Date": "2023/01/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/764f79f6804a57a75d9d4da97824b4f57b638ef5",
    "Root Cause": "N.A",
    "Bug report": "[Microbenchmark] microbench fix for triton template (#92282)\n\nFixes microbench bug due to triton template https://github.com/pytorch/pytorch/pull/91575\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92282\nApproved by: https://github.com/jgong5, https://github.com/desertfire, https://github.com/jansel",
    "Number of deleted lines": 8,
    "Deleted lines": "-    if inductor_config.triton.mm == \"aten\" and operator in (\n-        aten.mm.default,\n-        aten.bmm.default,\n-        aten.addmm.default,\n-        aten.matmul.default,\n-    ):\n-        return True\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 949,
    "Library": "pytorch",
    "Date": "2023/01/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/77b8aa6e439fc5c3cf1eeeaff3f705c54c150a2e",
    "Root Cause": "N.A",
    "Bug report": "Wrap a few more functions to ease their tracking during debugging (#92004)\n\nYup\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92004\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 1,
    "Deleted lines": "-from functools import lru_cache",
    "Added lines": "+from functools import lru_cache, wraps\n+    @wraps(func)\n+    @wraps(func)",
    "Label": "clean"
},
{
    "Id": 950,
    "Library": "pytorch",
    "Date": "2022/12/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fb4fc0dabea6ed2b1ca224628e18d0e752ba81d3",
    "Root Cause": "N.A",
    "Bug report": "[CUDA] Bump version requirement for CUDA Graphs debug dump function (#91429)\n\n#91417\n\nCC @ptrblck @vors\nPull Request resolved: https://github.com/pytorch/pytorch/pull/91429\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 2,
    "Deleted lines": "-#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000\n-  TORCH_CHECK(false, \"CUDA graphs may only be used in Pytorch built with CUDA >= 11.0 and is not yet supported on ROCM\");",
    "Added lines": "+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11030\n+  TORCH_CHECK(false, \"CUDA graphs debug dump may only be used in Pytorch built with CUDA >= 11.3 and is not yet supported on ROCM\");",
    "Label": "clean"
},
{
    "Id": 951,
    "Library": "pytorch",
    "Date": "2022/12/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/878719a2db1088c6c4a1e9562ff59422019efa5b",
    "Root Cause": "N.A",
    "Bug report": "initialise the members boolean_ and integer_ of at::indexing::TensorIndex (#91399)\n\ninitialise the members boolean_ and integer_ of at::indexing::TensorIndex to false and 0 respectively, because the compiler generated copy-ctor accesses them which is UB.  This resolves a compile time warning, a runtime error from UBSan + gcc, and a runtime error from MSVC when compiling debug.\n\nFixes #90951\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/91399\nApproved by: https://github.com/bdhirsh",
    "Number of deleted lines": 2,
    "Deleted lines": "-  int64_t integer_;\n-  bool boolean_;",
    "Added lines": "+  int64_t integer_ = 0;\n+  bool boolean_ = false;",
    "Label": "clean"
},
{
    "Id": 952,
    "Library": "pytorch",
    "Date": "2022/12/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0476201482dfdbbb4afbfcc7dae65fafbb2b9c19",
    "Root Cause": "N.A",
    "Bug report": "Update debug option for torch._dynamo (#91223)\n\nSeems outdated from https://www.youtube.com/watch?v=egZB5Uxki0I\n\nFixes #ISSUE_NUMBER\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/91223\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 1,
    "Deleted lines": "-   torchdynamo.config.debug = True",
    "Added lines": "+   torch._dynamo.config.output_code = True",
    "Label": "clean"
},
{
    "Id": 953,
    "Library": "pytorch",
    "Date": "2022/12/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c248f2f3793b7dfd1cffb6c4679c16a3baeeb805",
    "Root Cause": "N.A",
    "Bug report": "[ROCm] Modify GPUs visibility code when starting docker container (#91031)\n\nUse ROCR_VISIBLE_DEVICES to limit GPU visibility, in preparation for CI node upgrade to ROCm5.3 KFD and UB22.04.\n\n### PROBLEM\nAfter upgrading some of our CI nodes to UB22.04 and ROCm5.3KFD, rocminfo doesn't work inside the docker container if we use the following flags: `--device=/dev/dri/renderD128 --device=/dev/dri/renderD129`. It gives the error:\n\n```\n+ rocminfo\nROCk module is loaded\nFailed to set mem policy for GPU [0x6b0d]\nhsa api call failure at: /long_pathname_so_that_rpms_can_package_the_debug_info/src/rocminfo/rocminfo.cc:1140\nCall returned HSA_STATUS_ERROR_OUT_OF_RESOURCES: The runtime failed to allocate the necessary resources. This error may also occur when the core runtime library needs to spawn threads or create internal OS-specific events.\n```\n\n### WORKAROUND\nUse `--device=/dev/dri` instead, and use `ROCR_VISIBLE_DEVICES` to limit GPU visibility inside container.\n\n### BACKGROUND OF ORIGINAL CODE\nWe introduced these flags to prepare for 2 runners per CI node, to split up the GPU visibility among the runners: https://github.com/pytorch/pytorch/blame/master/.github/actions/setup-rocm/action.yml#L58\nThat effort - 2 runners per CI node - is still pending, and we might need to revisit this patch when we try to enable that.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/91031\nApproved by: https://github.com/jeffdaily, https://github.com/malfet",
    "Number of deleted lines": 2,
    "Deleted lines": "-            echo \"GPU_FLAG=--device=/dev/mem --device=/dev/kfd --device=/dev/dri/renderD130 --device=/dev/dri/renderD131 --group-add video --group-add daemon\" >> \"${GITHUB_ENV}\"\n-            echo \"GPU_FLAG=--device=/dev/mem --device=/dev/kfd --device=/dev/dri/renderD128 --device=/dev/dri/renderD129 --group-add video --group-add daemon\" >> \"${GITHUB_ENV}\"",
    "Added lines": "+            echo \"GPU_FLAG=--device=/dev/mem --device=/dev/kfd --device=/dev/dri --group-add video --group-add daemon -e ROCR_VISIBLE_DEVICES=2,3\" >> \"${GITHUB_ENV}\"\n+            echo \"GPU_FLAG=--device=/dev/mem --device=/dev/kfd --device=/dev/dri --group-add video --group-add daemon -e ROCR_VISIBLE_DEVICES=0,1\" >> \"${GITHUB_ENV}\"",
    "Label": "clean"
},
{
    "Id": 954,
    "Library": "pytorch",
    "Date": "2022/12/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/07c340bb2aecb2a4ff2bc3e2f057f1f11cb9139c",
    "Root Cause": "N.A",
    "Bug report": "Remove debug code (#91148)\n\nRemoves some debug code\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/91148\nApproved by: https://github.com/desertfire, https://github.com/williamwen42",
    "Number of deleted lines": 6,
    "Deleted lines": "-        with open(\"in1.txt\", \"w\") as f:\n-            print(example_inputs, file=f)\n-            print(list(model.parameters()), file=f)\n-\n-            with open(\"out1.txt\", \"w\") as f:\n-                print(fp64_outputs, file=f)",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 955,
    "Library": "pytorch",
    "Date": "2022/12/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d19791e4cd09688bc03aa8af5662765d787abbe2",
    "Root Cause": "N.A",
    "Bug report": "add autocast keys to pybind11 DispatchKey object (#90821)\n\nSummary:\n\nThis is useful for debugging what autocast is doing when\nit's running on top of torchdynamo, without this the Python dispatch\nkey for autocast prints as `???`.\n\nTest Plan:\n\n```\nimport torch\ndir(torch._C.DispatchKey)\n// the autocast keys show up now\n```\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90821\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      DEF_ONE(AutocastCPU)\n+      DEF_ONE(AutocastXPU)\n+      DEF_ONE(AutocastHPU)\n+      DEF_ONE(AutocastCUDA)",
    "Label": "clean"
},
{
    "Id": 956,
    "Library": "pytorch",
    "Date": "2022/12/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0b3316ad2c6ff61416597ef29e8865876dcb12f5",
    "Root Cause": "N.A",
    "Bug report": "Don't enable debug_fake_crossref for TORCH_COMPILE_DEBUG (#90666)\n\nIt is kind of flaky, it doesn't work with dynamic shapes, and I think the debug interpreter is a better way to detect if you've had a size/stride propagation accident.\n\nFixes https://github.com/pytorch/pytorch/issues/90652\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90666\nApproved by: https://github.com/voznesenskym",
    "Number of deleted lines": 1,
    "Deleted lines": "-    stack.enter_context(patch(\"functorch.compile.config.debug_fake_cross_ref\", True))",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 957,
    "Library": "pytorch",
    "Date": "2022/12/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b95ea4f1494dacf86b5789f5466025b1fac1f2bb",
    "Root Cause": "N.A",
    "Bug report": "[pt2] Reset dynamo log level when exiting inductor debug context (#90473)\n\nWhen entering an inductor debug context we increase the log level of\ndynamo; I guess this makes sense, since if we're debugging inductor, and\ninductor calls into dynamo, we probably want visibility into what dynamo is\ndoing.\n\nBut when we exit that context, we probably want to go back to whatever level of\ndynamo-specific logging was in place before.  Dynamo generates lots of debug\ninfo (guards, bytecode), and it's a lot to sift through if you're not\nspecifically interested in it.\n\nDifferential Revision: [D41841879](https://our.internmc.facebook.com/intern/diff/D41841879/)\n\nDifferential Revision: [D41841879](https://our.internmc.facebook.com/intern/diff/D41841879)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90473\nApproved by: https://github.com/mlazos, https://github.com/jansel",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+            def reset_log_level(level):\n+                dynamo_config.log_level = level\n+\n+            self._stack.callback(reset_log_level, dynamo_config.log_level)",
    "Label": "clean"
},
{
    "Id": 958,
    "Library": "pytorch",
    "Date": "2022/12/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/184f6b57878e16e0ac14abc5f98b9f0a8845fd7e",
    "Root Cause": "N.A",
    "Bug report": "Fix perf bug in #90528 (#90630)\n\nFixes a minor I noticed in #90528 also a follow up to #89000. @ezyang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90630\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 2,
    "Deleted lines": "-    T storage_offset) {\n-  self_->set_sizes_and_strides(size, stride, c10::make_optional(storage_offset));",
    "Added lines": "+    T&& storage_offset) {\n+  self_->set_sizes_and_strides(size, stride, c10::make_optional(std::forward<T>(storage_offset)));",
    "Label": "clean"
},
{
    "Id": 959,
    "Library": "pytorch",
    "Date": "2022/11/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/24b3b73c98110e59687b524ca208795c3f3e8ff2",
    "Root Cause": "N.A",
    "Bug report": "[Caffe2] Fix merge logic bug (#89551)\n\nSummary: `ExprGroup::getMergeCandidates()` had a logic bug. The vector being initialized had its arguments mis-ordered. This didn't trigger a build warning because the warning about implicit cast from an integral type to `bool` wasn't enabled.\n\nTest Plan: `buck test fbsource//arvr/mode/win/vs2019/cuda11/opt fbsource//arvr/mode/hybrid_execution //arvr/libraries/neural_net_inference/TorchScript/...`\n\nDifferential Revision: D41488939\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89551\nApproved by: https://github.com/davidberard98, https://github.com/jjsjann123",
    "Number of deleted lines": 2,
    "Deleted lines": "-  std::vector<bool> can_merge(true, neighbors.size());\n-  // Find neighbors with a level that is only 1 differant than this groups level",
    "Added lines": "+  std::vector<bool> can_merge(neighbors.size(), true);\n+  // Find neighbors with a level that is only 1 different than this group's\n+  // level",
    "Label": "clean"
},
{
    "Id": 960,
    "Library": "pytorch",
    "Date": "2022/11/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5abe454d6c02ac16e2e1fc87500aa46697385501",
    "Root Cause": "N.A",
    "Bug report": "[Vulkan][TCC] Fix conv2d pack biases (#89568)\n\nSummary: Fixed bug on pack_biases, where the weight scale and zero point were being assigned to the bias.\n\nTest Plan:\nOn Mac\n```\ncd ~/fbsource\nbuck1 run -c pt.vulkan_full_precision=1 //xplat/caffe2:pt_vulkan_quantized_api_test_binAppleMac\\#macosx-arm64\n```\n\nOn Android\n```\ncd ~/fbsource\nbuck1 build -c ndk.custom_libcxx=false -c pt.enable_qpl=0 -c pt.vulkan_full_precision=1 //xplat/caffe2:pt_vulkan_quantized_api_test_binAndroid\\#android-arm64 --show-output\nadb push buck-out/gen/xplat/caffe2/pt_vulkan_quantized_api_test_binAndroid\\#android-arm64 /data/local/tmp/vulkan_quantized_api_test\nadb shell \"/data/local/tmp/vulkan_quantized_api_test\"\n```\n\nReviewed By: SS-JIA\n\nDifferential Revision: D41350358\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89568\nApproved by: https://github.com/salilsdesai",
    "Number of deleted lines": 3,
    "Deleted lines": "-      weight.options(),\n-    v_bias.set_scale(weight.q_scale());\n-    v_bias.set_zero_point(weight.q_zero_point());",
    "Added lines": "+      bias_rearranged.options(),\n+    v_bias.set_scale(bias_rearranged.q_scale());\n+    v_bias.set_zero_point(bias_rearranged.q_zero_point());",
    "Label": "clean"
},
{
    "Id": 961,
    "Library": "pytorch",
    "Date": "2022/11/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7594e043b85b6e2c0cf4b2f257ac9606313cec90",
    "Root Cause": "N.A",
    "Bug report": "Fix Use-after-Free in qembeddingbag_byte_prepack_out (#84750)\n\nWhen FBGEMM is not used (either manually disabled or on platforms such as POWER where it isn't supported at all) the fallback code requests a `data_ptr<float>` on a `Tensor` object returned by `to(ScalarType::Float)` in the same line. This object will be destroyed at the end of the line leading to a dangling pointer.\n\nOn some platforms this manifests in wrong results being returned as the memory gets overwritten. On other platforms anything may happen due to this being undefined behavior, although most likely it will just crash or continue to return semi-random results which may even happen to be correct (when the memory is not reused yet)\n\nFix this by binding the temporary object (or initial object) to a const value reference which extents its lifetime and getting the `data_ptr` from that.\n\nFixes #84748\n\nThis bug was introduced by a seemingly unrelated change in #64081 hence ccing @d1jang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/84750\nApproved by: https://github.com/kimishpatel",
    "Number of deleted lines": 3,
    "Deleted lines": "-  const auto weight_data = weight_contig->scalar_type() == at::ScalarType::Half\n-      ? weight_contig->to(at::ScalarType::Float).data_ptr<float>()\n-      : weight_contig->data_ptr<float>();",
    "Added lines": "+  const Tensor& float_weight = weight_contig->scalar_type() == at::ScalarType::Half\n+    ? weight_contig->to(at::ScalarType::Float)\n+    : *weight_contig;\n+  const auto weight_data = float_weight.data_ptr<float>();",
    "Label": "clean"
},
{
    "Id": 962,
    "Library": "pytorch",
    "Date": "2022/11/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/35d5fc52f01f0314ab1bf1555ea27d6fedbb7d98",
    "Root Cause": "N.A",
    "Bug report": "[Profiler] Don't raise SOFT_ASSERT in debug builds. (#89240)\n\nEnough people are hitting this issue that we need to turn off hard failures until the fire rate is zero in steady state. (via scuba logging.)\n\nDifferential Revision: [D41382914](https://our.internmc.facebook.com/intern/diff/D41382914/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89240\nApproved by: https://github.com/aaronenyeshi",
    "Number of deleted lines": 7,
    "Deleted lines": "-  return soft_assert_raises_.value_or(\n-#ifdef NDEBUG\n-      false\n-#else\n-      true\n-#endif\n-  );",
    "Added lines": "+  return soft_assert_raises_.value_or(false);",
    "Label": "clean"
},
{
    "Id": 963,
    "Library": "pytorch",
    "Date": "2022/11/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/af448e84eb2978062dc6ca4d3d538ed46b58f3d6",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in dynamo dashboard summary stats diff (#89226)\n\nFixes issue where a suite may not be present in one of the logs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89226\nApproved by: https://github.com/anijain2305",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                    if suite + \"_prev\" not in row or suite + \"_cur\" not in row:\n+                        continue",
    "Label": "clean"
},
{
    "Id": 964,
    "Library": "pytorch",
    "Date": "2022/10/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/46ce92713dff83182f36b9f4d2a112f9e568825f",
    "Root Cause": "N.A",
    "Bug report": "fix github bug issue 87552 (#88059)\n\nFixes #87552\nPull Request resolved: https://github.com/pytorch/pytorch/pull/88059\nApproved by: https://github.com/jgong5, https://github.com/ngimel",
    "Number of deleted lines": 2,
    "Deleted lines": "-For CPU tensors, an error is thrown.\n-    >>> x.cpu().get_device()  # RuntimeError: get_device is not implemented for type torch.FloatTensor",
    "Added lines": "+For CPU tensors, this function returns `-1`.\n+    >>> x.cpu().get_device()\n+    -1",
    "Label": "clean"
},
{
    "Id": 965,
    "Library": "pytorch",
    "Date": "2022/10/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/166b5d3e7c5c230c455dcbcc05c84dd6bc03721b",
    "Root Cause": "N.A",
    "Bug report": "Revert \"[EZ] Fix simple bug in torchdynamo (#87821)\"\n\nThis reverts commit ce7fcab9bdf61a34bc56b7cd45a882e4ad6ba175.\n\nReverted https://github.com/pytorch/pytorch/pull/87821 on behalf of https://github.com/kit1980 due to Broke many dynamo tests https://github.com/pytorch/pytorch/actions/runs/3341984303/jobs/5534381456",
    "Number of deleted lines": 1,
    "Deleted lines": "-        return super().call_method(tx, name, args, kwargs)",
    "Added lines": "+        return super().call_method(tx, args, kwargs)",
    "Label": "clean"
},
{
    "Id": 966,
    "Library": "pytorch",
    "Date": "2022/10/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9c793b366faf49c78effb0c78d26c48f7664bc92",
    "Root Cause": "N.A",
    "Bug report": "Move incorrectly placed closing curly brace of `extern \"C\"` block (#87853)\n\n### Bug description\nWhen `__SYCL_DEVICE_ONLY__` is defined, while building PyTorch, the output of the preprocessing step would not have the closing curly brace of the `extern \"C\"` block, as it has been incorrectly placed. Compilers don't seem to report an error or a warning for a missing closing brace of an `extern \"C\"` block.\n\n### Impact of the bug\nIf `c10/macros/Macros.h` would be included in a C++ file, and after the preprocessing stage, if the preprocessed source file would have some templated code after `extern \"C\" {`, then, after compilation, linking might fail with the error `templates must have c++ linkage`). eg. https://stackoverflow.com/questions/61717819/template-with-c-linkage-error-when-using-template-keyword-in-main-cpp/61717908#61717908 (its answer also has a small snippet of code to reproduce such an issue).\n\n### Solution in this PR\none-liner bug fix that rectifies the placement of closing curly brace (`}`), so that the `extern \"C\"` block ends properly when `__SYCL_DEVICE_ONLY__` is defined.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87853\nApproved by: https://github.com/jgong5, https://github.com/kit1980, https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-}",
    "Added lines": "+}",
    "Label": "clean"
},
{
    "Id": 967,
    "Library": "pytorch",
    "Date": "2022/10/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ce7fcab9bdf61a34bc56b7cd45a882e4ad6ba175",
    "Root Cause": "N.A",
    "Bug report": "[EZ] Fix simple bug in torchdynamo (#87821)\n\ncc @jansel @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87821\nApproved by: https://github.com/voznesenskym, https://github.com/jansel",
    "Number of deleted lines": 1,
    "Deleted lines": "-        return super().call_method(tx, args, kwargs)",
    "Added lines": "+        return super().call_method(tx, name, args, kwargs)",
    "Label": "clean"
},
{
    "Id": 968,
    "Library": "pytorch",
    "Date": "2022/10/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5edbc926834327d471da505aca902180d30ff991",
    "Root Cause": "N.A",
    "Bug report": "print stderr for ghstack rebase (#87795)\n\ncurrent output tends to be empty on failure, which makes it hard to debug\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87795\nApproved by: https://github.com/huydhn, https://github.com/ZainRizvi",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            print(ghstack_result.stderr.decode(\"utf-8\"))",
    "Label": "clean"
},
{
    "Id": 969,
    "Library": "pytorch",
    "Date": "2022/10/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/65601f5ef3b86231ce886f534fbc8c1c4de9f11d",
    "Root Cause": "N.A",
    "Bug report": "[ONNX] Add Support on 0d tensor Broadcast (#87211)\n\nI am not sure if this will break things ...\n\nAlthough 0d tensor is an undefined behavior in ONNX spec, I did some experiments and found that ONNX shape inference actually provides 0d as inference from 0d and 1d Op calculations, and the bug happened in Broadcast function. But still, if this breaks things really bad, I think we can put 0d tensor handling on hold, as it's not very common usage on models?\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87211\nApproved by: https://github.com/jcwchen, https://github.com/BowenBao",
    "Number of deleted lines": 4,
    "Deleted lines": "-      final_shape[rank_max - 1 - idx] = ::c10::ShapeSymbol::fromStaticSize(\n-          std::max(static_0_sz, static_1_sz));\n-        final_shape[rank_max - 1 - idx] = ss_shape_0;\n-",
    "Added lines": "+    size_t shape_idx = rank_max - 1 - idx;\n+      // condition for corner case of 0d tensor\n+      // 0d tensor with 1d tensor would give us 0d tensor\n+      if (std::min(static_0_sz, static_1_sz) == 0) {\n+        final_shape[shape_idx] = ::c10::ShapeSymbol::fromStaticSize(\n+            std::min(static_0_sz, static_1_sz));\n+      } else {\n+        final_shape[shape_idx] = ::c10::ShapeSymbol::fromStaticSize(\n+            std::max(static_0_sz, static_1_sz));\n+      }\n+        final_shape[shape_idx] = ss_shape_0;",
    "Label": "clean"
},
{
    "Id": 970,
    "Library": "pytorch",
    "Date": "2022/10/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1655b47a384d5e6ba31420b5daee1c029a821387",
    "Root Cause": "N.A",
    "Bug report": "Add some common tools to docker base (#86993)\n\nI always need to install these 2 tools whenever I use Docker manually to debug build and test issues:\n\n* unzip is to extracted the zipped artifacts from PyTorch CI\n* gdb is to do you know what :)\n\nIMO, it makes sense to have them as part of the container image\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86993\nApproved by: https://github.com/ZainRizvi",
    "Number of deleted lines": 2,
    "Deleted lines": "-    libtool\n-    vim",
    "Added lines": "+    libtool \\\n+    unzip \\\n+    gdb\n+    vim \\\n+    unzip \\\n+    gdb",
    "Label": "clean"
},
{
    "Id": 971,
    "Library": "pytorch",
    "Date": "2022/10/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4f2d869095034301b903cd2ef807b416547c0d9c",
    "Root Cause": "N.A",
    "Bug report": "Fix distributed issue by including distributed files (#87615)\n\nThis fixes regression in distributed headers installation.\nCaused by following PR: https://github.com/pytorch/pytorch/pull/85953\nwhich removed the inclusions\n\nFixes #87173\n\nTest plan from wheel build by this CI: https://github.com/pytorch/pytorch/actions/runs/3314742519\n\n```\n[ec2-user@ip-10-0-9-132 c10d]$ pwd\n/home/ec2-user/actions-runner/_work/_temp/artifacts/torch/include/torch/csrc/distributed/c10d\n[ec2-user@ip-10-0-9-132 c10d]$ ls -las\ntotal 300\n 4 drwxr-xr-x 2 ec2-user ec2-user  4096 Oct 24 19:12 .\n 0 drwxr-xr-x 4 ec2-user ec2-user    29 Oct 24 19:12 ..\n12 -rw-r--r-- 1 ec2-user ec2-user  9051 Oct 24 17:28 Backend.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user   216 Oct 24 17:28 c10d.h\n 4 -rw-r--r-- 1 ec2-user ec2-user  3880 Oct 24 17:28 comm.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user   604 Oct 24 17:28 debug.h\n 4 -rw-r--r-- 1 ec2-user ec2-user  1717 Oct 24 17:28 default_comm_hooks.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  1316 Oct 24 17:28 error.h\n 4 -rw-r--r-- 1 ec2-user ec2-user   962 Oct 24 17:28 exception.h\n 4 -rw-r--r-- 1 ec2-user ec2-user  1461 Oct 24 17:28 FileStore.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user   771 Oct 24 17:28 GlooDeviceFactory.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  1154 Oct 24 17:28 HashStore.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  4058 Oct 24 17:28 logger.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  2059 Oct 24 17:28 logging.h\n 8 -rw-r--r-- 1 ec2-user ec2-user  7979 Oct 24 17:28 NCCLUtils.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  2756 Oct 24 17:28 Ops.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  1814 Oct 24 17:28 ParamCommsUtils.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  1478 Oct 24 17:28 PrefixStore.hpp\n16 -rw-r--r-- 1 ec2-user ec2-user 13235 Oct 24 17:28 ProcessGroupGloo.hpp\n12 -rw-r--r-- 1 ec2-user ec2-user 11298 Oct 24 17:28 ProcessGroup.hpp\n12 -rw-r--r-- 1 ec2-user ec2-user  8645 Oct 24 17:28 ProcessGroupMPI.hpp\n28 -rw-r--r-- 1 ec2-user ec2-user 26526 Oct 24 17:28 ProcessGroupNCCL.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  3805 Oct 24 17:28 ProcessGroupRoundRobin.hpp\n12 -rw-r--r-- 1 ec2-user ec2-user 10361 Oct 24 17:28 ProcessGroupUCC.hpp\n 8 -rw-r--r-- 1 ec2-user ec2-user  5062 Oct 24 17:28 ProcessGroupWrapper.hpp\n 8 -rw-r--r-- 1 ec2-user ec2-user  4201 Oct 24 17:28 PyProcessGroup.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  1072 Oct 24 17:28 python_comm_hook.h\n24 -rw-r--r-- 1 ec2-user ec2-user 23859 Oct 24 17:28 reducer.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  2330 Oct 24 17:28 reducer_timer.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  1683 Oct 24 17:28 sequence_num.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  2108 Oct 24 17:28 socket.h\n 4 -rw-r--r-- 1 ec2-user ec2-user  2589 Oct 24 17:28 Store.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  3264 Oct 24 17:28 TCPStore.hpp\n 8 -rw-r--r-- 1 ec2-user ec2-user  6944 Oct 24 17:28 TraceUtils.h\n 8 -rw-r--r-- 1 ec2-user ec2-user  4539 Oct 24 17:28 Types.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user   580 Oct 24 17:28 UCCForNCCL.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user  2301 Oct 24 17:28 UCCTracing.hpp\n 8 -rw-r--r-- 1 ec2-user ec2-user  4933 Oct 24 17:28 UCCUtils.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user   584 Oct 24 17:28 UnixSockUtils.hpp\n24 -rw-r--r-- 1 ec2-user ec2-user 20796 Oct 24 17:28 Utils.hpp\n 4 -rw-r--r-- 1 ec2-user ec2-user   575 Oct 24 17:28 WinSockUtils.hpp\n 8 -rw-r--r-- 1 ec2-user ec2-user  4259 Oct 24 17:28 Work.hpp\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87615\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-        'include/torch/csrc/distributed/c10d/exception.h',",
    "Added lines": "+        'include/torch/csrc/distributed/c10d/*.h',\n+        'include/torch/csrc/distributed/c10d/*.hpp',",
    "Label": "clean"
},
{
    "Id": 972,
    "Library": "pytorch",
    "Date": "2022/10/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/48f02312232d71f8e5cabfcc85b70f8330953057",
    "Root Cause": "N.A",
    "Bug report": "Fix Scalar(bool) handling in toIValue (#87179)\n\nAt the moment, they were casted to `int64`, which breaks quite a few\ncasting rules for example in `ops.aten`.\n\nQuite a vintage bug, circa 2020.\n\nWith this fix, the following code prints `torch.bool`, rather than `torch.int64`.\n```python\nimport torch\nmsk = torch.tensor([False])\nb = torch.tensor([False])\nprint(torch.ops.aten.where.ScalarSelf(msk, True, b).dtype)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87179\nApproved by: https://github.com/albanD",
    "Number of deleted lines": 1,
    "Deleted lines": "-      if (py::isinstance<py::int_>(obj)) {",
    "Added lines": "+      if (py::isinstance<py::bool_>(obj)) {\n+        return py::cast<bool>(obj);\n+      } else if (py::isinstance<py::int_>(obj)) {",
    "Label": "clean"
},
{
    "Id": 973,
    "Library": "pytorch",
    "Date": "2022/10/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1a7409c77199403153f1260e2281bae2f76745f6",
    "Root Cause": "N.A",
    "Bug report": "[CoreML][ios_crash] Use special throw macro when encountering CoreML API errors (#86938)\n\nError messages from TORCH_CHECK are stripped during production builds via  -DSTRIP_ERROR_MESSAGES. This diff introduces a new macro COREML_CHECK which will always preserve the error message. This macro is used when encountering errors produced by CoreML API calls so that we can heve enough context to debug.\n\nDifferential Revision: [D40351013](https://our.internmc.facebook.com/intern/diff/D40351013/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86938\nApproved by: https://github.com/salilsdesai",
    "Number of deleted lines": 1,
    "Deleted lines": "-      TORCH_CHECK(false, [[error description] UTF8String]);",
    "Added lines": "+// This is a utility macro that can be used to throw an exception when a CoreML\n+// API function produces a NSError. The exception will contain a message with\n+// useful info extracted from the NSError.\n+#define COREML_THROW_IF_ERROR(error, preamble)                                   \\\n+  do {                                                                           \\\n+    if C10_LIKELY(error) {                                                       \\\n+      throw c10::Error(                                                          \\\n+          {__func__, __FILE__, static_cast<uint32_t>(__LINE__)},                 \\\n+          c10::str(                                                              \\\n+              preamble,                                                          \\\n+              \" Error details: \",                                                \\\n+              \" Localized_description: \", error.localizedDescription.UTF8String, \\\n+              \" Domain: \", error.domain.UTF8String,                              \\\n+              \" Code: \", error.code,                                             \\\n+              \" User Info: \", error.userInfo.description.UTF8String));           \\\n+    }                                                                            \\\n+  } while (false)\n+\n+      COREML_THROW_IF_ERROR(error, \"Error running CoreML inference\");",
    "Label": "clean"
},
{
    "Id": 974,
    "Library": "pytorch",
    "Date": "2022/10/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6c604c9262307ffcaf1d7dd68bfa5f6b44513d06",
    "Root Cause": "N.A",
    "Bug report": "[CuDNN v8 API][Quantization]fix alignment function in quantized cuDNN V8 path (#86253)\n\nThis bug was in the native cuDNN V8 API integration and was fixed a while ago, but the change was never ported here.\n\nPreviously the returned alignment could be twice the actual alignment of the data if the alignment was smaller than 16.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86253\nApproved by: https://github.com/dzdang",
    "Number of deleted lines": 1,
    "Deleted lines": "-  while (address % alignment == 0 && alignment < 16) alignment *= 2;",
    "Added lines": "+  for (; alignment < 16; alignment *= 2) {\n+    if (address % (alignment * 2)) {\n+      return alignment;\n+    }\n+  }",
    "Label": "clean"
},
{
    "Id": 975,
    "Library": "pytorch",
    "Date": "2022/10/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/adfd8f382331adbf9cbfa14039ef3b61f2f4e10c",
    "Root Cause": "N.A",
    "Bug report": "[FSDP] assert to runtime error (#86336)\n\nPrefer raising an error over `assert` which should mostly to indicate a developer bug, but user can cause this error path.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86336\nApproved by: https://github.com/awgu",
    "Number of deleted lines": 1,
    "Deleted lines": "-        assert self._is_root, \"`no_sync()` on inner FSDP instances is not supported\"",
    "Added lines": "+        if not self._is_root:\n+            raise RuntimeError(\n+                \"`no_sync()` on inner FSDP instances is not supported. Please call `no_sync()` on root FSDP module.\"\n+            )",
    "Label": "clean"
},
{
    "Id": 976,
    "Library": "pytorch",
    "Date": "2022/10/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/795906f207bb95245d47501645876b5d165aee3e",
    "Root Cause": "N.A",
    "Bug report": "Add total GPU memory utilization (#86250)\n\nAlthough we already have per process GPU memory usage, I'm curious to see what is the number for `gpu_utilization.memory` per https://docs.nvidia.com/deploy/nvml-api/structnvmlUtilization__t.html.  Also fixing a tiny typo issue that has been bugging me for a while `total_gpu_utilizaiton`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86250\nApproved by: https://github.com/ZainRizvi",
    "Number of deleted lines": 3,
    "Deleted lines": "-                stats[\"total_gpu_utilizaiton\"] = pynvml.nvmlDeviceGetUtilizationRates(\n-                    handle\n-                ).gpu",
    "Added lines": "+                # https://docs.nvidia.com/deploy/nvml-api/structnvmlUtilization__t.html\n+                gpu_utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n+                stats[\"total_gpu_utilization\"] = gpu_utilization.gpu\n+                stats[\"total_gpu_mem_utilization\"] = gpu_utilization.memory",
    "Label": "clean"
},
{
    "Id": 977,
    "Library": "pytorch",
    "Date": "2022/10/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0e5a27fb8d7df8541251f1ebfc4373c1358c1bab",
    "Root Cause": "N.A",
    "Bug report": "Fix horribly double truncation bug in Scalar (#86304)\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86304\nApproved by: https://github.com/albanD",
    "Number of deleted lines": 1,
    "Deleted lines": "-      return toLong();",
    "Added lines": "+      return toDouble();",
    "Label": "clean"
},
{
    "Id": 978,
    "Library": "pytorch",
    "Date": "2022/10/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2f703c5956f3c861c80d5ac736ff2aeba6dfb476",
    "Root Cause": "N.A",
    "Bug report": "SymInt-ify TypeAndSize (#86044)\n\nCommit originally by anjali411, with bugfix from Edward.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86044\nApproved by: https://github.com/Chillee",
    "Number of deleted lines": 3,
    "Deleted lines": "-    : sizes(t.sizes().vec())\n-  Tensor zeros() { return at::zeros(sizes, options); }\n-  std::vector<int64_t> sizes;",
    "Added lines": "+#include <c10/core/SymIntArrayRef.h>\n+\n+    : sym_sizes(t.sym_sizes().vec())\n+  Tensor zeros() { return at::zeros_symint(sym_sizes, options); }\n+  std::vector<c10::SymInt> sym_sizes;",
    "Label": "clean"
},
{
    "Id": 979,
    "Library": "pytorch",
    "Date": "2022/09/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e09a84a184e1687f4ddc7f3fc875eaaf5b9ec74f",
    "Root Cause": "N.A",
    "Bug report": "Removed debug output that doesn't work with faketensors (#85992)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/85992\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 8,
    "Deleted lines": "-        if config.debug_partitioner:\n-            fw_outs = call_func_with_args(compiled_fw_func, deduped_flat_args)\n-            activation_sizes = 0\n-            for out in fw_outs[_num_outs:]:\n-                if isinstance(out, torch.Tensor):\n-                    activation_sizes += out.storage().nbytes()\n-            print(f\"Real Activations Stored(GB): {activation_sizes/1e9}\")\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 980,
    "Library": "pytorch",
    "Date": "2022/09/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/95681929e4c379c504d8a7761f8104118a5a16db",
    "Root Cause": "N.A",
    "Bug report": "Hotfix for S298125 (#85814)\n\nSummary:\nCrash error is:\n\n```\nMismatch in kernel C++ signatures\n\noperator: aten::cat\n\nno debug info\n\nkernel 1: FN2at6TensorEN3c108ArrayRefIS0_EExE\n\ndispatch key: Metal\n\nregistered at buck-out/gen/a1f97bbb/fbobjc/Libraries/FBPyTorchCore/torch_core_ig_ops_metal/aten/src/ATen/native/metal/ops/MetalConcat.mm:205\n\nkernel 2: FN2at6TensorERKN3c108IListRefIS0_EExE\n\ndispatch key: CPU\n\nregistered at buck-out/gen/a1f97bbb/fbobjc/Libraries/FBPyTorchCore/torch_core_ig_ops_aten/RegisterCPU.cpp:29749\n\nException raised from registerKernel at xplat/caffe2/aten/src/ATen/core/dispatch/OperatorEntry.cpp:130 (most recent call first):\n```\n\nWe fix it by changing the Metal kernel to take an IListRef instead of an ArrayRef.\n\nTest Plan: Build igios per https://www.internalfb.com/intern/wiki/IOS_On_Demand/iOS_On_Demand_Use_Guide/ and show it doesn't crash\n\nDifferential Revision: D39888394\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/85814\nApproved by: https://github.com/SS-JIA",
    "Number of deleted lines": 15,
    "Deleted lines": "-Tensor cat_batch(const TensorList tensors, MetalTensorImplStorage& mt) {\n-  at::Tensor tensor = tensors[0];\n-  for (int i = 0; i < tensors.size(); ++i) {\n-    const auto& t = tensors[i];\n-Tensor cat_feature(const TensorList tensors, MetalTensorImplStorage& mt) {\n-  at::Tensor tensor = tensors[0];\n-  for (int i = 0; i < tensors.size(); ++i) {\n-    MPSImage* X = imageFromTensor(tensors[i]);\n-    MetalCommandBuffer* Xcb = getCommandBuffer(tensors[i]);\n-Tensor cat(const TensorList tensors, int64_t dim) {\n-  at::Tensor tensor = tensors[0];\n-  for (int i = 0; i < tensors.size(); ++i) {\n-    const auto& t = tensors[i];\n-    return cat_feature(tensors, mt);\n-  return cat_batch(tensors, mt);",
    "Added lines": "+Tensor cat_batch(const Tensor& tensor, ITensorListRef tensors, MetalTensorImplStorage& mt) {\n+  for (const auto& t : tensors) {\n+Tensor cat_feature(const Tensor& tensor, ITensorListRef tensors, MetalTensorImplStorage& mt) {\n+  for (const auto& t : tensors) {\n+    MPSImage* X = imageFromTensor(t);\n+    MetalCommandBuffer* Xcb = getCommandBuffer(t);\n+Tensor cat(ITensorListRef tensors, int64_t dim) {\n+  TORCH_CHECK(!tensors.empty(), \"cat expected a non-empty list of Tensor\");\n+  at::Tensor tensor = *tensors.begin();\n+  for (const auto& t : tensors) {\n+    return cat_feature(tensor, tensors, mt);\n+  return cat_batch(tensor, tensors, mt);",
    "Label": "clean"
},
{
    "Id": 981,
    "Library": "pytorch",
    "Date": "2022/09/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a9183c0f9ecc9be47fcb7abf1b23204d26821aa8",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in PythonFallBack (#85795)\n\nSummary: Previously PythonCallBack fails to find interpreter to dispatch to when it encounters an op with OptionalTensorList parameter, this diff fixes that\n\nTest Plan: CI\n\nDifferential Revision: D39881382\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/85795\nApproved by: https://github.com/ezyang, https://github.com/bdhirsh",
    "Number of deleted lines": 1,
    "Deleted lines": "-    } else if (ivalue.isTensorList()) {",
    "Added lines": "+    } else if (ivalue.isTensorList() || (ivalue.isOptionalTensorList() && !ivalue.isNone())) {",
    "Label": "clean"
},
{
    "Id": 982,
    "Library": "pytorch",
    "Date": "2022/09/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7628603aeeeb8ed160c2479f75175bb3ea028a42",
    "Root Cause": "N.A",
    "Bug report": "[Profiler] bug fix: python object reference counting (#85847)\n\nSummary:\nWrong reference counting of Python Objects has made intermittent and corner-case-only segfault.\n- before : increment once decrement in a loop.\n- after: increment and decrement in different but consistent loops.\n\nTest Plan: buck run mode/opt //caffe2/test:profiler\n\nDifferential Revision: D39902973\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/85847\nApproved by: https://github.com/robieta, https://github.com/aaronenyeshi",
    "Number of deleted lines": 1,
    "Deleted lines": "-    Py_INCREF(frame);",
    "Added lines": "+      Py_INCREF(frame);",
    "Label": "clean"
},
{
    "Id": 983,
    "Library": "pytorch",
    "Date": "2022/09/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9c127986bfa5bf8759eb88eb2c77f2de7ad001ba",
    "Root Cause": "N.A",
    "Bug report": "Fix labeling detection bug (#85429)\n\nFixes a bug where if a PR is pre-labeled with both a `release notes:` label and a `topic:` label then our bot still pings on the PR, erroneously asking for those labels to be added\n\n&-ing sets computes the set intersection, which isn't what was desired here\nPull Request resolved: https://github.com/pytorch/pytorch/pull/85429\nApproved by: https://github.com/janeyx99",
    "Number of deleted lines": 1,
    "Deleted lines": "-    has_both_labels = bool(primary_labels.intersection(labels) and SECONDARY_LABELS.intersection(labels))",
    "Added lines": "+    has_both_labels = bool(primary_labels.intersection(labels)) and bool(SECONDARY_LABELS.intersection(labels))",
    "Label": "clean"
},
{
    "Id": 984,
    "Library": "pytorch",
    "Date": "2022/09/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5c0c8f2ce344f74849afaed88df93292cb30ce0b",
    "Root Cause": "N.A",
    "Bug report": "[coreml][bug] coreml gpu flag not set (#84725)\n\nSummary:\nDelegated CoreML models with cpuAndGPU flag set does not properly run models on CPU\n\n- Fix will allow us to target models on CPU\n\nTest Plan: brymkowski can you test this on your performance benchmarks?\n\nReviewed By: salilsdesai\n\nDifferential Revision: D39361382\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/84725\nApproved by: https://github.com/jmdetloff",
    "Number of deleted lines": 1,
    "Deleted lines": "-    if (backend == \"cpuandgpu\") {",
    "Added lines": "+    if (backend == \"cpuAndGPU\") {",
    "Label": "clean"
},
{
    "Id": 985,
    "Library": "pytorch",
    "Date": "2022/09/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e6ee8e613dfe764f61b2b29ad0a4a2c36f143eaa",
    "Root Cause": "N.A",
    "Bug report": "Return x.alias() when transpose is an nop (#84674)\n\nTo fix bug in https://gist.github.com/SherlockNoMad/b8dfbc614d3e65707d1bc379a098196d\n\n```\ndef f(x):\n    return x.t()\n\nx = torch.randn(2, requires_grad=True)\ny = f(x)\n\ncompiled_f = make_fx(f)(x)\ny_compiled = compiled_f(x)\n\nprint(compiled_f)\nprint(\"y.requires_grad\", y.requires_grad)\nprint(\"y_compiled.requires_grad\", y_compiled.requires_grad)\n\n# def forward(self, x_1):\n#     t_default = torch.ops.aten.t.default(x_1);  x_1 = None\n#     detach_default = torch.ops.aten.detach.default(t_default);  t_default = None\n#     return detach_default\n\n# y.requires_grad True\n# y_compiled.requires_grad False\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/84674\nApproved by: https://github.com/cpuhrsch",
    "Number of deleted lines": 6,
    "Deleted lines": "-  // Transpose of a tensor is a view operation.\n-  if (dim0 == dim1) {\n-    return self;\n-  }\n-\n-    return alias_with_sizes_and_strides(self, self.sizes(), self.strides());",
    "Added lines": "+  // Transpose of a tensor is a view operation.\n+  if (dim0 == dim1) {\n+    return self.alias();\n+  }\n+\n+  return alias_with_sizes_and_strides(self, self.sizes(), self.strides());",
    "Label": "clean"
},
{
    "Id": 986,
    "Library": "pytorch",
    "Date": "2022/09/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0909639c9045e9f9435165778319fdb59728baa6",
    "Root Cause": "N.A",
    "Bug report": "fix dispatch declaration bug about quantized op (#83649)\n\n# Motivation:\nFixes issue #83051.\n_fake_quantize_learnable_per_tensor_affine_backward and _fake_quantize_learnable_per_channel_affine_backward are implemented for CPU and CUDA. Currently, these two are in the CompositeImplicitAutograd category.\nIf this issue is not fixed. We need to provide their autograd function when we want to register a new backend. It doesn't make sense to implement autograd function for them since they are all backward operators implemented directly with TensorIterators.\n\n# Solution:\nAdd a dispatch keyword in aten/src/ATen/native/native_functions.yaml and explicitly dispatch operators to CPU and CUDA.\nlike this:\n`   dispatch:`\n`    CPU, CUDA: _fake_quantize_learnable_per_tensor_affine_backward`\n\n# Additional context:\nNo additional unit test because this change could not affect PyTorch's functionality. It only affects registration on other backends, like XPU. So it is difficult to add ut to test it.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/83649\nApproved by: https://github.com/jerryzh168",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  dispatch:\n+    CPU, CUDA: _fake_quantize_learnable_per_tensor_affine_backward\n+  dispatch:\n+    CPU, CUDA: _fake_quantize_learnable_per_channel_affine_backward",
    "Label": "clean"
},
{
    "Id": 987,
    "Library": "pytorch",
    "Date": "2022/08/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ece0002c4beaebaf083dc75b7bf8ceb19edf7a0b",
    "Root Cause": "N.A",
    "Bug report": "[ONNX] Disable autocast cache in exporter (#84219)\n\nThis PR provides a temporary fix on #84092 in exporter to avoid more cases falling into this bug.\nA long-term fix will be provided later.\n\nA simple repro with torch.onnx.export is still under investigation, as torch.jit.trace() is not the API we call inside torch.onnx.export, and it may introduce the difference. Therefore, a test case is provided here only.\nA specific test one can use,\n```python\nimport torch\nimport onnxruntime\nfrom onnxruntime.training.ortmodule import DebugOptions, LogLevel\nfrom onnxruntime.training.ortmodule import ORTModule\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cv1 = torch.nn.Conv2d(3, 3, 5, 2, 1)\n\n    def forward(self, x):\n        x = self.cv1(x)\n        return x\n\nx = torch.randn(10, 3, 20, 20) * 2\nm = MyModule().eval()\nx = x.cuda()\nm = m.cuda()\n\ndebug_options = DebugOptions(log_level=LogLevel.VERBOSE, save_onnx=True, onnx_prefix=\"ViT-B\")\nm = ORTModule(m, debug_options=debug_options)\n\nwith torch.cuda.amp.autocast(dtype=torch.float16, cache_enabled=True):\n    loss = m(x)\n```\nAND make assertion fail in ORTModule\nhttps://github.com/microsoft/onnxruntime/blob/17ccd6fa02877a1c8d3201344137b1ca105b681d/orttraining/orttraining/python/training/ortmodule/_io.py#L578-L581\n\nWithout the fix, the user will see the weight/bias of Conv node becomes constant.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/84219\nApproved by: https://github.com/BowenBao, https://github.com/thiagocrepaldi",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    # Disable Autocast cache because it replaces kernel's weight and bias\n+    # to be replaced by (undesired) constants\n+    # TODO: https://github.com/pytorch/pytorch/issues/84092\n+    prev_autocast_cache_enabled = torch.is_autocast_cache_enabled()\n+    # When weights are not reused, there is no perf impact\n+    # ONNX runtimes can also apply CSE optimization to compensate the lack of cache here\n+    torch.set_autocast_cache_enabled(False)\n+    torch.set_autocast_cache_enabled(prev_autocast_cache_enabled)\n+",
    "Label": "clean"
},
{
    "Id": 988,
    "Library": "pytorch",
    "Date": "2022/08/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/79e3a39f95e91af03823a8579da06c35bb519faf",
    "Root Cause": "N.A",
    "Bug report": "[BE] Remove unused `export.h` include (#84305)\n\nAs flatbuffer_serializer can be compiled without it\n\nFound while debugging cause of https://github.com/pytorch/pytorch/pull/82040#issuecomment-1229503604\nPull Request resolved: https://github.com/pytorch/pytorch/pull/84305\nApproved by: https://github.com/kit1980, https://github.com/qihqi",
    "Number of deleted lines": 1,
    "Deleted lines": "-#include <torch/csrc/jit/serialization/export.h>",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 989,
    "Library": "pytorch",
    "Date": "2022/08/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/15b560a5c4d638c82e738f3496e2faf95fc328a5",
    "Root Cause": "N.A",
    "Bug report": "Fix missing include for size_t (#84088)\n\nFixes the following issue:\n\n```C++\nIn file included from /home/gaoxiang/pytorch-ucc/c10/test/util/ConstexprCrc_test.cpp:1:\nIn file included from /home/gaoxiang/pytorch-ucc/c10/util/ConstexprCrc.h:3:\n/home/gaoxiang/pytorch-ucc/c10/util/IdWrapper.h:42:10: error: unknown type name 'size_t'; did you mean 'std::size_t'?\n  friend size_t hash_value(const concrete_type& v) {\n         ^~~~~~\n         std::size_t\n/usr/bin/../lib64/gcc/x86_64-pc-linux-gnu/12.2.0/../../../../include/c++/12.2.0/x86_64-pc-linux-gnu/bits/c++config.h:298:26: note: 'std::size_t' declared here\n  typedef __SIZE_TYPE__         size_t;\n                                ^\n1 error generated.\n[111/2069] Generating /home/gaoxiang/pytorch-ucc/torch/csrc/a...ch-ucc/torch/testing/_internal/generated/annotated_fn_args.py\nninja: build stopped: subcommand failed.\n```\n\nThis error happens with my GCC 12.2.0 + Clang 14.0.6.\n\nFull environment:\n```\nCollecting environment information...\nPyTorch version: 1.13.0a0+git14a53e6\nIs debug build: True\nCUDA used to build PyTorch: 11.7\nROCM used to build PyTorch: N/A\n\nOS: Arch Linux (x86_64)\nGCC version: (GCC) 12.2.0\nClang version: 14.0.6\nCMake version: version 3.24.1\nLibc version: glibc-2.36\n\nPython version: 3.10.6 (main, Aug  3 2022, 17:39:45) [GCC 12.1.1 20220730] (64-bit runtime)\nPython platform: Linux-5.19.3-arch1-1-x86_64-with-glibc2.36\nIs CUDA available: True\nCUDA runtime version: 11.7.99\nGPU models and configuration:\nGPU 0: NVIDIA GeForce RTX 3090\nGPU 1: NVIDIA GeForce RTX 2080 Ti\n\nNvidia driver version: 515.65.01\ncuDNN version: Probably one of the following:\n/usr/lib/libcudnn.so.8.4.1\n/usr/lib/libcudnn_adv_infer.so.8.4.1\n/usr/lib/libcudnn_adv_train.so.8.4.1\n/usr/lib/libcudnn_cnn_infer.so.8.4.1\n/usr/lib/libcudnn_cnn_train.so.8.4.1\n/usr/lib/libcudnn_ops_infer.so.8.4.1\n/usr/lib/libcudnn_ops_train.so.8.4.1\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nVersions of relevant libraries:\n[pip3] numpy==1.23.1\n[pip3] torch==1.13.0a0+gitbcc6f6c\n[pip3] torch-ucc==1.0.0\n[pip3] torchani==2.2\n[pip3] torchvision==0.2.2.post3\n[conda] Could not collect\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/84088\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include <cstddef>",
    "Label": "clean"
},
{
    "Id": 990,
    "Library": "pytorch",
    "Date": "2022/08/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d79ccb7b4589ab65727b16cde19918dfdd11d32c",
    "Root Cause": "N.A",
    "Bug report": "[pthreadpool] Cap max thread count to fix TSAN issues (#83950)\n\nSummary: Cap the thread count to 64 unconditionally to solve this tsan issue which leads to harder to debug, flaky test failures.\n\nTest Plan: CI\n\nReviewed By: kimishpatel\n\nDifferential Revision: D38136212\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/83950\nApproved by: https://github.com/kimishpatel",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+  /*\n+   * For llvm-tsan, holding limit for the number of locks for a single thread\n+   * is 64. pthreadpool's worst case is the number of threads in a pool. So we\n+   * want to limit the threadpool size to 64 when running with tsan. However,\n+   * sometimes it is tricky to detect if we are running under tsan, for now\n+   * capping the default threadcount to the tsan limit unconditionally.\n+   */\n+  int tsanThreadLimit = 64;\n+  numThreads = std::min(numThreads, tsanThreadLimit);\n+",
    "Label": "clean"
},
{
    "Id": 991,
    "Library": "pytorch",
    "Date": "2022/08/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0831813e26ebcd406b261ffb9629f933c627d4d3",
    "Root Cause": "N.A",
    "Bug report": "support more symintnode operations (#83877)\n\nremove debug code\nPull Request resolved: https://github.com/pytorch/pytorch/pull/83877\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 14,
    "Deleted lines": "-  TORCH_CHECK(\n-      !this->is_symbolic() && !sci.is_symbolic(),\n-      \"Symbolic Add isn't supported yet\");\n-  return SymInt(data_ + sci.data_);\n-  TORCH_CHECK(\n-      !this->is_symbolic() && !sci.is_symbolic(),\n-      \"Symbolic lt isn't supported yet\");\n-  return data_ < sci.data_;\n-  TORCH_CHECK(\n-      !this->is_symbolic() && !sci.is_symbolic(),\n-      \"Symbolic mul_ isn't supported yet\");\n-  data_ = data_ * sci.data_;\n-  TORCH_CHECK(!this->is_symbolic(), \"Symbolic lt isn't supported yet\");\n-  return data_ < sci;",
    "Added lines": "+  if (!is_symbolic() && !sci.is_symbolic()) {\n+    return SymInt(data_ + sci.data_);\n+  }\n+  auto res = normalize_symints(*this, sci);\n+  return SymInt::toSymInt(res[0]->add(res[1]));\n+  if (!is_symbolic() && !sci.is_symbolic()) {\n+    return data_ < sci.data_;\n+  }\n+  auto res = normalize_symints(*this, sci);\n+  return res[0]->lt(res[1])->bool_();\n+  *this = *this * sci;\n+  return *this < c10::SymInt(sci);",
    "Label": "clean"
},
{
    "Id": 992,
    "Library": "pytorch",
    "Date": "2022/08/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4902254b9b595adf1a0346d6f79a6c7b145dbcaa",
    "Root Cause": "N.A",
    "Bug report": "fix torch._C._nn.linear bug (#83682)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/83682\nApproved by: https://github.com/jansel",
    "Number of deleted lines": 1,
    "Deleted lines": "-    constraints, counter = linear_constraints(n, weight_dims[0], weight_dims[1], symbols, counter)",
    "Added lines": "+    constraints, counter = linear_constraints(n, weight_dims[1], weight_dims[0], symbols, counter)",
    "Label": "clean"
},
{
    "Id": 993,
    "Library": "pytorch",
    "Date": "2022/08/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1a38724ed3e189152f11ef576d0ff15a31a39eaa",
    "Root Cause": "N.A",
    "Bug report": "fix bug in a linear constraint (#82938)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/82938\nApproved by: https://github.com/jansel",
    "Number of deleted lines": 1,
    "Deleted lines": "-    equality_constraint = BinConstraintT(n.args[1], TensorType(weight_dims), op_eq)",
    "Added lines": "+    equality_constraint = BinConstraintT(symbols[n.args[1]], TensorType(weight_dims), op_eq)",
    "Label": "clean"
},
{
    "Id": 994,
    "Library": "pytorch",
    "Date": "2022/08/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/abb2204f6a9c5af4e14e11cc69de8bcb5cceaea0",
    "Root Cause": "N.A",
    "Bug report": "Fix TORCH_CHECK macros when glog is used (#83216)\n\nMakes TORCH_CHECK_* run unconditionally, leaving only TORCH_DCHECK_*\nspecial-cased to be optimized out in release builds.\n\nFixes a bug in #82032, relating to this comment\nhttps://github.com/pytorch/pytorch/pull/82032#issuecomment-1203726409\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/83216\nApproved by: https://github.com/ezyang, https://github.com/datumbox",
    "Number of deleted lines": 19,
    "Deleted lines": "-#ifndef NDEBUG\n-#define TORCH_CHECK_EQ(val1, val2) \\\n-  while (false)                    \\\n-  CHECK_EQ(val1, val2)\n-#define TORCH_CHECK_NE(val1, val2) \\\n-  while (false)                    \\\n-  CHECK_NE(val1, val2)\n-#define TORCH_CHECK_LE(val1, val2) \\\n-  while (false)                    \\\n-  CHECK_LE(val1, val2)\n-#define TORCH_CHECK_LT(val1, val2) \\\n-  while (false)                    \\\n-  CHECK_LT(val1, val2)\n-#define TORCH_CHECK_GE(val1, val2) \\\n-  while (false)                    \\\n-  CHECK_GE(val1, val2)\n-#define TORCH_CHECK_GT(val1, val2) \\\n-  while (false)                    \\\n-  CHECK_GT(val1, val2)",
    "Added lines": "+\n+#ifndef NDEBUG",
    "Label": "clean"
},
{
    "Id": 995,
    "Library": "pytorch",
    "Date": "2022/08/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9213751970e4bb4eecd0b5dc6317740f017460a0",
    "Root Cause": "N.A",
    "Bug report": "Add exception handler for stoull in caffe2 (#77557)\n\nHi!\n\nI was playing with libfuzzer and found bug when loading a model from file via `torch::jit::load` function.\nThere is an unhandled exception in caffe2/serialize when calling a `stoull` function on unsanitized version string.\n\nThe bug can be reproduced with `aot_model_compiler` binary:\n```\naot_model_compiler --model=crash-stoull --model_name=name --model_version=1 --input_dims='1,3,224,224;2,2' --input_types='float;float'\n```\n\nCrash file is provided in [crash.zip](https://github.com/pytorch/pytorch/files/8701504/crash.zip).\n\ngdb output:\n```\nTemporary breakpoint 1, main (argc=6, argv=0x7ffcd160f9f8) at /pytorch_master/binaries/aot_model_compiler.cc:87\n87\t      \"Run NNC AOT compiler for pytorch model. Example usage:\\n\"\n(gdb) c\nContinuing.\nterminate called after throwing an instance of 'std::invalid_argument'\n  what():  stoull\n\nProgram received signal SIGABRT, Aborted.\n__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50\n50\t../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\n(gdb) bt\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50\n#1  0x00007fa637f16859 in __GI_abort () at abort.c:79\n#2  0x00007fa6381c1911 in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6\n#3  0x00007fa6381cd38c in ?? () from /lib/x86_64-linux-gnu/libstdc++.so.6\n#4  0x00007fa6381cd3f7 in std::terminate() () from /lib/x86_64-linux-gnu/libstdc++.so.6\n#5  0x00007fa6381cd6a9 in __cxa_throw () from /lib/x86_64-linux-gnu/libstdc++.so.6\n#6  0x00007fa6381c42ce in std::__throw_invalid_argument(char const*) () from /lib/x86_64-linux-gnu/libstdc++.so.6\n#7  0x000000000247d567 in __gnu_cxx::__stoa<unsigned long long, unsigned long long, char, int> (__str=0x7ffcd160f228 \"ZZ\", __idx=0x0, __base=10, __convf=<optimized out>, __name=<optimized out>)\n    at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/ext/string_conversions.h:83\n#8  std::__cxx11::stoull (__str=\"ZZ\", __idx=0x0, __base=10) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/basic_string.h:6577\n#9  caffe2::serialize::PyTorchStreamReader::init (this=this@entry=0x8c11ce0) at /pytorch_master/caffe2/serialize/inline_container.cc:145\n#10 0x000000000247d9c7 in caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader (this=0x8c11ce0, in=std::shared_ptr<class caffe2::serialize::ReadAdapterInterface> (empty) = {...})\n    at /pytorch_master/caffe2/serialize/inline_container.cc:88\n#11 0x00000000035b7ba4 in __gnu_cxx::new_allocator<caffe2::serialize::PyTorchStreamReader>::construct<caffe2::serialize::PyTorchStreamReader, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (\n    __p=0x2, __args=..., this=<optimized out>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/ext/new_allocator.h:150\n#12 std::allocator_traits<std::allocator<caffe2::serialize::PyTorchStreamReader> >::construct<caffe2::serialize::PyTorchStreamReader, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (__a=...,\n    __p=0x2, __p@entry=0x8c11ce0, __args=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/alloc_traits.h:512\n#13 0x00000000035b1988 in std::_Sp_counted_ptr_inplace<caffe2::serialize::PyTorchStreamReader, std::allocator<caffe2::serialize::PyTorchStreamReader>, (__gnu_cxx::_Lock_policy)2>::_Sp_counted_ptr_inplace<std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (this=0x8c11cd0, __a=..., __args=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:551\n#14 std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count<caffe2::serialize::PyTorchStreamReader, std::allocator<caffe2::serialize::PyTorchStreamReader>, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (this=0x7ffcd160f3a8, __p=@0x7ffcd160f3a0: 0x10, __args=..., __a=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:683\n#15 std::__shared_ptr<caffe2::serialize::PyTorchStreamReader, (__gnu_cxx::_Lock_policy)2>::__shared_ptr<std::allocator<caffe2::serialize::PyTorchStreamReader>, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (this=0x7ffcd160f3a0, __args=..., __tag=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1371\n#16 std::shared_ptr<caffe2::serialize::PyTorchStreamReader>::shared_ptr<std::allocator<caffe2::serialize::PyTorchStreamReader>, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (this=0x7ffcd160f3a0,\n    __args=..., __tag=...) at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr.h:408\n#17 std::allocate_shared<caffe2::serialize::PyTorchStreamReader, std::allocator<caffe2::serialize::PyTorchStreamReader>, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (__args=..., __a=...)\n    at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr.h:859\n#18 std::make_shared<caffe2::serialize::PyTorchStreamReader, std::shared_ptr<caffe2::serialize::ReadAdapterInterface> > (__args=...)\n    at /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr.h:875\n#19 torch::jit::load (rai=std::shared_ptr<class caffe2::serialize::ReadAdapterInterface> (empty) = {...}, device=device@entry=..., Python Exception <class 'gdb.error'> No type named std::__detail::_Hash_node<struct std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, true>.:\nextra_files=std::unordered_map with 0 elements)\n    at /pytorch_master/torch/csrc/jit/serialization/import.cpp:474\n#20 0x00000000035b1ef6 in torch::jit::load (filename=\"crash-stoull\", device=device@entry=..., Python Exception <class 'gdb.error'> No type named std::__detail::_Hash_node<struct std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, true>.:\nextra_files=std::unordered_map with 0 elements) at /pytorch_master/torch/csrc/jit/serialization/import.cpp:444\n#21 0x00000000035b1d22 in torch::jit::load (filename=\"\", device=device@entry=...) at /pytorch_master/torch/csrc/jit/serialization/import.cpp:424\n#22 0x00000000008f9be3 in main (argc=1, argv=0x7ffcd160f9f8) at /pytorch_master/binaries/aot_model_compiler.cc:128\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/77557\nApproved by: https://github.com/Gamrix",
    "Number of deleted lines": 1,
    "Deleted lines": "-  version_ = caffe2::stoull(version);",
    "Added lines": "+  try {\n+    version_ = caffe2::stoull(version);\n+  } catch (const std::invalid_argument &e) {\n+    CAFFE_THROW(\"Couldn't parse the version \",\n+                 version,\n+                 \" as Long Long.\");\n+  }",
    "Label": "clean"
},
{
    "Id": 996,
    "Library": "pytorch",
    "Date": "2022/08/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/35b4ac4eebe80090fce05c2ce40dc0ea52004c9b",
    "Root Cause": "N.A",
    "Bug report": "remove unused/debug header (#82845)\n\n### Description\nMissed one of the review comments in https://github.com/pytorch/pytorch/pull/82731 . Namely, to remove an unused `<iostream>` that was used for debugging\n\n### Issue\n<!-- Link to Issue ticket or RFP -->\n\n### Testing\n<!-- How did you test your change? -->\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/82845\nApproved by: https://github.com/Chillee, https://github.com/albanD",
    "Number of deleted lines": 1,
    "Deleted lines": "-#include <iostream>",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 997,
    "Library": "pytorch",
    "Date": "2022/08/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/39ffad392c49aafcfeba05e2704bb1b666247471",
    "Root Cause": "N.A",
    "Bug report": "Fix faulty, vectorized `pow` function on VSX (#82646)\n\nThis fixes the remaining bug introduced by the VSX optimized code in https://github.com/pytorch/pytorch/pull/41541\n\nFollowup to https://github.com/pytorch/pytorch/pull/59382\n\n### Description\n\nThe code currently returns wrong results on POWER9LE making e.g. the `test_binary_ufuncs` fail.\n\n### Testing\n\nBuild and ran tests on PPC\nPull Request resolved: https://github.com/pytorch/pytorch/pull/82646\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 21,
    "Deleted lines": "-    auto x = *this;\n-    auto sign_bit = (*this) & sign_mask;\n-    // |b|\n-    auto exp_abs = exp.abs();\n-    auto exp_trunc = exp.trunc();\n-    Vectorized<float> odd_mask;\n-    odd_mask._vecb0 = (vec_signed(exp._vec0) & vi_1) != vi_0;\n-    odd_mask._vecb1 = (vec_signed(exp._vec1) & vi_1) != vi_0;\n-    // using ln fuction\n-    auto temp = (abs().log() * exp).exp();\n-\n-    // is odd or even check from Sleef\n-    auto is_int = (exp == exp_trunc) | (exp_abs >= vcheck);\n-    auto is_odd = odd_mask & is_int & (exp_abs < vcheck);\n-    // if even then then pow result should be absolute\n-    auto temp_sign = temp | sign_bit; // copy_sign\n-    auto out = blendv(temp, temp_sign, is_odd);\n-    // x<0 and y != N, then NAN\n-    auto out1 = blendv(out, v_nan, ((exp.floor() != exp) & (x < zero)));\n-    // y = 0 then 1\n-    return blendv(out1, one, (exp_abs == zero));",
    "Added lines": "+    return {Sleef_powf4_u10vsx(_vec0, exp._vec0), Sleef_powf4_u10vsx(_vec1, exp._vec1)};",
    "Label": "clean"
},
{
    "Id": 998,
    "Library": "pytorch",
    "Date": "2022/07/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6e56629efa281db262c79f8e2f0467ca4f2c190c",
    "Root Cause": "N.A",
    "Bug report": "[JIT] JIT script init verbose assert (#80495)\n\nLog the sizes of inputs in the assert of setInputTensorTypes(...)\nin jit/python/script_init.cpp for easy debugging.\nHelps/close:\nhttps://github.com/pytorch/pytorch/issues/72763\nFixes #72763\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/80495\nApproved by: https://github.com/davidberard98",
    "Number of deleted lines": 1,
    "Deleted lines": "-    TORCH_INTERNAL_ASSERT(input_values.size() == param_count_list.size());",
    "Added lines": "+    TORCH_INTERNAL_ASSERT(\n+        input_values.size() == param_count_list.size(),\n+        \" input_values:\",\n+        input_values.size(),\n+        \" vs param_count_list:\",\n+        param_count_list.size());",
    "Label": "clean"
},
{
    "Id": 999,
    "Library": "pytorch",
    "Date": "2022/07/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/df4226b48a52180dc5f080c4078c7f7546090c81",
    "Root Cause": "N.A",
    "Bug report": "[ONNX] Fix bug using std::copy_if (#80999)\n\n`std::copy_if` requires pre-allocated destination. Hence `scopes` needs to either be pre allocated, or use `std::inserter` for copy. Ref: https://en.cppreference.com/w/cpp/algorithm/copy\nPull Request resolved: https://github.com/pytorch/pytorch/pull/80999\nApproved by: https://github.com/AllenTiTaiWang, https://github.com/garymm",
    "Number of deleted lines": 3,
    "Deleted lines": "-        input_scopes.begin(), input_scopes.end(), scopes.begin(), IsValidScope);\n-        scopes.begin(),\n-",
    "Added lines": "+        input_scopes.begin(),\n+        input_scopes.end(),\n+        std::back_inserter(scopes),\n+        IsValidScope);\n+        std::back_inserter(scopes),",
    "Label": "clean"
},
{
    "Id": 1000,
    "Library": "pytorch",
    "Date": "2022/05/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ff9558a2ea626b1526ded6848e06e1e809fae53c",
    "Root Cause": "N.A",
    "Bug report": "[functorch] Update neural_tangent_kernels.ipynb (pytorch/functorch#788)\n\nFix a small bug\r\n```python3\r\n    if compute == 'full':\r\n        return result\r\n    if compute == 'trace':\r\n        return torch.einsum('NMKK->NM')        # should be torch.einsum('NMKK->NM', result)\r\n    if compute == 'diagonal':\r\n        return torch.einsum('NMKK->NMK')        # should be torch.einsum('NMKK->NMK', result)\r\n```",
    "Number of deleted lines": 2,
    "Deleted lines": "-    \"        return torch.einsum('NMKK->NM')\\n\",\n-    \"        return torch.einsum('NMKK->NMK')\"",
    "Added lines": "+    \"        return torch.einsum('NMKK->NM', result)\\n\",\n+    \"        return torch.einsum('NMKK->NMK', result)\"",
    "Label": "clean"
},
{
    "Id": 1001,
    "Library": "pytorch",
    "Date": "2022/02/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f21e573c99e10fdccc7abdb403f04357e121d7ed",
    "Root Cause": "N.A",
    "Bug report": "[functorch] Workaround to avoid Torchscript bug for new_empty (pytorch/functorch#538)",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if node.target == torch.ops.aten.new_zeros:",
    "Added lines": "+        if node.target in (torch.ops.aten.new_zeros, torch.ops.aten.new_empty):\n+        elif node.target == torch.ops.aten.avg_pool2d_backward:\n+            # Handle empty strides\n+            if node.args[3] == []:\n+                args = list(node.args)\n+                args[3] = [1, 1]\n+                node.args = tuple(args)",
    "Label": "clean"
},
{
    "Id": 1002,
    "Library": "pytorch",
    "Date": "2021/04/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/03550ccd619ed798fe049460d58abb98368b64ea",
    "Root Cause": "N.A",
    "Bug report": "[functorch] Added debug commands",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+## Debugging\n+`functorch._C.dump_tensor`: Dumps dispatch keys on stack\n+`torch._C._debug_only_display_vmap_fallback_warnings(True)`: Shows vmap fallbacks to loop/stack\n+",
    "Label": "clean"
},
{
    "Id": 1003,
    "Library": "pytorch",
    "Date": "2022/07/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fca695edbe3000d291d13b0e9015fba4c181630d",
    "Root Cause": "N.A",
    "Bug report": "modify get_attr to work with HF model and fix a bug in multiplication (#81376)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/81376\nApproved by: https://github.com/jamesr66a, https://github.com/anijain2305",
    "Number of deleted lines": 9,
    "Deleted lines": "-    op_eq, op_matching, op_consistency, op_leq, op_precision, op_gt, op_div, op_sub, op_neq, op_lt, op_add\n-from torch.fx.experimental.graph_gradual_typechecker import get_parameter\n-def add_inference_rule(n: Node, symbols, constraints, counter):\n-            c = Conj([BinConstraintD(my_output, BinConstraintD(e1, n.args[1], op_add), op_eq),\n-            c = Conj([BinConstraintD(my_output, BinConstraintD(e2, n.args[0], op_add), op_eq),\n-            t = get_parameter(self.traced, n.target)  # type: ignore[arg-type]\n-            if isinstance(t.data, torch.Tensor):\n-                if len(t.data.shape) > 0:\n-                    for t in t.data.shape:",
    "Added lines": "+    op_eq, op_matching, op_consistency, op_leq, op_precision, op_gt, op_div, op_sub, op_neq, op_lt, op_add, op_mul\n+def broadcasting_inference_rule(n: Node, symbols, constraints, counter):\n+\n+    op_code = None\n+    if n.target == operator.add or n.target == torch.add:\n+        op_code = op_add\n+    elif n.target == operator.mul:\n+        op_code = op_mul\n+            c = Conj([BinConstraintD(my_output, BinConstraintD(e1, n.args[1], op_code), op_eq),\n+            c = Conj([BinConstraintD(my_output, BinConstraintD(e2, n.args[0], op_code), op_eq),\n+        self.traced_params = dict(self.traced.named_parameters())\n+            t = self.traced_params.get(n.target, None)\n+\n+            if isinstance(t, torch.Tensor):\n+                if len(t.shape) > 0:\n+                    for t in t.shape:",
    "Label": "clean"
},
{
    "Id": 1004,
    "Library": "pytorch",
    "Date": "2022/07/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/72de816f5c6c74a866359ed394893ab88073e1a0",
    "Root Cause": "N.A",
    "Bug report": "GIL acquire needed in ValueCache::trimPrefixes (#81061)\n\nSummary: Dubugged a segfault issue in Ondemand python tracing. Committing as a a separate diff from D37410204.\n\nTest Plan:\n- run a python test case with the following command for on-demand flow:\necho -e \"PYTHON_STACK_TRACE=true\" > /tmp/scott_kineto.conf && dyno gputrace --gputrace_duration 300ms --gpuconf /tmp/scott_kineto.conf\n\nReviewed By: chaekit\n\nDifferential Revision: D37662988\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/81061\nApproved by: https://github.com/albanD",
    "Number of deleted lines": 3,
    "Deleted lines": "-  static auto prefixes = py::module::import(\"torch.profiler.python_tracer\")\n-                             .attr(\"_prefix_regex\")()\n-                             .cast<std::vector<std::string>>();",
    "Added lines": "+  static const auto prefixes = []() {\n+    pybind11::gil_scoped_acquire gil;\n+    return py::module::import(\"torch.profiler.python_tracer\")\n+        .attr(\"_prefix_regex\")()\n+        .cast<std::vector<std::string>>();\n+  }();",
    "Label": "clean"
},
{
    "Id": 1005,
    "Library": "pytorch",
    "Date": "2022/07/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1810c876df592ebaf4adc6ee109cc45d37974ec6",
    "Root Cause": "N.A",
    "Bug report": "Remove noexcept from dtype (#80991)\n\nAssuming we virtualize it soon the noexcept would cause bugs.\nLayout's noexcept is being removed by the layout slow path PR.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/80991\nApproved by: https://github.com/cpuhrsch",
    "Number of deleted lines": 1,
    "Deleted lines": "-  caffe2::TypeMeta dtype() const noexcept {",
    "Added lines": "+  caffe2::TypeMeta dtype() const {",
    "Label": "clean"
},
{
    "Id": 1006,
    "Library": "pytorch",
    "Date": "2022/07/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3c4c7d3e6b9c254f251b6643c2cbf077a0618b7f",
    "Root Cause": "N.A",
    "Bug report": "[Release Notes] fix bug with categorize call (#81284)\n\nThis was pointed out by @kit1980 in #78190\nPull Request resolved: https://github.com/pytorch/pytorch/pull/81284\nApproved by: https://github.com/kit1980",
    "Number of deleted lines": 1,
    "Deleted lines": "-            current_commits.commits[i] = CommitList.categorize(c.commit_hash, c.title)",
    "Added lines": "+            feature_item = get_commit_data_cache().get(c.commit_hash)\n+            features = features_to_dict(feature_item)\n+            category, topic = CommitList.categorize(features)\n+            current_commits[i] = dataclasses.replace(c, category=category, topic=topic)",
    "Label": "clean"
},
{
    "Id": 1007,
    "Library": "pytorch",
    "Date": "2022/07/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6ee54a878081f4ebde074dfc028ae181be70f290",
    "Root Cause": "N.A",
    "Bug report": "fix weight norm backward bug on CPU when OMP_NUM_THREADS <= 2 (#80930)\n\nfix https://github.com/pytorch/pytorch/issues/80569\nroot cause: `weight_norm_backward_last_dim_kernel` will create a temp buffer to\ndo vertical reduction, size of [num_threads, N] (N is the size of last dimension of v)\n\nto save additional memory allocation, the original kernel reuse the buffer after\nthe vertical sum:\n  1st row stores the final result of sum\n  2nd row stores coefficient a\n  3rd row stores coefficient b\n\nwhen OMP_NUM_THREADS <=2, this will cause illegal memory access since the buffer size\nwill be only 1*N or 2*N;\n\nthe fix is use a separate buffer (`a_b`) to store the coefficients of a and b.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/80930\nApproved by: https://github.com/frank-wei, https://github.com/malfet",
    "Number of deleted lines": 1,
    "Deleted lines": "-  Tensor buffer = at::empty({num_threads, N}, saved_norm.options()).zero_();",
    "Added lines": "+  // the temp buffer will be used twice:\n+  // 1. vertical reduction from [M, N] to [T, N]\n+  // 2. store the intermediate data of `sum`, `a` and `b`,\n+  //    so need to make sure it has at least 3 rows\n+  //\n+  int K = std::max(3, num_threads);\n+  Tensor buffer = at::empty({K, N}, saved_norm.options()).zero_();\n+  // reuse the 1st row of buffer to store the sum\n+  // 2nd row to store coefficient a\n+  // 3rd row to store coefficient b",
    "Label": "clean"
},
{
    "Id": 1008,
    "Library": "pytorch",
    "Date": "2022/07/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e89b1991c47740553e3a5256412d157a7d7f6dad",
    "Root Cause": "N.A",
    "Bug report": "Detect ProxyTensor layering violations (#80994)\n\nI found this code useful when debugging some proxy tensor confusion\nand I hope you will too.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/80994\nApproved by: https://github.com/zou3519",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        # This detects situations where you accidentally put a ProxyTensor\n+        # inside a ProxyTensor for the same trace; this is a layering violation\n+        assert not (isinstance(elem, ProxyTensor) and elem.proxy.tracer is proxy.tracer)",
    "Label": "clean"
},
{
    "Id": 1009,
    "Library": "pytorch",
    "Date": "2022/07/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/be14dca4811e6c258512d75e3140dab4f270fdf8",
    "Root Cause": "N.A",
    "Bug report": "Remove unnecessary debug asserts (#80971)\n\nFixes https://github.com/pytorch/pytorch/issues/79646\nPull Request resolved: https://github.com/pytorch/pytorch/pull/80971\nApproved by: https://github.com/peterbell10, https://github.com/bdhirsh",
    "Number of deleted lines": 8,
    "Deleted lines": "-  // currently a tensor should never have both conj and neg bit set\n-  // the only way to get an imag bit is complex_tensor.conj().imag but there's\n-  // no intended designed mechanism to enter the complex world with this imag bit\n-  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!self.is_conj());\n-  // currently a tensor should never have both conj and neg bit set\n-  // the only way to get an imag bit is complex_tensor.conj().imag but there's\n-  // no intended designed mechanism to enter the complex world with this imag bit\n-  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!self.is_neg());",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1010,
    "Library": "pytorch",
    "Date": "2022/07/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/45f77189c178597f683157106e8de24befd5f450",
    "Root Cause": "N.A",
    "Bug report": "fix the invalid configuration argument error when running layer norm backward (#80893)\n\nSummary: Fix the corner case with N = 0\n\nTest Plan:\nbuck run mode/opt //deeplearning/fbgemm/fbgemm_gpu/fb:layer_norm_test 2>&1 | tee out.log\n\nBefore this Diff\n```\ntest_swish_layer_norm (fbgemm_gpu.test.layer_norm_test.SparseOpsTest) ... INFO:2022-07-05 09:00:32 738347:738347 CuptiActivityProfiler.cpp:166] CUDA versions. CUPTI: 14; Runtime: 11040; Driver: 11040\nFalsifying example: test_swish_layer_norm(\n    self=<fbgemm_gpu.test.layer_norm_test.SparseOpsTest testMethod=test_swish_layer_norm>,\n    M=1,\n    N=0,\n    dtype=torch.float32,\n    device='cuda',\n    epsilon=0.1,\n)\nERROR\n\n======================================================================\nERROR: test_swish_layer_norm (fbgemm_gpu.test.layer_norm_test.SparseOpsTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/data/users/jianyuhuang/fbsource/fbcode/buck-out/opt/gen/aab7ed39/deeplearning/fbgemm/fbgemm_gpu/fb/layer_norm_test#binary,link-tree/fbgemm_gpu/test/layer_norm_test.py\", line 41, in test_swish_layer_norm\n    M=st.integers(0, 32),\n  File \"/data/users/jianyuhuang/fbsource/fbcode/buck-out/opt/gen/aab7ed39/deeplearning/fbgemm/fbgemm_gpu/fb/layer_norm_test#binary,link-tree/hypothesis/core.py\", line 1164, in wrapped_test\n    raise the_error_hypothesis_found\n  File \"/data/users/jianyuhuang/fbsource/fbcode/buck-out/opt/gen/aab7ed39/deeplearning/fbgemm/fbgemm_gpu/fb/layer_norm_test#binary,link-tree/fbgemm_gpu/test/layer_norm_test.py\", line 88, in test_swish_layer_norm\n    Y_ref.backward(grad_output, retain_graph=True)\n  File \"/data/users/jianyuhuang/fbsource/fbcode/buck-out/opt/gen/aab7ed39/deeplearning/fbgemm/fbgemm_gpu/fb/layer_norm_test#binary,link-tree/torch/_tensor.py\", line 401, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n  File \"/data/users/jianyuhuang/fbsource/fbcode/buck-out/opt/gen/aab7ed39/deeplearning/fbgemm/fbgemm_gpu/fb/layer_norm_test#binary,link-tree/torch/autograd/__init__.py\", line 191, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: CUDA error: invalid configuration argument\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\n----------------------------------------------------------------------\nRan 1 test in 3.578s\n\nFAILED (errors=1)\n```\n\nDifferential Revision: D37618022\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/80893\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (M > 0) {",
    "Added lines": "+  if (M > 0 && N > 0) {",
    "Label": "clean"
},
{
    "Id": 1011,
    "Library": "pytorch",
    "Date": "2022/06/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7549f901a51e8c02d12a5f88c9c2a51a9433b053",
    "Root Cause": "N.A",
    "Bug report": "`__launch_bounds__` for `torch.mode` with CUDA 11.7 (#79710)\n\nThis is a temporary fix for `TestReductionsCUDA.test_mode_large_cuda` which fails with CUDA 11.7 due to the following:\n\n```\nTraceback (most recent call last):\n  File \"/opt/pytorch/pytorch/torch/testing/_internal/common_utils.py\", line 1805, in wrapper\n    method(*args, **kwargs)\n  File \"/opt/pytorch/pytorch/torch/testing/_internal/common_utils.py\", line 1805, in wrapper\n    method(*args, **kwargs)\n  File \"/opt/pytorch/pytorch/torch/testing/_internal/common_device_type.py\", line 390, in instantiated_test\n    raise rte\n  File \"/opt/pytorch/pytorch/torch/testing/_internal/common_device_type.py\", line 377, in instantiated_test\n    result = test(self, **param_kwargs)\n  File \"/opt/pytorch/pytorch/torch/testing/_internal/common_device_type.py\", line 943, in only_fn\n    return fn(slf, *args, **kwargs)\n  File \"test_reductions.py\", line 891, in test_mode_large\n    testset_for_shape((10, 2048), 10)\n  File \"test_reductions.py\", line 883, in testset_for_shape\n    self._test_mode_intervals(shape, [(i, d - i)], device)\n  File \"test_reductions.py\", line 870, in _test_mode_intervals\n    values, indices = torch.mode(x, -1, False)\nRuntimeError: CUDA error: too many resources requested for launch\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n```\n\ncc @ptrblck\nPull Request resolved: https://github.com/pytorch/pytorch/pull/79710\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11070\n+__launch_bounds__(1024, 1)\n+#endif",
    "Label": "clean"
},
{
    "Id": 1012,
    "Library": "pytorch",
    "Date": "2022/06/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/848af372091bf4369e8fb02b8d0cef7dc8f363da",
    "Root Cause": "N.A",
    "Bug report": "Debug small ACC subgraphs elimination (#80117)\n\nReviewed By: yinghai\n\nDifferential Revision: D37368729\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/80117\nApproved by: https://github.com/yinghai, https://github.com/houseroad",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                    print(\n+                        \"Eliminating acc subgraph because it's smaller than the threshold: \"\n+                        f\"{len(subgraph.nodes)} < {self.settings.min_acc_module_size}\"\n+                    )\n+        acc_subgraphs_count = len([s for s in subgraphs if s.is_acc])\n+        non_acc_subgraphs_count = len(subgraphs) - acc_subgraphs_count\n+        print(f\"Got {acc_subgraphs_count} acc subgraphs and {non_acc_subgraphs_count} non-acc subgraphs\")",
    "Label": "clean"
},
{
    "Id": 1013,
    "Library": "pytorch",
    "Date": "2022/06/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e7ed01720a5911d89d6b0020d98720bcf538e390",
    "Root Cause": "N.A",
    "Bug report": "[Static Runtime] Fix MemoryPlanner dtor crash in debug mode (#79942)\n\nSummary:\nMemory planner destruction was hitting [this assertion](https://www.internalfb.com/code/fbsource/[f8baf8a0bab462c860d2eb7491a4e3fb40d2907a]/fbcode/caffe2/c10/util/intrusive_ptr.h?lines=117) in debug mode for a few models.\n\nHere's what was going on:\n\n1) The set of unmanaged `IValue`s acquires one or more owning refs of a managed `StorageImpl`\n2) Then, one or more tensors in that storage group have their `StorageImpl` swapped out during execution\n3) During `deallocateManagedTensors`, we swap the correct `StorageImpl` back in, [calling `unsafe_adapt_non_heap_allocated` again and resetting the refcount](https://www.internalfb.com/code/fbsource/[f8baf8a0bab462c860d2eb7491a4e3fb40d2907a]/fbcode/caffe2/torch/csrc/jit/runtime/static/memory_planner.cpp?lines=446-452)\n4) The unmanaged `IValues` are deallocated, decrementing the refcount into the danger zone.\n\nSo, we just have to make sure that unmanaged `IValue`s are destructed before we deallocate the managed tensors.\n\nTest Plan: CI\n\nDifferential Revision: D37303728\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/79942\nApproved by: https://github.com/tenpercent",
    "Number of deleted lines": 1,
    "Deleted lines": "-  deallocateManagedTensors();",
    "Added lines": "+  // It's important to call this function after all other owning refs\n+  // of the managed StorageImpls are cleaned up. It can reset the\n+  // the StorageImpl's refcount to (# tensors in storage group),\n+  // so destructing any owning refs afterwards will bring the refcount\n+  // lower than expected and trigger the debug assertion in\n+  // ~intrusive_ptr_target.\n+  deallocateManagedTensors();",
    "Label": "clean"
},
{
    "Id": 1014,
    "Library": "pytorch",
    "Date": "2022/06/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/88e2229a204fb54e48171b7e7f217cda7ebadac9",
    "Root Cause": "N.A",
    "Bug report": "Use C++17 for RocksDB 7 header. (#75741)\n\nCannot simply enable this globally because gcc doesn't handle constexpr well when mixing 14/17 together:\n- https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101957\n\nResolve https://github.com/pytorch/pytorch/issues/75496\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75741\nApproved by: https://github.com/malfet",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+# RocksDB 7 uses C++17 STL in header.\n+if(RocksDB_VERSION_MAJOR VERSION_GREATER_EQUAL 7)\n+  set_target_properties(caffe2_rocksdb PROPERTIES CXX_STANDARD 17)\n+endif()",
    "Label": "clean"
},
{
    "Id": 1015,
    "Library": "pytorch",
    "Date": "2022/06/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a7dde90173bbba2982a4ac838e2222f252349385",
    "Root Cause": "N.A",
    "Bug report": "[CI] further shard linux debug jobs to 4 (#79412)\n\nThey've been timing out on trunk sometimes https://github.com/pytorch/pytorch/runs/6858966332?check_suite_focus=true\nPull Request resolved: https://github.com/pytorch/pytorch/pull/79412\nApproved by: https://github.com/atalman, https://github.com/malfet",
    "Number of deleted lines": 2,
    "Deleted lines": "-          { config: \"default\", shard: 1, num_shards: 2, runner: \"linux.4xlarge.nvidia.gpu\" },\n-          { config: \"default\", shard: 2, num_shards: 2, runner: \"linux.4xlarge.nvidia.gpu\" },",
    "Added lines": "+          { config: \"default\", shard: 1, num_shards: 4, runner: \"linux.4xlarge.nvidia.gpu\" },\n+          { config: \"default\", shard: 2, num_shards: 4, runner: \"linux.4xlarge.nvidia.gpu\" },\n+          { config: \"default\", shard: 3, num_shards: 4, runner: \"linux.4xlarge.nvidia.gpu\" },\n+          { config: \"default\", shard: 4, num_shards: 4, runner: \"linux.4xlarge.nvidia.gpu\" },",
    "Label": "clean"
},
{
    "Id": 1016,
    "Library": "pytorch",
    "Date": "2022/06/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cec251fc4bfa76ad77ad748559d52055cc579f27",
    "Root Cause": "N.A",
    "Bug report": "[lint] Don't invoke lintrunner with --verbose\n\nIt's been running for a while and is stable, so we don't need debugging\nlogging anymore. This should reduce noise for people.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/79216\n\nApproved by: https://github.com/seemethere",
    "Number of deleted lines": 1,
    "Deleted lines": "-          if ! lintrunner --verbose --force-color --all-files --tee-json=lint.json; then",
    "Added lines": "+          if ! lintrunner --force-color --all-files --tee-json=lint.json; then",
    "Label": "clean"
},
{
    "Id": 1017,
    "Library": "pytorch",
    "Date": "2022/06/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7541b07f62c1af7763fa6ce49441bea63a1e5e26",
    "Root Cause": "N.A",
    "Bug report": "Prevent invariant assert failure on win+debug (#78636)\n\nSummary:\nProvide missing break statements to correct for IListRef\nhandling on Windows MSVC Debug builds.\n\nTest Plan:\nRun any torch IListRefIterator destruction on windows in a\ndebug mode build. Without this change, it will destruct incorrectly for\nBoxed IListRefTag, and invariably hit an always-fail assert for IListRef\ntag. With it, proper destruction, iterator payload assignment, and no\nassert encounter with valid IListRefTag will occur.\n\nDifferential Revision: D36811554\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78636\nApproved by: https://github.com/malfet, https://github.com/atalman",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        break;\n+        break;\n+        break;\n+        break;",
    "Label": "clean"
},
{
    "Id": 1018,
    "Library": "pytorch",
    "Date": "2022/06/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5158a6b41a7a2779786042c973cda6a31f24aa01",
    "Root Cause": "N.A",
    "Bug report": "Foward fix sharding bug for DL (#79124)\n\nThis PR solves a bug introduced by #79041\n\n`torch.utils.data.graph_settings.apply_sharding` changes the datapipe in-place and returns `None`\n\nIt would resolve the Error in TorchData. See: https://github.com/pytorch/data/actions/runs/2461030312\nPull Request resolved: https://github.com/pytorch/pytorch/pull/79124\nApproved by: https://github.com/VitalyFedyunin",
    "Number of deleted lines": 2,
    "Deleted lines": "-                self.dataset = torch.utils.data.graph_settings.apply_sharding(self.dataset, ws, rank)\n-                self.dataset = torch.utils.data.graph_settings.apply_sharding(self.dataset, ws, rank)",
    "Added lines": "+                torch.utils.data.graph_settings.apply_sharding(self.dataset, ws, rank)\n+                torch.utils.data.graph_settings.apply_sharding(self.dataset, ws, rank)",
    "Label": "clean"
},
{
    "Id": 1019,
    "Library": "pytorch",
    "Date": "2022/06/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a886bd387c407c11b1d040176fa142a6c4c7bec1",
    "Root Cause": "N.A",
    "Bug report": "ci: Add clickable debug messages for trymerge\n\nSigned-off-by: Eli Uriegas <eliuriegasfb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78941\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nApproved by: https://github.com/janeyx99",
    "Number of deleted lines": 1,
    "Deleted lines": "-            print(f\"Merged failed due to: {ex}. Retrying in 60 seconds.\")",
    "Added lines": "+        print(f\"Attempting merge of https://github.com/{org}/{project}/pull/{pr_num} ({elapsed_time / 60} minutes elapsed)\")\n+            print(f\"Merge of https://github.com/{org}/{project}/pull/{pr_num} failed due to: {ex}. Retrying in 60 seconds.\")",
    "Label": "clean"
},
{
    "Id": 1020,
    "Library": "pytorch",
    "Date": "2022/05/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ee86af17fb5bcc731c18b0d5d06b02ff40ae8eaa",
    "Root Cause": "N.A",
    "Bug report": "[CI]Preserve `.ninja_log` for Mac builds (#78387)\n\nAnd `compile_commands.json` which would be very useful for debug\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78387\nApproved by: https://github.com/suo",
    "Number of deleted lines": 1,
    "Deleted lines": "-          zip -1 -r artifacts.zip dist/",
    "Added lines": "+          zip -1 -r artifacts.zip dist/ build/.ninja_log build/compile_commands.json",
    "Label": "clean"
},
{
    "Id": 1021,
    "Library": "pytorch",
    "Date": "2022/05/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c1cbe3bad3a2a42118fca5e8a4061ab3bfb1326a",
    "Root Cause": "N.A",
    "Bug report": "Enhance the _rebuild_qtensor to support other device type other than CPU (#78234)\n\n## Motivation\nThere is a bug in torch._utils.rebuild_qtensor when to restore a qtensor from pickle for not CPU device type. The tensor is created on the CPU device but set to a storage which maybe a different device type.\n\n## Solution\nCreate the qtensor based on the storage device type.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78234\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 6,
    "Deleted lines": "-        tensor = torch._empty_affine_quantized(size, scale=scale, zero_point=zero_point, dtype=storage.dtype)\n-                scales = torch.tensor(scales, dtype=torch.double)\n-                zero_points = torch.tensor(zero_points, dtype=torch.long)\n-                scales = torch.tensor(scales, dtype=torch.float)\n-                zero_points = torch.tensor(zero_points, dtype=torch.float)\n-            size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype)",
    "Added lines": "+        tensor = torch._empty_affine_quantized(size, scale=scale, zero_point=zero_point, dtype=storage.dtype, device=storage.device)\n+                scales = torch.tensor(scales, dtype=torch.double, device=storage.device)\n+                zero_points = torch.tensor(zero_points, dtype=torch.long, device=storage.device)\n+                scales = torch.tensor(scales, dtype=torch.float, device=storage.device)\n+                zero_points = torch.tensor(zero_points, dtype=torch.float, device=storage.device)\n+            size, scales=scales, zero_points=zero_points, axis=axis, dtype=storage.dtype, device=storage.device)",
    "Label": "clean"
},
{
    "Id": 1022,
    "Library": "pytorch",
    "Date": "2022/05/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/de646c06d457277f24d790eb41545e72f2d76f66",
    "Root Cause": "N.A",
    "Bug report": "fix jit List[Optional[Tensor]] type singleton bug\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/77846\n\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 2,
    "Deleted lines": "-      fake_value = c10::TypeFactory::create<c10::OptionalType>(fake_value);\n-      real_value = c10::TypeFactory::create<c10::OptionalType>(real_value);",
    "Added lines": "+      fake_value = c10::OptionalType::get(fake_value);\n+      real_value = c10::OptionalType::get(real_value);",
    "Label": "clean"
},
{
    "Id": 1023,
    "Library": "pytorch",
    "Date": "2022/05/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/46e16737d4622437bc5caebd382b1fd0a778141e",
    "Root Cause": "N.A",
    "Bug report": "[GHA] Use silent checkout (#77547)\n\nThis switch changes the checkout action to https://github.com/malfet/checkout/tree/silent-checkout, that contains 2 commits on top of trunk:\n- @zhouzhuojie's [Add retry](https://github.com/zhouzhuojie/checkout/commit/ffc6f93ad4b6e3cdcdd1a34e8c896765002f9b34)\n- Mine [Add silent-checkout](https://github.com/malfet/checkout/commit/96ebf2582e6a298abb8d156b195838ff22be4bf8)\n\nTesting by pushing temporary https://github.com/pytorch/pytorch/pull/77547/commits/10b11f99129ab7efe69e4dfdbb0a541d2813e93e, and here is a lintrun with silent checkouts: https://github.com/pytorch/pytorch/runs/6455157988?check_suite_focus=true#step:3:69\n\nPotential downside of this change, is that checkout will be harder to debug if it hangs, but I don't think there were a single report of that\n\nFixes #77249\nPull Request resolved: https://github.com/pytorch/pytorch/pull/77547\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 1,
    "Deleted lines": "-      uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9",
    "Added lines": "+      uses: malfet/checkout@silent-checkout\n+        quiet-checkout: true",
    "Label": "clean"
},
{
    "Id": 1024,
    "Library": "pytorch",
    "Date": "2022/05/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/da3b8309cbeed2f4346a3b2015c4c2e1fcdc6321",
    "Root Cause": "N.A",
    "Bug report": "Disable ios 12.5.1 job as its logs don't reveal why it's failing (#77162)\n\nAfter the recent reverts (https://hud.pytorch.org/minihud?name_filter=trunk%20/%20ios-12-5-1-x86-64%20/%20build#2c5bf12584a8ec359cbce34fac73fb2bc3cd0af0), the iOS build is now failing. https://hud.pytorch.org/minihud?name_filter=trunk%20/%20ios-12-5-1-x86-64%20/%20build\n\nHowever, the logs are unhelpful about how to debug this failure and is inactionable as far as I can tell. Thus, I think the best path forward is to disable this build until an expert can let us know what's wrong and how we can surface the error better next time.\n<img width=\"1100\" alt=\"image\" src=\"https://user-images.githubusercontent.com/31798555/167633477-7966dbad-20df-4649-92e9-4f44af77025c.png\">\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/77162\nApproved by: https://github.com/atalman",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if: ${{ false }}",
    "Label": "clean"
},
{
    "Id": 1025,
    "Library": "pytorch",
    "Date": "2022/05/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4baf7c0899a2fa9c3630613f37d5fc65971db21c",
    "Root Cause": "N.A",
    "Bug report": "Dispatch to mv rather than mm in the case that tensor1.ndim == 1 and tensor2.ndim == 2\n\nThis should hopefully be faster, it makes the calling code simpler, and\nit solves a bug when using matmul with the out= parameter (before it\nwould throw an incorrect error).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75195\n\nApproved by: https://github.com/ezyang",
    "Number of deleted lines": 2,
    "Deleted lines": "-    return has_out ? at::mm_out(out, tensor1.unsqueeze(0), tensor2).squeeze_(0)\n-                   : tensor1.unsqueeze(0).mm(tensor2).squeeze_(0);",
    "Added lines": "+    return has_out ? at::mv_out(out, tensor2.t(), tensor1) : tensor2.t().mv(tensor1);",
    "Label": "clean"
},
{
    "Id": 1026,
    "Library": "pytorch",
    "Date": "2022/05/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/385e5ba561ff3f0fd834c935cf01d0d5f01383d5",
    "Root Cause": "N.A",
    "Bug report": "ns for fx: more meaningful error message when creating shadow model (#76468)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76468\n\nThis makes the error message when copying an unsupported node more verbose.\nThis is useful to debug where specifically in a user model this is failing.\n\nTest Plan:\n1. hardcode this condition to hit\n2. run NS tests\n3. verify the exception now prints details about the offending node\n\nReviewed By: jerryzh168\n\nDifferential Revision: D35978652\n\nPulled By: vkuzo\n\nfbshipit-source-id: 9cc93dfa46469bf6ef60aa38d4011041b6709df9\n(cherry picked from commit c6e382c2a69aba6ba66740f238bc14446521a433)",
    "Number of deleted lines": 1,
    "Deleted lines": "-            f\"handling of node with op {node_a.op} is not implemented\")",
    "Added lines": "+            f\"handling of node {node_a.format_node()} with op {node_a.op} is not implemented\")",
    "Label": "clean"
},
{
    "Id": 1027,
    "Library": "pytorch",
    "Date": "2022/04/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b7bd677eaeec12af4d1345e3562f4f42f5ea6782",
    "Root Cause": "N.A",
    "Bug report": "[quant][gpu][core][bug fix] Added memset to CacheKey for quantized cudnn linear op (#76447)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76447\n\nSee https://github.com/pytorch/pytorch/pull/76436. The same idea is\napplied here.\n\nTest Plan:\n```\npython test/test_quantization.py -k test_qlinear_cudnn\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D35970360\n\nPulled By: dzdang\n\nfbshipit-source-id: 452f0703d3c257638d2ef1e2467756aeed0ec8d4\n(cherry picked from commit ff52d26ad477bd2c0529b627ea3cc763523f80e6)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // memset is needed here because there is implicit packing added for CacheKey, and this can result in uninitialized padded values that are\n+  // used for hashing (see how at::native::ParamsHash is defined). without memset, we can potentially come across a situation where two\n+  // CacheKey objects have the same user defined parameters, but\n+  // different padded values, resulting in different hash outputs.\n+  memset(&key, 0, sizeof(key));",
    "Label": "clean"
},
{
    "Id": 1028,
    "Library": "pytorch",
    "Date": "2022/04/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b5b8139d46c051678f789f89fe87b795c4bee1f9",
    "Root Cause": "N.A",
    "Bug report": "[quant][gpu][core][bug fix] Added memset to CacheKey for quantized cudnn add op (#76445)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76445\n\nSee https://github.com/pytorch/pytorch/pull/76436. It is the same idea\nhere.\n\nTest Plan: python test/test_quantization.py -k test_qadd_relu_cudnn\n\nReviewed By: jerryzh168\n\nDifferential Revision: D35970281\n\nPulled By: dzdang\n\nfbshipit-source-id: 9e983b15fa630409fa0a0906f24d94f18f1f3e7b\n(cherry picked from commit 8b9be06ea7a2423e64f6ff760b1acab7423ad528)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // memset is needed here because there is implicit packing added for CacheKey, and this can result in uninitialized padded values that are\n+  // used for hashing (see how at::native::ParamsHash is defined). without memset, we can potentially come across a situation where two\n+  // CacheKey objects have the same user defined parameters, but\n+  // different padded values, resulting in different hash outputs.\n+  memset(&key, 0, sizeof(key));",
    "Label": "clean"
},
{
    "Id": 1029,
    "Library": "pytorch",
    "Date": "2022/04/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6c78dc46617a62b3fefe6951d78cbe55234716eb",
    "Root Cause": "N.A",
    "Bug report": "[quant][gpu][core][bug fix] Added memset to CacheKey for quantized cudnn conv2d op (#76436)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76436\n\nIn `quantized/cudnn/Conv.cpp`, memset was added for CacheKey. Memset is needed here because there is implicit padding added for CacheKey, and this can result in uninitialized padded values that are\n used for hashing ((see how at::native::ParamsHash is defined)). without memset, we can potentially come across a situation where two CacheKey objects have the same user defined parameters, but different padded values, resulting in different hash outputs.\n\nTest Plan:\n```\npython test/test_quantization.py -k test_qconv2d_cudnn\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D35965241\n\nPulled By: dzdang\n\nfbshipit-source-id: bdeab6c3d6d6066b71b2fb313ac851fe30ae5510\n(cherry picked from commit 4ac2b7a858ac62f78b49da3cf43c76d9a7371d29)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // memset is needed here because there is implicit packing added for CacheKey, and this can result in uninitialized padded values that are\n+  // used for hashing (see how at::native::ParamsHash is defined). without memset, we can potentially come across a situation where two\n+  // CacheKey objects have the same user defined parameters, but\n+  // different padded values, resulting in different hash outputs.\n+  memset(&key, 0, sizeof(key));",
    "Label": "clean"
},
{
    "Id": 1030,
    "Library": "pytorch",
    "Date": "2022/04/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f31d518283d1b3e4dceb248d16396e950d2bac52",
    "Root Cause": "N.A",
    "Bug report": "[GHF] Improve failures debugability\n\nPrint exception backtrace in the run log\nPrint GraphQL arguments as well as query itself when it fails\nPrint more concise message when PR is not reviewed\n\nMake error reports like https://github.com/pytorch/pytorch/pull/75851#issuecomment-1102868693 much easier to debug\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76121\nApproved by: https://github.com/janeyx99",
    "Number of deleted lines": 9,
    "Deleted lines": "-        raise RuntimeError(f\"GraphQL query {query} failed: {rc['errors']}\")\n-        rule_approvers_set = set()\n-        for approver in rule.approved_by:\n-            if \"/\" in approver:\n-                org, name = approver.split(\"/\")\n-                rule_approvers_set.update(gh_get_team_members(org, name))\n-            else:\n-                rule_approvers_set.add(approver)\n-        approvers_intersection = approved_by.intersection(rule_approvers_set)",
    "Added lines": "+        raise RuntimeError(f\"GraphQL query {query}, args {kwargs} failed: {rc['errors']}\")\n+        # If rule needs approvers but PR has not been reviewed, skip it\n+        if len(rule.approved_by) > 0 and len(approved_by) == 0:\n+            if reject_reason_score < 10000:\n+                reject_reason_score = 10000\n+                reject_reason = f\"Matched rule {rule_name}, but PR has not been reviewed yet\"\n+            continue\n+\n+        rule_approvers_set = set()\n+        for approver in rule.approved_by:\n+            if \"/\" in approver:\n+                org, name = approver.split(\"/\")\n+                rule_approvers_set.update(gh_get_team_members(org, name))\n+            else:\n+                rule_approvers_set.add(approver)\n+        approvers_intersection = approved_by.intersection(rule_approvers_set)\n+        import traceback\n+        traceback.print_exc()",
    "Label": "clean"
},
{
    "Id": 1031,
    "Library": "pytorch",
    "Date": "2022/04/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/911b2f2beb440369db7f8edf96aac36994529652",
    "Root Cause": "N.A",
    "Bug report": "[SR] Mark create_owned_ref with AliasAnalysisKind::CONSERVATIVE (#75381)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75381\n\nNoticed while debugging another issue that this op was triggering copies in `verify_and_correct_memory_overlap`.\n\nThis issue is that this op (by design) produces an alias, but `AliasDb` inferred a pure-function schema. Mark it conservative so it knows that aliases are produced.\nghstack-source-id: 154236396\n\nTest Plan: CI\n\nReviewed By: tenpercent\n\nDifferential Revision: D35450420\n\nfbshipit-source-id: 8c3535ab2514d41fdd4f578c1504adf2f377463f\n(cherry picked from commit 045f225bd1e8e4c533d5098e956010cea56639ec)",
    "Number of deleted lines": 1,
    "Deleted lines": "-  m.def(torch::schema(\"static_runtime::create_owned_ref(...) -> ...\"));",
    "Added lines": "+  m.def(torch::schema(\n+      \"static_runtime::create_owned_ref(...) -> ...\",\n+      c10::AliasAnalysisKind::CONSERVATIVE));",
    "Label": "clean"
},
{
    "Id": 1032,
    "Library": "pytorch",
    "Date": "2022/04/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/051802e4f4fc6c03b653dd4552998354730fdef2",
    "Root Cause": "N.A",
    "Bug report": "[quant][core][gpu][bug fix] Fixed off by one index issue in broadcasted_bias\n\nSummary:\nThere was an off by one index issue in new_size (used for broadcasted_bias).\nThis has now been corrected.\n\nThe matrix multiplication's output's last dimension is the number of out features, which is the\nsize of bias. We create `new_size` for `broadcasted_bias`, whom we want to have the same number\nof dimensions as the matmul output and for it have the same size for the last dimension for broadcasting purposes.\nPreviously, we had `new_size[1] = bias_.value().size(0);`, but this is wrong, in general.\n\nTest plan:\n```\npython test/test_quantization.py -k test_qlinear_cudnn\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75483\n\nApproved by: https://github.com/jerryzh168",
    "Number of deleted lines": 2,
    "Deleted lines": "-    // the input bias is a 1-D tensor whose size is the same as the size of the second dimension of quantized_output.\n-    new_size[1] = bias_.value().size(0);",
    "Added lines": "+    // the input bias is a 1-D tensor whose size is the same as the size of the last dimension of quantized_output\n+    new_size.back() = bias_.value().size(0);",
    "Label": "clean"
},
{
    "Id": 1033,
    "Library": "pytorch",
    "Date": "2022/04/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cf735d899a7a7effb520587e2a13638c278bef95",
    "Root Cause": "N.A",
    "Bug report": "[quant][core][bug fix][gpu] Added kReluFused to quantized cudnn conv operator's caching\n\nSummary:\nPrevious implementation of CacheKey neglected kReluFused, but we\nneed to be able to cache cases based on whether relu is activated or not,\notherwise we will run into situations in which uid is defined in VariantPack\nbut not in the operator graph.\n\nTest plan:\n```\npython test/test_quantization.py -k test_qconv2d_cudnn\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75711\n\nApproved by: https://github.com/jerryzh168",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  bool kReluFused;\n+  key.kReluFused = kReluFused;",
    "Label": "clean"
},
{
    "Id": 1034,
    "Library": "pytorch",
    "Date": "2022/04/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8d3e3ebc583eb3a4ca230fa41f06d8ef5d453d63",
    "Root Cause": "N.A",
    "Bug report": "Add note about testing inconsistency\n\nMitigates #56396\n\nThe true fix would be https://github.com/pytorch/pytorch/pull/75608 but there are many issues to debug and this is a good step forward to at least clarify.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75620\nApproved by: https://github.com/kit1980, https://github.com/seemethere, https://github.com/malfet",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+**Weird note:** In our CI (Continuous Integration) jobs, we actually run the tests from the `test` folder and **not** the root of the repo, since there are various dependencies we set up for CI that expects the tests to be run from the test folder. As such, there may be some inconsistencies between local testing and CI testing--if you observe an inconsistency, please [file an issue](https://github.com/pytorch/pytorch/issues/new/choose).\n+",
    "Label": "clean"
},
{
    "Id": 1035,
    "Library": "pytorch",
    "Date": "2022/04/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e0f9c69fcf9384a7d7e64ebacb219c11fc7ea928",
    "Root Cause": "N.A",
    "Bug report": "Fix addmm_cpu for int64\n\nThere was a bug for the case [a, 0] x [0, b] in addmm_cpu. We implement\na short-cut for this case.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75200\n\nApproved by: https://github.com/ngimel",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // Some paths in the code below do not handle multiplications of the form [a, 0] x [0, b]\n+  if (m1_sizes[1] == 0) {\n+    if (beta.toComplexDouble() == 0.0) {\n+      result.zero_();\n+    } else {\n+      if (!self.is_same(result)) {\n+        result.copy_(self);\n+      }\n+      result.mul_(beta);\n+    }\n+    return;\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 1036,
    "Library": "pytorch",
    "Date": "2022/04/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/78ba87ec4ba95c0b693f2c976d43bda358c7edc6",
    "Root Cause": "N.A",
    "Bug report": "[fx][ShapeProp] make shapes and args/kwargs concrete for minimizer (#75291)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75291\n\nAs the title. Also adds debugging messaging to `ShapeProp.run_node`\n\nReviewed By: yuhc\n\nDifferential Revision: D34930081\n\nfbshipit-source-id: ea4341ac2377b7b81404b14afeb5149d5556d92c\n(cherry picked from commit 8a929a910c17cff69fac501c5b9260bb23f260e2)",
    "Number of deleted lines": 1,
    "Deleted lines": "-        result = super().run_node(n)",
    "Added lines": "+import traceback\n+\n+\n+        try:\n+            result = super().run_node(n)\n+        except Exception:\n+            traceback.print_exc()\n+            raise RuntimeError(\n+                f\"ShapeProp error for: node={n.format_node()} with \"\n+                f\"meta={n.meta}\"\n+            )",
    "Label": "clean"
},
{
    "Id": 1037,
    "Library": "pytorch",
    "Date": "2022/03/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6707d67131b66f09d02d8cb3a5f092acd0607b16",
    "Root Cause": "N.A",
    "Bug report": "Expose GetMetaDataIfDebugging API (#74784)\n\nSummary:\nThis should allow torch/XLA to access this API\ncc wonjoolee95\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/74784\n\nReviewed By: shunting314\n\nDifferential Revision: D35159977\n\nPulled By: wconstab\n\nfbshipit-source-id: 1ad3ebb691acf6968314c1f37558d684d9bc1cdf\n(cherry picked from commit 7971ad0cd9ce3d40c56eb871b610c387faa30cac)",
    "Number of deleted lines": 1,
    "Deleted lines": "-MetaData GetMetaDataIfDebugging();",
    "Added lines": "+TORCH_API MetaData GetMetaDataIfDebugging();",
    "Label": "clean"
},
{
    "Id": 1038,
    "Library": "pytorch",
    "Date": "2022/03/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/37c5f11c16c2ef441c3577bddf1877ee68b82571",
    "Root Cause": "N.A",
    "Bug report": "[hotfix] hotfix a bug of shard tensor\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/74535\nApproved by: https://github.com/mrshenli",
    "Number of deleted lines": 1,
    "Deleted lines": "-            raise ValueError(f'Expecting ShardingSpec but got: {type(self._sharding_spec)}')",
    "Added lines": "+            raise ValueError(f'Expecting ShardingSpec but got: {type(sharding_spec)}')",
    "Label": "clean"
},
{
    "Id": 1039,
    "Library": "pytorch",
    "Date": "2022/03/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1f04a00ccf917e6837a479a2730af56920343374",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch Distributed] Update documentation about NCCL environment variables (#74006)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/74006\n\nupdated recommendations about environment variables to use during debug\nand performance tuning\n\nTest Plan: `make html`\n\nReviewed By: rohan-varma\n\nDifferential Revision: D34767454\n\nfbshipit-source-id: 08cd58469bf72b58702e50e82020fa19b43b5911\n(cherry picked from commit ac7e6630f8043f85d3d16be17c6a8ad1ebb2990c)",
    "Number of deleted lines": 8,
    "Deleted lines": "-NCCL has also provided a number of environment variables for fine-tuning purposes.\n-\n-Commonly used ones include the following for debugging purposes:\n-\n-- ``export NCCL_DEBUG=INFO``\n-- ``export NCCL_DEBUG_SUBSYS=ALL``\n-\n-For the full list of NCCL environment variables, please refer to",
    "Added lines": "+**Debugging** - in case of NCCL failure, you can set ``NCCL_DEBUG=INFO`` to print an explicit\n+warning message as well as basic NCCL initialization information.\n+\n+You may also use ``NCCL_DEBUG_SUBSYS`` to get more details about a specific\n+aspect of NCCL. For example, ``NCCL_DEBUG_SUBSYS=COLL`` would print logs of\n+collective calls, which may be helpful when debugging hangs, especially those\n+caused by collective type or message size mismatch. In case of topology\n+detection failure, it would be helpful to set ``NCCL_DEBUG_SUBSYS=GRAPH``\n+to inspect the detailed detection result and save as reference if further help\n+from NCCL team is needed.\n+\n+**Performance tuning** - NCCL performs automatic tuning based on its topology detection to save users'\n+tuning effort. On some socket-based systems, users may still try tuning\n+``NCCL_SOCKET_NTHREADS`` and ``NCCL_NSOCKS_PERTHREAD`` to increase socket\n+network bandwidth. These two environment variables have been pre-tuned by NCCL\n+for some cloud providers, such as AWS or GCP.\n+\n+For a full list of NCCL environment variables, please refer to",
    "Label": "clean"
},
{
    "Id": 1040,
    "Library": "pytorch",
    "Date": "2022/03/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/00b01ec056b778c77432e2d7f9250882c10bfda9",
    "Root Cause": "N.A",
    "Bug report": "disable LT interface (#74021)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/74021\n\nDisables cublasLt as it is buggy with cuda 11.0. We'll reenable it based on cuda version (some known bugs are fixed in 11.5)\n\nTest Plan: Existing tests\n\nReviewed By: jspark1105\n\nDifferential Revision: D34775050\n\nfbshipit-source-id: 3faf3d15a0d9e24ca99e3ab515d2bc73a4c51fb5\n(cherry picked from commit ac50bfc0ad5b96794292aedaf41d8232bf18355e)",
    "Number of deleted lines": 7,
    "Deleted lines": "-#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(_MSC_VER)\n-\n-    // https://docs.nvidia.com/cuda/cublas/index.html#cublasLt-general-description\n-    // Batch size > 65535 does not work in most cases.\n-    if (mat1_sizes[0] > 65535) {\n-      useLtInterface = false;\n-    }",
    "Added lines": "+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11040 && !defined(_MSC_VER)",
    "Label": "clean"
},
{
    "Id": 1041,
    "Library": "pytorch",
    "Date": "2022/03/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/122f8648ab8f872bd8ed970728c5ac894dce5300",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch Distributed] Add debug hint for NCCL async system error (#73897)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73897\n\nadd a debug hint that async system error can be caused by unexpected exit of\na remote process if not an actual network issue. For example, the exit of the remote process\ncan cause a closed network connection error at a local process. The hint helps to direct\nthe debug focus to the remote process.\n\nTest Plan: unit tests\n\nReviewed By: pritamdamania87, rohan-varma\n\nDifferential Revision: D34702348\n\nfbshipit-source-id: d19f9116e9efe5f6d76c0158a7a447616437ca69\n(cherry picked from commit 005e74b7b6764ecd832b3410063285bff2411b56)",
    "Number of deleted lines": 1,
    "Deleted lines": "-      return \"ncclSystemError: System call (socket, malloc, munmap, etc) failed.\";",
    "Added lines": "+      return \"ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. \"\n+        \"It can be also caused by unexpected exit of a remote peer, you can check NCCL warnings for failure reason and see if there is connection closure by a peer.\";",
    "Label": "clean"
},
{
    "Id": 1042,
    "Library": "pytorch",
    "Date": "2022/03/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e7051939fb56137ef171ca1a3b1a108fc4600f26",
    "Root Cause": "N.A",
    "Bug report": "[pkg] improve error message for module detection on saving pass (#73106)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73106\n\nThe original error message didn't have next steps, and someone got confused. This error message should make debugging a bit easier.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D34559499\n\nPulled By: PaliC\n\nfbshipit-source-id: fd5fec9c4db10a20775435a587bad24336a671ef\n(cherry picked from commit efdcf1e198389ee156a46cf0f8b185d8145d3266)",
    "Number of deleted lines": 1,
    "Deleted lines": "-                            \"pickled object doesn't contain any mocked objects.\"",
    "Added lines": "+                            \"pickled object doesn't contain any mocked objects. Try interning or externing\"\n+                            f\"{module} if {field} is supposed to be in the package.\"",
    "Label": "clean"
},
{
    "Id": 1043,
    "Library": "pytorch",
    "Date": "2022/02/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/20a037d80f51a74aabbcd305f2f3e3b55d41dd85",
    "Root Cause": "N.A",
    "Bug report": "[Core] Update Exception.h (#64553)\n\nSummary:\nJust a small thing that's a little annoying; currently `TORCH_INTERNAL_ASSERT` generates messages like this:\n\n```\nunknown file: Failure\nC++ exception with description \"g_outputs.count(out) > 0INTERNAL ASSERT FAILED at \"../torch/csrc/jit/passes/memory_planning.cpp\":481, please report a bug to PyTorch. 22\nException raised from getManagedLiveRangesFromMemEvents at ../torch/csrc/jit/passes/memory_planning.cpp:481 (most recent call first):\n```\n\ni.e. with no space between the `#cond` checked and `INTERNAL ASSERT FAILED...`\n\nRe the `STRIP_ERROR_MESSAGES` branch: I'm not sure whether the differences are intentional or accidental? I.e. we don't pass `__FILE__` and `__VA_ARGS__` to `torchCheckFail` whereas we do to `torchInternalAssertFail`. I can reconcile the two in this PR if it's simply an omission.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64553\n\nReviewed By: albanD\n\nDifferential Revision: D34482923\n\nPulled By: swolchok\n\nfbshipit-source-id: 40b648dadb5ceea5d9038efd60771d98aa7f0b46\n(cherry picked from commit 8d6f1168b2242dfc0d911cce39f0863260e384b1)",
    "Number of deleted lines": 17,
    "Deleted lines": "-#define TORCH_INTERNAL_ASSERT(cond, ...)                            \\\n-  if (C10_UNLIKELY_OR_CONST(!(cond))) {                             \\\n-    ::c10::detail::torchCheckFail(                                  \\\n-        __func__,                                                   \\\n-        __FILE__,                                                   \\\n-        static_cast<uint32_t>(__LINE__),                            \\\n-        #cond \"INTERNAL ASSERT FAILED at\" C10_STRINGIZE(__FILE__)); \\\n-#define TORCH_INTERNAL_ASSERT(cond, ...)                                        \\\n-  if (C10_UNLIKELY_OR_CONST(!(cond))) {                                         \\\n-    ::c10::detail::torchInternalAssertFail(                                     \\\n-        __func__,                                                               \\\n-        __FILE__,                                                               \\\n-        static_cast<uint32_t>(__LINE__),                                        \\\n-        #cond                                                                   \\\n-        \"INTERNAL ASSERT FAILED at \" C10_STRINGIZE(__FILE__) \":\" C10_STRINGIZE( \\\n-            __LINE__) \", please report a bug to PyTorch. \",                     \\\n-        c10::str(__VA_ARGS__));                                                 \\",
    "Added lines": "+#define TORCH_INTERNAL_ASSERT(cond, ...)                              \\\n+  if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n+    ::c10::detail::torchCheckFail(                                    \\\n+        __func__,                                                     \\\n+        __FILE__,                                                     \\\n+        static_cast<uint32_t>(__LINE__),                              \\\n+        #cond \" INTERNAL ASSERT FAILED at \" C10_STRINGIZE(__FILE__)); \\\n+#define TORCH_INTERNAL_ASSERT(cond, ...)                                         \\\n+  if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n+    ::c10::detail::torchInternalAssertFail(                                      \\\n+        __func__,                                                                \\\n+        __FILE__,                                                                \\\n+        static_cast<uint32_t>(__LINE__),                                         \\\n+        #cond                                                                    \\\n+        \" INTERNAL ASSERT FAILED at \" C10_STRINGIZE(__FILE__) \":\" C10_STRINGIZE( \\\n+            __LINE__) \", please report a bug to PyTorch. \",                      \\\n+        c10::str(__VA_ARGS__));                                                  \\",
    "Label": "clean"
},
{
    "Id": 1044,
    "Library": "pytorch",
    "Date": "2022/02/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1ef244e00348ec933b26642818eb61aa312c3928",
    "Root Cause": "N.A",
    "Bug report": "Fix tensor.__deepcopy__ for lazy device (#73197)\n\nSummary:\nA small bug that misses `lazy` in tensor.__deepcopy__, which results in segmentation when deepcopy a lazy model.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73197\n\nReviewed By: jbschlosser\n\nDifferential Revision: D34394482\n\nPulled By: wconstab\n\nfbshipit-source-id: c84fdb9b3a827677971fd3477a92679d7dbce3c0\n(cherry picked from commit c003d150cea969a6595ef8004ea82596fb9431b6)",
    "Number of deleted lines": 1,
    "Deleted lines": "-            if self.is_sparse or self.device.type in ['xla', 'mlc', 'ort', 'meta', 'hpu']:",
    "Added lines": "+            if self.is_sparse or self.device.type in ['lazy', 'xla', 'mlc', 'ort', 'meta', 'hpu']:",
    "Label": "clean"
},
{
    "Id": 1045,
    "Library": "pytorch",
    "Date": "2022/02/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/24c91e23d3bbb5cd6baa395ee45d88697255dd7f",
    "Root Cause": "N.A",
    "Bug report": "Fix nasty bug in bisect_percentile_op (#73147)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73147\n\nCode used `reserve` instead of `resize` leading to platform010 test failures:\n```\nTrying example: test_bisect_percentil_op_large(\n    self=<caffe2.caffe2.python.operator_test.bisect_percentile_op_test.TestBisectPercentileOp testMethod=test_bisect_percentil_op_large>,\n    N=20,\n    lengths=[2, 2],\n    max_value=100,\n    discrete=False,\n    p=0.0,\n    gc=,\n    dc=[],\n)\n\nstderr:\nE0219 13:14:52.601948 995877 JustKnobsConfigeratorLoader.cpp:114] Failed to load config justknobs/movefast/knobs after 55000ms timeout\nE0219 13:14:52.602150 995877 JustKnobsConfigeratorLoader.cpp:114] Failed to load config justknobs/pytorch/compiler after 55000ms timeout\ntest_bisect_percentil_op_large (caffe2.caffe2.python.operator_test.bisect_percentile_op_test.TestBisectPercentileOp) ... third-party-buck/platform010/build/libgcc/include/c++/trunk/bits/stl_vector.h:1045: std::vector::reference std::vector<int>::operator[](std::vector::size_type) [_Tp = int, _Alloc = std::allocator<int>]: Assertion '__n < this->size()' failed.\n*** Aborted at 1645305292 (Unix time, try 'date -d 1645305292') ***\n*** Signal 6 (SIGABRT) (0x8556000f3225) received by PID 995877 (pthread TID 0x7f13a79c51c0) (linux TID 995877) (maybe from PID 995877, UID 34134) (code: -6), stack trace: ***\nW0219 13:14:52.682251 995932 RetryingSender.cpp:433] Failed to make rpc. Sender name: pr-scubasing. Reason: apache::thrift::transport::TTransportException: AsyncSocketException: connect failed, type = Socket not open, errno = 111 (Connection refused): Connection refused.\n    @ 000000000000431b folly::symbolizer::(anonymous namespace)::signalHandler(int, siginfo_t*, void*)\n                       ./folly/experimental/symbolizer/SignalHandler.cpp:449\n    @ 0000000000000000 (unknown)\n    @ 000000000009c9f3 __GI___pthread_kill\n```\n\nTest Plan: Sandcastle\n\nReviewed By: luciang\n\nDifferential Revision: D34365188\n\nfbshipit-source-id: 65dcc23226c59096afd5fb3c338c3bd29c936ec3\n(cherry picked from commit a1d18e3e6aaea96ba2ac4bb6a95afe45678e0ec7)",
    "Number of deleted lines": 1,
    "Deleted lines": "-    index.reserve(n_features + 1);",
    "Added lines": "+    index.resize(n_features + 1);",
    "Label": "clean"
},
{
    "Id": 1046,
    "Library": "pytorch",
    "Date": "2022/02/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/46f9e16afecbab0a57f6d3a0bb489787cd8cf979",
    "Root Cause": "N.A",
    "Bug report": "Documenting cuda 11.5 windows issue (#73013)\n\nSummary:\nAdding documentation about compiling extension with CUDA 11.5 and Windows\n\nExample of failure: https://github.com/pytorch/pytorch/runs/4408796098?check_suite_focus=true\n\n Note: Don't use torch/extension.h In CUDA 11.5 under windows in your C++ code:\n    Use aten instead of torch interface in all cuda 11.5 code under windows. It has been failing with errors, due to a bug in nvcc.\n    Example use:\n        >>> #include <ATen/ATen.h>\n        >>> at::Tensor SigmoidAlphaBlendForwardCuda(....)\n    Instead of:\n        >>> #include <torch/extension.h>\n        >>> torch::Tensor SigmoidAlphaBlendForwardCuda(...)\n    Currently open issue for nvcc bug: https://github.com/pytorch/pytorch/issues/69460\n    Complete Workaround code example: https://github.com/facebookresearch/pytorch3d/commit/cb170ac024a949f1f9614ffe6af1c38d972f7d48\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73013\n\nReviewed By: malfet, seemethere\n\nDifferential Revision: D34306134\n\nPulled By: atalman\n\nfbshipit-source-id: 3c5b9d7a89c91bd1920dc63dbd356e45dc48a8bd\n(cherry picked from commit 87098e7f17fca1b98c90fafe2dde1defb6633f49)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    Note that CUDA-11.5 nvcc will hit internal compiler error while parsing torch/extension.h on Windows.\n+    To workaround the issue, move python binding logic to pure C++ file.\n+\n+    Example use:\n+        >>> #include <ATen/ATen.h>\n+        >>> at::Tensor SigmoidAlphaBlendForwardCuda(....)\n+\n+    Instead of:\n+        >>> #include <torch/extension.h>\n+        >>> torch::Tensor SigmoidAlphaBlendForwardCuda(...)\n+\n+    Currently open issue for nvcc bug: https://github.com/pytorch/pytorch/issues/69460\n+    Complete workaround code example: https://github.com/facebookresearch/pytorch3d/commit/cb170ac024a949f1f9614ffe6af1c38d972f7d48\n+",
    "Label": "clean"
},
{
    "Id": 1047,
    "Library": "pytorch",
    "Date": "2022/02/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d79aec91f759acb00d9af4bf92f57ec752dd65b7",
    "Root Cause": "N.A",
    "Bug report": "[easy][PTE] Reduce unnecessary ref count bumps in callstack debug (#72547)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72547\n\ntoTuple() returns a  new intrusive pointer that bumps its underlying ref count. Whereas, toTupeRef returns a reference. We can save an unnecessary ref count bump.\n\nBased on https://fb.workplace.com/groups/pytorch.edge.team/permalink/1021780808376658/\n\nsimilar to D34047666 (https://github.com/pytorch/pytorch/commit/85d7e73a8aa2dd74970017d11c7411b36b89dfc4)\nghstack-source-id: 148665193\n\nTest Plan:\n```\n> Executing task: buck: buck test //xplat/caffe2:test_lite_interpreter  --config client.id=nuclide <\n\nExecuting in directory: /data/users/pavithran/fbsource\nbuck test //xplat/caffe2:test_lite_interpreter  --config client.id=nuclide\n\nclang-9: warning: argument unused during compilation: '-pthread' [-Wunused-command-line-argument]\n\nParsing buck files: finished in 2.1 sec\nCreating action graph: finished in 0.5 sec\n[RE] Metadata: Session ID=[reSessionID-66858379-0761-4966-a933-bc7f0d0add95]\n[RE] Waiting on 0 remote actions. Completed 523 actions remotely, action cache hit rate: 0.00%.\nDownloaded 3947/5089 artifacts, 20.92 Mbytes, 12.5% cache miss (for updated rules)\nBuilding: finished in 01:04.0 min (100%) 5438/5438 jobs, 5192/5438 updated\n  Total time: 01:06.6 min\nTesting: finished in 06:53.7 min (71 PASS/0 FAIL)\nBUILD SUCCEEDED\nRESULTS FOR //xplat/caffe2:test_lite_interpreter\nPASS    406.0s 71 Passed   0 Skipped   0 Failed   //xplat/caffe2:test_lite_interpreter\nTESTS PASSED\n\nTerminal will be reused by tasks, press any key to close it.\n```\n\nReviewed By: kimishpatel\n\nDifferential Revision: D34082609\n\nfbshipit-source-id: 4bcbdb2d11dd4c3bc392010487dccd2270278222\n(cherry picked from commit dd64eb386d02335e566fb6496f2ff00a8879ccc3)",
    "Number of deleted lines": 1,
    "Deleted lines": "-  auto ivalues = std::move(*std::move(ival).toTuple()).elements();",
    "Added lines": "+  const auto& ivalues = ival.toTupleRef().elements();",
    "Label": "clean"
},
{
    "Id": 1048,
    "Library": "pytorch",
    "Date": "2022/02/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/00769060bc610dd40a92faef3b1994c2716ce2fe",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch] MHA: just use existing softmax on CPU (#72462)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72462\n\nEliminating one potential source of bugs.\nghstack-source-id: 149067329\n\nTest Plan: CI\n\nReviewed By: zrphercule\n\nDifferential Revision: D34006432\n\nfbshipit-source-id: 55fda186636dc457db7f3f9c8e18f1627ff33b6a\n(cherry picked from commit 5d8de9a12200db236d0fedfd3b13b1209fd4bc18)",
    "Number of deleted lines": 1,
    "Deleted lines": "-    const Tensor& attn_scores,",
    "Added lines": "+    Tensor& attn_scores,\n+  } else {\n+    at::_softmax_out(attn_scores, attn_scores, 3, false);\n+    return;",
    "Label": "clean"
},
{
    "Id": 1049,
    "Library": "pytorch",
    "Date": "2022/02/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f5d078088bca8e2df823d6219a22d774f6cd4c75",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch] MHA: fix dim_per_head / V bug (#72459)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72459\n\nThis was pointed out in a comment on the original diff, but not fixed.\nghstack-source-id: 149067331\n\nTest Plan: cosine similarity with the existing MHA impl result on CPU + float32 goes from 0.2457 to 0.5097\n\nReviewed By: zrphercule\n\nDifferential Revision: D33987869\n\nfbshipit-source-id: b560ade85f577e83bcaf5b37da2e89d8646d5909\n(cherry picked from commit 47511a2138a35b5e71ef3562a6e93cb59d965ab2)",
    "Number of deleted lines": 1,
    "Deleted lines": "-                for (auto dh = 0; dh < dim_per_head / V; dh += V) {",
    "Added lines": "+                TORCH_INTERNAL_ASSERT(dim_per_head % V == 0, \"epilogue not implemented yet\");\n+                for (auto dh = 0; dh < dim_per_head; dh += V) {",
    "Label": "clean"
},
{
    "Id": 1050,
    "Library": "pytorch",
    "Date": "2022/02/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2e82c1e5978936e783da5e87e68bb0235d5fa017",
    "Root Cause": "N.A",
    "Bug report": "[Static Runtime] Fix printing graphs in debug mode during fusion (#72222)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72222\n\nghstack-source-id: 148415879\n\nTest Plan: Only changing printing graphs during debugging.\n\nReviewed By: HarutMov\n\nDifferential Revision: D33960591\n\nfbshipit-source-id: 2db5e65258b6d30a4fa88cb7e115cbffcebfa15f\n(cherry picked from commit 22537573bd6ece3575d13f3b631e890772a92a11)",
    "Number of deleted lines": 2,
    "Deleted lines": "-  GRAPH_DEBUG(\"Graph before tracing: \", graph);\n-  GRAPH_DEBUG(\"Graph after tracing: \", traced_graph);",
    "Added lines": "+  GRAPH_DEBUG(\"Graph before tracing: \", *graph);\n+  GRAPH_DEBUG(\"Graph after tracing: \", *traced_graph);",
    "Label": "clean"
},
{
    "Id": 1051,
    "Library": "pytorch",
    "Date": "2022/01/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4bf1be898d2fda3e4b76e66e7f4adee3cddbb03d",
    "Root Cause": "N.A",
    "Bug report": "caffe: fix warning: overloaded virtual function \"torch::jit::Function::call\" is only partially overridden in class \"torch::jit::GraphFunction\"\n\nSummary:\nNeed to bring in all signatures\n\nhttps://www.internalfb.com/code/fbsource/[36035b9e4e41813e215ffd5f4377d65b7259237e]/fbcode/caffe2/aten/src/ATen/core/function.h?lines=91-101\n\nTest Plan:\n```\nAction Failed for fbcode//accelerators/pytorch/lib/cuda:ngram_repeat_block_cuda (ovr_config//platform/linux:x86_64-fbcode-platform010-clang-6dbc4bb1b9a32829)#5:\ncxx_compile ngram_repeat_block_cuda_kernel.cu (pic) failed with non-zero exit code 1\ndebug information: action_digest=988629a726bc4eabcaf334db2317a969958d5fd2:94\nstdout:\nstderr:\nfbcode/caffe2/torch/csrc/jit/api/function_impl.h(11): warning: overloaded virtual function \"torch::jit::Function::call\" is only partially overridden in class \"torch::jit::GraphFunction\"\n\nfbcode/caffe2/torch/csrc/jit/api/function_impl.h(11): warning: overloaded virtual function \"torch::jit::Function::call\" is only partially overridden in class \"torch::jit::GraphFunction\"\n\nfbcode/caffe2/torch/csrc/jit/frontend/tree_views.h: In instantiation of 'static torch::jit::Maybe<T> torch::jit::Maybe<T>::create(const torch::jit::SourceRange&, const T&) [with T = torch::jit::List<torch::jit::Property>]':\nfbcode/caffe2/torch/csrc/jit/frontend/tree_views.h:505:117:   required from here\nfbcode/caffe2/torch/csrc/jit/frontend/tree_views.h:220:33: error: cannot convert 'const torch::jit::List<torch::jit::Property>' to 'torch::jit::TreeList&&' {aka 'c10::SmallVector<c10::intrusive_ptr<torch::jit::Tree>, 4>&&'}\n  220 |     return Maybe<T>(Compound::create(TK_OPTION, range, {value}));\n      |                ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\nfbcode/caffe2/torch/csrc/jit/frontend/tree.h:144:1: note:   initializing argument 3 of 'static torch::jit::TreeRef torch::jit::Compound::create(int, const torch::jit::SourceRange&, torch::jit::TreeList&&)'\n  143 |       const SourceRange& range_,\n      |         ~~~~~~~~~~~~~~~~~~~~~~~~\n  144 |       TreeList&& trees_) {\n      | ^\nfbcode/caffe2/torch/csrc/jit/frontend/tree_views.h: In instantiation of 'static torch::jit::Maybe<T> torch::jit::Maybe<T>::create(const torch::jit::SourceRange&, const T&) [with T = torch::jit::List<torch::jit::Assign>]':\nfbcode/caffe2/torch/csrc/jit/frontend/tree_views.h:505:171:   required from here\nfbcode/caffe2/torch/csrc/jit/frontend/tree_views.h:220:33: error: cannot convert 'const torch::jit::List<torch::jit::Assign>' to 'torch::jit::TreeList&&' {aka 'c10::SmallVector<c10::intrusive_ptr<torch::jit::Tree>, 4>&&'}\n  220 |     return Maybe<T>(Compound::create(TK_OPTION, range, {value}));\n      |                ~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\nfbcode/caffe2/torch/csrc/jit/frontend/tree.h:144:1: note:   initializing argument 3 of 'static torch::jit::TreeRef torch::jit::Compound::create(int, const torch::jit::SourceRange&, torch::jit::TreeList&&)'\n  143 |       const SourceRange& range_,\n      |         ~~~~~~~~~~~~~~~~~~~~~~~~\n  144 |       TreeList&& trees_) {\n      | ^\ncc1plus: note: unrecognized command-line option '-Wno-ignored-optimization-argument' may have been intended to silence earlier diagnostics\ncc1plus: note: unrecognized command-line option '-Wno-ambiguous-reversed-operator' may have been intended to silence earlier diagnostics\ncc1plus: note: unrecognized command-line option '-Wno-ignored-optimization-argument' may have been intended to silence earlier diagnostics\ncc1plus: note: unrecognized command-line option '-Wno-ambiguous-reversed-operator' may have been intended to silence earlier diagnostics\ncommand: buck-out/v2/gen/fbcode/999b02f9444004c1/tools/build/__wrap_nvcc.py__/wrap_nvcc.py -_NVCC_BIN_ fbcode ...<omitted>... ors/pytorch/lib/cuda/__ngram_repeat_block_cuda__/__objects__/ngram_repeat_block_cuda_kernel.cu.pic.o (rerun with -v to view the untruncated command)\n```\n\nDifferential Revision: D33579670\n\nfbshipit-source-id: 9acb443732feb3e921ce0fa5f38f21ed44f64114",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  using Function::call;",
    "Label": "clean"
},
{
    "Id": 1052,
    "Library": "pytorch",
    "Date": "2022/01/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5749be4678a3b240b5b5d22a228a43141e2cd32c",
    "Root Cause": "N.A",
    "Bug report": "Fix the shape inconsistency of `out` and `elem` tensor (#71065)\n\nSummary:\nSee bug report  https://github.com/pytorch/pytorch/issues/71063\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71065\n\nReviewed By: anjali411\n\nDifferential Revision: D33549921\n\nPulled By: ejguan\n\nfbshipit-source-id: bc43f5f9a88f7dcd8729d0e0f4b90d20f40b3064",
    "Number of deleted lines": 2,
    "Deleted lines": "-            >>> default_collate([{'A': 0, 'B': 1}, {'A': 100, 'B': 100}]\n-            out = elem.new(storage)",
    "Added lines": "+            >>> default_collate([{'A': 0, 'B': 1}, {'A': 100, 'B': 100}])\n+            out = elem.new(storage).resize_(len(batch), *list(elem.size()))",
    "Label": "clean"
},
{
    "Id": 1053,
    "Library": "pytorch",
    "Date": "2022/01/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/479ce1c3a0dad2c3d182fc0b3f26e647b99c1253",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch] Add isUndefined to ExclusivelyOwnedTraits<TensorBase> debug msg (#70638)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70638\n\nWe are seeing these assertions fire infrequently. Add more information to aid in debugging when they fire.\nghstack-source-id: 146819527\n\nTest Plan: CI\n\nReviewed By: bdhirsh\n\nDifferential Revision: D33412651\n\nfbshipit-source-id: 7e35faf9f4eeaa5f2455a4392e00f62fe692811c",
    "Number of deleted lines": 4,
    "Deleted lines": "-        toDestroy->refcount_ == 1 || (toDestroy->refcount_ == 0 && toDestroy == UndefinedTensorImpl::singleton()),\n-        \"ExclusivelyOwned<Tensor> destroyed with refcount \", toDestroy->refcount_, \", expected 1!\");\n-        \"ExclusivelyOwned<Tensor> destroyed with weakcount \", toDestroy->weakcount_, \", expected 1!\");\n-    if (toDestroy != UndefinedTensorImpl::singleton()) {",
    "Added lines": "+    const bool isUndefined = toDestroy == UndefinedTensorImpl::singleton();\n+        toDestroy->refcount_ == 1 || (toDestroy->refcount_ == 0 && isUndefined),\n+        \"ExclusivelyOwned<Tensor> destroyed with isUndefined \", isUndefined, \" and refcount \", toDestroy->refcount_, \", expected 1 or, if isUndefined, 0!\");\n+        \"ExclusivelyOwned<Tensor> destroyed with isUndefined \", isUndefined, \" and weakcount \", toDestroy->weakcount_, \", expected 1 or, if isUndefined, 0!\");\n+    if (!isUndefined) {",
    "Label": "clean"
},
{
    "Id": 1054,
    "Library": "pytorch",
    "Date": "2022/01/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/569aeec1bcd63d835d0ce4cf07b1a113c8f4401d",
    "Root Cause": "N.A",
    "Bug report": "fix typo in debugging_hooks.py (#70956)\n\nSummary:\nI just fixed a small typo in the debugging_hooks documentation\n\ncc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan-varma gqchen aazzolini osalpekar jiayisuse SciPioneer H-Huang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70956\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33508898\n\nPulled By: dagitses\n\nfbshipit-source-id: fc5935e5a2e2ddc45657a22d3b33a11aba378d9b",
    "Number of deleted lines": 1,
    "Deleted lines": "-    This DDP communication hook returns the a future that wraps the input,",
    "Added lines": "+    This DDP communication hook returns a future that wraps the input,",
    "Label": "clean"
},
{
    "Id": 1055,
    "Library": "pytorch",
    "Date": "2022/01/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/785b6905de121179dda5ae2f4356b6a705693c57",
    "Root Cause": "N.A",
    "Bug report": "reduce plan generation log spam (#70880)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70880\n\nChange loglevel to `debug` in caffe2 `optimizer.py` for logging rowwise Adagrad engine.\n\nTest Plan: CI + sandcastle\n\nReviewed By: boryiingsu\n\nDifferential Revision: D33439337\n\nfbshipit-source-id: b158249b8df771c0ec8b642210ede39972929b00",
    "Number of deleted lines": 5,
    "Deleted lines": "-            logger.info(\n-            logger.info(\n-                logger.warn(\n-        logger.info(\n-            logger.info(\"using {} for {}\".format(op, str(param)))",
    "Added lines": "+            logger.debug(\n+            logger.debug(\n+                logger.warning(\n+        logger.debug(\n+            logger.debug(\"using {} for {}\".format(op, str(param)))",
    "Label": "clean"
},
{
    "Id": 1056,
    "Library": "pytorch",
    "Date": "2022/01/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dfb807d65ecf5aa4bf932b27fdf75b12c119a510",
    "Root Cause": "N.A",
    "Bug report": "dbr quant: do not attach auto_quant_state to observers (#70256)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70256\n\nSomewhere in previous PRs we started attaching AutoQuantState\nto observers. This PR removes this, as that has not purpose\nand makes model debugging more complicated.\n\nTest Plan:\n```\npython test/test_quantization.py -k DBR\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D33262299\n\nPulled By: vkuzo\n\nfbshipit-source-id: a3543b44c517325d57f5ed03b961a8955049e682",
    "Number of deleted lines": 1,
    "Deleted lines": "-        m.__module__.startswith('torch.nn.intrinsic')",
    "Added lines": "+    is_activation_post_process,\n+        m.__module__.startswith('torch.nn.intrinsic') or\n+        # observers and fake quants are leaves\n+        is_activation_post_process(m)",
    "Label": "clean"
},
{
    "Id": 1057,
    "Library": "pytorch",
    "Date": "2022/01/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/29ff596dcaba69fcb9dcf55b6e015e09f7129e3a",
    "Root Cause": "N.A",
    "Bug report": "[CUDA graphs] Changes batchnorm to increment num_batches_tracked in place for improved graph safety (#70444)\n\nSummary:\nThis PR was not my worst debugging annoyance, nor my smallest in lines changed, but it has the highest `debugging annoyance/lines changed` ratio.\n\nThe current pattern\n```\nself.num_batches_tracked = self.num_batches_tracked + 1\n```\n, if captured, deletes an eagerly-allocated tensor and overwrites it with a captured tensor. Replays read from the (deallocated) original tensor's address.\nThis can cause\n1. an IMA on graph replay\n2. failure to actually increment `num_batches_tracked` during graph replay, because every replay reads from the old location without adding to it\n3. numerical corruption if the allocator reassigns the original tensor's memory to some unrelated tensor\n4. combinations of 1, 2, and 3, depending on global allocation patterns and if/when the BN module is called eagerly sometimes between replays\n\n(ask me how I know).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70444\n\nReviewed By: albanD\n\nDifferential Revision: D33342203\n\nPulled By: ngimel\n\nfbshipit-source-id: 5f201cc25030517e75af010bbaa88c452155df21",
    "Number of deleted lines": 2,
    "Deleted lines": "-                self.num_batches_tracked = self.num_batches_tracked + 1  # type: ignore[has-type]\n-            self.num_batches_tracked = self.num_batches_tracked + 1",
    "Added lines": "+                self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n+            self.num_batches_tracked.add_(1)",
    "Label": "clean"
},
{
    "Id": 1058,
    "Library": "pytorch",
    "Date": "2022/01/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7c7eb351c306e1db683dc4318288fd8479f6ef36",
    "Root Cause": "N.A",
    "Bug report": "Populate __name__ for torch.nn.modules.utils.{_single,_pair,...} (#70459)\n\nSummary:\nThis helps with debug printouts and python level graph analysis.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70459\n\nReviewed By: wconstab\n\nDifferential Revision: D33340032\n\nPulled By: jansel\n\nfbshipit-source-id: 24d3fdf31e9e5e92bb47f0db30339cf373a1d4d4",
    "Number of deleted lines": 5,
    "Deleted lines": "-def _ntuple(n):\n-_single = _ntuple(1)\n-_pair = _ntuple(2)\n-_triple = _ntuple(3)\n-_quadruple = _ntuple(4)",
    "Added lines": "+def _ntuple(n, name=\"parse\"):\n+    parse.__name__ = name\n+_single = _ntuple(1, \"_single\")\n+_pair = _ntuple(2, \"_pair\")\n+_triple = _ntuple(3, \"_triple\")\n+_quadruple = _ntuple(4, \"_quadruple\")",
    "Label": "clean"
},
{
    "Id": 1059,
    "Library": "pytorch",
    "Date": "2021/12/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9ee3006d583e9853da79f7bd12431c67a863519a",
    "Root Cause": "N.A",
    "Bug report": "[fx-acc][graph-opts] bug fixes for transpose_to_reshape, optimize_quantization, finalize_kwargs_to_concrete\n\nSummary:\nFixes a couple of bugs that surfaced during integration of graph opts into `AcceleratedGraphModule` (D31484770).\n\n2. Fix bug in `graph_opt.transpose_to_reshape` implementation that causes it to incorrectly apply opt for `permute` op acting on shape `(B, N, N)` with `N > 1` and permutation `(0, 2, 1)`. Fixed the bug and added test case to cover this case.\n3. Revert part of D31671833 (https://github.com/pytorch/pytorch/commit/0e371e413dd12b93cf50ad2441652ee6e0eb7217), where I made `acc_out_ty` into a required argument\n4. Align `graph_opt.transpose_to_reshape` and `graph_opt.optimize_quantization` to not set `acc_out_ty` when adding a new node to graph and instead rely on tensor metadata\n5. Run `acc_utils.copy_acc_out_ty_from_meta_to_acc_ops_kwargs()` in `GraphOptsTest.verify_numerics` before running graph on sample inputs.\n\nTest Plan:\n```\nbuck test mode/opt glow/fb/fx/graph_opts:\n```\n\n```\n...\nSummary\n  Pass: 85\n  ListingSuccess: 4\nIf you need help understanding your runs, please follow the wiki: https://fburl.com/posting_in_tpx_users\nFinished test run: https://www.internalfb.com/intern/testinfra/testrun/562950163929022\n```\n\nReviewed By: jfix71\n\nDifferential Revision: D31851549\n\nfbshipit-source-id: 602affe2a2a0831d2f17b87025107ca87ecb0e59",
    "Number of deleted lines": 10,
    "Deleted lines": "-def quantized_linear(*, input, weight, bias, acc_out_ty):\n-def quantized_add(*, input, other, acc_out_ty):\n-def quantized_mul(*, input, other, acc_out_ty):\n-def quantize_per_tensor(*, input, acc_out_ty):\n-def quantize_per_channel(*, input, acc_out_ty):\n-def rescale_quantize_per_tensor(*, input, acc_out_ty):\n-def rescale_quantize_per_channel(*, input, acc_out_ty):\n-def reshape(*, input, acc_out_ty):\n-def to_dtype(input, acc_out_ty):\n-    assert acc_out_ty is not None, \"valid acc_out_ty needed\"",
    "Added lines": "+def quantized_linear(*, input, weight, bias, acc_out_ty=None):\n+    assert acc_out_ty is not None\n+def quantized_add(*, input, other, acc_out_ty=None):\n+    assert acc_out_ty is not None\n+def quantized_mul(*, input, other, acc_out_ty=None):\n+    assert acc_out_ty is not None\n+def quantize_per_tensor(*, input, acc_out_ty=None):\n+    assert acc_out_ty is not None\n+def quantize_per_channel(*, input, acc_out_ty=None):\n+    assert acc_out_ty is not None\n+def rescale_quantize_per_tensor(*, input, acc_out_ty=None):\n+    assert acc_out_ty is not None\n+def rescale_quantize_per_channel(*, input, acc_out_ty=None):\n+    assert acc_out_ty is not None\n+def reshape(*, input, acc_out_ty=None):\n+    assert acc_out_ty is not None\n+def to_dtype(input, acc_out_ty=None):\n+    assert acc_out_ty is not None",
    "Label": "clean"
},
{
    "Id": 1060,
    "Library": "pytorch",
    "Date": "2021/12/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/603a1de871ddaae39ba746ac652da23a9f568d40",
    "Root Cause": "N.A",
    "Bug report": "Fix inefficient recursive update in ShardedTensor.state_dict hook (#68806)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/68805\n\nThe bug is described in the linked issue. This PR is an attempt to make the functions `_recurse_update_dict` and `_recurse_update_module` more efficient in how they iterate over the submodules. The previous implementation was suboptimal, as it recursively called the update method on the submodules returned by `module.named_modules()`, while `module.named_modules()` already returned all submodules including nested ones.\n\ncc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan-varma gqchen aazzolini osalpekar jiayisuse SciPioneer H-Huang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68806\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D33053940\n\nPulled By: wanchaol\n\nfbshipit-source-id: 3e72822f65a641939fec40daef29c806af725df6",
    "Number of deleted lines": 23,
    "Deleted lines": "-    _recurse_update_dict(module, destination, prefix)\n-    _recurse_update_module(module, state_dict, prefix)\n-\n-def _recurse_update_module(module, state_dict, prefix):\n-    for attr_name, attr in module.__dict__.items():\n-        key = prefix + attr_name\n-        if key in state_dict:\n-            if isinstance(state_dict[key], ShardedTensor):\n-                setattr(module, attr_name, state_dict[key])\n-\n-    for submodule_name, submodule in module.named_modules():\n-        key = prefix + submodule_name\n-        if submodule_name:\n-            _recurse_update_module(submodule, state_dict, key + '.')\n-\n-\n-def _recurse_update_dict(module, destination, prefix):\n-    for attr_name, attr in module.__dict__.items():\n-        if isinstance(attr, ShardedTensor):\n-            destination[prefix + attr_name] = attr\n-\n-        if submodule_name != '':\n-            _recurse_update_dict(submodule, destination, prefix + submodule_name + '.')",
    "Added lines": "+    for submodule_name, submodule in module.named_modules():\n+        for attr_name, attr in submodule.__dict__.items():\n+            if isinstance(attr, ShardedTensor):\n+                destination[prefix + submodule_name + '.' + attr_name] = attr\n+        for attr_name, attr in submodule.__dict__.items():\n+            key = prefix + submodule_name + '.' + attr_name\n+            if key in state_dict:\n+                if isinstance(state_dict[key], ShardedTensor):\n+                    setattr(module, attr_name, state_dict[key])",
    "Label": "clean"
},
{
    "Id": 1061,
    "Library": "pytorch",
    "Date": "2021/12/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0420de3539f02e6cd3220467d7b0b892eec75abe",
    "Root Cause": "N.A",
    "Bug report": "[SR] Log SR options (#69809)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69809\n\nSR options is only printed out once per model per net. Logging it is actually pretty helpful for debugging.\n\nTest Plan: CI\n\nReviewed By: donaldong\n\nDifferential Revision: D33046814\n\nfbshipit-source-id: 536b34e00fbc8a273c5eb4d8ae5caca0dc1f4c24",
    "Number of deleted lines": 5,
    "Deleted lines": "-  VLOG(1) << \"StaticModuleOptions: cleanup_activations \"\n-          << opts.cleanup_activations << \", enable_out_variant \"\n-          << opts.enable_out_variant << \", optimize_memory \"\n-          << opts.optimize_memory << \", manage_output_tensors \"\n-          << opts.manage_output_tensors;",
    "Added lines": "+  LOG(INFO) << \"StaticModuleOptions: cleanup_activations \"\n+            << opts.cleanup_activations << \", enable_out_variant \"\n+            << opts.enable_out_variant << \", optimize_memory \"\n+            << opts.optimize_memory << \", manage_output_tensors \"\n+            << opts.manage_output_tensors;",
    "Label": "clean"
},
{
    "Id": 1062,
    "Library": "pytorch",
    "Date": "2021/12/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e06af7913692641dbcad1cf4d7931f32b8b4c2a7",
    "Root Cause": "N.A",
    "Bug report": "Fix sign op converter (#69580)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69580\n\nFix bug in sign converter\n\nReviewed By: 842974287\n\nDifferential Revision: D32934661\n\nfbshipit-source-id: f21d7c65b07ab2f0a0027939d660e56dacd9cdef",
    "Number of deleted lines": 2,
    "Deleted lines": "-        operation_type = trt.ActivationType.SIGN\n-        return add_activation_layer(network, input_val, operation_type, target, name)",
    "Added lines": "+        operation_type = trt.UnaryOperation.SIGN\n+        return add_unary_layer(network, input_val, operation_type, target, name)",
    "Label": "clean"
},
{
    "Id": 1063,
    "Library": "pytorch",
    "Date": "2021/11/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/49abda208be70071774bd53dc97bf785e526a41f",
    "Root Cause": "N.A",
    "Bug report": "[JIT] internal build bug fix (#69061)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69061\n\n`warning` breaks this build [D32622152](https://www.internalfb.com/diff/D32622152)\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D32712448\n\nPulled By: makslevental\n\nfbshipit-source-id: c7a70487bd0b95ac8b242522c36597d36072201f",
    "Number of deleted lines": 12,
    "Deleted lines": "-import warnings\n-from importlib.machinery import SourceFileLoader\n-\n-import os\n-try:\n-    shape_function_fp = (\n-        f\"{os.path.dirname(os.path.realpath(torch.__file__))}/include/torch/csrc/jit/runtime/shape_functions.h\"\n-    )\n-    _shapes = SourceFileLoader(\"shape_functions\", shape_function_fp).load_module()  # type: ignore[call-arg]\n-except Exception as e:\n-    warnings.warn(f\"Couldn't load shape functions: {e}\")\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1064,
    "Library": "pytorch",
    "Date": "2021/11/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2ad4727ad92aa9eabae9c63c4a2f3e1dd4a9ddc3",
    "Root Cause": "N.A",
    "Bug report": "dbr quant: fix debugging fqn info for converted model (#68840)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68840\n\nFixes the debugging FQN info for a converted model. Some of this\ninformation was missing because eager mode convert performed\nmodule swaps. This information is only used in debugging and is\nnot used for inference.\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeDBR\n```\n\nturn `enable_logging` on in `auto_trace.py`, the FQN is now displayed\nfor a converted model\n\nReviewed By: jerryzh168\n\nDifferential Revision: D32630884\n\nPulled By: vkuzo\n\nfbshipit-source-id: be8c43343abfdab9fe0af39499d908ed61a01b78",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+    # Since eager mode convert could have changed the IDs of some modules,\n+    # populate the FQN map again\n+    for k, v in module.named_modules():\n+        module_id_to_fqn[id(v)] = k\n+",
    "Label": "clean"
},
{
    "Id": 1065,
    "Library": "pytorch",
    "Date": "2021/11/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a03fe9ba61c77db37af91ebc15a7751bc8f8d5d2",
    "Root Cause": "N.A",
    "Bug report": "dbr quant overhead[12/x]: turn off overrides for module convert output hook (#68839)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68839\n\nWe can assume that there are no overrides needed for the hook which\ndequantizes the module outputs, so we can turn them off explicitly.\nWhile this does not lead to a measurable perf win, it makes things\neasier to debug by eliminating the no-op overrides.\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeDBR\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D32630886\n\nPulled By: vkuzo\n\nfbshipit-source-id: 1719c168f5f21f3e59c80a3b6d0f32ebb1c77ef8",
    "Number of deleted lines": 1,
    "Deleted lines": "-                        nonlocal global_disable_torch_function_override",
    "Added lines": "+                nonlocal global_disable_torch_function_override\n+\n+                        # For the sake of performance, we assume no overrides\n+                        # are needed for quantizing/dequantizing things\n+                        old_global_disable_torch_function_override = \\\n+                            global_disable_torch_function_override\n+                        global_disable_torch_function_override = True\n+\n+\n+                        global_disable_torch_function_override = \\\n+                            old_global_disable_torch_function_override\n+",
    "Label": "clean"
},
{
    "Id": 1066,
    "Library": "pytorch",
    "Date": "2021/11/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/62847a2b9c98ce7c9a9007b173e76a12a0ebd2e6",
    "Root Cause": "N.A",
    "Bug report": "Fix bug on empty GLOO_SOCKET_IFNAME_ENV (#68933)\n\nSummary:\nThis PR is trying to fix the no device bug when user resets the `GLOO_SOCKET_IFNAME_ENV` with\n\n```bash\nexport GLOO_SOCKET_IFNAME_ENV=\n```\n\nThank you for your time on reviewing this PR :).\n\ncc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan-varma gqchen aazzolini osalpekar jiayisuse SciPioneer H-Huang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68933\n\nReviewed By: soulitzer\n\nDifferential Revision: D32690633\n\nPulled By: mrshenli\n\nfbshipit-source-id: f6df2b8b067d23cf1ec177c77cc592dc870bda72",
    "Number of deleted lines": 1,
    "Deleted lines": "-            if (ifnameEnv) {",
    "Added lines": "+            if (ifnameEnv && strlen(ifnameEnv) > 1) {",
    "Label": "clean"
},
{
    "Id": 1067,
    "Library": "pytorch",
    "Date": "2021/11/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/96d116fec234e965bc735bf6b62e17942cd268d4",
    "Root Cause": "N.A",
    "Bug report": "[JIT] Add additional debug output when op cannot be found in AliasDb (#68099)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68099\n\nWhen an op in the graph cannot be matched to any known ops, alias_analysis.cpp throws an error.\n\nBefore:\n```\nRuntimeError: 0INTERNAL ASSERT FAILED at \"../torch/csrc/jit/ir/alias_analysis.cpp\":612, please report a bug to PyTorch. We don't have an op for aten::add but it isn't a special case. Argument types: Tensor, float, Tensor,\n```\n\nAfter:\n```\nRuntimeError: 0INTERNAL ASSERT FAILED at \"../torch/csrc/jit/ir/alias_analysis.cpp\":612, please report a bug to PyTorch. We don't have an op for a\nten::add but it isn't a special case.  Argument types: Tensor, float, Tensor,\n\nCandidates:\n        aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> (Tensor)\n        aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> (Tensor)\n        aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> (Tensor(a!))\n        aten::add.t(t[] a, t[] b) -> (t[])\n        aten::add.str(str a, str b) -> (str)\n        aten::add.int(int a, int b) -> (int)\n        aten::add.complex(complex a, complex b) -> (complex)\n        aten::add.float(float a, float b) -> (float)\n        aten::add.int_complex(int a, complex b) -> (complex)\n        aten::add.complex_int(complex a, int b) -> (complex)\n        aten::add.float_complex(float a, complex b) -> (complex)\n        aten::add.complex_float(complex a, float b) -> (complex)\n        aten::add.int_float(int a, float b) -> (float)\n        aten::add.float_int(float a, int b) -> (float)\n        aten::add(Scalar a, Scalar b) -> (Scalar)\n```\n\nTest Plan:\nRun\n```\nimport torch\n\nif __name__ == '__main__':\n    ir = \"\"\"\ngraph(%x : Tensor,\n      %y : Tensor):\n  %2 : float = prim::Constant[value=1.2]()\n  %result : Tensor= aten::add(%x, %2, %y)\n  return (%result)\n\"\"\"\n    x = torch.tensor([[1., 2.], [3., 4.]])\n    y = torch.tensor([[2., 1.], [2., 1.]])\n    graph = torch._C.parse_ir(ir)\n    print(graph)\n    graph.alias_db().analyze()\n    # print(script(x, y))\n```\n\nto get the results above\n\nImported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D32339639\n\nfbshipit-source-id: a79a3c2f157154b5fb1e3f33a23e43b7884e8e38",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      oss << \"\\n\\nCandidates:\";\n+      const auto& candidates = getAllOperatorsFor(node->kind());\n+      for (const auto& candidate : candidates) {\n+        oss << \"\\n\\t\" << candidate->schema();\n+      }",
    "Label": "clean"
},
{
    "Id": 1068,
    "Library": "pytorch",
    "Date": "2021/11/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/be4150139ac1a7a09134fb705d32958ef6604233",
    "Root Cause": "N.A",
    "Bug report": "bugfix for conditional functionalization (#67715)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67715\n\nI had original made the `vector<ViewMeta>` and `Tensor`s stored on the `Update` struct references, but will pointed out a bug in the conditional-functionalization PR due to a use-after-free error. This happens because the queued-up updates might not be synced until later, and can out-live the original tensor that was used to create them.\n\nIt was kind of strange that this doesn't show up in the existing `test/test_functionalization.py` tests that I have in this stack, which technically also should have this bug (they call sync_() after the mutated tensors have gone out of scope). I looked at it with gdb, and I'm wondering if it's just because the stored values in the free'd `ViewMeta`/`Tensor` just happen to not get clobbered by the time the sync is called in the test.\n\nEither way, copying the Tensor + vector<ViewMeta> is probably not ideal for performance, but I couldn't think of an easy work-around for now.\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D32136007\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 707c6392a31b967e8965b9b77f297fd10a0a095a",
    "Number of deleted lines": 2,
    "Deleted lines": "-        const at::Tensor& new_val;\n-        const std::vector<ViewMeta>& view_metas;",
    "Added lines": "+        const at::Tensor new_val;\n+        const std::vector<ViewMeta> view_metas;",
    "Label": "clean"
},
{
    "Id": 1069,
    "Library": "pytorch",
    "Date": "2021/11/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0dc99dcf59ce70145bec903ecabb5243ac2e4e2b",
    "Root Cause": "N.A",
    "Bug report": "Update __init__.py (#67900)\n\nSummary:\nfix bugs https://github.com/pytorch/pytorch/issues/67896\nfix a syntax error in pytorch/torch/cuda/__init__.py\nFixes https://github.com/pytorch/pytorch/issues/67896\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67900\n\nReviewed By: mruberry\n\nDifferential Revision: D32211978\n\nPulled By: soulitzer\n\nfbshipit-source-id: a313a5e23b4d79e5b7bb909eaf82c9ee6cab10c9",
    "Number of deleted lines": 1,
    "Deleted lines": "-                warnings.warn(old_gpu_warn.format(d, name, major, minor, min_arch // 10, min_arch % 10))",
    "Added lines": "+                warnings.warn(old_gpu_warn % (d, name, major, minor, min_arch // 10, min_arch % 10))",
    "Label": "clean"
},
{
    "Id": 1070,
    "Library": "pytorch",
    "Date": "2021/11/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6cc6a5fd9dfe10864c62e48546a0e20ccf4422b1",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in TorchBench ondemand CI. (#67743)\n\nSummary:\nUse the main branch when TorchBench branch is not specified.\n\nRUN_TORCHBENCH: soft_actor_critic\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67743\n\nReviewed By: seemethere\n\nDifferential Revision: D32142663\n\nPulled By: xuzhao9\n\nfbshipit-source-id: 160227835543b8e55c970025073839bf0f03aa81",
    "Number of deleted lines": 1,
    "Deleted lines": "-    branch_name: str",
    "Added lines": "+    branch_name: str = \"\"",
    "Label": "clean"
},
{
    "Id": 1071,
    "Library": "pytorch",
    "Date": "2021/11/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3e218dbd2763377e0aef2b2dea203fa0dd4d1329",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch] Capture function args from schema by reference (#65951)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65951\n\nProfiling shows that we do a bunch of heap allocations to copy Argument structs in append_operator. Capturing by reference here should be safe as long as the schema objects args is from outlive the operator function.\n\nIMPORTANT: Reviewers (or automated tests if we're lucky) need to\nconfirm that the above is true or we're going to have fun\nuse-after-free bugs.\nghstack-source-id: 142065422\n\nTest Plan:\nAIBench run for speech model on MilanBoard\n\ncontrol: https://www.internalfb.com/intern/aibench/details/485570882988661 (mean 906 ms)\ntest: https://our.intern.facebook.com/intern/aibench/details/620835625995669 (mean 818 ms)\n\nSo almost a 10% improvement in the wall time metric?\n\nReviewed By: iseeyuan\n\nDifferential Revision: D31319988\n\nfbshipit-source-id: 7da56357420df500df344f49007e070ebb1bc581",
    "Number of deleted lines": 1,
    "Deleted lines": "-        fn = [fn, num_specified_args, args](Stack& stack) {",
    "Added lines": "+        fn = [fn, num_specified_args, &args](Stack& stack) {",
    "Label": "clean"
},
{
    "Id": 1072,
    "Library": "pytorch",
    "Date": "2021/10/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a95c94f075d845f15813653a4a71c2bf8d8b5e1c",
    "Root Cause": "N.A",
    "Bug report": "[fx2trt] fix acc_tracer when run against module that contains ScriptModule submodules (#67567)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67567\n\n- Fix an issue to allow it to work against modules that contains ScriptModule submodules.\n- Fix a bug where `getattr(base_class, method_name)` could raise KeyError\n\nTest Plan: linter; CI;\n\nReviewed By: 842974287\n\nDifferential Revision: D31956070\n\nfbshipit-source-id: 1114937f380af437fd6d36cd811ef609d7faefe7",
    "Number of deleted lines": 1,
    "Deleted lines": "-                method = getattr(base_class, method_name)",
    "Added lines": "+import logging\n+import torch.jit as jit\n+_LOGGER = logging.getLogger(__name__)\n+\n+\n+        jit.ScriptModule,\n+        jit.RecursiveScriptModule,\n+        if isinstance(m, jit.ScriptModule):\n+            # ScriptModule cannot be rewritten, so bypass it. The issue is it\n+            # requires explicitly calling its `__init__()`, calling\n+            # `nn.Module.__init__()` in the derived `RewrittenModule` is not\n+            # enough. And even if we init it we can't do much with it.\n+            return m\n+\n+                method = getattr(base_class, method_name, None)\n+                if method is None:\n+                    _LOGGER.warning(f\"{__qualname__} does not have attribute {method_name}\")\n+\n+",
    "Label": "clean"
},
{
    "Id": 1073,
    "Library": "pytorch",
    "Date": "2021/10/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0e8bd0c8d618ce8d6e128fc676b754af65d0662f",
    "Root Cause": "N.A",
    "Bug report": "[Pytorch Delegated Backend] Add macro to define sentinel value of debug handle. (#66584)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66584\n\nThis will help avoid \"-1\"s in different places in our and backend codebase when\ndebug handle is not known.\n\nTest Plan: CI\n\nReviewed By: sxu\n\nDifferential Revision: D31614478\n\nfbshipit-source-id: 97fceb04e3e78f52feda7b1ba1da08fa4480dd77",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+\n+#define DEBUG_HANDLE_UNKNOWN -1",
    "Label": "clean"
},
{
    "Id": 1074,
    "Library": "pytorch",
    "Date": "2021/10/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b696d64ef47dd55c8239a4cab0f39cae86bf1180",
    "Root Cause": "N.A",
    "Bug report": "Binaries without AVX512 kernels shouldn't report CPU Capability as AVX512 on machines with AVX512 support (#66703)\n\nSummary:\n### BUG\nIf a PyTorch binary is built with a compiler that doesn't support all the AVX512 intrinsics in the codebase, then it won't have ATen AVX512 kernels, but at runtime, CPU capability would still be incorrectly returned as AVX512 on a machine that supports AVX512. It seems that PyTorch Linux releases are done on CentOS with `gcc 7.3`, so this bug would manifest in the 1.10 release, unless a fix such as this one is added. gcc versions below 9.0 don't support all the AVX512 intrinsics in the codebase, such as `_mm512_set_epi16`.\n\n### FIX\nCPU Capability would be returned as AVX512 at runtime only if the binary was built with a compiler that supports all the AVX512 intrinsics in the codebase, and if the hardware the binary is being run on supports all the required AVX512 instruction sets.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66703\n\nReviewed By: gchanan\n\nDifferential Revision: D31732625\n\nPulled By: malfet\n\nfbshipit-source-id: e52d06b87fbe2af9b303a2e9c264189c8512d5ec",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#ifdef HAVE_AVX512_CPU_DEFINITION\n+#endif\n+#ifdef HAVE_AVX2_CPU_DEFINITION\n+#endif\n+#ifdef HAVE_AVX512_CPU_DEFINITION\n+    // GCC supports some AVX512 intrinsics such as _mm512_set_epi16 only in\n+    // versions 9 & beyond. So, we want to ensure that only releases built with\n+    // supported compilers on supported hardware return CPU Capability AVX512,\n+    // if it's supported on the hardware PyTorch is running on.\n+#endif\n+#ifdef HAVE_AVX2_CPU_DEFINITION\n+#endif",
    "Label": "clean"
},
{
    "Id": 1075,
    "Library": "pytorch",
    "Date": "2021/10/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/53cf7e844f6d8c2461586c89e891b92544c5da6d",
    "Root Cause": "N.A",
    "Bug report": "[SR] Fix bug in FuseListUnpackV2 (#67021)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67021\n\nWhen applying the equally split optimization, we still need to delete the list unpack node.\n\nI did an accuracy test yesterday but didn't catch this issue because my diffs were not properly synced between devservers (I use hlu1's devbig for testing and it had an old version of \"Add FuseListUnpackV2\"). But I did another test this morning and realized that there was an issue.\n\nThis is not affecting anything in prod right now since D31742293 has not landed.\n\nReviewed By: hlu1\n\nDifferential Revision: D31827278\n\nfbshipit-source-id: c7b05e3d8ec942632adcff4bdfebb8c27c1a7a39",
    "Number of deleted lines": 1,
    "Deleted lines": "-      node->output(0)->replaceAllUsesWith(node->input(0));",
    "Added lines": "+      list_unpack_node->output()->replaceAllUsesWith(node->input(0));\n+      list_unpack_node->destroy();\n+      to_remove.push_back(node);",
    "Label": "clean"
},
{
    "Id": 1076,
    "Library": "pytorch",
    "Date": "2021/10/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/db4165892befd1b9d4e5483c88ffe71da4d812d5",
    "Root Cause": "N.A",
    "Bug report": "[SmartCompose][OnDevice]fix function name bug in mobile export & Script to convert mobile model (#66915)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66915\n\nPull Request resolved: https://github.com/pytorch/pytorch-canary/pull/3\n\nfix function name bug in mobile export\n\nTest Plan: buck run pytext/fb/assistant/smart_compose:mobile_converter -- --model_input=pytext_training/tree/teams/assistant/smart_compose/300555761/model.ts --model_output=pytext_training/tree/teams/assistant/smart_compose/300555761/mobile_model_test.ts\n\nReviewed By: JacobSzwejbka\n\nDifferential Revision: D31782983\n\nfbshipit-source-id: 7288bb65adc7346d218980a535d68a12d8ef2033",
    "Number of deleted lines": 4,
    "Deleted lines": "-        if not hasattr(function, \"__name__\"):\n-            raise Exception(\n-                'At least one of your functions has no attribute __name__ please ensure all have one. m.foo.__name__ = \"foo\"')\n-        function_name = function.__name__",
    "Added lines": "+        if hasattr(function, \"__name__\"):\n+            function_name = function.__name__\n+        else:\n+            if hasattr(function, \"name\"):\n+                function_name = function.name  # type: ignore[attr-defined]\n+            else:\n+                raise Exception(\n+                    'At least one of your functions has no attribute name please ensure all have one. m.foo.name = \"foo\"')\n+",
    "Label": "clean"
},
{
    "Id": 1077,
    "Library": "pytorch",
    "Date": "2021/10/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/77f98ea5e09d1bf41ae72457fbdd3e93fa9e8533",
    "Root Cause": "N.A",
    "Bug report": "assert no duplicate yaml keys in codegen (#66238)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66238\n\nThe codegen should error if it sees two yaml entries with the same key. The default behavior of python's yaml loader is to overwrite duplicate keys with the new value.\n\nThis would have caught a nasty bug that showed up in https://github.com/pytorch/pytorch/pull/66225/files#r723796194.\n\nI tested it on that linked PR, to confirm that it errors correctly (and gives the line number containing the duplicate).\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses, albanD, sean-ngo\n\nDifferential Revision: D31464585\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 5b35157ffa9a933bf4b344c4b9fe2878698370a3",
    "Number of deleted lines": 1,
    "Deleted lines": "-YamlLoader = Loader",
    "Added lines": "+# A custom loader for YAML that errors on duplicate keys.\n+# This doesn't happen by default: see https://github.com/yaml/pyyaml/issues/165\n+class YamlLoader(Loader):\n+    def construct_mapping(self, node, deep=False):  # type: ignore[no-untyped-def]\n+        mapping = []\n+        for key_node, value_node in node.value:\n+            key = self.construct_object(key_node, deep=deep)  # type: ignore[no-untyped-call]\n+            assert key not in mapping, f\"Found a duplicate key in the yaml. key={key}, line={node.start_mark.line}\"\n+            mapping.append(key)\n+        mapping = super().construct_mapping(node, deep=deep)  # type: ignore[no-untyped-call]\n+        return mapping\n+",
    "Label": "clean"
},
{
    "Id": 1078,
    "Library": "pytorch",
    "Date": "2021/10/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/acb0157a3d9ac72c3424a184bf1e78c76e7cffaa",
    "Root Cause": "N.A",
    "Bug report": "Specialization for `c10::util:get_type_index<std::string>` (#66290)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66290\n\nAdd full specialization for std::string type index\n\nIt slightly speeds up compilation as well as solves the ambiguity how template instantiations implemented in inline namespaces are rendered during `__PRETTY_FUNCTION__` computation.\n\nNot sure what `#pragma` controls this behaviour, but when code is compiled by clang-12+ using libstdc++, `__PRETTY_PRINT__`, sometimes resolve `std::string` to `std::basic_string<char>` and sometimes to `std::__cxx11::basic_string<char>`, even though in the object file symbol is always inside `std::__cxx11::` namespace, which might break caffe2 serialization code that depends on dynamic hash generation\n\nTemplate name resolution were debugged using https://gist.github.com/malfet/c83b9ebd35730ebf8bac7af42682ea37\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: r-barnes\n\nDifferential Revision: D31490050\n\nfbshipit-source-id: 127091574cf6b92c7ec3f972821e4e76f5f626a9",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#if !defined(TORCH_PEDANTIC)\n+// Use precomputed hashsum for std::string\n+// Needed to workaround ambiguity in class name resolution\n+// into __PRETTY_FUNCION__ when abovementioned class is defined in inlined\n+// namespace. In multi-ABI C++ library, `std::string` is an alias to\n+// `std::__cxx11::basic_string<char>` which depending on compiler flags can be\n+// resolved to `basic_string<char>` either in `std` namespace or in\n+// `std::__cxx11` one (`__cxx11` is an inline namespace)\n+template <>\n+inline constexpr type_index get_type_index<std::string>() {\n+  // hashsum for std::basic_string<char>\n+  return type_index{4193213214807308375ULL};\n+}\n+#endif\n+",
    "Label": "clean"
},
{
    "Id": 1079,
    "Library": "pytorch",
    "Date": "2021/10/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a7cc07f1098a4d4db0dd46992df7be60e8f71b20",
    "Root Cause": "N.A",
    "Bug report": "quantized embedding: make error message clearer (#66051)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66051\n\nMake the error message clearer when quantized embedding is converted\nwith an unsupported dtype. This is helpful when debugging quantization\nerrors on new models.\n\nTest Plan:\n```\nclass M(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Embedding(1, 1)\n\nm = M().eval()\nm.qconfig = torch.quantization.QConfig(\n    activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8),\n    weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8))\nm.embedding.qconfig = m.qconfig\nmp = torch.quantization.prepare(m)\nmq = torch.quantization.convert(m)\n// error message now includes the incorrect dtype\n```\n\nImported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31472848\n\nfbshipit-source-id: 86f6d90bc0ad611aa9d1bdae24497bc6f3d2acaa",
    "Number of deleted lines": 2,
    "Deleted lines": "-        assert dtype == torch.quint8, 'The only supported dtype for nnq.Embedding is torch.quint8'\n-            'The only supported dtype for nnq.EmbeddingBag is torch.quint8 and torch.quint4x2'",
    "Added lines": "+        assert dtype == torch.quint8, \\\n+            f'The only supported weight dtype for nnq.Embedding is torch.quint8, got {dtype}'\n+            f'The only supported dtype for nnq.EmbeddingBag is torch.quint8 and torch.quint4x2, got {dtype}'",
    "Label": "clean"
},
{
    "Id": 1080,
    "Library": "pytorch",
    "Date": "2021/10/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/480a1a88d6a4db1b16c5af7bdcd5c1f1a0b9ea7d",
    "Root Cause": "N.A",
    "Bug report": "[DDP] Log iteration in debug mode (#65770)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65770\n\nThis logging info is printed out in debug mode, make it log the\niteration as well for clarity.\nghstack-source-id: 139838595\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao, wayi1\n\nDifferential Revision: D31222132\n\nfbshipit-source-id: 14519aae1ba0b2a35b4b962e7d1a957c9142c8f8",
    "Number of deleted lines": 1,
    "Deleted lines": "-      \"[Rank {} / {}] Training {} unused_parameter_size={} \\n \"",
    "Added lines": "+      \"[Rank {} / {}] [iteration {}] Training {} unused_parameter_size={} \\n \"\n+      ddp_logging_data.ints_map[\"iteration\"],",
    "Label": "clean"
},
{
    "Id": 1081,
    "Library": "pytorch",
    "Date": "2021/10/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f14e5e636dec70c38bde068cc291d97db5fda51b",
    "Root Cause": "N.A",
    "Bug report": "[fx2trt]fix slice tensor converter (#65960)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65960\n\nFix a bug in the converter and add support for negative dim.\n\nTest Plan: buck test mode/dev-nosan caffe2/torch/fb/fx2trt:test_narrow\n\nReviewed By: wushirong\n\nDifferential Revision: D31310232\n\nfbshipit-source-id: 62887369d830202cae6d63b41747225b12dcf754",
    "Number of deleted lines": 2,
    "Deleted lines": "-    dims = kwargs[\"dims\"]\n-        output_shape[dim] = (stops[i] - start[i]) // steps[i]",
    "Added lines": "+    ranks = len(input_val.shape) + (1 if network.has_implicit_batch_dimension else 0)\n+    dims = [dim % ranks for dim in kwargs[\"dims\"]]\n+\n+        output_shape[dim] = (stops[i] - starts[i]) // steps[i]",
    "Label": "clean"
},
{
    "Id": 1082,
    "Library": "pytorch",
    "Date": "2021/10/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ea0de37d2e5aa71178580f3bce7559cc6a3b9135",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch] Avoid string construction from const char* and speedup empty string creation if error messages are suppressed (#65939)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65939\n\nThis change includes 2 separate optimizations.\n\n1. Provide an overload of `debugString(const char*, ...)` in addition to `debugString(std::string, ...)` for cases where `const char*` is passed in to avoid `std::string` construction in cases where `STRIP_ERROR_MESSAGES` is also defined and the caller is passing in a `const char*`\n2. Return `std::string(\"\", 0)` instead of `\"\"` since the former triggers no call to `std::basic_string`'s constructor whereas the latter does. [Godbolt Link](https://godbolt.org/z/oTExed5h8). However, I'm surprosed by this since the man page for [std::basic_string](https://en.cppreference.com/w/cpp/string/basic_string/basic_string) clearly states that the constexpr overload is since C++20, and I am building using `-Os -std=c++17`\n\nGodbolt Screenshot:\n\n{F667311023}\n\nghstack-source-id: 139507542\n\nTest Plan:\nCI and local build via:\n\n```\nbuck build //xplat/caffe2/fb/lite_predictor:lite_predictor\n```\n\nReviewed By: swolchok\n\nDifferential Revision: D31312942\n\nfbshipit-source-id: aa24abbfe1c16419f235d037595321982614c5ea",
    "Number of deleted lines": 4,
    "Deleted lines": "-    return \"\";\n-      return c10::str(\"registered at \", file, \":\", line);\n-            *ns_, debugString(\"\", file_, line_)\n-      debugString(\"\", file_, line_)",
    "Added lines": "+  std::string debugString(const char* file, uint32_t line) {\n+#ifdef STRIP_ERROR_MESSAGES\n+    return std::string();\n+#else\n+    return c10::str(\"registered at \", file, \":\", line);\n+#endif\n+  }\n+\n+    return std::string();\n+      return debugString(file, line);\n+            *ns_, debugString(file_, line_)\n+      debugString(file_, line_)",
    "Label": "clean"
},
{
    "Id": 1083,
    "Library": "pytorch",
    "Date": "2021/09/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c7ef620a14b1cadf0e36d637dba9226b919ac3ff",
    "Root Cause": "N.A",
    "Bug report": "[quant] Add imports to the torch/ao/quantization/__init__.py (#64911)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64911\n\nThe import statements that involve the `quantize.py` were not added to the module level __init__ file. Those imports are necessary to mimic the behavior of the old import locations. Otherwise, the user would need to change their import statements to `from torch.ao.quantization.quantize import quantize` (instead of `from torch.ao.quantization import quantize`.\n\nAnother change in this diff is that we don't use `__all__` anymore. The all dunder was never used in quantization anyway, and just creates a potential bug when using `from ... import *`.\nghstack-source-id: 139342483\n\nTest Plan: `buck test mode/dev //caffe2/test:quantization`\n\nReviewed By: vkuzo\n\nDifferential Revision: D30897663\n\nfbshipit-source-id: a7b4919a191755e3ba690a79ce3362889f416689",
    "Number of deleted lines": 1,
    "Deleted lines": "-from .fuse_modules import *  # noqa: F403",
    "Added lines": "+# flake8: noqa: F403\n+\n+from .fuse_modules import fuse_modules  # noqa: F403\n+from .fuser_method_mappings import *  # noqa: F403\n+from .observer import *  # noqa: F403\n+from .qconfig import *  # noqa: F403\n+from .quantization_mappings import *  # noqa: F403\n+from .quantize_jit import *  # noqa: F403\n+from .stubs import *  # noqa: F403\n+\n+def default_eval_fn(model, calib_data):\n+    r\"\"\"\n+    Default evaluation function takes a torch.utils.data.Dataset or a list of\n+    input Tensors and run the model on the dataset\n+    \"\"\"\n+    for data, target in calib_data:\n+        model(data)",
    "Label": "clean"
},
{
    "Id": 1084,
    "Library": "pytorch",
    "Date": "2021/09/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/12137db5e3df6d5b12f4fc6392afbe975c4f42c8",
    "Root Cause": "N.A",
    "Bug report": "Fix the slowdown of _object_to_tensor since 1.9 (#65721)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65721\n\n#Closes: https://github.com/pytorch/pytorch/issues/65696\n\nThe bug is introduced in https://github.com/pytorch/pytorch/pull/55861, and it causes 100X slowdown since 1.9.\nghstack-source-id: 139128267\n\nTest Plan:\nPerformance test:\n```\nimport time\n\nfrom torch.distributed.distributed_c10d import _object_to_tensor\n\nstart = time.time()\n_object_to_tensor(\"x\" * 50_000_000)\nprint(\"Time:\", time.time() - start)\n```\n\nReviewed By: rohan-varma\n\nDifferential Revision: D31219794\n\nfbshipit-source-id: 1abec38f9d51361c1eab6ad5efd87b589322e208",
    "Number of deleted lines": 2,
    "Deleted lines": "-    byte_tensor = torch.tensor(byte_storage, dtype=torch.uint8)\n-    local_size = torch.tensor([byte_tensor.numel()], dtype=torch.long)",
    "Added lines": "+    # Do not replace `torch.ByteTensor` or `torch.LongTensor` with torch.tensor and specifying dtype.\n+    # Otherwise, it will casue 100X slowdown.\n+    # See: https://github.com/pytorch/pytorch/issues/65696\n+    byte_tensor = torch.ByteTensor(byte_storage)\n+    local_size = torch.LongTensor([byte_tensor.numel()])",
    "Label": "clean"
},
{
    "Id": 1085,
    "Library": "pytorch",
    "Date": "2021/09/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cd2656a2e5c5caa4f56843a62e8a43721f25e08e",
    "Root Cause": "N.A",
    "Bug report": "[package] add some docs describing how to debug dependencies (#65704)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65704\n\nAs title.\n\nTest Plan: Imported from OSS\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D31209866\n\nPulled By: suo\n\nfbshipit-source-id: 4c8ec1d5418ea75b71c4b9a498b86f0ef5383544",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+See why a given module was included as a dependency?\n+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+Say there is a given module ``foo``, and you want to know why your :class:`PackageExporter` is pulling in ``foo`` as a dependency.\n+\n+:meth:`PackageExporter.get_rdeps` will return all modules that directly depend on ``foo``.\n+\n+If you would like to see how a given module ``src`` depends on ``foo``, the :meth:`PackageExporter.all_paths` method will\n+return a DOT-formatted graph showing all the dependency paths between ``src`` and ``foo``.\n+\n+If you would just like to see the whole dependency graph of your :class:`PackageExporter`, you can use :meth:`PackageExporter.dependency_graph_string`.\n+",
    "Label": "clean"
},
{
    "Id": 1086,
    "Library": "pytorch",
    "Date": "2021/09/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cd80bbe5f514bd383613594cb5f92be99fecc453",
    "Root Cause": "N.A",
    "Bug report": "Bug fixes in dataframe_wrapper (#65629)\n\nSummary:\n## Description\n- Updated functions in `dataframe_wrapper.py` to return values\n- Fixed bug in `set_df_wrapper` to update `global default_wrapper`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65629\n\nReviewed By: ejguan\n\nDifferential Revision: D31180110\n\nPulled By: Nayef211\n\nfbshipit-source-id: a8046e582fd6ce982fcdc89dae4932d0edc83d6b",
    "Number of deleted lines": 8,
    "Deleted lines": "-\n-    wrapper.create_dataframe(data, columns)\n-    wrapper.is_dataframe(data)\n-    wrapper.is_column(data)\n-    wrapper.concat(buffer)\n-    wrapper.iterate(data)\n-    wrapper.get_item(data, idx)\n-    wrapper.get_len(df)",
    "Added lines": "+    global default_wrapper\n+    return wrapper.create_dataframe(data, columns)\n+    return wrapper.is_dataframe(data)\n+    return wrapper.is_column(data)\n+    return wrapper.concat(buffer)\n+    return wrapper.iterate(data)\n+    return wrapper.get_item(data, idx)\n+    return wrapper.get_len(df)",
    "Label": "clean"
},
{
    "Id": 1087,
    "Library": "pytorch",
    "Date": "2021/09/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/32f0387ee8ac93dd67eb21e5fc48c2453f2661bc",
    "Root Cause": "N.A",
    "Bug report": "Bug in CosineAnnealingWarmRestarts in optim/lr_scheduler.py (#64758)\n\nSummary:\n## {emoji:1f41b} Bug\n'CosineAnnealingWarmRestarts'  object has no attribute 'T_cur'.\nIn the Constructor of the CosineAnnealingWarmRestarts, we're calling the constructor of the Parent class (_LRScheduler) which inturn calls the step method of the CosineAnnealingWarmRestarts.\nThe called method tries to update the object's attribute  'T_cur' which is not defined yet. So it raises the error.\nThis only holds, when we give the value for last_epoch argument as 0 or greater than 0 to the 'CosineAnnealingWarmRestarts', while initializing the object.\n\n![Bug_in_CosineAnnealingWarmRestarts](https://user-images.githubusercontent.com/77477328/132552212-70abc8b5-0357-4c35-90a9-832648bac607.png)\n## To Reproduce\n\nSteps to reproduce the behavior:\n\n1. Give the value for the last_epoch argument as zero OR\n1. Give the value for the last_epoch argument as a Positive integer.\n\n## Expected behavior\n\nI only expected the 'CosineAnnealingWarmRestarts' object to be initialized.\n\n## Environment\n\nPyTorch version: 1.9.0+cpu\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\nOS: Ubuntu 20.04.2 LTS (x86_64)\nGCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\nClang version: Could not collect\nCMake version: version 3.21.2\nLibc version: glibc-2.31\nPython version: 3.8.10  [GCC 9.4.0] (64-bit runtime)\nPython platform: Linux-5.8.0-59-generic-x86_64-with-glibc2.29\nIs CUDA available: False\nCUDA runtime version: No CUDA\n\n## Additional context\nWe can able to solve this bug by moving the line 'self.T_cur = self.last_epoch' above the 'super(CosineAnnealingWarmRestarts,self).__init__()' line. Since we've initialized the \"self.T_cur\" to the object.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64758\n\nReviewed By: ezyang\n\nDifferential Revision: D31113694\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 98c0e292291775895dc3566fda011f2d6696f721",
    "Number of deleted lines": 3,
    "Deleted lines": "-\n-        self.T_cur = self.last_epoch\n-",
    "Added lines": "+        self.T_cur = last_epoch",
    "Label": "clean"
},
{
    "Id": 1088,
    "Library": "pytorch",
    "Date": "2021/09/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ed30afd480c78673036f36420019c76a5d36dc54",
    "Root Cause": "N.A",
    "Bug report": "Speed up torch.unique_consecutive() (#64835)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/62690\n\nLike the way `unique_consecutive_cpu_template` implemented, this PR reimplements `_unique_dim_cpu_impl` to get better performance.\nAlso, because the overhead of `unique_dim_consecutive_cpu` is quite large, directly call `unique_consecutive_cpu_template` when we know the given input is a 1d-array.\n\n## Benchmark\n### Script\n```python\nimport torch\nimport time\n\ntorch.manual_seed(0)\nt = torch.randint(500, (10000000, ))\nt = torch.sort(t)[0]\n\nstart = time.time()\nuniques, inverse, counts = torch.unique_consecutive(t, dim=0, return_inverse=True, return_counts=True)\nend = time.time()\nprint(\"torch.unique_consecutive(dim=0) time:\", end - start)\n\nstart = time.time()\nuniques2, inverse2, counts2 = torch.unique_consecutive(t, return_inverse=True, return_counts=True)\nend = time.time()\nprint(\"torch.unique_consecutive() time:\", end - start)\n\nt = torch.randint(500, (10000000, 2))\nt = torch.sort(t)[0]\n\nstart = time.time()\nuniques, inverse, counts = torch.unique_consecutive(t, dim=0, return_inverse=True, return_counts=True)\nend = time.time()\nprint(\"torch.unique_consecutive(dim=0) time:\", end - start)\n\nstart = time.time()\nuniques, inverse, counts = torch.unique_consecutive(t, dim=1, return_inverse=True, return_counts=True)\nend = time.time()\nprint(\"torch.unique_consecutive(dim=1) time:\", end - start)\n```\n\n### Before\n```\ntorch.unique_consecutive(dim=0) time: 78.64345622062683\ntorch.unique_consecutive() time: 0.029544353485107422\ntorch.unique_consecutive(dim=0) time: 91.49796152114868\ntorch.unique_consecutive(dim=1) time: 0.30872368812561035\n```\n\n### After\n```\ntorch.unique_consecutive(dim=0) time: 0.08256125450134277\ntorch.unique_consecutive() time: 0.08162403106689453\ntorch.unique_consecutive(dim=0) time: 35.58408498764038\ntorch.unique_consecutive(dim=1) time: 1.6258199214935303\n```\n\n## System Information\n```\nCollecting environment information...\nPyTorch version: 1.10.0a0+git7f1932e\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.3 LTS (x86_64)\nGCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\nClang version: 10.0.0-4ubuntu1\nCMake version: version 3.16.3\nLibc version: glibc-2.31\n\nPython version: 3.8.10 (default, Jun  2 2021, 10:49:15)  [GCC 9.4.0] (64-bit runtime)\nPython platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.29\nIs CUDA available: False\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\n\nVersions of relevant libraries:\n[pip3] numpy==1.21.2\n[pip3] torch==1.10.0a0+gitbe09195\n[conda] Could not collect\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64835\n\nReviewed By: jbschlosser\n\nDifferential Revision: D30894906\n\nPulled By: ngimel\n\nfbshipit-source-id: 42ab76d638391ce6c4e589d9c71bdf7579310ad9",
    "Number of deleted lines": 14,
    "Deleted lines": "-    // save to calculate distance to iterators\n-    ForwardIt begin = first;\n-    // set first inverse index and count\n-    inverse_indices_vec[indices[0]] = 0;\n-    counts[0] += 1;\n-    while (++first != last) {\n-      if (!at::equal(*result, *first) && ++result != first) {\n-          *result = std::move(*first);\n-      int64_t idx_result = std::distance(begin, result);\n-      int64_t idx_first = std::distance(begin, first);\n-      inverse_indices_vec[indices[idx_first]] = idx_result;\n-      counts[idx_result] += 1;\n-\n-  if (!dim.has_value()) {",
    "Added lines": "+    TORCH_INTERNAL_ASSERT(inverse_indices_vec.is_contiguous(),\n+        \"_unique_dim_cpu_impl only support contiguous inverse_indices_vec\");\n+    TORCH_INTERNAL_ASSERT(counts.is_contiguous(),\n+        \"_unique_dim_cpu_impl only support contiguous counts\");\n+\n+    int64_t *indices_data = indices.data();\n+    int64_t *inverse_data = inverse_indices_vec.data_ptr<int64_t>();\n+    int64_t *counts_data = counts.data_ptr<int64_t>();\n+    ForwardIt previous = first;\n+    int64_t *current_counts = counts_data;\n+    for (ForwardIt current = first; current != last; current++) {\n+      if (!at::equal(*current, *result)) {\n+        *(++result) = std::move(*current);\n+        *(current_counts++) = std::distance(previous, current);\n+        previous = current;\n+      inverse_data[*(indices_data++)] = std::distance(first, result);\n+    *current_counts = std::distance(previous, last);\n+  if (!dim.has_value() || (dim.value() == 0 && self.dim() == 1)) {",
    "Label": "clean"
},
{
    "Id": 1089,
    "Library": "pytorch",
    "Date": "2021/09/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/46c886e8a6d3d9f502fa8c0985784436ab7f9543",
    "Root Cause": "N.A",
    "Bug report": "fix acc topk's handling of the case when dim=0, fix tests as well (#64727)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64727\n\nthe acc ops convertor for topk has a subtle bug (i found this while trying to introduce max/min)\nthe code does not differentiate between dim == None and dim ==0, but these are both different computations\n\nReviewed By: jfix71, 842974287\n\nDifferential Revision: D30833621\n\nfbshipit-source-id: 6cd84e6ca4e95bb1a6d6465e61830b76808a9c78",
    "Number of deleted lines": 1,
    "Deleted lines": "-    dim = (kwargs[\"dim\"] if kwargs[\"dim\"] else -1) % num_dims",
    "Added lines": "+    dim = (kwargs[\"dim\"] if kwargs[\"dim\"] is not None else -1) % num_dims",
    "Label": "clean"
},
{
    "Id": 1090,
    "Library": "pytorch",
    "Date": "2021/09/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/233e3e5bb499b97e0a68ba93b6928c2e96096777",
    "Root Cause": "N.A",
    "Bug report": "Fix lop1p lowering bug (#64724)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64724\n\n`1` will introduce a int tensor instead of float tensor, which doesn't work well with downstream operators (elementwise). Error would be like\n```\n[TensorRT] WARNING: IElementWiseLayer with inputs (Unnamed Layer* 1) [Unary]_output and (Unnamed Layer* 2) [Constant]_output: first input has type Float but second input has type Int32.\n```\nChanging the constant to be float type fixes this.\n\nReviewed By: 842974287\n\nDifferential Revision: D30796959\n\nfbshipit-source-id: 0538e4dd960df9ce87a2d4cafe8f1a0c061b6bad",
    "Number of deleted lines": 1,
    "Deleted lines": "-        add_kwargs = {\"input\": node.kwargs[\"input\"], \"other\": 1}",
    "Added lines": "+        add_kwargs = {\"input\": node.kwargs[\"input\"], \"other\": 1.0}",
    "Label": "clean"
},
{
    "Id": 1091,
    "Library": "pytorch",
    "Date": "2021/09/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b6544ef81511cc527e4945a969d8a680d2d5069f",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch] Fix MobileDebugInfo vector copy (#64030)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64030\n\nghstack-source-id: 137566816\n\nTest Plan:\nPixel 3 before:  https://our.intern.facebook.com/intern/aibench/details/320277034999340\nPixel 3 after: https://our.intern.facebook.com/intern/aibench/details/724509739115867\n\ncan see the vector copy disappear in the flame graph. Overall mean decreased from 354 ms to 348 ms (though I'm not sure if this is outside usual noise).\n\nReviewed By: raziel\n\nDifferential Revision: D30559032\n\nfbshipit-source-id: 6d8bb5396d3449cc63023ee7acf694b5d146ddc1",
    "Number of deleted lines": 5,
    "Deleted lines": "-          jit::unpickle(\n-              reinterpret_cast<const char*>(debug_data.get()), debug_size)\n-              .toTuple()\n-              ->elements();\n-        auto tup_elems = val.toTuple()->elements();",
    "Added lines": "+          std::move(\n+              *jit::unpickle(\n+                   reinterpret_cast<const char*>(debug_data.get()), debug_size)\n+                   .toTuple())\n+              .elements();\n+        auto tup_elems = std::move(*std::move(val).toTuple()).elements();",
    "Label": "clean"
},
{
    "Id": 1092,
    "Library": "pytorch",
    "Date": "2021/09/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1a033b45dd119c7dc8856b50eb679d174ab97af3",
    "Root Cause": "N.A",
    "Bug report": "[JIT] Fix a bug of rejecting ops with AliasAnalysisKind::CONSERVATIVE (#64336)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64336\n\nCurrently AliasDB rejects any user-defined ops with `AliasAnalysisKind::CONSERVATIVE` if they do not have a special treatment for alias analysis. For example, the following alias schema gets rejects:\n\n```\n  m.def(torch::schema(\n      \"namescope::my_op(...) -> ...\",\n      c10::AliasAnalysisKind::CONSERVATIVE));\n```\n\nThis rejection condition is contradictory: AliasDB can handle ops with `CONSERVATIVE` in a general way without any special casing at https://fburl.com/diffusion/op5u72sk calling https://fburl.com/diffusion/h3aws5dd which seems very appropriate to be conservative for alias analysis.\n\nThis change corrects the rejection condition to be satisfied for ops *with* special casing but have `CONSERVATIVE`, since they both cannot be used simultaneously.\n\nTest Plan:\nConfirmed that\n```\n  m.def(torch::schema(\n      \"namescope::my_op(...) -> ...\",\n      c10::AliasAnalysisKind::CONSERVATIVE));\n```\ngets accepted and `my_op`'s all inputs and outputs are put to point to wildcard(*) by AliasDB.\n\nReviewed By: eellison\n\nDifferential Revision: D30690121\n\nfbshipit-source-id: 431cc1a84edd5227f52b44a0fd85d5eb16f3c288",
    "Number of deleted lines": 2,
    "Deleted lines": "-    if (!aliasAnalysisHasSpecialCaseFor(s) &&\n-          \"Missing special case in alias analysis for non-schematized\"",
    "Added lines": "+    if (aliasAnalysisHasSpecialCaseFor(s) &&\n+          \"Conflict in special casing in alias analysis for non-schematized\"",
    "Label": "clean"
},
{
    "Id": 1093,
    "Library": "pytorch",
    "Date": "2021/09/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/778af565048e6160ce59fb5eedd8455e629f7942",
    "Root Cause": "N.A",
    "Bug report": "[DDP Comm Hook] Add debugging communication hooks to ddp_comm_hooks.rst (#64352)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64352\n\nas title\nghstack-source-id: 137246253\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D30694089\n\nfbshipit-source-id: a78110b11d59bb0718f43c99ede23f2fd8ab21d0",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+Debugging Communication Hooks\n+-----------------------------\n+\n+As the name implies, debugging communication hooks are **only** used for debugging and performance optimization purpose.\n+\n+.. currentmodule:: torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks\n+\n+.. warning ::\n+    Debugging communication hooks do not necessarily output the correct results.\n+\n+.. autofunction:: noop_hook\n+",
    "Label": "clean"
},
{
    "Id": 1094,
    "Library": "pytorch",
    "Date": "2021/08/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d96ef8c1b1860185f0bd91699f71a087cf9e9efe",
    "Root Cause": "N.A",
    "Bug report": "[Static Runtime] SR clones graph input (#63704)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63704\n\nPreviously SR did not clone the graph. This was leading to subtle bugs in `testStaticRuntime`; static runtime would modify its graph, and the graph used by the JIT interpreter would change as well. The JIT interpreter would then crash if SR-only ops were added!\n\nCloning the graph is more consistent with the behavior of the `Module` ctor.\n\nTest Plan: `buck test caffe2/benchmarks/static_runtime/...`\n\nReviewed By: hlu1\n\nDifferential Revision: D30463294\n\nfbshipit-source-id: b771551a1f55f95fde79373b23babcf3e5ddf726",
    "Number of deleted lines": 1,
    "Deleted lines": "-    : StaticModule(PrepareForStaticModule(g, opts), opts) {}",
    "Added lines": "+    : StaticModule(PrepareForStaticModule(g->copy(), opts), opts) {}",
    "Label": "clean"
},
{
    "Id": 1095,
    "Library": "pytorch",
    "Date": "2021/08/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e0fe5699c4b7dabd132834b690d6cc2513e0e978",
    "Root Cause": "N.A",
    "Bug report": "enable increment build for build_libtorch (#63074)\n\nSummary:\nSince issue https://github.com/pytorch/pytorch/issues/59859 is resolved.\n\nrerun_cmake in build_libtorch should not be hardcoded.\nbuild_libtorch is necessary to generate debug version libtorch.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63074\n\nReviewed By: VitalyFedyunin, seemethere\n\nDifferential Revision: D30306705\n\nPulled By: malfet\n\nfbshipit-source-id: f2077d334191f4973da0681560937bc8bab730c1",
    "Number of deleted lines": 1,
    "Deleted lines": "-                 rerun_cmake=True, cmake_only=False, cmake=CMake())",
    "Added lines": "+    parser.add_argument('--rerun-cmake', action=\"store_true\", help='rerun cmake')\n+    parser.add_argument('--cmake-only', action=\"store_true\",\n+                        help='Stop once cmake terminates. Leave users a chance to adjust build options')\n+                 rerun_cmake=options.rerun_cmake, cmake_only=options.cmake_only, cmake=CMake())",
    "Label": "clean"
},
{
    "Id": 1096,
    "Library": "pytorch",
    "Date": "2021/08/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6bb68ba5075a9948e9f52246453e964749226098",
    "Root Cause": "N.A",
    "Bug report": "Fix interpreter debug logging message (#63499)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63499\n\nhttps://github.com/pytorch/pytorch/pull/62418 combine the instruction and debug handle. This change fix the debugging message.\nghstack-source-id: 136184053\n\nTest Plan: Uncomment and it works\n\nReviewed By: kimishpatel, raziel\n\nDifferential Revision: D30390699\n\nfbshipit-source-id: e32b7b297ad3b7d8bffebd025d15519083a244c4",
    "Number of deleted lines": 8,
    "Deleted lines": "-      //    std::cout << \"RUNNING \" << pc << \" \" << code_->instructions_[pc];\n-      //    if (inst.op == OP) {\n-      //      std::cout << \", \" << code_->op_names_[inst.X].name;\n-      //      if (!code_->op_names_[inst.X].overload_name.empty()) {\n-      //        std::cout << \".\" << code_->op_names_[inst.X].overload_name;\n-      //      }\n-      //    }\n-      //    std::cout << std::endl;",
    "Added lines": "+      // std::cout << \"RUNNING \" << pc << \" \"\n+      //           << code_->instructions_with_handles_[pc].instruction;\n+      // if (inst.op == OP) {\n+      //   std::cout << \", \" << code_->op_names_[inst.X].name;\n+      //   if (!code_->op_names_[inst.X].overload_name.empty()) {\n+      //     std::cout << \".\" << code_->op_names_[inst.X].overload_name;\n+      //   }\n+      // }\n+      // std::cout << std::endl;",
    "Label": "clean"
},
{
    "Id": 1097,
    "Library": "pytorch",
    "Date": "2021/08/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8bdd5424173cd08ddafd77cb45d38c7540ae72d6",
    "Root Cause": "N.A",
    "Bug report": "[TensorExpr] Add debug logging to LoopNest::computeInline. (#63196)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/63196\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D30292778\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: d8a111b75466a9354f6d048119cc6f814c9d5abb",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include <torch/csrc/jit/jit_log.h>\n+      GRAPH_DEBUG(\n+          \"ComputeInline: Inline mapping: \",\n+          std::to_string(func_callee_arg),\n+          \" -> \",\n+          std::to_string(func_caller_param));\n+    GRAPH_DEBUG(\"ComputeInline: Before rewriting body: \", std::to_string(body));\n+    GRAPH_DEBUG(\n+        \"ComputeInline: After rewriting body: \", std::to_string(result));\n+      GRAPH_DEBUG(\"ComputeInline: Inline mapping: erasing\", std::to_string(v));\n+    GRAPH_DEBUG(\n+        \"ComputeInline: created random bindings for \", std::to_string(new_var));\n+  GRAPH_DEBUG(\"ComputeInline: Def: \", std::to_string(relevant_store));",
    "Label": "clean"
},
{
    "Id": 1098,
    "Library": "pytorch",
    "Date": "2021/08/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2fd14735d677d1c2cf87e4c76aea2111bc30c17b",
    "Root Cause": "N.A",
    "Bug report": "[easy][PyTorchEdge] print error message when failing to load model file (#63404)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63404\n\n# Context\nLoading a model file using `fopen` might error out for multiple reasons. Repro'ing the error on devices takes some time and efforts. Logging the error no# will help in debugging and fixing the error quickly.\n\n# Mitigation\nPrintout the error message of the `fopen` to help users debug the issue.\n\nTest Plan:\n```\n(base) [pavithran@devvm1803.vll0 /data/users/pavithran/fbsource] buck run xplat/caffe2/fb/lite_predictor:lite_predictor -- --model=/home/pavithran/models/prod/GAaNhAoTIV6cIvgJAHn30m8NR1QgbmQwAAAA.ptl --use_bundled_input=0\nBuilding: finished in 0.5 sec (100%) 354/354 jobs, 0/354 updated\n  Total time: 0.6 sec\nRun with 24 threads\nRun with 24 threads\nLoading model...\nterminate called after throwing an instance of 'c10::Error'\n  what():  open file failed because of errno 2 on fopen: No such file or directory, file path: /home/pavithran/models/prod/GAaNhAoTIV6cIvgJAHn30m8NR1QgbmQwAAAA.ptl\nException raised from RAIIFile at xplat/caffe2/caffe2/serialize/file_adapter.cc:15 (most recent call first):\n(no backtrace available)\n```\n\nReviewed By: dhruvbird\n\nDifferential Revision: D30372308\n\nfbshipit-source-id: 5346e828f53f6bc5d871b403586566a3332a389a",
    "Number of deleted lines": 2,
    "Deleted lines": "-\n-    AT_ERROR(\"open file failed, file path: \", file_name);",
    "Added lines": "+#include <cerrno>\n+#include <string>\n+    char buf[1024];\n+    buf[0] = '\\0';\n+#if defined(_WIN32) && (defined(__MINGW32__) || defined(_MSC_VER))\n+  strerror_s(buf, sizeof(buf), errno);\n+#else\n+  strerror_r(errno, buf, sizeof(buf));\n+#endif\n+    AT_ERROR(\n+        \"open file failed because of errno \",\n+        errno,\n+        \" on fopen: \",\n+        buf,\n+        \", file path: \",\n+        file_name);",
    "Label": "clean"
},
{
    "Id": 1099,
    "Library": "pytorch",
    "Date": "2021/08/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/63554cfb3d10e98e611bdd07c7b682b1e5723d32",
    "Root Cause": "N.A",
    "Bug report": "Remove backend_debug from torch_core srcs and replace with library dependency (#63111)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63111\n\n### Problem:\nBuck contains at least two libraries which have `backend_debug_info.cpp` as a source, `torch_core` and `backend_interface_lib`. `backend_debug_info.cpp` registers BackendDebugInfo as a class. If targets contain both libraries (e.g. sparkAR debug build with NNAPI delegation), then BackendDebugInfo is registered twice, causing a runtime error.\n### Solution:\nThese changes remove `backend_debug_info.cpp` and `backend_interface.cpp` as a source in `torch_core` and adds backend_interface_lib as a dependency instead.\n\n**build_variables.bzl:**\n- Added a list that excludes `backend_debug_info.cpp` and `backend_interface.cpp` ( both srcs already included by `backend_interface_lib`)\n\n**buck:**\n- torch_core: Removed `backend_debug_info.cpp` from srcs and added `backend_interface_lib` deps\n- backend_interface_lib: Replaced `torch_mobile_core` dep with more specific deps\n  - to avoid an indirect dep between `torch_core` and `torch_mobile_core`\n\nghstack-source-id: 135981061\n\nTest Plan:\n### Test Plan:\nBuild and run SparkAR internally with Android NNAPI Delegation (`buck build --show-output arstudioplayer_arm64_debug`)\nand internal tests.\n\nReviewed By: iseeyuan\n\nDifferential Revision: D30259034\n\nfbshipit-source-id: 0c14c827732f07fb9b9bd25a999828b51793cdcc",
    "Number of deleted lines": 3,
    "Deleted lines": "-core_sources_full_mobile = [\n-    \"torch/csrc/jit/backends/backend_debug_info.cpp\",\n-    \"torch/csrc/jit/backends/backend_interface.cpp\",",
    "Added lines": "+core_sources_full_mobile_no_backend_interface = [\n+core_sources_full_mobile = core_sources_full_mobile_no_backend_interface + [\n+    \"torch/csrc/jit/backends/backend_debug_info.cpp\",\n+    \"torch/csrc/jit/backends/backend_interface.cpp\",\n+]\n+",
    "Label": "clean"
},
{
    "Id": 1100,
    "Library": "pytorch",
    "Date": "2021/08/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3aecec609f5ad70f1e0b646caf4c637fe65fe37d",
    "Root Cause": "N.A",
    "Bug report": "Move Android Nnapi srcs from aten_native_cpu to aten_cpu (#62919)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/62919\n\nMove Android NNAPI srcs (nnapi_bind.cpp, nnapi_wrapper.cpp, nnapi_model_loader.cpp) from aten_native_cpu to aten_cpu, so that later the NNAPI delegate's execution library can depend on it.\n\naten_native_cpu is built selectively per app, but the srcs have no selective components and are required for the NNAPI delegate library in D30259033.\n\nSee Buck Dependencies: https://docs.google.com/document/d/17RuWkqWKCO6sc5fKzIDkGeNhhvMk7BvJOqeSnGsHZ8o/edit?usp=sharing\nghstack-source-id: 135981062\n\nTest Plan: `buck build --show-output arstudioplayer_arm64_debug` and internal tests\n\nReviewed By: iseeyuan\n\nDifferential Revision: D30164867\n\nfbshipit-source-id: 0beff481ff250e75664ce8393beabbeb9db66770",
    "Number of deleted lines": 3,
    "Deleted lines": "-    \"aten/src/ATen/nnapi/nnapi_bind.cpp\",\n-    \"aten/src/ATen/nnapi/nnapi_wrapper.cpp\",\n-    \"aten/src/ATen/nnapi/nnapi_model_loader.cpp\",",
    "Added lines": "+    \"aten/src/ATen/nnapi/nnapi_bind.cpp\",\n+    \"aten/src/ATen/nnapi/nnapi_wrapper.cpp\",\n+    \"aten/src/ATen/nnapi/nnapi_model_loader.cpp\",",
    "Label": "clean"
},
{
    "Id": 1101,
    "Library": "pytorch",
    "Date": "2021/08/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5cf32c1d090f4dbe41b14135c0c9cc15415c3f31",
    "Root Cause": "N.A",
    "Bug report": "Fix Nnapi backend execute's dangling pointer (#63092)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63092\n\nBug discovered while testing NNAPI Delegate on SparkAR.\nUsing\n```\nc10::IntArrayRef order = {0, 2, 3, 1};\nfixed_inputs.push_back(tensorInp.get(i).permute(order).contiguous());\n```\nresults in a garbage value for order in `permute()`.\nMoving order inside the call to `permute()` fixes this issue. Problem is seemingly related to https://github.com/pytorch/pytorch/issues/44409, but luckily the solution in this case is simple.\n\nBug wasn't caught earlier, since regular unit tests weren't affected by the dangling pointer, and address sanitizer NNAPI tests are turned off due to there being a different failure (T95764916).\nghstack-source-id: 135526129\n\nTest Plan:\nRun Unit tests: `python test/test_jit.py`\n\nBuild and run SparkAR on an Android phone at the top of this diff stack (D30173959): `buck build --show-output arstudioplayer_arm64_debug -c pt.enable_nnapi=1`\n\nReviewed By: raziel, iseeyuan\n\nDifferential Revision: D30237504\n\nfbshipit-source-id: c946d81feefc453b43d9295d8d6f509cafdcec03",
    "Number of deleted lines": 4,
    "Deleted lines": "-        c10::IntArrayRef order = {0, 2, 3, 1};\n-        fixed_inputs.push_back(tensorInp.get(i).permute(order).contiguous());\n-        c10::IntArrayRef order = {0, 3, 1, 2};\n-        outputs.set(i, outputs.get(i).permute(order));",
    "Added lines": "+        fixed_inputs.push_back(\n+            tensorInp.get(i).permute({0, 2, 3, 1}).contiguous());\n+        outputs.set(i, outputs.get(i).permute({0, 3, 1, 2}));",
    "Label": "clean"
},
{
    "Id": 1102,
    "Library": "pytorch",
    "Date": "2021/07/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/58d45d950bb65eb8582b7bd95177500dc2b1e82f",
    "Root Cause": "N.A",
    "Bug report": "[DDP] Log unused param names under DETAIL debug mode. (#62209)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/62209\n\nWhen `TORCH_DISTRIBUTED_DEBUG=DETAIL` is set, log names and indices of unused parameters when searching for them.\n\nMotivation is that we have seen a couple of issues occasionally when there are errors related to parameter possibly being marked as unused when it shouldn't, this can help narrow down the root cause by explicitly logging param names that are marked as unused.\nghstack-source-id: 134541461\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D29916085\n\nfbshipit-source-id: d84cf637cbbd811521e6264ffd6c50ca8a79595b",
    "Number of deleted lines": 1,
    "Deleted lines": "-#include <c10d/default_comm_hooks.hpp>",
    "Added lines": "+#include <c10d/Utils.hpp>\n+#include <c10d/default_comm_hooks.hpp>\n+\n+      if (ddp_debug_level_ == c10d::DistributedDebugLevel::DETAIL) {\n+        const auto param_info = param_names_.find(it.second);\n+        TORCH_INTERNAL_ASSERT(\n+            param_info != param_names_.end(),\n+            \"Did not find variable index \",\n+            it.second,\n+            \" in DDP parameter name mapping!\");\n+        const auto param_name = param_info->second;\n+        LOG(INFO) << \"[Rank \" << process_group_->getRank() << \"]: \"\n+                  << \"Parameter \" << param_name << \" at index \" << it.second\n+                  << \" is marked as unused.\";\n+      }",
    "Label": "clean"
},
{
    "Id": 1103,
    "Library": "pytorch",
    "Date": "2021/07/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5ebc7c9f97b2f1e656b51b8297349465d3eb355f",
    "Root Cause": "N.A",
    "Bug report": "Avoid holding GIL while calling retrieveContext. (#61588)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61588\n\nAs part of debugging https://github.com/pytorch/pytorch/issues/60290,\nwe discovered the following deadlock:\n\n```\nThread 79 (Thread 0x7f52ff7fe700 (LWP 205437)):\n#0  pthread_cond_timedwait@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_timedwait.S:225\n#1  0x0000564880199152 in PyCOND_TIMEDWAIT (cond=0x564880346080 <gil_cond>, mut=0x564880346100 <gil_mutex>, us=5000) at /home/builder/ktietz/cos6/ci_cos6/python_1622833237666/work/Python/condvar.h:103\n#2  take_gil (tstate=0x7f5254005ef0) at /home/builder/ktietz/cos6/ci_cos6/python_1622833237666/work/Python/ceval_gil.h:224\n#3  0x0000564880217b62 in PyEval_AcquireThread (tstate=0x7f5254005ef0) at /home/builder/ktietz/cos6/ci_cos6/python_1622833237666/work/Python/ceval.c:278\n#4  0x00007f557d54aabd in pybind11::gil_scoped_acquire::gil_scoped_acquire() () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so\n#5  0x00007f557da7792f in (anonymous namespace)::concrete_decref_fn(c10::impl::PyInterpreter const*, _object*) () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so\n#6  0x00007f5560dadba6 in c10::TensorImpl::release_resources() () from /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so\n#7  0x00007f5574c885bc in std::_Sp_counted_ptr_inplace<torch::distributed::autograd::DistAutogradContext, std::allocator<torch::distributed::autograd::DistAutogradContext>, (__gnu_cxx::_Lock_policy)2>::_M_dispose() () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so\n#8  0x00007f5574c815e9 in std::__detail::_Hashtable_alloc<std::allocator<std::__detail::_Hash_node<std::pair<long const, std::shared_ptr<torch::distributed::autograd::DistAutogradContext> >, false> > >::_M_deallocate_node(std::__detail::_Hash_node<std::pair<long const, std::shared_ptr<torch::distributed::autograd::DistAutogradContext> >, false>*) [clone .isra.325] () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so\n#9  0x00007f5574c81bf1 in torch::distributed::autograd::DistAutogradContainer::eraseContextIdAndReset(torch::distributed::autograd::DistAutogradContainer::ContextsShard&, long) () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so\n#10 0x00007f5574c86e83 in torch::distributed::autograd::DistAutogradContainer::releaseContextIfPresent(long) () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so\n#11 0x00007f5574cc6395 in torch::distributed::rpc::RequestCallbackNoPython::processCleanupAutogradContextReq(torch::distributed::rpc::RpcCommandBase&) const () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so\n#12 0x00007f5574cccf15 in torch::distributed::rpc::RequestCallbackNoPython::processRpc(torch::distributed::rpc::RpcCommandBase&, torch::distributed::rpc::MessageType const&, std::vector<c10::Stream, std::allocator<c10::Stream> >) const () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so\n\nThread 72 (Thread 0x7f53077fe700 (LWP 205412)):\n#0  __lll_lock_wait () at ../sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:135\n#1  0x00007f55bc62adbd in __GI___pthread_mutex_lock (mutex=0x564884396440) at ../nptl/pthread_mutex_lock.c:80\n#2  0x00007f5574c82a2f in torch::distributed::autograd::DistAutogradContainer::retrieveContext(long) () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so\n#3  0x00007f557de9bb2f in pybind11::cpp_function::initialize<torch::distributed::autograd::(anonymous namespace)::dist_autograd_init(_object*, _object*)::{lambda(long)#11}, pybind11::dict, long, pybind11::name, pybind11::scope, pybind11::sibling, char [931], pybind11::arg>(torch::distributed::autograd::(anonymous namespace)::dist_autograd_init(_object*, _object*)::{lambda(long)#11}&&, pybind11::dict (*)(long), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, char const (&) [931], pybind11::arg const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call) () from /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so\n\n```\n\nBasically Thread 72, holds GIL and tries to acquire the lock for\nDistAutogradContainer to perform a lookup on a map. On the other hand,\nThread 79 holds the lock on DistAutogradContainer to remove a Tensor and as\npart of TensorImpl destructor, concrete_decref_fn is called which waits for\nGIL. As a result, we have a deadlock.\n\nTo fix this issue, I've ensured we release GIL when we call `retrieveContext`\nand acquire it later when needed.\nghstack-source-id: 133493659\n\nTest Plan: waitforbuildbot\n\nReviewed By: mrshenli\n\nDifferential Revision: D29682624\n\nfbshipit-source-id: f68a1fb39040ca0447a26e456a97bce64af6b79c",
    "Number of deleted lines": 3,
    "Deleted lines": "-      py::return_value_policy::reference);\n-        return torch::jit::toPyObject(IValue(autogradContext->getGradients()));\n-      py::arg(\"context_id\"));",
    "Added lines": "+      py::return_value_policy::reference,\n+      py::call_guard<py::gil_scoped_release>());\n+        auto ival = IValue(autogradContext->getGradients());\n+\n+        // Acquire GIL only for pyobject conversion.\n+        pybind11::gil_scoped_acquire ag;\n+        return torch::jit::toPyObject(ival);\n+      py::arg(\"context_id\"),\n+      py::call_guard<py::gil_scoped_release>());",
    "Label": "clean"
},
{
    "Id": 1104,
    "Library": "pytorch",
    "Date": "2021/07/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8e6d8991b21945754b71985f4fce30ce685779b9",
    "Root Cause": "N.A",
    "Bug report": "[torch/elastic] Fix the agent store key prefix used by workers (#61590)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61590\n\nThis PR fixes the bug where the state of the first run of a failed training job leaks to the secondary runs due to constant worker key prefix.\nghstack-source-id: 133494239\n\nTest Plan: Run the existing integ tests.\n\nReviewed By: SciPioneer\n\nDifferential Revision: D29682743\n\nfbshipit-source-id: d96ecadcfe5b6563225ee19f5d0776c7f935393a",
    "Number of deleted lines": 1,
    "Deleted lines": "-        worker_process_prefix = \"/worker\"",
    "Added lines": "+        attempt = os.environ[\"TORCHELASTIC_RESTART_COUNT\"]\n+        worker_process_prefix = f\"/worker/attempt_{attempt}\"",
    "Label": "clean"
},
{
    "Id": 1105,
    "Library": "pytorch",
    "Date": "2021/07/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ac5b9106001f5f197c2cc57183a1fc339ab30d38",
    "Root Cause": "N.A",
    "Bug report": "clang-tidy patch (#60714)\n\nSummary:\nTwo changes made here:\n1. Set `LANG=C.UTF-8` for clang-tidy so we can properly decode symbols in comment;\n2. In case of file removed, `end` could be null and we should skip the chunk/file;\n3. tiny bug fix for the loop indent.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60714\n\nReviewed By: iramazanli\n\nDifferential Revision: D29617171\n\nPulled By: 1ntEgr8\n\nfbshipit-source-id: b1603929333529a174105baf51e18246d09c012e",
    "Number of deleted lines": 2,
    "Deleted lines": "-            end = hunk[-1].target_line_no\n-        files[file.path].append((start, end))",
    "Added lines": "+            end = int(hunk[-1].target_line_no or 0)\n+            if end == 0:\n+                continue\n+            files[file.path].append((start, end))",
    "Label": "clean"
},
{
    "Id": 1106,
    "Library": "pytorch",
    "Date": "2021/07/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/376dc500a98fd6efd7d9478bae0ae618e464906f",
    "Root Cause": "N.A",
    "Bug report": "Minor bug fix in the warning message (#61127)\n\nSummary:\nThe current example code does not work. The correct one is like this: https://github.com/pytorch/pytorch/blob/cb7d813275a13a4233951e7cbcbb8351dbb0fd87/torch/distributed/run.py#L266\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61127\n\nReviewed By: cbalioglu\n\nDifferential Revision: D29572003\n\nPulled By: mrshenli\n\nfbshipit-source-id: 05b470230f3d70f8a6164edb5f92894a1112069f",
    "Number of deleted lines": 1,
    "Deleted lines": "-            \" Please read local_rank from `os.environ('LOCAL_RANK')` instead.\"",
    "Added lines": "+            \" Please read local_rank from `os.environ['LOCAL_RANK']` instead.\"",
    "Label": "clean"
},
{
    "Id": 1107,
    "Library": "pytorch",
    "Date": "2021/07/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/19b6ee4d4ed4cf9c8edfc7f9ab8358611b8723fa",
    "Root Cause": "N.A",
    "Bug report": "model_dump working with delegate models (#61043)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61043\n\nTrying to make model_dump work with delegate models\nghstack-source-id: 132809755\n\nTest Plan:\nN509022.\n\nThe data.pkl in the lowered model:\n```\nbash-3.2$ python -m torch.utils.show_pickle /Users/myuan/models/backend/lowered_model.pt@*/data.pkl\ntorch.jit.backend_with_compiler_demo.LoweredModule.__torch__.___torch_mangle_5.ModuleAdd()(state=\n (torch.jit._pickle.restore_type_tag({'forward': torch.jit._pickle.restore_type_tag({'input_shapes': '((1, 1, 320, 240), (1, 3))',\n                   'some_other_option': 'True'},\n                  'Dict[str, str]')},\n    'Dict[str, Any]'),\n  torch.jit._pickle.restore_type_tag({'forward': 'prim::Constant#1<debug_handle>271,aten::add<debug_handle>272'},\n    'Dict[str, str]'),\n  True))\n```\nComparing to data.pkl in scripted_model.pt:\n```\n__torch__.___torch_mangle_7.ModuleAdd()(state=\n {'_is_full_backward_hook': None, 'training': True})\n```\n\nReviewed By: Amyh11325\n\nDifferential Revision: D29464860\n\nfbshipit-source-id: d738e98ea518339465f8e3375207cf83e3dac532",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if typename.startswith(\"__torch__.\"):",
    "Added lines": "+        if typename.startswith(\"__torch__.\") or typename.startswith(\"torch.jit.LoweredModule.\"):",
    "Label": "clean"
},
{
    "Id": 1108,
    "Library": "pytorch",
    "Date": "2021/06/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c4f718cb723987b73157b833e7f515b4369d65b2",
    "Root Cause": "N.A",
    "Bug report": "[nnc] Serialize initialization of LLVM targets (#60996)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60996\n\nWe've had a bug report of weird LLVM initialization errors, e.g.,\n```\nUnexpected failure in LLVM JIT: Cannot choose between targets \"x86-64\" and \"x86-64\"\n```\n\nWhile I haven't repro'ed that exact message, I did run a stress-test that\ncompiles on many threads simultaneously, and it deadlocks in\nTargetRegistry::lookupTarget.  And in fact I remember debugging this before in\na different system, and finding \"Clients are responsible for avoid race\nconditions in registration\" in\nhttps://llvm.org/doxygen/TargetRegistry_8cpp_source.html.\n\nSo yeah, let's lock this thing.\nghstack-source-id: 132719018\n\nTest Plan: Heavy multithreaded compilation.  Not sure if it's suitable for landing.\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D29471343\n\nfbshipit-source-id: b495e468b57e77796a08b627884d3efeca2d1f7c",
    "Number of deleted lines": 4,
    "Deleted lines": "-  llvm::InitializeAllTargets();\n-  llvm::InitializeAllTargetMCs();\n-  llvm::InitializeAllAsmPrinters();\n-  jit_ = std::make_unique<llvm::orc::PytorchLLVMJIT>(triple, cpu, attrs);",
    "Added lines": "+namespace {\n+// Global mutex to protect LLVM initialization.  TargetRegistry::lookupTarget\n+// in particular is not thread-safe.\n+static std::mutex llvmInitMutex;\n+} // namespace\n+\n+  {\n+    std::lock_guard<std::mutex> g(llvmInitMutex);\n+    llvm::InitializeAllTargets();\n+    llvm::InitializeAllTargetMCs();\n+    llvm::InitializeAllAsmPrinters();\n+    jit_ = std::make_unique<llvm::orc::PytorchLLVMJIT>(triple, cpu, attrs);\n+  }",
    "Label": "clean"
},
{
    "Id": 1109,
    "Library": "pytorch",
    "Date": "2021/06/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/608f12b81827a204affd9cb6b5468b1639525b45",
    "Root Cause": "N.A",
    "Bug report": "Fix --dry-run option in tools/linter/clang_tidy.py (#60744)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60744\n\nFixes #60741\n\nTest Plan:\nRun this command:\n```\npython3 tools/linter/clang_tidy.py --paths torch/csrc/fx --dry-run\n```\nOutput:\n```\nclang-tidy -p build -config '{\"InheritParentConfig\": true, \"Checks\": \" bugprone-*, -bugprone-forward-declaration-namespace, -bugprone-macro-parentheses, -bugprone-lambda-function-name, -bugprone-reserved-identifier, cppcoreguidelines-*, -cppcoreguidelines-avoid-magic-numbers, -cppcoreguidelines-interfaces-global-init, -cppcoreguidelines-macro-usage, -cppcoreguidelines-owning-memory, -cppcoreguidelines-pro-bounds-array-to-pointer-decay, -cppcoreguidelines-pro-bounds-constant-array-index, -cppcoreguidelines-pro-bounds-pointer-arithmetic, -cppcoreguidelines-pro-type-cstyle-cast, -cppcoreguidelines-pro-type-reinterpret-cast, -cppcoreguidelines-pro-type-static-cast-downcast, -cppcoreguidelines-pro-type-union-access, -cppcoreguidelines-pro-type-vararg, -cppcoreguidelines-special-member-functions, -facebook-hte-RelativeInclude, hicpp-exception-baseclass, hicpp-avoid-goto, modernize-*, -modernize-concat-nested-namespaces, -modernize-return-braced-init-list, -modernize-use-auto, -modernize-use-default-member-init, -modernize-use-using, -modernize-use-trailing-return-type, performance-*, -performance-noexcept-move-constructor, -performance-unnecessary-value-param, \", \"HeaderFilterRegex\": \"torch/csrc/.*\", \"AnalyzeTemporaryDtors\": false, \"CheckOptions\": null}' torch/csrc/fx/fx_init.cpp\n```\n\nReviewed By: ngimel\n\nDifferential Revision: D29394538\n\nPulled By: 1ntEgr8\n\nfbshipit-source-id: b824bc2aa63631f074e9ad17092e4e063d347395",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if options.dry_run:\n+        print(clang_tidy_output)",
    "Label": "clean"
},
{
    "Id": 1110,
    "Library": "pytorch",
    "Date": "2021/06/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0baad214b07ad35be1f10100168ed761cc7c51c0",
    "Root Cause": "N.A",
    "Bug report": "[static runtime][fix] resize to the input tensor size for full_like (#60229)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60229\n\nFix bug where we did not resize to the input tensor size, causing\nthe output to be incorrect\n\nTest Plan:\nTest on replayer, rebased on D29217781, with model 278203319_26.\n\nVerify with jit outputs (D28583950)\n\n`./buck-out/gen/admarket/lib/ranking/prediction_replayer/replayer --model_inference_type_target=DISAGG_ACCELERATOR --prediction_replayer_force_model_type=inline_cvr_post_imp_model --prediction_replayer_force_model=278203319_26 --prediction_replayer_target_tier=sigrid.predictor.perf.dianshi_staticruntime_debug_0604.test --prediction_replayer_input_stream_filename=/data/users/ansha/tmp/adfinder/filtered_requests_inline_cvr_100 --ignore_model_id_mismatch --check_performance --fully_remote_sr_connection_options=\"overall_timeout:10000000,processing_timeout:10000000\" --use_new_encoding_for_ads_services --use_new_encoding_from_model_id_to_shard_id --sigrid_force_model_dir=/data/users/ansha/tmp/adfinder/278203319_26/ --sigrid_predictor_model_suffix=.predictor.disagg.local \u2014use_new_encoding_from_model_id_to_shard_id=true --prediction_replayer_force_model_kind=19 --pytorch_predictor_static_runtime_enable=true --prediction_replayer_target_qps=1`\n\nReviewed By: hlu1, movefast1990\n\nDifferential Revision: D29218918\n\nfbshipit-source-id: dab4bbbabeaa8367174ed90edca43d6204c65409",
    "Number of deleted lines": 1,
    "Deleted lines": "-      const auto& in0_t = p_node->Input(0).toTensor();",
    "Added lines": "+    const auto& in0_t = p_node->Input(0).toTensor();\n+    at::native::resize_(out_t, in0_t.sizes(), c10::nullopt);",
    "Label": "clean"
},
{
    "Id": 1111,
    "Library": "pytorch",
    "Date": "2021/06/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cbd1e8c3350bb48d590b296b0f7d9eea4ad945c6",
    "Root Cause": "N.A",
    "Bug report": "[Static Runtime] Fix bug in aten::to (#59995)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/59995\n\nReviewed By: ajyu\n\nDifferential Revision: D29083106\n\nfbshipit-source-id: 687ffb121af2716d606c145474942650a2d9ac7e",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      // in case that Output(0) is an alias of in0_t, copy the tensor.\n+      if (p_node->Output(0).toTensor().unsafeGetTensorImpl() ==\n+          in0_t.unsafeGetTensorImpl()) {\n+        p_node->Output(0) = in0_t.clone();\n+      }",
    "Label": "clean"
},
{
    "Id": 1112,
    "Library": "pytorch",
    "Date": "2021/06/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2693b0bef3ceda434971f99c1442727823e9a85f",
    "Root Cause": "N.A",
    "Bug report": "Fix compile error when debugging (#59616)\n\nSummary:\nSigned-off-by: caozhong <zhong.z.cao@intel.com>\n\nTriggered this probably because my full debug version python. ezyang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/59616\n\nReviewed By: jbschlosser\n\nDifferential Revision: D28958685\n\nPulled By: albanD\n\nfbshipit-source-id: fdab622c4d1be93eb27e9006dcf3db7c5b44a04b",
    "Number of deleted lines": 1,
    "Deleted lines": "-  _Py_AddToAllObjects(op, 1);",
    "Added lines": "+  _Py_AddToAllObjects(reinterpret_cast<PyObject *>(self), 1);",
    "Label": "clean"
},
{
    "Id": 1113,
    "Library": "pytorch",
    "Date": "2021/06/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f1786b293d4632efbd8c79cf87d2a047bdaeac15",
    "Root Cause": "N.A",
    "Bug report": "Revert D28972444: [pytorch][PR] Document debugability features in torch.distributed\n\nTest Plan: revert-hammer\n\nDifferential Revision:\nD28972444 (https://github.com/pytorch/pytorch/commit/a9d28108175e626c14bcb2da0b78d88992fa96e7)\n\nOriginal commit changeset: da5e8ee84f0d\n\nfbshipit-source-id: 94d3b3b75ddec74ea5b2b76f6a7519dc921ee2a7",
    "Number of deleted lines": 208,
    "Deleted lines": "-\n-Debugging ``torch.distributed`` applications\n-\n-Debugging distributed applications can be challenging due to hard to understand hangs, crashes, or inconsistent behavior across ranks. ``torch.distributed`` provides\n-a suite of tools to help debug training applications in a self-serve fashion:\n-\n-As of v1.10, :func:`torch.distributed.monitored_barrier` exists as an alternative to :func:`torch.distributed.barrier` which fails with helpful information about which rank may be faulty\n-when crashing, i.e. not all ranks calling into :func:`torch.distributed.monitored_barrier` within the provided timeout. :func:`torch.distributed.monitored_barrier` implements a host-side\n-barrier using ``send``/``recv`` communication primitives in a process similar to acknowledgements, allowing rank 0 to report which rank(s) failed to acknowledge\n-the barrier in time. As an example, consider the following function where rank 1 fails to call into :func:`torch.distributed.monitored_barrier` (in practice this could be due\n-to an application bug or hang in a previous collective):\n-\n-::\n-\n-    import os\n-    from datetime import timedelta\n-\n-    import torch\n-    import torch.distributed as dist\n-    import torch.multiprocessing as mp\n-\n-\n-    def worker(rank):\n-        dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n-        # monitored barrier requires gloo process group to perform host-side sync.\n-        group_gloo = dist.new_group(backend=\"gloo\")\n-        if rank not in [1]:\n-            dist.monitored_barrier(group=group_gloo, timeout=timedelta(seconds=2))\n-\n-\n-    if __name__ == \"__main__\":\n-        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n-        os.environ[\"MASTER_PORT\"] = \"29501\"\n-        mp.spawn(worker, nprocs=2, args=())\n-\n-The following error message is produced on rank 0, allowing the user to determine which rank(s) may be faulty and investigate further:\n-\n-::\n-\n-  RuntimeError: Rank 1 failed to pass monitoredBarrier in 2000 ms\n-   Original exception:\n-  [gloo/transport/tcp/pair.cc:598] Connection closed by peer [2401:db00:eef0:1100:3560:0:1c05:25d]:8594\n-\n-\n-Next, the environment variable ``TORCH_DISTRIBUTED_DEBUG``  can be used to trigger additional useful logging and collective synchronization checks to ensure all ranks\n-are synchronized appropriately. ``TORCH_DISTRIBUTED_DEBUG`` can be set to either ``OFF`` (default), ``INFO``, or ``DETAIL`` depending on the debugging level\n-required. Please note that the most verbose option, ``DETAIL`` may impact the application performance and thus should only be used when debugging issues.\n-\n-Setting ``TORCH_DISTRIBUTED_DEBUG=INFO`` will result in additional debug logging when models trained with :func:`torch.nn.parallel.DistributedDataParallel` are initialized, and\n-``TORCH_DISTRIBUTED_DEBUG=DETAIL`` will additionally log runtime performance statistics a select number of iterations. These runtime statistics\n-include data such as forward time, backward time, gradient communication time, etc. As an example, given the following application:\n-\n-::\n-\n-    import os\n-\n-    import torch\n-    import torch.distributed as dist\n-    import torch.multiprocessing as mp\n-\n-\n-    class TwoLinLayerNet(torch.nn.Module):\n-        def __init__(self):\n-            super().__init__()\n-            self.a = torch.nn.Linear(10, 10, bias=False)\n-            self.b = torch.nn.Linear(10, 1, bias=False)\n-\n-        def forward(self, x):\n-            a = self.a(x)\n-            b = self.b(x)\n-            return (a, b)\n-\n-\n-    def worker(rank):\n-        dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n-        torch.cuda.set_device(rank)\n-        print(\"init model\")\n-        model = TwoLinLayerNet().cuda()\n-        print(\"init ddp\")\n-        ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n-\n-        inp = torch.randn(10, 10).cuda()\n-        print(\"train\")\n-\n-        for _ in range(20):\n-            output = ddp_model(inp)\n-            loss = output[0] + output[1]\n-            loss.sum().backward()\n-\n-\n-    if __name__ == \"__main__\":\n-        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n-        os.environ[\"MASTER_PORT\"] = \"29501\"\n-        os.environ[\n-            \"TORCH_DISTRIBUTED_DEBUG\"\n-        ] = \"DETAIL\"  # set to DETAIL for runtime logging.\n-        mp.spawn(worker, nprocs=2, args=())\n-\n-The following logs are rendered at initialization time:\n-\n-::\n-\n-  I0607 16:10:35.739390 515217 logger.cpp:173] [Rank 0]: DDP Initialized with:\n-  broadcast_buffers: 1\n-  bucket_cap_bytes: 26214400\n-  find_unused_parameters: 0\n-  gradient_as_bucket_view: 0\n-  is_multi_device_module: 0\n-  iteration: 0\n-  num_parameter_tensors: 2\n-  output_device: 0\n-  rank: 0\n-  total_parameter_size_bytes: 440\n-  world_size: 2\n-  backend_name: nccl\n-  bucket_sizes: 440\n-  cuda_visible_devices: N/A\n-  device_ids: 0\n-  dtypes: float\n-  master_addr: localhost\n-  master_port: 29501\n-  module_name: TwoLinLayerNet\n-  nccl_async_error_handling: N/A\n-  nccl_blocking_wait: N/A\n-  nccl_debug: WARN\n-  nccl_ib_timeout: N/A\n-  nccl_nthreads: N/A\n-  nccl_socket_ifname: N/A\n-  torch_distributed_debug: INFO\n-\n-\n-The following logs are rendered during runtime (when ``TORCH_DISTRIBUTED_DEBUG=DETAIL`` is set):\n-\n-::\n-\n-  I0607 16:18:58.085681 544067 logger.cpp:344] [Rank 1 / 2] Training TwoLinLayerNet unused_parameter_size=0\n-   Avg forward compute time: 40838608\n-   Avg backward compute time: 5983335\n-  Avg backward comm. time: 4326421\n-   Avg backward comm/comp overlap time: 4207652\n-  I0607 16:18:58.085693 544066 logger.cpp:344] [Rank 0 / 2] Training TwoLinLayerNet unused_parameter_size=0\n-   Avg forward compute time: 42850427\n-   Avg backward compute time: 3885553\n-  Avg backward comm. time: 2357981\n-   Avg backward comm/comp overlap time: 2234674\n-\n-\n-In addition, ``TORCH_DISTRIBUTED_DEBUG=INFO`` enhances crash logging in :func:`torch.nn.parallel.DistributedDataParallel` due to unused parameters in the model. Currently, ``find_unused_parameters=True``\n-must be passed into :func:`torch.nn.parallel.DistributedDataParallel` initialization if there are parameters that may be unused in the forward pass, and as of v1.10, all model outputs are required\n-to be used in loss computation as :func:`torch.nn.parallel.DistributedDataParallel` does not support unused parameters in the backwards pass. These constraints are challenging especially for larger\n-models, thus when crashing with an error, :func:`torch.nn.parallel.DistributedDataParallel` will log the fully qualified name of all parameters that went unused. For example, in the above application,\n-if we modify ``loss`` to be instead computed as ``loss = output[1]``, then ``TwoLinLayerNet.a`` does not receive a gradient in the backwards pass, and\n-thus results in ``DDP`` failing. On a crash, the user is passed information about parameters which went unused, which may be challenging to manually find for large models:\n-\n-\n-::\n-\n-  RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing\n-   the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by\n-  making sure all `forward` function outputs participate in calculating loss.\n-  If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return va\n-  lue of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n-  Parameters which did not receive grad for rank 0: a.weight\n-  Parameter indices which did not receive grad for rank 0: 0\n-\n-\n-Setting ``TORCH_DISTRIBUTED_DEBUG=DETAIL`` will trigger additional consistency and synchronization checks on every collective call issued by the user\n-either directly or indirectly (such as DDP ``allreduce``). This is done by creating a wrapper process group that wraps all process groups returned by\n-:func:`torch.distributed.init_process_group` and :func:`torch.distributed.new_group` APIs. As a result, these APIs will return a wrapper process group that can be used exactly like a regular process\n-group, but performs consistency checks before dispatching the collective to an underlying process group. Currently, these checks include a :func:`torch.distributed.monitored_barrier`,\n-which ensures all ranks complete their outstanding collective calls and reports ranks which are stuck. Next, the collective itself is checked for consistency by\n-ensuring all collective functions match and are called with consistent tensor shapes. If this is not the case, a detailed error report is included when the\n-application crashes, rather than a hang or uninformative error message. As an example, consider the following function which has mismatched input shapes into\n-:func:`torch.distributed.all_reduce`:\n-\n-::\n-\n-    import torch\n-    import torch.distributed as dist\n-    import torch.multiprocessing as mp\n-\n-\n-    def worker(rank):\n-        dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n-        torch.cuda.set_device(rank)\n-        tensor = torch.randn(10 if rank == 0 else 20).cuda()\n-        dist.all_reduce(tensor)\n-        torch.cuda.synchronize(device=rank)\n-\n-\n-    if __name__ == \"__main__\":\n-        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n-        os.environ[\"MASTER_PORT\"] = \"29501\"\n-        os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n-        mp.spawn(worker, nprocs=2, args=())\n-\n-With the ``NCCL`` backend, such an application would likely result in a hang which can be challenging to root-cause in nontrivial scenarios. If the user enables\n-``TORCH_DISTRIBUTED_DEBUG=DETAIL`` and reruns the application, the following error message reveals the root cause:\n-\n-::\n-\n-    work = default_pg.allreduce([tensor], opts)\n-    RuntimeError: Error when verifying shape tensors for collective ALLREDUCE on rank 0. This likely indicates that input shapes into the collective are mismatched across ranks. Got shapes:  10\n-    [ torch.LongTensor{1} ]\n-\n-In addition, `TORCH_DISTRIBUTED_DEBUG=DETAIL` can be used in conjunction with `TORCH_SHOW_CPP_STACKTRACES=1` to log the entire callstack when a collective desynchronization is detected. These\n-collective desynchronization checks will work for all applications that use ``c10d`` collective calls backed by process groups created with the\n- :func:`torch.distributed.init_process_group` and :func:`torch.distributed.new_group` APIs.",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1114,
    "Library": "pytorch",
    "Date": "2021/06/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0b6ec320049ad5b132c15498cf148421812cfbee",
    "Root Cause": "N.A",
    "Bug report": "Reland: \"[TensorExpr] Improve debug messages.\" (#59506)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/59506\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D28918343\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 168703f6368f5182cf9762600d7f0f6ea5b20280",
    "Number of deleted lines": 10,
    "Deleted lines": "-    GRAPH_DEBUG(\"only float32 allowed\");\n-    GRAPH_DEBUG(\"inputs are the wrong size\");\n-    GRAPH_DEBUG(\"not depthwise\");\n-    GRAPH_DEBUG(\"not 3x3\");\n-    GRAPH_DEBUG(\"unsupported stride\");\n-    GRAPH_DEBUG(\"unsupported pad\");\n-    GRAPH_DEBUG(\"unsupported dilation\");\n-      throw std::runtime_error(\"Unhandled node kind\");\n-          std::string(\"Unhandled node kind: \") + op.toQualString();\n-      std::string msg = std::string(\"Unhandled node kind: \") +",
    "Added lines": "+    GRAPH_DEBUG(\"conv2dIsSupported: only float32 allowed\");\n+    GRAPH_DEBUG(\"conv2dIsSupported: inputs are the wrong size\");\n+    GRAPH_DEBUG(\"conv2dIsSupported: not depthwise\");\n+    GRAPH_DEBUG(\"conv2dIsSupported: not 3x3\");\n+    GRAPH_DEBUG(\"conv2dIsSupported: unsupported stride\");\n+    GRAPH_DEBUG(\"conv2dIsSupported: unsupported pad\");\n+    GRAPH_DEBUG(\"conv2dIsSupported: unsupported dilation\");\n+      std::string msg =\n+          std::string(\"Unhandled node kind (in inferSizesForValue): \") +\n+          v->node()->kind().toQualString();\n+      throw malformed_input(msg);\n+          std::string(\"Unhandled node kind (in computeOperandValue): \") +\n+          op.toQualString();\n+      std::string msg = std::string(\"Unhandled node kind (in computeValue): \") +",
    "Label": "clean"
},
{
    "Id": 1115,
    "Library": "pytorch",
    "Date": "2021/06/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/528d82d6a6e368105e58ecf434ae73bf1e199017",
    "Root Cause": "N.A",
    "Bug report": "[torch] Add debug name to assert message for useOf\n\nSummary:\nMake an assert message in Pytorch's JIT provide better information by\nprinting the debug name of a value in `PythonPrintImpl::useOf` if it's not\nfound in any tables.\n\nTest Plan:\nTested printing a `module.code` where the module had an invalid value used\nas an operand. Before it asserted without any more details, afterwards it\nprinted the debug name which made it easy to track down the offending value.\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D28856026\n\nfbshipit-source-id: 479f66c458a0a2d9a161ade09f20382e7b19d60e",
    "Number of deleted lines": 2,
    "Deleted lines": "-        \"Value was not present in either expressions\"\n-        \" table or ident refs table\");",
    "Added lines": "+        \"Value (debug name: \\\"\",\n+        v->debugName(),\n+        \"\\\") was not present in either expressions table or ident refs table\");",
    "Label": "clean"
},
{
    "Id": 1116,
    "Library": "pytorch",
    "Date": "2021/06/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/88fb5ee84c5f16d7580bac3501646da3fd10ad08",
    "Root Cause": "N.A",
    "Bug report": "Revert D28819779: [TensorExpr] Improve debug messages.\n\nTest Plan: revert-hammer\n\nDifferential Revision:\nD28819779\n\nOriginal commit changeset: 2eaa0b78fb30\n\nfbshipit-source-id: babc22f75d87b1ba25f78ffe59266560413778ce",
    "Number of deleted lines": 14,
    "Deleted lines": "-    GRAPH_DEBUG(\"conv2dIsSupported: only float32 allowed\");\n-    GRAPH_DEBUG(\"conv2dIsSupported: inputs are the wrong size\");\n-    GRAPH_DEBUG(\"conv2dIsSupported: not depthwise\");\n-    GRAPH_DEBUG(\"conv2dIsSupported: not 3x3\");\n-    GRAPH_DEBUG(\"conv2dIsSupported: unsupported stride\");\n-    GRAPH_DEBUG(\"conv2dIsSupported: unsupported pad\");\n-    GRAPH_DEBUG(\"conv2dIsSupported: unsupported dilation\");\n-      std::string msg =\n-          std::string(\"Unhandled node kind (in inferSizesForValue): \") +\n-          v->node()->kind().toQualString();\n-      throw malformed_input(msg);\n-          std::string(\"Unhandled node kind (in computeOperandValue): \") +\n-          op.toQualString();\n-      std::string msg = std::string(\"Unhandled node kind (in computeValue): \") +",
    "Added lines": "+    GRAPH_DEBUG(\"only float32 allowed\");\n+    GRAPH_DEBUG(\"inputs are the wrong size\");\n+    GRAPH_DEBUG(\"not depthwise\");\n+    GRAPH_DEBUG(\"not 3x3\");\n+    GRAPH_DEBUG(\"unsupported stride\");\n+    GRAPH_DEBUG(\"unsupported pad\");\n+    GRAPH_DEBUG(\"unsupported dilation\");\n+      throw std::runtime_error(\"Unhandled node kind\");\n+          std::string(\"Unhandled node kind: \") + op.toQualString();\n+      std::string msg = std::string(\"Unhandled node kind: \") +",
    "Label": "clean"
},
{
    "Id": 1117,
    "Library": "pytorch",
    "Date": "2021/06/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/285b8a52520ab056e2eeb139066f9c23db3301d2",
    "Root Cause": "N.A",
    "Bug report": "[TensorExpr] Improve debug messages. (#59280)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/59280\n\nDifferential Revision:\nD28819779\nD28819779\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 2eaa0b78fb309cccb0efe9025a5c3b039e717027",
    "Number of deleted lines": 10,
    "Deleted lines": "-    GRAPH_DEBUG(\"only float32 allowed\");\n-    GRAPH_DEBUG(\"inputs are the wrong size\");\n-    GRAPH_DEBUG(\"not depthwise\");\n-    GRAPH_DEBUG(\"not 3x3\");\n-    GRAPH_DEBUG(\"unsupported stride\");\n-    GRAPH_DEBUG(\"unsupported pad\");\n-    GRAPH_DEBUG(\"unsupported dilation\");\n-      throw std::runtime_error(\"Unhandled node kind\");\n-          std::string(\"Unhandled node kind: \") + op.toQualString();\n-      std::string msg = std::string(\"Unhandled node kind: \") +",
    "Added lines": "+    GRAPH_DEBUG(\"conv2dIsSupported: only float32 allowed\");\n+    GRAPH_DEBUG(\"conv2dIsSupported: inputs are the wrong size\");\n+    GRAPH_DEBUG(\"conv2dIsSupported: not depthwise\");\n+    GRAPH_DEBUG(\"conv2dIsSupported: not 3x3\");\n+    GRAPH_DEBUG(\"conv2dIsSupported: unsupported stride\");\n+    GRAPH_DEBUG(\"conv2dIsSupported: unsupported pad\");\n+    GRAPH_DEBUG(\"conv2dIsSupported: unsupported dilation\");\n+      std::string msg =\n+          std::string(\"Unhandled node kind (in inferSizesForValue): \") +\n+          v->node()->kind().toQualString();\n+      throw malformed_input(msg);\n+          std::string(\"Unhandled node kind (in computeOperandValue): \") +\n+          op.toQualString();\n+      std::string msg = std::string(\"Unhandled node kind (in computeValue): \") +",
    "Label": "clean"
},
{
    "Id": 1118,
    "Library": "pytorch",
    "Date": "2021/06/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6627c00e63afbe0b4965afa3164228e2d6e58b11",
    "Root Cause": "N.A",
    "Bug report": "[Static Runtime] Fix bug in quantized::linear wrapper (#59407)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/59407\n\nReviewed By: ajyu\n\nDifferential Revision: D28881307\n\nfbshipit-source-id: 46c169f783cf05c585871c2e074d52255116b9c3",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if (!packed_weight) {",
    "Added lines": "+        if (packed_weight) {",
    "Label": "clean"
},
{
    "Id": 1119,
    "Library": "pytorch",
    "Date": "2021/06/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/258a9910277a406d0f147ac45eb9e71146a81e9e",
    "Root Cause": "N.A",
    "Bug report": "[reland] Set and propagate devices in RRef completion future (#59211)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/59211\n\nReland of https://github.com/pytorch/pytorch/pull/58674\n\nI found this missing parameter while debugging failures in the next PR.\nI'm very unhappy about this change. I think this future, which we know for sure won't contain tensors, shouldn't have to worry about CUDA devices. And yet, it does. This means that basically any future anywhere might have to worry about it, and this just doesn't scale, and thus it's bad.\nghstack-source-id: 130202843\n\nTest Plan: Should fix the next diff.\n\nReviewed By: mrshenli\n\nDifferential Revision: D28623886\n\nfbshipit-source-id: 6c82ed7c785ac3bf32fff7eec67cdd73b96aff28",
    "Number of deleted lines": 9,
    "Deleted lines": "-      // Also no need to specify any devices because the RRef object itself\n-      // doesn't contain any DataPtrs, it just provides means to retrieve them.\n-      auto futureOwner =\n-          c10::make_intrusive<JitFuture>(RRefType::create(c10::AnyType::get()));\n-    // No need to specify any devices because the RRef object itself doesn't\n-    // contain any DataPtrs, it just provides means to retrieve them.\n-    auto futureOwner =\n-        c10::make_intrusive<JitFuture>(RRefType::create(owner->type()));\n-  auto jitFuturePtr = c10::make_intrusive<JitFuture>(BoolType::get());",
    "Added lines": "+      // We need to set devices here, even if they won't be used by the value\n+      // (an RRef object doesn't contain any tensors, it just provides means to\n+      // retrieve them) because we need them to be propagated/ to child futures.\n+      // This is silly and we should find a way to avoid this.\n+      auto futureOwner = c10::make_intrusive<JitFuture>(\n+          RRefType::create(c10::AnyType::get()), agent_->getDevices());\n+    // We need to set devices here, even if they won't be used by the value (an\n+    // RRef object doesn't contain any tensors, it just provides means to\n+    // retrieve them) because we need them to be propagated/ to child futures.\n+    // This is silly and we should find a way to avoid this.\n+    auto futureOwner = c10::make_intrusive<JitFuture>(\n+        RRefType::create(owner->type()), agent_->getDevices());\n+  // We need to set devices here, even if they won't be used by the value (it's\n+  // a bool, it doesn't contain tensors!) because we need them to be propagated\n+  // to child futures. This is silly and we should find a way to avoid this.\n+  auto jitFuturePtr =\n+      c10::make_intrusive<JitFuture>(BoolType::get(), agent_->getDevices());",
    "Label": "clean"
},
{
    "Id": 1120,
    "Library": "pytorch",
    "Date": "2021/05/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ab372ba51087c5aecdc301cef21946505a15b250",
    "Root Cause": "N.A",
    "Bug report": "[iOS GPU] Add debug information to track memory allocation exception (#59112)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/59112\n\nghstack-source-id: 130027273\n\nTest Plan: CI\n\nReviewed By: linbinyu\n\nDifferential Revision: D28730604\n\nfbshipit-source-id: 2ec7ca1b722a9fe496635cb6eea7e0d88b0c18b1",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    TORCH_CHECK(_buffer, \"Allocate GPU memory failed!\");",
    "Label": "clean"
},
{
    "Id": 1121,
    "Library": "pytorch",
    "Date": "2021/05/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/26c1f0f72e71c096648a16993484234399da307c",
    "Root Cause": "N.A",
    "Bug report": "[skip ci] Skip debug info on PRs (#58897)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58897\n\nWe don't need to be building debug info on PRs since it's just filling up S3/CircleCI storage with useless 800 MB zips, this flips it so it's only run on master + release branches. See #58898 for CI signal\n\nAlso see pytorch/builder counterpart (unlike the last debuginfo PR there is no hard dependency between these two so there won't be any churn on un-rebased PRs): https://github.com/pytorch/builder/pull/778\n\nTest Plan: Imported from OSS\n\nReviewed By: seemethere, samestep\n\nDifferential Revision: D28689413\n\nPulled By: driazati\n\nfbshipit-source-id: 77a37e84afe492215008d5e023ceab0c24adb33c",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+if [[ \"$CIRCLE_BRANCH\" == \"master\" ]] || [[ \"$CIRCLE_BRANCH\" == release/* ]]; then\n+  export BUILD_DEBUG_INFO=1\n+fi\n+",
    "Label": "clean"
},
{
    "Id": 1122,
    "Library": "pytorch",
    "Date": "2021/05/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/796c97a88fedb90f6e1932b68003c6a30d00ae85",
    "Root Cause": "N.A",
    "Bug report": "[Pytorch Delegated Backend] Add python binding for (#57156)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57156\n\ngenerate_debug_handles\n\nTo be able to generate debug handles for preprocess written inpython.\n\nTest Plan:\nCI\n\nCI\n\nImported from OSS\n\nDifferential Revision:\nD28062328\nD28062328\n\nReviewed By: raziel\n\nPulled By: kimishpatel\n\nfbshipit-source-id: 8795d089edc00a292a2221cfe80bbc671468055c",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include <torch/csrc/jit/backends/generate_debug_handles.h>\n+\n+  m.def(\n+      \"_jit_backend_generate_debug_handles\", [](std::shared_ptr<Graph>& graph) {\n+        return generate_debug_handles(graph);\n+      });",
    "Label": "clean"
},
{
    "Id": 1123,
    "Library": "pytorch",
    "Date": "2021/05/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b842351b4fcce7eb088b58dfbcb86f54119f57f3",
    "Root Cause": "N.A",
    "Bug report": "Skip SVE acceleration on M1 (#58785)\n\nSummary:\nAs it's not supported by the chip and also crashes compiler, see https://bugs.llvm.org/show_bug.cgi?id=50407\n\nFixes https://github.com/pytorch/pytorch/issues/58653\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58785\n\nReviewed By: zhouzhuojie, driazati\n\nDifferential Revision: D28619231\n\nPulled By: malfet\n\nfbshipit-source-id: 34367c074f9624b21d239eec757891cbb51f5bed",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if(CMAKE_SYSTEM_PROCESSOR STREQUAL \"arm64\" AND CMAKE_SYSTEM_NAME STREQUAL \"Darwin\")\n+      set(DISABLE_SVE ON CACHE BOOL \"Xcode's clang-12.5 crashes while trying to compile SVE code\" FORCE)\n+    endif()",
    "Label": "clean"
},
{
    "Id": 1124,
    "Library": "pytorch",
    "Date": "2021/05/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/23df70359a3fb4d1325265fff6c0e6285f3a9fda",
    "Root Cause": "N.A",
    "Bug report": "Set and propagate devices in RRef completion future (#58674)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58674\n\nI found this missing parameter while debugging failures in the next PR.\n\nI'm very unhappy about this change. I think this future, which we know for sure won't contain tensors, shouldn't have to worry about CUDA devices. And yet, it does. This means that basically any future anywhere might have to worry about it, and this just doesn't scale, and thus it's bad.\nghstack-source-id: 129567042\n\nTest Plan: Should fix the next diff.\n\nReviewed By: mrshenli\n\nDifferential Revision: D28574083\n\nfbshipit-source-id: 5c89902cdc5cc12f1ebeea860b90cd9c3d7c7da1",
    "Number of deleted lines": 9,
    "Deleted lines": "-      // Also no need to specify any devices because the RRef object itself\n-      // doesn't contain any DataPtrs, it just provides means to retrieve them.\n-      auto futureOwner =\n-          c10::make_intrusive<JitFuture>(RRefType::create(c10::AnyType::get()));\n-    // No need to specify any devices because the RRef object itself doesn't\n-    // contain any DataPtrs, it just provides means to retrieve them.\n-    auto futureOwner =\n-        c10::make_intrusive<JitFuture>(RRefType::create(owner->type()));\n-  auto jitFuturePtr = c10::make_intrusive<JitFuture>(BoolType::get());",
    "Added lines": "+      // We need to set devices here, even if they won't be used by the value\n+      // (an RRef object doesn't contain any tensors, it just provides means to\n+      // retrieve them) because we need them to be propagated/ to child futures.\n+      // This is silly and we should find a way to avoid this.\n+      auto futureOwner = c10::make_intrusive<JitFuture>(\n+          RRefType::create(c10::AnyType::get()), agent_->getDevices());\n+    // We need to set devices here, even if they won't be used by the value (an\n+    // RRef object doesn't contain any tensors, it just provides means to\n+    // retrieve them) because we need them to be propagated/ to child futures.\n+    // This is silly and we should find a way to avoid this.\n+    auto futureOwner = c10::make_intrusive<JitFuture>(\n+        RRefType::create(owner->type()), agent_->getDevices());\n+  // We need to set devices here, even if they won't be used by the value (it's\n+  // a bool, it doesn't contain tensors!) because we need them to be propagated\n+  // to child futures. This is silly and we should find a way to avoid this.\n+  auto jitFuturePtr =\n+      c10::make_intrusive<JitFuture>(BoolType::get(), agent_->getDevices());",
    "Label": "clean"
},
{
    "Id": 1125,
    "Library": "pytorch",
    "Date": "2021/05/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b9b8522e00370d579fc3ba9febf540af2d70ba4e",
    "Root Cause": "N.A",
    "Bug report": "[profile] fix recorded data type (#58531)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58531\n\nfix data type of alltoall(v) when recording communication metadata via DebugInfo in NCCL PG\n\nReviewed By: chaekit\n\nDifferential Revision: D28529372\n\nfbshipit-source-id: 2917653f73f5fe4f6dc901803235994ca042bba2",
    "Number of deleted lines": 8,
    "Deleted lines": "-        at::kByte,                // dType\n-        rank_,                 // rank\n-        \"all_to_allv\",         // colName\n-        inputTensor.numel(),   // inSize\n-        outputTensor.numel(),  // outSize\n-        at::kByte,             // dType\n-        inputSplitSizes,       // inSplitSizes\n-        outputSplitSizes);     // outSplitSizes",
    "Added lines": "+        inputTensor.scalar_type(),// dType\n+        rank_,                    // rank\n+        \"all_to_allv\",            // colName\n+        inputTensor.numel(),      // inSize\n+        outputTensor.numel(),     // outSize\n+        inputTensor.scalar_type(),// dType\n+        inputSplitSizes,          // inSplitSizes\n+        outputSplitSizes);        // outSplitSizes",
    "Label": "clean"
},
{
    "Id": 1126,
    "Library": "pytorch",
    "Date": "2021/05/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a91375432a8c1e0a25c933fc2567f9d6dc4759ef",
    "Root Cause": "N.A",
    "Bug report": "model_dump: Accept variable-length debug info (#57660)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57660\n\nIgnore trailing elements so we're compatible with both old and new\nmodels.\n\nTest Plan: Dumped and old model.  Unit test.\n\nReviewed By: malfet\n\nDifferential Revision: D28531391\n\nPulled By: dreiss\n\nfbshipit-source-id: 197a55ab0e6a7d8e25cbee83852e194afacc988e",
    "Number of deleted lines": 2,
    "Deleted lines": "-                # accounting for source range serialization format change\n-                start, source_range, _ = di",
    "Added lines": "+                start, source_range, *_ = di",
    "Label": "clean"
},
{
    "Id": 1127,
    "Library": "pytorch",
    "Date": "2021/05/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c76405d3b1685a4ae0c3fd76d8d69199b340f83a",
    "Root Cause": "N.A",
    "Bug report": "[nnc] Enable CPU fusion inside Facebook, take 2 (#58347)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58347\n\nBack out \"Revert D27652484 (https://github.com/pytorch/pytorch/commit/ac04cc775bc1cc7e85e7bbc8a54744720fe7c068): [nnc] Enable CPU fusion inside Facebook\"\nOriginal commit changeset: ecfef3ee1e71\nghstack-source-id: 129279584\n\nTest Plan: Tests for bugfix included in this stack\n\nReviewed By: navahgar\n\nDifferential Revision: D28461013\n\nfbshipit-source-id: 79a80b6ffb653ab952ff5efaa143d3362bb7d966",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#ifdef FBCODE_CAFFE2\n+// NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n+C10_DEFINE_bool(torch_jit_enable_cpu_fusion, true, \"enable cpu fusion\");\n+#else\n+#endif\n+  FLAGS_torch_jit_enable_cpu_fusion = value;",
    "Label": "clean"
},
{
    "Id": 1128,
    "Library": "pytorch",
    "Date": "2021/05/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fd3d3ef900877842b64127b17feb508b9c0b908f",
    "Root Cause": "N.A",
    "Bug report": "[RPC Framework] Add _script_module_reducer unconditionally for RecursiveScriptModule in RPC pickler (#58020)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58020\n\nPreviously there is no RPC pickler for `RecursiveScriptModule`. Although it is a subclass of `ScriptModule`, the reducer of `ScriptModule` is not triggered for `RecursiveScriptModule` when a script remote module is sent over RPC.\n\nThis PR checkpoints the investigation of #58274, which makes sure that a RPC pickler is invoked here. This still cannot fix `test_send_remote_module_over_the_wire_script`. Will revisit this bug once there is a feature request from users.\n\nghstack-source-id: 128949642\n\nTest Plan:\nTODO: re-enable these tests\n\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_send_remote_module_over_the_wire_script\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_remote_module_py_pickle_not_supported_script\n\nReviewed By: rohan-varma\n\nDifferential Revision: D28346758\n\nfbshipit-source-id: 3cff84ca665da03da6ed6acb094a1f594fcd945e",
    "Number of deleted lines": 6,
    "Deleted lines": "-        # Install customized picklers.\n-        for class_name in self._class_reducer_dict.keys():\n-            p.dispatch_table[class_name] = self._class_reducer_dict[class_name]  # type: ignore[index]\n-\n-        # Add dispatch pickling for ScriptModule if needed.\n-        Deserilize binary string + tensor table to original obj",
    "Added lines": "+        # Add dispatch pickling for ScriptModule or its subclass.\n+        # TODO(58274): This reducer will be triggered when a script RemoteModule is sent over RPC.\n+        # Although `RecursiveScriptModule` is a subclass of `ScriptModule`, the above line somehow\n+        # cannot trigger the reducer.\n+        # Ignore type error because dispatch_table is defined in third-party package\n+        p.dispatch_table[torch.jit.RecursiveScriptModule] = self._script_module_reducer  # type: ignore[index]\n+\n+        # Install customized picklers.\n+        for class_name in self._class_reducer_dict.keys():\n+            p.dispatch_table[class_name] = self._class_reducer_dict[class_name]  # type: ignore[index]\n+\n+        Deserialize binary string + tensor table to original obj",
    "Label": "clean"
},
{
    "Id": 1129,
    "Library": "pytorch",
    "Date": "2021/05/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc55ab3f77918ca424446c92cac8604aad049e15",
    "Root Cause": "N.A",
    "Bug report": "[fbgemm] fix bug handling bias in rowwise quantization of FC (#58022)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58022\n\nCaffe2 Int8FC + rowwise quantization was not handling bias correctly.\n\nTest Plan: The example in D28347336 doesn't show bigger error with rowwise quantization any more\n\nReviewed By: hx89, janeyx99\n\nDifferential Revision: D28347336\n\nfbshipit-source-id: 3ac95fd2f29ef6e52705c3a2361b605813c2bcc5",
    "Number of deleted lines": 2,
    "Deleted lines": "-                b_quantized_data_[j], in_qparams_[2]);\n-                in_qparams_[0].scale * filter_qparams_[0].scale,",
    "Added lines": "+                b_quantized_data_[j],\n+                filter_qparams_[quantize_channelwise_ ? j : 0]);\n+                in_qparams_[0].scale *\n+                    filter_qparams_[quantize_channelwise_ ? j : 0].scale,",
    "Label": "clean"
},
{
    "Id": 1130,
    "Library": "pytorch",
    "Date": "2021/05/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e90fcffb659ca6eab1667751b8d2b0a5e936e3f6",
    "Root Cause": "N.A",
    "Bug report": "[c10d] Log when store based barrier succeeds (#57711)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57711\n\nSeeing some hangs/issues around store based barrier internally, would\nbe good to have this log to indicate whether store based barrier has completed\nsuccessfully or not for a particular rank to debug further.\nghstack-source-id: 128605600\n\nTest Plan: CI\n\nReviewed By: SciPioneer\n\nDifferential Revision: D28249087\n\nfbshipit-source-id: 644e5780519017ae780c3bc78bbe5def322db3f8",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    logger.info(\n+        f\"Rank {rank}: Completed store-based barrier for {world_size} nodes.\"\n+    )\n+",
    "Label": "clean"
},
{
    "Id": 1131,
    "Library": "pytorch",
    "Date": "2021/05/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ca8090f81bf41db1351e3b3114ff5699500438ae",
    "Root Cause": "N.A",
    "Bug report": "[Pytorch Edge] Enable eager symbolication in benchmarking binary (#57705)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57705\n\nThis will enable module level debug info for benchmarking binary.\n\nTest Plan: Run on AIBench\n\nReviewed By: larryliu0820\n\nDifferential Revision: D28230948\n\nfbshipit-source-id: 5d06c6853d049ff678995a2ed4a86f4e6c85bdc7",
    "Number of deleted lines": 2,
    "Deleted lines": "-      source_range_it != source_range_map.end(),\n-      \"Source range tag must exist in deserialized source range map.\");",
    "Added lines": "+      source_range_tag == -1 || source_range_it != source_range_map.end(),\n+      \"Source range tag must exist in deserialized source range map.\"\n+      \" Not found source range tag:\",\n+      source_range_tag);",
    "Label": "clean"
},
{
    "Id": 1132,
    "Library": "pytorch",
    "Date": "2021/05/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3cc733e45112848cb3660e0b8d0243a6a6f4b498",
    "Root Cause": "N.A",
    "Bug report": "fix for nvtxstring not printing name for aten kernels (#57407)\n\nSummary:\naten kernels have a sequence number of -1\n\nIn order to ensure the names are properly printed in every case, we must change the >= 0 to => -1\n\nExample of bug:\n![Capture](https://user-images.githubusercontent.com/20074092/116767312-45959280-a9e4-11eb-92a3-c2236a00d481.PNG)\nExample of fix:\n![image](https://user-images.githubusercontent.com/20074092/116919709-82d96a80-ac06-11eb-8b74-e34cf1214ea5.png)\nAdditionally, while fixing and investigating this issue another issue was detected and has now been filed:\nhttps://github.com/pytorch/pytorch/issues/57476\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57407\n\nReviewed By: anjali411\n\nDifferential Revision: D28165818\n\nPulled By: ngimel\n\nfbshipit-source-id: dd3d245f1ea23c4b2edfcedbed3b47705ec1e966",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (sequence_nr >= 0 || shapes.size() > 0) {",
    "Added lines": "+  if (sequence_nr >= -1 || shapes.size() > 0) {\n+#endif\n+    } else if (sequence_nr == -1) {\n+#ifdef __HIP_PLATFORM_HCC__\n+      s << msg;\n+#else\n+      s << name.str() << msg;",
    "Label": "clean"
},
{
    "Id": 1133,
    "Library": "pytorch",
    "Date": "2021/04/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d1def931666172413cec91792ab17833b11a6ee4",
    "Root Cause": "N.A",
    "Bug report": "[torch/debuggability] use log.info() in addition to print() in timeoutguard (#57296)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57296\n\nSeems many trainers disable print(), so we cannot see the thread dumps with CompleteInTimeOrDie(). So log.info() also.\n\nTest Plan: sandcastle\n\nReviewed By: aalmah\n\nDifferential Revision: D28098738\n\nfbshipit-source-id: dfdca8801bacf5c7bccecc2387cb7ef41dadfa46",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                log.info(\"-----After force------\")\n+                # Log also with logger, as it is comment practice to suppress print().\n+                log.info(\"\\n\".join(code))\n+            # Log also with logger, as it is comment practice to suppress print().\n+            log.info(\"\\n\".join(code))",
    "Label": "clean"
},
{
    "Id": 1134,
    "Library": "pytorch",
    "Date": "2021/04/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d0ea3183c1ddedfc26fb0ef6fc5bedfeb916f7b6",
    "Root Cause": "N.A",
    "Bug report": "Remove debugging print in randperm (#57218)\n\nSummary:\nSorry that I forget to delete this. Thank xwang233 for finding this.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57218\n\nReviewed By: mruberry\n\nDifferential Revision: D28081292\n\nPulled By: ngimel\n\nfbshipit-source-id: a75867aa82d8644ef3a863d94f225c37babfe249",
    "Number of deleted lines": 2,
    "Deleted lines": "-  printf(\"tid = %d, island_size = %d\\n\", tid, island_size);\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1135,
    "Library": "pytorch",
    "Date": "2021/04/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2dc3dc23248005fb3efa3a5aa5ca2817e1d370b1",
    "Root Cause": "N.A",
    "Bug report": "Enhance error message for Future.setErrorIfNeeded. (#56631)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56631\n\n`setErrorIfNeeded` did not mention whether the future was already\ncompleted or there was some other exception. This particular change ensures\nthat we also print out the original exception as part of the error message.\n\nThis would help in debugging issues where this codepath is triggered.\nghstack-source-id: 127248844\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27919974\n\nfbshipit-source-id: 2273a93f3475929b14f721c976f194f33a5aa746",
    "Number of deleted lines": 4,
    "Deleted lines": "-      LOG(INFO)\n-          << \"Skipping setting following error on the Future since \"\n-          << \"it is already marked completed (this is not neccessarily an error): \"\n-          << tryRetrieveErrorMessageInternal(eptr);",
    "Added lines": "+      std::string msg = c10::str(\n+          \"Skipping setting following error on the Future since \"\n+          \"it is already marked completed (this is not neccessarily \"\n+          \"an error):\\n\", tryRetrieveErrorMessageInternal(eptr));\n+      if (eptr_) {\n+        msg += c10::str(\n+            \", \\nOriginal exception:\\n\",\n+            tryRetrieveErrorMessageInternal(eptr_));\n+      }\n+      LOG(INFO) << msg;",
    "Label": "clean"
},
{
    "Id": 1136,
    "Library": "pytorch",
    "Date": "2021/04/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/882e27366361c1347fb08d370d1950ded5ef199c",
    "Root Cause": "N.A",
    "Bug report": "[caffe2] fix bug when weight_decay is used with fused rowwise + SLWS grad (#57090)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57090\n\nWe did loop-invariant code motion to avoid multiplying with in_weight_temp for each element but this breaks down when weight decay is not zero.\n\nTest Plan:\nIn devgpu\nbuck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- test_fuse_sparse_adagrad_with_sparse_lengths_weighted_sum_gradient --run-disabled\n\nReviewed By: jianyuh\n\nDifferential Revision: D28051026\n\nfbshipit-source-id: f8906b72a41a87c2d43c447197b5fd695373ae23",
    "Number of deleted lines": 3,
    "Deleted lines": "-      const float x_ij = grad[group * block_size + i] +\n-      param_mom[index] +=\n-          static_cast<T>(row_sum_squares_avg * in_weight_temp * in_weight_temp);",
    "Added lines": "+      const float x_ij = grad[group * block_size + i] * in_weight_temp +\n+      param_mom[index] += static_cast<T>(row_sum_squares_avg);",
    "Label": "clean"
},
{
    "Id": 1137,
    "Library": "pytorch",
    "Date": "2021/04/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/267b554b6fd7d8dfdfb9885c663fbe48fb26b2f8",
    "Root Cause": "N.A",
    "Bug report": "fx: Fix type_matches for Optional[List[int]] arguments (#56790)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56790\n\nIf the argument doesn't match `List[int]`, this code falls through to\n`issubclass(argument_type, List[int])` which is invalid and raises a\n`TypeError`. If this happens during the processing of a `Union` (e.g.\n`Optional`), the other union types aren't given the chance to match against the\nsignature.\n\nThis also stop normalize_function from indescriminately swallowing exceptions,\nwhich let this bug go unnoticed.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27987746\n\nPulled By: mruberry\n\nfbshipit-source-id: c5aa5f61a215f0f39925e7053f33bff4b5d5acc2",
    "Number of deleted lines": 7,
    "Deleted lines": "-    if signature_type is List[int] and is_homogeneous_int_tuple(argument_type):\n-        return True\n-                        sig_matches = True\n-                            for arg_name, arg_type in bound_types.arguments.items():\n-                                param = candidate_signature.parameters[arg_name]\n-                                sig_matches = sig_matches and type_matches(param.annotation, arg_type)\n-                            sig_matches = False",
    "Added lines": "+    if signature_type is List[int]:\n+        return is_homogeneous_int_tuple(argument_type)\n+                            continue\n+\n+                        sig_matches = True\n+                        for arg_name, arg_type in bound_types.arguments.items():\n+                            param = candidate_signature.parameters[arg_name]\n+                            sig_matches = sig_matches and type_matches(param.annotation, arg_type)\n+",
    "Label": "clean"
},
{
    "Id": 1138,
    "Library": "pytorch",
    "Date": "2021/04/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6de1d9b2d07a53a9fecde6ac2b9b46d5c424f121",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in emitUse to drop all values that are marked as drop (#56652)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56652\n\nPrevious code doesn't drop prim::Constant values even when they are marked as drop.\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D27927413\n\nfbshipit-source-id: 67cd52cf292e111be2830ccf93b0e7b089e49001",
    "Number of deleted lines": 2,
    "Deleted lines": "-      } else if (drop) {\n-        op = DROPR;",
    "Added lines": "+\n+      if (drop) {\n+        op = DROPR;\n+      }",
    "Label": "clean"
},
{
    "Id": 1139,
    "Library": "pytorch",
    "Date": "2021/04/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bc3d892c20ee8cf6c765742481526f307e20312a",
    "Root Cause": "N.A",
    "Bug report": "README: Minor improvements (#56193)\n\nSummary:\n* Visual studio versions: clarify and shorten.\n* Remove obsolete note about a bug that has been fixed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56193\n\nReviewed By: albanD\n\nDifferential Revision: D27939766\n\nPulled By: ezyang\n\nfbshipit-source-id: e142ec04ba98d5468f28ddf2e8bba5d99d3cfc26",
    "Number of deleted lines": 4,
    "Deleted lines": "-Visual Studio upgrades are very often. Sometimes, there're regressions in some new versions.\n-It'd best to use the same Visual Studio Version as [PyTorch CI's](https://github.com/pytorch/pytorch/blob/a1bd7918cc5a06cbef6c5178259bf0a7b5ab1ce3/.circleci/scripts/vs_install.ps1#L4).\n-Note: There's a [compilation issue](https://github.com/oneapi-src/oneDNN/issues/812) in several Visual Studio 2019 versions since 16.7.1, so please make sure your Visual Studio 2019 version is not in 16.7.1 ~ 16.7.5\n-",
    "Added lines": "+Sometimes there are regressions in new versions of Visual Studio, so\n+it's best to use the same Visual Studio Version as [PyTorch CI's](https://github.com/pytorch/pytorch/blob/a1bd7918cc5a06cbef6c5178259bf0a7b5ab1ce3/.circleci/scripts/vs_install.ps1#L4).",
    "Label": "clean"
},
{
    "Id": 1140,
    "Library": "pytorch",
    "Date": "2021/04/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3e0c226eed261073871c159635bc8640a1422bad",
    "Root Cause": "N.A",
    "Bug report": "Raise TypeErrors when IValue::getSubValues fails (#56510)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56510\n\nThe comment for `TORCH_INTERNAL_ASSERT` say to use it for \"enforcement of internal invariants in code\", meaning \"assuming no bugs in PyTorch, the conditions tested by this macro should always be true\". However this wasn't the case here, at least for the RPC code: CUDAFuture is calling the `getSubValues` method on a generic IValue of which it doesn't know (or care about) the type. It was thus sometimes triggering the internal assert when users provided non-inspectable types, which was producing an exception with a message containing \"please report a bug to PyTorch\", which was confusing to users.\n\nIt makes more sense to me to consider this a type error, which can thus be reported more clearly to the user (and, later on in this stack, to catch). Hence the difference introduced here is just the type and the message of the exception. I don't expect there to be any code depending on the old behavior (as it would mean depending on a violation of an internal invariant).\nghstack-source-id: 127035768\n\nTest Plan: Unit tests\n\nReviewed By: mrshenli\n\nDifferential Revision: D27861066\n\nfbshipit-source-id: 6d41c922257cba5f37c7a4614d8e5ab5c7c87b92",
    "Number of deleted lines": 4,
    "Deleted lines": "-      TORCH_INTERNAL_ASSERT(match.success(),\n-            \"Cannot infer type of \", py_obj->toStr(), \"\\n:\", match.reason());\n-      TORCH_INTERNAL_ASSERT(\n-          false, \"sub ivalue is nat enabled for: \", this->tagKind());",
    "Added lines": "+      TORCH_CHECK_TYPE(match.success(),\n+            \"Cannot infer type of \", py_obj->toStr(), \": \", match.reason());\n+      TORCH_CHECK_TYPE(\n+          false, \"Cannot inspect value of type \", this->tagKind());",
    "Label": "clean"
},
{
    "Id": 1141,
    "Library": "pytorch",
    "Date": "2021/04/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3e55fc91fd5f9846bfe78d9e2441cf2d9f7f0bed",
    "Root Cause": "N.A",
    "Bug report": "[pet] Remove additional @record in elastic_launch to fix file existing error\n\nSummary:\nSince `launch_agent()` in api.py is already decorated with record, we can remove the usage in elastic_launch.\nIt also fix the bug for FileExistError on MAST\n\nWe run an experiment to count how many times record is invoked in D27901961 to ensure the assumption.\n\nTest Plan:\n```\nfbpkg build -E torchelastic_distributed_sum\n\nbuck run mode/dev-nosan //pytorch/elastic/torchelastic/tsm/fb/cli:tsm -- run_ddp --scheduler mast --fbpkg torchelastic_distributed_sum:fde7879   --nnodes 1 --nproc_per_node 1 --resource T1 --run_cfg hpcIdentity=oncall_dai_pet,hpcClusterUuid=MastNaoTestCluster main.par\n```\n\nhttps://www.internalfb.com/mast/job/tsm_wilsonhong-torchelastic_distributed_sum_a92f97e7\n\nReviewed By: borovsky-d\n\nDifferential Revision: D27902034\n\nfbshipit-source-id: e08b02d4b9c7a7c70fbb0dbcb24b95af55d2ea95",
    "Number of deleted lines": 1,
    "Deleted lines": "-@record",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1142,
    "Library": "pytorch",
    "Date": "2021/04/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a31fd7f45373be3f01e5b822be513a164927c64b",
    "Root Cause": "N.A",
    "Bug report": "Fix onnx/constant_fold.cpp compilation on Windows (#55770) (#56167)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56167\n\nVC++ does not recognize `or` as a valid operator. This breaks the build under `Debug` configuration.\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D27866143\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 490cee57b9762170ce02a6f73130772a3542e76d",
    "Number of deleted lines": 1,
    "Deleted lines": "-    assert(inputTensorValues.size() == 2 or inputTensorValues.size() == 1);",
    "Added lines": "+    assert(inputTensorValues.size() == 2 || inputTensorValues.size() == 1);",
    "Label": "clean"
},
{
    "Id": 1143,
    "Library": "pytorch",
    "Date": "2021/04/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b1282bc10925764fe1ea81037f23be7cded88336",
    "Root Cause": "N.A",
    "Bug report": "Use stack trace implementation in common/process on fbcode (#56400)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56400\n\nSee https://github.com/pytorch/pytorch/issues/56399\n\nI don't have time to fix this properly, so this is just to stem the\nbleeding.  Someone should go and figure out what it is that common/process\nis doing better.\nghstack-source-id: 126868405\n\nTest Plan:\nI manually patched this into D27765125 and triggered a\nexception and observed that everything symbolized good:\n\n```\n[9]   what():  new_refcount != 1INTERNAL ASSERT FAILED at \"caffe2/c10/util/intrusive_ptr.h\":234, please report a bug to PyTorch. intrusive_ptr: Cannot increase refcount after it reached zero.\nException raised from retain_ at caffe2/c10/util/intrusive_ptr.h:234 (most recent call first):\n# 0  c10::get_backtrace[abi:cxx11](unsigned long, unsigned long, bool)\n# 1  c10::(anonymous namespace)::GetFetchStackTrace[abi:cxx11]()::$_0::operator()[abi:cxx11]() const\n# 2  std::_Function_handler<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > (), c10::(anonymous namespace)::Ge\ntFetchStackTrace()::$_0>::_M_invoke(std::_Any_data const&)\n# 3  std::function<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > ()>::operator()() const\n# 4  c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)\n# 5  c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocat\nor<char> > const&)\n# 6  c10::detail::torchInternalAssertFail(char const*, char const*, unsigned int, char const*, char const*)\n# 7  c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> >::retain_()\n# 8  c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> >::intrusive_ptr(c10::intrusiv\ne_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&)\n# 9  c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> >& c10::intrusive_ptr<c10d::Pr\nocessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> >::operator=<c10d::ProcessGroup, c10::detail::intrusive_target\n_default_null_type<c10d::ProcessGroup> >(c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessG\nroup> > const&) &\n```\n\nReviewed By: driazati\n\nDifferential Revision: D27861908\n\nfbshipit-source-id: 84c1dfb1ef28c460b020646f836c153562ad5c44",
    "Number of deleted lines": 1,
    "Deleted lines": "-#if SUPPORTS_BACKTRACE",
    "Added lines": "+#ifdef FBCODE_CAFFE2\n+#include <common/process/StackTrace.h>\n+#endif\n+\n+#ifdef FBCODE_CAFFE2\n+  // For some reason, the stacktrace implementation in fbcode is\n+  // better than ours, see  https://github.com/pytorch/pytorch/issues/56399\n+  // When it's available, just use that.\n+  facebook::process::StackTrace st;\n+  return st.toString();\n+\n+#elif SUPPORTS_BACKTRACE",
    "Label": "clean"
},
{
    "Id": 1144,
    "Library": "pytorch",
    "Date": "2021/04/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7629477ff713d9f8f763830519da0080eec1b030",
    "Root Cause": "N.A",
    "Bug report": "Filter out more expected errors from sccache log (#56281)\n\nSummary:\nThis PR extends `.jenkins/pytorch/print_sccache_log.py` to filter out a distracting \"error\" message that walterddr came across while debugging failures in https://github.com/pytorch/pytorch/issues/55176:\n\n```\n=================== sccache compilation log ===================\nERROR 2021-04-05T15:44:18Z: sccache::server: Compilation failed: Output { status: ExitStatus(ExitStatus(256)), stdout: \"\", stderr: \"/var/lib/jenkins/.cache/torch_extensions/test_compilation_error_formatting/main.cpp: In function \u2018int main()\u2019:\\n/var/lib/jenkins/.cache/torch_extensions/test_compilation_error_formatting/main.cpp:2:23: error: expected \u2018;\u2019 before \u2018}\u2019 token\\n int main() { return 0 }\\n                       ^\\n\" }\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56281\n\nTest Plan: TODO (reviewers: is there an easy way to test this?)\n\nReviewed By: walterddr\n\nDifferential Revision: D27826064\n\nPulled By: samestep\n\nfbshipit-source-id: 7322a830c1246820a5b2b7bbeaa4697ebd13b617",
    "Number of deleted lines": 3,
    "Deleted lines": "-    # Ignore errors from CPU instruction set or symbol existing testing\n-    keywords = ['src.c', 'CheckSymbolExists.c']\n-    if all([keyword not in line for keyword in keywords]):",
    "Added lines": "+    # Ignore errors from CPU instruction set, symbol existing testing,\n+    # or compilation error formatting\n+    ignored_keywords = [\n+        'src.c',\n+        'CheckSymbolExists.c',\n+        'test_compilation_error_formatting',\n+    ]\n+    if all([keyword not in line for keyword in ignored_keywords]):",
    "Label": "clean"
},
{
    "Id": 1145,
    "Library": "pytorch",
    "Date": "2021/04/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d56f4518209d233d7da603cf306d862c73970d8d",
    "Root Cause": "N.A",
    "Bug report": "[nnc] Separate printing of optimized llvm bitcode from assembly (#56117)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56117\n\nI was debugging an issue during instruction selection and wanted to\nsee the input bitcode.  This way we always print it before going into the asm\ngeneration pass.\nghstack-source-id: 126592596\n\nTest Plan: Run with `PYTORCH_JIT_LOG_LEVEL=\">>llvm_codegen\"`\n\nReviewed By: huiguoo\n\nDifferential Revision: D27781683\n\nfbshipit-source-id: 84635d0ca2a1318ae7a9a73cc1d2df450d8b6a08",
    "Number of deleted lines": 3,
    "Deleted lines": "-  // print graph debug info after optimization\n-  GRAPH_DEBUG(\n-      \"\\nLLVM module after optimizations\\n\\n\", llvmCode, \"\\n\", asmCode, \"\\n\");",
    "Added lines": "+  GRAPH_DEBUG(\n+      \"\\nLLVM module after optimizations\\n\\n\", asmStream.str().str(), \"\\n\");\n+\n+  // print graph debug info after optimization\n+  GRAPH_DEBUG(\"\\nLLVM generated assembly code\\n\\n\", asmCode, \"\\n\");",
    "Label": "clean"
},
{
    "Id": 1146,
    "Library": "pytorch",
    "Date": "2021/04/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0b8bd2261450d21e0dd0ae259aa06994d1a0d9b8",
    "Root Cause": "N.A",
    "Bug report": "Fix bug with rebuilding extensions every import (#56015)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56015\n\nReviewed By: mruberry\n\nDifferential Revision: D27765934\n\nPulled By: ezyang\n\nfbshipit-source-id: 65cace951fce5f2284ab91d8bd687ac89a2311fb",
    "Number of deleted lines": 1,
    "Deleted lines": "-            nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'",
    "Added lines": "+            if IS_WINDOWS:\n+                nvcc_gendeps = '--generate-dependencies-with-compile --dependency-output $out.d'",
    "Label": "clean"
},
{
    "Id": 1147,
    "Library": "pytorch",
    "Date": "2021/04/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/34b46359e3cdca01f460fe8f4ce894868a2b4191",
    "Root Cause": "N.A",
    "Bug report": "Fix forwarding/move bug (#53556)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53556\n\nWhen packing a `Tensor&` (mutable lvalue reference) into an IValue, we accidentally didn't increase the refcount.\nThis wasn't triggered anywhere, until I tried to enable backend fallbacks. Backend fallbacks for ops that\nhave out arguments (i.e. ops that take `Tensor&` arguments and return `Tensor&` arguments) pack those returns\ninto an IValue stack (and accidentally don't increase the refcount), then later that stack gets destructed\n(which decreases the refcount and possibly destroys the Tensor), and the `Tensor&` passed in as an out argument\nis suddenty freed memory.\n\nThis PR fixes that by forwarding instead of moving when wrapping Tensors into IValues.\nghstack-source-id: 125886986\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: waitforsandcastle\n\nReviewed By: swolchok\n\nDifferential Revision: D26896507\n\nfbshipit-source-id: 62102fa89e522699b5174c33279a2b1a775066a4",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return IValue(std::move(x));",
    "Added lines": "+  return IValue(std::forward<T>(x));",
    "Label": "clean"
},
{
    "Id": 1148,
    "Library": "pytorch",
    "Date": "2021/03/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/394b720e38455c2c07e6a31ef816edd0c5afbeb4",
    "Root Cause": "N.A",
    "Bug report": "Fix raw_deleter() bug with PYTORCH_NO_CUDA_MEMORY_CACHING=1 (#54775)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54775\n\nThanks danpovey for reporting. Fixes https://github.com/pytorch/pytorch/issues/54770\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27363730\n\nPulled By: ezyang\n\nfbshipit-source-id: 81777aff7d9194b060fb076ef97cf788f2a4f43e",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return &raw_delete;",
    "Added lines": "+    if (forceUncachedAllocator()) {\n+      return &uncached_delete;\n+    } else {\n+      return &raw_delete;\n+    }",
    "Label": "clean"
},
{
    "Id": 1149,
    "Library": "pytorch",
    "Date": "2021/03/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/20d8fe83cd5ee9cbda8d629c95667d8e4954a799",
    "Root Cause": "N.A",
    "Bug report": "[TSAN] Suppress data races in caffe2/c10/util/Logging.cpp\n\nSummary:\nThis suppresses some data races reported by TSAN. See the associated\ntask(s) below for context, including sample stack traces caused by these races\nand reproduction instructions.\n\nThis diff is automatically generated. Therefore, the way it makes suppressions\nmay not be as beautiful as if written by hand. *However, we don't have the\nresources to manually adjust these diffs, nor do we have the capacity to\nactually fix the bugs*; we just want to get the existing bugs\nout of the way so we can enable TSAN across the fleet. If you are a reviewer\nplease do one of the following:\n\n1. Accept the diff as is, and you may follow up with more changes (or fix the\n   bugs) later.\n2. Fix the data races in a different diff and land it within a reasonable amount\n   of time (e.g. a week), and comment about it here.\n3. Comment to suggest us a different code location(s) to suppress these data\n   races.\n\nTest Plan: Unit tests were automatically run as part of https://www.internalfb.com/intern/sandcastle/job/22517998509525934/\n\nReviewed By: ezyang\n\nDifferential Revision: D26094360\n\nfbshipit-source-id: 06c285570bcf7a1491d8f17d1885d065ef0bc537",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#ifdef FBCODE_CAFFE2\n+#include <folly/synchronization/SanitizeThread.h>\n+#endif\n+#ifdef FBCODE_CAFFE2\n+  // TODO(T82645998): Fix data race exposed by TSAN.\n+  folly::annotate_ignore_thread_sanitizer_guard g(__FILE__, __LINE__);\n+#endif",
    "Label": "clean"
},
{
    "Id": 1150,
    "Library": "pytorch",
    "Date": "2021/03/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c0bcd5a58faf7ef6d42835ecd592a56bad1886e8",
    "Root Cause": "N.A",
    "Bug report": "Remove NestedTensor from DefaultBackend alias (#54559)\n\nSummary:\nKernels such as \"add\" are registered to DefaultBackend. At a minimum NestedTensor is not compatible with structured kernels due to missing fields such as size, which can therefore cause difficult to catch bugs when being passed into a function without a NestedTensor-specific kernel.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54559\n\nReviewed By: ezyang\n\nDifferential Revision: D27283591\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: fad7c03ca3b2190f2f90039dd2872184e9bc5049",
    "Number of deleted lines": 1,
    "Deleted lines": "-        DispatchKey::NestedTensor,",
    "Added lines": "+// NestedTensor has been explicitly removed due to incompatibility with some\n+// kernels, such as structured kernels, that use the DefaultBackend key.",
    "Label": "clean"
},
{
    "Id": 1151,
    "Library": "pytorch",
    "Date": "2021/03/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1442a92741d8f39e3d6228af2ed8800cc29ed16f",
    "Root Cause": "N.A",
    "Bug report": "Ensure local_used_maps_tmp is distinct from local_used_maps_[i] (#54474)\n\nSummary:\nFollowup/hotfix for https://github.com/pytorch/pytorch/pull/53160. rohan-varma and zhaojuanmao were seeing https://github.com/pytorch/pytorch/pull/53160/files#diff-9273e5ff7b40f30d6a4444d1c7be9fe9a5c2068070c68af4e7b0ac2d4cff0923R582 fire in some internal workloads, indicating `local_used_maps_tmp` wasn't actually being created as a distinct temporary, in other words, `local_used_maps_[i]` was already pinned for some reason. This seems like a bug with the CPU allocator: [`local_used_maps_` should not have been pinned on construction](https://github.com/pytorch/pytorch/blob/9be4c75fa0399a292e95ef8f3c79457e4b5b2338/torch/lib/c10d/reducer.cpp#L180-L183). We should [investigate that separately](https://github.com/pytorch/pytorch/pull/53160/files#r599188373).\n\nIn the meantime, the present PR should ensure `local_used_maps_tmp` is always distinct from `local_used_maps_[i]` (and therefore prevents the race condition described in https://github.com/pytorch/pytorch/pull/51360) even if `local_used_maps_[i]`is already pinned.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54474\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27268039\n\nPulled By: rohan-varma\n\nfbshipit-source-id: ab9af3dd845098bde788cb28a9217caea246ddfa",
    "Number of deleted lines": 3,
    "Deleted lines": "-          auto local_used_maps_tmp = local_used_maps_[i].pin_memory();\n-          // Defensively ensures a deep copy to a pinned temporary\n-          TORCH_INTERNAL_ASSERT(local_used_maps_tmp.data_ptr() != local_used_maps_[i].data_ptr())",
    "Added lines": "+          //\n+          // Defensively ensures local_used_maps_tmp is distinct from local_used_maps_[i]\n+          auto local_used_maps_tmp = at::native::empty_like(local_used_maps_[i],\n+                                                            local_used_maps_[i].options().pinned_memory(true));\n+          // Paranoid asserts here because in some workloads, the pinned allocator behaves in a way we\n+          // don't understand, and may be bugged. See https://github.com/pytorch/pytorch/pull/54474\n+          TORCH_INTERNAL_ASSERT(local_used_maps_tmp.is_pinned());\n+          TORCH_INTERNAL_ASSERT(local_used_maps_tmp.data_ptr() != local_used_maps_[i].data_ptr());\n+          local_used_maps_tmp.copy_(local_used_maps_[i]);",
    "Label": "clean"
},
{
    "Id": 1152,
    "Library": "pytorch",
    "Date": "2021/03/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2355f61f1937552fd87820e14dcb68baecff5c94",
    "Root Cause": "N.A",
    "Bug report": "Add logging for debugging S223170\n\nSummary: more context in T86752810. Add info for tensor lengths size to see if it fails on in complete batch\n\nTest Plan: manually created failed run: f258719092\n\nReviewed By: aartibasant\n\nDifferential Revision: D27181049\n\nfbshipit-source-id: 341c020a3430c410f9726d92315efb80d36e9452",
    "Number of deleted lines": 1,
    "Deleted lines": "-            N);",
    "Added lines": "+            N,\n+            \", actual batch length is \",\n+            M);",
    "Label": "clean"
},
{
    "Id": 1153,
    "Library": "pytorch",
    "Date": "2021/03/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/98baad57642115d1f66723f6f10585ed933fd731",
    "Root Cause": "N.A",
    "Bug report": "[nnc] Remove cached argv from LLVMCodeGen to fix race condition (#54286)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54286\n\nA generated code object was holding not just a function pointer but a\npre-allocated argument buffer.  I assume this was a performance optimization to\navoid allocating a vector on each call?\n\nThis cached buffer makes it unsafe to call a generated function from multiple\nthreads, which is too severe a limitation.  This diff fixes it by locally\nallocating a SmallVector to hold the args.\n\nA better fix will be to avoid creating CallArgs, so the function can be called\ndirectly without this packing-and-unpacking nonsense, but that's a slightly\nmore involved fix, possibly involving changing the kernel codegen, and this bug\nneeds fixing now.\nghstack-source-id: 124333028\n\nTest Plan: `threads=64 scripts/bwasti/static_runtime/run.sh`\n\nReviewed By: asuhan\n\nDifferential Revision: D27175715\n\nfbshipit-source-id: 44dafe77b95ede69c63ae6d64f39f0aa4877712f",
    "Number of deleted lines": 9,
    "Deleted lines": "-  std::unique_ptr<void* []> argv_ { nullptr };\n-  void** getArgvAddress() const;\n-  void** argv = impl_->getArgvAddress();\n-  value<float>(argv);\n-void** LLVMCodeGenImpl::getArgvAddress() const {\n-  return argv_.get();\n-}\n-\n-  argv_ = std::make_unique<void*[]>(params.size());",
    "Added lines": "+  constexpr unsigned nargs = 8;\n+  c10::SmallVector<void*, nargs> argv;\n+  argv.resize(buf_args.size());\n+  value<float>(argv.data());",
    "Label": "clean"
},
{
    "Id": 1154,
    "Library": "pytorch",
    "Date": "2021/03/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b936abd840d8fa3122d3a6166c0d7344d8352901",
    "Root Cause": "N.A",
    "Bug report": "fix nest openmp performance bug in thnn_conv2d (#52577)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52577\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D27063800\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 000e17b722b2b1d48e1012b3fa222729e26777fb",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  if (end - begin == 1) {\n+    f(begin, end);\n+    return;\n+  }",
    "Label": "clean"
},
{
    "Id": 1155,
    "Library": "pytorch",
    "Date": "2021/03/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0c2fe02ec178e2222e4fb4cd701468f52b05ab01",
    "Root Cause": "N.A",
    "Bug report": "[DDP] Fix wrong call to dist.get_rank() (#53793)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53793\n\nThis call should pass in the process group so it works appropriately\nfor subgroups instead of whole world being passed into DDP.\n\nAside: This wasn't caught by tests since we don't have good testing around\npassing subgroups into DDP, I believe nearly all tests use the entire world.\nShould we add better testing for subgroups which may potentially bring up more\nsubtle bugs?\nghstack-source-id: 123640712\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D26972367\n\nfbshipit-source-id: 8330bd51e2ad66841e4c12e96b67d3e78581ec74",
    "Number of deleted lines": 1,
    "Deleted lines": "-                    authoritative_rank = self._find_common_rank(dist.get_rank(), True)",
    "Added lines": "+                    authoritative_rank = self._find_common_rank(\n+                        dist.get_rank(self.process_group), True\n+                    )",
    "Label": "clean"
},
{
    "Id": 1156,
    "Library": "pytorch",
    "Date": "2021/03/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/409a76f72ceda78a78fe42edb7becd15fbf4282b",
    "Root Cause": "N.A",
    "Bug report": "[Static Runtime] Fix bug in static_runtime::to_copy (#53634)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53634\n\nMake the op signature of `static_runtime::to_copy` consistent with that of native_functions.yaml so it works with 2-5 args:\n```\n- func: to.dtype(Tensor self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor\n  variants: method\n  device_guard: False\n```\n\n(Note: this ignores all push blocking failures!)\n\nReviewed By: ajyu\n\nDifferential Revision: D26906726\n\nfbshipit-source-id: b9203eb23619aba42b1bfed1a077401f9fe2ddf0",
    "Number of deleted lines": 5,
    "Deleted lines": "-      [](at::Tensor self, ArrayRef<int64_t> dims) -> at::Tensor {\n-      \"static_runtime::to_copy(Tensor self, ScalarType dtype, bool non_blocking, bool copy) -> Tensor\",\n-      [](at::Tensor self, at::ScalarType dtype, bool non_blocking, bool copy)\n-          -> at::Tensor {\n-        return out.to(dtype, non_blocking, copy);",
    "Added lines": "+      [](const at::Tensor& self, ArrayRef<int64_t> dims) -> at::Tensor {\n+      \"static_runtime::to_copy(Tensor self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor\",\n+      [](const at::Tensor& self,\n+         at::ScalarType dtype,\n+         bool non_blocking,\n+         bool copy,\n+         c10::optional<c10::MemoryFormat> format) -> at::Tensor {\n+        return out.to(dtype, non_blocking, copy, format);",
    "Label": "clean"
},
{
    "Id": 1157,
    "Library": "pytorch",
    "Date": "2021/03/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/707fc354ebd505b506e3c8bd396fb675bb25c1eb",
    "Root Cause": "N.A",
    "Bug report": "Add debug only layout assert for empty_cpu (#53396)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53396\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26891806\n\nPulled By: ezyang\n\nfbshipit-source-id: 4789ab5587d1a11d50e9a60bbfa1c21c1222823e",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(layout_or_default(layout_opt) == Layout::Strided);",
    "Label": "clean"
},
{
    "Id": 1158,
    "Library": "pytorch",
    "Date": "2021/02/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b9e12a0e8290af1c05107cf7e9a6117355e5ac1a",
    "Root Cause": "N.A",
    "Bug report": "[pytorch] Fix mkldnn heuristic for multithreaded convolution (#52909)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52909\n\nPR #46675 introduced heuristics to use thnn_conv2d for 1x1\nconvolutions, since mkldnn had a bug that was slowing those cases\ndown. Unfortunately, the test plan for that PR only tested single-threaded\nconvolutions; mkldnn is considerably faster on multithreaded convolutions.\n\nAn example from yolov3, on 24 cores of a Xeon Platinum 8175M CPU @ 2.50GHz\n```\ninput:{1, 64, 192, 256}, weight:{32, 64, 1, 1}\nthnn_conv2d: GFLOPS/s=104.574G/s\nmkldnn_convolution: GFLOPS/s=467.357G/s\n```\nghstack-source-id: 122627564\n\nTest Plan: Multithreaded 1x1 convolutions\n\nReviewed By: wconstab, xuzhao9\n\nDifferential Revision: D26685272\n\nfbshipit-source-id: e8e05db89e43856969e26570a170c13b3e73ac74",
    "Number of deleted lines": 2,
    "Deleted lines": "-      weight.size(-1) != 1 || weight.size(-2) != 1) &&\n-      ); ",
    "Added lines": "+#include <ATen/Parallel.h>\n+     // For 1x1 filters, MKLDNN is faster than THNN when multi-threaded,\n+     // but THNN is faster when single-threaded.\n+      weight.size(-1) != 1 || weight.size(-2) != 1 || at::get_num_threads() > 1) &&\n+      );",
    "Label": "clean"
},
{
    "Id": 1159,
    "Library": "pytorch",
    "Date": "2021/02/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/94da8b9816689e563f7983822b62bb518bcf120f",
    "Root Cause": "N.A",
    "Bug report": "Fix resource leak bug in TCPStore constructor (#52860)\n\nSummary:\nThis PR fixes a resource leakage bug in the constructor of `TCPStore` where an exception thrown in `TCPStoreDaemon` or `tcputil::connect()` can leave the server socket dangling. The ideal long-term solution would be to have a RAII wrapper for TCP sockets returned by `tcputil`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52860\n\nReviewed By: osalpekar\n\nDifferential Revision: D26671775\n\nPulled By: cbalioglu\n\nfbshipit-source-id: ccebbd7533ac601a4b80e6e759f2fb4fe01c70fa",
    "Number of deleted lines": 9,
    "Deleted lines": "-    // Now start the daemon\n-    tcpStoreDaemon_ = std::unique_ptr<TCPStoreDaemon>(\n-        new TCPStoreDaemon(masterListenSocket_));\n-  // Connect to the daemon\n-  storeSocket_ = tcputil::connect(\n-      tcpStoreAddr_, tcpStorePort_, /* wait= */ true, timeout_);\n-  if (numWorkers.value_or(-1) >= 0 && waitWorkers) {\n-    waitForWorkers();\n-    tcpStoreDaemon_.reset(nullptr);",
    "Added lines": "+  try {\n+      if (isServer_) {\n+        // Now start the daemon\n+        tcpStoreDaemon_ = std::make_unique<TCPStoreDaemon>(masterListenSocket_);\n+      }\n+      // Connect to the daemon\n+      storeSocket_ = tcputil::connect(\n+          tcpStoreAddr_, tcpStorePort_, /* wait= */ true, timeout_);\n+      if (numWorkers.value_or(-1) >= 0 && waitWorkers) {\n+        waitForWorkers();\n+      }\n+  } catch (const std::exception&) {\n+    if (isServer_) {\n+        tcpStoreDaemon_ = nullptr;\n+        tcputil::closeSocket(masterListenSocket_);\n+    }\n+    throw;\n+    tcpStoreDaemon_ = nullptr;",
    "Label": "clean"
},
{
    "Id": 1160,
    "Library": "pytorch",
    "Date": "2021/02/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/76af821c361dd6de0bc09e894a872d7dee0cdb0a",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch] \"Fix\" wrong-looking move in TensorImpl (#52344)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52344\n\nThis line is a bug-prone use of std::move combined with a reference to the moved-from parameter in the same series of function call arguments. This is normally a problem because the order of evaluation is undefined -- if the move happens before the call to `storage.device()`, we may have problems. It is not a problem here because we are merely forwarding from one `Storage&&` parameter to another.\nghstack-source-id: 121837267\n\nTest Plan: See no clang-tidy/HowToEven warning on the diff, I hope\n\nReviewed By: bhosmer\n\nDifferential Revision: D26436550\n\nfbshipit-source-id: da85d79be854ff42c5a0cab9649ba82295816eca",
    "Number of deleted lines": 1,
    "Deleted lines": "-    : TensorImpl(std::move(storage), key_set, data_type, storage.device()) {}",
    "Added lines": "+    // Use std::forward to suppress static analyzer false positive.\n+    : TensorImpl(std::forward<Storage>(storage), key_set, data_type, storage.device()) {}",
    "Label": "clean"
},
{
    "Id": 1161,
    "Library": "pytorch",
    "Date": "2021/02/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c442776f3ccc9930cc51e951e1a23bc5cc12deb0",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch] Debug-gate static_assert in KernelFunction::makeFromUnboxedFunctor (#51367)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51367\n\nTemplight said that this assertion was taking about 5% of build time for RegisterCPU.cpp (a hopefully-representative example I picked to shorten my iteration cycle).\n\nI've debug-gated it on the grounds that 1) we at least try to build\neverything in debug mode and 2) optimized builds presumably take\nlonger in general, so we can more afford to pay the build time cost in\ndebug builds.\n\nThe win is not entirely clear; please see the test plan for details.\nghstack-source-id: 121378960\n\nTest Plan:\n1) Built RegisterCPU.cpp with -ftime-trace before and after. It doesn't seem to call out any difference in the details, but the overall time is stably down more like 10% (55s before and 49s after).\n2) Did a full rebuild of aten-cpu with -ftime-trace before and\nafter. No significant difference in build times shown (it says *after*\nis a regression, but it's using wall-time data and the machine is\nloaded during builds so there's some noise).\n3) Re-profiled with Templight.\n\nBefore:\n\n{F366557311}\n\nAfter:\n\n{F366557501}\n\nNot sure what to conclude overall. A known problem with templight is that template instantiations form more of a dependency graph than a tree because they're cached internally, so eliminating the first caller of a template may just move the time to another caller. However, it looks like we have actually reduced is_functor traffic.\n\nUPDATE: I don't think that the -ftime-trace measurement was reliable; it seems to skew running times. I built this diff vs its base 5 times and measured the CPU (\"user\") time each time. Results (in seconds):\n\nprevious diff: [51.97, 50.54, 50.49, 52.89, 51.61]\nmean: 51.5 std: 0.906\n\nthis diff: [50.53, 50.41, 50.57, 50.67, 50.94]\nmean: 50.6 std: 0.179\n\nReviewed By: ezyang\n\nDifferential Revision: D26153793\n\nfbshipit-source-id: 9a66912c1b2b068f453e78be57454e4e62b7107b",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#ifndef NDEBUG\n+  // This assertion is costly for build time so it's debug-gated.\n+#endif",
    "Label": "clean"
},
{
    "Id": 1162,
    "Library": "pytorch",
    "Date": "2021/02/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9c2dd5775a59fc6a08d8509193294846dcdce73f",
    "Root Cause": "N.A",
    "Bug report": "Fixed slight bug in FX docs (#51779)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51779\n\nReviewed By: ngimel\n\nDifferential Revision: D26279623\n\nPulled By: Chillee\n\nfbshipit-source-id: 0cd2a487ce6b80ce0d3f81e2b2334ade20d816bb",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        env = {}",
    "Label": "clean"
},
{
    "Id": 1163,
    "Library": "pytorch",
    "Date": "2021/01/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/534aabce147129fc2fa54d448093c79603d45781",
    "Root Cause": "N.A",
    "Bug report": "[nnc] Don't use sleef where it's slower (#51246)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51246\n\nUsing sleef is sometimes slower than libm (I haven't debugged why).\nThe easy solution is to not use sleef in those cases.  With this diff, plus the\nprior one to use sleef period, we've sped up every unary op:\nghstack-source-id: 120614087\n\nTest Plan: `buck run mode/opt -c python.package_style=inplace //caffe2/benchmarks/cpp/tensorexpr:bench_ops`\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D26113672\n\nfbshipit-source-id: 6b731ac935b3652c8b3e3f4a5d2baa39ff31323a",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (hasAVX && jit_->hasSymbol(sleefName)) {",
    "Added lines": "+static bool wantSleef(const std::string& name) {\n+  // Using sleef on these ops is slower than libm.\n+  static std::unordered_set<std::string> noSleef = {\n+      \"sqrt\",\n+      \"ceil\",\n+      \"trunc\",\n+      \"fabs\",\n+      \"floor\",\n+      \"sqrtf\",\n+      \"ceilf\",\n+      \"truncf\",\n+      \"fabsf\",\n+      \"floorf\",\n+  };\n+  return noSleef.find(name) == noSleef.end();\n+}\n+\n+  if (wantSleef(basename) && hasAVX && jit_->hasSymbol(sleefName)) {",
    "Label": "clean"
},
{
    "Id": 1164,
    "Library": "pytorch",
    "Date": "2021/01/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8a8fac668175c266cdada84a76b3269312009f3f",
    "Root Cause": "N.A",
    "Bug report": "Remove debug-only assertion from vulkan::api::Command::Command as the buffer can legitimately be null. (#51160)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51160\n\nTest Plan: Imported from OSS\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D26131252\n\nPulled By: AshkanAliabadi\n\nfbshipit-source-id: 69f324ceed711753d77ab7c6b6a20a29cdbdf5f9",
    "Number of deleted lines": 3,
    "Deleted lines": "-  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(\n-      command_buffer_,\n-      \"Invalid Vulkan command buffer!\");",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1165,
    "Library": "pytorch",
    "Date": "2021/01/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e2eb97dd7682d2810071ce78b76543acc1584a9c",
    "Root Cause": "N.A",
    "Bug report": "[ONNX] Fix param names (#50764) (#50955)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50955\n\nPreserve name of parameters for ONNX.\n\nLooks like output->copyMetadata(input) API is giving the same debugName to the output. So the name of the original input is changed. This update avoid the name change by just copying the type.\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D26050880\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 8b04e41e6df7f33c5c9c873fb323c21462fc125b",
    "Number of deleted lines": 11,
    "Deleted lines": "-          auto newNode = node->owningGraph()->create(aten::list, 1);\n-          newNode->output()->copyMetadata(input);\n-          newNode->insertBefore(node);\n-          node->replaceInput(index, newNode->output());\n-          input->replaceAllUsesAfterNodeWith(node, newNode->output());\n-          auto newNode = node->owningGraph()->create(aten::clone, 1);\n-          newNode->output()->copyMetadata(input);\n-\n-          newNode->insertBefore(node);\n-          node->replaceInput(index, newNode->output());\n-          input->replaceAllUsesAfterNodeWith(node, newNode->output());",
    "Added lines": "+        Node* newNode = nullptr;\n+          newNode = node->owningGraph()->create(aten::list, 1);\n+          newNode->output()->setType(input->type());\n+          b->prependNode(newNode);\n+          newNode = node->owningGraph()->create(aten::clone, 1);\n+          newNode->output()->setType(input->type());\n+          b->prependNode(newNode);\n+        node->replaceInput(index, newNode->output());\n+        input->replaceAllUsesAfterNodeWith(node, newNode->output());",
    "Label": "clean"
},
{
    "Id": 1166,
    "Library": "pytorch",
    "Date": "2021/01/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/31194750f2c2d27b83b9351dbf4f1db1e92452c6",
    "Root Cause": "N.A",
    "Bug report": "[jit] Fix ResolutionCallback definition (#51089)\n\nSummary:\n`ResolutionCallback` returns `py::object` (i.e. `Any`) rather than `py::function` (i.e. `Callable`)\n\nDiscovered while debugging test failures after updating pybind11\n\nThis also makes resolution code slightly faster, as it eliminates casts from object to function and back for every `py::object obj = rcb_(name);` statement.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51089\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26069295\n\nPulled By: malfet\n\nfbshipit-source-id: 6876caf9b4653c8dc8e568aefb6778895decea05",
    "Number of deleted lines": 1,
    "Deleted lines": "-using ResolutionCallback = std::function<py::function(std::string)>;",
    "Added lines": "+using ResolutionCallback = std::function<py::object(std::string)>;",
    "Label": "clean"
},
{
    "Id": 1167,
    "Library": "pytorch",
    "Date": "2021/01/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/98e291461472631829bddf40ae7e872455d1a1b0",
    "Root Cause": "N.A",
    "Bug report": "[android] Fix YUV camera image to tensor (#50871)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50871\n\nIssue: https://discuss.pytorch.org/t/trouble-with-yuv420-to-float-tensor-conversion/106721/3\nDecoding was wrong and the result image had artifacts.\n\nTesting:\nPatch test_app with:\n[input_tensor_to_bitmap.txt](https://github.com/pytorch/pytorch/files/5847553/input_tensor_to_bitmap.txt)\n\ngradle -p android test_app:installMnetLocalCameraDebug -PABI_FILTERS=arm64-v8a\n\nBefore fix:\n![before_yuv_fix](https://user-images.githubusercontent.com/6638825/105317604-63a35980-5b90-11eb-9609-2ed5818130bd.png)\n\nAfter fix:\n![after_yuv_fix](https://user-images.githubusercontent.com/6638825/105317643-70c04880-5b90-11eb-88b7-92dd90db8ed2.png)\n\nTest Plan: Imported from OSS\n\nReviewed By: fmassa\n\nDifferential Revision: D25992519\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 4a46ed39c1cd70f8987fcc1023520e9659ae5d59",
    "Number of deleted lines": 7,
    "Deleted lines": "-  int uvRowStride = uRowStride >> 1;\n-  int yIdx, uvIdx, ui, vi, a0, ri, gi, bi;\n-      uvIdx = (yBeforeRtn >> 1) * uvRowStride + xBeforeRtn * uvPixelStride;\n-      a0 = 1192 * (yData[yIdx] - 16);\n-      ri = (a0 + 1634 * (vi - 128)) >> 10;\n-      gi = (a0 - 832 * (vi - 128) - 400 * (ui - 128)) >> 10;\n-      bi = (a0 + 2066 * (ui - 128)) >> 10;",
    "Added lines": "+  int uvRowStride = uRowStride;\n+  int yi, yIdx, uvIdx, ui, vi, a0, ri, gi, bi;\n+      uvIdx = (yBeforeRtn >> 1) * uvRowStride + (xBeforeRtn >> 1) * uvPixelStride;\n+      yi = yData[yIdx];\n+      yi = (yi - 16) < 0 ? 0 : (yi - 16);\n+      ui -= 128;\n+      vi -= 128;\n+      a0 = 1192 * yi;\n+      ri = (a0 + 1634 * vi) >> 10;\n+      gi = (a0 - 833 * vi - 400 * ui) >> 10;\n+      bi = (a0 + 2066 * ui) >> 10;",
    "Label": "clean"
},
{
    "Id": 1168,
    "Library": "pytorch",
    "Date": "2021/01/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/327539ca7913dc2e2623ce43fd98de70830ee4f9",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in hipify if include_dirs is not specified in setup.py (#50703)\n\nSummary:\nBugs:\n1) would introduce -I* in compile commands\n2) wouldn't hipify source code directly in build_dir, only one level down or more\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50703\n\nReviewed By: mrshenli\n\nDifferential Revision: D25949070\n\nPulled By: ngimel\n\nfbshipit-source-id: 018c2a056b68019a922e20e5db2eb8435ad147fe",
    "Number of deleted lines": 3,
    "Deleted lines": "-        if not include_dirs:\n-            include_dirs = ['*']\n-            includes=[os.path.join(os.path.relpath(include_dir, build_dir), '*') for include_dir in include_dirs],",
    "Added lines": "+            includes=[os.path.join(os.path.relpath(include_dir, build_dir), '*') for include_dir in include_dirs] if include_dirs else ['*'],",
    "Label": "clean"
},
{
    "Id": 1169,
    "Library": "pytorch",
    "Date": "2021/01/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/05036564cfe7259f17d83609033c988b708246d9",
    "Root Cause": "N.A",
    "Bug report": "Remove workaround for TensorPipe failing to get device of CUDA ptr (#50580)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50580\n\nDue to what looked like a bug in CUDA, TensorPipe was sometimes failing to auto-detect the device of a CUDA pointer. A workaround, on the PyTorch side, was to always initialize a CUDA context on device 0. Now that TensorPipe has fixed that we can undo the workaround.\n\nReviewed By: mrshenli\n\nDifferential Revision: D25952929\n\nfbshipit-source-id: 57a5f73241f7371661855c767e44a64ca3b84a74",
    "Number of deleted lines": 4,
    "Deleted lines": "-        # FIXME: this is needed for now because TensorPipe calls\n-        # cudaPointerGetAttributes() on the default device.\n-        # This error was also reported in https://github.com/pytorch/pytorch/issues/36594\n-        torch.zeros([1], device=\"cuda:0\")",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1170,
    "Library": "pytorch",
    "Date": "2021/01/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/39aac654306b8f148b0587e11af038c28b79ff63",
    "Root Cause": "N.A",
    "Bug report": "[quant][bug] Fixing the mapping getter to return a copy (#50297)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50297\n\nCurrent implementation has a potential bug: if a user modifies the quantization mappings returned by the getters, the changes will propagate.\nFor example, the bug will manifest itself if the user does the following:\n\n```\nmy_mapping = get_default_static_quant_module_mappings()\nmy_mapping[nn.Linear] = UserLinearImplementation\nmodel_A = convert(model_A, mapping=my_mapping)\n\ndefault_mapping = get_default_static_quant_module_mappings()\nmodel_B = convert(model_B, mapping=default_mapping)\n```\n\nIn that case the `model_B` will be quantized with with the modified mapping.\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D25855753\n\nPulled By: z-a-f\n\nfbshipit-source-id: 0149a0c07a965024ba7d1084e89157a9c8fa1192",
    "Number of deleted lines": 6,
    "Deleted lines": "-    return DEFAULT_STATIC_QUANT_MODULE_MAPPINGS\n-    return static_quant_module_class\n-    return dynamic_quant_module_class\n-    return DEFAULT_QAT_MODULE_MAPPINGS\n-    return QCONFIG_PROPAGATE_MODULE_CLASS_LIST\n-    return NUMERIC_SUITE_COMPARE_MODEL_OUTPUT_MODULE_LIST",
    "Added lines": "+import copy\n+\n+    return copy.deepcopy(DEFAULT_STATIC_QUANT_MODULE_MAPPINGS)\n+    return copy.deepcopy(static_quant_module_class)\n+    return copy.deepcopy(dynamic_quant_module_class)\n+    return copy.deepcopy(DEFAULT_QAT_MODULE_MAPPINGS)\n+    return copy.deepcopy(QCONFIG_PROPAGATE_MODULE_CLASS_LIST)\n+    return copy.deepcopy(NUMERIC_SUITE_COMPARE_MODEL_OUTPUT_MODULE_LIST)",
    "Label": "clean"
},
{
    "Id": 1171,
    "Library": "pytorch",
    "Date": "2021/01/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7377bfb1bdd96d42484221ef62132f391d3a2f13",
    "Root Cause": "N.A",
    "Bug report": "Fix compiler warnings pertaining to uniform_int() (#49914)\n\nSummary:\n**PROBLEM DESCRIPTION:**\nGitHub issue 46391 suggests that compiler warnings pertaining to _floating-point value does not fit in required integral type_ might cause some confusion.\n\nThese compiler-warnings arise during compilation of the templated function `uniform_int()`. The warnings are misleading because they arise from the way the compiler compiles templated functions, but the if-else statements in the function obviate the possibilities that the warnings describe. So, the purpose of a fix would only be to fix the compiler warnings, and not to fix any sort of a bug.\n\n**FIX DESCRIPTION:**\n[EDITED, after inputs from malfet]: In the function `uniform_int()`, the if-else conditions pertaining to types `double` & `float` can be removed, and then an overloaded specialized function can be added for floating-point types. The current version of the function can be specialized to not have its return type as a floating point type.\n\nAn unrelated observation is that the if-else condition pertaining to the type `double` (line 57 in the original code) was redundant, as line 61 in the original code covered it (`std::is_floating_point<T>::value` would also have been true for the type `double`).\n\nFixes https://github.com/pytorch/pytorch/issues/46391\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49914\n\nReviewed By: H-Huang\n\nDifferential Revision: D25808037\n\nPulled By: malfet\n\nfbshipit-source-id: 3f94c4bca877f09720b0d6efa5e1788554aba074",
    "Number of deleted lines": 4,
    "Deleted lines": "-C10_HOST_DEVICE inline T uniform_int(V val) {\n-  } else if (std::is_same<T, double>::value) {\n-    return static_cast<T>(val % static_cast<uint64_t>((1ULL << std::numeric_limits<T>::digits) + 1));\n-  } else if (std::is_floating_point<T>::value || std::is_same<T, at::Half>::value || std::is_same<T, at::BFloat16>::value) {",
    "Added lines": "+ * In order to prevent compiler warnings reported in GitHub issue 46391, T can't be float or double\n+ * in this overloaded version\n+C10_HOST_DEVICE inline typename std::enable_if<!(std::is_floating_point<T>::value), T>::type uniform_int(V val) {\n+  } else if (std::is_same<T, at::Half>::value || std::is_same<T, at::BFloat16>::value) {\n+/**\n+ * An overloaded transformation function for `torch.Tensor.random_()`, when used without specifying `from` and `to`,\n+ * added to fix compiler warnings reported in GitHub issue 46391. T is either float or double in this version.\n+ */\n+template<typename T, typename V>\n+C10_HOST_DEVICE inline typename std::enable_if<std::is_floating_point<T>::value, T>::type uniform_int(V val) {\n+  return static_cast<T>(val % static_cast<uint64_t>((1ULL << std::numeric_limits<T>::digits) + 1));\n+}\n+",
    "Label": "clean"
},
{
    "Id": 1172,
    "Library": "pytorch",
    "Date": "2021/01/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e49372d460c80102bd6bae3dd2f8c1e2b61ebc1b",
    "Root Cause": "N.A",
    "Bug report": "Bugfix nightly checkout tool to work on Windows (#49274)\n\nSummary:\nI am submitting this PR on behalf of Janne Hellsten(nurpax) from NVIDIA, for the convenience of CLA. Thanks Janne a lot for the contribution!\n\nThis fixes the bug when running `\n./tools/nightly.py checkout -b my-nightly-branch` on windows. Before this fix, this command gets the following error on Windows.\n\n```\nERROR:root:Fatal exception\nTraceback (most recent call last):\n  File \"./tools/nightly.py\", line 166, in logging_manager\n    yield root_logger\n  File \"./tools/nightly.py\", line 644, in main\n    install(\n  File \"./tools/nightly.py\", line 552, in install\n    spdir = _site_packages(pytdir.name, platform)\n  File \"./tools/nightly.py\", line 325, in _site_packages\n    os.path.join(pytdir.name, \"Lib\", \"site-packages\")\nNameError: name 'pytdir' is not defined\nlog file: d:\\pytorch\\nightly\\log\\2020-12-11_16h10m14s_6867a21e-3c0e-11eb-878e-04ed3363a33e\\nightly.log\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49274\n\nReviewed By: H-Huang\n\nDifferential Revision: D25808156\n\nPulled By: malfet\n\nfbshipit-source-id: 00778016366ab771fc3fb152710c7849210640fb",
    "Number of deleted lines": 2,
    "Deleted lines": "-        os.path.join(pytdir.name, \"Lib\", \"site-packages\")\n-        spdir = glob.glob(template)[0]",
    "Added lines": "+        template = os.path.join(dirname, \"Lib\", \"site-packages\")\n+    spdir = glob.glob(template)[0]",
    "Label": "clean"
},
{
    "Id": 1173,
    "Library": "pytorch",
    "Date": "2021/01/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/57d489e43a5b915cdb4bd8a16112ac68eb792581",
    "Root Cause": "N.A",
    "Bug report": "Fix for possible RNG offset calculation bug in cuda vectorized dropout with VEC=2 (#50110)\n\nSummary:\nThe [offset calculation](https://github.com/pytorch/pytorch/blob/e3c56ddde67ca1a49159ffa886d889b6e65c7033/aten/src/ATen/native/cuda/Dropout.cu#L328) (which gives an estimated ceiling on the most 32-bit values in the philox sequence any thread in the launch will use) uses the hardcoded UNROLL value of 4, and assumes the hungriest threads can use every value (.x, .y, .z, and .w) their curand_uniform4 calls provide.  However, the way fused_dropout_kernel_vec is currently written, that assumption isn't true in the VEC=2 case:  Each iteration of the `grid x VEC` stride loop, each thread calls curand_uniform4 once, uses rand.x and rand.y, and discards rand.z and rand.w.  This means (I _think_) curand_uniform4 may be called twice as many times per thread in the VEC=2 case as for the VEC=4 case or the fully unrolled code path, which means the offset calculation (which is a good estimate for the latter two cases) is probably wrong for the `fused_dropout_kernel_vec<..., /*VEC=*/2>` code path.\n\nThe present PR inserts some value-reuse in fused_dropout_kernel_vec to align the number of times curand_uniform4 is called for launches with the same totalElements in the VEC=2 and VEC=4 cases.  The diff should\n- make the offset calculation valid for all code paths\n- provide a very small perf boost by reducing the number of curand_uniform4 calls in the VEC=2 path\n- ~~make results bitwise accurate for all code paths~~ nvm, tensor elements are assigned to threads differently in the unrolled, VEC 2 and VEC 4 cases, so we're screwed here no matter what.\n\nngimel what do you think?\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50110\n\nReviewed By: smessmer\n\nDifferential Revision: D25790121\n\nPulled By: ngimel\n\nfbshipit-source-id: f8f533ad997268c6f323cf4d225de547144247a8",
    "Number of deleted lines": 3,
    "Deleted lines": "-    float4 rand = curand_uniform4(&state);\n-    rand.z = rand.z < p;\n-    rand.w = rand.w < p;",
    "Added lines": "+  // Helps align the total number of times curand_uniform4 is called by each thread for the same totalElements\n+  // in the vec=2 and vec=4 cases.\n+  bool gridxvec_loop_state = 0;\n+\n+  float4 rand;\n+\n+    if ((VEC == 4) || (gridxvec_loop_state == 0)) {\n+      rand = curand_uniform4(&state);\n+    } else {\n+      // sets up the last two values we generated last iteration to be used this iteration.\n+      rand.x = rand.z;\n+      rand.y = rand.w;\n+      gridxvec_loop_state ^= 1;\n+    }\n+    if (VEC == 4) {\n+      rand.z = rand.z < p;\n+      rand.w = rand.w < p;\n+    }",
    "Label": "clean"
},
{
    "Id": 1174,
    "Library": "pytorch",
    "Date": "2020/12/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/edce6b138d92117088fcffe065039df7cac1bddb",
    "Root Cause": "N.A",
    "Bug report": "fx quant: fix types on _find_quants (#49616)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49616\n\nAdd types to `_find_quants` I/O and fix resulting errors,\nneeded for an upcoming bug fix.\n\nTest Plan:\n```\nmypy torch/quantization\npython test/test_quantization.py TestQuantizeFx\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D25645719\n\nfbshipit-source-id: 4bf788b55fd4fd086c83a4438b9c2df22b9cff49",
    "Number of deleted lines": 17,
    "Deleted lines": "-        node: Node, observed_node_names_set: Set[str], quants: Dict[str, Any],\n-        quants = self._find_quants(model.graph, matches)\n-        quants = self._find_quants(model.graph, matches)\n-            if n.name not in quant_env:\n-                assert n.name in env, \\\n-                    'trying to load quantized node but did not find node:' + \\\n-                    n.name + ' in float environment:' + str(env)\n-                assert n.name in quants, \\\n-                    'did not find quant object for node:' + n.name\n-                quant = quants[n.name][0]\n-                quant_env[n.name] = quant.convert(self, env[n.name])\n-                     ) -> Dict[str, Any]:\n-        quants: Dict[str, Any] = {}\n-                    quants[arg.name] = (\n-                        DefaultQuantizeHandler(self, arg), qconfig, is_weight)\n-                    # overwrite previous activation post process constructor if\n-                    # necessary",
    "Added lines": "+        node: Node, observed_node_names_set: Set[str],\n+        quants: Dict[str, Tuple[DefaultQuantizeHandler, Callable]],\n+        quants: Dict[str, Tuple[DefaultQuantizeHandler, Callable]] = \\\n+            self._find_quants(model.graph, matches)\n+        quants: Dict[str, Tuple[DefaultQuantizeHandler, Callable]] = \\\n+            self._find_quants(model.graph, matches)\n+            assert n.name in quant_env, \\\n+                'trying to load quantized node but did not find node:' + \\\n+                n.name + ' in quant environment:' + str(quant_env)\n+                     ) -> Dict[str, Tuple[DefaultQuantizeHandler, Callable]]:\n+        quants: Dict[str, Tuple[DefaultQuantizeHandler, Callable]] = {}",
    "Label": "clean"
},
{
    "Id": 1175,
    "Library": "pytorch",
    "Date": "2020/12/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc92f25b383ee4480ca1a87780482d5c7e8ac926",
    "Root Cause": "N.A",
    "Bug report": "[te] Use c10::ScalarType utility functions in te::Dtype (#49148)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49148\n\nInstead of defining our own variants.  I'm pretty sure this fixes a bug too, in that Bfloat16 wasn't being considered FP.  Otoh, I don't think it's possible to create TEs with Bfloat so...\nghstack-source-id: 118415314\n\nTest Plan: `buck test //caffe2/test:jit`\n\nReviewed By: robieta\n\nDifferential Revision: D25456767\n\nfbshipit-source-id: bd5822114b76c4fde82f566308909bd2a55f4f21",
    "Number of deleted lines": 27,
    "Deleted lines": "-  switch (type) {\n-    case ScalarType::Bool:\n-    case ScalarType::Byte:\n-    case ScalarType::Char:\n-    case ScalarType::Short:\n-    case ScalarType::Int:\n-    case ScalarType::Long:\n-      return true;\n-    default:\n-      return false;\n-  }\n-\n-  return false;\n-  switch (type) {\n-    case ScalarType::Half:\n-    case ScalarType::Float:\n-    case ScalarType::Double:\n-      return true;\n-    default:\n-      return false;\n-  }\n-\n-  return false;\n-  if (is_c10_type(type)) {\n-    return c10::isSignedType(static_cast<c10::ScalarType>(type));\n-  }\n-  return false;",
    "Added lines": "+  return is_c10_type(type)\n+      ? c10::isIntegralType(static_cast<c10::ScalarType>(type), true)\n+      : false;\n+  return is_c10_type(type)\n+      ? c10::isFloatingType(static_cast<c10::ScalarType>(type))\n+      : false;\n+  return is_c10_type(type)\n+      ? c10::isSignedType(static_cast<c10::ScalarType>(type))\n+      : false;",
    "Label": "clean"
},
{
    "Id": 1176,
    "Library": "pytorch",
    "Date": "2020/12/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e69c2f85f6ff14598614fe364b753b01581d723f",
    "Root Cause": "N.A",
    "Bug report": "Add version_info tuple (#48414)\n\nSummary:\nAdd a `version_info` similar to `sys.version_info` for being able to make version tests. Example generated `version.py`:\n\n```\n__version__ = '1.8.0a0'\nversion_info = (1, 8, 0, 'a0')\n# or version_info = (1, 8, 0, 'a0', 'deadbeef') if you're in a Git checkout\ndebug = False\ncuda = None\ngit_version = '671ee71ad4b6f507218d1cad278a8e743780b716'\nhip = None\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48414\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D25416620\n\nPulled By: malfet\n\nfbshipit-source-id: 20b561a0c76ac0b16ff92f4bd43f8b724971e444",
    "Number of deleted lines": 2,
    "Deleted lines": "-    return version\n-    version = get_torch_version(sha)",
    "Added lines": "+\n+    first_non_numeric = min(i for i, c in enumerate(version) if c not in \"0123456789.\")\n+    version_suffix = version[first_non_numeric:]\n+    version_info = tuple(\n+        [int(part) for part in version[:first_non_numeric].split(\".\")]\n+        + version_suffix.split(\"+\")\n+    )\n+\n+    return version, version_info\n+    version, version_info = get_torch_version(sha)\n+        f.write(\"version_info = {}\\n\".format(version_info))",
    "Label": "clean"
},
{
    "Id": 1177,
    "Library": "pytorch",
    "Date": "2020/12/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ca3ae7dc73e714329e646a9456e4426ef8e56763",
    "Root Cause": "N.A",
    "Bug report": "[DI] create a new key for threadLocalDebugInfo (#48762)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48762\n\nIn distributed inference, we want to use a new type info to pass some information to operators. add a new key to threadLocalDebugInfo to unblock the development.\n\nTest Plan: Only add a new key. Should have not effect on current build.\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D25291242\n\nfbshipit-source-id: c71565ff7a38cc514d7cd65246c7d5f6b2ce3b8b",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  INFERENCE_CONTEXT, // for inference usage",
    "Label": "clean"
},
{
    "Id": 1178,
    "Library": "pytorch",
    "Date": "2020/12/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cf1e5d7d2b72d0a9ee69e70683ffcd213b12b99c",
    "Root Cause": "N.A",
    "Bug report": "Ignore MSVC's pdb file (#47963)\n\nSummary:\nThese files are generated by MSVC when building with debug symbols `REL_WITH_DEB_INFO=1`:\n```\nPS C:\\Users\\Xiang Gao\\source\\repos\\pytorch> git status\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        torch/lib/asmjit.pdb\n        torch/lib/c10.pdb\n        torch/lib/c10_cuda.pdb\n        torch/lib/caffe2_detectron_ops_gpu.pdb\n        torch/lib/caffe2_module_test_dynamic.pdb\n        torch/lib/caffe2_observers.pdb\n        torch/lib/fbgemm.pdb\n        torch/lib/shm.pdb\n        torch/lib/torch_cpu.pdb\n        torch/lib/torch_cuda.pdb\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47963\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D25311564\n\nPulled By: malfet\n\nfbshipit-source-id: 1a7125f3c6ff296b4bb0975ee97b59c23586b1cb",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+torch/lib/*.pdb",
    "Label": "clean"
},
{
    "Id": 1179,
    "Library": "pytorch",
    "Date": "2020/11/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7f869dca70606c42994d822ba11362a353411a1c",
    "Root Cause": "N.A",
    "Bug report": "[ROCm] update debug flags (#46717)\n\nSummary:\nImproves support for rocgdb when setting DEBUG=1 and building for ROCm.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/46717\n\nReviewed By: mrshenli\n\nDifferential Revision: D25171544\n\nPulled By: malfet\n\nfbshipit-source-id: b4699ba2277dcb89f07efb86f7153fae82a80dc3",
    "Number of deleted lines": 1,
    "Deleted lines": "-       list(APPEND HIP_CXX_FLAGS -g)",
    "Added lines": "+       list(APPEND HIP_CXX_FLAGS -g2)\n+       list(APPEND HIP_HIPCC_FLAGS -fdebug-info-for-profiling)",
    "Label": "clean"
},
{
    "Id": 1180,
    "Library": "pytorch",
    "Date": "2020/11/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7828a220947fb06de07a04b17e89addbb52ff18b",
    "Root Cause": "N.A",
    "Bug report": "fix a bug in leakyReLU (#48265)\n\nSummary:\nThe scale variable needs to be a scalar, otherwise it will report the following error: \"RuntimeError: Cannot input a tensor of dimension other than 0 as a scalar argument\"\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48265\n\nTest Plan: Tested locally and the error disappeared.\n\nReviewed By: zhizhengwu\n\nDifferential Revision: D25105423\n\nPulled By: jerryzh168\n\nfbshipit-source-id: 2a0df24cf7e40278a950bffe6e0a9552f99da1d1",
    "Number of deleted lines": 2,
    "Deleted lines": "-        self.register_buffer('scale', torch.tensor([scale]))\n-        self.register_buffer('zero_point', torch.tensor([zero_point]))",
    "Added lines": "+        self.register_buffer('scale', torch.tensor(scale))\n+        self.register_buffer('zero_point', torch.tensor(zero_point))",
    "Label": "clean"
},
{
    "Id": 1181,
    "Library": "pytorch",
    "Date": "2020/11/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fe6bb2d287ab039127d2443bbb85b6152fc55bc4",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch] Declare the instantiation of PackedConvWeightsQnnp<2>::prepack (#48256)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48256\n\n`PackedConvWeightsQnnp<2>::prepack` is referenced by both `quantized::conv_prepack` and fbgemm.cpp. Since `quantized::conv_prepack` is in the same compilation unit as the class template it was fine. However, if we make operator registration selective, the reference from `quantized::conv_prepack` is gone. The reference from fbgemm.cpp is in another compilation unit and there is link error.\n\nTo avoid the link error, instantiate the symbol in the cpp file. It should also work to move all implementations to .h file, but to keep the existing code structure and to avoid (small chance of) code bloat, the implementations are kept as is.\nghstack-source-id: 117123564\n\nTest Plan:\nCI\nbuck build //fbandroid/apps/oculus/assistant:assistant_arm64_debug\n\nReviewed By: dhruvbird\n\nDifferential Revision: D24941989\n\nfbshipit-source-id: adc96d0e55c89529fb71a43352aa68a1088a62a2",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+template\n+c10::intrusive_ptr<ConvPackedParamsBase<2>> PackedConvWeightsQnnp<\n+    2>::\n+    prepack(\n+        at::Tensor weight,\n+        c10::optional<at::Tensor> bias_in,\n+        torch::List<int64_t> stride,\n+        torch::List<int64_t> padding,\n+        torch::List<int64_t> output_padding,\n+        torch::List<int64_t> dilation,\n+        int64_t groups,\n+        bool transpose);",
    "Label": "clean"
},
{
    "Id": 1182,
    "Library": "pytorch",
    "Date": "2020/11/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0adace370671735cf5be2fe42a8424dfa2983406",
    "Root Cause": "N.A",
    "Bug report": "fix calculate_extra_mem_bytes_needed_for (#48102)\n\nSummary:\nThis PR fixes a bug in calculate_extra_mem_bytes_needed_for in get_device_to_partitions_mapping\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48102\n\nReviewed By: gcatron\n\nDifferential Revision: D25029059\n\nPulled By: scottxu0730\n\nfbshipit-source-id: 7447b70e8da96b3dc2c5922cf9b62eb306877317",
    "Number of deleted lines": 9,
    "Deleted lines": "-            if node in all_nodes or node.op in {'placeholder', 'get_attr'}:\n-                continue\n-            else:\n-                extra_size_needed += get_extra_size_of(node, all_nodes)\n-                get_bfs_level_partition(self.partitions)\n-                get_device_to_partitions_mapping(self.partitions, self.devices)\n-                return True\n-            return False\n-",
    "Added lines": "+def reset_partition_device(partitions):\n+    for partition in partitions:\n+        partition.logical_device_ids = []\n+        if len(all_nodes) == 0:\n+            return partition.used_mem_bytes\n+        all_nodes = all_nodes.union(partition.nodes)\n+            extra_size_needed += get_extra_size_of(node, all_nodes)\n+                reset_partition_device(partitions)\n+                    reorganize_partitions(self.partitions)\n+            get_bfs_level_partition(self.partitions)\n+            reset_partition_device(self.partitions)\n+            get_device_to_partitions_mapping(self.partitions, self.devices)\n+            return len(partition_pair) != 0",
    "Label": "clean"
},
{
    "Id": 1183,
    "Library": "pytorch",
    "Date": "2020/11/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1aeac9771221bcc72e2b10ce5ba4e73b887d03f3",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch] Remove unnecessary shared_ptr copies in ThreadLocalDebugInfo::get (#47791)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47791\n\n`debug_info` is `thread_local` and this function is a leaf, so nobody else could free it out from under us. Regular pointer should be fine.\nghstack-source-id: 116456975\n\nTest Plan: Run framework overhead benchmarks\n\nReviewed By: bhosmer\n\nDifferential Revision: D24901749\n\nfbshipit-source-id: c01a60b609fd08e5200264d8e98d356e2c78cf28",
    "Number of deleted lines": 2,
    "Deleted lines": "-  auto cur = debug_info;\n-    cur = cur->parent_info_;",
    "Added lines": "+  ThreadLocalDebugInfo* cur = debug_info.get();\n+    cur = cur->parent_info_.get();",
    "Label": "clean"
},
{
    "Id": 1184,
    "Library": "pytorch",
    "Date": "2020/11/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e8a73fbf34b796f270f8d07ea0b5d8643beeb302",
    "Root Cause": "N.A",
    "Bug report": "Workaround PyTorch debug build crash using old GCC (#47805)\n\nSummary:\ngcc-7.4.x or older fails to compile XNNPACK in debug mode with internal compiler error\nWorkaround this in a build script by pasing -O1 optimisation flag to XNNPACK if compiled on older compilers\n\nFixes https://github.com/pytorch/pytorch/issues/47292\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47805\n\nReviewed By: seemethere\n\nDifferential Revision: D24905758\n\nPulled By: malfet\n\nfbshipit-source-id: 93f4e3b3b5c10b69734627c50e36b2eb544699c8",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    # Workaround for https://github.com/pytorch/pytorch/issues/47292\n+    if(CMAKE_BUILD_TYPE STREQUAL \"Debug\" AND CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.5.0))\n+      # Compiling qu8-requantization/precise-psimd.c without any optimization flags on gcc-7.4 or older i\n+      # Fails with internal compiler error\n+      # Workaround by forcing -O1 for XNNPACK (i.e. build it with RelWithDebInfo)\n+      set_property(TARGET XNNPACK APPEND_STRING PROPERTY COMPILE_FLAGS \"-O1\")\n+    endif()",
    "Label": "clean"
},
{
    "Id": 1185,
    "Library": "pytorch",
    "Date": "2020/11/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8e3af9faa891b2b6b206035db4ddb7ccabc348df",
    "Root Cause": "N.A",
    "Bug report": "[pytorch] fix debug symbol flag for android clang (#46331)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/46331\n\nFix the android build size issue #46246.\n\nTest Plan: Imported from OSS\n\nReviewed By: dhruvbird\n\nDifferential Revision: D24390061\n\nPulled By: ljk53\n\nfbshipit-source-id: b4a6f297e89b9c08dff4297c6a41aabd41d9fff5",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  elseif(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n+    string(APPEND CMAKE_CXX_FLAGS \" -g0\")",
    "Label": "clean"
},
{
    "Id": 1186,
    "Library": "pytorch",
    "Date": "2020/11/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/373246733d05b978eb29426c7b24132d4caf8793",
    "Root Cause": "N.A",
    "Bug report": "[FX] get the correct error message (#47108)\n\nSummary:\nCurrently, code like\n```\nclass Test(nn.Module):\n    def __init__(self):\n        super(Test, self).__init__()\n        self.W = torch.nn.Parameter(torch.randn(5))\n\n    def forward(self, x):\n        return torch.dot(self.W, x)\n\nmod = Test()\nprint(fx.symbolic_trace(Test())(5))\n```\ngives an error like the below, which does not show the actual code that throws the error.\n```\nTraceback (most recent call last):\n  File \"t.py\", line 20, in <module>\n    print(fx.symbolic_trace(Test())(5))\n  File \"/home/chilli/fb/pytorch/torch/nn/modules/module.py\", line 744, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/chilli/fb/pytorch/torch/fx/graph_module.py\", line 191, in debug_forward\n    return src_forward(self, *args, **kwargs)\n  File \"<eval_with_key_0>\", line 5, in forward\nTypeError: dot(): argument 'tensor' (position 2) must be Tensor, not int\n```\n\nThis is particularly annoying when your function has already been transformed several times.\n\nSo, the really annoying thing is that the error clearly has the requisite information in `exception.__traceback__` - it just isn't printing it.\n\nI think the right way of doing this is simply replacing `sys.excepthook`. This appears to be the standard way to modify exception messages.\n\n**Scratch the below**\n\nThe 2 methods in the PR right now are:\n1. Just prepend the final part of the traceback to the beginning of your error message. Looks like\n```\nTraceback (most recent call last):\n  File \"t.py\", line 20, in <module>\n    print(fx.symbolic_trace(Test())(5))\n  File \"/home/chilli/fb/pytorch/torch/nn/modules/module.py\", line 744, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/chilli/fb/pytorch/torch/fx/graph_module.py\", line 197, in debug_forward\n    raise e\n  File \"/home/chilli/fb/pytorch/torch/fx/graph_module.py\", line 192, in debug_forward\n    return src_forward(self, *args, **kwargs)\n  File \"<eval_with_key_0>\", line 5, in forward\nTypeError:   File \"<eval_with_key_0>\", line 5, in forward\n    dot_1 = torch.dot(w, x)\ndot(): argument 'tensor' (position 2) must be Tensor, not int\n```\n\n2. Use the `from exception` feature in Python. Looks like\n```\nTraceback (most recent call last):\n  File \"/home/chilli/fb/pytorch/torch/fx/graph_module.py\", line 192, in debug_forward\n    return src_forward(self, *args, **kwargs)\n  File \"<eval_with_key_0>\", line 5, in forward\nTypeError:   File \"<eval_with_key_0>\", line 5, in forward\n    dot_1 = torch.dot(w, x)\ndot(): argument 'tensor' (position 2) must be Tensor, not int\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"t.py\", line 20, in <module>\n    print(fx.symbolic_trace(Test())(5))\n  File \"/home/chilli/fb/pytorch/torch/nn/modules/module.py\", line 744, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/chilli/fb/pytorch/torch/fx/graph_module.py\", line 197, in debug_forward\n    raise Exception(last_tb) from e\nException:   File \"<eval_with_key_0>\", line 5, in forward\n    dot_1 = torch.dot(w, x)\n```\n\nI think the first one looks better, but it's pretty hacky since we're shoving the traceback in the message.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47108\n\nReviewed By: jamesr66a\n\nDifferential Revision: D24751019\n\nPulled By: Chillee\n\nfbshipit-source-id: 83e6ed0165f98632a77c73de75504fd6263fff40",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+import sys\n+import traceback\n+        cls_call = cls.__call__\n+\n+        def print_full_traceback(exctype, value, tb):\n+            traceback.print_exception(exctype, value, tb)\n+\n+        def wrapped_call(self, *args, **kwargs):\n+            old_excepthook = sys.excepthook\n+            try:\n+                sys.excepthook = print_full_traceback\n+                return cls_call(self, *args, **kwargs)\n+            finally:\n+                sys.excepthook = old_excepthook\n+        cls.__call__ = wrapped_call\n+",
    "Label": "clean"
},
{
    "Id": 1187,
    "Library": "pytorch",
    "Date": "2020/11/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/084b71125f91d4dcac96dc9229f4e7c0ddeb4001",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in toComplexWithDefault (#43841)\n\nSummary:\nI don't think this method is used anywhere, so I don't know how to test it. But the diff should justify itself.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43841\n\nReviewed By: mruberry\n\nDifferential Revision: D24696505\n\nPulled By: anjali411\n\nfbshipit-source-id: f2a249ae2e078b16fa11941a048b7d093e60241b",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return toDouble(i);",
    "Added lines": "+  return toComplex(i);",
    "Label": "clean"
},
{
    "Id": 1188,
    "Library": "pytorch",
    "Date": "2020/10/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/058f43fc514f0505ffd78decbd3191edaf676449",
    "Root Cause": "N.A",
    "Bug report": "Fix torch.version.debug generation (#47006)\n\nSummary:\nargparser type bool returns True for any argument passed as input\n\nUse `distutils.util.strtobool` which returns 0 for input values like \"0\", \"no\", \"n\", \"f\", \"false\" and 1 for \"1\", \"yes\", \"y\", \"t\", \"true\"\n\nFixes https://github.com/pytorch/pytorch/issues/46973 and https://github.com/pytorch/pytorch/issues/47003\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47006\n\nReviewed By: samestep\n\nDifferential Revision: D24598193\n\nPulled By: malfet\n\nfbshipit-source-id: e8f6688d6883011f301b49a0f03c452c611f7001",
    "Number of deleted lines": 2,
    "Deleted lines": "-    parser.add_argument(\"--is_debug\", type=bool, help=\"Whether this build is debug mode or not.\")\n-        f.write(\"debug = {}\\n\".format(repr(args.is_debug)))",
    "Added lines": "+from distutils.util import strtobool\n+    parser.add_argument(\"--is_debug\", type=strtobool, help=\"Whether this build is debug mode or not.\")\n+        f.write(\"debug = {}\\n\".format(repr(bool(args.is_debug))))",
    "Label": "clean"
},
{
    "Id": 1189,
    "Library": "pytorch",
    "Date": "2020/10/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cbf90dafe1cdb4f3f87b7a0804b601dc41bceb5b",
    "Root Cause": "N.A",
    "Bug report": "Fix CPUCaching allocator guard bug (#46922)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/46922\n\nEarlier bug wrongly captures the previous value to be saved.\n\nTest Plan: cpu_caching_allocator_test\n\nReviewed By: dreiss\n\nDifferential Revision: D24566514\n\nfbshipit-source-id: 734a4c1f810bbec16fe007f31fffa360898955ac",
    "Number of deleted lines": 1,
    "Deleted lines": "-  caching_allocator_ptr = allocator;",
    "Added lines": "+  caching_allocator_ptr = allocator;",
    "Label": "clean"
},
{
    "Id": 1190,
    "Library": "pytorch",
    "Date": "2020/10/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/37da6d26ffc3a5d1c3dcdc48358d4569f6e20123",
    "Root Cause": "N.A",
    "Bug report": "add fburl link to error message (#46795)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/46795\n\nadd fburl link to the error message of missing ops so user can debug themselves.\n\nTest Plan: fburl.com/missing_ops\n\nReviewed By: iseeyuan\n\nDifferential Revision: D24519992\n\nfbshipit-source-id: d2d16db7e9d9c84ce2c4600532eb253c30b31971",
    "Number of deleted lines": 2,
    "Deleted lines": "-      \"May need to add them explicitly to the selective build operator whitelist, \",\n-      \"or re-run the export_opnames to update the whitelist:\",",
    "Added lines": "+      \"Check fburl.com/missing_ops for the fix.\",",
    "Label": "clean"
},
{
    "Id": 1191,
    "Library": "pytorch",
    "Date": "2020/10/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a602811da748783f310c85abb898bf356a5edf66",
    "Root Cause": "N.A",
    "Bug report": "[ROCm] fix bug in miopen findAlgorithm. (#46852)\n\nSummary:\nfindAlgorithm should return if and only if a suitable algorithm is found.\nThe default algorithm is not guaranteed to have been cached.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/46852\n\nReviewed By: izdeby\n\nDifferential Revision: D24546748\n\nPulled By: bhosmer\n\nfbshipit-source-id: 171137b377193e0825769b61d42a05016f02c34c",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return;",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1192,
    "Library": "pytorch",
    "Date": "2020/10/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/511f89eaa9a99f619001a6a95bb0b62ef71555d2",
    "Root Cause": "N.A",
    "Bug report": "Add nvtx.range() context manager (#42925)\n\nSummary:\nSmall quality-of-life improvement to NVTX Python bindings, that we're using internally and that would be useful to other folks using NVTX annotations via PyTorch. (And my first potential PyTorch contribution.)\n\nInstead of needing to be careful with try/finally to make sure all your range_push'es are range_pop'ed:\n\n```\nnvtx.range_push(\"Some event\")\ntry:\n    # Code here...\nfinally:\n    nvtx.range_pop()\n```\n\nyou can simply do:\n\n```\nwith nvtx.range(\"Some event\"):\n    # Code here...\n```\n\nor even use it as a decorator:\n\n```\nclass MyModel(nn.Module):\n\n    # Other methods here...\n\n    nvtx.range(\"MyModel.forward()\")\n    def forward(self, *input):\n        # Forward pass code here...\n```\n\nA couple small open questions:\n\n1. I also added the ability to call `msg.format()` inside `range()`, with the intention that, if there is nothing listening to NVTX events, we should skip the string formatting, to lower the overhead in that case. If you like that idea, I could add the actual \"skip string formatting if nobody is listening to events\" parts. We can also just leave it as is. Or I can remove that if you folks don't like it. (In the first two cases, should we add that to `range_push()` and `mark()` too?) Just let me know which one it is, and I'll update the pull request.\n\n2. I don't think there are many places for bugs to hide in that function, but I can certainly add a quick test, if you folks want.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42925\n\nReviewed By: gchanan\n\nDifferential Revision: D24476977\n\nPulled By: ezyang\n\nfbshipit-source-id: 874882818d958e167e624052e42d52fae3c4abf1",
    "Number of deleted lines": 1,
    "Deleted lines": "-__all__ = ['range_push', 'range_pop', 'mark']",
    "Added lines": "+from contextlib import contextmanager\n+\n+__all__ = ['range_push', 'range_pop', 'mark', 'range']\n+\n+\n+@contextmanager\n+def range(msg, *args, **kwargs):\n+    \"\"\"\n+    Context manager / decorator that pushes an NVTX range at the beginning\n+    of its scope, and pops it at the end. If extra arguments are given,\n+    they are passed as arguments to msg.format().\n+\n+    Arguments:\n+        msg (string): message to associate with the range\n+    \"\"\"\n+    range_push(msg.format(*args, **kwargs))\n+    yield\n+    range_pop()",
    "Label": "clean"
},
{
    "Id": 1193,
    "Library": "pytorch",
    "Date": "2020/10/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a38eeeff5c2c6306bad9822aec494da9e2165867",
    "Root Cause": "N.A",
    "Bug report": "Make setup.py python 2 friendly (#46317)\n\nSummary:\nimport print_function to make setup.py invoked by Python2 print human readable error:\n```\n% python2 setup.py\nPython 2 has reached end-of-life and is no longer supported by PyTorch.\n```\nAlso, remove `future` from the list of the PyTorch package install dependencies\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/46317\n\nReviewed By: walterddr, bugra\n\nDifferential Revision: D24305004\n\nPulled By: malfet\n\nfbshipit-source-id: 9181186170562384dd2c0e6a8ff0b1e93508f221",
    "Number of deleted lines": 2,
    "Deleted lines": "-\n-    'future',",
    "Added lines": "+# This future is needed to print Python2 EOL message\n+from __future__ import print_function",
    "Label": "clean"
},
{
    "Id": 1194,
    "Library": "pytorch",
    "Date": "2020/10/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6ef41953e62d20ce34f2fc3494ed01c6614351be",
    "Root Cause": "N.A",
    "Bug report": "[RFC] Generate generated_unboxing_wrappers_everything.cpp for unboxing wrappers codegen to aid debugging (#45872)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45872\n\n`VariableType_N.cpp` is generated in a sharded manner to speed up compilationt time. Same for `generated_unboxing_wrappers_N.cpp`. However, `VariableTypeEverything.cpp` exists, but `generated_unboxing_wrappers_everything.cpp` does not. These files have all the registration/implementation code in them for easier debugging of codegen logic.\n\nThis diff adds `generated_unboxing_wrappers_everything.cpp`.\n\nghstack-source-id: 113606771\n\nTest Plan: Build + CI\n\nReviewed By: iseeyuan\n\nDifferential Revision: D24124405\n\nfbshipit-source-id: 1f6c938105e17cd4b14502978483a1b178c777dd",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+from functools import reduce\n+    all_shards = reduce(\n+        lambda lhs, rhs: lhs + rhs,\n+        shards,\n+    )\n+    env = {\n+        'constructors': all_shards,\n+    }\n+    write(out, 'generated_unboxing_wrappers_everything.cpp', GENERATED_UNBOXING_WRAPPERS_CPP, env)\n+",
    "Label": "clean"
},
{
    "Id": 1195,
    "Library": "pytorch",
    "Date": "2020/09/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f2c2b75e80a2ac23fe7c7717ff5b63ad9feb841f",
    "Root Cause": "N.A",
    "Bug report": "flush the buffer when printing the IR (#45585)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45585\n\nI discovered this bug when I was trying to print the graph to a file. Turns out I had to close the file, but flushing should be a good safeguard in case other users forget.\n\nTest Plan:\nTested with and without flushing.\nwith P144064292\nwithout P144064767\n\nReviewed By: mortzur\n\nDifferential Revision: D24023819\n\nfbshipit-source-id: 39574b3615feb28e5b5939664c04ddfb1257706a",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  out.flush();\n+",
    "Label": "clean"
},
{
    "Id": 1196,
    "Library": "pytorch",
    "Date": "2020/09/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6befc09465bf3d4d602ff3870ff339fcdf5847b0",
    "Root Cause": "N.A",
    "Bug report": "Fix misuse of PyObject_IsSubclass (#44769)\n\nSummary:\nPyObject_IsSubclass may set python live exception bit if given object is not a class. `IsNamedTuple` is currently using it incorrectly, which may trip all following python operations in debug-build python. Normal release-build python is not affected because `assert` is no-op in release-build.\n\nFixes https://github.com/pytorch/pytorch/issues/43577\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44769\n\nReviewed By: jamesr66a\n\nDifferential Revision: D23725584\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 2dabd4f8667a045d5bf75813500876c6fd81542b",
    "Number of deleted lines": 7,
    "Deleted lines": "-  return PyObject_IsSubclass(obj.ptr(), tuple_type) &&\n-      py::hasattr(obj, \"_fields\");\n-  py::bool_ is_class = py::module::import(\"inspect\").attr(\"isclass\")(obj);\n-  if (!py::cast<bool>(is_class)) {\n-    return false;\n-  }\n-",
    "Added lines": "+  int is_tuple_class = PyObject_IsSubclass(obj.ptr(), tuple_type);\n+  if (is_tuple_class == -1) {\n+    PyErr_Clear();\n+    return false;\n+  }\n+  return is_tuple_class == 1 && py::hasattr(obj, \"_fields\");\n+  if (ret == -1) {\n+    PyErr_Clear();\n+    return false;\n+  }",
    "Label": "clean"
},
{
    "Id": 1197,
    "Library": "pytorch",
    "Date": "2020/09/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a011b86115541365ebd55598f85ff9a42a6875d8",
    "Root Cause": "N.A",
    "Bug report": "change self.generator to generator (#44461)\n\nSummary:\nbug fix\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44461\n\nReviewed By: mruberry\n\nDifferential Revision: D23725053\n\nPulled By: ngimel\n\nfbshipit-source-id: 89706313013d9eae96aaaf144924867457efd2c0",
    "Number of deleted lines": 1,
    "Deleted lines": "-            yield from torch.randperm(n, generator=self.generator).tolist()",
    "Added lines": "+            yield from torch.randperm(n, generator=generator).tolist()",
    "Label": "clean"
},
{
    "Id": 1198,
    "Library": "pytorch",
    "Date": "2020/09/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/21a09ba94d66f8dd5f3d5fd01d0fd83126191955",
    "Root Cause": "N.A",
    "Bug report": "Fix lerp.cu bug when given discontiguous out tensor (#44559)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44559\n\nPlease refer to the discussion at the bottom of https://github.com/pytorch/pytorch/pull/43541 about the bug.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D23655403\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 10e4ce5c2fe7bf6e95bcfac4033202430292b03f",
    "Number of deleted lines": 2,
    "Deleted lines": "-  result.resize_as_(b_self, b_self.suggest_memory_format());\n-  result.resize_as_(b_self, b_self.suggest_memory_format());",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1199,
    "Library": "pytorch",
    "Date": "2020/09/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d191caa3e7f736bca2a1bd8276e127369844fadf",
    "Root Cause": "N.A",
    "Bug report": "Cleanup workarounds for compiler bug of ROCm (#44579)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44579\n\nReviewed By: mruberry\n\nDifferential Revision: D23664481\n\nPulled By: ngimel\n\nfbshipit-source-id: ef698f26455e5827c5b5c0e5d42a1c95bcac8af4",
    "Number of deleted lines": 26,
    "Deleted lines": "-#ifdef __HIP_PLATFORM_HCC__\n-template<typename T, int size>\n-struct ROCm_Bug {\n-  char bytes[sizeof(T) * size];\n-  __device__ T& operator[](int i) {\n-    return *reinterpret_cast<T *>(&bytes[i * sizeof(T)]);\n-  }\n-};\n-#endif\n-\n-#ifndef __HIP_PLATFORM_HCC__\n-#else\n-    ROCm_Bug<arg_t, input_vec_size> value_list;\n-#endif\n-#ifndef __HIP_PLATFORM_HCC__\n-#else\n-    ROCm_Bug<scalar_t, input_vec_size> values;\n-#endif\n-#ifndef __HIP_PLATFORM_HCC__\n-#else\n-    ROCm_Bug<arg_vec_t, vt0> value_list;\n-#endif\n-#ifndef __HIP_PLATFORM_HCC__\n-#else\n-    ROCm_Bug<load_t, vt0> values;\n-#endif",
    "Added lines": "+\n+",
    "Label": "clean"
},
{
    "Id": 1200,
    "Library": "pytorch",
    "Date": "2020/09/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c10f30647fadab0239e69c1be7aeb8d07a494903",
    "Root Cause": "N.A",
    "Bug report": "Fix CUDA debug nightly build failure (#44085)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/43607.\nTested in https://github.com/pytorch/pytorch/pull/44007.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44085\n\nReviewed By: malfet\n\nDifferential Revision: D23493663\n\nPulled By: ezyang\n\nfbshipit-source-id: 4c01f3fc5a52814a23773a56b980c455851c2686",
    "Number of deleted lines": 2,
    "Deleted lines": "-struct CAFFE2_API VaryingShape {\n-  VaryingShape merge(const VaryingShape& other) const;",
    "Added lines": "+struct VaryingShape {\n+  CAFFE2_API VaryingShape merge(const VaryingShape& other) const;",
    "Label": "clean"
},
{
    "Id": 1201,
    "Library": "pytorch",
    "Date": "2020/09/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b2aaf212aa1a34ef5b9ce170510bd4d6f9f1181f",
    "Root Cause": "N.A",
    "Bug report": "[TensorExpr] Add option to enforce TensorExprKernel fallbacks. (#43972)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43972\n\nIt is useful when debugging a bug to disable NNC backend to see whether\nthe bug is there or in the fuser logic.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D23455624\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: f7c0452a29b860afc806e2d58acf35aa89afc060",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+bool fallbackEnforced() {\n+  static const char* enable_c_str = std::getenv(\"PYTORCH_TENSOREXPR_FALLBACK\");\n+  if (!enable_c_str) {\n+    return fallback_allowed;\n+  }\n+  if (std::string(enable_c_str) == \"2\") {\n+    return true;\n+  }\n+  return false;\n+}\n+\n+  if (fallbackEnforced()) {\n+    fallback(stack);\n+    return;\n+  }",
    "Label": "clean"
},
{
    "Id": 1202,
    "Library": "pytorch",
    "Date": "2020/09/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b167402e2e66a663cd9913885552929b4c045ffa",
    "Root Cause": "N.A",
    "Bug report": "[redo] Fix SyncBatchNorm forward pass for non-default process group (#43861)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43861\n\nThis is a redo of https://github.com/pytorch/pytorch/pull/38874, and\nfixing my original bug from\nhttps://github.com/pytorch/pytorch/pull/38246.\n\nTest Plan:\nCI\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D23418816\n\nfbshipit-source-id: 2a3a3d67fc2d03bb0bf30a87cce4e805ac8839fb",
    "Number of deleted lines": 1,
    "Deleted lines": "-        dist.all_gather(combined_list, combined, async_op=False)",
    "Added lines": "+        dist.all_gather(combined_list, combined, process_group, async_op=False)",
    "Label": "clean"
},
{
    "Id": 1203,
    "Library": "pytorch",
    "Date": "2020/08/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dc5d365514faca63b98c44430d62b813478aee72",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in caching allocator. (#43719)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43719\n\nAccidentally this slipped through: with guard did not update the current\ncontext\n\nTest Plan: cpu_caching_allocator_test\n\nReviewed By: linbinyu\n\nDifferential Revision: D23374453\n\nfbshipit-source-id: 1d3ef21cc390d0a8bde98fb1b5c2175b40ab571b",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  caching_allocator_ptr = allocator;",
    "Label": "clean"
},
{
    "Id": 1204,
    "Library": "pytorch",
    "Date": "2020/08/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5ca6cbbd93a4ad9fdd07f47cfee9ff87967719ff",
    "Root Cause": "N.A",
    "Bug report": "Remove unnecessary copies in ProcessGroupGloo for multiple inputs allreduce (#43543)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43543\n\nCloses https://github.com/pytorch/pytorch/issues/14691. This is not needed in the multiple outputs case, because gloo allreduce\nwill broadcast the result tensor to all the outputs. See\nhttps://github.com/facebookincubator/gloo/issues/152 and commit\nhttps://github.com/facebookincubator/gloo/commit/9cabb5aaa4f02356bc8db05e5630cb550b3f5b5c\nfor more details. Came across this when debugging https://github.com/pytorch/pytorch/pull/42577.\n\nThis effectively reverts https://github.com/pytorch/pytorch/pull/14688 while still keeping the tests.\n\nTested by ensuring `test_allreduce_basics` in `test_c10d.py` still works as expected.\nghstack-source-id: 110636498\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D23173945\n\nfbshipit-source-id: d1ae08f84b4ac9919c53080949b8fffcb2fe63a8",
    "Number of deleted lines": 14,
    "Deleted lines": "-\n-    // Only the first output in the tensor list contains the results.\n-    // See https://github.com/facebookincubator/gloo/issues/152.\n-    // The contents is the same for every entry in the tensor list, so\n-    // we can use the first entry as the source of the copy below.\n-    for (size_t i = 1; i < inputs.size(); i++) {\n-      inputs[i].copy_(inputs[0]);\n-    }\n-    // Kick off copy back to the CUDA tensors.\n-    // Only the first output in the tensor list contains the results.\n-    // See https://github.com/facebookincubator/gloo/issues/152.\n-    // The contents is the same for every entry in the tensor list, so\n-    // we can use the first entry as the source of the copy below.\n-      inputs[i].copy_(tmp[0], /* non_blocking */ true);",
    "Added lines": "+      inputs[i].copy_(tmp[i], /* non_blocking */ true);",
    "Label": "clean"
},
{
    "Id": 1205,
    "Library": "pytorch",
    "Date": "2020/08/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3dcfe84861480acba27165ba31ad62271c5b1110",
    "Root Cause": "N.A",
    "Bug report": "Grammatical corrections (#43473)\n\nSummary:\n**Few documentation corrections.**\n\n1. [...] If there is hard-to-debug error in one of your TorchScript **models**, you can use this flag [...]\n2. [...] Since TorchScript (scripting and tracing) **is** disabled with this flag [...]\n\n**Before corrections (as of now):**\n![before-fix](https://user-images.githubusercontent.com/45713346/90977203-d8bc2580-e543-11ea-9609-fbdf5689dcb9.jpg)\n\n**After corrections:**\n![after-fix](https://user-images.githubusercontent.com/45713346/90977209-dbb71600-e543-11ea-8259-011618efd95b.jpg)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43473\n\nReviewed By: mruberry\n\nDifferential Revision: D23296167\n\nPulled By: ngimel\n\nfbshipit-source-id: 932c9b25cc79d6e266e5ddb3744573b0bd63d925",
    "Number of deleted lines": 2,
    "Deleted lines": "-TorchScript model, you can use this flag to force everything to run using native\n-Python. Since TorchScript (scripting and tracing) are disabled with this flag,",
    "Added lines": "+TorchScript models, you can use this flag to force everything to run using native\n+Python. Since TorchScript (scripting and tracing) is disabled with this flag,",
    "Label": "clean"
},
{
    "Id": 1206,
    "Library": "pytorch",
    "Date": "2020/08/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e4af45f3aa64d214ff789cca0ce627119bc0a4a3",
    "Root Cause": "N.A",
    "Bug report": "Fix bugs in vec256_float_neon.h (#43321)\n\nSummary:\nfixing neon vector conversion problems.\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43321\n\nReviewed By: pbelevich\n\nDifferential Revision: D23241536\n\nPulled By: kimishpatel\n\nfbshipit-source-id: 37a4e10989c9342ae5e8c78f6875b7aad785dd76",
    "Number of deleted lines": 6,
    "Deleted lines": "-  float32x4_t r0 = vreinterpretq_u32_f32(vandq_u32(\n-  float32x4_t r1 = vreinterpretq_u32_f32(vandq_u32(\n-  float32x4_t r0 = vreinterpretq_u32_f32(vorrq_u32(\n-  float32x4_t r1 = vreinterpretq_u32_f32(vorrq_u32(\n-  float32x4_t r0 = vreinterpretq_u32_f32(veorq_u32(\n-  float32x4_t r1 = vreinterpretq_u32_f32(veorq_u32(",
    "Added lines": "+  float32x4_t r0 = vreinterpretq_f32_u32(vandq_u32(\n+  float32x4_t r1 = vreinterpretq_f32_u32(vandq_u32(\n+  float32x4_t r0 = vreinterpretq_f32_u32(vorrq_u32(\n+  float32x4_t r1 = vreinterpretq_f32_u32(vorrq_u32(\n+  float32x4_t r0 = vreinterpretq_f32_u32(veorq_u32(\n+  float32x4_t r1 = vreinterpretq_f32_u32(veorq_u32(",
    "Label": "clean"
},
{
    "Id": 1207,
    "Library": "pytorch",
    "Date": "2020/08/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/aa53b2d4279cac379d8ac327501b8bed1d25dac5",
    "Root Cause": "N.A",
    "Bug report": "Workaround bugs in user side embedding meta info and better msgs (#43355)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43355\n\nThere seem to be some bugs where we cannot guarantees that blobs in `PARAMETERS_BLOB_TYPE_FULLY_REMOTE_REQUEST_ONLY` and `PARAMETERS_BLOB_TYPE_DISAGG_ACC_REMOTE_OTHER` are disjoint. Hence we need to walk around this.\n\nAlso make the msg more informative.\n\nTest Plan:\n```\nflow-cli test-locally --mode opt dper.workflows.evaluation.eval_workflow --parameters-file=/mnt/shared/yinghai/v0_ctr_mbl_feed_1120_onnx.json\n```\n\nReviewed By: ehsanardestani\n\nDifferential Revision: D23141538\n\nfbshipit-source-id: 8e311f8fc0e40eff6eb2c778213f78592e6bf079",
    "Number of deleted lines": 1,
    "Deleted lines": "-      \" needs to be TensorCPU or Int8TensorCPU or Int8FCDNNLowPPackedWeightBlob Based class\");",
    "Added lines": "+      \" needs to be TensorCPU or Int8TensorCPU or Int8FCDNNLowPPackedWeightBlob Based class: \",\n+      blob->TypeName());",
    "Label": "clean"
},
{
    "Id": 1208,
    "Library": "pytorch",
    "Date": "2020/08/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/743cff4a1a3ac0177a6b18575b6de9593a7ae573",
    "Root Cause": "N.A",
    "Bug report": "Fix PackedGemmMatrixFP16 repacking (#43320)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43320\n\nPrevious impl seem to be buggy although I don't why. New impl is copied from https://fburl.com/diffusion/cing6mxv\n\nReviewed By: jianyuh\n\nDifferential Revision: D23235964\n\nfbshipit-source-id: 780b6e388ef895232e3ba34b125c2492b1cee60c",
    "Number of deleted lines": 10,
    "Deleted lines": "-            K,\n-            W->numCols(),\n-            W->blockRowSize(),\n-            W->lastBrow(),\n-            W->blockColSize(),\n-            W->numBrow(),\n-            W->numBcol(),\n-            W->matSize());\n-        // TODO: now we only pack with Transpose=true\n-        packed_w_->packFromSrc(fbgemm::matrix_op_t::Transpose, W->pmat());",
    "Added lines": "+        std::vector<float> src_mat(W->matSize());\n+        for (int i = 0; i < W->matSize(); ++i) {\n+          src_mat[i] =\n+            fbgemm::cpu_half2float(W->pmat()[i]);\n+        }\n+            fbgemm::matrix_op_t::Transpose,\n+            W->numRows(), W->numCols(),\n+            1.0,\n+            src_mat.data());",
    "Label": "clean"
},
{
    "Id": 1209,
    "Library": "pytorch",
    "Date": "2020/08/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/29700c009280ce63c113ad7300171af4cee26174",
    "Root Cause": "N.A",
    "Bug report": "[JIT] Fix torch.jit.is_tracing() (#42486)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42486\n\n**Summary**\nThis commit fixes a small bug in which `torch.jit.is_tracing()` returns\n`torch._C.is_tracing`, the function object, instead of calling the\nfunction and returning the result.\n\n**Test Plan**\nContinuous integration?\n\n**Fixes**\nThis commit fixes #42448.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D22911062\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: b94eca0c1c65ca6f22acc6c5542af397f2dc37f0",
    "Number of deleted lines": 1,
    "Deleted lines": "-    return torch._C._is_tracing",
    "Added lines": "+    return torch._C._is_tracing()",
    "Label": "clean"
},
{
    "Id": 1210,
    "Library": "pytorch",
    "Date": "2020/07/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f8c5800bb5723ef1122d1fb2a6f33b8d2974ee29",
    "Root Cause": "N.A",
    "Bug report": "[TensorExpr] Add debug dumps to kernel.cpp. (#42196)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42196\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D22803676\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 109372ca45d86478826190b868d005d2fb2c9ba7",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  GRAPH_DEBUG(\"Original Stmt:\\n\", std::to_string(l.root_stmt()), \"\\n\");\n+  GRAPH_DEBUG(\"Final Stmt:\\n\", std::to_string(stmt), \"\\n\");",
    "Label": "clean"
},
{
    "Id": 1211,
    "Library": "pytorch",
    "Date": "2020/07/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f30ac66e7970dae2833e390bc623ea29141c8ff9",
    "Root Cause": "N.A",
    "Bug report": "[caffe2] Fix a performance bug in Dedup SparseAdagrad op (#42287)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/42287\n\nWe shouldn't use block_size for thread dimensions in linear_index_weight_offsets_dedup_kernel, since the kernel doesn't iterate the embedding dimensions.\nghstack-source-id: 108834058\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_sum_gradient \\(caffe2\\.caffe2\\.fb\\.net_transforms\\.tests\\.fuse_sparse_ops_test\\.TestFuseSparseOps\\)' --print-passing-details\n```\n\nReviewed By: jspark1105\n\nDifferential Revision: D22800959\n\nfbshipit-source-id: 641d52a51070715c04f9fd286e7e22ac62001f61",
    "Number of deleted lines": 2,
    "Deleted lines": "-  for (int line = start; line < end; ++line) {\n-           std::min(maxThreads, block_size),",
    "Added lines": "+  for (int line = start; line < end; line += threadIdx.x) {\n+           32,",
    "Label": "clean"
},
{
    "Id": 1212,
    "Library": "pytorch",
    "Date": "2020/07/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5336ccc1b280d07b436e7647d22298b16b1e7916",
    "Root Cause": "N.A",
    "Bug report": "[BugFix] Fix bug in onnx::SsaRewrite (#42148)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42148\n\nDifferential Revision: D22687388\n\nfbshipit-source-id: facf7a186dd48d6f919d0ff5d42f756977c3f9f4",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  revertRenamedExternalOutput(if_op, renamed_external_outputs);\n+",
    "Label": "clean"
},
{
    "Id": 1213,
    "Library": "pytorch",
    "Date": "2020/07/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/30ce7b374071653fe3c672ff6000232a89e58569",
    "Root Cause": "N.A",
    "Bug report": "Fix bug when compiling with caffe2 (#41868)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41868\n\nFix bug when compiling with caffe2\n\nReviewed By: jianyuh\n\nDifferential Revision: D22670707\n\nfbshipit-source-id: aa654d7b9004257e0288c8ae8819ca5752eea443",
    "Number of deleted lines": 4,
    "Deleted lines": "-#ifdef REDUCE_BLOCK\n-#define REDUCE_SIZE REDUCE_BLOCK\n-#define REDUCE_SIZE CAFFE_CUDA_NUM_THREADS\n-    // TODO: Not compatible with embedding dim larger than maxThread, set Volta as default",
    "Added lines": "+// Whoever include this header should define REDUCE_BLOCK_SIZE\n+// which is the maximum row-wise length\n+// Default is 1024 (maxThreads per block in Volta GPU)\n+#ifdef REDUCE_BLOCK_SIZE\n+#define REDUCE_SIZE REDUCE_BLOCK_SIZE\n+#define REDUCE_SIZE 1024\n+    // TODO: Not compatible with embedding dim larger than maxThread",
    "Label": "clean"
},
{
    "Id": 1214,
    "Library": "pytorch",
    "Date": "2020/07/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/abea7cd5612f4a723dc4d9fd2a607ac8086487db",
    "Root Cause": "N.A",
    "Bug report": "msvc anonymous namespace bug (#41199)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41199\n\nworkaround for: https://developercommunity.visualstudio.com/content/problem/900452/variable-in-anonymous-namespace-has-external-linka.html\n\nTest Plan: CI green, ovrsource green\n\nReviewed By: malfet\n\nDifferential Revision: D22462050\n\nfbshipit-source-id: 11a2fd6a4db1f29ce350699cfc3121dc89ab7ef6",
    "Number of deleted lines": 17,
    "Deleted lines": "-  };\n-  };\n-ReduceMultiply reduce_multiply;\n-  };\n-ReduceAdd reduce_add;\n-  };  \n-ReduceSubtract reduce_subtract;\n-  \n-  };  \n-ReduceDivide reduce_divide;\n-  };    \n-TensorAssign tensor_assign;\n-    \n-        \n-  \n-  template <typename func_t>  \n-    ",
    "Added lines": "+  }\n+  }\n+static ReduceMultiply reduce_multiply;\n+  }\n+static ReduceAdd reduce_add;\n+  }\n+static ReduceSubtract reduce_subtract;\n+\n+  }\n+static ReduceDivide reduce_divide;\n+  }\n+static TensorAssign tensor_assign;\n+\n+\n+\n+  template <typename func_t>\n+",
    "Label": "clean"
},
{
    "Id": 1215,
    "Library": "pytorch",
    "Date": "2020/07/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e1afa9daffc254bba6a259390e220b51073304d7",
    "Root Cause": "N.A",
    "Bug report": "fix cmake bug (#39930)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/39930\n\nDifferential Revision: D22391207\n\nPulled By: ezyang\n\nfbshipit-source-id: bde19a112846e124d4e5316ba947f48d4dccf361",
    "Number of deleted lines": 8,
    "Deleted lines": "-        if cmake3 is not None:\n-            cmake = which('cmake')\n-            if cmake is not None:\n-                bare_version = CMake._get_version(cmake)\n-                if (bare_version < LooseVersion(\"3.5.0\") and\n-                        CMake._get_version(cmake3) > bare_version):\n-                    cmake_command = 'cmake3'\n-        return cmake_command",
    "Added lines": "+        cmake = which('cmake')\n+        if cmake3 is not None and CMake._get_version(cmake3) >= LooseVersion(\"3.5.0\"):\n+            cmake_command = 'cmake3'\n+            return cmake_command\n+        elif cmake is not None and CMake._get_version(cmake) >= LooseVersion(\"3.5.0\"):\n+            return cmake_command\n+        else:\n+            raise RuntimeError('no cmake or cmake3 with version >= 3.5.0 found')",
    "Label": "clean"
},
{
    "Id": 1216,
    "Library": "pytorch",
    "Date": "2020/06/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5f9e7240f5c41ca7fb96d6a03a7044f2218ee871",
    "Root Cause": "N.A",
    "Bug report": "Fix bug where explicitly providing a namespace never worked. (#40830)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40830\n\nFixes #40725\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22323886\n\nPulled By: ezyang\n\nfbshipit-source-id: b8a61496923d9f086d4c201024748505ba783238",
    "Number of deleted lines": 2,
    "Deleted lines": "-    TORCH_CHECK(false,\n-      *ns_opt == *ns_,",
    "Added lines": "+    TORCH_CHECK(*ns_opt == *ns_,",
    "Label": "clean"
},
{
    "Id": 1217,
    "Library": "pytorch",
    "Date": "2020/06/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7676682584d0caf9243bce74ea0a88711ec4a807",
    "Root Cause": "N.A",
    "Bug report": "Fix illegal opcode bug in caffe2 (#40584)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40584\n\nAlso patch [this github issue](https://github.com/pytorch/pytorch/issues/33124)\ninvolving an illegal assembly instruction in 8x8-dq-aarch64-neon.S.\n\nTest Plan:\nBuild binaries, copy to shaker, run executables. Also run all\nexisting caffe tests.\n\nReviewed By: kimishpatel\n\nDifferential Revision: D22240670\n\nfbshipit-source-id: 51960266ce58699fe6830bcf75632b92a122f638",
    "Number of deleted lines": 8,
    "Deleted lines": "-    MOV V8.4s, V9.4s\n-    MOV v10.4s, v11.4s\n-    MOV v12.4s, V13.4s\n-    MOV V14.4s, V15.4s\n-    MOV V16.4s, V17.4s\n-    MOV V18.4s, V19.4s\n-    MOV V20.4s, V21.4s\n-    MOV V22.4s, V23.4s",
    "Added lines": "+    MOV V8.16b, V9.16b\n+    MOV v10.16b, v11.16b\n+    MOV v12.16b, V13.16b\n+    MOV V14.16b, V15.16b\n+    MOV V16.16b, V17.16b\n+    MOV V18.16b, V19.16b\n+    MOV V20.16b, V21.16b\n+    MOV V22.16b, V23.16b",
    "Label": "clean"
},
{
    "Id": 1218,
    "Library": "pytorch",
    "Date": "2020/06/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ea06db9466fcdc1b94b44df795056b2af7501346",
    "Root Cause": "N.A",
    "Bug report": "Release GIL during DDP construction. (#40495)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40495\n\nAs part of debugging flaky ddp_under_dist_autograd tests, I realized\nwe were running into the following deadlock.\n\n1) Rank 0 would go into DDP construction, hold GIL and wait for broadcast in\nDDP construction.\n2) Rank 3 is a little slower and performs an RRef fetch call before the DDP\nconstruction.\n3) The RRef fetch call is done on Rank 0 and tries to acquire GIL.\n4) We now have a deadlock since Rank 0 is waiting for Rank 3 to enter the\ncollective and Rank 3 is waiting for Rank 0 to release GIL.\nghstack-source-id: 106534442\n\nTest Plan:\n1) Ran ddp_under_dist_autograd 500 times.\n2) waitforbuildbot\n\nDifferential Revision: D22205180\n\nfbshipit-source-id: 6afd55342e801b9edb9591ff25158a244a8ea66a",
    "Number of deleted lines": 1,
    "Deleted lines": "-          py::arg(\"bucket_bytes_cap\") = ::c10d::kDefaultBucketBytesCap)",
    "Added lines": "+          py::arg(\"bucket_bytes_cap\") = ::c10d::kDefaultBucketBytesCap,\n+          py::call_guard<py::gil_scoped_release>())",
    "Label": "clean"
},
{
    "Id": 1219,
    "Library": "pytorch",
    "Date": "2020/06/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a2ef54c598261ad9fd15e1e85c71f15055652572",
    "Root Cause": "N.A",
    "Bug report": "[pytorch] fix CUDA_KERNEL_ASSERT macro for android build (#40151)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40151\n\nFor debug android build it throws the following error:\n```\n  In file included from src/pytorch/android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp:9:\n  In file included from src/pytorch/android/pytorch_android/src/main/cpp/pytorch_jni_common.h:2:\n  In file included from ../../../../src/main/cpp/libtorch_include/armeabi-v7a/torch/csrc/api/include/torch/types.h:3:\n  In file included from ../../../../src/main/cpp/libtorch_include/armeabi-v7a/ATen/ATen.h:5:\n  In file included from ../../../../src/main/cpp/libtorch_include/armeabi-v7a/ATen/Context.h:4:\n  In file included from ../../../../src/main/cpp/libtorch_include/armeabi-v7a/ATen/Tensor.h:3:\n  In file included from ../../../../src/main/cpp/libtorch_include/armeabi-v7a/ATen/core/TensorBody.h:7:\n  In file included from ../../../../src/main/cpp/libtorch_include/armeabi-v7a/c10/core/Scalar.h:13:\n  ../../../../src/main/cpp/libtorch_include/armeabi-v7a/c10/util/TypeCast.h:157:22: error: use of undeclared identifier '__assert_fail'\n  AT_FORALL_QINT_TYPES(DEFINE_UNCASTABLE)\n                       ^\n```\n\nSeems __assert_fail() isn't available on Android by default - in NDEBUG mode it forward declares the function and CI passes.\n\nBut CUDA_KERNEL_ASSERT() shouldn't be relevant for mobile build at all and we already bypass `__APPLE__` so the easiest fix is to add `__ANDROID__`.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22095562\n\nPulled By: ljk53\n\nfbshipit-source-id: 793108a7bc64db161a0747761c0fbd70262e7d5a",
    "Number of deleted lines": 2,
    "Deleted lines": "-// code that when building Release.\n-#if defined(__APPLE__) || defined(__HIP_PLATFORM_HCC__)",
    "Added lines": "+// code that would otherwise be suppressed when building Release.\n+#if defined(__ANDROID__) || defined(__APPLE__) || defined(__HIP_PLATFORM_HCC__)",
    "Label": "clean"
},
{
    "Id": 1220,
    "Library": "pytorch",
    "Date": "2020/06/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5843854e661c60f111cea6ce3740011cba1eb79b",
    "Root Cause": "N.A",
    "Bug report": "[TensorPipe] Fix transport/channel priorities (#40090)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40090\n\nI messed up in #39957: TensorPipe used to have a bug where it inverted priorities and preferred lower ones over higher ones. I had fixed that bug at the same time as I was writing that PR but then forgot to update the priority values once that PR landed. So this meant that TensorPipe was trying to bootstrap using SHM and then upgrade to UV. That worked in our tests because they are all run on the same machine, but that broke using TensorPipe across different machines. I'll take suggestions on how to have tests in place to prevent this type of breakages from happening.\n\nThe silver lining is that for some time our tests were testing the UV transport, instead of the SHM one, and it seems to be working alright. ;)\nghstack-source-id: 105967203\n\nDifferential Revision: D22067264\n\nfbshipit-source-id: c6e3ae7a86038714cfba754b0811ca8a9a6f1347",
    "Number of deleted lines": 7,
    "Deleted lines": "-  return std::make_unique<TransportRegistration>(\n-      TransportRegistration{std::move(context), 1, std::move(address)});\n-  return std::make_unique<TransportRegistration>(\n-      TransportRegistration{std::move(context), 0, std::move(address)});\n-      ChannelRegistration{std::move(context), 1});\n-      ChannelRegistration{std::move(context), 0});\n-",
    "Added lines": "+// These priorities instruct TensorPipe on which transport/channel to pick\n+// during handshake. Higher priorities will take precedence over lower ones.\n+// The transport with lowest priority will be the one used to bootstrap pipes.\n+\n+constexpr int64_t kShmTransportPriority = 100;\n+// The UV transport just uses TCP and should work everywhere, thus keep it last.\n+constexpr int64_t kUvTransportPriority = 0;\n+\n+constexpr int64_t kCmaChannelPriority = 100;\n+// The basic channel reuses a transport as a channel, and is thus our fallback.\n+constexpr int64_t kBasicChannelPriority = 0;\n+\n+  return std::make_unique<TransportRegistration>(TransportRegistration{\n+      std::move(context), kUvTransportPriority, std::move(address)});\n+  return std::make_unique<TransportRegistration>(TransportRegistration{\n+      std::move(context), kShmTransportPriority, std::move(address)});\n+      ChannelRegistration{std::move(context), kBasicChannelPriority});\n+      ChannelRegistration{std::move(context), kCmaChannelPriority});",
    "Label": "clean"
},
{
    "Id": 1221,
    "Library": "pytorch",
    "Date": "2020/06/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/399dd84c8c37351489bf8f986c1ddb356eb89ef7",
    "Root Cause": "N.A",
    "Bug report": "Fix TensorPipeAgent shutdown to ensure it drains all outstanding work. (#40060)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40060\n\nAs part of debugging https://github.com/pytorch/pytorch/issues/39855,\nI noticed that TensorPipeAgent's ThreadPool was still executing tasks when the\npython interpreter was shutting down. This caused issues with\npybind::gil_scoped_acquire() since it can't be called when the interpreter is\nshutting down resulting in a crash.\n\nThe reason for this was that TensorPipeAgent was calling waitWorkComplete and\nthen shutting down the listeners. This meant that after waitWorkComplete\nreturned, there could still be a race where an RPC call gets enqueued before we\nshutdown listeners.\n\nTo avoid this situation, I've moved the call to waitWorkComplete at the end of\nshutdown (similar to ProcessGroupAgent).\n\nCloses: https://github.com/pytorch/pytorch/issues/39855\nghstack-source-id: 105926653\n\nTest Plan:\n1) Ran test_backward_node_failure\n(__main__.TensorPipeAgentDistAutogradTestWithSpawn) 100 times to verify the\nfix.\n2) waitforbuildbot\n\nDifferential Revision: D22055708\n\nfbshipit-source-id: 2cbe388e654b511d85ad416e696f3671bd369372",
    "Number of deleted lines": 4,
    "Deleted lines": "-  threadPool_.waitWorkComplete();\n-  VLOG(1) << \"RPC agent for \" << workerInfo_.name_\n-          << \" done waiting for thread pool to complete work\";\n-",
    "Added lines": "+\n+  // NOTE: We need to call waitWorkComplete in the end after we have shutdown\n+  // all listeners for Tensorpipe. This is to drain any already accepted work\n+  // in the ThreadPool. If this is done before we shutdown the listeners,\n+  // additional work could be added after this call and before we shutdown\n+  // listeners. This work would continue executing in the threadpool and might\n+  // cause issues during shutdown of the system.\n+  threadPool_.waitWorkComplete();\n+  VLOG(1) << \"RPC agent for \" << workerInfo_.name_\n+          << \" done waiting for thread pool to complete work\";\n+",
    "Label": "clean"
},
{
    "Id": 1222,
    "Library": "pytorch",
    "Date": "2020/06/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ede9bc97c3d734f3c80f4c0c08e1fe3dc2ab0250",
    "Root Cause": "N.A",
    "Bug report": "Fix the processing logic of bernoulli on amd (#40001)\n\nSummary:\n- Fixed the bug discussed in https://github.com/pytorch/pytorch/issues/38558\n- This PR is aim to make the processing of bernoulli on amd can move to the default version, even though `AT_MKL_ENABLED` is setting to `TRUE`.\n- This logic used to be in the old code, but was broken by the latest update, this pr will be the fix for that.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/40001\n\nDifferential Revision: D22037646\n\nPulled By: pbelevich\n\nfbshipit-source-id: c0aa4ba37416d2568daf3463cfede6838ffaeac1",
    "Number of deleted lines": 5,
    "Deleted lines": "-#if !AT_MKL_ENABLED()\n-void bernoulli_scalar_kernel(Tensor& self, double p, c10::optional<Generator> gen) {\n-    // Use AT_ASSERTM because this should never be reached, and AT_ASSERTM tells\n-    // users to report this as a bug.\n-    AT_ASSERTM(false, \"ATen not compiled with MKL\");",
    "Added lines": "+void bernoulli_scalar_kernel_default(Tensor& self, double p, c10::optional<Generator> gen) {\n+\n+#if !AT_MKL_ENABLED()\n+void bernoulli_scalar_kernel(Tensor& self, double p, c10::optional<Generator> gen) {\n+  bernoulli_scalar_kernel_default(self, p, gen);\n+}\n+    // The situation of AMD, move to using the default version\n+    bernoulli_scalar_kernel_default(self, p, gen);",
    "Label": "clean"
},
{
    "Id": 1223,
    "Library": "pytorch",
    "Date": "2020/06/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9bfb91b50b53d332493d5179d3e17871f92efd2a",
    "Root Cause": "N.A",
    "Bug report": "Fix possible deadlock in _wait_all_workers (#39535)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39535\n\nThis is my understanding of what could happen: on workerN (N != 0), `_wait_all_workers_sequence_id_to_states`, which is a `defaultdict`, is accessed twice: once in the body of `_wait_all_workers` (by the \"main thread\" of workerN) and once in `_set_proceed_shutdown_signal`, called by worker0 through a RPC call. I think the two could race and access the `_wait_all_workers_sequence_id_to_states` at the same time, and thus create two separate copies of `WaitAllWorkersStates`. One of those threads would wait  on the event of one copy, but the other thread would set the event of the other copy. This lead to a deadlock, as the main thread would end up waiting forever.\nghstack-source-id: 105283327\n\nTest Plan: I added additional logging in those functions, ran a stress test of the RPC test suite, based on the logs I suspected that this could be the issue, fixed it and re-run the stress test and didn't see the bug anymore. This is admittedly not very convincing evidence, as I may just have been lucky that second time...\n\nDifferential Revision: D21889752\n\nfbshipit-source-id: 05ec710bd2930313e1480ae896b4b2f5f503aa17",
    "Number of deleted lines": 5,
    "Deleted lines": "-_wait_all_workers_dict_lock = threading.Lock()\n-    proceed_signal = _wait_all_workers_sequence_id_to_states[sequence_id].proceed_signal\n-    proceed_signal = _wait_all_workers_sequence_id_to_states[\n-        sequence_id\n-    ].proceed_signal",
    "Added lines": "+_wait_all_workers_dict_lock = threading.RLock()\n+    with _wait_all_workers_dict_lock:\n+        proceed_signal = _wait_all_workers_sequence_id_to_states[\n+            sequence_id\n+        ].proceed_signal\n+    with _wait_all_workers_dict_lock:\n+        proceed_signal = _wait_all_workers_sequence_id_to_states[\n+            sequence_id\n+        ].proceed_signal",
    "Label": "clean"
},
{
    "Id": 1224,
    "Library": "pytorch",
    "Date": "2020/05/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c02cb7aa08bc36085da217f0527a87e1c1b1cfa6",
    "Root Cause": "N.A",
    "Bug report": "[nnpi fake ops] bug fix int8QuantizeNNPI (#39271)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39271\n\nCaused 10% NE loss. Bug in emulation itself and NNPI is fine.\n\nTest Plan: mobile_cvr has no NE loss after this fix: https://fburl.com/mlhub/z6hd8rhn\n\nReviewed By: hyuen\n\nDifferential Revision: D21793205\n\nfbshipit-source-id: a908e95c26c2353f982d05e0a20f02f3c724715d",
    "Number of deleted lines": 2,
    "Deleted lines": "-  float offset_tmp = -Y_offset;\n-",
    "Added lines": "+  float offset_tmp = Y_offset;",
    "Label": "clean"
},
{
    "Id": 1225,
    "Library": "pytorch",
    "Date": "2020/05/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f44fca882eddc865901c07266b7f3b3976fc48dd",
    "Root Cause": "N.A",
    "Bug report": "Update NNPI backend to v0.6.0.5 (#4539)\n\nSummary:\nUpdating to NNPI Backend version v0.6.0.5\nPull Request resolved: https://github.com/pytorch/glow/pull/4539\n\nTest Plan:\nImported from GitHub, without a `Test Plan:` line.\n\n**Glow Dev Mode Testing ICEREF**\n```\nbuck test //glow: -- NNPI\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/844425092762063/\n\n**Glow Opt Mode Testing ICEREF**\n```\nbuck test mode/opt //glow: -- NNPI\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/8444249313953808/\n\n**Glow Opt Mode Testing On Device**\n```\nbuck test mode/opt -c glow.nnpi_use_inf_api=true //glow: -- NNPI -j 1\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/5629499560791922/\n\n**Net_runner defaults OPT ICEREF**\n```\nbuck test mode/opt //glow/fb/test:net_runner_nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/1970324865910264/\n\n**Net_runner defaults OPT CARD**\n```\nUSE_INF_API=1 buck test mode/opt //glow/fb/test:net_runner_nnpi\n```\nFAIL https://www.internalfb.com/intern/testinfra/testconsole/testrun/6192449501601838/\n\n**Net_runner tiny ctr_mbl_feed_2020q1 OPT CARD**\n```\nUSE_INF_API=1 LD_LIBRARY_PATH=third-party-buck/platform007/build/fb-nnpi/lib ./buck-out/opt/gen/glow/fb/test/net_runner_nnpi --logfiledb  ~/test/161676462_0.predictor --opt_net ~/test/debug_optimized_net_0.pb_txt --use_input ~/test/inputs.pb.recordio --glow-nnpi-memory=13000000 --glow-num-devices=2 --ref_impl=glow --test_impls=glow,c2_fp16 --caffe2_fbgemm_fake_fp16_clamp --glow_global_fp16 --glow_clip_fp16 --glow_global_fused_scale_offset_fp16 --fbgemm_deserialize_to_original_format --inference_threads 16 --load_model_by_blob --glow_global_fp16_placeholders --glow_global_fp16_constants --glow_clip_fp16_skip_inputs --glow_nnpi_lower_all_batch_matmul=false --glow_nnpi_num_parallel_chunks=6 --print_latency\n```\nsuccess\n\n**Net_runner FP16 OPT CARD**\n```\nUSE_INF_API=1 LD_LIBRARY_PATH=third-party-buck/platform007/build/fb-nnpi/lib ./buck-out/opt/gen/glow/fb/test/net_runner_nnpi  --opt_net buck-out/opt/gen/glow/fb/test/instagram_ctr_model_debug_optimized_net/debug_optimized_net_0.pb_txt --logfiledb buck-out/opt/gen/glow/fb/test/instagram_ctr_model_tiny/105533872_0.predictor --use_input buck-out/opt/gen/glow/fb/test/instagram_ctr_model_inputs_pb/inputs.pb --inference_threads=1 --glow_global_fp16 --glow_global_fused_scale_offset_fp16=1 --glow_clip_fp16\n```\nFAIL P131738351\n\n**numerics tests on IceRef**\n```\nbuck test //caffe2/caffe2/contrib/fakelowp/test:test_batchmatmul_nnpi_fp16nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/1688849890354625/\n```\nbuck test //caffe2/caffe2/contrib/fakelowp/test:test_batchnorm_nnpi_fp16nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/1125900069449763/\n```\nbuck test //caffe2/caffe2/contrib/fakelowp/test:test_fc_nnpi_fp16nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/5910974535833584/\n```\nbuck test //caffe2/caffe2/contrib/fakelowp/test:test_op_nnpi_fp16nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/5910974535833516/\n```\nbuck test //caffe2/caffe2/contrib/fakelowp/test:test_int8_ops_nnpinnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/2533274820486771/\n```\nbuck test //caffe2/caffe2/contrib/fakelowp/test:test_sls_4bit_nnpi_fp16nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/5910974536132077/\n```\nbuck test //caffe2/caffe2/contrib/fakelowp/test:test_sls_8bit_nnpi_fp16nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/4222124677262742/\n```\nbuck test //caffe2/caffe2/contrib/fakelowp/test:test_sls_8bit_nnpi_fp32nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/1688849890355133/\n\nPreviously disabled tests `test_slws_fused_8bit_rowwise_acc32_nnpi` and `test_small_sls_acc32` still FAIL\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/7599824383821851/\n\n**numerics tests on Card**\n```\nbuck test -c glow.nnpi_use_inf_api=true //caffe2/caffe2/contrib/fakelowp/test:test_batchmatmul_nnpi_fp16nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/4222124677263984/\n```\nbuck test -c glow.nnpi_use_inf_api=true  //caffe2/caffe2/contrib/fakelowp/test:test_batchnorm_nnpi_fp16nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/1125900069450391/\n```\nbuck test -c glow.nnpi_use_inf_api=true //caffe2/caffe2/contrib/fakelowp/test:test_fc_nnpi_fp16nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/1688849890354230/\n\n```\nbuck test -c glow.nnpi_use_inf_api=true //caffe2/caffe2/contrib/fakelowp/test:test_op_nnpi_fp16nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/5910974535833691/\n\n```\nbuck test -c glow.nnpi_use_inf_api=true //caffe2/caffe2/contrib/fakelowp/test:test_int8_ops_nnpinnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/3659174723845865/\n\n```\nbuck test -c glow.nnpi_use_inf_api=true //caffe2/caffe2/contrib/fakelowp/test:test_sls_4bit_nnpi_fp16nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/5910974536135102/\n\n```\nbuck test -c glow.nnpi_use_inf_api=true //caffe2/caffe2/contrib/fakelowp/test:test_sls_8bit_nnpi_fp16nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/2251799842675543/\n\n```\nbuck test -c glow.nnpi_use_inf_api=true //caffe2/caffe2/contrib/fakelowp/test:test_sls_8bit_nnpi_fp32nnpi\n```\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/2533274820487658/\n\nPreviously disabled tests `test_slws_fused_8bit_rowwise_acc32_nnpi` and `test_small_sls_acc32` still FAIL\nhttps://www.internalfb.com/intern/testinfra/testconsole/testrun/1407375046178756/\n\nReviewed By: arunm-git\n\nDifferential Revision: D21697616\n\nPulled By: hl475\n\nfbshipit-source-id: 3732324986eb40e644686cdd10e44951678508a7",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if (output_block_size % 2 == 0 && output_block_size <= 96) {",
    "Added lines": "+        if (output_block_size % 2 == 0 && output_block_size <= 96 &&\n+            data.size(1) % 2 == 0) {",
    "Label": "clean"
},
{
    "Id": 1226,
    "Library": "pytorch",
    "Date": "2020/05/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7b90ed1117f4018dab9896f4755eac6a4c479e1c",
    "Root Cause": "N.A",
    "Bug report": "[TensorPipe] Pass names of endpoints to context/pipe for easier debugging (#38926)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38926\n\nTensorPipe supports for the user to provide a meaningful name for each context and to specify what it thinks the name of the endpoint it's connecting to is, so that these names can be logged and matched to the otherwise not-very-informative ID of a pipe (given by the PID and some counters) for easier debugging.\nghstack-source-id: 104760688\n\nTest Plan: Ran RPC tests with `TP_VERBOSE_LOGGING=1`.\n\nDifferential Revision: D21479799\n\nfbshipit-source-id: 856d2ffac239a3f9b11318a92ba4534133865dc8",
    "Number of deleted lines": 2,
    "Deleted lines": "-      context_(std::make_shared<tensorpipe::Context>()),\n-        toWorkerInfo.id_, ClientPipe(context_->connect(url)));",
    "Added lines": "+      context_(std::make_shared<tensorpipe::Context>(\n+          tensorpipe::ContextOptions().name(workerInfo_.name_))),\n+        toWorkerInfo.id_,\n+        ClientPipe(context_->connect(\n+            url, tensorpipe::PipeOptions().name(toWorkerInfo.name_))));",
    "Label": "clean"
},
{
    "Id": 1227,
    "Library": "pytorch",
    "Date": "2020/05/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/341fd63ff63743ee60c2cf743fc24f23a6917a83",
    "Root Cause": "N.A",
    "Bug report": "add eq.str op to lite interpreter (#38859)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38859\n\nThis error message indicates aten::eq expects different types\n\n```\nRUNNING 379 OP 76, aten::eq\nterminate called after throwing an instance of 'c10::Error'\n  what():  isInt() INTERNAL ASSERT FAILED at \"buck-out/gen/68e83026/xplat/caffe2/aten_header#header-mode-symlink-tree-with-header-map,headers/ATen/core/ivalue.h\":331, please report a bug to PyTorch.\n```\n\nIt turns out that there are two aten::eq in lite interpreter (https://www.internalfb.com/intern/diffusion/FBS/browse/master/xplat/caffe2/torch/csrc/jit/runtime/register_prim_ops.cpp?lines=417)\naten::eq(int, int)\naten::eq(str, str)\n\nThis diff add overload name for str and it fixed the problem.\n\nTest Plan: local test\n\nReviewed By: pengtxiafb\n\nDifferential Revision: D21681838\n\nfbshipit-source-id: 1f17ecdadb9bc1c16915a24c60fa57a6fc273865",
    "Number of deleted lines": 9,
    "Deleted lines": "-#define DEFINE_STR_CMP_OP(aten_op, op)     \\\n-  Operator(                                \\\n-      #aten_op \"(str a, str b) -> bool\",   \\\n-      [](Stack& stack) {                   \\\n-        auto b = pop(stack).toStringRef(); \\\n-        auto a = pop(stack).toStringRef(); \\\n-        push(stack, op);                   \\\n-        return 0;                          \\\n-      },                                   \\",
    "Added lines": "+#define DEFINE_STR_CMP_OP(aten_op, op)       \\\n+  Operator(                                  \\\n+      #aten_op \".str(str a, str b) -> bool\", \\\n+      [](Stack& stack) {                     \\\n+        auto b = pop(stack).toStringRef();   \\\n+        auto a = pop(stack).toStringRef();   \\\n+        push(stack, op);                     \\\n+        return 0;                            \\\n+      },                                     \\",
    "Label": "clean"
},
{
    "Id": 1228,
    "Library": "pytorch",
    "Date": "2020/05/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6d4d508d8e19a2e9b7f3a7d429c3868e3e07ee02",
    "Root Cause": "N.A",
    "Bug report": "Log incorrect device in ProcessGroupGloo (#38844)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38844\n\nEnhances error message in ProcessGroupGloo to log the unsupported\ndevice. Been seeing a few issues with this and this will provide more debug\ninformation.\n\nTest Plan: CI\n\nDifferential Revision: D21676881\n\nfbshipit-source-id: 1fd727162682e1a55003adff67c4358dab488455",
    "Number of deleted lines": 7,
    "Deleted lines": "-      invalidArgument(\"unsupported device type\");\n-      invalidArgument(\"unsupported device type\");\n-      invalidArgument(\"unsupported device type\");\n-      invalidArgument(\"unsupported device type\");\n-      invalidArgument(\"unsupported device type\");\n-      invalidArgument(\"unsupported device type\");\n-      invalidArgument(\"unsupported device type\");",
    "Added lines": "+#include <c10/util/StringUtil.h>\n+      invalidArgument(c10::str(\"unsupported device type \", device.type()));\n+      invalidArgument(c10::str(\"unsupported device type \", device.type()));\n+      invalidArgument(c10::str(\"unsupported device type \", device.type()));\n+      invalidArgument(c10::str(\"unsupported device type \", device.type()));\n+      invalidArgument(c10::str(\"unsupported device type \", device.type()));\n+      invalidArgument(c10::str(\"unsupported device type \", device.type()));\n+      invalidArgument(c10::str(\"unsupported device type \", device.type()));",
    "Label": "clean"
},
{
    "Id": 1229,
    "Library": "pytorch",
    "Date": "2020/05/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e9902358df14dc4809e4f50b12088a5200a1862d",
    "Root Cause": "N.A",
    "Bug report": "Support fp16 output in OnnxifiOp (#38846)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38846\n\nWe begin to have fp16 inputs/outputs. Adding this will help with the debugging.\n\nTest Plan: run.\n\nReviewed By: jfix71\n\nDifferential Revision: D21676805\n\nfbshipit-source-id: 47788e631164d24aef0f659b281c59822b009e18",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      {ONNXIFI_DATATYPE_FLOAT16, TypeMeta::Make<c10::Half>()},",
    "Label": "clean"
},
{
    "Id": 1230,
    "Library": "pytorch",
    "Date": "2020/05/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/320c35681df43aced804602c6209d64b3846958f",
    "Root Cause": "N.A",
    "Bug report": "[TensorExpr] (trivial) unique Kernel input names (#38678)\n\nSummary:\nWe have a bug where Function names are not uniqued which produces bad printed output, e.g:\n```\n{\n  for (int i0 = 0; i0 < 1024; i0++) {\n    input[i0] = t0[0 + i0 * 1];\n  }\n  for (int i0_1 = 0; i0_1 < 1024; i0_1++) {\n    input_1[i0_1] = t1[0 + i0_1 * 1];\n  }\n  for (int v = 0; v < 1024; v++) {\n    aten_add[v] = (input(v)) + float(1) * (input(v));\n  }\n  for (int v_1 = 0; v_1 < 1024; v_1++) {\n    aten_sub[v_1] = (aten_add(v_1)) - float(1) * (input(v_1));\n  }\n}\n```\n\nNotice the names of the vars in the `aten_add` line which make it appear as though input_1 isn't used. This is because the Buf names are uniqued by the unique_name_manager but the FunctionCall names are not.\n\nNot fixing this right now, but working around it by reducing the number of Tensors that are created with the same name (\"input\") in kernel.cpp. That example now looks like:\n```\n{\n  for (int i0 = 0; i0 < 1024; i0++) {\n    input1[i0] = t0[0 + i0 * 1];\n  }\n  for (int i0_1 = 0; i0_1 < 1024; i0_1++) {\n    input2[i0_1] = t1[0 + i0_1 * 1];\n  }\n  for (int v = 0; v < 1024; v++) {\n    aten_add[v] = (input1(v)) + float(1) * (input2(v));\n  }\n  for (int v_1 = 0; v_1 < 1024; v_1++) {\n    aten_sub[v_1] = (aten_add(v_1)) - float(1) * (input1(v_1));\n  }\n}\n```\n\nTo be clear, the bug still exists but it's not blocking what I'm trying to do right now \ud83d\ude04\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38678\n\nDifferential Revision: D21630276\n\nPulled By: nickgg\n\nfbshipit-source-id: 39dec2178cf492302bc5a61e1e688ae81513858a",
    "Number of deleted lines": 1,
    "Deleted lines": "-              \"input\",",
    "Added lines": "+              \"input\" + c10::to_string(tensors_.size() + 1),",
    "Label": "clean"
},
{
    "Id": 1231,
    "Library": "pytorch",
    "Date": "2020/05/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/62afc2d63dbe7fe1ee411d428fa7799b0bab0f08",
    "Root Cause": "N.A",
    "Bug report": "[JIT] Remove debug print statement added in #37994 (#38524)\n\nSummary:\n**Summary**\nThis commit removes a print statement added in https://github.com/pytorch/pytorch/issues/37994 that appears to\nbe for debugging and was most likely not intended to be commited.\n\n**Test Plan**\nContinuous integration.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38524\n\nDifferential Revision: D21587268\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 6bdcdce647c45f5c0a2ba179a3545a1c0cae1492",
    "Number of deleted lines": 1,
    "Deleted lines": "-    print(methods)",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1232,
    "Library": "pytorch",
    "Date": "2020/05/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d1eeb3b7bb65fadf24e58fa70711d49fac9e0a0b",
    "Root Cause": "N.A",
    "Bug report": "[Tensorexpr] Fix and improve handling multiple gpu devices (#38365)\n\nSummary:\nThese commits fixes a bug which was exposed when we took away the fallback path. The fix is to set the appropriate device before setting CUDA stream.\nThe improvement is when compiling, setting the device to new device only if it's different from prior device, and removing redundant call to cudaFree\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38365\n\nReviewed By: zheng-xq\n\nDifferential Revision: D21537469\n\nPulled By: protonu\n\nfbshipit-source-id: b9662dd623b5c7cfd23eb6894e992a43665641e4",
    "Number of deleted lines": 11,
    "Deleted lines": "-\n-}\n-void CudaSetContext(CUcontext pctx) {\n-  if (!pctx) {\n-    std::unique_lock<std::mutex> cudaFreeMutexLock(\n-        *(c10::cuda::CUDACachingAllocator::getFreeMutex()));\n-    cudaFree(0);\n-  at::cuda::set_device(this->device().index());\n-  CudaSetContext(pctx);\n-\n-  at::cuda::set_device(prior_device);",
    "Added lines": "+  const auto prior_device = at::cuda::current_device();\n+  if (prior_device != this->device().index()) {\n+    at::cuda::set_device(this->device().index());\n+  }\n+  if (prior_device != this->device().index()) {\n+    at::cuda::set_device(prior_device);\n+  if (prior_device != this->device().index()) {\n+    at::cuda::set_device(this->device().index());\n+  }\n+  if (!pctx) {\n+    std::unique_lock<std::mutex> cudaFreeMutexLock(\n+        *(c10::cuda::CUDACachingAllocator::getFreeMutex()));\n+    cudaFree(nullptr);\n+    AT_CUDA_DRIVER_CHECK(nvrtc().cuCtxGetCurrent(&pctx));\n+  }\n+\n+  if (prior_device != this->device().index()) {\n+    at::cuda::set_device(prior_device);\n+  }",
    "Label": "clean"
},
{
    "Id": 1233,
    "Library": "pytorch",
    "Date": "2020/05/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7c66ad8941088756cc6c8cddb29e6664fc3a40a3",
    "Root Cause": "N.A",
    "Bug report": "[caffe2/fakelowp] fix bug in ref code (#38331)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38331\n\nC_ref size was wrong\n\nTest Plan: CI\n\nReviewed By: hyuen\n\nDifferential Revision: D21525639\n\nfbshipit-source-id: 59f4709238cdd46bb38f7c534335eb79229f6c7f",
    "Number of deleted lines": 1,
    "Deleted lines": "-  std::vector<uint8_t> C_ref(M * K);",
    "Added lines": "+  std::vector<uint8_t> C_ref(M * N);",
    "Label": "clean"
},
{
    "Id": 1234,
    "Library": "pytorch",
    "Date": "2020/05/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b57d82fcbb1d4e385a0f639a7e75020e5bc1a15e",
    "Root Cause": "N.A",
    "Bug report": "workaround nvcc host function bug (#37867)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37867\n\nthis is to work around internal issue we are hitting with nvcc in ovrsource.\nIt does not seem to overload to the correct device version of `isinf` and `isnan` without this fudging of the code.\n\nTest Plan:\nCI green,\ninternal builds pass\n\nReviewed By: malfet\n\nDifferential Revision: D21408263\n\nfbshipit-source-id: 1ff44e088b5c885d729cc95f00cf8fa07e525f6d",
    "Number of deleted lines": 2,
    "Deleted lines": "-  if (::isnan(v.real()) || ::isnan(v.imag()) || both_inf(v.real(), v.imag())) {\n-  } else if (::isinf(v.real()) || ::isinf(v.imag())) {",
    "Added lines": "+  auto either_inf = [](T real, T imag) {\n+    return ::isinf(real) || ::isinf(imag);\n+  };\n+\n+  auto either_nan = [](T real, T imag) {\n+    return ::isnan(real) || ::isnan(imag);\n+  };\n+\n+  if (either_nan(v.real(), v.imag()) || both_inf(v.real(), v.imag())) {\n+  } else if (either_inf(v.real(), v.imag())) {",
    "Label": "clean"
},
{
    "Id": 1235,
    "Library": "pytorch",
    "Date": "2020/05/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1d43d7caa215a7eeca6f4fcefa15b61141febff2",
    "Root Cause": "N.A",
    "Bug report": "Use `gpu_kernel` in Affine Quantizer (#37312)\n\nSummary:\nRemoves `CUDA_tensor_apply2` from Affine Quantizer.\n\ncc: zasdfgbnm\n\n# Profiling\n\n## This PR\n\n### quint8\n\n```==4458==       Range \"quantize_per_tensor, seq = 0\"\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n          Range:  100.00%  4.8703ms        20  243.52us  207.60us  312.66us  quantize_per_tensor, seq = 0\n GPU activities:  100.00%  751.95us        10  75.194us  74.372us  79.044us  _ZN2at6native6modern29vectorized_elementwise_kernelILi4EZZZNS0_75_GLOBAL__N__51_tmpxft_0000424b_00000000_6_affine_quantizer_cpp1_ii_92f2f7d738quantize_tensor_per_tensor_affine_cudaENS_6TensorES4_dlENKUlvE_clEvENKUlvE0_clEvEUlfN3c106quint8EE_NS_6detail5ArrayIPcLi3EEEEEviT0_T1_\n      API calls:  100.00%  162.48us        10  16.247us  13.383us  35.997us  cudaLaunchKernel\n```\n\n### qint8\n\n```==14289==       Range \"quantize_per_tensor, seq = 0\"\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n          Range:  100.00%  4.8143ms        20  240.71us  155.68us  327.78us  quantize_per_tensor, seq = 0\n GPU activities:  100.00%  748.85us        10  74.884us  73.892us  78.565us  _ZN2at6native6modern29vectorized_elementwise_kernelILi4EZZZNS0_75_GLOBAL__N__51_tmpxft_0000424b_00000000_6_affine_quantizer_cpp1_ii_92f2f7d738quantize_tensor_per_tensor_affine_cudaENS_6TensorES4_dlENKUlvE_clEvENKUlvE_clEvEUlfN3c105qint8EE_NS_6detail5ArrayIPcLi3EEEEEviT0_T1_\n      API calls:  100.00%  166.61us        10  16.661us  13.387us  39.237us  cudaLaunchKernel\n```\n\n### qint32\n\n```\n==17303==       Range \"quantize_per_tensor, seq = 0\"\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n          Range:  100.00%  19.011ms        20  950.55us  308.07us  1.0331ms  quantize_per_tensor, seq = 0\n GPU activities:  100.00%  1.1440ms        10  114.40us  113.42us  117.74us  _ZN2at6native6modern29vectorized_elementwise_kernelILi4EZZZNS0_75_GLOBAL__N__51_tmpxft_0000424b_00000000_6_affine_quantizer_cpp1_ii_92f2f7d738quantize_tensor_per_tensor_affine_cudaENS_6TensorES4_dlENKUlvE_clEvENKUlvE1_clEvEUlfN3c106qint32EE_NS_6detail5ArrayIPcLi3EEEEEviT0_T1_\n      API calls:  100.00%  163.78us        10  16.378us  13.747us  35.668us  cudaLaunchKernel\n```\n\n## Original\n\ncommit: b428f454e13f6e8055124ea19c32b554017137d0\n\n### quint8\n\n```\n==4361==       Range \"quantize_per_tensor, seq = 0\"\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n          Range:  100.00%  5.6212ms        20  281.06us  230.17us  352.82us  quantize_per_tensor, seq = 0\n GPU activities:  100.00%  780.85us        10  78.084us  77.633us  78.561us  _ZN2at4cuda75_GLOBAL__N__51_tmpxft_00007fda_00000000_6_affine_quantizer_cpp1_ii_13ee0d7721kernelPointwiseApply2IZZZNS_6native75_GLOBAL__N__51_tmpxft_00007fda_00000000_6_affine_quantizer_cpp1_ii_13ee0d7738quantize_tensor_per_tensor_affine_cudaENS_6TensorES5_dlENKUlvE_clEvENKUlvE0_clEvEUlRfRN3c106quint8EE_fSA_jLi1ELi1ELi1EEEvNS0_6detail10TensorInfoIT0_T2_EENSE_IT1_SG_EESG_T_\n      API calls:  100.00%  166.07us        10  16.606us  13.535us  36.578us  cudaLaunchKernel\n```\n\n### qint8\n\n```\n==12583==       Range \"quantize_per_tensor, seq = 0\"\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n          Range:  100.00%  5.5765ms        20  278.82us  226.51us  351.23us  quantize_per_tensor, seq = 0\n GPU activities:  100.00%  783.28us        10  78.328us  77.826us  80.386us  _ZN2at4cuda75_GLOBAL__N__51_tmpxft_00007fda_00000000_6_affine_quantizer_cpp1_ii_13ee0d7721kernelPointwiseApply2IZZZNS_6native75_GLOBAL__N__51_tmpxft_00007fda_00000000_6_affine_quantizer_cpp1_ii_13ee0d7738quantize_tensor_per_tensor_affine_cudaENS_6TensorES5_dlENKUlvE_clEvENKUlvE_clEvEUlRfRN3c105qint8EE_fSA_jLi1ELi1ELi1EEEvNS0_6detail10TensorInfoIT0_T2_EENSE_IT1_SG_EESG_T_\n      API calls:  100.00%  161.05us        10  16.104us  13.363us  34.284us  cudaLaunchKernel\n```\n\n### qint32\n\n```\n==17267==       Range \"quantize_per_tensor, seq = 0\"\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n          Range:  100.00%  19.815ms        20  990.77us  381.03us  1.0717ms  quantize_per_tensor, seq = 0\n GPU activities:  100.00%  1.1778ms        10  117.78us  117.51us  118.44us  _ZN2at4cuda75_GLOBAL__N__51_tmpxft_00007fda_00000000_6_affine_quantizer_cpp1_ii_13ee0d7721kernelPointwiseApply2IZZZNS_6native75_GLOBAL__N__51_tmpxft_00007fda_00000000_6_affine_quantizer_cpp1_ii_13ee0d7738quantize_tensor_per_tensor_affine_cudaENS_6TensorES5_dlENKUlvE_clEvENKUlvE1_clEvEUlRfRN3c106qint32EE_fSA_jLi1ELi1ELi1EEEvNS0_6detail10TensorInfoIT0_T2_EENSE_IT1_SG_EESG_T_\n      API calls:  100.00%  172.26us        10  17.226us  14.094us  37.952us  cudaLaunchKernel\n```\n\n##\n\n# Environment\n\n```shell\nCollecting environment information...\nPyTorch version: 1.6.0a0+010771e\nIs debug build: No\nCUDA used to build PyTorch: 10.2\n\nOS: Ubuntu 18.04.3 LTS\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\nCMake version: version 3.14.0\n\nPython version: 3.7\nIs CUDA available: Yes\nCUDA runtime version: 10.2.89\nGPU models and configuration: GPU 0: TITAN V\nNvidia driver version: 440.33.01\ncuDNN version: /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.7\n\nVersions of relevant libraries:\n[pip] numpy==1.18.1\n[pip] torch==1.6.0a0+010771e\n[conda] blas                      1.0                         mkl\n[conda] magma-cuda102             2.5.2                         1    pytorch\n[conda] mkl                       2020.0                      166\n[conda] mkl-include               2020.0                      166\n[conda] mkl-service               2.3.0            py37he904b0f_0\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\n[conda] torch                     1.6.0a0+010771e           dev_0    <develop>\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37312\n\nDifferential Revision: D21383938\n\nPulled By: jerryzh168\n\nfbshipit-source-id: 21539675267c64508a6b9eafcde1a8861d1fb421",
    "Number of deleted lines": 13,
    "Deleted lines": "-        at::cuda::CUDA_tensor_apply2<float, scalar_t>(\n-            /*a=*/rtensor,\n-            /*b=*/qtensor,\n-            [=] __device__(float& rtensor_val, scalar_t& qtensor_val) {\n-              int64_t qvalue;\n-              qvalue = static_cast<int64_t>(\n-                  nearbyint(rtensor_val / scale + zero_point));\n-              qvalue = std::max<int64_t>(qvalue, qmin);\n-              qvalue = std::min<int64_t>(qvalue, qmax);\n-              qtensor_val.val_ = qvalue;\n-            },\n-            /*aType=*/at::cuda::TensorArgType::ReadOnly,\n-            /*bType=*/at::cuda::TensorArgType::ReadWrite);",
    "Added lines": "+\n+        auto iter = TensorIterator();\n+        iter.add_output(qtensor);\n+        iter.add_input(rtensor);\n+        iter.add_input(qtensor);\n+        iter.dont_compute_common_dtype();\n+        iter.build();\n+\n+        gpu_kernel(iter,\n+          [=] GPU_LAMBDA (float raw_val, scalar_t quantized_val) -> scalar_t {\n+            int64_t qvalue = static_cast<int64_t>(nearbyint(raw_val / scale + zero_point));\n+            qvalue = std::max<int64_t>(qvalue, qmin);\n+            qvalue = std::min<int64_t>(qvalue, qmax);\n+            quantized_val.val_ = qvalue;\n+            return quantized_val;\n+        });",
    "Label": "clean"
},
{
    "Id": 1236,
    "Library": "pytorch",
    "Date": "2020/04/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9d0891f886e8a1ea2ee338c1e909ec76905c88fd",
    "Root Cause": "N.A",
    "Bug report": "[pytorch][buck] tweak code analyzer e2e script\n\nSummary:\n- Add debug mode to include debug information.\n- Move codegen comment to FB shell script (as it's only checked-in FB repo).\n- Analyze lite-predictor instead of full-JIT as full-JIT BUCK target contains variable kernels thus pull in a lot more dependencies.\n- Use pre-opt bitcode instead of pre-codegen bitcode - there is one special `callOp()` case in RNN.cpp where optimized bitcode has opname string and API body inlined together: https://fburl.com/diffusion/8rz6u4rg; pre-optimization bitcode should give more stable result.\n\nTest Plan: - Tested the bash script with stacked diff.\n\nReviewed By: iseeyuan\n\nDifferential Revision: D21298837\n\nfbshipit-source-id: be33e2db5d8cb0f804460c503e52beb0dcb4857f",
    "Number of deleted lines": 3,
    "Deleted lines": "-# Generated for selective build without using static dispatch.\n-# Manually run the script to update:\n-# ANALYZE_TORCH=1 DEPLOY=1 tools/code_analyzer/build.sh",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1237,
    "Library": "pytorch",
    "Date": "2020/04/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5b6f6da18cae1965d3d1da889b7c4a88830e4bde",
    "Root Cause": "N.A",
    "Bug report": "[caffe2] Copy tensor in single tensor input case in UnPackRecordsOp (#37454)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37454\n\nFix a bug introduced in D21224497.\n\nIn the case of having a single unpacked tensor as input, we still need to copy the underline memory because only inputs are guaranteed to be read-only. The output could be overwritten later during inference. If we share the tensor, we could potentially overwrite the input, which in principle should be read only.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/python/operator_test:dataset_ops_test\n```\n\nAdIndexer canary:\nhttps://our.intern.facebook.com/intern/ads/canary/426290361213982683\n\nReviewed By: yinghai\n\nDifferential Revision: D21274309\n\nfbshipit-source-id: 71931d4b1afbdc700ba070ea618d1679f1bbe5a7",
    "Number of deleted lines": 1,
    "Deleted lines": "-      *Output(0) = Input(0);",
    "Added lines": "+      Output(0)->CopyFrom(Input(0));",
    "Label": "clean"
},
{
    "Id": 1238,
    "Library": "pytorch",
    "Date": "2020/04/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7604f470ed083d55c6a25bee3f995c7e71ea488f",
    "Root Cause": "N.A",
    "Bug report": "Add weight info in debug_ssa_net (#37262)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37262\n\nIt's convenient to have weights info in the debug_ssa_net so that we can tell what is weight and what is primary inputs. We can get their shape and size info with some post-processing script easily.\n\nReviewed By: ChunliF\n\nDifferential Revision: D21237537\n\nfbshipit-source-id: 1fadc605283ef2eed78c44494e062a16ccf135ab",
    "Number of deleted lines": 1,
    "Deleted lines": "-    dumpNet(*pred_net, shape_hints, \"debug_ssa_net.pb_txt\");",
    "Added lines": "+    caffe2::NetDef ssa_net;\n+    ssa_net.CopyFrom(*pred_net);\n+    auto* w_arg = ssa_net.add_arg();\n+    w_arg->set_name(kInitializers);\n+    for (const auto& w : weights) {\n+      w_arg->add_strings(w);\n+    }\n+    dumpNet(ssa_net, shape_hints, \"debug_ssa_net.pb_txt\");",
    "Label": "clean"
},
{
    "Id": 1239,
    "Library": "pytorch",
    "Date": "2020/04/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4d2502a0c2a281ac71bba5fb5d13d06f8b9f14ba",
    "Root Cause": "N.A",
    "Bug report": "fix explicitly defaulted constexpr assignment operator fails to compile error for gcc 5.3.0 (#36561)\n\nSummary:\ngcc 5.3.0 has an issue which can't define default function as constexpr, see  https://gcc.gnu.org/bugzilla/show_bug.cgi?id=68754. for works for gcc 5.3.0, not define default function as constexpr function now.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36561\n\nDifferential Revision: D21024109\n\nPulled By: ezyang\n\nfbshipit-source-id: 58fce704625b7d0926e40b6b12841ebbe392c59c",
    "Number of deleted lines": 2,
    "Deleted lines": "-  constexpr bitset& operator=(const bitset&) noexcept = default;\n-  constexpr bitset& operator=(bitset&&) noexcept = default;",
    "Added lines": "+  // there is an issure for gcc 5.3.0 when define default function as constexpr\n+  // see https://gcc.gnu.org/bugzilla/show_bug.cgi?id=68754.\n+  bitset& operator=(const bitset&) noexcept = default;\n+  bitset& operator=(bitset&&) noexcept = default;",
    "Label": "clean"
},
{
    "Id": 1240,
    "Library": "pytorch",
    "Date": "2020/04/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c7631716da203849d15d17231db6ee0304363471",
    "Root Cause": "N.A",
    "Bug report": "Output more debugging information for reduce kernel (#35946)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/35946\n\nDifferential Revision: D21007660\n\nPulled By: ngimel\n\nfbshipit-source-id: 83dc257c3d9ff722d30270214c413d8a16bcffc0",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  out << \"block_width=\" << config.block_width << \", \";\n+  out << \"block_height=\" << config.block_height << \", \";\n+  out << \"num_threads=\" << config.num_threads << \", \";",
    "Label": "clean"
},
{
    "Id": 1241,
    "Library": "pytorch",
    "Date": "2020/04/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/82dd01150c182b39d2560b61d3983d9f08242245",
    "Root Cause": "N.A",
    "Bug report": "Fix race during RPC shutdown. (#36113)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36113\n\nAs part of debugging https://github.com/pytorch/pytorch/issues/35863,\nI discovered that the unit test would timeout during clean shutdown.\n\nLooking into this further, it looks like there is a race in\n`_on_leader_follower_report_shutdown_intent` when multiple followers call the\nsame method on the leader.\n\nTo fix this, I've ensured we have an appropriate lock in\n`_on_leader_follower_report_shutdown_intent` to guard against this.\n\nI ran the test 500 times to validate that this fix works.\n\nCloses #35863\nghstack-source-id: 101641463\n\nTest Plan:\n1) waitforbuildbot\n2) Ran the test 500 times.\n\nDifferential Revision: D20884373\n\nfbshipit-source-id: 9d580e9892adffc0c9a4c2e832881fb291a1ff16",
    "Number of deleted lines": 17,
    "Deleted lines": "-    assert (\n-        worker_name in _ALL_WORKER_NAMES\n-    ), \"{worker_name} is not expected by leader.\".format(worker_name=worker_name)\n-    intent_worker_names = _wait_all_workers_sequence_id_to_states[\n-        sequence_id\n-    ].intent_worker_names\n-    assert (\n-        worker_name not in intent_worker_names\n-    ), \"{worker_name} reported intent sequence id {sequence_id} twice. \".format(\n-        worker_name=worker_name, sequence_id=sequence_id\n-    )\n-    intent_worker_names.add(worker_name)\n-    if _ALL_WORKER_NAMES == intent_worker_names:\n-        _set_proceed_shutdown_signal(sequence_id)\n-    ), \"Termination signal sequence id {} got set twice.\".format(\n-        sequence_id=sequence_id\n-    )",
    "Added lines": "+    with _wait_all_workers_dict_lock:\n+        assert (\n+            worker_name in _ALL_WORKER_NAMES\n+        ), \"{worker_name} is not expected by leader.\".format(worker_name=worker_name)\n+        intent_worker_names = _wait_all_workers_sequence_id_to_states[\n+            sequence_id\n+        ].intent_worker_names\n+        assert (\n+            worker_name not in intent_worker_names\n+        ), \"{worker_name} reported intent sequence id {sequence_id} twice. \".format(\n+            worker_name=worker_name, sequence_id=sequence_id\n+        )\n+        intent_worker_names.add(worker_name)\n+        if _ALL_WORKER_NAMES == intent_worker_names:\n+            _set_proceed_shutdown_signal(sequence_id)\n+    ), \"Termination signal sequence id {} got set twice.\".format(sequence_id)",
    "Label": "clean"
},
{
    "Id": 1242,
    "Library": "pytorch",
    "Date": "2020/04/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ae71c5c7e6922c54189399dc1e5554cef7e0011d",
    "Root Cause": "N.A",
    "Bug report": "Optimized bincount for the CPU by removing extra size() calls (#35822)\n\nSummary:\nBy removing the calls of `size` that were effectively nops, I've managed to make `bincount_cpu` run around 6 times faster on my machine. EDIT: (Running Windows 10, I'm suspecting this may be a Windows-specific bug)\n\nFor histogramming 1e7 samples with 1e5 bins, best of 20 with 10 runs each\nBefore: 3.201189\nAfter: 0.466188\nPull Request resolved: https://github.com/pytorch/pytorch/pull/35822\n\nDifferential Revision: D20919885\n\nPulled By: ezyang\n\nfbshipit-source-id: 1657056d69a02f1e61434f4cc8fa800f8d4e1fe8",
    "Number of deleted lines": 2,
    "Deleted lines": "-    for (int64_t i = 0; i < self.size(0); i++) {\n-    for (int64_t i = 0; i < self.size(0); i++) {",
    "Added lines": "+  int64_t self_size = self.size(0);\n+    for (int64_t i = 0; i < self_size; i++) {\n+    for (int64_t i = 0; i < self_size; i++) {",
    "Label": "clean"
},
{
    "Id": 1243,
    "Library": "pytorch",
    "Date": "2020/03/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/063275fd33d6269b091ff7d49d153d168ff0c0a5",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in subgraph rewriters. (#35704)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/35704\n\nDue to not clearing nodes_to_delete_, when we try to write graph rewrite\npass with multiple patterns, this is observed:\nIndexError: vector::_M_range_check: __n (which is 0) >= this->size() (which is 0)\n\nTest Plan:\nThe PR stacked on top of this run into this error in the unit test.\n\nImported from OSS\n\nDifferential Revision: D20746593\n\nfbshipit-source-id: 9b55604f49ff2ee2a81a61827880cb679c44607a",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  nodes_to_delete_.clear();",
    "Label": "clean"
},
{
    "Id": 1244,
    "Library": "pytorch",
    "Date": "2020/03/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/dbd2b8bb4170f48e0e58bfd3d29f0e4e7b644ded",
    "Root Cause": "N.A",
    "Bug report": "[SigridHashOp] Fix converter (#34836)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34836\n\nOnce SigridHashOp argument is supplied, I realized the shape inference is still wrong because the argument is not supplied in the debug_ssa. Thanks to yinghai, I didn't fix the converter, fixing it in this diff\n\nTest Plan:\nRun the binary, and checked the exported op\n\n  op {\n\t\tinput: \"sequential_250/parallel/normalization/dper_feature_normalization/sparse_features_processor/sparse_feature_transform/gather_ranges_GSF_IDLIST_COOCCUR_APP_ID_NEKO_ORGANIC_1D_7D_INSTALL_V1/gathered_values_0\"\n\t\toutput: \"sequential_250/parallel/normalization/dper_feature_normalization/sparse_features_processor/sparse_feature_transform/sequential_1/hash_feature_ids/SigridHash:0_0\"\n\t\ttype: \"SigridHash\"\n\t\targ {\n\t\t\tname: \"salt\"\n\t\t\ti: 0\n\t\t}\n\t\targ {\n\t\t\tname: \"maxValue\"\n\t\t\ti: 100000\n\t\t}\n\t\targ {\n\t\t\tname: \"hashIntoInt32\"\n\t\t\ti: 1\n\t\t}\n\t\targ {\n\t\t\tname: \"net_pos\"\n\t\t\ti: 3\n\t\t}\n\t}\n\nit now have hashIntInt32\n\nReviewed By: yinghai\n\nDifferential Revision: D20457057\n\nfbshipit-source-id: 023ade5e66df82037a8f2da3174383dda8aff230",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    c->setHashIntoInt32(args.GetSingleArgument<bool>(\"hashIntoInt32\", false));\n+    op.add_arg()->CopyFrom(caffe2::MakeArgument<bool>(\n+        \"hashIntoInt32\", sigridHash->getHashIntoInt32()));",
    "Label": "clean"
},
{
    "Id": 1245,
    "Library": "pytorch",
    "Date": "2020/03/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/27410318ad84bb37525b3a8e128acbcb2da251ac",
    "Root Cause": "N.A",
    "Bug report": "[PyTorch][Mobile] Fix the operator latency issue.\n\nSummary: Last diff enabled operator stats for non-production build including AIBench. But the operator latency is off: https://our.intern.facebook.com/intern/aibench/details/414567479798816 as it is representing operator execution end time, and as the threadLocalDebugInfo was not set, the start time is 0. So this diff is fixing it by creating a new ThreadLocalDebugInfo object when op starts to run and store the model information for logging.\n\nTest Plan:\n```buck run mode/mac aibench:run_bench_macos -- -b aibench/specifications/models/pytorch/pytext/pytext_mobile_inference.json --platform android --framework pytorch --remote --devices SM-G960F-8.0.0-26```\nhttps://our.intern.facebook.com/intern/aibench/details/922804117425407\n\n```buck run mode/mac aibench:run_bench_macos -- -b aibench/specifications/models/pytorch/fbnet/fbnet_mobile_inference.json --platform android --framework pytorch --remote --devices SM-G960F-8.0.0-26```\nhttps://our.intern.facebook.com/intern/aibench/details/593403202250750\n\nReviewed By: xta0\n\nDifferential Revision: D20436388\n\nfbshipit-source-id: 740bc94c3f51daef6af9b45c1ed7a708f5fc8836",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#if defined(PYTORCH_MOBILE_OPERATOR_OBSERVER)\n+  auto debug_info = std::make_shared<MobileDebugInfo>();\n+  debug_info->setModelName(name());\n+  debug_info->setMethodName(method_name);\n+  at::setThreadLocalDebugInfo(debug_info);\n+#endif\n+",
    "Label": "clean"
},
{
    "Id": 1246,
    "Library": "pytorch",
    "Date": "2020/03/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fc6dce6033d4896c0413d49923aa869da00523f2",
    "Root Cause": "N.A",
    "Bug report": "[c10] Fix TORCH_INTERNAL_ASSERT_DEBUG_ONLY MSVC bug (#34173)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/34173\n\nTest Plan:\nTemporarily change `AT_ASSERTM` to `TORCH_INTERNAL_ASSERT_DEBUG_ONLY` to test MSVC fix.\n\n```\nbuck test mode/opt //caffe2/caffe2:caffe2_test_cpu -- 'BlobTest'\n```\n\n& CI\n\nReviewed By: yinghai\n\nDifferential Revision: D20235886\n\nfbshipit-source-id: 2b7d618e924a0ede95f4a6b8f60cc08e9d58b09d",
    "Number of deleted lines": 3,
    "Deleted lines": "-  while (false)           \\\n-  TORCH_INTERNAL_ASSERT(__VA_ARGS__)\n-#define TORCH_INTERNAL_ASSERT_DEBUG_ONLY(...) C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__))",
    "Added lines": "+  while (false)                               \\\n+  C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__))\n+#define TORCH_INTERNAL_ASSERT_DEBUG_ONLY(...) \\\n+  C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__))",
    "Label": "clean"
},
{
    "Id": 1247,
    "Library": "pytorch",
    "Date": "2020/03/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a19db54b368eecc515387bb83907bb7dc7d93c88",
    "Root Cause": "N.A",
    "Bug report": "[Redo][ATen] Remove AT_ASSERTM from Blob::free_() (#34168)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34168\n\nRedo D19153199. It was reverted because it broke CI, due to the change of `AT_ASSERTM` to `TORCH_INTERNAL_ASSERT_DEBUG_ONLY`. Two problems:\n1) bug in `TORCH_INTERNAL_ASSERT_DEBUG_ONLY` about MSVC. I'm sending another diff to fix this bug.\n2) BlobTest was expecting `Blob::template Get<T>()` to throw when there is a type mismatch.\n\nFor now I'll leave `AT_ASSERTM` as it is.\n\nTest Plan:\n```\nbuck test mode/dev //caffe2/caffe2:caffe2_test_cpu -- 'BlobTest' --run-disabled\nbuck test mode/opt //caffe2/caffe2:caffe2_test_cpu -- 'BlobTest' --run-disabled\n```\n\nReviewed By: yinghai\n\nDifferential Revision: D20235225\n\nfbshipit-source-id: 594dad97c03c419afaa8f9023408bc5a119b3cfa",
    "Number of deleted lines": 6,
    "Deleted lines": "-  // TODO Remove ShareExternal() and have Blob always own its content\n-    pointer_ = static_cast<void*>(allocated);\n-    if (has_ownership_) {\n-      AT_ASSERTM(pointer_ != nullptr, \"Can't have ownership of nullptr\");\n-  void* pointer_ = nullptr;\n-  bool has_ownership_ = false;",
    "Added lines": "+    pointer_ = allocated;\n+    if (has_ownership_ && pointer_ != nullptr) {\n+  void* pointer_;\n+  bool has_ownership_;",
    "Label": "clean"
},
{
    "Id": 1248,
    "Library": "pytorch",
    "Date": "2020/02/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d41c8d0461aef76e5d9a67b2f603d09f540746b5",
    "Root Cause": "N.A",
    "Bug report": "Correctly preserve \"not set anywhere\" TensorOptions when merging. (#33510)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33510\n\nPreviously, we would fill in TensorOption with defaults whenever an\nitem was missing from both the left and right side of the merge.  This\nis morally incorrect: if we don't have an item on the left or right,\nwe should keep the entry empty (so the downstream user can apply\nthe appropriate defaulting rule).\n\nI don't think this caused any bugs, but I noticed this error when\nworking on a later patch in my diff stack.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20001775\n\nPulled By: ezyang\n\nfbshipit-source-id: 88139fc268b488cd1834043584a0d73f46c8ecaa",
    "Number of deleted lines": 5,
    "Deleted lines": "-    if (!r.has_device()) r.set_device(device());\n-    if (!r.has_dtype()) r.set_dtype(dtype());\n-    if (!r.has_layout()) r.set_layout(layout());\n-    if (!r.has_requires_grad()) r.set_requires_grad(requires_grad());\n-    if (!r.has_pinned_memory()) r.set_pinned_memory(pinned_memory());",
    "Added lines": "+    if (!r.has_device()) r.set_device(device_opt());\n+    if (!r.has_dtype()) r.set_dtype(dtype_opt());\n+    if (!r.has_layout()) r.set_layout(layout_opt());\n+    if (!r.has_requires_grad()) r.set_requires_grad(requires_grad_opt());\n+    if (!r.has_pinned_memory()) r.set_pinned_memory(pinned_memory_opt());",
    "Label": "clean"
},
{
    "Id": 1249,
    "Library": "pytorch",
    "Date": "2020/02/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fd175fa8a235569bda75718e3f9573e28d889f12",
    "Root Cause": "N.A",
    "Bug report": "fix bugs in gen_pyi.py (#33748)\n\nSummary:\nThis loop should generate type hints for inplace binary operator methods (`binop` variable) but had been using `name` variable. That's why that wrong type hints had been generated.\n\nResolve https://github.com/pytorch/pytorch/issues/33698\n\n ---\n\nCurrent `__init__.pyi` has these type hints.\n\n```python\nclass Tensor:\n\n    # some codes here...\n\n    overload\n    def zeros_like_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like_(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like_(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like_(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like__(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like__(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like__(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like__(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like___(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like___(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like___(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like___(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like____(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like____(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def zeros_like____(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def zeros_like____(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n\n    # some codes here...\n```\n\nBut `__init__.pyi` should generate these type hints.\n\n```python\nclass Tensor:\n\n    # some codes here...\n\n    overload\n    def add_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def add_(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def add_(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def add_(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n\n    # some codes here...\n\n    overload\n    def div_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def div_(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def div_(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def div_(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n\n    # some codes here...\n\n    overload\n    def mul_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def mul_(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def mul_(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def mul_(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n\n    # some codes here...\n\n    overload\n    def sub_(self, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def sub_(self, value: Number, other: Union[Tensor, Number]) -> Tensor: ...\n    overload\n    def sub_(self, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n    overload\n    def sub_(self, value: Number, other: Union[Tensor, Number], *, out: Optional[Tensor]=None) -> Tensor: ...\n\n    # some codes here...\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33748\n\nDifferential Revision: D20090444\n\nPulled By: ngimel\n\nfbshipit-source-id: e4a5dd08126629ec4c54b630a87ee540e669ec9a",
    "Number of deleted lines": 5,
    "Deleted lines": "-                name += '_'\n-            unsorted_tensor_method_hints[name].append(\n-                ' -> Tensor: ...'.format(name, out_suffix))\n-            unsorted_tensor_method_hints[name].append(\n-                ' -> Tensor: ...'.format(name, out_suffix))",
    "Added lines": "+                binop += '_'\n+            unsorted_tensor_method_hints[binop].append(\n+                ' -> Tensor: ...'.format(binop, out_suffix))\n+            unsorted_tensor_method_hints[binop].append(\n+                ' -> Tensor: ...'.format(binop, out_suffix))",
    "Label": "clean"
},
{
    "Id": 1250,
    "Library": "pytorch",
    "Date": "2020/02/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/54e41a87eb2f8c64641cc359c5c14095e9b32b90",
    "Root Cause": "N.A",
    "Bug report": "Make ELU great again (#33244)\n\nSummary:\nDue to compiler bug, we have to make some workaround on ELU for CUDA. A necessary condition for this bug to happen is `invoke_with_array` in `Loops.cuh`. Now, https://github.com/pytorch/pytorch/issues/33222 will kill that function, and we need to remove that workaround once https://github.com/pytorch/pytorch/issues/33222 is landed.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33244\n\nDifferential Revision: D20076197\n\nPulled By: ngimel\n\nfbshipit-source-id: 39f99783014c78cecad1c39cb46092278ff220b9",
    "Number of deleted lines": 15,
    "Deleted lines": "-        // WARNING (@zasdfgbnm, 2020-01-16): This is very fragile!\n-        //\n-        // The code below does not look like a great implementation because both positive\n-        // and negative case are computed regardless of the condition, and you might want\n-        // to optimize this. But this implementation is due to a compiler bug in handling\n-        // branch. If we implement it in a correct way, the generated code will produce\n-        // wrong result. This bug should be fixed in future CUDA, but we will need this\n-        // workaround for maybe years until all the current CUDAs become obsolete.\n-        //\n-        // TODO: this workaround might become no longer necessary if the implementation of\n-        // GPU loop in `Loops.cuh` is changed. We should make this great again once that\n-        // change happens.\n-        scalar_t positive_case = a * poscoef;\n-        scalar_t negative_case = (static_cast<scalar_t>(std::exp(a * negiptcoef)) - scalar_t(1.)) * negcoef;\n-        return a > scalar_t(0) ? positive_case : negative_case;",
    "Added lines": "+        return a > scalar_t(0) ? a * poscoef : (static_cast<scalar_t>(std::exp(a * negiptcoef)) - scalar_t(1.)) * negcoef;",
    "Label": "clean"
},
{
    "Id": 1251,
    "Library": "pytorch",
    "Date": "2020/02/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5fa03d4dbbb8cd1460e5c017c09f0894c8a452eb",
    "Root Cause": "N.A",
    "Bug report": "Fix bug where we were trying to get a schema for prim::Constant, which is not registered as an operator. (#33645)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33645\n\nFix bug where we were trying to get a schema for prim::Constant, which is not registered as an operator.\nghstack-source-id: 98785729\n\nTest Plan: buck test mode/dev //pytext/models/test:scripted_seq2seq_generator_test -- 'test_generator \\(pytext\\.models\\.test\\.scripted_seq2seq_generator_test\\.ScriptedSeq2SeqGeneratorTest\\)'\n\nDifferential Revision: D20050833\n\nfbshipit-source-id: cc38510b0135b750fdf57fb9c1e66ce1d91ee128",
    "Number of deleted lines": 1,
    "Deleted lines": "-  auto compiled_graphs = gradientInfoForSchema(node->schema());",
    "Added lines": "+  auto maybe_schema = node->maybeSchema();\n+  if (!maybe_schema) {\n+    return c10::nullopt;\n+  }\n+  auto compiled_graphs = gradientInfoForSchema(*maybe_schema);",
    "Label": "clean"
},
{
    "Id": 1252,
    "Library": "pytorch",
    "Date": "2020/02/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/879cf0b15a54c7848ae710e3d0ec62c4a9d7d3dd",
    "Root Cause": "N.A",
    "Bug report": "fix typing bug of LambdaLR.__init__ (#33271)\n\nSummary:\n## problem\n\n```python\nclass LambdaLR(_LRScheduler):\n    \"\"\"Sets the learning rate of each parameter group to the initial lr\n    times a given function. When last_epoch=-1, sets initial lr as lr.\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        lr_lambda (function or list): A function which computes a multiplicative\n            factor given an integer parameter epoch, or a list of such\n            functions, one for each group in optimizer.param_groups.\n        last_epoch (int): The index of last epoch. Default: -1.\n\n    Example:\n        >>> # Assuming optimizer has two groups.\n        >>> lambda1 = lambda epoch: epoch // 30\n        >>> lambda2 = lambda epoch: 0.95 ** epoch\n        >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n        >>> for epoch in range(100):\n        >>>     train(...)\n        >>>     validate(...)\n        >>>     scheduler.step()\n    \"\"\"\n```\n\n`LambdaLR` takes a lambda that returns a float and takes a int, or a list of such lambdas.\n\n## related issue\n\nResolve https://github.com/pytorch/pytorch/issues/32645\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33271\n\nDifferential Revision: D19878665\n\nPulled By: vincentqb\n\nfbshipit-source-id: 50b16caea13de5a3cbd187e688369f33500499d0",
    "Number of deleted lines": 2,
    "Deleted lines": "-from typing import Iterable, Any, Optional, Callable\n-    def __init__(self, optimizer: Optimizer, lr_lambda: float, last_epoch: int=...) -> None: ...",
    "Added lines": "+from typing import Iterable, Any, Optional, Callable, Union, List\n+    def __init__(self, optimizer: Optimizer, lr_lambda: Union[Callable[[int], float], List[Callable[[int], float]]], last_epoch: int=...) -> None: ...",
    "Label": "clean"
},
{
    "Id": 1253,
    "Library": "pytorch",
    "Date": "2020/02/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cb4e6d025aadf57a089b49c1e7044a2a635f3105",
    "Root Cause": "N.A",
    "Bug report": "Updates numpy to tensor negative stride error message (#33254)\n\nSummary:\nSee https://discuss.pytorch.org/t/bugs-about-torch-from-numpy-array/43312.\n\nThis update incorporates albanD 's suggestion into the error message, saving future users from having to ask or look on the forums if they encounter this issue and don't mind making their arrays contiguous.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33254\n\nDifferential Revision: D19885808\n\nPulled By: mruberry\n\nfbshipit-source-id: 8f0fd994cf8c088bf3c3940ab4dfb3ddbc5b3ede",
    "Number of deleted lines": 2,
    "Deleted lines": "-          \"some of the strides of a given numpy array are negative. This is \"\n-          \"currently not supported, but will be added in future releases.\");",
    "Added lines": "+          \"At least one stride in the given numpy array is negative, \"\n+          \"and tensors with negative strides are not currently supported. \"\n+          \"(You can probably work around this by making a copy of your array \"\n+          \" with array.copy().) \");",
    "Label": "clean"
},
{
    "Id": 1254,
    "Library": "pytorch",
    "Date": "2020/02/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e45343fa1428148a3de1540edd559e3c6029e14e",
    "Root Cause": "N.A",
    "Bug report": "TORCH_INTERNAL_ASSERT_DEBUG_ONLY not eating message string (#33251)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33251\n\nSomehow this was preventing `c10::Error` exceptions from ever being thrown on windows when `defined(NDEBUG) == false`. Kinda scary.\n\nTest Plan: sandcastle green, made sure `intrusive_ptr_test.cpp` (givenStackObject_whenReclaimed_thenCrashes) passed inside ovrsource using `mode/win/dev-debug`\n\nReviewed By: malfet\n\nDifferential Revision: D19865667\n\nfbshipit-source-id: c32d5752025c043e57d16c6d14a94b069bed0bc3",
    "Number of deleted lines": 1,
    "Deleted lines": "-#define TORCH_INTERNAL_ASSERT_DEBUG_ONLY(...) TORCH_INTERNAL_ASSERT(__VA_ARGS__)",
    "Added lines": "+#define TORCH_INTERNAL_ASSERT_DEBUG_ONLY(...) C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(__VA_ARGS__))",
    "Label": "clean"
},
{
    "Id": 1255,
    "Library": "pytorch",
    "Date": "2020/02/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e025f393f6078fe8ccdb5e43e575d9f074bc4e53",
    "Root Cause": "N.A",
    "Bug report": "windows template specialization bug (#33076)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33076\n\nattempt at fixing https://github.com/pytorch/pytorch/issues/30886\n\nTest Plan: circleCI with `call \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=14.16` passes\n\nDifferential Revision: D19784550\n\nfbshipit-source-id: 9fb42c3854d1d00d96cd7179bef9dd1aa2972ea6",
    "Number of deleted lines": 1,
    "Deleted lines": "-#ifdef __cpp_lib_logical_traits",
    "Added lines": "+#if defined(__cpp_lib_logical_traits) && !(defined(_MSC_VER) && _MSC_VER < 1920)",
    "Label": "clean"
},
{
    "Id": 1256,
    "Library": "pytorch",
    "Date": "2020/01/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/de5821d291032ec5f8fb4d289beab3751aa58c8e",
    "Root Cause": "N.A",
    "Bug report": "Torchscript print to logcat (#31456)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/31456\n\nExternal request https://discuss.pytorch.org/t/jit-android-debugging-the-model/63950\n\nBy default torchscript print function goes to stdout. For android it is not seen in logcat by default.\nThis change propagates it to logcat.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19171405\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: f9c88fa11d90bb386df9ed722ec9345fc6b25a34",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#include <torch/csrc/jit/print_handler.h>\n+#include <android/log.h>\n+  static void preModuleLoadSetupOnce() {\n+#ifdef __ANDROID__\n+    torch::jit::setPrintHandler([](const std::string& s) {\n+      __android_log_print(ANDROID_LOG_DEBUG, \"pytorch-print\", \"%s\", s.c_str());\n+    });\n+#endif\n+  }\n+\n+    static const int once = []() {\n+      preModuleLoadSetupOnce();\n+      return 0;\n+    }();\n+    ((void)once);\n+",
    "Label": "clean"
},
{
    "Id": 1257,
    "Library": "pytorch",
    "Date": "2019/12/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a009fc14befcd3939bfe5904c6f90919dd528a3e",
    "Root Cause": "N.A",
    "Bug report": "Workaround hcc bug regarding extern \"C\" definitions (#30313)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30313\n\nSee comments in code about the bug.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18762360\n\nPulled By: ezyang\n\nfbshipit-source-id: 406a01f2f0c3722b381428c89afd67b3c3c19142",
    "Number of deleted lines": 2,
    "Deleted lines": "-THC_API __host__ void THCRandom_getRNGState(at::Generator *gen_, THByteTensor *rng_state)\n-THC_API __host__ void THCRandom_setRNGState(at::Generator *gen_, THByteTensor *rng_state)",
    "Added lines": "+// NB: ROCm compiler seems to have a bug where __host__ functions must be\n+// explicitly specified extern \"C\" otherwise ROCm compiler doesn't respect it.\n+// See https://github.com/RadeonOpenCompute/hcc/issues/839\n+extern \"C\" __host__ void THCRandom_getRNGState(at::Generator *gen_, THByteTensor *rng_state)\n+extern \"C\" __host__ void THCRandom_setRNGState(at::Generator *gen_, THByteTensor *rng_state)",
    "Label": "clean"
},
{
    "Id": 1258,
    "Library": "pytorch",
    "Date": "2019/11/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8e3486de81d848e5c9a375134b3b14998ac36654",
    "Root Cause": "N.A",
    "Bug report": "No debug symbols in release android buidls (#30123)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/30123\n\nIn groovy string `'false'` is resolved as boolean `true`\n\nthats why even as in `gradle.properties`:\n```\nnativeLibsDoNotStrip=false\n```\nbranch `if (nativeLibsDoNotStrip)` always passed\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18606907\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: c10140e775624294c732e78ae3c41e05c7c9ad92",
    "Number of deleted lines": 1,
    "Deleted lines": "-        if (nativeLibsDoNotStrip) {",
    "Added lines": "+        if (nativeLibsDoNotStrip.toBoolean()) {\n+            logger.warn('WARNING: nativeLibsDoNotStrip==true; debug symbols included')",
    "Label": "clean"
},
{
    "Id": 1259,
    "Library": "pytorch",
    "Date": "2019/11/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7434da2c3f49b8e01dfcd2d504dba617a300c41b",
    "Root Cause": "N.A",
    "Bug report": "value assigned but never used in _recursive.py (#29181)\n\nSummary:\n# Description\nI'm new to this project just wanted to start with small bug fixes. I found some unused local variables and I've removed them in this pr.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29181\n\nDifferential Revision: D18319893\n\nPulled By: suo\n\nfbshipit-source-id: e4f9f13b6db2ca213015569deb12d3fd9beb74a8",
    "Number of deleted lines": 3,
    "Deleted lines": "-        found = False\n-    name = [def_.name().name for def_ in defs]\n-            original_name = item.__name__",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1260,
    "Library": "pytorch",
    "Date": "2019/10/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/15deee25bc5d4e648992f489fc6dc1040090edf9",
    "Root Cause": "N.A",
    "Bug report": "Fix aten::format regex for clang8 (#28916)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28916\n\nThe previous regex caused a `std::regex_error` under clang8 complaining about `error_brack`, which is strange because the square brackets are balanced. Seems like a stdlib bug to me. So to workaround this, I've switched to the older regex with a non-greedy match in the inner atom\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D18232654\n\nPulled By: jamesr66a\n\nfbshipit-source-id: f82a9a24acf090010b03f23454d2b0f7a1e3589e",
    "Number of deleted lines": 1,
    "Deleted lines": "-          std::regex unsupported_options(\"\\\\{([^}]+)\\\\}\");",
    "Added lines": "+          std::regex unsupported_options(\"\\\\{(.*?)\\\\}\");",
    "Label": "clean"
},
{
    "Id": 1261,
    "Library": "pytorch",
    "Date": "2019/10/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/50fd20b64a97cc75a94f95c5b08cbf9d0580340c",
    "Root Cause": "N.A",
    "Bug report": "fix bug on setup.py to include header files on caffe2/utils/math (#28869)\n\nSummary:\nThis problem is from issue [https://github.com/pytorch/pytorch/issues/28753](https://github.com/pytorch/pytorch/issues/28753)\n\nThe header files on directories`math` and `threadpool` should be included on the built package because they are included on the other header files, such as on file `torch/include/caffe2/utils/math.h`\n```\n#include \"caffe2/core/common.h\"\n#include \"caffe2/core/types.h\"\n#include \"caffe2/utils/math/broadcast.h\"\n#include \"caffe2/utils/math/elementwise.h\"\n#include \"caffe2/utils/math/reduce.h\"\n#include \"caffe2/utils/math/transpose.h\"\n#include \"caffe2/utils/math/utils.h\"\n```\nBut the `setup.py` doesn't include the header files on `master` branch. The header files on `utils` directory of a built `torch` package are the following:\n```\n> ls include/caffe2/utils\nbench_utils.h  conversions.h  eigen_utils.h    map_utils.h    murmur_hash3.h   proto_wrap.h      smart_tensor_printer.h\ncast.h         cpuid.h        filler.h         math-detail.h  proto_convert.h  signal_handler.h  string_utils.h\ncblas.h        cpu_neon.h     fixed_divisor.h  math.h         proto_utils.h    simple_queue.h    zmq_helper.h\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28869\n\nDifferential Revision: D18226319\n\nPulled By: soumith\n\nfbshipit-source-id: 51575ddc559181c069b3324aa9b2d1669310ba25",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                'include/caffe2/utils/**/*.h',",
    "Label": "clean"
},
{
    "Id": 1262,
    "Library": "pytorch",
    "Date": "2019/10/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c36552c4cbc8fd9c9346d213b11654c126615d5d",
    "Root Cause": "N.A",
    "Bug report": "Fixing dispatch error in windows debug builds (#24360)\n\nSummary:\nnullptr initialization values for dispatch pointers were overwriting values set using the REGISTER_DISPATCH macro.\n\nRelevant issue: https://github.com/pytorch/pytorch/issues/22681\nPull Request resolved: https://github.com/pytorch/pytorch/pull/24360\n\nDifferential Revision: D17952241\n\nPulled By: ezyang\n\nfbshipit-source-id: 4bf86dc24153e504bbeacb526c58fd8230bb972a",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+// Fixing dispatch error in Windows debug builds.\n+// See https://github.com/pytorch/pytorch/issues/22681 for more details.\n+#if defined(_MSC_VER) && defined(_DEBUG)\n+  FnPtr cpu_dispatch_ptr;\n+  FnPtr cuda_dispatch_ptr;\n+  FnPtr hip_dispatch_ptr;\n+#else\n+#endif",
    "Label": "clean"
},
{
    "Id": 1263,
    "Library": "pytorch",
    "Date": "2019/10/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f7d7c4b72fd936181416f55b4f15c05de7076110",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug of C++ L-BFGS optimizer (#27606)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/27605: The C++ L-BFGS Optimizer will not work properly if there are one or more registered tensors with no grad in the model:\n```\nterminate called after throwing an instance of 'c10::Error'\n  what():  There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::view.  This usually means that this function requires a non-empty list of Tensors.  Available functions are [CUDATensorId, QuantizedCPUTensorId, VariableTensorId, CPUTensorId, MkldnnCPUTensorId] (lookup_ at /pytorch/aten/src/ATen/core/dispatch/DispatchTable.h:245)\n```\n\nAdd some `if (!parameter.grad().defined()) {...}` in the ` torch/csrc/api/src/optim/lbfgs.cpp`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27606\n\nDifferential Revision: D17866550\n\nPulled By: yf225\n\nfbshipit-source-id: bcaf0bf75b93c57304856b03d8984c1617ebbfef",
    "Number of deleted lines": 1,
    "Deleted lines": "-    views.push_back(parameter.grad().view(-1));",
    "Added lines": "+    if (!parameter.grad().defined()) {\n+      views.push_back(parameter.new_empty({parameter.numel()}).zero_());\n+    }\n+    else if (parameter.grad().is_sparse()) {\n+      views.push_back(parameter.grad().to_dense().view(-1));\n+    }\n+    else {\n+      views.push_back(parameter.grad().view(-1));\n+    }",
    "Label": "clean"
},
{
    "Id": 1264,
    "Library": "pytorch",
    "Date": "2019/09/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/93836015234cc7a7ed95ea183a2d539e0e91feb9",
    "Root Cause": "N.A",
    "Bug report": "fix to operate on cuda kernel with clang and libc++ (#25553)\n\nSummary:\nWe find a bug about `std::tuple` with nvcc.\n\nIn C++11, `std::tuple` constructor is constexpr in libstdc++, but is not constexpr in libc++.\n\nhttps://github.com/pytorch/pytorch/blob/c36b77fcdad3d54227cf0fd51693eb57035002c0/aten/src/ATen/native/cuda/Loops.cuh#L109-L111\n\nThe lines have occurred crashes in CUDA with a message `scan failed with synchronize`. It is a error message of cuda initialization.\n\nThe purpose of this PR is fixed for loop in nvcc and libc++ by not using `std::tuple`.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25553\n\nDifferential Revision: D17582118\n\nPulled By: yf225\n\nfbshipit-source-id: d6f62ed46c2415b48eb49f8a051cf3c0e7cb23ce",
    "Number of deleted lines": 21,
    "Deleted lines": "-template <typename traits, typename index_t, std::size_t... I>\n-C10_HOST_DEVICE typename traits::ArgsTuple\n-dereference_impl(char* const C10_RESTRICT data[], const index_t strides[], int i,\n-                 c10::guts::index_sequence<I...>) {\n-  return std::make_tuple(\n-      *(typename traits::template arg<I>::type*)\n-        (data[I] + i * strides[I])...);\n-template <typename traits, typename index_t>\n-C10_HOST_DEVICE typename traits::ArgsTuple\n-dereference(char* const C10_RESTRICT data[], const index_t strides[], int i) {\n-  return dereference_impl<traits>(data, strides, i, Indices{});\n-    launch_kernel<launch_size_1d, 1>(numel, [=]__device__(int idx) {\n-      *out = c10::guts::apply(f, dereference<traits>(\n-          &data.data[1],\n-          &strides.data[1],\n-          idx));\n-    launch_kernel<launch_size_nd, launch_bound2>(numel, [=]__device__(int idx) {\n-      *out = c10::guts::apply(f, dereference<traits>(\n-          &data.data[1],\n-          &offsets.data[1],\n-          1));",
    "Added lines": "+template <typename traits, typename func_t, typename index_t, size_t... I>\n+C10_HOST_DEVICE typename traits::result_type\n+invoke_impl(const func_t &f, char *const C10_RESTRICT data[], const index_t strides[], int i,\n+            c10::guts::index_sequence<I...>) {\n+  return f(*(typename traits::template arg<I>::type*)(data[I] + i * strides[I])...);\n+template <typename func_t, typename index_t, typename traits = function_traits<func_t>>\n+C10_HOST_DEVICE typename traits::result_type\n+invoke(const func_t &f, char *const C10_RESTRICT data[], const index_t strides[], int i) {\n+  return invoke_impl<traits>(f, data, strides, i, Indices{});\n+\n+   \n+\n+    launch_kernel<launch_size_1d, 1>(numel, [=]GPU_LAMBDA(int idx) {\n+      *out = invoke(f, &data.data[1], &strides.data[1], idx);\n+    launch_kernel<launch_size_nd, launch_bound2>(numel, [=]GPU_LAMBDA(int idx) {\n+      *out = invoke(f, &data.data[1], &offsets.data[1], 1);",
    "Label": "clean"
},
{
    "Id": 1265,
    "Library": "pytorch",
    "Date": "2019/09/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/21314cfddee746dc32980bdf7d6ea41b3b095d14",
    "Root Cause": "N.A",
    "Bug report": "\"fixing\" gcc bug introduced with cuda 10.1 (#26445)\n\nSummary:\n> Cuda 10.1: Nvidia, you're now \"fixing\" gcc bugs that gcc doesn't even have\n\nsee [discussion]( https://devtalk.nvidia.com/default/topic/1048037/linux/cuda-10-1-nvidia-you-re-now-quot-fixing-quot-gcc-bugs-that-gcc-doesn-t-even-have/) for detail\nPull Request resolved: https://github.com/pytorch/pytorch/pull/26445\n\nReviewed By: soumith\n\nDifferential Revision: D17533850\n\nPulled By: mingbowan\n\nfbshipit-source-id: d668b0c4a3c71d58b4a0fa8e00d873708add3dea",
    "Number of deleted lines": 4,
    "Deleted lines": "-    const int input_size = this->InputSize();\n-    \n-    \n-    ",
    "Added lines": "+    // the code is written this way because of 10.1 + gcc 7.3.1 compiler bug\n+    // as discussed at https://devtalk.nvidia.com/default/topic/1048037/linux/cuda-10-1-nvidia-you-re-now-quot-fixing-quot-gcc-bugs-that-gcc-doesn-t-even-have/\n+    const int input_size = (*this).InputSize();\n+\n+\n+",
    "Label": "clean"
},
{
    "Id": 1266,
    "Library": "pytorch",
    "Date": "2019/09/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/aeb6532e7f7684e82c1f7ebef59b9fee110fb227",
    "Root Cause": "N.A",
    "Bug report": "BlobReference __getattr__ can only throw AttributeError (#26654)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/26654\n\nAs per python contract, __getattr__ can only throw AttributeError. Throwing something else breaks hasattr() and causes upstream issues.\n\nSimilar bug was in pytorch earlier.\n\nTest Plan: builds\n\nDifferential Revision: D17529471\n\nfbshipit-source-id: bb6ac6c9e3be8b80fa2967e6a2e293afd1594cf9",
    "Number of deleted lines": 2,
    "Deleted lines": "-            raise RuntimeError(\n-            raise RuntimeError(",
    "Added lines": "+            raise AttributeError(\n+            raise AttributeError(",
    "Label": "clean"
},
{
    "Id": 1267,
    "Library": "pytorch",
    "Date": "2019/09/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e4821012ada89993695737d15f271c01fe7d0dd6",
    "Root Cause": "N.A",
    "Bug report": "prevent generating caffe2::mkldnn for multiple times (#25257)\n\nSummary:\nThis is a similar problem to https://github.com/pytorch/pytorch/issues/25004. After the merge of https://github.com/pytorch/pytorch/issues/25167, I recompiled torch and discovered another similar bug.\n\nezyang please take a look\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25257\n\nDifferential Revision: D17528116\n\nPulled By: ezyang\n\nfbshipit-source-id: 1657d9ee6dced3548f246010b05e2b3c25c37dee",
    "Number of deleted lines": 1,
    "Deleted lines": "-add_library(caffe2::mkldnn INTERFACE IMPORTED)",
    "Added lines": "+if(NOT TARGET caffe2::mkldnn)\n+  add_library(caffe2::mkldnn INTERFACE IMPORTED)\n+endif()\n+",
    "Label": "clean"
},
{
    "Id": 1268,
    "Library": "pytorch",
    "Date": "2019/09/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/afa5d0823b749a134e85c57a611fef9f7cefaa83",
    "Root Cause": "N.A",
    "Bug report": "Fixes big endian arch bugs. (#26383)\n\nSummary:\nSerialization.cpp fails on big endian machines.\nThis patch fixes the endian bugs and also makes the pytorch\nmodel files portable across different endian architectures.\nx86 generated model file can be read on s390 arch.\n\nFirst problem, is serialization.cpp forgets to convert \"size\" value\nof the storage elements to the native byte order.\ntorch.load throws an assertion as a result\n(see the first stack trace below).\n\nSecond problem is when it reads the model from storage (doRead)\nit decodes values to little endian which is the wrong order\non a big endian machine.  The decode should be\nto THP_nativeByteOrder() instead\n\t(see the model dump below)\n```loaded_model = torch.load( opt.model_file, map_location=torch.device(\"cpu\"))\nFile \"/usr/local/lib/python3.6/dist-packages/torch/serialization.py\", line 422, in load\nreturn _load(f, map_location, pickle_module, **pickle_load_args)\nFile \"/usr/local/lib/python3.6/dist-packages/torch/serialization.py\", line 616, in _load\ndeserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)\nRuntimeError: storage has wrong size: expected 2305843009213693952 got 32\n\t(the very long number is actually 32 in the wrong endianness)\n```\n\nModel file load on x86 (correct output)\n```>>> import torch\n>>> torch.load('400f2k_best.model', map_location=torch.device(\"cpu\"))\n{'epoch': 24, 'model_type': 'emb_aec', 'classifier_model': OrderedDict([('model.0.weight', tensor([[ 2.4608e-01, -1.1174e-01, -1.0854e-01,  4.0124e-01, -1.5261e-02,\n         -1.2206e-01,  1.3229e-01, -1.2615e-01, -5.2773e-01,  2.6333e-01,\n         -3.1462e-03, -1.4902e-01,  9.8545e-02, -1.5789e-01, -2.2625e-01,\n         -1.0776e-01, -9.0895e-02, -3.8530e-01,  9.1152e-01, -3.9720e-01,\n         -8.5848e-01, -4.7837e-02, -1.5178e-01,  8.5023e-02,  1.5013e-01,\n         -9.9294e-02, -2.7422e-01, -4.3986e-01, -4.4297e-01, -3.9570e-01,\n```\n\nModel file load on s390x (wrong endianness; notice the exponents)\n```>>> import torch\n>>> torch.load( \"400f2k_best.model\", map_location=torch.device(\"cpu\"))\n{'epoch': 24, 'model_type': 'emb_aec', 'classifier_model': OrderedDict([('model.0.weight', tensor([[ 9.2780e+21, -9.7722e-11,  4.1350e+33,  7.782e+34,  4.2056e-31,\n          9.0784e+18,  1.1846e-32,  3.3320e-32, -4.8288e-28, -7.2679e+12,\n          1.5379e-16, -5.2604e+12, -4.7240e+17,  4.6092e-21, -1.8360e-20,\n         -2.7712e-31,  1.4548e-16, -2.5089e-27,  7.9094e-10,  7.1977e+34,\n          1.1930e+26,  8.4536e+15,  2.7757e+23, -5.8455e-10, -1.5611e+09,\n         -1.1311e-23,  6.6451e+19, -2.0970e+20,  3.4878e-19, -1.0857e-12,\n          7.8098e+22,  5.3998e-35],\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/26383\n\nDifferential Revision: D17480891\n\nfbshipit-source-id: f40569c7b9c4a1935dceb41f1a2508ce21ea3491",
    "Number of deleted lines": 4,
    "Deleted lines": "-  doWrite(fd, &size, sizeof(int64_t));\n-            THPByteOrder::THP_LITTLE_ENDIAN,\n-            THPByteOrder::THP_LITTLE_ENDIAN,\n-            THPByteOrder::THP_LITTLE_ENDIAN,",
    "Added lines": "+  if (THP_nativeByteOrder() == THPByteOrder::THP_LITTLE_ENDIAN)\n+    doWrite(fd, &size, sizeof(int64_t));\n+  else {\n+    int64_t nsize; // convert big endian cpu to little endian storage\n+    THP_encodeInt64Buffer((uint8_t*)&nsize, (const int64_t *)&size, THPByteOrder::THP_LITTLE_ENDIAN, 1);\n+    doWrite(fd, &nsize, sizeof(int64_t));\n+  }\n+  if (THP_nativeByteOrder() == THPByteOrder::THP_BIG_ENDIAN) {\n+    int64_t nsize; // convert little endian storage to big endian cpu\n+    nsize = size;\n+    THP_decodeInt64Buffer(&size, (const uint8_t*)&nsize, THP_nativeByteOrder(), 1);\n+  }\n+            THP_nativeByteOrder(),\n+            THP_nativeByteOrder(),\n+            THP_nativeByteOrder(),",
    "Label": "clean"
},
{
    "Id": 1269,
    "Library": "pytorch",
    "Date": "2019/09/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a6a7f354816ffcbc074f7aec787562ad3046cbda",
    "Root Cause": "N.A",
    "Bug report": "Better error messages in C2 ONNX backend (#25809)\n\nSummary:\nJust a tiny fix to make debugging easier (output errors to stderr and include in the exception message)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25809\n\nReviewed By: zrphercule\n\nDifferential Revision: D17329957\n\nPulled By: houseroad\n\nfbshipit-source-id: 0d73dd9f62c735fbc5096e6a7c0e5f58e4cd90ae",
    "Number of deleted lines": 5,
    "Deleted lines": "-        success = True\n-                    success = False\n-                    print('ONNX FATAL:', e)\n-        if not success:\n-            raise RuntimeError('ONNX conversion failed')",
    "Added lines": "+import sys\n+        errors = []\n+                    msg = 'Error while processing node: {}. Exception: {}'.format(node, e)\n+                    errors.append(msg)\n+                    print('ONNX FATAL:', msg, file=sys.stderr)\n+        if len(errors) > 0:\n+            raise RuntimeError(\n+                \"ONNX conversion failed, encountered {} errors:\\n\\n{}\".format(\n+                    len(errors), \"\\n\\n\".join(errors)))",
    "Label": "clean"
},
{
    "Id": 1270,
    "Library": "pytorch",
    "Date": "2019/09/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/21e9d1144ea74d82bc260efdb67e243469c86da6",
    "Root Cause": "N.A",
    "Bug report": "fix use-after-free bug\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/25965\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D17300835\n\nPulled By: zdevito\n\nfbshipit-source-id: dd22d71687f03a5900aec4e36b795e1b13904eee",
    "Number of deleted lines": 1,
    "Deleted lines": "-        for (int64_t s : tensor_types.at(0)->sizes().concrete_sizes().value())",
    "Added lines": "+        auto concrete_sizes =\n+            tensor_types.at(0)->sizes().concrete_sizes().value();\n+        for (int64_t s : concrete_sizes)",
    "Label": "clean"
},
{
    "Id": 1271,
    "Library": "pytorch",
    "Date": "2019/08/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/eb756746ab0172b7f1a35a4a34b520700da93db6",
    "Root Cause": "N.A",
    "Bug report": "Fix possible deadlock in SharedCache inside a forked child proc (#25158)\n\nSummary:\nRelated: https://github.com/pytorch/pytorch/issues/24927#issuecomment-524608021\n\n`fork` inherits lock state. So if we happen to unfortunately fork when the `SharedCache` lock is held. We could deadlock in the child process when some code tries to acquire it.\n\nFollowing pytorch multiprocessing library design, this patch resets the lock to a new object after fork. A similar example from python core lib for `multiprocessing.Queue` is :\n\n```py\nclass Queue(object):\n    def __init__(self, ...):\n        ...\n        self._after_fork()\n        if sys.platform != 'win32':\n            register_after_fork(self, Queue._after_fork)\n\n    def _after_fork(self):\n        debug('Queue._after_fork()')\n        self._notempty = threading.Condition(threading.Lock())\n        self._buffer = collections.deque()\n        self._thread = None\n        self._jointhread = None\n        self._joincancelled = False\n        self._closed = False\n        self._close = None\n        self._send_bytes = self._writer.send_bytes\n        self._recv_bytes = self._reader.recv_bytes\n        self._poll = self._reader.poll\n```\n\nhttps://github.com/python/cpython/blob/d4d60134b29290049e28df54f23493de4f1824b6/Lib/multiprocessing/queues.py#L54-L78\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25158\n\nDifferential Revision: D17091227\n\nPulled By: soumith\n\nfbshipit-source-id: ee7130f47d7bbd42fc34a2598f1f6974d8d7cdb7",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+from multiprocessing.util import register_after_fork\n+        # `fork` inherits lock state, so in case we fork when the lock is held,\n+        # we register a function to reset the lock to a new object to avoid\n+        # possible deadlocks, following python multiprocessing library design.\n+        self._after_fork()\n+        register_after_fork(self, SharedCache._after_fork)\n+\n+    def _after_fork(self):",
    "Label": "clean"
},
{
    "Id": 1272,
    "Library": "pytorch",
    "Date": "2019/08/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/febcb3b7b37d0ce8a8fdfd1165b37bed2536c118",
    "Root Cause": "N.A",
    "Bug report": "int8 static quantization in the numerical debugger\n\nSummary: Fix the static int8 transformation in the numerical debugger\n\nTest Plan:\nExample:\nbuck run mode/opt caffe2/caffe2/fb/fbgemm/numerical_debugger:multithreaded_sparsenn_emulator --  --warmup 0 --iter 1 --threads 1 --runs 1  --local_model_path=/data/models/mobile_cvr_int8/101245796_428 --filler=\"logfiledb\" --benchmark_model_transformation=\"int8_static\" --run_dir=/data/models/mobile_cvr_int8/local_output/ --local_dataset_path=/data/models/mobile_cvr_int8/dataset/test/dataset_cached_reader.db  --output_tensors=\"Sigmoid/sigmoid\" --attach_ne_reporter=true --output_prediction_blob=\"Sigmoid/sigmoid\" --activation-histogram-file=/data/models/mobile_cvr_int8/activation_histograms/101245796_428_train_10000_hist.txt.0x7f1f36133ea0 --dump_nets  --caffe2_logging_print_net_summary=1\n\nReviewed By: amylittleyang\n\nDifferential Revision: D16368515\n\nfbshipit-source-id: b2649cec0fa35b852842a419fea1ea7105e5225c",
    "Number of deleted lines": 1,
    "Deleted lines": "-      for (int i = 0; i < Output(0)->numel(); ++i) {",
    "Added lines": "+      for (int i = 0; i < OutputTensorCPU_(0)->numel(); ++i) {",
    "Label": "clean"
},
{
    "Id": 1273,
    "Library": "pytorch",
    "Date": "2019/08/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5c78e0c47071f3e4c3447095cf526b627695cc4e",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in creating a prefix string in jit log. (#25051)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25051\n\nIn #24355 I factored out a function for creating a prefix in jit_log,\nbut I made a copypasta error there: the prefix stringstream was\ninitialized from the input string instead of an empty string.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D16974156\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 014fe0e3366e85e984a6936ec9bb17f571107f6e",
    "Number of deleted lines": 1,
    "Deleted lines": "-  std::stringstream prefix_ss(in_str);",
    "Added lines": "+  std::stringstream prefix_ss;",
    "Label": "clean"
},
{
    "Id": 1274,
    "Library": "pytorch",
    "Date": "2019/08/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/40be39e4c7063486fa82f4a7917041d985e13e6f",
    "Root Cause": "N.A",
    "Bug report": "Fix perf bug with indexed assignment (index_put_) (#24083)\n\nSummary:\nTensorIterator was incorrectly moving the stride 0 dimension to the\ninner-most dim in the assignment:\n\n  a[idx] = b\n\nNote that the corresponding read was still fast:\n\n  c = a[idx]\n\nThis was noticed by adamlerer\n\n```\nimport torch\nimport time\nimport sys\nN = 300000\n\ntorch.set_num_threads(1)\na = torch.zeros(N, 128)\nb = torch.zeros(N, 128)\nidx = torch.arange(N)\n\n%timeit c = a[idx]  # before and after: ~91.3 ms\n%timeit a[idx] = b  # before: 4.38 sec after: 44.1 ms\n```\n\nNote that the indexed read is slower than the indexed assignment on\nmy computer because the read has to allocate a new output (which is\nzero'ed by the kernel). The indexed assignment doesn't allocate any new\nTensors.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/24083\n\nDifferential Revision: D16805440\n\nPulled By: colesbury\n\nfbshipit-source-id: 70a2e74ae79691afbfa9f75b3d7d1e6806f603f5",
    "Number of deleted lines": 1,
    "Deleted lines": "-      if (operands_[arg].is_output) {",
    "Added lines": "+      if (is_reduction_ && operands_[arg].is_output) {",
    "Label": "clean"
},
{
    "Id": 1275,
    "Library": "pytorch",
    "Date": "2019/08/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/06c09a266b93dfaf2bc5542ac4501fd4b651e7b0",
    "Root Cause": "N.A",
    "Bug report": "Ignore bugprone-lambda-function-name in clang-tidy. (#24190)\n\nSummary:\nFixes: https://github.com/pytorch/pytorch/issues/23947.\n\nIn https://github.com/pytorch/pytorch/pull/23970, I ignored these in dispatch macros, but I think it's more maintainable to just block this globally.  And it's a pretty minor issue if it happens anyway.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/24190\n\nDifferential Revision: D16766329\n\nPulled By: gchanan\n\nfbshipit-source-id: 7ae7b7781562a8974d974f7eefa8ec7551eb09fc",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  ,-bugprone-lambda-function-name",
    "Label": "clean"
},
{
    "Id": 1276,
    "Library": "pytorch",
    "Date": "2019/08/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5faecc8b1f8a11cd652b285b5a1c71674b5d087b",
    "Root Cause": "N.A",
    "Bug report": "Perform string uniquing by value in pickle serialization. (#23741)\n\nSummary:\nOn my testcase, this reduces the uncompressed size of TorchScript\ndebug info from 281KB to 76KB.  With zip compression enabled, this\nsaves about 2.5KB of final size.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/23741\n\nDifferential Revision: D16624128\n\nfbshipit-source-id: ce45659d6b20d40608ace05639b69b93696b00d9",
    "Number of deleted lines": 1,
    "Deleted lines": "-    pushStringImpl(ivalue.toStringRef());",
    "Added lines": "+    pushString(ivalue.toStringRef());",
    "Label": "clean"
},
{
    "Id": 1277,
    "Library": "pytorch",
    "Date": "2019/07/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/39de49c7ec73362f1fbc8d9b259e6328d49b5e2e",
    "Root Cause": "N.A",
    "Bug report": "Fix a tiny bug and refactor (#22808)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/22808\n\n- Use ```size_to_dim_```.\n- ```mod``` is not in the scope. Should be ```module```.\n\nReviewed By: mingzhe09088\n\nDifferential Revision: D16225799\n\nfbshipit-source-id: 9a263227d2d508eefdfddfee15fd0822819de946",
    "Number of deleted lines": 4,
    "Deleted lines": "-  int64_t M = 1;\n-  for (size_t i = 0; i < input.dim() - 1; ++i) {\n-    M *= input.size(i);\n-  }",
    "Added lines": "+  int64_t M = size_to_dim_(input.dim() - 1, input.sizes());",
    "Label": "clean"
},
{
    "Id": 1278,
    "Library": "pytorch",
    "Date": "2019/07/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ae5b52086ea87df2035e02003fbc7e45345c2c75",
    "Root Cause": "N.A",
    "Bug report": "Support converting Python number to IValue in pybind_utils.h (#22817)\n\nSummary:\nI ran into the following error when trying to pass a Python int as an arg to `torch::jit::createStackForSchema`, and I think it is due to the missing support for `NumberType` in [toIValue](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/pybind_utils.h#L448).\n\n> RuntimeError: Missing cases in toIValue for type: Scalar! File a bug report. (toIValue at ../torch/csrc/jit/pybind_utils.h:449)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/22817\n\nDifferential Revision: D16276006\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7f63519bb37219445e836ec1f51ca4f98bf52c44",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      if (py::isinstance<py::int_>(obj)) {\n+        return py::cast<int64_t>(obj);\n+      } else if (py::isinstance<py::float_>(obj)) {\n+        return py::cast<double>(obj);\n+      }",
    "Label": "clean"
},
{
    "Id": 1279,
    "Library": "pytorch",
    "Date": "2019/07/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6eb3969ac7f6ea1065b75ba15b0e7805573bf29c",
    "Root Cause": "N.A",
    "Bug report": "keep reuqires_grad unchanged after converting bn to syncbn (#22569)\n\nSummary:\nAfter converting BN layers to SyncBN layers, the function will set all `requires_grad = True` regardless of the original requires_grad states. I think it is a bug and have fixed it in this PR.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/22569\n\nDifferential Revision: D16151647\n\nPulled By: zou3519\n\nfbshipit-source-id: e2ad1886c94d8882485e7fb8be51ad76469ecc67",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                # keep reuqires_grad unchanged\n+                module_output.weight.requires_grad = module.weight.requires_grad\n+                module_output.bias.requires_grad = module.bias.requires_grad",
    "Label": "clean"
},
{
    "Id": 1280,
    "Library": "pytorch",
    "Date": "2019/06/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1e7bd7586d0d7770a462be421f31c4536f382d47",
    "Root Cause": "N.A",
    "Bug report": "Query caffe2 operator stats for detailed execution info (#20924)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20924\n\nI found a python3 bug for deserializing caffe2 code. The exception thrown is Unicode related error instead of just decode error, and we need to catch that as well\n\nReviewed By: ipiszy\n\nDifferential Revision: D15293221\n\nfbshipit-source-id: 29820800d1b4cbe5bf3f5a189fe2023e655d0508",
    "Number of deleted lines": 1,
    "Deleted lines": "-    except text_format.ParseError:",
    "Added lines": "+    except (text_format.ParseError, UnicodeDecodeError):",
    "Label": "clean"
},
{
    "Id": 1281,
    "Library": "pytorch",
    "Date": "2019/05/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ac2314fdebdf5ced36cefbecfe5aa45fce469257",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in quantize_linear (#20711)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20711\n\nFor uint8_t, ```std::numeric_limits::digits``` returns 8;\nFor int8_t, ```std::numeric_limits::digits``` returns 7.\n\nFBGEMM wants to get the ```qparams.precision``` to be always 8 for both int8_t and uint8_t.\n\nReviewed By: jerryzh168\n\nDifferential Revision: D15410695\n\nfbshipit-source-id: 17dc3842d7c426947454c201bcb167b87b7301ce",
    "Number of deleted lines": 1,
    "Deleted lines": "-  qparams.precision = std::numeric_limits<typename T::underlying>::digits;",
    "Added lines": "+  qparams.precision = CHAR_BIT * sizeof(typename T::underlying);",
    "Label": "clean"
},
{
    "Id": 1282,
    "Library": "pytorch",
    "Date": "2019/05/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0beecbdaadb8b4996ce94757724fc7dd639bae40",
    "Root Cause": "N.A",
    "Bug report": "fix soft_nms_cpu call in BoxWithNMSLimit (#20738)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20738\n\nD15348610 introduces a bug of misaligned arguments.\n\nReviewed By: isameer\n\nDifferential Revision: D15425627\n\nfbshipit-source-id: b6345863847426ae04eb31245d13f7fcb69d0355",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            -1, /* topN */",
    "Label": "clean"
},
{
    "Id": 1283,
    "Library": "pytorch",
    "Date": "2019/05/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d9dcfacd9e77f690bfa68f5b1629e54006134a7d",
    "Root Cause": "N.A",
    "Bug report": "Improve CPUAllocator OOM message (#20618)\n\nSummary:\nSpotted while debugging some problem\n\nBefore\n```\n>>> torch.empty(10**15)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: [enforce fail at CPUAllocator.cpp:56] posix_memalign(&data, gAlignment, nbytes) == 0. 12 vs 0\n```\n\nAfter\n```\n>>> torch.empty(10**15)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: [enforce fail at CPUAllocator.cpp:65] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 4000000000000000 bytes. Error code 12 (Cannot allocate memory)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20618\n\nReviewed By: ezyang\n\nDifferential Revision: D15390400\n\nPulled By: dzhulgakov\n\nfbshipit-source-id: 31f448303e4bd5f8c2bad8ca0f05bcece22a4b5e",
    "Number of deleted lines": 3,
    "Deleted lines": "-  CAFFE_ENFORCE_EQ(posix_memalign(&data, gAlignment, nbytes), 0);\n-      \"DefaultCPUAllocator: not enough memory: you tried to allocate %dGB. Buy new RAM!\",\n-      nbytes / 1073741824);",
    "Added lines": "+  int err = posix_memalign(&data, gAlignment, nbytes);\n+  if (err != 0) {\n+    CAFFE_THROW(\n+        \"DefaultCPUAllocator: can't allocate memory: you tried to allocate \",\n+        nbytes,\n+        \" bytes. Error code \",\n+        err,\n+        \" (\",\n+        strerror(err),\n+        \")\");\n+  }\n+      \"DefaultCPUAllocator: not enough memory: you tried to allocate \",\n+      nbytes,\n+      \" bytes. Buy new RAM!\");",
    "Label": "clean"
},
{
    "Id": 1284,
    "Library": "pytorch",
    "Date": "2019/05/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/39af9563e28f9daa9c5a0decc0607fc290fd51e1",
    "Root Cause": "N.A",
    "Bug report": "Re-enable CUDA tests for C++ API (#20238)\n\nSummary:\nCUDA tests for C++ API were not running on the CI due to a missing character in https://github.com/pytorch/pytorch/pull/11554. This PR fixes the bug.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20238\n\nDifferential Revision: D15279945\n\nPulled By: yf225\n\nfbshipit-source-id: 1fa7439cd40b6f2fba6792eb790bacf0d67d0054",
    "Number of deleted lines": 1,
    "Deleted lines": "-if [[ \"$BUILD_ENVIRONMENT\" == *pytorch-linux-xenial-cuda9-cudnn7-py3 ]] || \\",
    "Added lines": "+if [[ \"$BUILD_ENVIRONMENT\" == *pytorch-linux-xenial-cuda9-cudnn7-py3* ]] || \\",
    "Label": "clean"
},
{
    "Id": 1285,
    "Library": "pytorch",
    "Date": "2019/05/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cc06e2f9479b8fc09e4557eeba6a0806c66ed160",
    "Root Cause": "N.A",
    "Bug report": "fix build with python-2.7.5 (#20137)\n\nSummary:\npytorch failed to build with the following error, complaining about the first regex match\nIt may be caused by a bug in python 2.7.5\nThis change proposed is a workaround for building pytorch with python 2.7.5\nSince the '*' star notation is greedy in python regex, the new expression shall produce the identical result with the old one.\n\n```\nTraceback (most recent call last):\n  File \"/data2/nihuini/pytorch/cmake/../aten/src/ATen/gen.py\", line 14, in <module>\n    import preprocess_declarations\n  File \"/data2/nihuini/pytorch/aten/src/ATen/preprocess_declarations.py\", line 3, in <module>\n    from function_wrapper import TYPE_FORMAL_GENERIC\n  File \"/data2/nihuini/pytorch/aten/src/ATen/function_wrapper.py\", line 5, in <module>\n    from code_template import CodeTemplate\n  File \"/data2/nihuini/pytorch/aten/src/ATen/code_template.py\", line 13, in <module>\n    class CodeTemplate(object):\n  File \"/data2/nihuini/pytorch/aten/src/ATen/code_template.py\", line 23, in CodeTemplate\n    subtitution = re.compile(substitution_str, re.MULTILINE)\n  File \"/usr/lib64/python2.7/re.py\", line 190, in compile\n    return _compile(pattern, flags)\n  File \"/usr/lib64/python2.7/re.py\", line 242, in _compile\n    raise error, v # invalid expression\nsre_constants.error: nothing to repeat\n--\nCMake Error at cmake/Codegen.cmake:162 (message):\n  Failed to get generated_cpp list\nCall Stack (most recent call first):\n  caffe2/CMakeLists.txt:2 (include)\n\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20137\n\nDifferential Revision: D15218122\n\nPulled By: ezyang\n\nfbshipit-source-id: 10b618ff92a04e9074f5d83e31411fc2341e0cf8",
    "Number of deleted lines": 1,
    "Deleted lines": "-    substitution_str = r'(^[^\\n\\S]*)?\\$([^\\d\\W]\\w*|\\{,?[^\\d\\W]\\w*\\,?})'",
    "Added lines": "+    # Python 2.7.5 has a bug where the leading (^[^\\n\\S]*)? does not work,\n+    # workaround via appending another [^\\n\\S]? inside\n+\n+    substitution_str = r'(^[^\\n\\S]*[^\\n\\S]?)?\\$([^\\d\\W]\\w*|\\{,?[^\\d\\W]\\w*\\,?})'",
    "Label": "clean"
},
{
    "Id": 1286,
    "Library": "pytorch",
    "Date": "2019/05/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/767c82e15133a73303b0eb2d2f2b60c7a7ddff14",
    "Root Cause": "N.A",
    "Bug report": "Initialize last_epoch in _LRScheduler.__init__() (#20059)\n\nSummary:\nClass attributes preferably be explicitly initiated within\nthe __init__() call. Otherwise, overriding step() is\nprone to bugs.\n\nThis patch partially reverts #7889\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20059\n\nDifferential Revision: D15195747\n\nPulled By: soumith\n\nfbshipit-source-id: 3d1a51d8c725d6f14e3e91ee94c7bc7a7d6c1713",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        self.last_epoch = last_epoch",
    "Label": "clean"
},
{
    "Id": 1287,
    "Library": "pytorch",
    "Date": "2019/05/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e846ccd7bcd489291807f0720d245c040519fe74",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in dumpNet (#20001)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/20001\n\natt\n\nReviewed By: zrphercule\n\nDifferential Revision: D15164116\n\nfbshipit-source-id: dab19fb84fa0ab648103317af5509703db918682",
    "Number of deleted lines": 1,
    "Deleted lines": "-  WriteProtoToTextFile(shape_net, \"debug_ssa_net.pb_txt\");",
    "Added lines": "+  WriteProtoToTextFile(shape_net, fname);",
    "Label": "clean"
},
{
    "Id": 1288,
    "Library": "pytorch",
    "Date": "2019/04/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/268859ce0d4c02e6dde915f04e3ab47f4cef1450",
    "Root Cause": "N.A",
    "Bug report": "Fix CUDA stream syncing bug in allgather and reduce_scatter (#19631)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/19631\nghimport-source-id: edc47e77d6ef03e966944ff98eefc22f2574eeaa\n\nReviewed By: mrshenli\n\nDifferential Revision: D15110077\n\nPulled By: mxw\n\nfbshipit-source-id: 27a68308ade5ea511e2ea568a071eedb5d21c1ba",
    "Number of deleted lines": 2,
    "Deleted lines": "-            outputTensors[i][i].storage().data(), ncclStreams[i]);\n-            inputTensors[i][i].storage().data(), ncclStreams[i]);",
    "Added lines": "+            outputTensors[i][j].storage().data(), ncclStreams[i]);\n+            inputTensors[i][j].storage().data(), ncclStreams[i]);",
    "Label": "clean"
},
{
    "Id": 1289,
    "Library": "pytorch",
    "Date": "2019/04/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7e73783c6fefb592a112859781cf662bfb0f408d",
    "Root Cause": "N.A",
    "Bug report": "Fix promoteTypes for QInt types (#19182)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/19182\n\nThis is a bug discovered by zafartahirov, right now if one of the tensor is QInt\ntype we'll return undefined, but actually we want to allow ops that accepts\nTensors of the same QInt type to work.\n\nReviewed By: zafartahirov\n\nDifferential Revision: D14909172\n\nfbshipit-source-id: 492fd6403da8c56e180efe9d632a3b7fc879aecf",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  // For QInt types, we only allow exact match\n+  if (isQIntType(a) && a == b) {\n+    return a;\n+  }\n+",
    "Label": "clean"
},
{
    "Id": 1290,
    "Library": "pytorch",
    "Date": "2019/04/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b461689cfde2a14a46d670eef36b6f4f91455f8f",
    "Root Cause": "N.A",
    "Bug report": "Clear input/ouput shape cache for each inference (#19085)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/19085\n\nThis is a bug where input_shapes_ and output_shapes_ will grow indefinitely. Fix it here.\n\nReviewed By: bertmaher, rdzhabarov\n\nDifferential Revision: D14861695\n\nfbshipit-source-id: d59116f27c3b54f5cc5a33533de4b9222dbb7afc",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  input_shapes_.clear();\n+  output_shapes_.clear();",
    "Label": "clean"
},
{
    "Id": 1291,
    "Library": "pytorch",
    "Date": "2019/04/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/17adce1b6945b647673bf2913b3a08c367bb0097",
    "Root Cause": "N.A",
    "Bug report": "do not use constexpr with CUDA >= 9.2 compiler on Windows. (#18986)\n\nSummary:\nDefine `AT_CPP14_CONSTEXPR` from `constexpr` to empty on Windows with CUDA >= 9.2 as workaround.\n\nDiscussed in #18425.\n\nWhen using CUDA 10.1 on Windows, I faced following errors:\n~~~\nD:/data/source/pytorch\\c10/util/ArrayRef.h(144): error: variable in constexpr function does not have automatic storage duration\n          detected during instantiation of \"const T &c10::ArrayRef<T>::front() const [with T=at::Tensor]\"\nD:/data/source/pytorch/aten/src\\ATen/DeviceGuard.h(30): here\n~~~\n\nFrom documentation of CUDA Toolkit v10.1.105, compiler supports `constexpr` and relaxing requirements (in C++14), but compilation failed.\n\nI suppose this could be compiler bug and require this workaround.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18986\n\nDifferential Revision: D14821836\n\nPulled By: ezyang\n\nfbshipit-source-id: 9800da2fe7291e7c09e8e5e882adebab08d83ae3",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#if defined(_MSC_VER) && defined(__CUDACC__) && \\\n+    (__CUDACC_VER_MAJOR__ >= 10 || (__CUDACC_VER_MAJOR__ == 9 && __CUDACC_VER_MINOR__ >= 2))\n+// workaround: CUDA >= v9.2 compiler cannot compile correctly on Windows.\n+#  define AT_CPP14_CONSTEXPR\n+#else\n+#endif",
    "Label": "clean"
},
{
    "Id": 1292,
    "Library": "pytorch",
    "Date": "2019/04/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/84a9694ed0c85c0bc844915adf9f2c8fd53bacb1",
    "Root Cause": "N.A",
    "Bug report": "Fix windows msbuild bug (#18748)\n\nSummary:\nFix the bug introduced by #18681 where an undefined variable was being used to limit max cpu count when building for Windows without Ninja.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18748\n\nDifferential Revision: D14733209\n\nPulled By: soumith\n\nfbshipit-source-id: 52fc0dd4dde99da75a6956b63f02da2e647eed4f",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            j = max_jobs or str(multiprocessing.cpu_count())",
    "Label": "clean"
},
{
    "Id": 1293,
    "Library": "pytorch",
    "Date": "2019/03/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/84f020fe098bfaad308bd820869fafa44d3d4fff",
    "Root Cause": "N.A",
    "Bug report": "Fix bug in tensor feed which caused crash due to wrong tensor type (#18552)\n\nSummary:\nIn blob feeder for ideep device, the wrong device option is given and led to a crash issue.\nThis patch aims to correct the device option to fix this bug.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18552\n\nDifferential Revision: D14679838\n\nPulled By: yinghai\n\nfbshipit-source-id: bde11e6a6fe44822166881dcb7c9bd0b34b4ecf3",
    "Number of deleted lines": 3,
    "Deleted lines": "-      if (meta.Match<float>() && !ZeroDim(original_array)) {\n-              option,\n-              BlobGetMutableTensor(blob, OptionToDevice(option).type()),",
    "Added lines": "+\n+      if ((in_place && blob->IsType<itensor>())\n+          || (meta.Match<float>() && !ZeroDim(original_array))) {\n+              cpu_option,\n+              BlobGetMutableTensor(blob, OptionToDevice(cpu_option).type()),",
    "Label": "clean"
},
{
    "Id": 1294,
    "Library": "pytorch",
    "Date": "2019/03/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c0a2452ffe1bf4202763bbfcafa5867461158de3",
    "Root Cause": "N.A",
    "Bug report": "multiline KeyError msg python bug workaround (#18557)\n\nSummary:\nmake multiline KeyError msg readable by working around a python bug https://bugs.python.org/issue2651\n\ndiscussion: https://github.com/pytorch/pytorch/issues/16647\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18557\n\nDifferential Revision: D14681086\n\nPulled By: soumith\n\nfbshipit-source-id: acbd13a823302c854c3d364028ed414fd8ce6bc8",
    "Number of deleted lines": 1,
    "Deleted lines": "-            raise batch.exc_type(batch.exc_msg)",
    "Added lines": "+            # make multiline KeyError msg readable by working around\n+            # a python bug https://bugs.python.org/issue2651\n+            if batch.exc_type == KeyError and \"\\n\" in batch.exc_msg:\n+                raise Exception(\"KeyError:\" + batch.exc_msg)\n+            else:\n+                raise batch.exc_type(batch.exc_msg)",
    "Label": "clean"
},
{
    "Id": 1295,
    "Library": "pytorch",
    "Date": "2019/03/20",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bd1271338ada8eda28a72e028a5521480d118bfb",
    "Root Cause": "N.A",
    "Bug report": "Add python_variable._is_view for debugging. (#18197)\n\nSummary:\nI don't know if we actually want to expose this or not, but it's useful for debugging.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18197\n\nReviewed By: ezyang\n\nDifferential Revision: D14530712\n\nPulled By: gchanan\n\nfbshipit-source-id: 98fdba9cf113738f0db3a198c49365de536b9919",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+static PyObject * THPVariable__is_view(PyObject *self, PyObject* args)\n+{\n+  HANDLE_TH_ERRORS\n+  auto& self_ = reinterpret_cast<THPVariable*>(self)->cdata;\n+  if (self_.is_view()) {\n+    Py_RETURN_TRUE;\n+  } else {\n+    Py_RETURN_FALSE;\n+  }\n+  END_HANDLE_TH_ERRORS\n+}\n+\n+  {\"_is_view\", (PyCFunction)THPVariable__is_view, METH_NOARGS, NULL},",
    "Label": "clean"
},
{
    "Id": 1296,
    "Library": "pytorch",
    "Date": "2019/03/19",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6758f5587f43dfef63fca516aee176c80daea6b0",
    "Root Cause": "N.A",
    "Bug report": "Delete bugbear from Python 2 lint. (#18192)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18192\nghimport-source-id: 9523a09d7ec202ef08cf0ecdf48c42739ea6b0ce\n\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* **#18192 Delete bugbear from Python 2 lint.**\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: D14529240\n\nfbshipit-source-id: 1a433b53dd38d1c455e8c0750d97c594ac51ef09",
    "Number of deleted lines": 1,
    "Deleted lines": "-        install: pip install flake8 flake8-bugbear",
    "Added lines": "+        install: pip install flake8",
    "Label": "clean"
},
{
    "Id": 1297,
    "Library": "pytorch",
    "Date": "2019/03/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/87b6cbb6fdec3789a409b02665e731ba67b4377c",
    "Root Cause": "N.A",
    "Bug report": "fix bug in pool_dnnlowp_op_avx2.cc (#18141)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18141\n\nVLEN should've been 32\n\nReviewed By: jianyuh\n\nDifferential Revision: D14510780\n\nfbshipit-source-id: ddf12746e1c69677a268432432ddb088cc210084",
    "Number of deleted lines": 1,
    "Deleted lines": "-      constexpr int VLEN = 8;",
    "Added lines": "+      constexpr int VLEN = 32;",
    "Label": "clean"
},
{
    "Id": 1298,
    "Library": "pytorch",
    "Date": "2019/03/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c2c32340a4a8a258c97665a5cf2d2054f06cf0f6",
    "Root Cause": "N.A",
    "Bug report": "add command line option to use hive filler; add README (#17619)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/17619\n\n--filler hive --iter -1 will let debugger exhaust all batches from a hive partition before exiting.\nadd README that summarizes command line options and usage.\n\nReviewed By: yinghai\n\nDifferential Revision: D14220166\n\nfbshipit-source-id: daa23b7e8a9184481c6d7b67acf1599e5c99d74a",
    "Number of deleted lines": 1,
    "Deleted lines": "-    CAFFE_ENFORCE(bytes > 0, \"input bytes should be positive\");",
    "Added lines": "+    if (bytes == 0) {\n+      LOG(WARNING) << \"0 input bytes filled\";\n+    }",
    "Label": "clean"
},
{
    "Id": 1299,
    "Library": "pytorch",
    "Date": "2019/02/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4ac91b2d64eeea5ca21083831db5950dc08441d6",
    "Root Cause": "N.A",
    "Bug report": "add debug/release tip to cpp docs (#17452)\n\nSummary:\nas title. These were already added to the tutorials, but I didn't add them to the cpp docs.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/17452\n\nDifferential Revision: D14206501\n\nPulled By: suo\n\nfbshipit-source-id: 89b5c8aaac22d05381bc4a7ab60d0bb35e43f6f5",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  \n+.. tip::\n+  On Windows, debug and release builds are not ABI-compatible. If you plan to\n+  build your project in debug mode, we recommend\n+  `building PyTorch from source <https://github.com/pytorch/pytorch#from-source>`_.",
    "Label": "clean"
},
{
    "Id": 1300,
    "Library": "pytorch",
    "Date": "2019/02/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a1f2ed008fb8bf0c938f05795d9f3af159e7d292",
    "Root Cause": "N.A",
    "Bug report": "Minor fix of the histogram observer in FBL eval flows (#17118)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/17118\n\nFix the bug in quantization eval workflow; Add mul_nets option in histogram observer pybind\n\nReviewed By: yinghai\n\nDifferential Revision: D14085321\n\nfbshipit-source-id: 08e3153148522ebc9512a57144d9a8ad154bb6f8",
    "Number of deleted lines": 6,
    "Deleted lines": "-      [](const string& out_file_name, int dump_freq) {\n-        AddGlobalNetObserverCreator([out_file_name, dump_freq](NetBase* net) {\n-          return make_unique<HistogramNetObserver>(\n-              net, out_file_name, 2048, dump_freq);\n-        });\n-      pybind11::arg(\"dump_freq\") = -1);",
    "Added lines": "+      [](const string& out_file_name, int dump_freq, bool mul_nets) {\n+        AddGlobalNetObserverCreator(\n+            [out_file_name, dump_freq, mul_nets](NetBase* net) {\n+              return make_unique<HistogramNetObserver>(\n+                  net, out_file_name, 2048, dump_freq, mul_nets);\n+            });\n+      pybind11::arg(\"dump_freq\") = -1,\n+      pybind11::arg(\"mul_nets\") = false);",
    "Label": "clean"
},
{
    "Id": 1301,
    "Library": "pytorch",
    "Date": "2019/02/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/db82fc7ca6e6bf261f51a6794b400a04eaf55bd0",
    "Root Cause": "N.A",
    "Bug report": "Add more debugging facilities to ONNXIFI transform (#17043)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/17043\n\nAdd more debugging facilities for ONXNIFI transform.\n\nReviewed By: ipiszy\n\nDifferential Revision: D14019492\n\nfbshipit-source-id: 8c258ccba2f8ce77db096031fc8a61e15bd8af93",
    "Number of deleted lines": 3,
    "Deleted lines": "-    WriteProtoToTextFile(onnxifi_net, \"debug_onnxifi_net.pb_txt\");\n-    WriteProtoToTextFile(net_opt, \"debug_optimized_net.pb_txt\");\n-      CAFFE_ENFORCE_EQ(weights.count(output), 0);",
    "Added lines": "+    WriteProtoToTextFile(\n+        onnxifi_net,\n+        \"debug_onnxifi_net_\" + c10::to_string(onnxifi_op_id_) + \".pb_txt\");\n+    WriteProtoToTextFile(\n+        net_opt,\n+        \"debug_optimized_net_\" + c10::to_string(onnxifi_op_id_) + \".pb_txt\");\n+      CAFFE_ENFORCE_EQ(\n+          weights.count(output),\n+          0,\n+          \"Weight \",\n+          output,\n+          \" shouldn't appear in the output\");",
    "Label": "clean"
},
{
    "Id": 1302,
    "Library": "pytorch",
    "Date": "2019/02/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e0323a6aea2c8b4c9267bb4d7ed538dcd792d121",
    "Root Cause": "N.A",
    "Bug report": "ctc_loss error message bug fix. (#16917)\n\nSummary:\nCTCLLoss argument error message is wrong.\nPlease fix this. (sorry if I made some mistakes.)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/16917\n\nDifferential Revision: D14019983\n\nPulled By: ezyang\n\nfbshipit-source-id: 3337a2e86da6f3f7594c73fddb73340494a19ce2",
    "Number of deleted lines": 1,
    "Deleted lines": "-\t     \"Expected tensor to have size at least \", max_input_length, \" at dimension 1, but got size \", targets.size(0), \" for \", targets_arg,",
    "Added lines": "+\t     \"Expected tensor to have size at least \", max_input_length, \" at dimension 1, but got size \", input_lengths[b], \" for \", log_probs_arg,",
    "Label": "clean"
},
{
    "Id": 1303,
    "Library": "pytorch",
    "Date": "2019/02/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/19790b218f6d53f83c7e8a02cf46810552e7f822",
    "Root Cause": "N.A",
    "Bug report": "Register coalescer bug was fixed in ROCm 2.1 (#16923)\n\nSummary:\nRemove specialization/workaround for ROCm.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/16923\n\nDifferential Revision: D14018521\n\nPulled By: bddppq\n\nfbshipit-source-id: d88162740bca6dc8ad37397dfbf8c84408074a00",
    "Number of deleted lines": 25,
    "Deleted lines": "-#ifdef __HIPCC__\n-template <>\n-void sum_kernel_impl<int16_t, int16_t>(TensorIterator& iter) {\n-  // There is a Register Coalescing bug in LLVM causing the hcc\n-  // compiler segfaults:\n-  // https://bugs.llvm.org/show_bug.cgi?id=39602\n-  // To work around it, use int32 as the accumulate type.\n-  gpu_reduce_kernel<int16_t, int16_t>(iter, func_wrapper<int16_t> ([]GPU_LAMBDA(int32_t a, int32_t b) -> int32_t {\n-    return a + b;\n-  }));\n-}\n-#endif\n-\n-#ifdef __HIPCC__\n-template <>\n-void mean_kernel_impl<int16_t, int16_t, int16_t>(TensorIterator& iter) {\n-  // There is a Register Coalescing bug in LLVM causing the hcc\n-  // compiler segfaults:\n-  // https://bugs.llvm.org/show_bug.cgi?id=39602\n-  // To work around it, use int32 as the accumulate type.\n-  float factor = float(iter.num_output_elements()) / iter.numel();\n-  gpu_reduce_kernel<int16_t, int16_t>(iter, MeanOps<int32_t, float> {factor});\n-}\n-#endif // __HIPCC__\n-",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1304,
    "Library": "pytorch",
    "Date": "2019/02/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/72f070a124c3062939873a5468463d7c9e3ac4eb",
    "Root Cause": "N.A",
    "Bug report": "Backport the stable doc build on v1.0.1 to master (#16503)\n\nSummary:\nList of changes:\n- Always push the final state of the doc build docker for debugging purposes.\n- Adds code for the stable doc build. This code is never actually run on master, only the v1.0.1 branch. There is a big note for this behavior.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/16503\n\nDifferential Revision: D13972469\n\nPulled By: zou3519\n\nfbshipit-source-id: 68f459650ef0de200a34edd43fc1372143923972",
    "Number of deleted lines": 4,
    "Deleted lines": "-          if [[ \"${CIRCLE_BRANCH}\" != \"master\" ]]; then\n-            # Do a dry_run of the docs build. This will build the docs but not push them.\n-            export COMMAND='((echo \"export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}\" && echo \"source ./workspace/env\" && echo \"sudo chown -R jenkins workspace && cd workspace && ./doc_push_script.sh docs/master master dry_run\") | docker exec -u jenkins -i \"$id\" bash) 2>&1'\n-          else",
    "Added lines": "+          # master branch docs push\n+          if [[ \"${CIRCLE_BRANCH}\" == \"master\" ]]; then\n+\n+          # stable release docs push. Due to some circleci limitations, we keep\n+          # an eternal PR open (#16502) for merging v1.0.1 -> master for this job.\n+          # XXX: The following code is only run on the v1.0.1 branch, which might\n+          # not be exactly the same as what you see here.\n+          elif [[ \"${CIRCLE_BRANCH}\" == \"v1.0.1\" ]]; then\n+            export COMMAND='((echo \"export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}\" && echo \"source ./workspace/env\" && echo \"sudo chown -R jenkins workspace && cd workspace && ./doc_push_script.sh docs/stable 1.0.1\") | docker exec -u jenkins -i \"$id\" bash) 2>&1'\n+\n+          # For open PRs: Do a dry_run of the docs build, don't push build\n+          else\n+            export COMMAND='((echo \"export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}\" && echo \"source ./workspace/env\" && echo \"sudo chown -R jenkins workspace && cd workspace && ./doc_push_script.sh docs/master master dry_run\") | docker exec -u jenkins -i \"$id\" bash) 2>&1'\n+\n+          # Save the docs build so we can debug any problems\n+          export DEBUG_COMMIT_DOCKER_IMAGE=${COMMIT_DOCKER_IMAGE}-debug\n+          docker commit \"$id\" ${DEBUG_COMMIT_DOCKER_IMAGE}\n+          docker push ${DEBUG_COMMIT_DOCKER_IMAGE}\n+",
    "Label": "clean"
},
{
    "Id": 1305,
    "Library": "pytorch",
    "Date": "2019/01/31",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/536f647bae7eaa0ab1091808084ed2c24a8dabb4",
    "Root Cause": "N.A",
    "Bug report": "respect MAX_JOBS (#16641)\n\nSummary:\nWe inadvertently switch the OSX build over to ninja on CI. It then fails to respect MAX_JOBS and hits the same scache deadlock bug, this makes the ninja build respect MAX_JOBS.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/16641\n\nDifferential Revision: D13910751\n\nPulled By: zdevito\n\nfbshipit-source-id: 61bec500539519b019b74421a13cd87fc1d86090",
    "Number of deleted lines": 10,
    "Deleted lines": "-# The ninja package is different in Anaconda Cloud and PYPI. The one in Anaconda Cloud\n-# doesn't have the python code (ninja_syntax.py) in it, while the one in PYPI does.\n-# Since we don't use the python part here, it is also acceptable if we use the executable\n-# directly if it is in `PATH`.\n-if which('ninja'):\n-    USE_NINJA = True\n-else:\n-    USE_NINJA = False\n-            check_call(['ninja', 'install'], cwd=build_dir, env=my_env)\n-            max_jobs = os.getenv('MAX_JOBS', str(multiprocessing.cpu_count()))",
    "Added lines": "+# Use ninja if it is on the PATH. Previous version of PyTorch required the\n+# ninja python package, but we no longer use it, so we do not have to import it\n+USE_NINJA = which('ninja') is not None\n+        max_jobs = os.getenv('MAX_JOBS', None)\n+            ninja_cmd = ['ninja', 'install']\n+            if max_jobs is not None:\n+                ninja_cmd += ['-j', max_jobs]\n+            check_call(ninja_cmd, cwd=build_dir, env=my_env)\n+            max_jobs = max_jobs or str(multiprocessing.cpu_count())",
    "Label": "clean"
},
{
    "Id": 1306,
    "Library": "pytorch",
    "Date": "2019/01/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bd19dd4b90aa94359b84986b0f9a298538ac7889",
    "Root Cause": "N.A",
    "Bug report": "url download bugfix for URLs served without Content-Length header (#16153)\n\nSummary:\nSome HTTP servers dont return Content-Length, account for that\n\nFixes: https://github.com/pytorch/pytorch/issues/16152\n\nDifferential Revision: D13858882\n\nPulled By: soumith\n\nfbshipit-source-id: e4293e9368ed4c87548d22adec1ce0c25ea4bd8f",
    "Number of deleted lines": 4,
    "Deleted lines": "-        file_size = int(u.headers[\"Content-Length\"])\n-            file_size = int(meta.getheaders(\"Content-Length\")[0])\n-            file_size = int(meta.get_all(\"Content-Length\")[0])\n-            sys.stderr.write(\"\\r{0:.1f}%\".format(100 * self.n / float(self.total)))",
    "Added lines": "+    file_size = None\n+        if hasattr(u.headers, \"Content-Length\"):\n+            file_size = int(u.headers[\"Content-Length\"])\n+            content_length = meta.getheaders(\"Content-Length\")\n+            content_length = meta.get_all(\"Content-Length\")\n+        if content_length is not None and len(content_length) > 0:\n+            file_size = int(content_length[0])\n+            if self.total is None:\n+                sys.stderr.write(\"\\r{0:.1f} bytes\".format(self.n))\n+            else:\n+                sys.stderr.write(\"\\r{0:.1f}%\".format(100 * self.n / float(self.total)))",
    "Label": "clean"
},
{
    "Id": 1307,
    "Library": "pytorch",
    "Date": "2019/01/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c7547dbd5eeed5fd24ec96dfc64d5ae98cdcb7c2",
    "Root Cause": "N.A",
    "Bug report": "Fix compiler error in swapBytes64 for rare architectures (#16418)\n\nSummary:\nswapBytes64 used to use SwapByteOrder_32 and value, both of which dont exist. This commit rewrites that part from scratch.\nThis happened on Debugbuild on Microsoft compiler. For that case \" && !defined(_DEBUG)\" is also removed, because _byteswap_uint64 works fine in debug mode (if it is necessary it should me commented why).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/16418\n\nDifferential Revision: D13843306\n\nPulled By: ezyang\n\nfbshipit-source-id: dde1c7baeccec3aaa750d4b7200b3f4ccb4a00cb",
    "Number of deleted lines": 4,
    "Deleted lines": "-#if defined(_MSC_VER) && !defined(_DEBUG)\n-   uint64_t Hi = SwapByteOrder_32(uint32_t(value));\n-   uint32_t Lo = SwapByteOrder_32(uint32_t(value >> 32));\n-   return (Hi << 32) | Lo;",
    "Added lines": "+#if defined(_MSC_VER)\n+  uint64_t Byte0 = output & 0x00000000000000FF;\n+  uint64_t Byte1 = output & 0x000000000000FF00;\n+  uint64_t Byte2 = output & 0x0000000000FF0000;\n+  uint64_t Byte3 = output & 0x00000000FF000000;\n+  uint64_t Byte4 = output & 0x000000FF00000000;\n+  uint64_t Byte5 = output & 0x0000FF0000000000;\n+  uint64_t Byte6 = output & 0x00FF000000000000;\n+  uint64_t Byte7 = output & 0xFF00000000000000;\n+  output = (Byte0 << (7*8)) | (Byte1 << (5*8)) | (Byte2 << (3*8)) | (Byte3 << (1*8)) | \n+           (Byte7 >> (7*8)) | (Byte6 >> (5*8)) | (Byte5 >> (3*8)) | (Byte4 >> (1*8));",
    "Label": "clean"
},
{
    "Id": 1308,
    "Library": "pytorch",
    "Date": "2019/01/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4cf76574b958b41ae1df7fba6e87af101414549e",
    "Root Cause": "N.A",
    "Bug report": "Raise CalledProcessError when torch.distributed launch process not return 0 (#16069)\n\nSummary:\n`torch.distributed.launch.py` will not raise error when `subprocess.Popen` is not return 0.\nFor better debugging it should always raise an error if processes launched have unusual behavior\nPull Request resolved: https://github.com/pytorch/pytorch/pull/16069\n\nDifferential Revision: D13709467\n\nPulled By: ezyang\n\nfbshipit-source-id: 31d32a5ec8fed7bccd62d845bfba0e670ed3fe20",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        if process.returncode != 0:\n+            raise subprocess.CalledProcessError(returncode=process.returncode,\n+                                                cmd=process.args)",
    "Label": "clean"
},
{
    "Id": 1309,
    "Library": "pytorch",
    "Date": "2019/01/16",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/aa6b0f50ad4daadb5e35ab89ecc325e35d1207dc",
    "Root Cause": "N.A",
    "Bug report": "add a constexpr in c10::Half (#16091)\n\nSummary:\nDebug build generates references which are not resolved otherwise\n\nas recognized by dlibenzi\nPull Request resolved: https://github.com/pytorch/pytorch/pull/16091\n\nDifferential Revision: D13703584\n\nPulled By: soumith\n\nfbshipit-source-id: 6ac5666d2c6b1520e083f6eac9c535a1609d9c6b",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+constexpr Half::from_bits_t Half::from_bits;\n+",
    "Label": "clean"
},
{
    "Id": 1310,
    "Library": "pytorch",
    "Date": "2019/01/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8f11147d43386acec2642a23cf08710ee0ab1af5",
    "Root Cause": "N.A",
    "Bug report": "Use CUDAGuard when serializing CUDA Tensors (#15807)\n\nSummary:\nFixes #15308. Before this change, `torch.save` and `torch.load` would\ninitialize the CUDA context on GPU 0 if it hadn't been initialized\nalready, even if the serialized tensors are only on GPU 1.\n\nThis PR fixes that bug by using CUDAGuard in the storage serialization\npath.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/15807\n\nDifferential Revision: D13593201\n\nPulled By: zou3519\n\nfbshipit-source-id: 4addc91ea5a5278d56a03f3d422577ee39e99897",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+#ifdef THC_GENERIC_FILE\n+#include <c10/cuda/CUDAGuard.h>\n+#endif\n+\n+#ifdef THC_GENERIC_FILE\n+  c10::cuda::CUDAGuard guard(self->device());\n+#endif\n+\n+#ifdef THC_GENERIC_FILE\n+  c10::cuda::OptionalCUDAGuard guard;\n+  if (_storage != nullptr) {\n+    guard.set_device(_storage->device());\n+  }\n+#endif\n+",
    "Label": "clean"
},
{
    "Id": 1311,
    "Library": "pytorch",
    "Date": "2018/12/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cbd1c519c46186eb7c70590ba02f126297ee251f",
    "Root Cause": "N.A",
    "Bug report": "Replace non-printable-ascii characters in ProtoDebugString (#14918)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/14918\n\nWhen ProtoBuf-Lite is in use, ProtoDebugString just calls SerializeAsString.\nThis produces binary output, which is not a very suitable \"debug\" string.\nSpecifically, we've observed it causing problems when calling code tries to\nadd the debug string to a Java exception message (which requires valid UTF-8).\nNow, we replace all non-ASCII bytes with \"?\".\n\nThis is not a very fast implementation, but generating debug strings shouldn't\nbe a performance-sensitive operation in any application.\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D13385540\n\nfbshipit-source-id: 8868172baf20efaf53fecf7d666a6980f59b64f5",
    "Number of deleted lines": 1,
    "Deleted lines": "-  return proto.SerializeAsString();",
    "Added lines": "+  string serialized = proto.SerializeAsString();\n+  for (char& c : serialized) {\n+    if (c < 0x20 || c >= 0x7f) {\n+      c = '?';\n+    }\n+  }\n+  return serialized;",
    "Label": "clean"
},
{
    "Id": 1312,
    "Library": "pytorch",
    "Date": "2018/11/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bb7fb7e45fb0af97e701e72e7194b4a1b6b19d48",
    "Root Cause": "N.A",
    "Bug report": "remove CAFFE2_API from IdWrapper (#14044)\n\nSummary:\nit doesn't really make sense on a template class. Also it breaks if\nyou try to build in debug on Windows, so this will save someone some\nfrustration in the future.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/14044\n\nDifferential Revision: D13202960\n\nPulled By: anderspapitto\n\nfbshipit-source-id: 617d78366993d5ecc2ba1f23bb90010f10df41f3",
    "Number of deleted lines": 1,
    "Deleted lines": "-class C10_API IdWrapper {",
    "Added lines": "+class IdWrapper {",
    "Label": "clean"
},
{
    "Id": 1313,
    "Library": "pytorch",
    "Date": "2018/11/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f6e4fc071a63b8ffe89bbea892e97c598f008120",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug that causes nvcc to emit an unknown option error (#13904)\n\nSummary:\nUsing `\"-Xcompiler -fPIC\"` causes nvcc to emit the following:\n\n    nvcc fatal   : Unknown option 'Xcompiler -fPIC'\n\nAs per fixes lower down in the file (see also issue #7126 on GitHub),\nthe fix is to replace it with `\"-Xcompiler\" \"-fPIC\"`. This one was\napparently missed when the original fix was applied.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/13904\n\nDifferential Revision: D13043189\n\nPulled By: soumith\n\nfbshipit-source-id: 6dc6d325671e4d08cd8e6242ffc93b3bd1f65351",
    "Number of deleted lines": 1,
    "Deleted lines": "-  list(APPEND CUDA_NVCC_FLAGS \"-Xcompiler -fPIC\")",
    "Added lines": "+  list(APPEND CUDA_NVCC_FLAGS \"-Xcompiler\" \"-fPIC\")",
    "Label": "clean"
},
{
    "Id": 1314,
    "Library": "pytorch",
    "Date": "2018/11/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0bfbdcac891ce3f4943759ddcc65d901ffa680ff",
    "Root Cause": "N.A",
    "Bug report": "fix bug in D13017777\n\nSummary:\nMistakenly created an infinite recursive call.\n\n(Note: this ignores all push blocking failures!)\n\nReviewed By: jianyuh\n\nDifferential Revision: D13038053\n\nfbshipit-source-id: 8b760cb73b5369647d8ef651b8c196ac3f7af04d",
    "Number of deleted lines": 1,
    "Deleted lines": "-  if (!this->GetQuantizationParameters_()) {",
    "Added lines": "+  if (!BaseType::GetQuantizationParameters_()) {",
    "Label": "clean"
},
{
    "Id": 1315,
    "Library": "pytorch",
    "Date": "2018/11/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/cc3cecdba0cd74cdd7c0cb3d97f70c6f81bebf14",
    "Root Cause": "N.A",
    "Bug report": "Fix the bug when compile using nvcc compiler. (#13509)\n\nSummary:\nI found a bug about compiling the cuda file when I install maskrcnn-benchmark lib.\n\n`python setup.py build develop` will throw the error:\n```\n  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/cpp_extension.py\", line 214, in unix_wrap_compile\n    original_compile(obj, src, ext, cc_args, cflags, pp_opts)\n  File \"/usr/lib/python2.7/distutils/unixccompiler.py\", line 125, in _compile\n    self.spawn(compiler_so + cc_args + [src, '-o', obj] +\nTypeError: coercing to Unicode: need string or buffer, list found\n```\n\nFor more information, please see [issue](https://github.com/facebookresearch/maskrcnn-benchmark/issues/99).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/13509\n\nDifferential Revision: D12902675\n\nPulled By: soumith\n\nfbshipit-source-id: b9149f5de21ae29f94670cb2bbc93fa368f4e0f7",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+                    if not isinstance(nvcc, list):\n+                        nvcc = [nvcc]",
    "Label": "clean"
},
{
    "Id": 1316,
    "Library": "pytorch",
    "Date": "2018/10/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5b15a501dacc5e60606c63b37f26b7bb4875bac6",
    "Root Cause": "N.A",
    "Bug report": "Refactor & unit test feed predictor\n\nSummary:\n1. Refactor DDPG predictor.  Merge the critic predictor with ParametricDQNPredictor since they are the same\n2. Fix bug where loss was multiplied by the batch size\n3. Create DDPGFeedPredictor which uses the feed predictor output format\n4. Add support for gridworld simulation memoization to DDPG.  Also memoize normalization tables.\n\nReviewed By: kittipatv\n\nDifferential Revision: D10161240\n\nfbshipit-source-id: 2813890043de1241c1fb9b9c2b6a897403f9fc12",
    "Number of deleted lines": 2,
    "Deleted lines": "-    # type: (Tensor, float) -> Tensor\n-    # type: (Tensor, Tensor, float, float, bool) -> Tensor",
    "Added lines": "+    # type: (Tensor, float) -> Tensor\n+    # type: (Tensor, Tensor, float, float, bool) -> Tensor",
    "Label": "clean"
},
{
    "Id": 1317,
    "Library": "pytorch",
    "Date": "2018/10/18",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/79709f02e9100306f9aa9bf5887dc5afbe751b5e",
    "Root Cause": "N.A",
    "Bug report": "fix overwriting of CMAKE_EXE_LINKER_FLAGS (#12834)\n\nSummary:\nbug lurking since 2016\nPull Request resolved: https://github.com/pytorch/pytorch/pull/12834\n\nReviewed By: bddppq\n\nDifferential Revision: D10452484\n\nPulled By: anderspapitto\n\nfbshipit-source-id: 352584af06e2fb35338fb66b3d8eb1050b716349",
    "Number of deleted lines": 1,
    "Deleted lines": "-    set(CMAKE_EXE_LINKER_FLAGS ${MPI_CXX_LINK_FLAGS})",
    "Added lines": "+    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${MPI_CXX_LINK_FLAGS}\")",
    "Label": "clean"
},
{
    "Id": 1318,
    "Library": "pytorch",
    "Date": "2018/10/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/038d5ca943d5aa732c94c3af2692db79f30de327",
    "Root Cause": "N.A",
    "Bug report": "Remove incompatibility MSVC, Cuda and Debug (#12572)\n\nSummary:\nExperimentally this works.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/12572\n\nDifferential Revision: D10342468\n\nPulled By: ezyang\n\nfbshipit-source-id: dc36587c32ab0910aa14b7351ca12532acd41c7d",
    "Number of deleted lines": 4,
    "Deleted lines": "-    message(FATAL_ERROR\n-            \"Caffe2 currently does not support the combination of MSVC, Cuda \"\n-            \"and Debug mode. Either set USE_CUDA=OFF or set the build type \"\n-            \"to Release\")",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1319,
    "Library": "pytorch",
    "Date": "2018/10/08",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/def655ec27302f9ee36d05f929ecfacf52692bde",
    "Root Cause": "N.A",
    "Bug report": "fix critical section of atomic add op\n\nSummary: When testing D10220313, I ran into this bug.\n\nReviewed By: aazzolini\n\nDifferential Revision: D10224295\n\nfbshipit-source-id: f46d7333612bce437c1ae6c0b0b579fc2a639665",
    "Number of deleted lines": 3,
    "Deleted lines": "-    c->Resize(std::vector<int64_t>());\n-    d->Resize(std::vector<int64_t>());\n-    std::lock_guard<std::mutex> lg(*mutex);",
    "Added lines": "+    std::lock_guard<std::mutex> lg(*mutex);\n+    c->Resize();\n+    d->Resize();",
    "Label": "clean"
},
{
    "Id": 1320,
    "Library": "pytorch",
    "Date": "2018/10/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/06f535d8a0e0d21e983fc7715123e47c69a2f12f",
    "Root Cause": "N.A",
    "Bug report": "More debug info in plan executor (#12183)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/12183\n\nAdding more debug info printed from plan executor\n\nReviewed By: manojkris\n\nDifferential Revision: D10113104\n\nfbshipit-source-id: dddc9aec8012c8575ab305033388412fdaaac537",
    "Number of deleted lines": 1,
    "Deleted lines": "-    LOG(INFO) << \"Processing net '\" << net_def.name() << \"'\";",
    "Added lines": "+    LOG(INFO) << \"Processing net '\" << net_def.name() << \"', type: '\"\n+              << net_def.type() << \"', #ops: \" << net_def.op_size()\n+              << \", num_workers: \" << net_def.num_workers();",
    "Label": "clean"
},
{
    "Id": 1321,
    "Library": "pytorch",
    "Date": "2018/09/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8c533c2c90fc495b0dbd93105e0d93c85ff66a99",
    "Root Cause": "N.A",
    "Bug report": "Fix bug where Reshape() trashes strides.\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/12092\n\nReviewed By: jerryzh168\n\nDifferential Revision: D10051005\n\nfbshipit-source-id: c36d1c8d12fb41baf8d1a1a9f38776deeff242de",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    update_to_contiguous_strides();",
    "Label": "clean"
},
{
    "Id": 1322,
    "Library": "pytorch",
    "Date": "2018/09/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b7c302da1ad1f2051a931cb8cb4fb583ae34c685",
    "Root Cause": "N.A",
    "Bug report": "Make gen_jit_dispatch runnable (#12018)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/12018\n\nTried to use the file and ran into a small bug, this fixes it\n\nDifferential Revision: D10013231\n\nfbshipit-source-id: 4cf8c29cf9e2cedd7a28fa0cc0196e5144a54bf2",
    "Number of deleted lines": 1,
    "Deleted lines": "-    parser.add_argument('template-path', metavar='TEMPLATE_PATH',",
    "Added lines": "+    parser.add_argument('template_path', metavar='TEMPLATE_PATH',",
    "Label": "clean"
},
{
    "Id": 1323,
    "Library": "pytorch",
    "Date": "2018/09/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/00fe2c56061828ea3613ea854a4863124adcd53d",
    "Root Cause": "N.A",
    "Bug report": "Use -O1 for sleef build in Debug mode (#11942)\n\nSummary:\n`-O0` is problematic for compiling sleef kernels since they consist of a bunch of vector intrinsics. In `-O0`, the compiler spills *every* intermediate value to the stack. In one example (TestEndToEndHybridFrontendModels.test_snli in test_jit.py) the function `Sleef_tanhf8_u10avx2` would spill 30kB of AVX registers onto the stack and run two orders of magnitude slower than in opt mode, causing the test to take minutes rather than seconds. I've verified that this behavior is not present with `-O1`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/11942\n\nDifferential Revision: D9994658\n\nPulled By: jamesr66a\n\nfbshipit-source-id: cdd9474c6ae3aa9898d5715ac19a900f5f90468a",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  # Bump up optimization level for sleef to -O1, since at -O0 the compiler\n+  # excessively spills intermediate vector registers to the stack\n+  # and makes things run impossibly slowly\n+  set(OLD_CMAKE_C_FLAGS_DEBUG ${CMAKE_C_FLAGS_DEBUG})\n+  IF(${CMAKE_C_FLAGS_DEBUG} MATCHES \"-O0\")\n+    string(REGEX REPLACE \"-O0\" \"-O1\" CMAKE_C_FLAGS_DEBUG ${OLD_CMAKE_C_FLAGS_DEBUG})\n+  ELSE()\n+    set(CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG} -O1\")\n+  ENDIF()\n+\n+  set(CMAKE_C_FLAGS_DEBUG ${OLD_CMAKE_C_FLAGS_DEBUG})",
    "Label": "clean"
},
{
    "Id": 1324,
    "Library": "pytorch",
    "Date": "2018/09/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/48c8adfe1b211ab728e8a1b4698ba70128f4bcf0",
    "Root Cause": "N.A",
    "Bug report": "Turn storage on UndefinedTensorImpl into nullptr. (#11738)\n\nSummary:\nI also fix a bug that crept in while we had incorrect semantics where UndefinedTensorImpl was a CPU tensor, and thus some moves which shouldn't have been legal didn't crash. Moving out the Tensor* also moved out the Tensor* in the blob, and it's not supported to store an undefined tensor in a blob.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/11738\n\nReviewed By: gchanan\n\nDifferential Revision: D9847859\n\nfbshipit-source-id: db6be0f76a8e6526a89fd0e87b6a23b9cc820c8d",
    "Number of deleted lines": 1,
    "Deleted lines": "-  UndefinedTensorImpl() : TensorImpl(CPU){};",
    "Added lines": "+  UndefinedTensorImpl() : TensorImpl(at::Storage()){};",
    "Label": "clean"
},
{
    "Id": 1325,
    "Library": "pytorch",
    "Date": "2018/09/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d24bcfd9304f0e38c4024620de4da8f2379f6956",
    "Root Cause": "N.A",
    "Bug report": "Suppress hiprand \"duplicate-decl-specifier\" warning (#11698)\n\nSummary:\nOtherwise each build produces 65MB of warnings log, which makes the CI hard to debug.\n\niotamudelta Jorghi12\nPull Request resolved: https://github.com/pytorch/pytorch/pull/11698\n\nDifferential Revision: D9840356\n\nPulled By: bddppq\n\nfbshipit-source-id: b69bf6a5c38a97b188221f9c084c608ffc9b37c8",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    set(HIP_HIPCC_FLAGS \"${HIP_HIPCC_FLAGS} -Wno-duplicate-decl-specifier\")",
    "Label": "clean"
},
{
    "Id": 1326,
    "Library": "pytorch",
    "Date": "2018/09/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ea0ee77c61beacfe67c0209de4e142e22ce136f1",
    "Root Cause": "N.A",
    "Bug report": "Fix katex math rendering (#11472)\n\nSummary:\nI'm 80% sure that this fixes the math bug. But I can't repro locally so I don't know.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/11472\n\nDifferential Revision: D9755328\n\nPulled By: SsnL\n\nfbshipit-source-id: 130be664d3c6ceee3c0c166c1a86fc9ec3b79d74",
    "Number of deleted lines": 19,
    "Deleted lines": "-# katex (mathjax replacement) macros\n-#\n-#\n-\n-katex_macros = r'''\n-\"\\\\op\": \"\\\\operatorname{{#1}}\",\n-\"\\\\i\": \"\\\\mathrm{i}\",\n-\"\\\\e\": \"\\\\mathrm{e}^{#1}\",\n-\"\\\\w\": \"\\\\omega\",\n-\"\\\\vec\": \"\\\\mathbf{#1}\",\n-\"\\\\x\": \"\\\\vec{x}\",\n-\"\\\\d\": \"\\\\operatorname{d}\\\\!{}\",\n-\"\\\\dirac\": \"\\\\operatorname{\\\\delta}\\\\left(#1\\\\right)\",\n-\"\\\\scalarprod\": \"\\\\left\\\\langle#1,#2\\\\right\\\\rangle\",\n-'''\n-\n-   {left: \"\\\\(\", right: \"\\\\)\", display: true},\n-],\n-strict : false",
    "Added lines": "+   {left: \"\\\\(\", right: \"\\\\)\", display: false},\n+]",
    "Label": "clean"
},
{
    "Id": 1327,
    "Library": "pytorch",
    "Date": "2018/09/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e9da2dd3ccf7c5720ab79f2d1952e0c6bdf59a83",
    "Root Cause": "N.A",
    "Bug report": "Do not use PERSISTENT cudnn mode for spatialBN (#11382)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/11382\n\nWe found this cudnn bug in S163230 that causes accuracy loss. We fix this in D9601217, but due to the reimplementation of spatialBN it's overwritten. Let's land this fix again.\n\nReviewed By: kuttas\n\nDifferential Revision: D9702347\n\nfbshipit-source-id: 11547e9edaf7b2ba7f4aa7263ffb4f0281bbf078",
    "Number of deleted lines": 2,
    "Deleted lines": "-        mode_(CUDNN_BATCHNORM_SPATIAL_PERSISTENT) {\n-        mode_(CUDNN_BATCHNORM_SPATIAL_PERSISTENT) {",
    "Added lines": "+        // TODO(T31829456): The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was\n+        // introduced in CuDNN 7 for performance optimization, but it results in\n+        // accuracy losses in convolution models such as ResNeXt-101 and\n+        // video R(2+1)D. We will fall back to the normal\n+        // CUDNN_BATCHNORM_SPATIAL for now\n+        mode_(CUDNN_BATCHNORM_SPATIAL) {\n+        // TODO(T31829456): The new CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode was\n+        // introduced in CuDNN 7 for performance optimization, but it results in\n+        // accuracy losses in convolution models such as ResNeXt-101 and\n+        // video R(2+1)D. We will fall back to the normal\n+        // CUDNN_BATCHNORM_SPATIAL for now\n+        mode_(CUDNN_BATCHNORM_SPATIAL) {",
    "Label": "clean"
},
{
    "Id": 1328,
    "Library": "pytorch",
    "Date": "2018/09/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/3f30c296d374ddbef7043508e80d3cb276d53ea6",
    "Root Cause": "N.A",
    "Bug report": "Export CAFFE2_PLEASE_ADD_OPERATOR_SCHEMA_FOR_* (#11233)\n\nSummary:\nThis PR resolved the following compilation errors on devgpu:\n/home/mingzhe0908/pytorch/build/lib/libcaffe2_gpud.so: undefined reference to `caffe2::CAFFE2_PLEASE_ADD_OPERATOR_SCHEMA_FOR_Tan()'\n/home/mingzhe0908/pytorch/build/lib/libcaffe2_gpud.so: undefined reference to `caffe2::CAFFE2_PLEASE_ADD_OPERATOR_SCHEMA_FOR_MaxPool3D()'\n....\n\nThe same error has been happening with caffe2 build with debug mode before build_caffe2 was removed.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/11233\n\nReviewed By: orionr\n\nDifferential Revision: D9645527\n\nPulled By: mingzhe09088\n\nfbshipit-source-id: 68a45aa7fd815cac41b7fd64cfd9838b3226345a",
    "Number of deleted lines": 2,
    "Deleted lines": "-  void CAFFE2_PLEASE_ADD_OPERATOR_SCHEMA_FOR_##name(){};          \\\n-  void CAFFE2_PLEASE_ADD_OPERATOR_SCHEMA_FOR_##name(){};          \\",
    "Added lines": "+  CAFFE2_EXPORT void CAFFE2_PLEASE_ADD_OPERATOR_SCHEMA_FOR_##name(){};          \\\n+  CAFFE2_EXPORT void CAFFE2_PLEASE_ADD_OPERATOR_SCHEMA_FOR_##name(){};          \\",
    "Label": "clean"
},
{
    "Id": 1329,
    "Library": "pytorch",
    "Date": "2018/08/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/17ecc06b656ee9efd595b652f7d44af68d61e646",
    "Root Cause": "N.A",
    "Bug report": "static casting TIndex (#10514)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/10514\n\nfix the bug which break the windows build in fused_rowwise_random_quantization_ops.h\n\nReviewed By: ezyang, jspark1105\n\nDifferential Revision: D9322291\n\nfbshipit-source-id: a6a27e87423b6caa973414ffd7ccb12076f2e1e4",
    "Number of deleted lines": 7,
    "Deleted lines": "-    return maxq_ & q;\n-    return orval | (T)(shiftval << shift_);\n-    const std::vector<TIndex> output_dimensions = {input_rows,\n-                                                   10 + segment_size};\n-        output_seg = output_seg.binaryExpr(qvals, cwiseshftor);\n-    const std::vector<TIndex> output_dimensions = {input_rows, output_columns};\n-        sub_output_row = input_seg.unaryExpr(cwisedec);",
    "Added lines": "+    return (uint8_t)(maxq_ & q);\n+    return (T)(orval | (shiftval << shift_));\n+    const std::vector<TIndex> output_dimensions = {\n+        input_rows, static_cast<TIndex>(10 + segment_size)};\n+        output_seg = output_seg.binaryExpr(qvals.cast<uint8_t>(), cwiseshftor);\n+    const std::vector<TIndex> output_dimensions = {\n+        input_rows, static_cast<TIndex>(output_columns)};\n+        sub_output_row = input_seg.unaryExpr(cwisedec).cast<float>();",
    "Label": "clean"
},
{
    "Id": 1330,
    "Library": "pytorch",
    "Date": "2018/08/14",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/16ecd6f99c468027e4a08feb273e4fb31bee50bb",
    "Root Cause": "N.A",
    "Bug report": "Fix Debug Build On Windows (#10359)\n\nSummary:\ncompile files in torch/csrc with /MDd runtime library option for debug build on Windows\nPull Request resolved: https://github.com/pytorch/pytorch/pull/10359\n\nDifferential Revision: D9316946\n\nPulled By: SsnL\n\nfbshipit-source-id: c84bfad81d61cd49f39b7bce7177edd2b1e8bd69",
    "Number of deleted lines": 1,
    "Deleted lines": "-    /MD",
    "Added lines": "+  if (${CMAKE_BUILD_TYPE} MATCHES \"Debug\")\n+    set (MSVC_RUNTIME_LIBRARY_FLAG \"/MDd\")\n+  else()\n+    set (MSVC_RUNTIME_LIBRARY_FLAG \"/MD\")\n+  endif()\n+  \n+    ${MSVC_RUNTIME_LIBRARY_OPTION}",
    "Label": "clean"
},
{
    "Id": 1331,
    "Library": "pytorch",
    "Date": "2018/08/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/acbc2744d8ee95eedcf05c9c25de071a0e87fe2e",
    "Root Cause": "N.A",
    "Bug report": "fix bug in 3d group convolution (#9860)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/9860\n\nFor 3D group convolution, in the case of CUDNN 7 and NCHWD order, filter dim is (M, C/group_, k_h, h_w, k_d).\n\nAccording to CUDA doc (https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#grouped-convolutions), the existing implementation is incorrect, and will crash the 3d video model training with group convolution.\n\nIn the implementation, `filter.dims(1)` is already `C/group_`. So don't need to divide it by `group_` again.\n\nReviewed By: BIT-silence\n\nDifferential Revision: D9008807\n\nfbshipit-source-id: 2f0d6eb47f4e16d7417a7e3baeba709e3254154f",
    "Number of deleted lines": 4,
    "Deleted lines": "-        dims[0] /= group_;\n-        dims[filter.ndim() - 1] /= group_;\n-        dims[0] /= group_;\n-#endif",
    "Added lines": "+        // We only need to divide dims by group_ when CUDNN version < 7.0\n+        // see CUDA group convolution doc: https://fburl.com/dgj6dvpd\n+        // We only need to divide dims by group_ when CUDNN version < 7.0\n+        // see CUDA group convolution doc: https://fburl.com/dgj6dvpd\n+#endif\n+",
    "Label": "clean"
},
{
    "Id": 1332,
    "Library": "pytorch",
    "Date": "2018/07/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/46d800280050ef531e1a446e173b38bb1c492755",
    "Root Cause": "N.A",
    "Bug report": "Fix bug that always uses the same blob when repeating poolings\n\nReviewed By: houseroad\n\nDifferential Revision: D9027902\n\nfbshipit-source-id: 957702ad9736812ec5aa32066d286c2c3adffc49",
    "Number of deleted lines": 1,
    "Deleted lines": "-                self.input_record.lengths() + '_sid')",
    "Added lines": "+                net.NextScopedBlob(self.input_record.lengths() + '_sid'))",
    "Label": "clean"
},
{
    "Id": 1333,
    "Library": "pytorch",
    "Date": "2018/07/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6c6a353a6691bf6ab3c67481a27d1fb1241a9b48",
    "Root Cause": "N.A",
    "Bug report": "Fix speedbenchmark bug (#9770)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/9770\n\nAdd zero ops to operators that do not have a valid schema\n\nReviewed By: hlu1\n\nDifferential Revision: D8957472\n\nfbshipit-source-id: d8d0a351183e88ace2e050a87c1e1c363af67e33",
    "Number of deleted lines": 1,
    "Deleted lines": "-                  << to_string(1.0e-6 * flops_per_op[idx] / time_per_op[idx])",
    "Added lines": "+          } else {\n+            flops_per_op.emplace_back(0);\n+            memory_bytes_read_per_op.emplace_back(0);\n+            memory_bytes_written_per_op.emplace_back(0);\n+            param_bytes_per_op.emplace_back(0);\n+                  << to_string(\n+                         1.0e-6 * flops_per_op[idx] / time_per_op[idx] *\n+                         main_runs)",
    "Label": "clean"
},
{
    "Id": 1334,
    "Library": "pytorch",
    "Date": "2018/07/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/82539472562b84bc1e17f545e9175ce4c9b7f37a",
    "Root Cause": "N.A",
    "Bug report": "Make error message more informative (#9352)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/9352\n\nI am debugging a failed workflow f61490672, and found the original error message to be not informative.\n\nDifferential Revision: D8808181\n\nfbshipit-source-id: 3f524ca092881186a492c5c0456124ce31d54751",
    "Number of deleted lines": 3,
    "Deleted lines": "-            assert self.BlobIsDefined(output)\n-            assert self.BlobIsDefined(output)\n-            assert self.BlobIsDefined(blob)",
    "Added lines": "+            assert self.BlobIsDefined(output), \"{} is not defined\".format(output)\n+            assert self.BlobIsDefined(output), \"{} is not defined\".format(output)\n+            assert self.BlobIsDefined(blob), \"{} is not defined\".format(blob)",
    "Label": "clean"
},
{
    "Id": 1335,
    "Library": "pytorch",
    "Date": "2018/06/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e8d6ac50b4805483b5be326f916f32134922185d",
    "Root Cause": "N.A",
    "Bug report": "Add retry logic to sccache download for Windows build (#7697)\n\n* Add retry logic to sccache download for Windows build\r\n\r\n* fix script bug\r\n\r\n* clean up",
    "Number of deleted lines": 2,
    "Deleted lines": "-if \"%REBUILD%\"==\"\" ( aws s3 cp s3://ossci-windows/sccache.exe %CD%\\\\tmp_bin\\\\sccache.exe --quiet )\n-sccache --stop-server || set ERRORLEVEL=0",
    "Added lines": "+if \"%REBUILD%\"==\"\" (\n+  :check_sccache\n+  %CD%\\\\tmp_bin\\\\sccache.exe --show-stats || (\n+    taskkill /im sccache.exe /f /t || set ERRORLEVEL=0\n+    del %CD%\\\\tmp_bin\\\\sccache.exe\n+    aws s3 cp s3://ossci-windows/sccache.exe %CD%\\\\tmp_bin\\\\sccache.exe\n+    goto :check_sccache\n+  )\n+)\n+sccache --stop-server",
    "Label": "clean"
},
{
    "Id": 1336,
    "Library": "pytorch",
    "Date": "2018/05/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9b1abd2f81a3bdb653d20dca5b9e709bd5f3ed5d",
    "Root Cause": "N.A",
    "Bug report": "[Caffe2] Keep name of caffe2_pybind11_state and caffe2_pybind11_state_gpu in debug build (#7155)",
    "Number of deleted lines": 2,
    "Deleted lines": "-    set_target_properties(caffe2_pybind11_state PROPERTIES PREFIX \"\")\n-      set_target_properties(caffe2_pybind11_state_gpu PROPERTIES PREFIX \"\")",
    "Added lines": "+    set_target_properties(caffe2_pybind11_state PROPERTIES PREFIX \"\" DEBUG_POSTFIX \"\")\n+      set_target_properties(caffe2_pybind11_state_gpu PROPERTIES PREFIX \"\" DEBUG_POSTFIX \"\")",
    "Label": "clean"
},
{
    "Id": 1337,
    "Library": "pytorch",
    "Date": "2018/05/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ed6f79ccd26d2808b2f73abeddf4c1df681f7c26",
    "Root Cause": "N.A",
    "Bug report": "[caffe2][build] Add ASAN to the debug release of caffe2 (#7107)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+set (CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -fno-omit-frame-pointer -fsanitize=address\")\n+set (CMAKE_LINKER_FLAGS_DEBUG \"${CMAKE_STATIC_LINKER_FLAGS_DEBUG} -fno-omit-frame-pointer -fsanitize=address\")\n+",
    "Label": "clean"
},
{
    "Id": 1338,
    "Library": "pytorch",
    "Date": "2018/05/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f06fcc6efad81bb2619820ada21744d3e6e2f2c4",
    "Root Cause": "N.A",
    "Bug report": "Fix bug that introduced in pull #3280 (#7292)\n\nApparently get() is a function of requests, not a module (not sure if in\r\nthe past get() used to be a module). Therefore, the syntax in #3280 will\r\nalway fail with ImportError, and requests lib will never be used (kind\r\nof defeat the purpose of that pull request).\r\nAlso, if requests lib is used, should add stream=True parameter,\r\notherwise requests.get() will load the whole response into memory.",
    "Number of deleted lines": 2,
    "Deleted lines": "-    import requests.get as urlopen\n-    u = urlopen(url)",
    "Added lines": "+    from requests import get as urlopen\n+        u = urlopen(url, stream=True)\n+        u = urlopen(url)",
    "Label": "clean"
},
{
    "Id": 1339,
    "Library": "pytorch",
    "Date": "2018/05/02",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/604f907bc78d8189b0e8fe5cb8bd880979d279e7",
    "Root Cause": "N.A",
    "Bug report": "Restore filename and line number on AT_ASSERT. (#7152)\n\nAT_ASSERT is an internal, PyTorch specific error, so we should\r\ngive a little more debug information (than with the ordinary\r\nerrors.)\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>",
    "Number of deleted lines": 2,
    "Deleted lines": "-    AT_ERROR(#cond \" ASSERT FAILED, please report a bug to PyTorch.\");   \\\n-    AT_ERROR(at::str(#cond, \" ASSERT FAILED, please report a bug to PyTorch. \", __VA_ARGS__));   \\",
    "Added lines": "+///\n+/// NB: at::Error is handled specially by the default torch to suppress the\n+/// backtrace, see torch/csrc/Exceptions.h\n+    AT_ERROR(#cond \" ASSERT FAILED at \", __FILE__, \":\", __LINE__, \", please report a bug to PyTorch.\");   \\\n+    AT_ERROR(at::str(#cond, \" ASSERT FAILED at \", __FILE__, \":\", __LINE__, \", please report a bug to PyTorch. \", __VA_ARGS__));   \\",
    "Label": "clean"
},
{
    "Id": 1340,
    "Library": "pytorch",
    "Date": "2018/04/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fc6a846cc5db92f5d011d450dd6ea605d5688533",
    "Root Cause": "N.A",
    "Bug report": "[Caffe2] Fixing bug in conda builds (#7061)\n\n* Fixing bug in conda builds\r\n\r\n* Update to other PR",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+caffe2_cmake_args+=(\"-DUSE_LMDB=OFF\")\n+",
    "Label": "clean"
},
{
    "Id": 1341,
    "Library": "pytorch",
    "Date": "2018/04/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b038b3d7beb498301ab3bd5f3a2c3a37adcfe868",
    "Root Cause": "N.A",
    "Bug report": "Always dumping final meta.yaml for debugging (#6977)",
    "Number of deleted lines": 4,
    "Deleted lines": "-\n-  # Show what the final meta.yaml looks like\n-  echo \"Finalized meta.yaml is\"\n-  cat $META_YAML",
    "Added lines": "+# Show what the final meta.yaml looks like\n+echo \"Finalized meta.yaml is\"\n+cat $META_YAML\n+",
    "Label": "clean"
},
{
    "Id": 1342,
    "Library": "pytorch",
    "Date": "2018/04/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/7d32f6fdc3c97522010dd1eb5d87abd2cdd249a1",
    "Root Cause": "N.A",
    "Bug report": "Adding runtime warning for checkpointing inputs to have requires_grad=True (#6883)\n\n* Adding the warning for the checkpointing inputs to have requires_grad=True\r\n\r\n* fix bug",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+import warnings\n+def check_backward_validity(inputs):\n+    if not any(inp.requires_grad for inp in inputs):\n+        warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n+\n+\n+        check_backward_validity(args)\n+    .. warning:\n+        At least one of the inputs needs to have :code:`requires_grad=True` if\n+        grads are needed for model inputs, otherwise the checkpointed part of the\n+        model won't have gradients.\n+\n+    .. warning:\n+        At least one of the inputs needs to have :code:`requires_grad=True` if\n+        grads are needed for model inputs, otherwise the checkpointed part of the\n+        model won't have gradients.\n+",
    "Label": "clean"
},
{
    "Id": 1343,
    "Library": "pytorch",
    "Date": "2018/04/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/8fc11748feec2374316c46442df3fe95c050e633",
    "Root Cause": "N.A",
    "Bug report": "Fix debug build for Windows (#6758)\n\n* Fix debug build for Windows\r\n\r\n* Fix for wrong placement\r\n\r\n* Fix variable name",
    "Number of deleted lines": 4,
    "Deleted lines": "-  SET(MSVC_OPT_FLAG \"\")\n-    set_target_properties(ATen PROPERTIES LINK_FLAGS_RELEASE \"/NODEFAULTLIB:vcomp\")\n-    set_target_properties(ATen PROPERTIES LINK_FLAGS_DEBUG \"/NODEFAULTLIB:vcomp\")\n-    set_target_properties(ATen PROPERTIES STATIC_LIBRARY_FLAGS \"/NODEFAULTLIB:vcomp\")",
    "Added lines": "+  SET(VCOMP_LIB \"vcomp\")\n+  SET(MSVC_OPT_FLAG \" \")\n+  SET(VCOMP_LIB \"vcompd\")\n+    set_target_properties(ATen PROPERTIES LINK_FLAGS_RELEASE \"/NODEFAULTLIB:${VCOMP_LIB}\")\n+    set_target_properties(ATen PROPERTIES LINK_FLAGS_DEBUG \"/NODEFAULTLIB:${VCOMP_LIB}\")\n+    set_target_properties(ATen PROPERTIES STATIC_LIBRARY_FLAGS \"/NODEFAULTLIB:${VCOMP_LIB}\")",
    "Label": "clean"
},
{
    "Id": 1344,
    "Library": "pytorch",
    "Date": "2018/04/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6ce6c0ed659577061bb54d502bd76317025f312a",
    "Root Cause": "N.A",
    "Bug report": "[caffe2] Fix bug in NNPACK bindings for convolution in precomputed transform (#6555)\n\nCaffe2-NNPACK integration created blobs for precomputed kernel transorms based on the name of Conv operator.\r\nWhen Conv operators have the same name (e.g. empty string), or the blobs for precomputed transforms get the same name and overwrite each other.\r\nThis patch ensures that blobs for all precomputed transforms in the network get a unique name.",
    "Number of deleted lines": 1,
    "Deleted lines": "-                     debug_def().name() + \"_transformed_\" + to_string(g))",
    "Added lines": "+  /* Global variable with a unique ID of the pre-transformed kernel blob */\n+  volatile static uint32_t precomputed_transform_id = 0;\n+\n+                     \"__transformed_kernel_\" +\n+                     to_string(__sync_fetch_and_add(&precomputed_transform_id, 1)))",
    "Label": "clean"
},
{
    "Id": 1345,
    "Library": "pytorch",
    "Date": "2018/04/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/67bbf585cda43fc63a8450421aaef24e2f7b3501",
    "Root Cause": "N.A",
    "Bug report": "Fix the c2-onnx exporter bug on Gemm (#6331)",
    "Number of deleted lines": 3,
    "Deleted lines": "-  int64_t axis = 0;\n-    auto axis_w = it->second->i();\n-    const auto& w_shape = shapes.at(w);",
    "Added lines": "+  const auto& w_shape = shapes.at(w);\n+  CAFFE_ENFORCE_GE(x_shape.dims().size(), 2);\n+  CAFFE_ENFORCE_GE(w_shape.dims().size(), 2);\n+  int64_t axis = 1;\n+  }\n+  if (x_shape.dims().size() > 2) {\n+    // we need to reshape only when dimension is higher than 2\n+  int64_t axis_w = 1;\n+    axis_w = it->second->i();\n+  }\n+  if (w_shape.dims().size() > 2) {\n+    // we need to reshape only when dimension is higher than 2",
    "Label": "clean"
},
{
    "Id": 1346,
    "Library": "pytorch",
    "Date": "2018/04/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/83926393d3f475b89f203d32dc80fb3ecd6241bb",
    "Root Cause": "N.A",
    "Bug report": "Detect re-initialization of _C shared library (#6232)\n\nWe had a bug in the Buck build of PyTorch due to symbols from _C\r\nbeing present in two shared libraries that were both loaded at\r\nruntime. This caused global variables to be initialized twice and\r\ndestructed twice on exit. The second destruction often caused\r\nsegfaults on exit.\r\n\r\nThis attempts to detect that sort of situation early on. If\r\nModule.cpp is compiled twice, the symbol\r\npytorch_duplicate_guard()::initialized will be shared. The second\r\ninitialization will print an error message and abort.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+// Checks that the _C shared library isn't initialized multiple times. This\n+// can happen if the same csrc files are compiled into multiple shared\n+// libraries.\n+inline void pytorch_duplicate_guard() {\n+  static int initialized = 0;\n+  if (initialized) {\n+    fprintf(stderr, \"pytorch: _C shared library re-initialized\\n\");\n+    abort();\n+  }\n+  initialized = 1;\n+;}\n+\n+struct call_duplicate_guard {\n+  call_duplicate_guard() { pytorch_duplicate_guard(); }\n+};\n+\n+static call_duplicate_guard _call_duplicate_guard;\n+",
    "Label": "clean"
},
{
    "Id": 1347,
    "Library": "pytorch",
    "Date": "2018/03/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c9dbfca275e375e97135a7a04af091c21e68e9cf",
    "Root Cause": "N.A",
    "Bug report": "bugfix im2col op\n\nfixes grad bug in im2col op",
    "Number of deleted lines": 2,
    "Deleted lines": "-        std::vector<string>{O(0), I(0)},\n-        \"Im2Col\", \"\", std::vector<string>{O(0)}, std::vector<string>{GI(0)});",
    "Added lines": "+        std::vector<string>{GO(0), I(0)},\n+        \"Im2Col\", \"\", std::vector<string>{GO(0)}, std::vector<string>{GI(0)});",
    "Label": "clean"
},
{
    "Id": 1348,
    "Library": "pytorch",
    "Date": "2018/03/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/91162a74edd57390a2e7fa7c099df4ea32f987e3",
    "Root Cause": "N.A",
    "Bug report": "[easy] Improving error message\n\n`_EQ` variation prints the values in case of failure; make it easier to debug",
    "Number of deleted lines": 4,
    "Deleted lines": "-  CAFFE_ENFORCE(\n-      X1.size() == X2.size(),\n-  CAFFE_ENFORCE(\n-      X1.size() == Y.size(), \"The input and label should have the same size.\");",
    "Added lines": "+  CAFFE_ENFORCE_EQ(\n+      X1.size(),\n+      X2.size(),\n+  CAFFE_ENFORCE_EQ(\n+      X1.size(), Y.size(), \"The input and label should have the same size.\");",
    "Label": "clean"
},
{
    "Id": 1349,
    "Library": "pytorch",
    "Date": "2018/03/29",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/21aba57744462ddb80fe72df9603aeedaa57a1a0",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in ONNX symbolic of average 3d pooling op (#6101)",
    "Number of deleted lines": 1,
    "Deleted lines": "-                pads_i=_triple(padding))",
    "Added lines": "+                pads_i=_triple(padding) * 2)",
    "Label": "clean"
},
{
    "Id": 1350,
    "Library": "pytorch",
    "Date": "2018/03/27",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ebc0194950e816d5c6eb39f9b71ee2e4d053b657",
    "Root Cause": "N.A",
    "Bug report": "Fix use-after-free bug in peephole pass (#6037)\n\n* Fix use after free bug in peephole pass\r\n\r\n* Move the loop befor the switch",
    "Number of deleted lines": 4,
    "Deleted lines": "-\n-    for (Block * sub_block : n->blocks()) {\n-      PeepholeOptimize(sub_block);\n-    }",
    "Added lines": "+    for (Block * sub_block : n->blocks()) {\n+        PeepholeOptimize(sub_block);\n+    }\n+",
    "Label": "clean"
},
{
    "Id": 1351,
    "Library": "pytorch",
    "Date": "2018/03/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/aa4af1a5f997a24a832e643a0cef79ee23c0e953",
    "Root Cause": "N.A",
    "Bug report": "[tiny] make debug info optional, CAFFE2_DEBUG env variable driven",
    "Number of deleted lines": 2,
    "Deleted lines": "-    stack = traceback.format_stack()\n-    operator.debug_info = \"\".join(stack[:-1])",
    "Added lines": "+import os\n+    if (os.environ.get('CAFFE2_DEBUG')):\n+        stack = traceback.format_stack()\n+        operator.debug_info = \"\".join(stack[:-1])",
    "Label": "clean"
},
{
    "Id": 1352,
    "Library": "pytorch",
    "Date": "2018/03/17",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0ca046c68de6a25ee376b52e7783ec33f1f6186a",
    "Root Cause": "N.A",
    "Bug report": "Fix bug (#5836)",
    "Number of deleted lines": 1,
    "Deleted lines": "-    CPU_tensor_apply2<scalar_t, double>(ret, lambda,",
    "Added lines": "+    CPU_tensor_apply2<scalar_t, double>(ret, lambda_,",
    "Label": "clean"
},
{
    "Id": 1353,
    "Library": "pytorch",
    "Date": "2018/03/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5c51bb6c0fa9cf233ed21a6c358b46a5d667e341",
    "Root Cause": "N.A",
    "Bug report": "bugfix in onnx export of batch_first = True (#5753)",
    "Number of deleted lines": 4,
    "Deleted lines": "-        from torch.onnx import symbolic\n-        input = symbolic.t(g, input)\n-        from torch.onnx import symbolic\n-        data = symbolic.t(g, data)",
    "Added lines": "+        input = g.op('Transpose', input, perm_i=[1, 0, 2])\n+        data = g.op('Transpose', data, perm_i=[1, 0, 2])",
    "Label": "clean"
},
{
    "Id": 1354,
    "Library": "pytorch",
    "Date": "2018/03/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/abd6f82709482012be24e6bd27e6ad750cc4c885",
    "Root Cause": "N.A",
    "Bug report": "Fix debug build failure on Windows (#5771)",
    "Number of deleted lines": 1,
    "Deleted lines": "-    NANOPB_STATIC_LIB = os.path.join(lib_path, 'protobuf-nanopb.lib')",
    "Added lines": "+    if DEBUG:\n+        NANOPB_STATIC_LIB = os.path.join(lib_path, 'protobuf-nanopbd.lib')\n+    else:\n+        NANOPB_STATIC_LIB = os.path.join(lib_path, 'protobuf-nanopb.lib')",
    "Label": "clean"
},
{
    "Id": 1355,
    "Library": "pytorch",
    "Date": "2018/03/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/82777815f8013f881bd8f40171b29de92fa3c360",
    "Root Cause": "N.A",
    "Bug report": "Fix bmm memory leak (#5744)\n\nFixes #5611.\r\n\r\nTHCTensor_(baddbmm) assumes that newContiguous will always return a new tensor (this is a bad assumption). At the end of the function, tensors are freed if tensor_new != tensor_old. As a result, some tensors aren't freed if they were initially contiguous and newContiguous is called on them.\r\n\r\nTest Plan\r\ncode reading\r\nrun the following (from the #5611 bug report) and assert that the memory doesn't leak anymore\r\nimport subprocess\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\n# This is from https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4\r\ndef get_gpu_memory_map():\r\n    \"\"\"Get the current gpu usage.\r\n\r\n    Returns\r\n    -------\r\n    usage: dict\r\n        Keys are device ids as integers.\r\n        Values are memory usage as integers in MB.\r\n    \"\"\"\r\n    result = subprocess.check_output(\r\n        [\r\n            'nvidia-smi', '--query-gpu=memory.used',\r\n            '--format=csv,nounits,noheader'\r\n        ], encoding='utf-8')\r\n    # Convert lines into a dictionary\r\n    gpu_memory = [int(x) for x in result.strip().split('\\n')]\r\n    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\r\n    return gpu_memory_map\r\n\r\nl, m, n = 1, 9, 1\r\nw = torch.nn.Parameter(torch.Tensor(1024, 2, l, m).cuda())\r\nfor i in range(10000):\r\n    a = Variable(torch.Tensor(1024, 2, m, n).cuda())\r\n    torch.matmul(w, a).permute(0, 3, 1, 2).mean().backward()\r\n    if i % 100 == 0:\r\n        gpu_mem = get_gpu_memory_map()\r\n        print(\"GPU: {:.2f} KB\".format(gpu_mem[0]))",
    "Number of deleted lines": 2,
    "Deleted lines": "-    batch1_ = THCTensor_(newContiguous)(state, batch1);\n-    batch2_ = THCTensor_(newContiguous)(state, batch2);",
    "Added lines": "+    // batch1_ is later freed if batch1_ != batch1\n+    if (THCTensor_(isContiguous)(state, batch1)) {\n+      batch1_ = batch1;\n+    } else {\n+      batch1_ = THCTensor_(newContiguous)(state, batch1);\n+    }\n+    // batch2_ is later freed if batch2_ != batch2\n+    if (THCTensor_(isContiguous)(state, batch2)) {\n+      batch2_ = batch2;\n+    } else {\n+      batch2_ = THCTensor_(newContiguous)(state, batch2);\n+    }",
    "Label": "clean"
},
{
    "Id": 1356,
    "Library": "pytorch",
    "Date": "2018/03/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/16ba087b640b37958cc90a8c169bf641751cb487",
    "Root Cause": "N.A",
    "Bug report": "[oncall]fix unittest dper/layer_models/tests:utils_test\n\nas titled -- fix offending diff D7091725 due to added debug_info in operator\nproto",
    "Number of deleted lines": 2,
    "Deleted lines": "-            assert initializer_op == \\\n-                self.global_constant_initializers[blob_name], \\",
    "Added lines": "+\n+        def op_equal(operator1, operator2):\n+            o1 = copy.deepcopy(operator1)\n+            o2 = copy.deepcopy(operator2)\n+            # debug_info is supposed to be different, and we don't need to\n+            # compare debug_info\n+            if hasattr(o1, 'debug_info'):\n+                o1.debug_info = ''\n+            if hasattr(o2, 'debug_info'):\n+                o2.debug_info = ''\n+            return o1 == o2\n+\n+            assert op_equal(initializer_op,\n+                            self.global_constant_initializers[blob_name]), \\",
    "Label": "clean"
},
{
    "Id": 1357,
    "Library": "pytorch",
    "Date": "2018/03/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f2ec5b7b0e9d65f8786993a17e78ea33b3fb3e29",
    "Root Cause": "N.A",
    "Bug report": "[DPER] Fix bug in uint8 quantization shortcut.\n\nAfter D6953547 some of the blobs were no longer impacted by uint8 quanitzation,\nbut they would still generate operators expecting uint8 inputs and thus fail.\n\nThis diff is adding a temporal hack to avoid doing this quantization when layer\nis not quantized.\n\nWill fix it with switching to Net rewriting instead.",
    "Number of deleted lines": 1,
    "Deleted lines": "-    def get_8bits_compatible_parameters(self, fused=True):",
    "Added lines": "+    def support_8bit(self):\n+            return False\n+        return True\n+\n+    def get_8bits_compatible_parameters(self, fused=True):\n+        if not self.support_8bit():\n+        # TODO(amalevich): Layer should not be responsible for decision about\n+        # quantization.\n+        if not self.support_8bit() and version in {'uint8rowwise',\n+                                                   'fused_uint8rowwise'}:\n+            version = 'fp32'\n+",
    "Label": "clean"
},
{
    "Id": 1358,
    "Library": "pytorch",
    "Date": "2018/03/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/2d3aebd5fb7e8c94a7f01a97d40ff5728677e8a8",
    "Root Cause": "N.A",
    "Bug report": "fix bug for conv3d Op cpu\n\nThere is a bug in ConvOp. SetDeviceTensor function only copies data to tensor when the sizes of the two are different. In the 3d convolution case for video models, img_shape_device_ (NCTWH) is modified only in the first processed example, and for the following examples, it won't get updated, because img_shape_device_.size() == img_shape.size(). However, it should get updated for each example, because T is changing for different videos. It is the same with col_buffer_shape_device_.\n\nIn this diff, if any dimension of img_shape_device_ is different from img_shape, img_shape_device_ get updated.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    bool reset_tensor_device_ = false;\n+\n+      reset_tensor_device_ = true;\n+    } else {\n+      const int* tensor_data = tensor->template data<int>();\n+      for (int d_i = 0; d_i < data.size(); ++d_i) {\n+        if (tensor_data[d_i] != data[d_i]) {\n+          reset_tensor_device_ = true;\n+          break;\n+        }\n+      }\n+    }\n+\n+    if (reset_tensor_device_) {",
    "Label": "clean"
},
{
    "Id": 1359,
    "Library": "pytorch",
    "Date": "2018/03/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/c7e69e9015716ab11607665ccfa43b1de5bf8260",
    "Root Cause": "N.A",
    "Bug report": "Test documentation build in CI. (#5492)\n\n* Test documentation build in CI.\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\r\n\r\n* bugfix.\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>",
    "Number of deleted lines": 1,
    "Deleted lines": "-   cd extension-ffi/script",
    "Added lines": "+   pushd extension-ffi/script\n+   popd\n+fi\n+\n+# Test documentation build\n+if [[ \"$JOB_NAME\" == *xenial-cuda8-cudnn6-py3* ]]; then\n+  pushd docs\n+  pip install -r requirements.txt || true\n+  make html\n+  popd",
    "Label": "clean"
},
{
    "Id": 1360,
    "Library": "pytorch",
    "Date": "2018/02/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ec547ce640581846d0c5afdc334beaec5da9e745",
    "Root Cause": "N.A",
    "Bug report": "RNN ONNX export: concat hidden/cell states on the right axis (#2055)\n\nTest Plan: existing tests in onnx-fb-universe catch this, modulo a bug\r\nin the tests which I am fixing in a separate diff",
    "Number of deleted lines": 4,
    "Deleted lines": "-                               [n.outputs[1], dummy_name()], axis=2)\n-                               [n.outputs[1], dummy_name()], axis=2)\n-                               [n.outputs[2], dummy_name()], axis=2)\n-                               [n.outputs[1], dummy_name()], axis=2)",
    "Added lines": "+                               [n.outputs[1], dummy_name()], axis=0)\n+                               [n.outputs[1], dummy_name()], axis=0)\n+                               [n.outputs[2], dummy_name()], axis=0)\n+                               [n.outputs[1], dummy_name()], axis=0)",
    "Label": "clean"
},
{
    "Id": 1361,
    "Library": "pytorch",
    "Date": "2018/02/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/013ed5b88fc869c3fe5934eaef346ebdada05d5c",
    "Root Cause": "N.A",
    "Bug report": "Add lazy_init.h into build for Windows and refactor code (#5365)\n\n* Add lazy_init.h into build for Windows and refactor code\r\n\r\n* Remove minor bugs",
    "Number of deleted lines": 9,
    "Deleted lines": "-                # To generate .obj files for AutoGPU for the export class\n-                if not os.path.exists(\"torch/csrc/generated\"):\n-                    os.mkdir(\"torch/csrc/generated\")\n-                if os.path.exists(\"torch/csrc/generated/AutoGPU_cpu_win.cpp\"):\n-                    os.remove(\"torch/csrc/generated/AutoGPU_cpu_win.cpp\")\n-                shutil.copyfile(\"torch/csrc/cuda/AutoGPU.h\", \"torch/csrc/generated/AutoGPU_cpu_win.cpp\")\n-if IS_WINDOWS and not WITH_CUDA:\n-    main_sources += [\"torch/csrc/generated/AutoGPU_cpu_win.cpp\"]\n-",
    "Added lines": "+                # To generate .obj files for those .h files for the export class\n+                temp_dir = 'torch/csrc/generated'\n+                hfile_list = ['torch/csrc/cuda/AutoGPU.h',\n+                              'torch/csrc/cuda/lazy_init.h']\n+                hname_list = [os.path.basename(hfile) for hfile in hfile_list]\n+                rname_list = [os.path.splitext(hname)[0]\n+                              for hname in hname_list]\n+                cfile_list = [temp_dir + '/' + rname +\n+                              '_cpu_win.cpp' for rname in rname_list]\n+\n+                if not os.path.exists(temp_dir):\n+                    os.mkdir(temp_dir)\n+\n+                for hfile, cfile in zip(hfile_list, cfile_list):\n+                    if os.path.exists(cfile):\n+                        os.remove(cfile)\n+                    shutil.copyfile(hfile, cfile)\n+\n+                C.main_sources += cfile_list",
    "Label": "clean"
},
{
    "Id": 1362,
    "Library": "pytorch",
    "Date": "2018/02/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/9bf9f0e613db0d496f214cf75e82e93fc5ad106b",
    "Root Cause": "N.A",
    "Bug report": "Fix the bug of only processing one attribute (#5334)",
    "Number of deleted lines": 3,
    "Deleted lines": "-              case TK_LIST:\n-            break;\n-            break;",
    "Added lines": "+              case TK_LIST: {\n+              } break;\n+            default:\n+                throw ErrorReport(attr) << \"Unexpected kind of attribute value: \" << value.kind();\n+                break;",
    "Label": "clean"
},
{
    "Id": 1363,
    "Library": "pytorch",
    "Date": "2018/01/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ea0283325cdba3ad3b96ef239af70e5bbc70b0a8",
    "Root Cause": "N.A",
    "Bug report": "fix copy/paste error in debug message",
    "Number of deleted lines": 1,
    "Deleted lines": "-                    hidden_label, input.size(1), self.input_size))",
    "Added lines": "+                    hidden_label, hx.size(1), self.hidden_size))",
    "Label": "clean"
},
{
    "Id": 1364,
    "Library": "pytorch",
    "Date": "2018/01/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e7d4bbc9dd03f387ee7bce179641328f009a65b0",
    "Root Cause": "N.A",
    "Bug report": "Add CaffeEnforce in SafeDequeueOp\n\nSummary:\nPreivously in SafeDequeueOp, the in.dims()[0] would fail if in.ndim()=0.\nHowever the error message if not informative. I added a Caffe_Enforce,\nwhich would print out the input and output blob name. This is very helpful for\nfuture debugging as well.\n\nDifferential Revision: D6821421\n\nfbshipit-source-id: b07e5829a2c580aaaac88b0d9ff8d05f6da11713",
    "Number of deleted lines": 3,
    "Deleted lines": "-        \"Expected \" + caffe2::to_string(size + 1) + \", \" + \" got: \" +\n-            caffe2::to_string(size));\n-}",
    "Added lines": "+        \"Expected \" + caffe2::to_string(size + 1) + \", \" +\n+            \" got: \" + caffe2::to_string(size));\n+\n+          CAFFE_ENFORCE(\n+              in.ndim() > 0,\n+              \"Empty tensor to dequeue at column \",\n+              col,\n+              \" within \",\n+              size,\n+              \" total columns\");\n+\n+} // namespace caffe2",
    "Label": "clean"
},
{
    "Id": 1365,
    "Library": "pytorch",
    "Date": "2018/01/26",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/fe9121ff59484c529e85fa139fc7b945722a1941",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in BatchMM JIT pass\n\nadd node has multiple overloads, including one that only takes a\nsingle input. This wasn't checked previously and could lead to\nsegfaults.",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      // NOTE: x + 2 is add[other={2}](%x)\n+      if (node->inputs().size() != 2) continue;",
    "Label": "clean"
},
{
    "Id": 1366,
    "Library": "pytorch",
    "Date": "2018/01/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/db6777eaf4a08ab79509cbc7052f938c9587eacd",
    "Root Cause": "N.A",
    "Bug report": "fix gru_cell bug\n\nSummary:\nthe fc needs to be in the output_gate_t scope so it can find its input\nweights correctly\nCloses https://github.com/caffe2/caffe2/pull/1739\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D6705443\n\nPulled By: anderspapitto\n\nfbshipit-source-id: 139e83ac77589a203ffe404fedab98eea5b1a51c",
    "Number of deleted lines": 3,
    "Deleted lines": "-                self.scope('output_gate_fc'),\n-                self.scope('output_gate_t')\n-            self.scope('output_gate_t'),",
    "Added lines": "+                self.scope('output_gate_t'),\n+                self.scope('output_gate_t_mul')\n+            self.scope('output_gate_t_summed'),",
    "Label": "clean"
},
{
    "Id": 1367,
    "Library": "pytorch",
    "Date": "2018/01/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1f8a8cc941488b438a6418250752627c48524a2b",
    "Root Cause": "N.A",
    "Bug report": "Fix two bugs in thnn_conv_depthwise2d_backward gradient.\n\n- Out of bounds grads[2] access (thnn_conv_depthwise2d_backward\n  doesn't compute bias gradient)\n\n- Groups was not set appropriately for depthwise convolution\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>",
    "Number of deleted lines": 1,
    "Deleted lines": "-  grad_output, self, weight: _convolution_double_backward(grads[0], grads[1], grads[2], grad_output, weight, self, stride, padding, dilation, false, {{0, 0}}, 1, false, false, false, grad_input_mask)",
    "Added lines": "+  grad_output, self, weight: _convolution_double_backward(grads[0], grads[1], {}, grad_output, weight, self, stride, padding, dilation, false, {{0, 0}}, self.size(1), false, false, false, grad_input_mask)",
    "Label": "clean"
},
{
    "Id": 1368,
    "Library": "pytorch",
    "Date": "2018/01/03",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/28eea8b0325990e2535082548a854ad628f410b5",
    "Root Cause": "N.A",
    "Bug report": "Adding commandline flags to disable implicit engine preferences.\n\nSummary:\nDuring debugging I found that our recently added automatic engine preference actually makes debugging a bit harder - it implicitly routes computation to e.g. CUDNN when we actually want to test out the default GPU implementations.\n\nThis diff adds a commandline flag that disables it.\nCloses https://github.com/caffe2/caffe2/pull/1696\n\nReviewed By: pietern\n\nDifferential Revision: D6658765\n\nPulled By: Yangqing\n\nfbshipit-source-id: ef56a16e778eeea6ecdd4dc6002421236e15371a",
    "Number of deleted lines": 2,
    "Deleted lines": "-  if (g_per_op_engine_pref().count(device_type) &&\n-  if (g_global_engine_pref().count(device_type)) {",
    "Added lines": "+CAFFE2_DEFINE_bool(\n+    caffe2_disable_implicit_engine_preference,\n+    false,\n+    \"If set, disable implicit engine preferences. This is useful for unit \"\n+    \"testing and debugging cases.\");\n+  if (!FLAGS_caffe2_disable_implicit_engine_preference &&\n+      g_per_op_engine_pref().count(device_type) &&\n+    VLOG(2) << \"Inserting per-op engine preference: \" << preferred_engines;\n+  if (!FLAGS_caffe2_disable_implicit_engine_preference &&\n+      g_global_engine_pref().count(device_type)) {\n+    VLOG(2) << \"Inserting global engine preference: \" << preferred_engines;",
    "Label": "clean"
},
{
    "Id": 1369,
    "Library": "pytorch",
    "Date": "2017/11/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/14cc15e8f44dac72626b90d8c1ab5a5826fd1022",
    "Root Cause": "N.A",
    "Bug report": "fixed NCCL bug in data_parallel_model.py\n\nSummary:\nChanged the dict of viewvalues into a python list\n\nSee issue: https://github.com/caffe2/caffe2/issues/1516\nCloses https://github.com/caffe2/caffe2/pull/1532\n\nDifferential Revision: D6425901\n\nPulled By: akyrola\n\nfbshipit-source-id: 37988abe29726aea86637e18eedb948b7c281008",
    "Number of deleted lines": 2,
    "Deleted lines": "-                    viewvalues(model._device_grouped_blobs[param]),\n-                    viewvalues(model._device_grouped_blobs[param]),",
    "Added lines": "+                    list(viewvalues(model._device_grouped_blobs[param])),\n+                    list(viewvalues(model._device_grouped_blobs[param])),",
    "Label": "clean"
},
{
    "Id": 1370,
    "Library": "pytorch",
    "Date": "2017/11/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/a3afca6fc979b107788fe8cc062ca4d511dd00e1",
    "Root Cause": "N.A",
    "Bug report": "Minor documentation fix in NetBuiler\n\nSummary: Came across this bug in doc when I was figuring out NetBuilder form the code.\n\nReviewed By: volkhin\n\nDifferential Revision: D6341821\n\nfbshipit-source-id: 8818f3d92681366bfe7b90d9d4da9f68ef6e4672",
    "Number of deleted lines": 1,
    "Deleted lines": "-                    ops.Add([d, ops.Const(10)])",
    "Added lines": "+                    ops.Add([d, ops.Const(10)], [d])",
    "Label": "clean"
},
{
    "Id": 1371,
    "Library": "pytorch",
    "Date": "2017/11/10",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/1a58775e19ed35268b63008faf1c477d44dbce63",
    "Root Cause": "N.A",
    "Bug report": "Fix AppVeyor Windows build due to template chaining\n\nSummary:\nThe windows compiler has a bug with chained templates. This diff avoids using such pattern in `plan_executor.cc`.\nCloses https://github.com/caffe2/caffe2/pull/1442\n\nReviewed By: Yangqing\n\nDifferential Revision: D6300046\n\nPulled By: heslami\n\nfbshipit-source-id: 1dc74441d6e2f0586c636e799eb5e88ced289063",
    "Number of deleted lines": 5,
    "Deleted lines": "-      int node_id =\n-          node_id_blob->template Get<TensorCPU>().template data<int32_t>()[0];\n-      global_ws_id_blob->template GetMutable<TensorCPU>()->template Resize();\n-      global_ws_id_blob->template GetMutable<TensorCPU>()\n-          ->template mutable_data<int64_t>()[0] = global_ws_id;",
    "Added lines": "+      TensorCPU node_id_tensor = node_id_blob->template Get<TensorCPU>();\n+      int node_id = node_id_tensor.template data<int32_t>()[0];\n+      TensorCPU* global_ws_id_tensor =\n+          global_ws_id_blob->template GetMutable<TensorCPU>();\n+      global_ws_id_tensor->Resize();\n+      global_ws_id_tensor->template mutable_data<int64_t>()[0] = global_ws_id;",
    "Label": "clean"
},
{
    "Id": 1372,
    "Library": "pytorch",
    "Date": "2017/11/09",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/12e4af94e8588d69204f47123b00f5e8def611a0",
    "Root Cause": "N.A",
    "Bug report": "add better gradient creation error message\n\nSummary: Print the full operator definition when gradient creation fails. This helps debugging cases where same op type is used in many places.\n\nDifferential Revision: D6282832\n\nfbshipit-source-id: 4b9dab2602c7c53f795da93a3085cf5c8ca741c1",
    "Number of deleted lines": 2,
    "Deleted lines": "-                    \"Exception when creating the gradient for [{}]: {}.\".\n-                    format(op.type, e)",
    "Added lines": "+                    \"Exception when creating gradient for [{}]:{}.\\nOp: \\n{}\".\n+                    format(op.type, e, str(op))",
    "Label": "clean"
},
{
    "Id": 1373,
    "Library": "pytorch",
    "Date": "2017/11/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/84b76a0712b3b203d3ef1fa58b914e6f20f66e45",
    "Root Cause": "N.A",
    "Bug report": "fix shape info in concat layer\n\nSummary:\nThe output shape info is incorrect, e.g. if we have 4 embeddings with dim size 32, the actual shape is (4, 32),\nbut the previous implementation in concat layer will give us (128, 1). This bug doesn't affect the dot products\ncalculation because the actual shape of the blob is still (4, 32) in concat_split_op\n\nDifferential Revision: D6264793\n\nfbshipit-source-id: 82995e83a8c859cbd15617ff7850a35b30b453b6",
    "Number of deleted lines": 1,
    "Deleted lines": "-                shapes[i].insert(axis, 1)",
    "Added lines": "+                shapes[i].insert(axis - 1, 1)",
    "Label": "clean"
},
{
    "Id": 1374,
    "Library": "pytorch",
    "Date": "2017/07/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/782be20ac264ccdfb27a35f3fd63f7659505ecc1",
    "Root Cause": "N.A",
    "Bug report": "fix bug in method declarations",
    "Number of deleted lines": 1,
    "Deleted lines": "-        return arg.get('ignore_check')",
    "Added lines": "+        return arg.get('ignore_check') or arg['type'] == 'CONSTANT'",
    "Label": "clean"
},
{
    "Id": 1375,
    "Library": "pytorch",
    "Date": "2017/07/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/52a561e5832e02045a9a112f566740bf86b0a164",
    "Root Cause": "N.A",
    "Bug report": "Fix ATen build for debug python",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+# Filter out anything that is not a source file\n+filter_list(generated_cpp generated_cpp \"(\\\\.cpp|\\\\.h)$\")",
    "Label": "clean"
},
{
    "Id": 1376,
    "Library": "pytorch",
    "Date": "2017/10/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/f7b15c52ff31a89945a849cc76f964a86b9e5d82",
    "Root Cause": "N.A",
    "Bug report": "partial revert of D6155510 to fix a race condition\n\nSummary:\nI (actually by mistake) included some premature optimization in D6155510 for the threaded rnn executor. Unfortunately, there was a subtle race condition when some ops where run out-of-order, but i had made the count down only to count down in the last timestep. Hard to explain.\n\nFor caution, revert D6155510's changes to recurrent_network_executor.cc excluding one assertion and setting of the debug flag.\n\nDifferential Revision: D6195544\n\nfbshipit-source-id: 24a275e185e5a80835401a8cdcb162dbc2411789",
    "Number of deleted lines": 9,
    "Deleted lines": "-  countdown_ = timestep_ops_[0].size();\n-  countdown_ = timestep_ops_[0].size();\n-  if (last_timestep) {\n-    if (countdown_.fetch_sub(1) == 1) {\n-      CAFFE_ENFORCE_EQ(0, job_queue_.size());\n-      cv_.notify_one();\n-    }\n-  CAFFE_ENFORCE_EQ(false, failed_, \"Recurrent network execution failed\");\n-  CAFFE_ENFORCE_EQ(job_queue_.size(), 0);",
    "Added lines": "+  countdown_ = T * timestep_ops_[0].size();\n+  countdown_ = T * timestep_ops_[0].size();\n+  if (countdown_.fetch_sub(1) == 1) {\n+    CAFFE_ENFORCE_EQ(0, job_queue_.size());\n+    std::unique_lock<std::mutex> lk(countdown_mtx_);\n+    cv_.notify_one();\n+  CAFFE_ENFORCE_EQ(false, failed_);",
    "Label": "clean"
},
{
    "Id": 1377,
    "Library": "pytorch",
    "Date": "2017/10/28",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/e0d7de5b61c3cf5305c595e0c22cef69564a5c49",
    "Root Cause": "N.A",
    "Bug report": "Fix bug introduced in a recent commit\n\nSummary:\nThis is introduced in https://github.com/caffe2/caffe2/commit/8539a1e78b7fd8aa92b08a8f24b2b0f7978bf5a5 - vector<float> should not be used in Tensor shape inference.\nCloses https://github.com/caffe2/caffe2/pull/1393\n\nReviewed By: akyrola\n\nDifferential Revision: D6181075\n\nPulled By: Yangqing\n\nfbshipit-source-id: 002144a137148b5b16118d0c123132890e8d325a",
    "Number of deleted lines": 1,
    "Deleted lines": "-      out[1] = CreateTensorShape(vector<float>{batch_size}, TensorProto::FLOAT);",
    "Added lines": "+      out[1] = CreateTensorShape(vector<int>{batch_size}, TensorProto::FLOAT);",
    "Label": "clean"
},
{
    "Id": 1378,
    "Library": "pytorch",
    "Date": "2017/10/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/63297e1a1f632f920a980bcaeb04e455fe54d404",
    "Root Cause": "N.A",
    "Bug report": "RunNetOnce->RunNet (removes rnn_executor overhead)\n\nSummary:\nseq2seq/translate.py was running much slower on RNNExecutor. This was because RNNExecutor has significant init overhead (I have another diff to reduce, but not completely eliminate it), and translate was calling the decoder with RunNetOnce -- thus always recreating the net and the ops. Changhing this to RunNet() makes translate run faster than without executor. RunNet uses the net name and uses the already created net, while RunNetOnce passes the whole protobuffer.\n\nNoticed similar bug in seq2seq ensemble bean model, which also calls CreateNet() but uses RunNetOnce() instead of RunNet().\n\nReviewed By: jhcross\n\nDifferential Revision: D6156566\n\nfbshipit-source-id: a933453e36a0d8fd163d0584186fda427a680687",
    "Number of deleted lines": 1,
    "Deleted lines": "-        workspace.RunNetOnce(self.model.net)",
    "Added lines": "+        workspace.RunNet(self.model.net)",
    "Label": "clean"
},
{
    "Id": 1379,
    "Library": "pytorch",
    "Date": "2017/10/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/b92e06e50e9e196e8d60809d945722a36a4abd0c",
    "Root Cause": "N.A",
    "Bug report": "Fix reference counting bug in python_nn_functions.cpp (#3236)\n\nPy_InitModule returns a borrowed reference. PyModule_AddObject steals\r\nthe reference, so we need to incref the `_nn` object.\r\n\r\n(The Python 3 function PyModule_Create returns a new reference.)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+  Py_XINCREF(nn);  // Py_InitModule returns \"borrowed\" reference\n+  // steals a reference to nn",
    "Label": "clean"
},
{
    "Id": 1380,
    "Library": "pytorch",
    "Date": "2017/10/12",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ca392b7c76d1eb6a2fdd411179604b14f74758df",
    "Root Cause": "N.A",
    "Bug report": "remove timeout from RNN executor\n\nSummary:\nI had a 30 sec timeout in RNN executor to find out deadlock bugs, but looks like people are occasionally bumping on it in the course of normal business -- perhaps when CPU is heavily used, the threads don't get enough time and run out of the timeout.\n\nRemoved the timeout but retain the warning logging.\n\nReviewed By: salexspb\n\nDifferential Revision: D6001960\n\nfbshipit-source-id: 5b2293359ee68c1c24f0d9e0406d88391e531280",
    "Number of deleted lines": 15,
    "Deleted lines": "-    VLOG(1) << \"Start RNN worker \" << workers_.size() << \" / \"\n-              << num_threads_;\n-  cv_.wait_for(lk, std::chrono::seconds(30), [&] {\n-    // Log if we are still running, so that we catch deadlocks.. there\n-    // should not be any deadlocks, but...\n-    if (t.Seconds() > 10) {\n-      LOG(INFO) << \"RNN Executor still running, remaining ops: \" << countdown_;\n-    }\n-    return failed_ || countdown_ == 0;\n-  });\n-\n-  CAFFE_ENFORCE_EQ(false, failed_, \"Recurrent network execution failed\");\n-  CAFFE_ENFORCE_EQ(\n-      0, countdown_, \"Recurrent network execution did not finish in time\");\n-  CAFFE_ENFORCE_EQ(job_queue_.size(), 0);",
    "Added lines": "+    VLOG(1) << \"Start RNN worker \" << workers_.size() << \" / \" << num_threads_;\n+  while (!failed_ && countdown_ > 0) {\n+    cv_.wait_for(lk, std::chrono::seconds(30), [&] {\n+      // Log if we are still running, so that we catch deadlocks.. there\n+      // should not be any deadlocks, but...\n+      if (t.Seconds() > 10) {\n+        LOG(INFO) << \"RNN Executor still running, remaining ops: \"\n+                  << countdown_;\n+      }\n+      return failed_ || countdown_ == 0;\n+    });\n+  }",
    "Label": "clean"
},
{
    "Id": 1381,
    "Library": "pytorch",
    "Date": "2017/10/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6a91f556d0c3ba53757f27bf08df9484e0122bf4",
    "Root Cause": "N.A",
    "Bug report": "fix a bug in exporter, we forgot to copy type to the new node for index op",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+        squeeze->setType(first_select->typeOption());",
    "Label": "clean"
},
{
    "Id": 1382,
    "Library": "pytorch",
    "Date": "2017/10/06",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/5f8bab47c8f87c73f08ad13705aea1fda747e997",
    "Root Cause": "N.A",
    "Bug report": "bugfix for 2428 ussue (#3000)",
    "Number of deleted lines": 11,
    "Deleted lines": "-        # cross-node buffer sync\n-        flat_buffers = _flatten_tensors(buffers)\n-        dist.broadcast(flat_buffers, 0)\n-        for buf, synced in zip(buffers, _unflatten_tensors(flat_buffers, buffers)):\n-            buf.copy_(synced)\n-\n-        # intra-node buffer sync\n-        result = broadcast_coalesced(buffers, self.device_ids, self.broadcast_bucket_size)\n-        for tensors, module in zip(result[1:], self._module_copies[1:]):\n-            for tensor, buf in zip(tensors, module._all_buffers()):\n-                buf.set_(tensor)",
    "Added lines": "+        if len(buffers) > 0:\n+            # cross-node buffer sync\n+            flat_buffers = _flatten_tensors(buffers)\n+            dist.broadcast(flat_buffers, 0)\n+            for buf, synced in zip(buffers, _unflatten_tensors(flat_buffers, buffers)):\n+                buf.copy_(synced)\n+\n+            # intra-node buffer sync\n+            result = broadcast_coalesced(buffers, self.device_ids, self.broadcast_bucket_size)\n+            for tensors, module in zip(result[1:], self._module_copies[1:]):\n+                for tensor, buf in zip(tensors, module._all_buffers()):\n+                    buf.set_(tensor)",
    "Label": "clean"
},
{
    "Id": 1383,
    "Library": "pytorch",
    "Date": "2017/09/25",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/ded3a3b317fbc8b165235c98f238c193e1482ad5",
    "Root Cause": "N.A",
    "Bug report": "fix small bug in nccl setup helper",
    "Number of deleted lines": 1,
    "Deleted lines": "-                NCCL_INCLUDE_DIR = path",
    "Added lines": "+                NCCL_INCLUDE_DIR = os.path.join(path, '../include')",
    "Label": "clean"
},
{
    "Id": 1384,
    "Library": "pytorch",
    "Date": "2017/09/24",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/289dc2a870c0f51cc2dd5a168d3de5170b7e7e35",
    "Root Cause": "N.A",
    "Bug report": "fix argument passing bug in build_libs and allow external NCCL_ROOT_DIR via environment variable",
    "Number of deleted lines": 2,
    "Deleted lines": "-    GLOO_FLAGS=\"-DUSE_CUDA=1 -DNCCL_ROOT_DIR=$INSTALL_DIR\"\n-              $2",
    "Added lines": "+NCCL_ROOT_DIR=${NCCL_ROOT_DIR:-$INSTALL_DIR}\n+    GLOO_FLAGS=\"-DUSE_CUDA=1 -DNCCL_ROOT_DIR=$NCCL_ROOT_DIR\"\n+              ${@:2}",
    "Label": "clean"
},
{
    "Id": 1385,
    "Library": "pytorch",
    "Date": "2017/09/22",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/bf9ab91779a67b39ec712f7878e895e97b2c77e8",
    "Root Cause": "N.A",
    "Bug report": "Indicate if the last invocation of setup.py was debug or not.\n\nHow to use:\n\n    import torch.version\n    print(torch.version.debug)\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            # NB: This is not 100% accurate, because you could have built the\n+            # library code with DEBUG, but csrc without DEBUG (in which case\n+            # this would claim to be a release build when it's not.)\n+            f.write(\"debug = {}\\n\".format(repr(DEBUG)))",
    "Label": "clean"
},
{
    "Id": 1386,
    "Library": "pytorch",
    "Date": "2017/09/11",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/af688905e4c5501dd15cbf98e02042d9f5160cf3",
    "Root Cause": "N.A",
    "Bug report": "Fix a bug in CppOp (missing cloneFrom)",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    JIT_ASSERT(fn);\n+  virtual void cloneFrom(Node * other_) override {\n+    Node::cloneFrom(other_);\n+    auto other = other_->cast<CppOp>();\n+    this->fn = other->fn;\n+  }",
    "Label": "clean"
},
{
    "Id": 1387,
    "Library": "pytorch",
    "Date": "2017/09/15",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6b44a00c712f89421d613e3b094779cba23943d6",
    "Root Cause": "N.A",
    "Bug report": "remove in-place Dropout from rnn_cell (bug in PR-1185)\n\nSummary: This caused gradient generation problems. Output was made in-place in PR-1185, by mistake, I believe.\n\nDifferential Revision: D5844825\n\nfbshipit-source-id: 4ad84d0fb468aafde9f78463b9acf89316e633ca",
    "Number of deleted lines": 1,
    "Deleted lines": "-                    str(output),",
    "Added lines": "+                    str(output) + '_with_dropout_mask{}'.format(self.mask),",
    "Label": "clean"
},
{
    "Id": 1388,
    "Library": "pytorch",
    "Date": "2017/09/13",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/efda016108f16d4ac6087a3bcb0909729c1a7e11",
    "Root Cause": "N.A",
    "Bug report": "fix dynamic-type-mismatch (ubsan) in caffe2/caffe2/core/tensor.h\n\nSummary:\nUBSan report:\n\n```\nUndefinedBehaviorSanitizer: dynamic-type-mismatch caffe2/caffe2/core/tensor.h:786:22 in\ncaffe2/caffe2/core/tensor.h:787:19: runtime error: member call on address 0x60c01f610440 which does not point to an object of type 'caffe2::Tensor<caffe2::Tensor<caffe2::CPUContext> >'\n*** Aborted at 1505298367 (Unix time, try 'date -d 1505298367') ***\n*** Signal 6 (SIGABRT) (0xf2) received by PID 242 (pthread TID 0x7fb376f06700) (linux TID 33215) (maybe from PID 242, UID 0), stack trace: ***\n0x60c01f610440: note: object is of type 'N6caffe26TensorINS_10CPUContextEEE'\n 07 5e 81 60  c8 47 13 35 00 00 00 00  90 f3 73 80 20 60 00 00  98 f3 73 80 20 60 00 00  a0 f3 73 80\n              ^~~~~~~~~~~~~~~~~~~~~~~\n              vptr for 'N6caffe26TensorINS_10CPUContextEEE'\n    #0 0x1f0d1c22 in std::vector<long, std::allocator<long> > caffe2::GetTensorInfo<caffe2::Tensor<caffe2::CPUContext> >(void const*, bool*, unsigned long*, caffe2::DeviceOption*) caffe2/caffe2/core/tensor.h:787:19\n    #1 0x9a5e0a1 in caffe2::FacebookOperatorObserver::log() caffe2/caffe2/fb/init/net_observer.cpp:300:15\n    #2 0x9a5b49d in caffe2::FacebookOperatorObserver::Stop() caffe2/caffe2/fb/init/net_observer.cpp:229:11\n    #3 0x447d046 in caffe2::Operator<caffe2::CPUContext>::Run(int) caffe2/caffe2/core/operator.h:308:20\n    #4 0x1ecedb2f in caffe2::SimpleNet::Run() caffe2/caffe2/core/net_simple.cc:51:14\n    #5 0x1f1ba169 in caffe2::Workspace::RunNet(std::basic_fbstring<char, std::char_traits<char>, std::allocator<char>, std::fbstring_core<char> > const&) caffe2/caffe2/core/workspace.cc:211:26\n...\n```\n\nThe bug is that `GetTensorType` and `GetTensorType` take context as template argument, not tensor itself.\n\nReviewed By: bddppq\n\nDifferential Revision: D5826781\n\nfbshipit-source-id: 9cfd2ca1aaef6f8ee8a556ce7b553c0a4f43a100",
    "Number of deleted lines": 2,
    "Deleted lines": "-  {TypeMeta::Id<Tensor<CPUContext>>(), GetTensorType<Tensor<CPUContext>>}\n-    {TypeMeta::Id<Tensor<CPUContext>>(), GetTensorInfo<Tensor<CPUContext>>}};",
    "Added lines": "+  {TypeMeta::Id<Tensor<CPUContext>>(), GetTensorType<CPUContext>}\n+    {TypeMeta::Id<Tensor<CPUContext>>(), GetTensorInfo<CPUContext>}};",
    "Label": "clean"
},
{
    "Id": 1389,
    "Library": "pytorch",
    "Date": "2017/06/21",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/6f9774d7dbaff0c9245b982d79a08a734a5ba398",
    "Root Cause": "N.A",
    "Bug report": "Minor bugfix.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>",
    "Number of deleted lines": 3,
    "Deleted lines": "-          if (THPFunction_Check(obj)) throw std::logic_error(\"Not a function\");\n-          THPObjectPtr{ THPFunction_do_forward(fn, input_objs) };\n-          THPObjectPtr{ THPFunction_apply(obj, input_objs) };",
    "Added lines": "+  // TODO: this is wrong\n+          if (!THPFunction_Check(obj)) throw std::logic_error(\"Not a function in THPFunction_do_forward\");\n+          output_objs = THPFunction_do_forward(fn, input_objs);\n+          output_objs = THPFunction_apply(obj, input_objs);\n+        }\n+        if (!output_objs) {\n+          throw python_error();",
    "Label": "clean"
},
{
    "Id": 1390,
    "Library": "pytorch",
    "Date": "2017/09/05",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/77a02eaa7fe6d95d836e77f1c12af32acbdfa448",
    "Root Cause": "N.A",
    "Bug report": "Enable reader checkpoint\n\nSummary:\nReader checkpointing was disabled due to bug captured in T21143272\nNow that we have resolved that issue, re-enabling reader checkpointing\n\nReviewed By: boryiingsu, rayleichen\n\nDifferential Revision: D5730545\n\nfbshipit-source-id: 7fae48b03e07eaf530bfc9e8e8b6683d8ed4e206",
    "Number of deleted lines": 6,
    "Deleted lines": "-                # TODO(aartibasant, T21070353): Enable the checkpoints for\n-                # readers.\n-                # The checkpointing for readers is broken because of D5582328.\n-                # Disabling the reader checkpoints until it is fixed.\n-                if \"reader\" in str(node):\n-                    continue",
    "Added lines": "",
    "Label": "clean"
},
{
    "Id": 1391,
    "Library": "pytorch",
    "Date": "2017/09/04",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/db78f3cf468549b206c3c8bdc9fb42df86ded2a7",
    "Root Cause": "N.A",
    "Bug report": "fix bug for THTensor data access",
    "Number of deleted lines": 1,
    "Deleted lines": "-      if(q_data[large] < 1.0)",
    "Added lines": "+      if(q_data[large * q->stride[0]] < 1.0)",
    "Label": "clean"
},
{
    "Id": 1392,
    "Library": "pytorch",
    "Date": "2017/09/01",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/0c324ba4171bca7ce20a8a389ed174ab6d43cef9",
    "Root Cause": "N.A",
    "Bug report": "set stream for cudnn handle correctly in cudnn wapper\n\nSummary:\nCuDNNWRapper inline_cudnn_handle() should set the stream every time, since it can change. This caused problems in RNN scenarious. Also this bug rendered singlethread_async_net incorrect / slow!\n\nI found out the problem by using nvprof --print-gpu-trace and noticing that some kernels were run in different stream than i expected.\n\nReviewed By: ajtulloch, Yangqing\n\nDifferential Revision: D5758426\n\nfbshipit-source-id: 651c62fe28eaf09e1675d4adf3f1fac8b4c8e75b",
    "Number of deleted lines": 4,
    "Deleted lines": "-    if (cudnn_handle_) {\n-      return cudnn_handle_;\n-    } else {\n-      CUDNN_ENFORCE(cudnnSetStream(cudnn_handle_, context_->cuda_stream()));",
    "Added lines": "+    if (!cudnn_handle_) {\n+    CUDNN_ENFORCE(cudnnSetStream(cudnn_handle_, context_->cuda_stream()));",
    "Label": "clean"
},
{
    "Id": 1393,
    "Library": "pytorch",
    "Date": "2017/08/30",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/59540847b136b20c816a27bf9162d8f1b6572e53",
    "Root Cause": "N.A",
    "Bug report": "Include Caffe2 headers before anything else\n\nSummary:\nThis was a tricky one to debug. After pulling from master, my build\nwas complaining that certain identifiers in updated source files were\nundefined. After building with VERBOSE=1, extracting the compilation\ncommands, and adding -M, I saw that CMake had included the Caffe2\ninstallation directory as include path. Worse yet, this path had\nprecedence over the path to the actual source code. The compiler\nincluded older headers when compiling newer source files.\n\nThis change forces the path to the Caffe2 source code to take\nprecedence over all other include paths. The only path that takes\nprecedence over *that* path is PROJECT_BINARY_DIR, which holds the\nheaders that are generated at compile time.\nCloses https://github.com/caffe2/caffe2/pull/1140\n\nReviewed By: Yangqing\n\nDifferential Revision: D5727133\n\nPulled By: pietern\n\nfbshipit-source-id: c60c89e82e8b1ab1cfca0907d31b84417788d79b",
    "Number of deleted lines": 5,
    "Deleted lines": "-# ---[ Include path needed for proto\n-# ---[ Third party builds.\n-include_directories(${PROJECT_SOURCE_DIR})\n-include_directories(${PROJECT_SOURCE_DIR}/build_host_protoc/include)\n-",
    "Added lines": "+# Prefix path to Caffe2 headers.\n+# If a directory containing installed Caffe2 headers was inadvertently\n+# added to the list of include directories, prefixing\n+# PROJECT_SOURCE_DIR means this source tree always takes precedence.\n+include_directories(BEFORE ${PROJECT_SOURCE_DIR})\n+include_directories(BEFORE ${PROJECT_SOURCE_DIR}/build_host_protoc/include)\n+\n+# Prefix path to generated Caffe2 headers.\n+# These need to take precedence over their empty counterparts located\n+# in PROJECT_SOURCE_DIR.",
    "Label": "clean"
},
{
    "Id": 1394,
    "Library": "pytorch",
    "Date": "2017/08/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/d368b59177dab6761c670e0ac72dd758cb533ef9",
    "Root Cause": "N.A",
    "Bug report": "logging the blob that has type error\n\nSummary: Currently, it's not easy to track down which tensor is missing type and shape info. Print it out for easier debuggin.\n\nReviewed By: volkhin, xianjiec\n\nDifferential Revision: D5695223\n\nfbshipit-source-id: 7f0be0be777a35bb5a71b3799b29b91f0763c159",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+            logger.warning(\"Blob {} has type error\".format(blob))",
    "Label": "clean"
},
{
    "Id": 1395,
    "Library": "pytorch",
    "Date": "2017/08/23",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/4c69697d2acbbe8e418a0921464c09aaf09da82a",
    "Root Cause": "N.A",
    "Bug report": "Distribtued bug fixes. (#2434)",
    "Number of deleted lines": 3,
    "Deleted lines": "-        if len(self.device_ids) == 1:\n-            return self.module(*inputs, **kwargs)\n-            nccl.reduce(dev_coalesced, root=device_ids[0], streams=nccl_streams)",
    "Added lines": "+        if len(self.device_ids) == 1:\n+            return self.module(*inputs[0], **kwargs[0])\n+            nccl.reduce(dev_coalesced, root=0, streams=nccl_streams)",
    "Label": "clean"
},
{
    "Id": 1396,
    "Library": "pytorch",
    "Date": "2017/08/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/73e0b3f4014b9f5b716eb1216d11f13347207f27",
    "Root Cause": "N.A",
    "Bug report": "fixing the bug with squeezing a singleton dimension in torch.min and torch.max",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+    if (!keepdim) {\n+      THCTensor_(squeeze1d)(state, values, values, dimension);\n+      THCudaLongTensor_squeeze1d(state, indices, indices, dimension);\n+    }",
    "Label": "clean"
},
{
    "Id": 1397,
    "Library": "pytorch",
    "Date": "2017/08/07",
    "Commit Link": "https://github.com/pytorch/pytorch/commit/98ac4542e0e097cd1b26c62d0ffe7fb37230347c",
    "Root Cause": "N.A",
    "Bug report": "fixing the bug with squeezing a singleton dimension in torch.min and torch.max",
    "Number of deleted lines": 0,
    "Deleted lines": "",
    "Added lines": "+      if (!keepdim) {\n+        THTensor_(squeeze1d)(values_, values_, dimension);\n+        THLongTensor_squeeze1d(indices_, indices_, dimension);\n+      }\n+      if (!keepdim) {\n+        THTensor_(squeeze1d)(values_, values_, dimension);\n+        THLongTensor_squeeze1d(indices_, indices_, dimension);\n+      }",
    "Label": "clean"
}
