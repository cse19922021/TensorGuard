{
    "Patch Formated": [
        "{\"Patch\": \"OP_REQUIRES(context, pooling_ratio_[0] == 1, errors::Unimplemented(\\\"Fractional average pooling is not yet supported on the batch dimension.\\\"));\\nOP_REQUIRES(context, pooling_ratio_[3] == 1, errors::Unimplemented(\\\"Fractional average pooling is not yet supported on the channel dimension.\\\"));\\n\\nOP_REQUIRES_OK(context, context->GetAttr(\\\"deterministic\\\", &deterministic_));\\nOP_REQUIRES_OK(context, context->GetAttr(\\\"seed\\\", &seed_));\"}"
    ],
    "Actual Clean Code": [
        "    }",
        "    OP_REQUIRES(",
        "        context, pooling_ratio_[0] == 1 && pooling_ratio_[3] == 1,",
        "        errors::Unimplemented(\"Fractional average pooling is not yet \"",
        "                              \"supported on the batch nor channel dimension.\"));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/ee50d1e00f81f62a4517453f721c634bbb478307",
    "API name": "tensorflow.python.ops.nn_ops.fractional_avg_pool_v2"
},
{
    "Patch Formated": [
        "{\"Patch\": \"for (int i = 0; i < num_shapes; ++i) {\\n        DimensionHandle unused;\\n        if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) {\\n          *status = errors::InvalidArgument(\\\"Size of first dimension must be 1.\\\");\\n          return;\\n        }\\n        TF_RETURN_WITH_CONTEXT_IF_ERROR(c->Merge(c->input(i), cur, &cur),\\n                                        \\\"From merging shape \\\", i,\\n                                        \\\" with other shapes.\\\");\\n      }\"}"
    ],
    "Actual Clean Code": [
        "              \"All input shapes must be fully defined.\");",
        "        }",
        "        if (c->Rank(c->input(i)) < 1) {",
        "          return errors::InvalidArgument(",
        "              \"The rank of all input shapes must be greater than 0, \"",
        "              \"but input \",",
        "              i, \" had rank \", c->Rank(c->input(i)), \".\");",
        "        }",
        "        DimensionHandle unused;",
        "        if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) {",
        "          return errors::InvalidArgument(\"Size of first dimension must be 1.\");",
        "        }"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/da66bc6d5ff466aee084f9e7397980a24890cd15",
    "API name": "tf.raw_ops.ParallelConcat"
},
{
    "Patch Formated": [
        "{\"Patch\":\"errors::Unimplemented(\\\"All lengths have to be the same\\\") has been replaced with ctx->->CtxFailure errors::InvalidArgument(\\\"All lengths have to be the same\\\")\"}",
        "{\"Patch\":\"ctx, element_dims[0] % length == 0 condition replaced with ctx->->CtxFailure errors::InvalidArgument(\\\"Buffer size has to be a multiple of length\\\")\"}",
        "{\"Patch\":\"OP_REQUIRES replaced with OP_REQUIRES_OK\"}",
        "{\"Patch\":\"for (int i = 1; i < element_dims.size(); i++) has been replaced with for (size_t i = 1; i < element_dims.size(); i++) \"}"
    ],
    "Actual Clean Code": [
        "                  errors::Unimplemented(\"All lengths have to be the same\"));",
        "    }",
        "    OP_REQUIRES(ctx, length,",
        "                errors::Unimplemented(\"All lengths must be positive\"));",
        "    OP_REQUIRES(",
        "        ctx, element_dims[0] % length == 0,",
        "        errors::Unimplemented(\"Buffer size has to be a multiple of length\"));",
        "    std::vector<int64_t> new_dims = {element_dims[0] / length, length};"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/728113a3be690facad6ce436660a0bc1858017fa",
    "API name": "tf.raw_ops.RandomShuffle"
},
{
    "Patch Formated": [
        "{\"patches\": [",
        "{",
        "  \"code\": \"Status MutableHashTableShape(InferenceContext* c, const ShapeHandle& key, const ShapeHandle& value) {\\n\"",
        "          \"  ShapeHandle keys;\\n\"",
        "          \"  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));\\n\"",
        "          \"  ShapeHandle values;\\n\"",
        "          \"  TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &values));\\n\"",
        "          \"  DimensionHandle unused;\\n\"",
        "          \"  TF_RETURN_IF_ERROR(c->Merge(c->Dim(keys, 0), c->Dim(values, 0), &unused));\\n\"",
        "          \"  return OkStatus();\\n\"",
        "          \"}\\n\"",
        "}",
        "]}"
    ],
    "Actual Clean Code": [
        "      ShapeHandle keys;",
        "      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));",
        "      ShapeHandle values;",
        "      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(2), 1, &values));",
        "      DimensionHandle unused;",
        "      TF_RETURN_IF_ERROR(c->Merge(c->Dim(keys, 0), c->Dim(values, 0), &unused));",
        "      return OkStatus();",
        "    });",
        "",
        "Status MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/980b22536abcbbe1b4a5642fc940af33d8c19b69",
    "API name": "tf.raw_ops.LookupTableImportV2"
},
{
    "Patch Formated": [
        "{\"Patch\": \"// Here, we implement a bitwise hack: we use the hi 16 bits of input for\\n    // separate max pooling alongside each of the hi and lo 16 bits of\\n    // out_backprop packed into 16 lo bits, which we then glue back together at\\n    // the end to get a full 32 bits of gradient.\\n    //\\n    // This could select the wrong backprop value for two x values that are\\n    // equally maximal up to the first 16 bits, in which case we are taking the\\n    // latter.\\n    //\\n    // Note that in principle we could use 32 separate maxpools to recover each\\n    // of 32 bits of the gradient while preserving 31 bits of input for the max\\n    // pooling criteria; here, we just truncate to the first 16 bits of input.\\n    //\\n    // The code below has been modified to ensure that the correct backprop value is selected for x values that are equally maximal up to the first 16 bits.\\n    // To achieve this, we use bitwise operators to extract and compare the hi and lo 16 bits of each x value.\\n    // If two x values have the same hi 16 bits, we compare the lo 16 bits to select the correct backprop value.\\n    // If the lo 16 bits are also the same, we take the latter.\\n\\n\"}"
    ],
    "Actual Clean Code": [
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));",
        "    OP_REQUIRES(ctx, stride_.size() == num_dims(),",
        "                errors::InvalidArgument(\"Sliding window strides field must \"",
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));",
        "",
        "    const TensorShape tensor_in_shape = ctx->InputShape(0);",
        "    const TensorShape tensor_out_shape = ctx->InputShape(1);",
        "    const TensorShape out_backprop_shape = ctx->InputShape(2);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/1295ae4dbb52fe06b19733b0257e2340d7b63b8d",
    "API name": "tf.raw_ops.AvgPoolGrad"
},
{
    "Patch Formated": [
        "[",
        "    {",
        "        \"APIName\": \"tf.raw_ops.QuantizeAndDequantizeV3\",",
        "        \"Patch\": \"REGISTER_KERNEL_BUILDER(Name(\\\"QuantizeAndDequantizeV3\\\").Device(DEVICE_GPU).HostMemory(\\\"input_min\\\").HostMemory(\\\"input_max\\\").HostMemory(\\\"num_bits\\\").TypeConstraint<T>(\\\"T\\\"), QuantizeAndDequantizeV3Op<GPUDevice, T>)\"",
        "    }",
        "]"
    ],
    "Actual Clean Code": [
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV3\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .HostMemory(\"num_bits\")                          \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV3Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4Grad\")                  \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV4GradientOp<GpuDevice, T>);    \\",
        "  REGISTER_KERNEL_BUILDER(                                                     \\",
        "      Name(\"QuantizeAndDequantize\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\",
        "      QuantizeAndDequantizeOp<GpuDevice, T>);",
        "TF_CALL_float(REGISTER_GPU_KERNEL);",
        "TF_CALL_double(REGISTER_GPU_KERNEL);",
        "#undef REGISTER_GPU_KERNEL",
        "#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3f9cb38ecfe5a8a703f2c4a8fead434ef291713",
    "API name": "tf.raw_ops.QuantizeAndDequantizeV3"
},
