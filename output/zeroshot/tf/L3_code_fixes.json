{
    "Patch Formated": [
        "{\"Patch\":\"OP_REQUIRES(context, pooling_ratio_[0] == 1.0 && pooling_ratio_[3] == 1.0,\\n            errors::Unimplemented(\\\"Fractional average pooling is not yet \\\",\\n                                  \\\"supported on the batch nor channel dimension.\\\"));\\nOP_REQUIRES_OK(context, context->GetAttr(\\\"deterministic\\\", &deterministic_));\\nOP_REQUIRES_OK(context, context->GetAttr(\\\"seed\\\", &seed_));\"}"
    ],
    "Actual Clean Code": [
        "    }",
        "    OP_REQUIRES(",
        "        context, pooling_ratio_[0] == 1 && pooling_ratio_[3] == 1,",
        "        errors::Unimplemented(\"Fractional average pooling is not yet \"",
        "                              \"supported on the batch nor channel dimension.\"));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/ee50d1e00f81f62a4517453f721c634bbb478307",
    "API name": "tensorflow.python.ops.nn_ops.fractional_avg_pool_v2"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"if (c->input(i).dims() < 1) {",
        "                    return errors::InvalidArgument(\\\"Shape must have rank greater than zero.\\\");",
        "                }\"",
        "}"
    ],
    "Actual Clean Code": [
        "              \"All input shapes must be fully defined.\");",
        "        }",
        "        if (c->Rank(c->input(i)) < 1) {",
        "          return errors::InvalidArgument(",
        "              \"The rank of all input shapes must be greater than 0, \"",
        "              \"but input \",",
        "              i, \" had rank \", c->Rank(c->input(i)), \".\");",
        "        }",
        "        DimensionHandle unused;",
        "        if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) {",
        "          return errors::InvalidArgument(\"Size of first dimension must be 1.\");",
        "        }"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/da66bc6d5ff466aee084f9e7397980a24890cd15",
    "API name": "tf.raw_ops.ParallelConcat"
},
{
    "Patch Formated": [
        "{\"Patch\": \"if (length <= 0) {\\n   OP_REQUIRES(\\n      ctx, length > 0,\\n      errors::InvalidArgument(\\\"Length value has to be greater than zero\\\"));\\n} \\nif (element_dims[0] % length != 0) {\\n   OP_REQUIRES(\\n      ctx, element_dims[0] % length == 0,\\n      errors::Unimplemented(\\\"Buffer size has to be a multiple of length\\\"));\\n}\\nstd::vector<int64_t> new_dims = {element_dims[0] / length, length};\\nfor (int i = 1; i < element_dims.size(); i++) {\\n   new_dims.push_back(element_dims[i]);\\n}\" }"
    ],
    "Actual Clean Code": [
        "                  errors::Unimplemented(\"All lengths have to be the same\"));",
        "    }",
        "    OP_REQUIRES(ctx, length,",
        "                errors::Unimplemented(\"All lengths must be positive\"));",
        "    OP_REQUIRES(",
        "        ctx, element_dims[0] % length == 0,",
        "        errors::Unimplemented(\"Buffer size has to be a multiple of length\"));",
        "    std::vector<int64_t> new_dims = {element_dims[0] / length, length};"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/728113a3be690facad6ce436660a0bc1858017fa",
    "API name": "tf.raw_ops.RandomShuffle"
},
{
    "Patch Formated": [
        "{",
        "\"Patch\": \"In the MutableHashTableShape() function, we need to validate the input parameters 'key' and 'value' to handle scalars properly. By checking the rank of the input, we can identify whether it is a scalar or not. If the rank is 0, it means it is a scalar and we need to reshape it to have a rank of 1 before further processing. If the rank is not 0, it means it is already a tensor and we can continue with the existing code. We can modify the code as follows:\\n\\nStatus MutableHashTableShape(InferenceContext* c, const ShapeHandle& key, const ShapeHandle& value) {\\n  ShapeHandle keys;\\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));\\n  if (c->Rank(key) == 0) {\\n    // Reshape key scalar\\n    const DimensionHandle dim_one = c->MakeDim(1);\\n    keys = c->MakeShape({dim_one});\\n  }\\n  DimensionHandle unused;\\n  TF_RETURN_IF_ERROR(c->Merge(c->Dim(keys, 0), c->Dim(c->input(2), 0), &unused));\\n  return OkStatus();\\n}\"",
        "}"
    ],
    "Actual Clean Code": [
        "      ShapeHandle keys;",
        "      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));",
        "      ShapeHandle values;",
        "      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(2), 1, &values));",
        "      DimensionHandle unused;",
        "      TF_RETURN_IF_ERROR(c->Merge(c->Dim(keys, 0), c->Dim(values, 0), &unused));",
        "      return OkStatus();",
        "    });",
        "",
        "Status MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/980b22536abcbbe1b4a5642fc940af33d8c19b69",
    "API name": "tf.raw_ops.LookupTableImportV2"
},
{
    "Patch Formated": [
        "{\"Patch\": \"Fix for the vulnerability in AvgPoolGrad: Ensuring that the stride and window size are positive to avoid FPE.\\n\\n// Here, we implement a bitwise hack: we use the hi 16 bits of input for separate max pooling alongside each of the hi and lo 16 bits of out_backprop packed into 16 lo bits, which we then glue back together at the end to get a full 32 bits of gradient.\\n//\\n// This could select the wrong backprop value for two x values that are equally maximal up to the first 16 bits, in which case we are taking the latter.\\n//\\n// Note that in principle we could use 32 separate maxpools to recover each of 32 bits of the gradient while preserving 31 bits of input for the max pooling criteria; here, we just truncate to the first 16 bits of input.\"}"
    ],
    "Actual Clean Code": [
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));",
        "    OP_REQUIRES(ctx, stride_.size() == num_dims(),",
        "                errors::InvalidArgument(\"Sliding window strides field must \"",
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));",
        "",
        "    const TensorShape tensor_in_shape = ctx->InputShape(0);",
        "    const TensorShape tensor_out_shape = ctx->InputShape(1);",
        "    const TensorShape out_backprop_shape = ctx->InputShape(2);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/1295ae4dbb52fe06b19733b0257e2340d7b63b8d",
    "API name": "tf.raw_ops.AvgPoolGrad"
},
{
    "Patch Formated": [
        "{\"Patch\": \"In the given vulnerable code, the issue lies with the improper input validation. To fix this vulnerability, we need to add proper input validation to ensure that only scalar input tensors are accepted. To achieve this, we can modify the code as follows:\\n\\nREGISTER_OP(\\\"QuantizeAndDequantizeV3\\\")\\n    .Input(\\\"input: T\\\")\\n    .Input(\\\"input_min: float\\\")\\n    .Input(\\\"input_max: float\\\")\\n    .Input(\\\"num_bits: int32\\\")\\n    .Output(\\\"output: T\\\")\\n    .Attr(\\\"T: {float, double}\\\")\\n    .SetShapeFn([](InferenceContext* c) {\\n      ShapeHandle input_shape;\\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &input_shape));\\n      c->set_output(0, input_shape);\\n      return Status::OK();\\n    });\\n\\nThis patch adds a shape validation check to ensure that the input tensor has a rank of 0, i.e., it is a scalar input. If the input tensor is not a scalar, an error will be returned. This helps to prevent potential security vulnerabilities that may arise from non-scalar input tensors.\\n\\nNote: This patch assumes that the vulnerability exists in the REGISTER_OP macro code snippet provided.\"}"
    ],
    "Actual Clean Code": [
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV3\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .HostMemory(\"num_bits\")                          \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV3Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4Grad\")                  \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV4GradientOp<GpuDevice, T>);    \\",
        "  REGISTER_KERNEL_BUILDER(                                                     \\",
        "      Name(\"QuantizeAndDequantize\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\",
        "      QuantizeAndDequantizeOp<GpuDevice, T>);",
        "TF_CALL_float(REGISTER_GPU_KERNEL);",
        "TF_CALL_double(REGISTER_GPU_KERNEL);",
        "#undef REGISTER_GPU_KERNEL",
        "#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3f9cb38ecfe5a8a703f2c4a8fead434ef291713",
    "API name": "tf.raw_ops.QuantizeAndDequantizeV3"
},
