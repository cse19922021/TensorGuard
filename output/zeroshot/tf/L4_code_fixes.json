{
    "Patch Formated": [
        "{\"Patch\": \"    }\\",
        "    \\",
        "    OP_REQUIRES(\\",
        "        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\\",
        "        errors::InvalidArgument(\"Pooling ratio must have a value of 1 for the batch or channel dimension.\"));\\",
        "    \\",
        "    OP_REQUIRES_OK(context, context->GetAttr(\\\"deterministic\\\", &deterministic_));\\",
        "    OP_REQUIRES_OK(context, context->GetAttr(\\\"seed\\\", &seed_));\"}"
    ],
    "Actual Clean Code": [
        "    }",
        "    OP_REQUIRES(",
        "        context, pooling_ratio_[0] == 1 && pooling_ratio_[3] == 1,",
        "        errors::Unimplemented(\"Fractional average pooling is not yet \"",
        "                              \"supported on the batch nor channel dimension.\"));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/ee50d1e00f81f62a4517453f721c634bbb478307",
    "API name": "tensorflow.python.ops.nn_ops.fractional_avg_pool_v2"
},
{
    "Patch Formated": [
        "{\"Patch\":\"        } \\n\\",
        "        DimensionHandle unused;\\n\\",
        "        for (int i = 0; i < c->num_inputs(); i++) {\\n\\",
        "          if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) {\\n\\",
        "            return errors::InvalidArgument(\\\"Size of first dimension must be 1.\\\");\\n\\",
        "          }\\n\\",
        "          TF_RETURN_WITH_CONTEXT_IF_ERROR(c->Merge(c->input(i), cur, &cur),\\n\\",
        "                                          \\\"From merging shape \\\", i,\\n\\",
        "                                          \\\" with other shapes.\\\");\\n\\",
        "        }\\n\\",
        "        c->set_output(0, passed_shape);\"}"
    ],
    "Actual Clean Code": [
        "              \"All input shapes must be fully defined.\");",
        "        }",
        "        if (c->Rank(c->input(i)) < 1) {",
        "          return errors::InvalidArgument(",
        "              \"The rank of all input shapes must be greater than 0, \"",
        "              \"but input \",",
        "              i, \" had rank \", c->Rank(c->input(i)), \".\");",
        "        }",
        "        DimensionHandle unused;",
        "        if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) {",
        "          return errors::InvalidArgument(\"Size of first dimension must be 1.\");",
        "        }"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/da66bc6d5ff466aee084f9e7397980a24890cd15",
    "API name": "tf.raw_ops.ParallelConcat"
},
{
    "Patch Formated": [
        "{\"patches\": [",
        "    {",
        "        \"category\": \"Improper Input Validation\",",
        "        \"description\": \"Validate that all lengths are positive\",",
        "        \"diff\": \"@@ -13,7 +13,7 @@ void RandomShuffleOp::Compute(OpKernelContext* ctx) {\\n   } else if (length != element_dims[0] / input_dims[0]) {\\n       OP_REQUIRES(\\n         ctx, false,\\n-        errors::Unimplemented(\\\"All lengths have to be the same\\\"));\\n+        errors::InvalidArgument(\\\"All lengths have to be positive and the same\\\"));\\n     }\\n \\n     OP_REQUIRES(\\n@@ -16,9 +16,14 @@ void RandomShuffleOp::Compute(OpKernelContext* ctx) {\\n         ctx, element_dims[0] % length == 0,\\n         errors::Unimplemented(\\\"Buffer size has to be a multiple of length\\\"));\\n     std::vector<int64_t> new_dims = {element_dims[0] / length, length};\\n-    for (int i = 1; i < element_dims.size(); i++) {\\n-      new_dims.push_back(element_dims[i]);\\n-    }\\n+    if(input_dims[0] % length != 0) {\\n+        OP_REQUIRES(\\n+            ctx, false,\\n+            errors::InvalidArgument(\\\"Buffer size has to be a multiple of length\\\"));\\n+    }\\n+\\n+    for (int i = 1; i < input_dims.size(); i++) {\\n+        new_dims.push_back(input_dims[i]);\\n+    }\\n     \\n     output->Resize(new_dims);\\n \\n \"",
        "    }",
        "]}",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "                  errors::Unimplemented(\"All lengths have to be the same\"));",
        "    }",
        "    OP_REQUIRES(ctx, length,",
        "                errors::Unimplemented(\"All lengths must be positive\"));",
        "    OP_REQUIRES(",
        "        ctx, element_dims[0] % length == 0,",
        "        errors::Unimplemented(\"Buffer size has to be a multiple of length\"));",
        "    std::vector<int64_t> new_dims = {element_dims[0] / length, length};"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/728113a3be690facad6ce436660a0bc1858017fa",
    "API name": "tf.raw_ops.RandomShuffle"
},
{
    "Patch Formated": [
        "{\"imports\": [], \"code\": \"\\nStatus MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,\\n                             const ShapeHandle& value) {\\n  ShapeHandle keys;\\n  DimensionHandle unused;\\n\\n  // Check if key is scalar\\n  if (TensorShapeUtils::IsScalar(c->input(1))) {\\n    keys = c->input(1);\\n  } else {\\n    TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));\\n  }\\n\\n  // Check if value is scalar\\n  if (TensorShapeUtils::IsScalar(value)) {\\n    unused = c->UnknownDim();\\n  } else {\\n    TF_RETURN_IF_ERROR(\\n        c->Merge(c->Dim(keys, 0), c->Dim(value, 0), &unused));\\n  }\\n  return OkStatus();\\n}\"}"
    ],
    "Actual Clean Code": [
        "      ShapeHandle keys;",
        "      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));",
        "      ShapeHandle values;",
        "      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(2), 1, &values));",
        "      DimensionHandle unused;",
        "      TF_RETURN_IF_ERROR(c->Merge(c->Dim(keys, 0), c->Dim(values, 0), &unused));",
        "      return OkStatus();",
        "    });",
        "",
        "Status MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/980b22536abcbbe1b4a5642fc940af33d8c19b69",
    "API name": "tf.raw_ops.LookupTableImportV2"
},
{
    "Patch Formated": [
        "{\"Patch\": \"// Here, we implement a bitwise hack: we use the hi 16 bits of input for \\n// separate max pooling alongside each of the hi and lo 16 bits of \\n// out_backprop packed into 16 lo bits, which we then glue back together at \\n// the end to get a full 32 bits of gradient.\\n//\\n// This could select the wrong backprop value for two x values that are\\n// equally maximal up to the first 16 bits, in which case we are taking the \\n// latter.\\n//\\n// Note that in principle we could use 32 separate maxpools to recover each \\n// of 32 bits of the gradient while preserving 31 bits of input for the max \\n// pooling criteria; here, we just truncate to the first 16 bits of input.\\n//\\n// Check if sliding window ksize field for dimension is positive\\ntf.assert_positive(ksize, check_name='ksize')\"}"
    ],
    "Actual Clean Code": [
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));",
        "    OP_REQUIRES(ctx, stride_.size() == num_dims(),",
        "                errors::InvalidArgument(\"Sliding window strides field must \"",
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));",
        "",
        "    const TensorShape tensor_in_shape = ctx->InputShape(0);",
        "    const TensorShape tensor_out_shape = ctx->InputShape(1);",
        "    const TensorShape out_backprop_shape = ctx->InputShape(2);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/1295ae4dbb52fe06b19733b0257e2340d7b63b8d",
    "API name": "tf.raw_ops.AvgPoolGrad"
},
{
    "Patch Formated": [
        "{\"Patch\": \"The vulnerability in the QuantizeAndDequantizeV3Op can be fixed by adding input validation to ensure that the 'num_bits' tensor has a rank <= 1 and the number of elements is 1. The updated code fixes this vulnerability:\\n\\nQuantizeAndDequantizeV3Op::Compute(OpKernelContext* ctx) {\\n  ...\\n  const Tensor& num_bits_tensor = ctx->input(2);\\n  OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(num_bits_tensor.shape()) || num_bits_tensor.NumElements() == 1,\\n              errors::InvalidArgument(\\\"'num_bits' must be a scalar or have exactly one element.\\\"));\\n  ...\\n}\" }"
    ],
    "Actual Clean Code": [
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV3\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .HostMemory(\"num_bits\")                          \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV3Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4Grad\")                  \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV4GradientOp<GpuDevice, T>);    \\",
        "  REGISTER_KERNEL_BUILDER(                                                     \\",
        "      Name(\"QuantizeAndDequantize\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\",
        "      QuantizeAndDequantizeOp<GpuDevice, T>);",
        "TF_CALL_float(REGISTER_GPU_KERNEL);",
        "TF_CALL_double(REGISTER_GPU_KERNEL);",
        "#undef REGISTER_GPU_KERNEL",
        "#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3f9cb38ecfe5a8a703f2c4a8fead434ef291713",
    "API name": "tf.raw_ops.QuantizeAndDequantizeV3"
},
