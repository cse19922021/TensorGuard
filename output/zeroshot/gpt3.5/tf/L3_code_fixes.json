[{
    "Patch Formated": [
        "{\"Patch\":\"pooling_ratio_[0] == 1.0 || pooling_ratio_[3] == 1.0\"}"
    ],
    "Actual Clean Code": [
        "    }",
        "    OP_REQUIRES(",
        "        context, pooling_ratio_[0] == 1 && pooling_ratio_[3] == 1,",
        "        errors::Unimplemented(\"Fractional average pooling is not yet \"",
        "                              \"supported on the batch nor channel dimension.\"));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/ee50d1e00f81f62a4517453f721c634bbb478307",
    "API name": "tensorflow.python.ops.nn_ops.fractional_avg_pool_v2"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\":\"        c->set_output(0, passed_shape);\\n\"",
        "}"
    ],
    "Actual Clean Code": [
        "              \"All input shapes must be fully defined.\");",
        "        }",
        "        if (c->Rank(c->input(i)) < 1) {",
        "          return errors::InvalidArgument(",
        "              \"The rank of all input shapes must be greater than 0, \"",
        "              \"but input \",",
        "              i, \" had rank \", c->Rank(c->input(i)), \".\");",
        "        }",
        "        DimensionHandle unused;",
        "        if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) {",
        "          return errors::InvalidArgument(\"Size of first dimension must be 1.\");",
        "        }"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/da66bc6d5ff466aee084f9e7397980a24890cd15",
    "API name": "tf.raw_ops.ParallelConcat"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"OP_REQUIRES(ctx, length > 0, errors::InvalidArgument(\\\"Length value must be positive\\\"));\"",
        "}"
    ],
    "Actual Clean Code": [
        "                  errors::Unimplemented(\"All lengths have to be the same\"));",
        "    }",
        "    OP_REQUIRES(ctx, length,",
        "                errors::Unimplemented(\"All lengths must be positive\"));",
        "    OP_REQUIRES(",
        "        ctx, element_dims[0] % length == 0,",
        "        errors::Unimplemented(\"Buffer size has to be a multiple of length\"));",
        "    std::vector<int64_t> new_dims = {element_dims[0] / length, length};"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/728113a3be690facad6ce436660a0bc1858017fa",
    "API name": "tf.raw_ops.RandomShuffle"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"      ShapeHandle keys;\\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &keys));\\n      DimensionHandle unused;\\n      TF_RETURN_IF_ERROR(\\n          c->Merge(c->Dim(keys, 0), c->Dim(c->input(2), 0), &unused));\\n      return OkStatus();\\n    });\\n\\nStatus MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,\\n                             const ShapeHandle& value) {\"",
        "}"
    ],
    "Actual Clean Code": [
        "      ShapeHandle keys;",
        "      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));",
        "      ShapeHandle values;",
        "      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(2), 1, &values));",
        "      DimensionHandle unused;",
        "      TF_RETURN_IF_ERROR(c->Merge(c->Dim(keys, 0), c->Dim(values, 0), &unused));",
        "      return OkStatus();",
        "    });",
        "",
        "Status MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/980b22536abcbbe1b4a5642fc940af33d8c19b69",
    "API name": "tf.raw_ops.LookupTableImportV2"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"// Here, we implement a bitwise hack: we use the hi 16 bits of input for\\n// separate max pooling alongside each of the hi and lo 16 bits of\\n// out_backprop packed into 16 lo bits, which we then glue back together at\\n// the end to get a full 32 bits of gradient.\\n//\\n// This could select the wrong backprop value for two x values that are\\n// equally maximal up to the first 16 bits, in which case we are taking the\\n// latter.\\n//\\n// Note that in principle we could use 32 separate maxpools to recover each\\n// of 32 bits of the gradient while preserving 31 bits of input for the max\\n// pooling criteria; here, we just truncate to the first 16 bits of input.\"",
        "}"
    ],
    "Actual Clean Code": [
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));",
        "    OP_REQUIRES(ctx, stride_.size() == num_dims(),",
        "                errors::InvalidArgument(\"Sliding window strides field must \"",
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));",
        "",
        "    const TensorShape tensor_in_shape = ctx->InputShape(0);",
        "    const TensorShape tensor_out_shape = ctx->InputShape(1);",
        "    const TensorShape out_backprop_shape = ctx->InputShape(2);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/1295ae4dbb52fe06b19733b0257e2340d7b63b8d",
    "API name": "tf.raw_ops.AvgPoolGrad"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"#undef REGISTER_GPU_KERNEL\"",
        "}"
    ],
    "Actual Clean Code": [
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV3\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .HostMemory(\"num_bits\")                          \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV3Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4Grad\")                  \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV4GradientOp<GpuDevice, T>);    \\",
        "  REGISTER_KERNEL_BUILDER(                                                     \\",
        "      Name(\"QuantizeAndDequantize\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\",
        "      QuantizeAndDequantizeOp<GpuDevice, T>);",
        "TF_CALL_float(REGISTER_GPU_KERNEL);",
        "TF_CALL_double(REGISTER_GPU_KERNEL);",
        "#undef REGISTER_GPU_KERNEL",
        "#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3f9cb38ecfe5a8a703f2c4a8fead434ef291713",
    "API name": "tf.raw_ops.QuantizeAndDequantizeV3"
},
{
    "Patch Formated": [
        "{\"Patch\":\"OP_REQUIRES(context, max.dim_size(0) == depth,\\n                        InvalidArgumentError(\\\"max has incorrect size, expected \\\", depth,\\n                                            \\\" was \\\", max.dim_size(0)));\\n\\n    Tensor* grad_wrt_input;\\n    OP_REQUIRES_OK(context,\\n                   context->allocate_output(0, input.shape(), &grad_wrt_input));\\n\\n    TensorShape min_max_shape({input.dim_size(input.dims() - 1)});\\n    Tensor* grad_wrt_min;\\n    OP_REQUIRES_OK(context,\\n                   context->allocate_output(1, min_max_shape, &grad_wrt_min));\\n\\n    Tensor* grad_wrt_max;\\n    OP_REQUIRES_OK(context,\\n                   context->allocate_output(2, min_max_shape, &grad_wrt_max));\"}"
    ],
    "Actual Clean Code": [
        "    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.",
        "    const Tensor& min = context->input(2);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(min.shape()),",
        "        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "    const Tensor& max = context->input(3);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(max.shape()),",
        "        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));",
        "    OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3cf67ac5705f4f04721d15e485e192bb319feed",
    "API name": "tf.quantization.fake_quant_with_min_max_vars_gradient"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\":\"    const int64_t samples_per_alpha = alpha_t.NumElements();\\n\\n    TensorShape samples_shape(alpha_t.shape());\\n\\n    // Allocate output samples.\\n    Tensor* samples_t = nullptr;\\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, samples_shape, &samples_t));\"",
        "}"
    ],
    "Actual Clean Code": [
        "    const int64_t samples_per_alpha = samples_shape.num_elements();",
        "",
        "    OP_REQUIRES_OK(ctx, samples_shape.AppendShapeWithStatus(alpha_t.shape()));",
        "    // Allocate output samples.",
        "    Tensor* samples_t = nullptr;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, samples_shape, &samples_t));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/552bfced6ce4809db5f3ca305f60ff80dd40c5a3",
    "API name": "tf.random.gamma"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"const auto& input = context->input(2);\\nif (input.shape().num_elements() > 1) { \\n    const int64_t batch_key = input.scalar<int64_t>()();\\n    const Tensor& batch_index_t = context->input(1);\\n    const bool nonempty_input = batch_index_t.dim_size(0) > 0;\\n\\n    // If we have a non-empty tensor, slice it up.\\n    // (It is important to do this outside of the critical section below.)\\n    // The following variables are populated iff 'nonempty_input==true'.\\n    std::vector<int64_t> sizes;\\n    std::vector<int64_t> batch_keys;\\n    std::vector<Tensor> split_inputs;\\n    if (nonempty_input) {\\n        auto batch_indices =\"",
        "}"
    ],
    "Actual Clean Code": [
        "    }",
        "",
        "    if (!TensorShapeUtils::IsScalar(context->input(2).shape())) {",
        "      return errors::InvalidArgument(",
        "          \"Input id should be scalar; \"",
        "          \"Got: \",",
        "          context->input(2).DebugString(), \".\");",
        "    }",
        "    const int64_t batch_key = context->input(2).scalar<int64_t>()();",
        "    const bool nonempty_input = batch_index_t.dim_size(0) > 0;",
        "",
        "    // If we have a non-empty tensor, slice it up."
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/4419d10d576adefa36b0e0a9425d2569f7c0189f",
    "API name": "tf.raw_ops.Unbatch"
},
{
    "Patch Formated": [
        "{",
        "\"patch\": \"for (int64_t b = 0; b < batch_size; ++b) {\\n      const int64_t num_boxes = boxes.dim_size(1);\\n      const auto tboxes = boxes.tensor<T, 3>();\\n      for (int64_t bb = 0; bb < num_boxes; ++bb) {\\n        int64_t color_index = bb % color_table.size();\\n        const float min_box_row =\\n            static_cast<float>(tboxes(b, bb, 0)) * (height - 1);\"",
        "}"
    ],
    "Actual Clean Code": [
        "    for (int64_t b = 0; b < batch_size; ++b) {",
        "      const int64_t num_boxes = boxes.dim_size(1);",
        "      const auto tboxes = boxes.tensor<float, 3>();",
        "      for (int64_t bb = 0; bb < num_boxes; ++bb) {",
        "        int64_t color_index = bb % color_table.size();",
        "        const int64_t min_box_row =",
        "            static_cast<float>(tboxes(b, bb, 0)) * (height - 1);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/da0d65cdc1270038e72157ba35bf74b85d9bda11",
    "API name": "tf.raw_ops.DrawBoundingBoxes"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"void LinearAlgebraOp<InputScalar, OutputScalar>::ComputeTensorSlice(\\n    OpKernelContext* context, int64_t matrix_index, const TensorInputs& inputs,\\n    const TensorShapes& input_matrix_shapes, const TensorOutputs& outputs,\\n    const TensorShapes& output_matrix_shapes) {\\n  InputConstMatrixMaps matrix_inputs;\\n  for (size_t i = 0; i < inputs.size(); ++i) {\\n    if (Tout == nullptr) {\\n      continue;\\n    }\\n    // TODO(kalakris): Handle alignment if possible. Eigen::Map is\\n    // unaligned by default.\\n    matrix_inputs.emplace_back(\\n        inputs[i]->flat<InputScalar>().data() +\\n            matrix_index * input_matrix_shapes[i].num_elements(),\\n        matrix_index * input_matrix_shapes[i].num_elements());\\n  }\\n}\"",
        "}"
    ],
    "Actual Clean Code": [
        "                                  output_idx, output_tensor_shape, &out));",
        "    }",
        "    OP_REQUIRES(",
        "        context, out->dtype() == DataTypeToEnum<OutputScalar>::v(),",
        "        errors::InvalidArgument(\"Invalid output dtype \", out->dtype(), \" vs \",",
        "                                DataTypeToEnum<OutputScalar>::v()));",
        "",
        "    outputs->emplace_back(out);",
        "  }",
        "}",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/aed36912609fc07229b4d0a7b44f3f48efc00fd0",
    "API name": "tf.raw_ops.Eig"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"if (pad_left == pad_right && pad_top == pad_bottom) {\\n  if (LaunchXsmmBackwardInputConvolution<Device, T>()(\\n          context, context->eigen_device<Device>(),\\n          in_backprop->tensor<T, 4>(), filter.tensor<T, 4>(),\\n          out_backprop.tensor<T, 4>(),\\n          strides_)) {\\n    allocator = xnn_resource_manager_get_aligned_allocator(allocator, sizeof(T));\\n    return xnn_status_unsupported_parameter;\\n  }\\n}\"",
        "}"
    ],
    "Actual Clean Code": [
        "    }",
        "",
        "    // If shapes are valid but `out_backprop` is empty, in_backprop should be",
        "    // set to all zeros.  Otherwise, cudnn/dnnl fail with an empty input.",
        "    if (out_backprop.NumElements() == 0) {",
        "      functor::SetZeroFunctor<Device, T> set_zero;",
        "      set_zero(context->eigen_device<Device>(),",
        "               in_backprop->template flat<T>());",
        "      return;",
        "    }",
        "",
        "// TODO(ezhulenev): Remove custom kernel and move XSMM support to",
        "// LaunchConv2DBackpropInputOp functor.",
        "#if defined TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS && \\",
        "    defined TENSORFLOW_USE_LIBXSMM_BACKWARD_CONVOLUTIONS"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/27a65a43cf763897fecfa5cdb5cc653fc5dd0346",
    "API name": "tf.raw_ops.Conv2DBackpropInput"
},
{
    "Patch Formated": [
        "{\"Patch\":\"if (TensorShapeUtils::IsScalar(t.shape())) {\\n    return PartialTensorShape::MakePartialShape(t.flat<DT_INT32>().data(), 1, out);\\n  }\\n\\n  if (TensorShapeUtils::IsScalar(t.shape())) {\\n    return PartialTensorShape::MakePartialShape(t.flat<DT_INT64>().data(), 1, out);\\n  }\"}"
    ],
    "Actual Clean Code": [
        "        \"The only valid scalar shape tensor is the fully unknown shape \"",
        "        \"specified as -1.\");",
        "  } else if (t.shape().dims() != 1) {",
        "    return errors::InvalidArgument(\"Shape must be at most rank 1 but is rank \",",
        "                                   t.shape().dims());",
        "  }",
        "  if (t.dtype() == DT_INT32) {",
        "    return PartialTensorShape::MakePartialShape(t.vec<int32>().data(),",
        "                                                t.NumElements(), out);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/c8ba76d48567aed347508e0552a257641931024d",
    "API name": "tf.raw_ops.EmptyTensorList"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"    if (!sep_t->scalar<tstring>().defined()) return errors::InvalidArgument(\\\"Input separator must be a scalar.\\\");\\n    const tstring separator = sep_t->scalar<tstring>()();\\n\\n    std::vector<std::unique_ptr<ColumnInterface<tstring>>> columns =\\n        GenerateColumnsFromInput<tstring>(indices_list_in, values_list_in,\\n                                          shapes_list_in, dense_list_in);\\n    Tensor* indices_out;\\n    Tensor* values_out;\\n    Tensor* shape_out;\\n    const int64_t batch_size =\\n        CalculateBatchSize(shapes_list_in, dense_list_in);\"",
        "}"
    ],
    "Actual Clean Code": [
        "    const Tensor* sep_t;",
        "    OP_REQUIRES_OK(context, context->input(\"sep\", &sep_t));",
        "    OP_REQUIRES(context, TensorShapeUtils::IsScalar(sep_t->shape()),",
        "                errors::InvalidArgument(\"Input separator should be a scalar. \"",
        "                                        \"Received: \",",
        "                                        sep_t->DebugString()));",
        "    const tstring separator = sep_t->scalar<tstring>()();",
        "",
        "    std::vector<std::unique_ptr<ColumnInterface<tstring>>> columns =",
        "        GenerateColumnsFromInput<tstring>(indices_list_in, values_list_in,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/83dcb4dbfa094e33db084e97c4d0531a559e0ebf",
    "API name": "tf.sparse.cross"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS\\n    if (params_.padding != EXPLICIT &&\\n        dimensions.in_depth != 0 && dimensions.out_depth != 0 &&\\n        LaunchXsmmConvOp<Device, T>::Run(\\n            context, input, filter, dimensions.batch, dimensions.input_rows,\\n            dimensions.input_cols, dimensions.in_depth, dimensions.filter_rows,\\n            dimensions.filter_cols, dimensions.pad_rows_before,\\n            dimensions.pad_cols_before, dimensions.out_rows,\\n            dimensions.out_cols, dimensions.out_depth, dimensions.dilation_rows,\\n            dimensions.dilation_cols, dimensions.stride_rows,\\n            dimensions.stride_cols, output, params_.data_format)) {\\n      return;\\n    }\\n#endif\"",
        "}"
    ],
    "Actual Clean Code": [
        "    }",
        "",
        "    // If the input is empty, result can only be due to padding.",
        "    if (input.NumElements() == 0) {",
        "      // Zero-out output and return.",
        "      functor::SetZeroFunctor<Device, T>()(context->eigen_device<Device>(),",
        "                                           output->template flat<T>());",
        "",
        "      return;",
        "    }",
        "",
        "#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS",
        "    if (params_.padding != EXPLICIT &&",
        "        LaunchXsmmConvOp<Device, T>::Run(",
        "            context, input, filter, dimensions.batch, dimensions.input_rows,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/611d80db29dd7b0cfb755772c69d60ae5bca05f9",
    "API name": "tf.raw_ops.Conv2D"
},
{
    "Patch Formated": [
        "{\"Patch\":\"    if (!has_sample_rate_attr_) {\\n        const Tensor& sample_rate_tensor = c->input(2);\\n        OP_REQUIRES(c, sample_rate_tensor.NumElements() == 1, errors::InvalidArgument(\\\"sample_rate must have exactly one element\\\"));\\n        sample_rate = sample_rate_tensor.scalar<float>()();\\n    } else { \\n        sample_rate = 0.0f; \\n    }\\n    OP_REQUIRES(c, sample_rate > 0.0f, errors::InvalidArgument(\\\"sample_rate must be > 0\\\"));\\n\\n    const int batch_size = tensor.dim_size(0);\\n    const int64_t length_frames = tensor.dim_size(1);\\n    const int64_t num_channels =\\n        tensor.dims() == 2 ? 1 : tensor.dim_size(tensor.dims() - 1);\" }"
    ],
    "Actual Clean Code": [
        "    if (!has_sample_rate_attr_) {",
        "      const Tensor& sample_rate_tensor = c->input(2);",
        "      OP_REQUIRES(c,",
        "                  sample_rate_tensor.IsAligned() &&",
        "                      sample_rate_tensor.NumElements() == 1,",
        "                  errors::InvalidArgument(",
        "                      \"sample_rate must be rank-0 or contain a single value\"));",
        "      sample_rate = sample_rate_tensor.scalar<float>()();",
        "    }",
        "    OP_REQUIRES(c, sample_rate > 0.0f,",
        "                errors::InvalidArgument(\"sample_rate must be > 0\"));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bf6b45244992e2ee543c258e519489659c99fb7f",
    "API name": "tf.raw_ops.AudioSummaryV2"
},
{
    "Patch Formated": [
        "{\"Patch\": \"output_shape.set_dim(0, output_shape.dim_size(0) * col_params_->group.group_size);\"}"
    ],
    "Actual Clean Code": [
        "                        DoneCallback done) override {",
        "    auto output_shape = c->input(0).shape();",
        "    OP_REQUIRES_ASYNC(c, output_shape.dims() > 0,",
        "                      errors::InvalidArgument(\"input should have rank > 0, \",",
        "                                              \"recieved \", output_shape.dims()),",
        "                      done);",
        "    output_shape.set_dim(",
        "        0, output_shape.dim_size(0) * col_params_->group.group_size);",
        "    col_params_->instance.shape = output_shape;",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/c1f491817dec39a26be3c574e86a88c30f3c4770",
    "API name": "tf.raw_ops.CollectiveGather"
},
{
    "Patch Formated": [
        "{",
        "   \"Patch\":\"// Assume row-major order.\\n  TensorShape shape;\\n  TF_RETURN_IF_ERROR(TensorShape::BuildTensorShape(\\n      ctx->input(base_index + 2).vec<int64_t>(), &shape));\\n  if (shape.dims() < 1) return errors::InvalidArgument(\\\"set_shape must be a 1D tensor\\\");\\n  std::vector<int64_t> order(shape.dims());\\n  std::iota(order.begin(), order.end(), 0);\\n\\n  Status status = sparse::SparseTensor::Create(\\n      ctx->input(base_index), ctx->input(base_index + 1), shape, order, tensor);\\n\\n  if (!validate_indices || !status.ok()) return status;\"",
        "}"
    ],
    "Actual Clean Code": [
        "  // Assume row-major order.",
        "  TensorShape shape;",
        "  const Tensor& shape_tensor = ctx->input(base_index + 2);",
        "  if (shape_tensor.dims() != 1) {",
        "    return errors::InvalidArgument(\"Shape must be a 1D tensor.\");",
        "  }",
        "  TF_RETURN_IF_ERROR(",
        "      TensorShape::BuildTensorShape(shape_tensor.vec<int64_t>(), &shape));",
        "  CheckRankAtLeast2(ctx, shape);",
        "  std::vector<int64_t> order(shape.dims());",
        "  std::iota(order.begin(), order.end(), 0);",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/cf70b79d2662c0d3c6af74583641e345fc939467",
    "API name": "tf.raw_ops.SetSize"
},
{
    "Patch Formated": [
        "{\"Patch\": \"    OP_REQUIRES_OK(c, c->allocate_output(0, {}, &output_tensor, attr));\\n    PartialTensorShape element_shape;\\n    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(1), &element_shape));\\n    TensorList output_list;\\n    const Tensor& t = c->input(0);\\n    output_list.element_dtype = t.dtype();\\n    OP_REQUIRES(c, TensorShapeUtils::IsVectorOrHigher(t.shape()),\\n                errors::InvalidArgument(\\n                    \\\"Tensor must be at least a vector, but saw shape: \\\",\\n                    t.shape().DebugString()));\\n    TensorShape output_shape = element_shape.IsCompatibleWith(t.shape()) ?\\n                              element_shape.with_rank(t.shape().dims()) : TensorShape();\"}"
    ],
    "Actual Clean Code": [
        "    OP_REQUIRES_OK(c, c->allocate_output(0, {}, &output_tensor, attr));",
        "    PartialTensorShape element_shape;",
        "    OP_REQUIRES(",
        "        c, !TensorShapeUtils::IsMatrixOrHigher(c->input(1).shape()),",
        "        errors::InvalidArgument(",
        "            \"TensorListFromTensor: element_shape must be at most rank 1 but \",",
        "            \"has the shape of \", c->input(1).shape().DebugString()));",
        "    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(1), &element_shape));",
        "    TensorList output_list;",
        "    const Tensor& t = c->input(0);",
        "    output_list.element_dtype = t.dtype();"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/3db59a042a38f4338aa207922fa2f476e000a6ee",
    "API name": "tf.raw_ops.TensorListFromTensor"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\":\"Tensor indices = c->input(1);\\nPartialTensorShape element_shape;\\nOP_REQUIRES_OK(c, PartialTensorShape::MakePartialShape(c->input(2), &element_shape));\\nint num_elements = c->num_inputs() >= 4 ? c->input(3).scalar<int>()() : -1;\\nOP_REQUIRES(c, num_elements >= -1,\\nerrors::InvalidArgument(\\n\\\"TensorListScatter expects num_elements >= -1, found: \\\",\\nnum_elements));\\nTensorList output_list;\"",
        "}"
    ],
    "Actual Clean Code": [
        "    Tensor indices = c->input(1);",
        "    PartialTensorShape element_shape;",
        "    OP_REQUIRES(",
        "        c, !TensorShapeUtils::IsMatrixOrHigher(c->input(2).shape()),",
        "        errors::InvalidArgument(",
        "            \"TensorListScatter: element_shape must be at most rank 1 but has \",",
        "            \"the shape of \", c->input(2).shape().DebugString()));",
        "    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(2), &element_shape));",
        "    // TensorListScatterV2 passes the num_elements input, TensorListScatter does",
        "    // not.",
        "    int num_elements = c->num_inputs() >= 4 ? c->input(3).scalar<int>()() : -1;"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bb03fdf4aae944ab2e4b35c7daa051068a8b7f61",
    "API name": "tf.raw_ops.TensorListScatter"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        "",
        "    OP_REQUIRES_OK(context,",
        "                   context->allocate_output(0, input.shape(), &grad_wrt_input));",
        "",
        "    TensorShape min_max_shape({input.dim_size(input.dims() - 1)});",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "",
        "    Tensor* grad_wrt_min;",
        "    OP_REQUIRES_OK(context,",
        "                   context->allocate_output(1, min_max_shape, &grad_wrt_min));",
        "",
        "    Tensor* grad_wrt_max;",
        "    OP_REQUIRES_OK(context,",
        "                   context->allocate_output(2, min_max_shape, &grad_wrt_max));\"",
        "}"
    ],
    "Actual Clean Code": [
        "    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.",
        "    const Tensor& min = context->input(2);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(min.shape()),",
        "        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "    const Tensor& max = context->input(3);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(max.shape()),",
        "        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));",
        "    OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3cf67ac5705f4f04721d15e485e192bb319feed",
    "API name": "tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\":\"                        params.out_width = std::min(params.out_width, input_width);\"",
        "}"
    ],
    "Actual Clean Code": [
        "                        params.out_width, params.depth);",
        "",
        "    // Degenerate pooling output should return an empty tensor.",
        "    if (out_shape.num_elements() == 0) {",
        "      Tensor* output = nullptr;",
        "      OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));",
        "      return;",
        "    }",
        "",
        "    // Assuming qint8 <--> NCHW_VECT_C (int8x4) here.",
        "    constexpr bool is_int8x4 = std::is_same<T, qint8>::value;",
        "    OP_REQUIRES(context, (is_int8x4 == (data_format_ == FORMAT_NCHW_VECT_C)),",
        "                errors::InvalidArgument("
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/32d7bd3defd134f21a4e344c8dfd40099aaf6b18",
    "API name": "tf.raw_ops.MaxPool"
},
{
    "Patch Formated": [
        "{\"Patch\":\"if (a.cols() == 0 || a.rows() == 0) {\\n      LOG(ERROR) << \\\"Empty input tensor.\\\";\\n      return;\\n    }\"}"
    ],
    "Actual Clean Code": [
        "                         done);",
        "",
        "    // If there are zero batches, we are done.",
        "    if (shapeRaw.num_elements() == 0) {",
        "      done();",
        "      return;",
        "    }",
        "",
        "    if (n == 0 || m == 0) {",
        "      if (n == m || !compute_uv_ || !full_matrices_) {",
        "        // S, U, and V are all empty. Nothing to do.",
        "        done();"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/c55b476aa0e0bd4ee99d0f3ad18d9d706cd1260a",
    "API name": "tf.linalg.matrix_rank"
},
{
    "Patch Formated": [
        "{\"Patch\": \"ERROR: Vulnerability category 'Improper Input Validation' is not supported by the bug fix model.\"}"
    ],
    "Actual Clean Code": [
        "                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",",
        "                                        size_t.dims()));",
        "    OP_REQUIRES(ctx,",
        "                weights.shape() == data.shape() || weights.NumElements() == 0,",
        "                errors::InvalidArgument(",
        "                    \"`weights` must be the same shape as `arr` or a length-0 \"",
        "                    \"`Tensor`, in which case it acts as all weights equal to \"",
        "                    \"1. Received \",",
        "                    weights.shape().DebugString()));",
        "",
        "    Tidx size = size_t.scalar<Tidx>()();",
        "    OP_REQUIRES(",
        "        ctx, size >= 0,",
        "        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bf4c14353c2328636a18bfad1e151052c81d5f43",
    "API name": "tf.raw_ops.DenseBincount"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"int batch_idx = 0;\\n\\nOP_REQUIRES(ctx, splits.NumElements() > 0,\\n            errors::InvalidArgument(\\\"Splits must not be empty.\\\"));\\n\\nOP_REQUIRES(ctx, splits(0) == 0,\\n            errors::InvalidArgument(\\\"Splits must start with 0, not with \\\",\\n                                    splits(0)));\\n\\nOP_REQUIRES(ctx, splits(splits.NumElements() - 1) == num_values,\\n            errors::InvalidArgument(\\n                \\\"Splits must end with the number of values, got \\\",\\n                splits(splits.NumElements() - 1)));\\n\"",
        "}"
    ],
    "Actual Clean Code": [
        "    int batch_idx = 0;",
        "",
        "    OP_REQUIRES(ctx, splits.size() > 0,",
        "                errors::InvalidArgument(\"Splits must be non-empty\"));",
        "",
        "    OP_REQUIRES(ctx, splits(0) == 0,",
        "                errors::InvalidArgument(\"Splits must start with 0, not with \",",
        "                                        splits(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/7a4591fd4f065f4fa903593bc39b2f79530a74b8",
    "API name": "tf.raw_ops.RaggedBincount"
},
{
    "Patch Formated": [
        "{\"Patch\": \"Modify the code to check if the input tensors have the correct shape before proceeding with the computation.\\n\\nif (input_grads.dim_size(0) == batch && input_grads.dim_size(1) == rows &&\\n    input_grads.dim_size(2) == cols && input_grads.dim_size(3) == depth &&\\n    in_image.dim_size(0) == batch && in_image.dim_size(1) == rows &&\\n    in_image.dim_size(2) == cols && in_image.dim_size(3) == depth &&\\n    out_image.dim_size(0) == batch && out_image.dim_size(1) == rows &&\\n    out_image.dim_size(2) == cols && out_image.dim_size(3) == depth) {\\n  // Perform computation\\n  ....\\n} else {\\n  // Raise an error\\n  tf::errors::InvalidArgument(\\\"input_grads, in_image, and out_image should have the same shape\\\");\\n}\\n\\nTensor* output = nullptr;\"}"
    ],
    "Actual Clean Code": [
        "            in_image.dim_size(2) == cols && in_image.dim_size(3) == depth &&",
        "            out_image.dim_size(0) == batch && out_image.dim_size(1) == rows &&",
        "            out_image.dim_size(2) == cols && out_image.dim_size(3) == depth &&",
        "            out_image.dims() == 4,",
        "        errors::InvalidArgument(",
        "            \"input_grads, input_image, and out_image should have the same \"",
        "            \"shape\"));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bd90b3efab4ec958b228cd7cfe9125be1c0cf255",
    "API name": "tf.raw_ops.LRNGrad"
},
{
    "Patch Formated": [
        "{\"Patch\":\"// Convert shape to int32\\nTensorShapeProto shape_proto = shape_tensor.shape();\\nEigen::Tensor<int32_t, 1, Eigen::RowMajor> shape_vec(shape_proto.dim_size());\\nfor (int i = 0; i < shape_proto.dim_size(); i++)\\n{\\n    shape_vec(i) = static_cast<int32_t>(shape_proto.dim(i).size());\\n}\\nTensorShape tensor_shape(shape_vec);\\n\\nTensor* samples_tensor;\\nOP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));\"}"
    ],
    "Actual Clean Code": [
        "                errors::InvalidArgument(\"Shape tensor must not be empty, got \",",
        "                                        shape_tensor.DebugString()));",
        "    TensorShape tensor_shape;",
        "    OP_REQUIRES_OK(ctx, tensor::MakeShape(shape_tensor, &tensor_shape));",
        "",
        "    int32_t num_batches = tensor_shape.dim_size(0);",
        "    int32_t samples_per_batch = 1;",
        "    const int32_t num_dims = tensor_shape.dims();",
        "    for (int32_t i = 1; i < num_dims; i++) {",
        "      samples_per_batch *= tensor_shape.dim_size(i);",
        "    }",
        "    const int32_t num_elements = num_batches * samples_per_batch;",
        "",
        "    // Allocate the output before fudging num_batches and samples_per_batch.",
        "    Tensor* samples_tensor;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));",
        "",
        "    // Parameters must be 0-d or 1-d."
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/72180be03447a10810edca700cbc9af690dfeb51",
    "API name": "tf.raw_ops.ParameterizedTruncatedNormal"
},
{
    "Patch Formated": [
        "{\"Patch\": \"size_t size_bound = ss->ByteSize() + kTensorProtoHeaderBytes +\\n                      (num_elements * MaxBytesPerElement(DT_INT32));\\n  for (int64_t i = 0; i < num_elements; ++i) {\\n    size_bound += data[i].size();\\n    if (size_bound > kMaxMessageBytes) {\\n        return errors::InvalidArgument(\\n            \\\"Tensor slice is too large to serialize (conservative estimate: \\\",\\n            size_bound, \\\") for slice \",i,\\\".\\\");\\n    }\\n  }\\n\"}"
    ],
    "Actual Clean Code": [
        "    case DT_BFLOAT16:",
        "    default:",
        "      return 0;",
        "  }",
        "}",
        "",
        "template <>",
        "Status TensorSliceWriter::SaveData(const tstring* data, int64_t num_elements,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/5dd7b86b84a864b834c6fa3d7f9f51c87efa99d4",
    "API name": "tf.raw_ops.Save"
},
{
    "Patch Formated": [
        "{\"Patch\": \"            OP_REQUIRES(\\n                ctx,\\n                batch >= 0 && batch < out.dimension(0),\\n                errors::InvalidArgument(\\\"Index out of bound. `batch` (\\\" + std::to_string(batch) + \\\") must be less than the dimension size (\\\" + std::to_string(out.dimension(0)) + \\\").\\\"));\\n            OP_REQUIRES(\\n                ctx,\\n                bin >= 0 && bin < out.dimension(1),\\n                errors::InvalidArgument(\\\"Index out ouf bound. `bin` (\\\" + std::to_string(bin) + \\\") must be less then the dimension size (\\\" + std::to_string(out.dimension(1)) + \\\").\\\"));\"}"
    ],
    "Actual Clean Code": [
        "      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {",
        "        const int64_t batch = indices_mat(i, 0);",
        "        const Tidx bin = values_flat(i);",
        "        OP_REQUIRES(",
        "            ctx, batch < out.dimension(0),",
        "            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,",
        "                                    \") must be less than the dimension size (\","
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/40adbe4dd15b582b0210dfbf40c243a62f5119fa",
    "API name": "tf.raw_ops.SparseBincount"
},
{
    "Patch Formated": [
        "{\"Patch\": \"for (int i = 0; i < ragged_nested_splits_len; i++) {\\n  OP_REQUIRES(context, IsRankOne(&ragged_nested_splits_in[i]), errors::InvalidArgument(\\\"All ragged_nested_splits[i] must have rank 1.\\\"));\\n  batched_ragged_input.append_splits(ragged_nested_splits_in[i]);\\n}\"}"
    ],
    "Actual Clean Code": [
        "        ragged_nested_splits_len);",
        "    for (int i = 0; i < ragged_nested_splits_len; i++) {",
        "      OP_REQUIRES(context, ragged_nested_splits_in[i].dims() == 1,",
        "                  errors::InvalidArgument(\"Requires nested_row_splits[\", i, \"]\",",
        "                                          \" to be rank 1 but is rank \",",
        "                                          ragged_nested_splits_in[i].dims()));",
        "      batched_ragged_input.append_splits(ragged_nested_splits_in[i]);",
        "    }",
        "",
        "    if (!batched_input_) {"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/88f93dfe691563baa4ae1e80ccde2d5c7a143821",
    "API name": "tf.raw_ops.RaggedTensorToVariant"
},
{
    "Patch Formated": [
        "{\"Patch\":\"CHECK(input_backprop_index >= 0 && input_backprop_index < num_total_inputs) << \\\"Invalid input backprop index: \\\" << input_backprop_index << \\\", \\\" << num_total_inputs;}\"}"
    ],
    "Actual Clean Code": [
        "    for (int index = 0; index < num_total_outputs; ++index) {",
        "      int input_backprop_index = out_arg_max_flat(index);",
        "      OP_REQUIRES(",
        "          context,",
        "          input_backprop_index >= 0 && input_backprop_index < num_total_inputs,",
        "          errors::InvalidArgument(",
        "              \"Invalid input backprop index: \", input_backprop_index, \", \",",
        "              num_total_inputs));",
        "      input_backprop_flat(input_backprop_index) += out_backprop_flat(index);",
        "    }",
        "  }",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/8741e57d163a079db05a7107a7609af70931def4",
    "API name": "tf.raw_ops.FractionalMaxPoolGrad"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\":\"tensorflow/core/kernels/quantized_relu_op.h | 4 ++--\\n tensorflow/core/kernels/quantized_relu_op.cc | 3 ++-\\n tensorflow/core/kernels/quantized_relu_op_gpu.cu.cc | 8 +++++++-\\n 3 files changed, 11 insertions(+), 4 deletions(-)\",",
        "    \"Description\": \"In the Tensorflow core kernel file 'quantized_relu_op.h', I made the following changes:\\n\\nChanged:\\n\\n   1. Changed the signature of the QuantizedReluCPU function to include scalar inputs for min_features and max_features. This ensures that only scalar inputs are allowed for these parameters.\\n\\nIn the Tensorflow core kernel file 'quantized_relu_op.cc', I made the following changes:\\n\\nChanged:\\n\\n   1. The QuantizedReluCPU function signature to include scalar inputs for min_features and max_features.\\n   2. The create kernel function to ensure that only scalar inputs are allowed for min_features and max_features.\\n\\nIn the Tensorflow core kernel file 'quantized_relu_op_gpu.cu.cc', I made the following changes:\\n\\nChanged:\\n\\n   1. The signature of the QuantizedReluGPU function to include scalar inputs for min_features and max_features.\\n   2. The create kernel function to ensure that only scalar inputs are allowed for min_features and max_features.\\n\\nThese changes ensure that only scalar inputs are allowed for min_features and max_features in the QuantizedRelu operation.\"",
        "}"
    ],
    "Actual Clean Code": [
        "  void Compute(OpKernelContext* context) override {",
        "    const Tensor& input = context->input(0);",
        "    const Tensor& min_input_tensor = context->input(1);",
        "    const Tensor& max_input_tensor = context->input(2);",
        "",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),",
        "        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",",
        "                                min_input_tensor.dims()));",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),",
        "        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",",
        "                                max_input_tensor.dims()));",
        "",
        "    const float min_input = min_input_tensor.scalar<float>()();",
        "    const float max_input = max_input_tensor.scalar<float>()();",
        "",
        "    Tensor* output = nullptr;",
        "    OP_REQUIRES_OK(context,",
        "                   context->allocate_output(0, input.shape(), &output));",
        "    const T min_as_quantized = FloatToQuantized<T>(0.0f, min_input, max_input);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/49b3824d83af706df0ad07e4e677d88659756d89",
    "API name": "tf.raw_ops.QuantizedRelu6"
},
{
    "Patch Formated": [
        "{\"Patch\":\"The vulnerability is triggered when QuantizeDownAndShrinkRange is given nonscalar inputs for input_min or input_max. To fix this, we need to ensure that the inputs are scalar values. Here is the fixed code:\\n\\nvoid Compute(OpKernelContext* ctx) override {\\n  const Tensor& input = ctx->input(0);\\n  const float input_min_float = ctx->input(1).scalar<float>()();\\n  const float input_max_float = ctx->input(2).scalar<float>()();\\n  Tensor* output = nullptr;\\n  OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\\n  Tensor* output_min = nullptr;\\n  OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\\n  Tensor* output_max = nullptr;\\n  OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));\\n\\n  // See QuantizationRangeOp as well, which has a copy of this logic.\\n  auto input_array = input.flat<T1>();\\n  const int32_t input_lowest_quantized = static_cast<int32>(Eigen::NumTraits<T1>::lowest());\\n  const int32_t input_highest_quantized = static_cast<int32>(Eigen::NumTraits<T1>::highest());\\n  T1 actual_min_quantized = input_highest_quantized;\\n  T1 actual_max_quantized = input_lowest_quantized;\\n  for (int i = 0; i < input_array.size(); ++i) {\"}"
    ],
    "Actual Clean Code": [
        "  void Compute(OpKernelContext* ctx) override {",
        "    const Tensor& input = ctx->input(0);",
        "    const Tensor& input_min = ctx->input(1);",
        "    const Tensor& input_max = ctx->input(2);",
        "",
        "    OP_REQUIRES(",
        "        ctx, TensorShapeUtils::IsScalar(input_min.shape()),",
        "        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",",
        "                                input_min.dims()));",
        "    OP_REQUIRES(",
        "        ctx, TensorShapeUtils::IsScalar(input_max.shape()),",
        "        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",",
        "                                input_max.dims()));",
        "",
        "    const float input_min_float = input_min.scalar<float>()();",
        "    const float input_max_float = input_max.scalar<float>()();",
        "    Tensor* output = nullptr;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));",
        "    Tensor* output_min = nullptr;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/73ad1815ebcfeb7c051f9c2f7ab5024380ca8613",
    "API name": "tf.raw_ops.QuantizeDownAndShrinkRange"
},
{
    "Patch Formated": [
        "{\"Patch\":\"    // Check that the dimensions of the two matrices are valid.\\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(a.shape()),\\n                errors::InvalidArgument(\\\"In[0] is not a matrix\\\"));\\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(b.shape()),\\n                errors::InvalidArgument(\\\"In[1] is not a matrix\\\"));\"}"
    ],
    "Actual Clean Code": [
        "    const Tensor& a = context->input(0);",
        "    const Tensor& b = context->input(1);",
        "    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(2).shape()),",
        "                errors::InvalidArgument(\"min_a must be a scalar, but got shape\",",
        "                                        context->input(2).shape()));",
        "    const float min_a = context->input(2).flat<float>()(0);",
        "    OP_REQUIRES(context, context->input(3).NumElements() == 1,",
        "                errors::InvalidArgument(\"max_a must be a scalar, but got shape\",",
        "                                        context->input(3).shape()));",
        "    const float max_a = context->input(3).flat<float>()(0);",
        "    OP_REQUIRES(context, context->input(4).NumElements() == 1,",
        "                errors::InvalidArgument(\"min_b must be a scalar, but got shape\",",
        "                                        context->input(4).shape()));",
        "    const float min_b = context->input(4).flat<float>()(0);",
        "    OP_REQUIRES(context, context->input(5).NumElements() == 1,",
        "                errors::InvalidArgument(\"max_b must be a scalar, but got shape\",",
        "                                        context->input(5).shape()));",
        "    const float max_b = context->input(5).flat<float>()(0);",
        "",
        "    // Make sure that we have valid quantization ranges for the input buffers.",
        "    // If the difference between the min and max is negative or zero, it makes"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/aca766ac7693bf29ed0df55ad6bfcc78f35e7f48",
    "API name": "tf.raw_ops.QuantizedMatMul"
},
{
    "Patch Formated": [
        "{\"Patch\":\"    OP_REQUIRES(context, min.dims() == 1, errors::InvalidArgument(\\\"Expected min to be a vector, but it wasn't.  Instead it was \\\", min.dims()));\\n    OP_REQUIRES(context, max.dims() == 1, errors::InvalidArgument(\\\"Expected max to be a vector, but it wasn't.  Instead it was \\\", max.dims()));\\n\"}"
    ],
    "Actual Clean Code": [
        "    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.",
        "    const Tensor& min = context->input(1);",
        "    const Tensor& max = context->input(2);",
        "",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(min.shape()),",
        "        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(max.shape()),",
        "        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));",
        "    OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/785d67a78a1d533759fcd2f5e8d6ef778de849e0",
    "API name": "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"Conv2DBackpropDimensions dims;\\nOP_REQUIRES_OK(context, ConvBackpropComputeDimensionsV2(\\\"Conv2DCustomBackpropInput\\\", /*num_spatial_dims=*/2, input_sizes, filter.shape(), out_backprop.shape(), /*dilations=*/{1, 1, 1, 1}, strides_, padding_, &dims));\"",
        "}"
    ],
    "Actual Clean Code": [
        "    const Tensor& filter = context->input(1);",
        "    const Tensor& out_backprop = context->input(2);",
        "    OP_REQUIRES(",
        "        context, out_backprop.dims() == 4,",
        "        errors::InvalidArgument(\"input_sizes must be 4-dimensional, got: \",",
        "                                out_backprop.dims()));",
        "",
        "    TensorShape input_shape;",
        "    OP_REQUIRES_OK(context,",
        "                   Conv2DBackpropComputeInputShape(input_sizes, filter.shape(),"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/50156d547b9a1da0144d7babe665cf690305b33c",
    "API name": "tf.raw_ops.Conv2DBackpropInput"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"auto shape_vec = tensor_in_shape.vec<int32_t>();\\n    for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {\\n      output_shape.AddDim(shape_vec[i]);\\n    }\\n    if (output_shape.num_elements() == 0) {\\n      Tensor* output = nullptr;\\n      OP_REQUIRES_OK(context,\"",
        "}"
    ],
    "Actual Clean Code": [
        "    auto shape_vec = tensor_in_shape.vec<int32>();",
        "    for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {",
        "      OP_REQUIRES_OK(context, output_shape.AddDimWithStatus(shape_vec(i)));",
        "    }",
        "    if (output_shape.num_elements() == 0) {",
        "      Tensor* output = nullptr;",
        "      OP_REQUIRES_OK(context,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/3a6ac52664c6c095aa2b114e742b0aa17fdce78f",
    "API name": "tf.raw_ops.AvgPoolGrad"
}]