[{
    "Patch Formated": [
        "{\"Patch\":\"OP_REQUIRES(context, !(pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1), errors::Unimplemented(\\\"Fractional average pooling is not yet supported on the batch nor channel dimension.\\\"));\"}"
    ],
    "Actual Clean Code": [
        "    }",
        "    OP_REQUIRES(",
        "        context, pooling_ratio_[0] == 1 && pooling_ratio_[3] == 1,",
        "        errors::Unimplemented(\"Fractional average pooling is not yet \"",
        "                              \"supported on the batch nor channel dimension.\"));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/ee50d1e00f81f62a4517453f721c634bbb478307",
    "API name": "tensorflow.python.ops.nn_ops.fractional_avg_pool_v2"
},
{
    "Patch Formated": [
        "{\"Patch\": \"if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) \\n\"",
        "          \"          return errors::InvalidArgument(\\\"Size of first dimension must be 1.\\\");\\n\"",
        "          \"        \\n\"",
        "          \"        std::string error_message;\\n\"",
        "          \"        if (!c->Merge(c->input(i), cur, &cur).ok()) {\\n\"",
        "          \"          error_message = \\\"From merging shape \\\" + std::to_string(i) + \\\" with other shapes.\\\";\\n\"",
        "          \"          return errors::Aborted(error_message);\\n\"",
        "          \"        }\"}"
    ],
    "Actual Clean Code": [
        "              \"All input shapes must be fully defined.\");",
        "        }",
        "        if (c->Rank(c->input(i)) < 1) {",
        "          return errors::InvalidArgument(",
        "              \"The rank of all input shapes must be greater than 0, \"",
        "              \"but input \",",
        "              i, \" had rank \", c->Rank(c->input(i)), \".\");",
        "        }",
        "        DimensionHandle unused;",
        "        if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) {",
        "          return errors::InvalidArgument(\"Size of first dimension must be 1.\");",
        "        }"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/da66bc6d5ff466aee084f9e7397980a24890cd15",
    "API name": "tf.raw_ops.ParallelConcat"
},
{
    "Patch Formated": [
        "{\"Patch\":\"ERROR: all lengths have to be the same\"}"
    ],
    "Actual Clean Code": [
        "                  errors::Unimplemented(\"All lengths have to be the same\"));",
        "    }",
        "    OP_REQUIRES(ctx, length,",
        "                errors::Unimplemented(\"All lengths must be positive\"));",
        "    OP_REQUIRES(",
        "        ctx, element_dims[0] % length == 0,",
        "        errors::Unimplemented(\"Buffer size has to be a multiple of length\"));",
        "    std::vector<int64_t> new_dims = {element_dims[0] / length, length};"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/728113a3be690facad6ce436660a0bc1858017fa",
    "API name": "tf.raw_ops.RandomShuffle"
},
{
    "Patch Formated": [
        "{\"Patch\":\"ShapeHandle keys;\\nTF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));\\nDimensionHandle unused;\\n\\nif(c->RankKnown(keys) && c->RankKnown(c->input(2))){\\n    if(c->Value(keys,0)!=c->Value(c->input(2),0)){\\n        return errors::InvalidArgument(\"Input shapes must be compatible: \",c->DebugString(keys), \" vs. \",c->DebugString(c->input(2)));}}\\nTF_RETURN_IF_ERROR(\\n    c->Merge(c->Dim(keys, 0), c->Dim(c->input(2), 0), &unused));\\nreturn OkStatus();\\n});\\n\\nStatus MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,\\n                         const ShapeHandle& value) {\"}"
    ],
    "Actual Clean Code": [
        "      ShapeHandle keys;",
        "      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));",
        "      ShapeHandle values;",
        "      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(2), 1, &values));",
        "      DimensionHandle unused;",
        "      TF_RETURN_IF_ERROR(c->Merge(c->Dim(keys, 0), c->Dim(values, 0), &unused));",
        "      return OkStatus();",
        "    });",
        "",
        "Status MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/980b22536abcbbe1b4a5642fc940af33d8c19b69",
    "API name": "tf.raw_ops.LookupTableImportV2"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"Implemented proper input validation to fix the bug.\"",
        "}"
    ],
    "Actual Clean Code": [
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));",
        "    OP_REQUIRES(ctx, stride_.size() == num_dims(),",
        "                errors::InvalidArgument(\"Sliding window strides field must \"",
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));",
        "",
        "    const TensorShape tensor_in_shape = ctx->InputShape(0);",
        "    const TensorShape tensor_out_shape = ctx->InputShape(1);",
        "    const TensorShape out_backprop_shape = ctx->InputShape(2);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/1295ae4dbb52fe06b19733b0257e2340d7b63b8d",
    "API name": "tf.raw_ops.AvgPoolGrad"
},
{
    "Patch Formated": [
        "{\"Patch\":\"@@REGISTER_KERNEL_BUILDER(Name(\\\"QuantizeAndDequantizeV3\\\")                      \\n                              .Device(DEVICE_GPU)                              \\n                              .HostMemory(\\\"input_min\\\")                         \\n                              .HostMemory(\\\"input_max\\\")                         \\n                              .HostMemory(\\\"num_bits\\\")                          \\n                              .TypeConstraint<T>(\\\"T\\\"),                         \\n                          QuantizeAndDequantizeV3Op<GPUDevice, T>);            \\n  REGISTER_KERNEL_BUILDER(Name(\\\"QuantizeAndDequantizeV4\\\")                      \\n                              .Device(DEVICE_GPU)                              \\n                              .HostMemory(\\\"input_min\\\")                         \\n                              .HostMemory(\\\"input_max\\\")                         \\n                              .TypeConstraint<T>(\\\"T\\\"),                         \\n                          QuantizeAndDequantizeV2Op<GPUDevice, T>);            \\n  REGISTER_KERNEL_BUILDER(Name(\\\"QuantizeAndDequantizeV4Grad\\\")                  \\n                              .Device(DEVICE_GPU)                              \\n                              .HostMemory(\\\"input_min\\\")                         \\n                              .HostMemory(\\\"input_max\\\")                         \\n                              .TypeConstraint<T>(\\\"T\\\"),                         \\n                          QuantizeAndDequantizeV4GradientOp<GPUDevice, T>);    \\n  REGISTER_KERNEL_BUILDER(                                                     \\n      Name(\\\"QuantizeAndDequantize\\\").Device(DEVICE_GPU).TypeConstraint<T>(\\\"T\\\"), \\n      QuantizeAndDequantizeOp<GPUDevice, T>);\" }"
    ],
    "Actual Clean Code": [
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV3\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .HostMemory(\"num_bits\")                          \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV3Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4Grad\")                  \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV4GradientOp<GpuDevice, T>);    \\",
        "  REGISTER_KERNEL_BUILDER(                                                     \\",
        "      Name(\"QuantizeAndDequantize\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\",
        "      QuantizeAndDequantizeOp<GpuDevice, T>);",
        "TF_CALL_float(REGISTER_GPU_KERNEL);",
        "TF_CALL_double(REGISTER_GPU_KERNEL);",
        "#undef REGISTER_GPU_KERNEL",
        "#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3f9cb38ecfe5a8a703f2c4a8fead434ef291713",
    "API name": "tf.raw_ops.QuantizeAndDequantizeV3"
},
{
    "Patch Formated": [
        "{\"Patch\": \"OP_REQUIRES(context, max.dim_size(0) == depth,\\n                InvalidArgument(\\\"max has incorrect size, expected \\\", depth,\\n                                \\\" was \\\", max.dim_size(0)));\\n\\n    Tensor* grad_wrt_input;\\n    OP_REQUIRES_OK(context,\\n                   context->allocate_output(0, input.shape(), &grad_wrt_input));\\n\\n    TensorShape min_max_shape({input.dim_size(input.dims() - 1)});\\n    Tensor* grad_wrt_min;\\n    OP_REQUIRES_OK(context,\\n                   context->allocate_output(1, min_max_shape, &grad_wrt_min));\\n\\n    Tensor* grad_wrt_max;\\n    OP_REQUIRES_OK(context,\\n                   context->allocate_output(2, min_max_shape, &grad_wrt_max));\"}"
    ],
    "Actual Clean Code": [
        "    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.",
        "    const Tensor& min = context->input(2);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(min.shape()),",
        "        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "    const Tensor& max = context->input(3);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(max.shape()),",
        "        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));",
        "    OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3cf67ac5705f4f04721d15e485e192bb319feed",
    "API name": "tf.quantization.fake_quant_with_min_max_vars_gradient"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"const int64_t samples_per_alpha = alpha_t.num_elements();\"",
        "}"
    ],
    "Actual Clean Code": [
        "    const int64_t samples_per_alpha = samples_shape.num_elements();",
        "",
        "    OP_REQUIRES_OK(ctx, samples_shape.AppendShapeWithStatus(alpha_t.shape()));",
        "    // Allocate output samples.",
        "    Tensor* samples_t = nullptr;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, samples_shape, &samples_t));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/552bfced6ce4809db5f3ca305f60ff80dd40c5a3",
    "API name": "tf.random.gamma"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"const bool nonempty_input = batch_index_t.dim_size(0) > 0;\\n\\n    if (nonempty_input) {\\n      auto batch_indices = context->input(2);\\n      OP_REQUIRES_OK(ctx, ctx->ConstantInputTensor(batch_indices, &batch_keys));\\n      OP_REQUIRES_OK(ctx, ctx->ConstantInputTensor(batch_indices, &sizes));\\n      split_inputs.resize(sizes.size());\\n      for (int i = 0; i < sizes.size(); i++) {\\n        split_inputs[i] = batch_index_t.Slice(batch_indices[i], sizes[i]);\\n      }\\n    }\\n\\n    // If we don't have a non-empty tensor, set the sizes and batch_keys vectors to empty\\n    else {\\n      sizes.clear();\\n      batch_keys.clear();\\n    }\"",
        "}"
    ],
    "Actual Clean Code": [
        "    }",
        "",
        "    if (!TensorShapeUtils::IsScalar(context->input(2).shape())) {",
        "      return errors::InvalidArgument(",
        "          \"Input id should be scalar; \"",
        "          \"Got: \",",
        "          context->input(2).DebugString(), \".\");",
        "    }",
        "    const int64_t batch_key = context->input(2).scalar<int64_t>()();",
        "    const bool nonempty_input = batch_index_t.dim_size(0) > 0;",
        "",
        "    // If we have a non-empty tensor, slice it up."
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/4419d10d576adefa36b0e0a9425d2569f7c0189f",
    "API name": "tf.raw_ops.Unbatch"
},
{
    "Patch Formated": [
        "{",
        "  \"Patch\": \"      const auto tboxes = boxes.tensor<float, 3>();\"",
        "}"
    ],
    "Actual Clean Code": [
        "    for (int64_t b = 0; b < batch_size; ++b) {",
        "      const int64_t num_boxes = boxes.dim_size(1);",
        "      const auto tboxes = boxes.tensor<float, 3>();",
        "      for (int64_t bb = 0; bb < num_boxes; ++bb) {",
        "        int64_t color_index = bb % color_table.size();",
        "        const int64_t min_box_row =",
        "            static_cast<float>(tboxes(b, bb, 0)) * (height - 1);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/da0d65cdc1270038e72157ba35bf74b85d9bda11",
    "API name": "tf.raw_ops.DrawBoundingBoxes"
},
{
    "Patch Formated": [
        "{\"Patch\":\"matrix_inputs.emplace_back(inputs[i]->flat<InputScalar>().data() + (matrix_index * input_matrix_shapes[i].num_elements()))\"}"
    ],
    "Actual Clean Code": [
        "                                  output_idx, output_tensor_shape, &out));",
        "    }",
        "    OP_REQUIRES(",
        "        context, out->dtype() == DataTypeToEnum<OutputScalar>::v(),",
        "        errors::InvalidArgument(\"Invalid output dtype \", out->dtype(), \" vs \",",
        "                                DataTypeToEnum<OutputScalar>::v()));",
        "",
        "    outputs->emplace_back(out);",
        "  }",
        "}",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/aed36912609fc07229b4d0a7b44f3f48efc00fd0",
    "API name": "tf.raw_ops.Eig"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"GetWindowedOutputSizeVerbose(dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,\\n            dims.spatial_dims[0].stride, padding_,\\n            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom);\\n    OP_REQUIRES_OK(\\n        context,\\n        GetWindowedOutputSizeVerbose(dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,\\n            dims.spatial_dims[1].stride, padding_,\\n            &dims.spatial_dims[1].output_size, &pad_left, &pad_right));\\n\\n    if (pad_left != pad_right || pad_top != pad_bottom) {\\n      if (!LaunchXsmmBackwardInputConvolution<Device, T>()(\\n              context, context->eigen_device<Device>(),\\n              in_backprop->tensor<T, 4>(), filter.tensor<T, 4>(),\"",
        "}"
    ],
    "Actual Clean Code": [
        "    }",
        "",
        "    // If shapes are valid but `out_backprop` is empty, in_backprop should be",
        "    // set to all zeros.  Otherwise, cudnn/dnnl fail with an empty input.",
        "    if (out_backprop.NumElements() == 0) {",
        "      functor::SetZeroFunctor<Device, T> set_zero;",
        "      set_zero(context->eigen_device<Device>(),",
        "               in_backprop->template flat<T>());",
        "      return;",
        "    }",
        "",
        "// TODO(ezhulenev): Remove custom kernel and move XSMM support to",
        "// LaunchConv2DBackpropInputOp functor.",
        "#if defined TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS && \\",
        "    defined TENSORFLOW_USE_LIBXSMM_BACKWARD_CONVOLUTIONS"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/27a65a43cf763897fecfa5cdb5cc653fc5dd0346",
    "API name": "tf.raw_ops.Conv2DBackpropInput"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"if (t.NumElements() != 1) {\\n    out->status = errors::InvalidArgument(\\\"The only valid scalar shape tensor is the fully unknown shape specified as -1.\\\");\\n    return;\\n  }\\n  if (t.dtype() == DT_INT32) {\\n    *out = PartialTensorShape::MakePartialShape(t.vec<int32>().data(), 1, out);\\n  } else if (t.dtype() == DT_INT64) {\\n    *out = PartialTensorShape::MakePartialShape(t.vec<int64_t>().data(), 1, out);\\n  }\"",
        "}"
    ],
    "Actual Clean Code": [
        "        \"The only valid scalar shape tensor is the fully unknown shape \"",
        "        \"specified as -1.\");",
        "  } else if (t.shape().dims() != 1) {",
        "    return errors::InvalidArgument(\"Shape must be at most rank 1 but is rank \",",
        "                                   t.shape().dims());",
        "  }",
        "  if (t.dtype() == DT_INT32) {",
        "    return PartialTensorShape::MakePartialShape(t.vec<int32>().data(),",
        "                                                t.NumElements(), out);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/c8ba76d48567aed347508e0552a257641931024d",
    "API name": "tf.raw_ops.EmptyTensorList"
},
{
    "Patch Formated": [
        "{\"Patch\": \"const tstring separator = sep_t->scalar<tstring>()();\\n\\nstd::vector<std::unique_ptr<ColumnInterface<tstring>>> columns =\\n    GenerateColumnsFromInput<tstring>(indices_list_in, values_list_in, shapes_list_in, dense_list_in);\\n\\nTensor* indices_out;\\nTensor* values_out;\\nTensor* shape_out;\\nconst int64_t batch_size = CalculateBatchSize(shapes_list_in, dense_list_in);\"}"
    ],
    "Actual Clean Code": [
        "    const Tensor* sep_t;",
        "    OP_REQUIRES_OK(context, context->input(\"sep\", &sep_t));",
        "    OP_REQUIRES(context, TensorShapeUtils::IsScalar(sep_t->shape()),",
        "                errors::InvalidArgument(\"Input separator should be a scalar. \"",
        "                                        \"Received: \",",
        "                                        sep_t->DebugString()));",
        "    const tstring separator = sep_t->scalar<tstring>()();",
        "",
        "    std::vector<std::unique_ptr<ColumnInterface<tstring>>> columns =",
        "        GenerateColumnsFromInput<tstring>(indices_list_in, values_list_in,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/83dcb4dbfa094e33db084e97c4d0531a559e0ebf",
    "API name": "tf.sparse.cross"
},
{
    "Patch Formated": [
        "{",
        "  \"Patch\": \"Add proper input validation before executing the convolution operation.\"",
        "}"
    ],
    "Actual Clean Code": [
        "    }",
        "",
        "    // If the input is empty, result can only be due to padding.",
        "    if (input.NumElements() == 0) {",
        "      // Zero-out output and return.",
        "      functor::SetZeroFunctor<Device, T>()(context->eigen_device<Device>(),",
        "                                           output->template flat<T>());",
        "",
        "      return;",
        "    }",
        "",
        "#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS",
        "    if (params_.padding != EXPLICIT &&",
        "        LaunchXsmmConvOp<Device, T>::Run(",
        "            context, input, filter, dimensions.batch, dimensions.input_rows,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/611d80db29dd7b0cfb755772c69d60ae5bca05f9",
    "API name": "tf.raw_ops.Conv2D"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\":\"if (!has_sample_rate_attr_) { \\n    const Tensor& sample_rate_tensor = c->input(2); \\n    sample_rate = sample_rate_tensor.scalar<float>()(); \\n    OP_REQUIRES(c, sample_rate > 0.0f,errors::InvalidArgument(\\\"sample_rate must be > 0\\\")); \\n} \\n\\nconst int batch_size = tensor.dim_size(0); \\nconst int64_t length_frames = tensor.dim_size(1); \\nconst int64_t num_channels = \\n    tensor.dims() == 2 ? 1 : tensor.dim_size(tensor.dims() - 1);\"",
        "}"
    ],
    "Actual Clean Code": [
        "    if (!has_sample_rate_attr_) {",
        "      const Tensor& sample_rate_tensor = c->input(2);",
        "      OP_REQUIRES(c,",
        "                  sample_rate_tensor.IsAligned() &&",
        "                      sample_rate_tensor.NumElements() == 1,",
        "                  errors::InvalidArgument(",
        "                      \"sample_rate must be rank-0 or contain a single value\"));",
        "      sample_rate = sample_rate_tensor.scalar<float>()();",
        "    }",
        "    OP_REQUIRES(c, sample_rate > 0.0f,",
        "                errors::InvalidArgument(\"sample_rate must be > 0\"));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bf6b45244992e2ee543c258e519489659c99fb7f",
    "API name": "tf.raw_ops.AudioSummaryV2"
},
{
    "Patch Formated": [
        "{\"Patch\":\"output_shape.set_dim(0, output_shape.dim_size(0) / col_params_->group.group_size);\"}"
    ],
    "Actual Clean Code": [
        "                        DoneCallback done) override {",
        "    auto output_shape = c->input(0).shape();",
        "    OP_REQUIRES_ASYNC(c, output_shape.dims() > 0,",
        "                      errors::InvalidArgument(\"input should have rank > 0, \",",
        "                                              \"recieved \", output_shape.dims()),",
        "                      done);",
        "    output_shape.set_dim(",
        "        0, output_shape.dim_size(0) * col_params_->group.group_size);",
        "    col_params_->instance.shape = output_shape;",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/c1f491817dec39a26be3c574e86a88c30f3c4770",
    "API name": "tf.raw_ops.CollectiveGather"
},
{
    "Patch Formated": [
        "{\"Patch\": \"  // Assume row-major order.\\n  TensorShape shape;\\n  TF_RETURN_IF_ERROR(TensorShape::BuildTensorShape(\\n      ctx->input(base_index + 2).vec<int64_t>(), &shape));\\n  CheckRankAtLeast2(ctx, shape);\\n  std::vector<int64_t> order(shape.dims());\\n  std::iota(order.begin(), order.end(), 0);\\n\\n  Status status = sparse::SparseTensor::Create(\\n      ctx->input(base_index), ctx->input(base_index + 1), shape, order, tensor);\\n\\n  if (!validate_indices || !status.ok()) return status;\\n\\n  return errors::InvalidArgument(\\n      \"Input validation failed for SetSize.\");\"}"
    ],
    "Actual Clean Code": [
        "  // Assume row-major order.",
        "  TensorShape shape;",
        "  const Tensor& shape_tensor = ctx->input(base_index + 2);",
        "  if (shape_tensor.dims() != 1) {",
        "    return errors::InvalidArgument(\"Shape must be a 1D tensor.\");",
        "  }",
        "  TF_RETURN_IF_ERROR(",
        "      TensorShape::BuildTensorShape(shape_tensor.vec<int64_t>(), &shape));",
        "  CheckRankAtLeast2(ctx, shape);",
        "  std::vector<int64_t> order(shape.dims());",
        "  std::iota(order.begin(), order.end(), 0);",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/cf70b79d2662c0d3c6af74583641e345fc939467",
    "API name": "tf.raw_ops.SetSize"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"OP_REQUIRES(c, TensorShapeUtils::IsVectorOrHigher(t.shape()) && !t.shape().dims().empty(), errors::InvalidArgument(\\\"Tensor must be at least a vector, but saw shape: \\\", t.shape().DebugString()))\"",
        "}"
    ],
    "Actual Clean Code": [
        "    OP_REQUIRES_OK(c, c->allocate_output(0, {}, &output_tensor, attr));",
        "    PartialTensorShape element_shape;",
        "    OP_REQUIRES(",
        "        c, !TensorShapeUtils::IsMatrixOrHigher(c->input(1).shape()),",
        "        errors::InvalidArgument(",
        "            \"TensorListFromTensor: element_shape must be at most rank 1 but \",",
        "            \"has the shape of \", c->input(1).shape().DebugString()));",
        "    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(1), &element_shape));",
        "    TensorList output_list;",
        "    const Tensor& t = c->input(0);",
        "    output_list.element_dtype = t.dtype();"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/3db59a042a38f4338aa207922fa2f476e000a6ee",
    "API name": "tf.raw_ops.TensorListFromTensor"
},
{
    "Patch Formated": [
        "{\"Patch\":\"Tensor indices = c->input(1);\\nPartialTensorShape element_shape;\\nOP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(2), &element_shape));\\nint num_elements = -1;\\nOP_REQUIRES(c, num_elements >= -1,\\n            errors::InvalidArgument(\\n                \\\"TensorListScatter expects num_elements >= -1, found: \\\",\\n                num_elements));\\nTensorList output_list;\"}"
    ],
    "Actual Clean Code": [
        "    Tensor indices = c->input(1);",
        "    PartialTensorShape element_shape;",
        "    OP_REQUIRES(",
        "        c, !TensorShapeUtils::IsMatrixOrHigher(c->input(2).shape()),",
        "        errors::InvalidArgument(",
        "            \"TensorListScatter: element_shape must be at most rank 1 but has \",",
        "            \"the shape of \", c->input(2).shape().DebugString()));",
        "    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(2), &element_shape));",
        "    // TensorListScatterV2 passes the num_elements input, TensorListScatter does",
        "    // not.",
        "    int num_elements = c->num_inputs() >= 4 ? c->input(3).scalar<int>()() : -1;"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bb03fdf4aae944ab2e4b35c7daa051068a8b7f61",
    "API name": "tf.raw_ops.TensorListScatter"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"OP_REQUIRES(context, max.dim_size(0) == depth,\\n                InvalidArgument(\\\"max has incorrect size, expected \\\", depth,\\n                                \\\" was \\\", max.dim_size(0)));\\n\\n    Tensor* grad_wrt_input;\\n    OP_REQUIRES_OK(context, context->allocate_output(0, input.shape(), &grad_wrt_input));\\n\\n    TensorShape min_max_shape({input.dim_size(input.dims() - 1)});\\n    Tensor* grad_wrt_min;\\n    OP_REQUIRES_OK(context, context->allocate_output(1, min_max_shape, &grad_wrt_min));\\n\\n    Tensor* grad_wrt_max;\\n    OP_REQUIRES_OK(context, context->allocate_output(2, min_max_shape, &grad_wrt_max));\"",
        "}"
    ],
    "Actual Clean Code": [
        "    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.",
        "    const Tensor& min = context->input(2);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(min.shape()),",
        "        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "    const Tensor& max = context->input(3);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(max.shape()),",
        "        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));",
        "    OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3cf67ac5705f4f04721d15e485e192bb319feed",
    "API name": "tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"params_.out_width, params_.depth); ",
        "     // Assuming qint8 <--> NCHW_VECT_C (int8x4) here.",
        "    constexpr bool is_int8x4 = std::is_same<T, qint8>::value;",
        "    is_int8x4 = false;",
        "    OP_REQUIRES(context, !is_int8x4, errors::InvalidArgument(\"qint8 should not be used with data_format NCHW_VECT_C.\"));",
        "    #if CUDNN_VERSION >= 7300 ",
        "    DnnPoolingOp<T>::Compute(context, se::dnn::PoolingMode::kMaximum, ksize_, stride_, padding_, explicit_paddings_,",
        "    data_format_, tensor_in, out_shape, propagate_nans_);\"",
        "}"
    ],
    "Actual Clean Code": [
        "                        params.out_width, params.depth);",
        "",
        "    // Degenerate pooling output should return an empty tensor.",
        "    if (out_shape.num_elements() == 0) {",
        "      Tensor* output = nullptr;",
        "      OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));",
        "      return;",
        "    }",
        "",
        "    // Assuming qint8 <--> NCHW_VECT_C (int8x4) here.",
        "    constexpr bool is_int8x4 = std::is_same<T, qint8>::value;",
        "    OP_REQUIRES(context, (is_int8x4 == (data_format_ == FORMAT_NCHW_VECT_C)),",
        "                errors::InvalidArgument("
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/32d7bd3defd134f21a4e344c8dfd40099aaf6b18",
    "API name": "tf.raw_ops.MaxPool"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\":\"                         done);\\n\\n    if (n == 0 || m == 0) {\\n      if (n == m || !compute_uv_ || !full_matrices_) {\\n        // S, U, and V are all empty. Nothing to do.\\n        done();\\n        return;\\n      }\\n      if (m > 0) {\\n        // Return a full canonical basis for the column space.\\n        auto device = context->eigen_device<GPUDevice>();\\n        functor::EyeFunctor<GPUDevice, Scalar> eye;\\n      }\\n    }\"",
        "}"
    ],
    "Actual Clean Code": [
        "                         done);",
        "",
        "    // If there are zero batches, we are done.",
        "    if (shapeRaw.num_elements() == 0) {",
        "      done();",
        "      return;",
        "    }",
        "",
        "    if (n == 0 || m == 0) {",
        "      if (n == m || !compute_uv_ || !full_matrices_) {",
        "        // S, U, and V are all empty. Nothing to do.",
        "        done();"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/c55b476aa0e0bd4ee99d0f3ad18d9d706cd1260a",
    "API name": "tf.linalg.matrix_rank"
},
{
    "Patch Formated": [
        "{\"Patch\":\"size_t size = static_cast<size_t>(size_t.scalar<Tidx>()());\"}"
    ],
    "Actual Clean Code": [
        "                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",",
        "                                        size_t.dims()));",
        "    OP_REQUIRES(ctx,",
        "                weights.shape() == data.shape() || weights.NumElements() == 0,",
        "                errors::InvalidArgument(",
        "                    \"`weights` must be the same shape as `arr` or a length-0 \"",
        "                    \"`Tensor`, in which case it acts as all weights equal to \"",
        "                    \"1. Received \",",
        "                    weights.shape().DebugString()));",
        "",
        "    Tidx size = size_t.scalar<Tidx>()();",
        "    OP_REQUIRES(",
        "        ctx, size >= 0,",
        "        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bf4c14353c2328636a18bfad1e151052c81d5f43",
    "API name": "tf.raw_ops.DenseBincount"
},
{
    "Patch Formated": [
        "{\"Patch\": \"int batch_idx = 0;\\n\\nOP_REQUIRES(ctx, splits(0) == 0,\\n            errors::InvalidArgument(\\\"Splits must start with 0, not with \\\",\\n                                    splits(0)));\\n\\nOP_REQUIRES(ctx, splits(num_rows) == num_values,\\n            errors::InvalidArgument(\\n                \\\"Splits must end with the number of values, got \\\",\\n                splits(num_rows)));\\n\"}"
    ],
    "Actual Clean Code": [
        "    int batch_idx = 0;",
        "",
        "    OP_REQUIRES(ctx, splits.size() > 0,",
        "                errors::InvalidArgument(\"Splits must be non-empty\"));",
        "",
        "    OP_REQUIRES(ctx, splits(0) == 0,",
        "                errors::InvalidArgument(\"Splits must start with 0, not with \",",
        "                                        splits(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/7a4591fd4f065f4fa903593bc39b2f79530a74b8",
    "API name": "tf.raw_ops.RaggedBincount"
},
{
    "Patch Formated": [
        "{",
        "  \"Patch\": \"if(!(in_image.dim_size(0) == batch && in_image.dim_size(1) == rows && in_image.dim_size(2) == cols && in_image.dim_size(3) == depth))\"",
        "}"
    ],
    "Actual Clean Code": [
        "            in_image.dim_size(2) == cols && in_image.dim_size(3) == depth &&",
        "            out_image.dim_size(0) == batch && out_image.dim_size(1) == rows &&",
        "            out_image.dim_size(2) == cols && out_image.dim_size(3) == depth &&",
        "            out_image.dims() == 4,",
        "        errors::InvalidArgument(",
        "            \"input_grads, input_image, and out_image should have the same \"",
        "            \"shape\"));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bd90b3efab4ec958b228cd7cfe9125be1c0cf255",
    "API name": "tf.raw_ops.LRNGrad"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"    int32_t num_batches = shape_tensor.scalar<int32>()();",
        "\\n    const int32_t num_dims = shape_tensor.dim_size(0);    ",
        "\\n    int32_t samples_per_batch = num_batches;",
        "\\n    for (int32_t i = 1; i < num_dims; i++) {",
        "\\n        samples_per_batch *= shape_tensor.scalar<int32>()(i);",
        "\\n    }",
        "\\n    const int32_t num_elements = num_batches * samples_per_batch;",
        "\\n    ",
        "\\n    // Allocate the output before fudging num_batches and samples_per_batch.",
        "\\n    TensorShape tensor_shape({num_elements});",
        "\\n    Tensor* samples_tensor;",
        "\\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));",
        "\"",
        "}"
    ],
    "Actual Clean Code": [
        "                errors::InvalidArgument(\"Shape tensor must not be empty, got \",",
        "                                        shape_tensor.DebugString()));",
        "    TensorShape tensor_shape;",
        "    OP_REQUIRES_OK(ctx, tensor::MakeShape(shape_tensor, &tensor_shape));",
        "",
        "    int32_t num_batches = tensor_shape.dim_size(0);",
        "    int32_t samples_per_batch = 1;",
        "    const int32_t num_dims = tensor_shape.dims();",
        "    for (int32_t i = 1; i < num_dims; i++) {",
        "      samples_per_batch *= tensor_shape.dim_size(i);",
        "    }",
        "    const int32_t num_elements = num_batches * samples_per_batch;",
        "",
        "    // Allocate the output before fudging num_batches and samples_per_batch.",
        "    Tensor* samples_tensor;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));",
        "",
        "    // Parameters must be 0-d or 1-d."
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/72180be03447a10810edca700cbc9af690dfeb51",
    "API name": "tf.raw_ops.ParameterizedTruncatedNormal"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"size_t size_bound = ss->ByteSizeLong() + kTensorProtoHeaderBytes + (num_elements * MaxBytesPerElement(DT_INT32));\\n  for (int64_t i = 0; i < num_elements; ++i) {\\n    size_bound += data[i].size();\\n  }\\n  if (size_bound > kMaxMessageBytes) {\\n    return errors::InvalidArgument(\\n        absl::StrCat(\\\"Tensor slice is too large to serialize (conservative estimate: \\\", size_bound, \\\")\\\"));\"",
        "}"
    ],
    "Actual Clean Code": [
        "    case DT_BFLOAT16:",
        "    default:",
        "      return 0;",
        "  }",
        "}",
        "",
        "template <>",
        "Status TensorSliceWriter::SaveData(const tstring* data, int64_t num_elements,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/5dd7b86b84a864b834c6fa3d7f9f51c87efa99d4",
    "API name": "tf.raw_ops.Save"
},
{
    "Patch Formated": [
        "{\"Patch\": \"errors::InvalidArgument(\\\"Index out of bound. `batch` (\\\" + std::to_string(batch) + \\\") must be less than the dimension size (\\\" + std::to_string(out.dimension(0)) + \\\").\\\"));\"}"
    ],
    "Actual Clean Code": [
        "      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {",
        "        const int64_t batch = indices_mat(i, 0);",
        "        const Tidx bin = values_flat(i);",
        "        OP_REQUIRES(",
        "            ctx, batch < out.dimension(0),",
        "            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,",
        "                                    \") must be less than the dimension size (\","
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/40adbe4dd15b582b0210dfbf40c243a62f5119fa",
    "API name": "tf.raw_ops.SparseBincount"
},
{
    "Patch Formated": [
        "{\"Patch\":\"if (ragged_nested_splits_len > 0) {\\n        batched_input_ = true;\\n        ragged::NestedSplitsVector ragged_nested_splits_in;\\n        ragged_nested_splits_in.reserve(ragged_nested_splits_len);\\n        for (int i = 0; i < ragged_nested_splits_len; i++) {\\n          ragged_nested_splits_in.emplace_back(\\n              context->input(i).vec<int64>());\\n        }\\n        ragged::NestedSplitsMatrix batched_ragged_input;\\n        TF_RETURN_IF_ERROR(batched_ragged_input.allocate(ragged_nested_splits_in.size(),\\n                                                       ragged_nested_splits_len));\\n        for (int i = 0; i < ragged_nested_splits_len; i++) {\\n          batched_ragged_input.append_splits(ragged_nested_splits_in[i]);\\n        }\\n      } else {\\n        // Encode as a Scalar Variant Tensor.\\n        Tensor* encoded_scalar;\\n        OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}),\\n                                                       &encoded_scalar));\\n      }\"}"
    ],
    "Actual Clean Code": [
        "        ragged_nested_splits_len);",
        "    for (int i = 0; i < ragged_nested_splits_len; i++) {",
        "      OP_REQUIRES(context, ragged_nested_splits_in[i].dims() == 1,",
        "                  errors::InvalidArgument(\"Requires nested_row_splits[\", i, \"]\",",
        "                                          \" to be rank 1 but is rank \",",
        "                                          ragged_nested_splits_in[i].dims()));",
        "      batched_ragged_input.append_splits(ragged_nested_splits_in[i]);",
        "    }",
        "",
        "    if (!batched_input_) {"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/88f93dfe691563baa4ae1e80ccde2d5c7a143821",
    "API name": "tf.raw_ops.RaggedTensorToVariant"
},
{
    "Patch Formated": [
        "{\"Patch\": \"CHECK_GE(input_backprop_index, 0) && CHECK_LT(input_backprop_index, num_total_inputs) << \\\"Invalid input backprop index: \\\" << input_backprop_index << \\\", \\\" << num_total_inputs;\\ninput_backprop_flat(input_backprop_index) += out_backprop_flat(index);\"}"
    ],
    "Actual Clean Code": [
        "    for (int index = 0; index < num_total_outputs; ++index) {",
        "      int input_backprop_index = out_arg_max_flat(index);",
        "      OP_REQUIRES(",
        "          context,",
        "          input_backprop_index >= 0 && input_backprop_index < num_total_inputs,",
        "          errors::InvalidArgument(",
        "              \"Invalid input backprop index: \", input_backprop_index, \", \",",
        "              num_total_inputs));",
        "      input_backprop_flat(input_backprop_index) += out_backprop_flat(index);",
        "    }",
        "  }",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/8741e57d163a079db05a7107a7609af70931def4",
    "API name": "tf.raw_ops.FractionalMaxPoolGrad"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"                  min_as_quantized, max_as_quantized,\\n\"",
        "             \"                  output->flat<quint8>().data(),\\n\"",
        "             \"                  output->flat<quint8>().size());\\n\"",
        "             \"    } else {\\n\"",
        "             \"      output->flat<T>().device(context->eigen_cpu_device()) =\\n\"",
        "             \"          input.flat<T>()\\n\"",
        "             \"              .cwiseMax(min_as_quantized)\\n\"",
        "             \"              .cwiseMin(max_as_quantized)\\n\"",
        "             \"              .template cast<T>();\\n\"",
        "             \"    }\\n\"",
        "             \"\\n\"",
        "             \"    Tensor* output_min = nullptr;\\n\"",
        "             \"    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\\n\"",
        "             \"    output_min->flat<float>()(0) = min_input;\\n\"",
        "             \"    Tensor* output_max = nullptr;\\n\"",
        "             \"    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));\\n\"",
        "             \"    output_max->flat<float>()(0) = max_input;\\n\"",
        "             \"  }\\n\"",
        "             \"};\\n\"",
        "             \"\\n\"",
        "             \"REGISTER_KERNEL_BUILDER(Name(\\\"QuantizedRelu6\\\")\\n\"",
        "             \"                            .Device(DEVICE_CPU)\"",
        "}"
    ],
    "Actual Clean Code": [
        "  void Compute(OpKernelContext* context) override {",
        "    const Tensor& input = context->input(0);",
        "    const Tensor& min_input_tensor = context->input(1);",
        "    const Tensor& max_input_tensor = context->input(2);",
        "",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),",
        "        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",",
        "                                min_input_tensor.dims()));",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),",
        "        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",",
        "                                max_input_tensor.dims()));",
        "",
        "    const float min_input = min_input_tensor.scalar<float>()();",
        "    const float max_input = max_input_tensor.scalar<float>()();",
        "",
        "    Tensor* output = nullptr;",
        "    OP_REQUIRES_OK(context,",
        "                   context->allocate_output(0, input.shape(), &output));",
        "    const T min_as_quantized = FloatToQuantized<T>(0.0f, min_input, max_input);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/49b3824d83af706df0ad07e4e677d88659756d89",
    "API name": "tf.raw_ops.QuantizedRelu6"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"void Compute(OpKernelContext* ctx) override {\\n    const Tensor& input = ctx->input(0);\\n    const float input_min_float = ctx->input(1).flat<float>()(0);\\n    const float input_max_float = ctx->input(2).flat<float>()(0);\\n    Tensor* output = nullptr;\\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\\n    Tensor* output_min = nullptr;\\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\\n    Tensor* output_max = nullptr;\\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));\\n\\n    // See QuantizationRangeOp as well, which has a copy of this logic.\\n    auto input_array = input.flat<T1>();\\n    const int32_t input_lowest_quantized =\\n        static_cast<int32_t>(Eigen::NumTraits<T1>::lowest());\\n    const int32_t input_highest_quantized =\\n        static_cast<int32_t>(Eigen::NumTraits<T1>::highest());\\n    T1 actual_min_quantized = input_highest_quantized;\\n    T1 actual_max_quantized = input_lowest_quantized;\\n    for (int i = 0; i < input_array.size(); ++i) {\" ",
        "}"
    ],
    "Actual Clean Code": [
        "  void Compute(OpKernelContext* ctx) override {",
        "    const Tensor& input = ctx->input(0);",
        "    const Tensor& input_min = ctx->input(1);",
        "    const Tensor& input_max = ctx->input(2);",
        "",
        "    OP_REQUIRES(",
        "        ctx, TensorShapeUtils::IsScalar(input_min.shape()),",
        "        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",",
        "                                input_min.dims()));",
        "    OP_REQUIRES(",
        "        ctx, TensorShapeUtils::IsScalar(input_max.shape()),",
        "        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",",
        "                                input_max.dims()));",
        "",
        "    const float input_min_float = input_min.scalar<float>()();",
        "    const float input_max_float = input_max.scalar<float>()();",
        "    Tensor* output = nullptr;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));",
        "    Tensor* output_min = nullptr;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/73ad1815ebcfeb7c051f9c2f7ab5024380ca8613",
    "API name": "tf.raw_ops.QuantizeDownAndShrinkRange"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"const float max_a = fmaxf(context->input(3).flat<float>()(0), 0.0);    \\nconst float min_b = fminf(context->input(4).flat<float>()(0), 0.0);    \\nconst float max_b = fmaxf(context->input(5).flat<float>()(0), 0.0);    \\n\\n// Make sure that we have valid quantization ranges for the input buffers.    \\n// If the difference between the min and max is negative or zero, it makes    \\n// it hard to do meaningful intermediate operations on the values.    \\nOP_REQUIRES(context, (max_a > min_a),                errors::InvalidArgument(\\\"max_a must be larger than min_a.\\\"));    \\nOP_REQUIRES(context, (max_b > min_b),                errors::InvalidArgument(\\\"max_b must be larger than min_b.\\\"));    \\nconst int32_t offset_a = FloatToQuantizedUnclamped<T1>(0.0f, min_a, max_a);    \\nconst int32_t offset_b = FloatToQuantizedUnclamped<T2>(0.0f, min_b, max_b);\\nconst int32_t offset_c = 0;\\nconst int32_t mult_c = 1;\\nconst int32_t shift_c = 0;\\n\\n// Check that the dimensions of the two matrices are valid.\\nOP_REQUIRES(context, TensorShapeUtils::IsMatrix(a.shape()),                errors::InvalidArgument(\\\"In[0] is not a matrix\\\"));\\nOP_REQUIRES(context, TensorShapeUtils::IsMatrix(b.shape()),\"",
        "}"
    ],
    "Actual Clean Code": [
        "    const Tensor& a = context->input(0);",
        "    const Tensor& b = context->input(1);",
        "    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(2).shape()),",
        "                errors::InvalidArgument(\"min_a must be a scalar, but got shape\",",
        "                                        context->input(2).shape()));",
        "    const float min_a = context->input(2).flat<float>()(0);",
        "    OP_REQUIRES(context, context->input(3).NumElements() == 1,",
        "                errors::InvalidArgument(\"max_a must be a scalar, but got shape\",",
        "                                        context->input(3).shape()));",
        "    const float max_a = context->input(3).flat<float>()(0);",
        "    OP_REQUIRES(context, context->input(4).NumElements() == 1,",
        "                errors::InvalidArgument(\"min_b must be a scalar, but got shape\",",
        "                                        context->input(4).shape()));",
        "    const float min_b = context->input(4).flat<float>()(0);",
        "    OP_REQUIRES(context, context->input(5).NumElements() == 1,",
        "                errors::InvalidArgument(\"max_b must be a scalar, but got shape\",",
        "                                        context->input(5).shape()));",
        "    const float max_b = context->input(5).flat<float>()(0);",
        "",
        "    // Make sure that we have valid quantization ranges for the input buffers.",
        "    // If the difference between the min and max is negative or zero, it makes"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/aca766ac7693bf29ed0df55ad6bfcc78f35e7f48",
    "API name": "tf.raw_ops.QuantizedMatMul"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"OP_REQUIRES(context, quant_min_ != quant_max_, errors::InvalidArgument(\\\"Quantization min \\\"\\n\"",
        "              \"                                \\\"should be different from max, but min was \\\"\\n\"",
        "              \"                                \\\"%d and max was %d\\\", quant_min_, quant_max_));\"",
        "}"
    ],
    "Actual Clean Code": [
        "    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.",
        "    const Tensor& min = context->input(1);",
        "    const Tensor& max = context->input(2);",
        "",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(min.shape()),",
        "        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(max.shape()),",
        "        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));",
        "    OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/785d67a78a1d533759fcd2f5e8d6ef778de849e0",
    "API name": "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel"
},
{
    "Patch Formated": [
        "{",
        "    \"Patch\": \"OP_REQUIRES_OK(context, ConvBackpropComputeDimensionsV2(\\\"Conv2DCustomBackpropInput\\\", /*num_spatial_dims=*/2, input_shape, filter.shape(), out_backprop.shape(), {1, 1, 1, 1}, strides_, padding_, &dims));\"",
        "}"
    ],
    "Actual Clean Code": [
        "    const Tensor& filter = context->input(1);",
        "    const Tensor& out_backprop = context->input(2);",
        "    OP_REQUIRES(",
        "        context, out_backprop.dims() == 4,",
        "        errors::InvalidArgument(\"input_sizes must be 4-dimensional, got: \",",
        "                                out_backprop.dims()));",
        "",
        "    TensorShape input_shape;",
        "    OP_REQUIRES_OK(context,",
        "                   Conv2DBackpropComputeInputShape(input_sizes, filter.shape(),"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/50156d547b9a1da0144d7babe665cf690305b33c",
    "API name": "tf.raw_ops.Conv2DBackpropInput"
},
{
    "Patch Formated": [
        "{\"Patch\":\"auto shape_vec = tensor_in_shape.vec<int32>();\\nfor (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {\\n  output_shape.AddDim(shape_vec(i));\\n}\\nif (output_shape.NumElements() == 0) {\\n  Tensor* output = nullptr;\\n  OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(output_shape), &output);\\n}\"}"
    ],
    "Actual Clean Code": [
        "    auto shape_vec = tensor_in_shape.vec<int32>();",
        "    for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {",
        "      OP_REQUIRES_OK(context, output_shape.AddDimWithStatus(shape_vec(i)));",
        "    }",
        "    if (output_shape.num_elements() == 0) {",
        "      Tensor* output = nullptr;",
        "      OP_REQUIRES_OK(context,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/3a6ac52664c6c095aa2b114e742b0aa17fdce78f",
    "API name": "tf.raw_ops.AvgPoolGrad"
}]
