{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['    }', '    OP_REQUIRES(', '        context, pooling_ratio_[0] == 1 && pooling_ratio_[3] == 1,', '        errors::Unimplemented(\\\"Fractional average pooling is not yet \\\"', '                              \\\"supported on the batch nor channel dimension.\\\"));', '    OP_REQUIRES_OK(context, context->GetAttr(\\\"deterministic\\\", &deterministic_));', '    OP_REQUIRES_OK(context, context->GetAttr(\\\"seed\\\", &seed_));']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    }",
        "    OP_REQUIRES(",
        "        context, pooling_ratio_[0] == 1 && pooling_ratio_[3] == 1,",
        "        errors::Unimplemented(\"Fractional average pooling is not yet \"",
        "                              \"supported on the batch nor channel dimension.\"));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/ee50d1e00f81f62a4517453f721c634bbb478307",
    "API name": "tensorflow.python.ops.nn_ops.fractional_avg_pool_v2"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['        }', '        DimensionHandle unused;', '        if (!c->WithValue(c->Dim(c->input(i), 1), 1, &unused).ok()) {', '          return errors::InvalidArgument(\\\"Size of first dimension must be 1.\\\");', '        }', '        TF_RETURN_WITH_CONTEXT_IF_ERROR(c->Merge(c->input(i), cur, &cur),', '                                        \\\"From merging shape \\\", i,', '                                        \\\" with other shapes.\\\");', '      }', '', '      c->set_output(0, passed_shape);', '']\" ",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "              \"All input shapes must be fully defined.\");",
        "        }",
        "        if (c->Rank(c->input(i)) < 1) {",
        "          return errors::InvalidArgument(",
        "              \"The rank of all input shapes must be greater than 0, \"",
        "              \"but input \",",
        "              i, \" had rank \", c->Rank(c->input(i)), \".\");",
        "        }",
        "        DimensionHandle unused;",
        "        if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) {",
        "          return errors::InvalidArgument(\"Size of first dimension must be 1.\");",
        "        }"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/da66bc6d5ff466aee084f9e7397980a24890cd15",
    "API name": "tf.raw_ops.ParallelConcat"
},
{
    "Patch Formated": [
        "\"Patch\": \"['                  errors::Unimplemented(\\\"All lengths have to be the same\\\"));', '    }', '    OP_REQUIRES(', '        ctx, element_dims[0] % length == 0,', '        errors::Unimplemented(\\\"Buffer size has to be a multiple of length\\\"));', '    std::vector<int64_t> new_dims = {element_dims[0] / length, length};', '    for (int i = 1; i < element_dims.size(); i++) {', '      new_dims.push_back(element_dims[i]);', '      OP_REQUIRES(ctx, element_dims[i] == length, errors::Unimplemented(\\\"All lengths have to be the same\\\"));']\""
    ],
    "Actual Clean Code": [
        "                  errors::Unimplemented(\"All lengths have to be the same\"));",
        "    }",
        "    OP_REQUIRES(ctx, length,",
        "                errors::Unimplemented(\"All lengths must be positive\"));",
        "    OP_REQUIRES(",
        "        ctx, element_dims[0] % length == 0,",
        "        errors::Unimplemented(\"Buffer size has to be a multiple of length\"));",
        "    std::vector<int64_t> new_dims = {element_dims[0] / length, length};"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/728113a3be690facad6ce436660a0bc1858017fa",
    "API name": "tf.raw_ops.RandomShuffle"
},
{
    "Patch Formated": [
        "\"Patch\": \"['      ShapeHandle keys;', '      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));', '      DimensionHandle unused;', '      if (!c->RankKnown(keys) || !c->RankKnown(c->input(2))) {', '        return errors::InvalidArgument(\\\"Both keys and values must have known ranks.\\\");', '      }', '      TF_RETURN_IF_ERROR(', '          c->Merge(c->Dim(keys, 0), c->Dim(c->input(2), 0), &unused));', '      return OkStatus();', '    });', '', 'Status MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,', '                             const ShapeHandle& value) {']\""
    ],
    "Actual Clean Code": [
        "      ShapeHandle keys;",
        "      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));",
        "      ShapeHandle values;",
        "      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(2), 1, &values));",
        "      DimensionHandle unused;",
        "      TF_RETURN_IF_ERROR(c->Merge(c->Dim(keys, 0), c->Dim(values, 0), &unused));",
        "      return OkStatus();",
        "    });",
        "",
        "Status MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/980b22536abcbbe1b4a5642fc940af33d8c19b69",
    "API name": "tf.raw_ops.LookupTableImportV2"
},
{
    "Patch Formated": [
        "I'm sorry, but I don't have the ability to generate C++ code fixes for TensorFlow raw operations."
    ],
    "Actual Clean Code": [
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));",
        "    OP_REQUIRES(ctx, stride_.size() == num_dims(),",
        "                errors::InvalidArgument(\"Sliding window strides field must \"",
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));",
        "",
        "    const TensorShape tensor_in_shape = ctx->InputShape(0);",
        "    const TensorShape tensor_out_shape = ctx->InputShape(1);",
        "    const TensorShape out_backprop_shape = ctx->InputShape(2);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/1295ae4dbb52fe06b19733b0257e2340d7b63b8d",
    "API name": "tf.raw_ops.AvgPoolGrad"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "{",
        "\"Patch\": \"['                              .TypeConstraint<T>(\\\"T\\\"),                         \\\\\\\\',",
        "'                          QuantizeAndDequantizeV3Op<GPUDevice, T>);            \\\\\\\\',",
        "'  REGISTER_KERNEL_BUILDER(Name(\\\"QuantizeAndDequantizeV3\\\")                      \\\\\\\\',",
        "'                              .Device(DEVICE_GPU)                              \\\\\\\\',",
        "'                              .HostMemory(\\\"input_min\\\")                         \\\\\\\\',",
        "'                              .HostMemory(\\\"input_max\\\")                         \\\\\\\\',",
        "'                              .HostMemory(\\\"num_bits\\\")                          \\\\\\\\',",
        "'                              .TypeConstraint<T>(\\\"T\\\"),                         \\\\\\\\',",
        "'                          QuantizeAndDequantizeV3Op<GPUDevice, T>);            \\\\\\\\',",
        "'  REGISTER_KERNEL_BUILDER(Name(\\\"QuantizeAndDequantizeV4\\\")                      \\\\\\\\',",
        "'                              .Device(DEVICE_GPU)                              \\\\\\\\',",
        "'                              .HostMemory(\\\"input_min\\\")                         \\\\\\\\',",
        "'                              .HostMemory(\\\"input_max\\\")                         \\\\\\\\',",
        "'                              .TypeConstraint<T>(\\\"T\\\"),                         \\\\\\\\',",
        "'                          QuantizeAndDequantizeV4Op<GPUDevice, T>);            \\\\\\\\',",
        "'  REGISTER_KERNEL_BUILDER(Name(\\\"QuantizeAndDequantizeV4Grad\\\")                  \\\\\\\\',",
        "'                              .Device(DEVICE_GPU)                              \\\\\\\\',",
        "'                              .HostMemory(\\\"input_min\\\")                         \\\\\\\\',",
        "'                              .HostMemory(\\\"input_max\\\")                         \\\\\\\\',",
        "'                              .TypeConstraint<T>(\\\"T\\\"),                         \\\\\\\\',",
        "'                          QuantizeAndDequantizeV4GradientOp<GPUDevice, T>);    \\\\\\\\',",
        "'  REGISTER_KERNEL_BUILDER(                                                     \\\\\\\\',",
        "'      Name(\\\"QuantizeAndDequantize\\\").Device(DEVICE_GPU).TypeConstraint<T>(\\\"T\\\"), \\\\\\\\',",
        "'      QuantizeAndDequantizeOp<GPUDevice, T>);',",
        "'TF_CALL_float(REGISTER_GPU_KERNEL);',",
        "'TF_CALL_double(REGISTER_GPU_KERNEL);',",
        "'#undef REGISTER_GPU_KERNEL',",
        "'#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM',",
        "'}  // namespace tensorflow']\"",
        "}",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV3\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .HostMemory(\"num_bits\")                          \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV3Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4Grad\")                  \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV4GradientOp<GpuDevice, T>);    \\",
        "  REGISTER_KERNEL_BUILDER(                                                     \\",
        "      Name(\"QuantizeAndDequantize\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\",
        "      QuantizeAndDequantizeOp<GpuDevice, T>);",
        "TF_CALL_float(REGISTER_GPU_KERNEL);",
        "TF_CALL_double(REGISTER_GPU_KERNEL);",
        "#undef REGISTER_GPU_KERNEL",
        "#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3f9cb38ecfe5a8a703f2c4a8fead434ef291713",
    "API name": "tf.raw_ops.QuantizeAndDequantizeV3"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['    OP_REQUIRES(context, max.dim_size(0) == depth,', '                InvalidArgument(\\\\\"max has incorrect size, expected \\\\\", depth,', '                                \\\\\" was \\\\\", max.dim_size(0)));', '', '    Tensor* grad_wrt_input;', '    OP_REQUIRES_OK(context,', '      context->allocate_output(0, max.dim_size(0) == depth ? input.shape() : TensorShape{}, &grad_wrt_input));', '', '    TensorShape min_max_shape({input.dim_size(input.dims() - 1)});', '    Tensor* grad_wrt_min;', '    OP_REQUIRES_OK(context,', '                   context->allocate_output(1, min_max_shape, &grad_wrt_min));', '', '    Tensor* grad_wrt_max;', '    OP_REQUIRES_OK(context,', '                   context->allocate_output(2, min_max_shape, &grad_wrt_max));']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.",
        "    const Tensor& min = context->input(2);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(min.shape()),",
        "        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "    const Tensor& max = context->input(3);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(max.shape()),",
        "        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));",
        "    OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3cf67ac5705f4f04721d15e485e192bb319feed",
    "API name": "tf.quantization.fake_quant_with_min_max_vars_gradient"
},
{
    "Patch Formated": [
        "\"Patch\": \"['    const int64_t samples_per_alpha = samples_shape.num_elements();', '', '    OP_REQUIRES(ctx, alpha_t.shape().dims() <= samples_shape.dims(), errors::InvalidArgument(\\\"alpha rank must be less or equal to samples rank\\\"));','    samples_shape.AppendShape(alpha_t.shape());', '    // Allocate output samples.', '    Tensor* samples_t = nullptr;', '    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, samples_shape, &samples_t));', '']\""
    ],
    "Actual Clean Code": [
        "    const int64_t samples_per_alpha = samples_shape.num_elements();",
        "",
        "    OP_REQUIRES_OK(ctx, samples_shape.AppendShapeWithStatus(alpha_t.shape()));",
        "    // Allocate output samples.",
        "    Tensor* samples_t = nullptr;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, samples_shape, &samples_t));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/552bfced6ce4809db5f3ca305f60ff80dd40c5a3",
    "API name": "tf.random.gamma"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\":\"['', '    const int64_t batch_key = context->input(2).scalar<int64_t>()();', \\",
        "'    if(!(batch_key)) throw std::invalid_argument(\\\"Input argument batch_key is invalid\\\");', '    const bool nonempty_input = batch_index_t.dim_size(0) > 0;', '', \\",
        "'    // If we have a non-empty tensor, slice it up.', '    // (It is important to do this outside of the critical section below.)', \\\"    // The following variables are populated iff 'nonempty_input==true'.\\\", '    std::vector<int64_t> sizes;', '    std::vector<int64_t> batch_keys;', '    std::vector<Tensor> split_inputs;', '    if (nonempty_input) {', '      auto batch_indices =']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    }",
        "",
        "    if (!TensorShapeUtils::IsScalar(context->input(2).shape())) {",
        "      return errors::InvalidArgument(",
        "          \"Input id should be scalar; \"",
        "          \"Got: \",",
        "          context->input(2).DebugString(), \".\");",
        "    }",
        "    const int64_t batch_key = context->input(2).scalar<int64_t>()();",
        "    const bool nonempty_input = batch_index_t.dim_size(0) > 0;",
        "",
        "    // If we have a non-empty tensor, slice it up."
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/4419d10d576adefa36b0e0a9425d2569f7c0189f",
    "API name": "tf.raw_ops.Unbatch"
},
{
    "Patch Formated": [
        "{\"Patch\": \"for (int64_t b = 0; b < batch_size; ++b) {\\n      const int64_t num_boxes = boxes.dim_size(1);\\n      const auto tboxes = boxes.tensor<T, 3>();\\n      for (int64_t bb = 0; bb < num_boxes; ++bb) {\\n        int64_t color_index = bb % color_table.size();\\n        const int64_t min_box_row =\\n            static_cast<int64_t>(tboxes(b, bb, 0) * (height - 1));\\n    }\"}"
    ],
    "Actual Clean Code": [
        "    for (int64_t b = 0; b < batch_size; ++b) {",
        "      const int64_t num_boxes = boxes.dim_size(1);",
        "      const auto tboxes = boxes.tensor<float, 3>();",
        "      for (int64_t bb = 0; bb < num_boxes; ++bb) {",
        "        int64_t color_index = bb % color_table.size();",
        "        const int64_t min_box_row =",
        "            static_cast<float>(tboxes(b, bb, 0)) * (height - 1);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/da0d65cdc1270038e72157ba35bf74b85d9bda11",
    "API name": "tf.raw_ops.DrawBoundingBoxes"
},
{
    "Patch Formated": [
        "{\"Patch\": \"['void LinearAlgebraOp<InputScalar, OutputScalar>::ComputeTensorSlice(', '    OpKernelContext* context, int64_t matrix_index, const TensorInputs& inputs,', '    const TensorShapes& input_matrix_shapes, const TensorOutputs& outputs,', '    const TensorShapes& output_matrix_shapes) {', '  InputConstMatrixMaps matrix_inputs;', '  for (size_t i = 0; i < inputs.size(); ++i) {', '    // TODO(kalakris): Handle alignment if possible. Eigen::Map is', '    // unaligned by default.', '    int64_t offset = matrix_index * input_matrix_shapes[i].num_elements();', '    if (offset < 0 || offset >= inputs[i]->NumElements())', '      return;', '    matrix_inputs.emplace_back(', '        inputs[i]->flat<InputScalar>().data() + offset,']\"}"
    ],
    "Actual Clean Code": [
        "                                  output_idx, output_tensor_shape, &out));",
        "    }",
        "    OP_REQUIRES(",
        "        context, out->dtype() == DataTypeToEnum<OutputScalar>::v(),",
        "        errors::InvalidArgument(\"Invalid output dtype \", out->dtype(), \" vs \",",
        "                                DataTypeToEnum<OutputScalar>::v()));",
        "",
        "    outputs->emplace_back(out);",
        "  }",
        "}",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/aed36912609fc07229b4d0a7b44f3f48efc00fd0",
    "API name": "tf.raw_ops.Eig"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "{",
        "\"Patch\": \"['        OP_REQUIRES_OK(', '            context,', '            GetWindowedOutputSizeVerbose(', '                dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,', '                dims.spatial_dims[0].stride, padding_,', '                &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom));', '        OP_REQUIRES(context, pad_top >=0 && pad_bottom >=0,', '                      errors::InvalidArgument(\\\"top padding and bottom padding should be greater than or equal to zero.\\\"));', '        OP_REQUIRES_OK(', '            context,', '            GetWindowedOutputSizeVerbose(', '                dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,', '                dims.spatial_dims[1].stride, padding_,', '                &dims.spatial_dims[1].output_size, &pad_left, &pad_right));', '        OP_REQUIRES(context, pad_left >=0 && pad_right >=0,', '                      errors::InvalidArgument(\\\"left padding and right padding should be greater than or equal to zero.\\\"));', '    if (pad_left == pad_right && pad_top == pad_bottom) {', '      if (LaunchXsmmBackwardInputConvolution<Device, T>()(', '              context, context->eigen_device<Device>(),', '              in_backprop->tensor<T, 4>(), filter.tensor<T, 4>(),']\"",
        "}",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    }",
        "",
        "    // If shapes are valid but `out_backprop` is empty, in_backprop should be",
        "    // set to all zeros.  Otherwise, cudnn/dnnl fail with an empty input.",
        "    if (out_backprop.NumElements() == 0) {",
        "      functor::SetZeroFunctor<Device, T> set_zero;",
        "      set_zero(context->eigen_device<Device>(),",
        "               in_backprop->template flat<T>());",
        "      return;",
        "    }",
        "",
        "// TODO(ezhulenev): Remove custom kernel and move XSMM support to",
        "// LaunchConv2DBackpropInputOp functor.",
        "#if defined TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS && \\",
        "    defined TENSORFLOW_USE_LIBXSMM_BACKWARD_CONVOLUTIONS"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/27a65a43cf763897fecfa5cdb5cc653fc5dd0346",
    "API name": "tf.raw_ops.Conv2DBackpropInput"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\":\"['        \\\"The only valid scalar shape tensor is the fully unknown shape \\\"', '        \\\"specified as -1.\\\");', '  }', '  if (t.dtype() == DT_INT32 && t.NumElements() > 0) {', '    return PartialTensorShape::MakePartialShape(t.vec<int32>().data(),', '                                                t.NumElements(), out);', '  } else if (t.dtype() == DT_INT64 && t.NumElements() > 0) {', '    return PartialTensorShape::MakePartialShape(t.vec<int64_t>().data(),', '                                                t.NumElements(), out);']\" ",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "        \"The only valid scalar shape tensor is the fully unknown shape \"",
        "        \"specified as -1.\");",
        "  } else if (t.shape().dims() != 1) {",
        "    return errors::InvalidArgument(\"Shape must be at most rank 1 but is rank \",",
        "                                   t.shape().dims());",
        "  }",
        "  if (t.dtype() == DT_INT32) {",
        "    return PartialTensorShape::MakePartialShape(t.vec<int32>().data(),",
        "                                                t.NumElements(), out);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/c8ba76d48567aed347508e0552a257641931024d",
    "API name": "tf.raw_ops.EmptyTensorList"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"const tstring separator = sep_t->scalar<tstring>()();",
        "",
        "if (!separator.empty())",
        "{",
        "    std::vector<std::unique_ptr<ColumnInterface<tstring>>> columns =",
        "        GenerateColumnsFromInput<tstring>(indices_list_in, values_list_in,",
        "                                            shapes_list_in, dense_list_in);",
        "",
        "    Tensor* indices_out;",
        "    Tensor* values_out;",
        "    Tensor* shape_out;",
        "",
        "    const int64_t batch_size =",
        "        CalculateBatchSize(shapes_list_in, dense_list_in);",
        "}\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    const Tensor* sep_t;",
        "    OP_REQUIRES_OK(context, context->input(\"sep\", &sep_t));",
        "    OP_REQUIRES(context, TensorShapeUtils::IsScalar(sep_t->shape()),",
        "                errors::InvalidArgument(\"Input separator should be a scalar. \"",
        "                                        \"Received: \",",
        "                                        sep_t->DebugString()));",
        "    const tstring separator = sep_t->scalar<tstring>()();",
        "",
        "    std::vector<std::unique_ptr<ColumnInterface<tstring>>> columns =",
        "        GenerateColumnsFromInput<tstring>(indices_list_in, values_list_in,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/83dcb4dbfa094e33db084e97c4d0531a559e0ebf",
    "API name": "tf.sparse.cross"
},
{
    "Patch Formated": [
        "\"Patch\": \"\\n#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS\\n    if (params_.padding != EXPLICIT && LaunchXsmmConvOp<Device, T>::Run( context, input, filter, dimensions.batch, dimensions.input_rows, dimensions.input_cols, dimensions.in_depth, dimensions.filter_rows, dimensions.filter_cols, dimensions.pad_rows_before, dimensions.pad_cols_before, dimensions.out_rows, dimensions.out_cols, dimensions.out_depth, dimensions.dilation_rows, dimensions.dilation_cols, dimensions.stride_rows, dimensions.stride_cols, output, params_.data_format)) { \\n      // input validation\\n      if (input == NULL || filter == NULL || output == NULL) {\\n          return;\\n      }\\n      return;\\n    }\\n#endif\\n\""
    ],
    "Actual Clean Code": [
        "    }",
        "",
        "    // If the input is empty, result can only be due to padding.",
        "    if (input.NumElements() == 0) {",
        "      // Zero-out output and return.",
        "      functor::SetZeroFunctor<Device, T>()(context->eigen_device<Device>(),",
        "                                           output->template flat<T>());",
        "",
        "      return;",
        "    }",
        "",
        "#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS",
        "    if (params_.padding != EXPLICIT &&",
        "        LaunchXsmmConvOp<Device, T>::Run(",
        "            context, input, filter, dimensions.batch, dimensions.input_rows,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/611d80db29dd7b0cfb755772c69d60ae5bca05f9",
    "API name": "tf.raw_ops.Conv2D"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['    if (!has_sample_rate_attr_) {', '      const Tensor& sample_rate_tensor = c->input(2);', '      sample_rate = sample_rate_tensor.scalar<float>()();', '    }', '    OP_REQUIRES(c, sample_rate > 0.0f,', '                errors::InvalidArgument(\\\"sample_rate must be > 0\\\"));', '', '    const int batch_size = tensor.dim_size(0);', '    const int64_t length_frames = tensor.dim_size(1);', '    const int64_t num_channels =', '        tensor.dims() == 2 ? 1 : tensor.dim_size(tensor.dims() - 1);', '    OP_REQUIRES(c, batch_size >= 0 && length_frames >= 0 && num_channels > 0,', '                errors::InvalidArgument(\\\"batch_size, length_frames must be >= 0 and num_channels must be > 0\\\"));']\"    ",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    if (!has_sample_rate_attr_) {",
        "      const Tensor& sample_rate_tensor = c->input(2);",
        "      OP_REQUIRES(c,",
        "                  sample_rate_tensor.IsAligned() &&",
        "                      sample_rate_tensor.NumElements() == 1,",
        "                  errors::InvalidArgument(",
        "                      \"sample_rate must be rank-0 or contain a single value\"));",
        "      sample_rate = sample_rate_tensor.scalar<float>()();",
        "    }",
        "    OP_REQUIRES(c, sample_rate > 0.0f,",
        "                errors::InvalidArgument(\"sample_rate must be > 0\"));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bf6b45244992e2ee543c258e519489659c99fb7f",
    "API name": "tf.raw_ops.AudioSummaryV2"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\":\"['DoneCallback done) override {', '    auto output_shape = c->input(0).shape();', '    if (output_shape.dim_size(0) > 0) {', '    output_shape.set_dim(', '        0, output_shape.dim_size(0) * col_params_->group.group_size);', '    }', '    col_params_->instance.shape = output_shape;', '', '    // Allocate output on the first pass through this function.  This must be', \\\"   // done immediately, while we're still in the executor thread.  Otherwise\\\", '    // the memory is not guaranteed to be unused by any concurrently executing', '    // GPU kernel.']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "                        DoneCallback done) override {",
        "    auto output_shape = c->input(0).shape();",
        "    OP_REQUIRES_ASYNC(c, output_shape.dims() > 0,",
        "                      errors::InvalidArgument(\"input should have rank > 0, \",",
        "                                              \"recieved \", output_shape.dims()),",
        "                      done);",
        "    output_shape.set_dim(",
        "        0, output_shape.dim_size(0) * col_params_->group.group_size);",
        "    col_params_->instance.shape = output_shape;",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/c1f491817dec39a26be3c574e86a88c30f3c4770",
    "API name": "tf.raw_ops.CollectiveGather"
},
{
    "Patch Formated": [
        "\"Patch\": \"",
        "@@ -6,6 +6,9 @@ ",
        "   TF_RETURN_IF_ERROR(TensorShape::BuildTensorShape(",
        "       ctx->input(base_index + 2).vec<int64_t>(), &shape));",
        "   CheckRankAtLeast2(ctx, shape);",
        "+  ",
        "+  if (!shape.IsFullyDefined())",
        "+    return errors::InvalidArgument('Shape must be fully defined');",
        "   ",
        "   std::vector<int64_t> order(shape.dims());",
        "   std::iota(order.begin(), order.end(), 0);\"",
        ""
    ],
    "Actual Clean Code": [
        "  // Assume row-major order.",
        "  TensorShape shape;",
        "  const Tensor& shape_tensor = ctx->input(base_index + 2);",
        "  if (shape_tensor.dims() != 1) {",
        "    return errors::InvalidArgument(\"Shape must be a 1D tensor.\");",
        "  }",
        "  TF_RETURN_IF_ERROR(",
        "      TensorShape::BuildTensorShape(shape_tensor.vec<int64_t>(), &shape));",
        "  CheckRankAtLeast2(ctx, shape);",
        "  std::vector<int64_t> order(shape.dims());",
        "  std::iota(order.begin(), order.end(), 0);",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/cf70b79d2662c0d3c6af74583641e345fc939467",
    "API name": "tf.raw_ops.SetSize"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['    OP_REQUIRES_OK(c, c->allocate_output(0, {}, &output_tensor, attr));', '    PartialTensorShape element_shape;', '    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(1), &element_shape));', '    TensorList output_list;', '    const Tensor& t = c->input(0);', '    output_list.element_dtype = t.dtype();', '    OP_REQUIRES(c, TensorShapeUtils::IsVectorOrHigher(t.shape()),', '                errors::InvalidArgument(', '                    \\\"Tensor must be at least a vector, but saw shape: \\\",', '                    t.shape().DebugString()));', '    CHECK(TensorShapeUtils::IsVectorOrHigher(t.shape())) << \\\"Tensor must be at least a vector, but saw shape: \\\" << t.shape().DebugString();', '    TensorShape output_shape(t.shape());']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    OP_REQUIRES_OK(c, c->allocate_output(0, {}, &output_tensor, attr));",
        "    PartialTensorShape element_shape;",
        "    OP_REQUIRES(",
        "        c, !TensorShapeUtils::IsMatrixOrHigher(c->input(1).shape()),",
        "        errors::InvalidArgument(",
        "            \"TensorListFromTensor: element_shape must be at most rank 1 but \",",
        "            \"has the shape of \", c->input(1).shape().DebugString()));",
        "    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(1), &element_shape));",
        "    TensorList output_list;",
        "    const Tensor& t = c->input(0);",
        "    output_list.element_dtype = t.dtype();"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/3db59a042a38f4338aa207922fa2f476e000a6ee",
    "API name": "tf.raw_ops.TensorListFromTensor"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"",
        "    Tensor indices = c->input(1);",
        "    PartialTensorShape element_shape;",
        "    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(2), &element_shape)); ",
        "    int num_elements = c->num_inputs() >= 4 ? c->input(3).scalar<int>()() : -1; ",
        "    OP_REQUIRES(c, num_elements >= -1 && num_elements <= indices.NumElements(), ",
        "                errors::InvalidArgument(",
        "                    'TensorListScatter expects num_elements to be between -1 and the number of elements in indices tensor; found: ', ",
        "                    num_elements));",
        "    TensorList output_list;\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    Tensor indices = c->input(1);",
        "    PartialTensorShape element_shape;",
        "    OP_REQUIRES(",
        "        c, !TensorShapeUtils::IsMatrixOrHigher(c->input(2).shape()),",
        "        errors::InvalidArgument(",
        "            \"TensorListScatter: element_shape must be at most rank 1 but has \",",
        "            \"the shape of \", c->input(2).shape().DebugString()));",
        "    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(2), &element_shape));",
        "    // TensorListScatterV2 passes the num_elements input, TensorListScatter does",
        "    // not.",
        "    int num_elements = c->num_inputs() >= 4 ? c->input(3).scalar<int>()() : -1;"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bb03fdf4aae944ab2e4b35c7daa051068a8b7f61",
    "API name": "tf.raw_ops.TensorListScatter"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['    OP_REQUIRES(context, max.dim_size(0) == depth && min.dim_size(0) == depth,', '                InvalidArgument(\\\"max and min has incorrect size, expected \\\", depth,', '                                \\\" was \\\", max.dim_size(0), \\\" and \\\", min.dim_size(0)));', '', '    Tensor* grad_wrt_input;', '    OP_REQUIRES_OK(context,', '                   context->allocate_output(0, input.shape(), &grad_wrt_input));', '', '    TensorShape min_max_shape({input.dim_size(input.dims() - 1)});', '    Tensor* grad_wrt_min;', '    OP_REQUIRES_OK(context,', '                   context->allocate_output(1, min_max_shape, &grad_wrt_min));', '', '    Tensor* grad_wrt_max;', '    OP_REQUIRES_OK(context,', '                   context->allocate_output(2, min_max_shape, &grad_wrt_max));']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.",
        "    const Tensor& min = context->input(2);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(min.shape()),",
        "        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "    const Tensor& max = context->input(3);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(max.shape()),",
        "        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));",
        "    OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3cf67ac5705f4f04721d15e485e192bb319feed",
    "API name": "tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient"
},
{
    "Patch Formated": [
        "\"Patch\":\"['params.out_width, params.depth);', '', ' // Assuming qint8 <--> NCHW_VECT_C (int8x4) here.', ' constexpr bool is_int8x4 = std::is_same<T, qint8>::value;', ' OP_REQUIRES(context, (is_int8x4 == (data_format_ == FORMAT_NCHW_VECT_C) && tensor_in.dtype() == DT_QINT8),', ' errors::InvalidArgument(', ' \\\"qint8 should be used with data_format NCHW_VECT_C.\\\"));', '', '#if CUDNN_VERSION >= 7300', ' DnnPoolingOp<T>::Compute(context, se::dnn::PoolingMode::kMaximum, ksize_,', ' stride_, padding_, explicit_paddings_,', ' data_format_, tensor_in, out_shape,', ' propagate_nans_);']\""
    ],
    "Actual Clean Code": [
        "                        params.out_width, params.depth);",
        "",
        "    // Degenerate pooling output should return an empty tensor.",
        "    if (out_shape.num_elements() == 0) {",
        "      Tensor* output = nullptr;",
        "      OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));",
        "      return;",
        "    }",
        "",
        "    // Assuming qint8 <--> NCHW_VECT_C (int8x4) here.",
        "    constexpr bool is_int8x4 = std::is_same<T, qint8>::value;",
        "    OP_REQUIRES(context, (is_int8x4 == (data_format_ == FORMAT_NCHW_VECT_C)),",
        "                errors::InvalidArgument("
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/32d7bd3defd134f21a4e344c8dfd40099aaf6b18",
    "API name": "tf.raw_ops.MaxPool"
},
{
    "Patch Formated": [
        "\"Patch\": \"['                         done);', '', '    if (n == 0 || m == 0) {', '      if (n == m && !compute_uv_ && !full_matrices_) {', '        // S, U, and V are all empty. Nothing to do.', '        done();', '        return;', '      }', '      auto device = context->eigen_device<GPUDevice>();', '      functor::EyeFunctor<GPUDevice, Scalar> eye;', '      if (m > 0) {', '        // Return a full canonical basis for the column space.']\""
    ],
    "Actual Clean Code": [
        "                         done);",
        "",
        "    // If there are zero batches, we are done.",
        "    if (shapeRaw.num_elements() == 0) {",
        "      done();",
        "      return;",
        "    }",
        "",
        "    if (n == 0 || m == 0) {",
        "      if (n == m || !compute_uv_ || !full_matrices_) {",
        "        // S, U, and V are all empty. Nothing to do.",
        "        done();"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/c55b476aa0e0bd4ee99d0f3ad18d9d706cd1260a",
    "API name": "tf.linalg.matrix_rank"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['                OP_REQUIRES(ctx, size_t.dims() == 0,', '                                        errors::InvalidArgument(\\\"Shape must be rank 0 but is rank \\\",', '                                        size_t.dims()));', '    Tidx size = size_t.scalar<Tidx>()();', '    OP_REQUIRES(', '        ctx, size >= 0,', '        errors::InvalidArgument(\\\"size (\\\", size, \\\") must be non-negative\\\"));', '', '    Tensor* out_t;', '    functor::SetZeroFunctor<Device, T> fill;', '    if (data.dims() == 1) {', '      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));', '      auto out = out_t->flat<T>();', '      fill(ctx->eigen_device<Device>(), out);', '      if (binary_output_) {']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",",
        "                                        size_t.dims()));",
        "    OP_REQUIRES(ctx,",
        "                weights.shape() == data.shape() || weights.NumElements() == 0,",
        "                errors::InvalidArgument(",
        "                    \"`weights` must be the same shape as `arr` or a length-0 \"",
        "                    \"`Tensor`, in which case it acts as all weights equal to \"",
        "                    \"1. Received \",",
        "                    weights.shape().DebugString()));",
        "",
        "    Tidx size = size_t.scalar<Tidx>()();",
        "    OP_REQUIRES(",
        "        ctx, size >= 0,",
        "        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bf4c14353c2328636a18bfad1e151052c81d5f43",
    "API name": "tf.raw_ops.DenseBincount"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['    int batch_idx = 0;', '', '    OP_REQUIRES(ctx, splits(0) >= 0,', '                errors::InvalidArgument(', '                    \\\\\\\"Splits should at least start with 0 or greater, not with \\\\\\\",', '                                        splits(0)));', '', '    OP_REQUIRES(ctx, splits(num_rows) == num_values,', '                errors::InvalidArgument(', '                    \\\\\\\"Splits should end with the number of values, not with \\\\\\\",']\" <answer json end>"
    ],
    "Actual Clean Code": [
        "    int batch_idx = 0;",
        "",
        "    OP_REQUIRES(ctx, splits.size() > 0,",
        "                errors::InvalidArgument(\"Splits must be non-empty\"));",
        "",
        "    OP_REQUIRES(ctx, splits(0) == 0,",
        "                errors::InvalidArgument(\"Splits must start with 0, not with \",",
        "                                        splits(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/7a4591fd4f065f4fa903593bc39b2f79530a74b8",
    "API name": "tf.raw_ops.RaggedBincount"
},
{
    "Patch Formated": [
        "\"Patch\": \"OP_REQUIRES(ctx, in_image.dims() == 4 && in_image.dim_size(0) == batch && in_image.dim_size(1) == rows && in_image.dim_size(2) == cols && in_image.dim_size(3) == depth && out_image.dims() == 4 && out_image.dim_size(0) == batch && out_image.dim_size(1) == rows && out_image.dim_size(2) == cols && out_image.dim_size(3) == depth, errors::InvalidArgument(\\\"input_grads, input_image, and out_image should all have the same shape\\\"));\\n\\n  Tensor* output = nullptr;\""
    ],
    "Actual Clean Code": [
        "            in_image.dim_size(2) == cols && in_image.dim_size(3) == depth &&",
        "            out_image.dim_size(0) == batch && out_image.dim_size(1) == rows &&",
        "            out_image.dim_size(2) == cols && out_image.dim_size(3) == depth &&",
        "            out_image.dims() == 4,",
        "        errors::InvalidArgument(",
        "            \"input_grads, input_image, and out_image should have the same \"",
        "            \"shape\"));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bd90b3efab4ec958b228cd7cfe9125be1c0cf255",
    "API name": "tf.raw_ops.LRNGrad"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['shape_tensor.DebugString()));', '    int64_t num_batches = shape_tensor.flat<int64>().size() > 0 ? shape_tensor.flat<int64>()(0) : 0;', '', '    int64_t samples_per_batch = 1;', '    const int64_t num_dims = shape_tensor.dim_size(0);', '    for (int64_t i = 1; i < num_dims; i++) {', '      samples_per_batch *= shape_tensor.flat<int64>().size() > i ? shape_tensor.flat<int64>()(i) : 1;', '    }', '    const int64_t num_elements = num_batches * samples_per_batch;', '', '    // Allocate the output before fudging num_batches and samples_per_batch.', '    auto shape_vec = shape_tensor.flat<int64>();', '    TensorShape tensor_shape;', '    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(', '                            shape_vec.data(), shape_vec.size(), &tensor_shape));', '    Tensor* samples_tensor;', '    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));', '']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "                errors::InvalidArgument(\"Shape tensor must not be empty, got \",",
        "                                        shape_tensor.DebugString()));",
        "    TensorShape tensor_shape;",
        "    OP_REQUIRES_OK(ctx, tensor::MakeShape(shape_tensor, &tensor_shape));",
        "",
        "    int32_t num_batches = tensor_shape.dim_size(0);",
        "    int32_t samples_per_batch = 1;",
        "    const int32_t num_dims = tensor_shape.dims();",
        "    for (int32_t i = 1; i < num_dims; i++) {",
        "      samples_per_batch *= tensor_shape.dim_size(i);",
        "    }",
        "    const int32_t num_elements = num_batches * samples_per_batch;",
        "",
        "    // Allocate the output before fudging num_batches and samples_per_batch.",
        "    Tensor* samples_tensor;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));",
        "",
        "    // Parameters must be 0-d or 1-d."
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/72180be03447a10810edca700cbc9af690dfeb51",
    "API name": "tf.raw_ops.ParameterizedTruncatedNormal"
},
{
    "Patch Formated": [
        "\"Patch\": \"size_t size_bound = ss->ByteSize() + kTensorProtoHeaderBytes + static_cast<size_t>(num_elements * MaxBytesPerElement(DT_INT32)); for (int64_t i = 0; i < num_elements; ++i) { size_bound += static_cast<size_t>(data[i].size()); } if (size_bound > kMaxMessageBytes) { return errors::InvalidArgument( 'Tensor slice is too large to serialize (conservative estimate: ',}\""
    ],
    "Actual Clean Code": [
        "    case DT_BFLOAT16:",
        "    default:",
        "      return 0;",
        "  }",
        "}",
        "",
        "template <>",
        "Status TensorSliceWriter::SaveData(const tstring* data, int64_t num_elements,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/5dd7b86b84a864b834c6fa3d7f9f51c87efa99d4",
    "API name": "tf.raw_ops.Save"
},
{
    "Patch Formated": [
        "\"Patch\": \"",
        "        OP_REQUIRES(",
        "            ctx, (batch >= 0 && batch < out.dimension(0)),",
        "            errors::InvalidArgument(\\\"Index out of bound. `batch` (\\\", batch,",
        "                                    \\\") must be non-negative and less than the dimension size (\\\", ",
        "                                    out.dimension(0), \\\").\\\"));",
        "        OP_REQUIRES(",
        "            ctx, (bin >= 0 && bin < out.dimension(1)), ",
        "            errors::InvalidArgument(\\\"Index out ouf bound. `bin` (\\\", bin,",
        "                                    \\\") must be non-negative and less then the dimension size (\\\", ",
        "                                    out.dimension(1), \\\".\\\"));\""
    ],
    "Actual Clean Code": [
        "      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {",
        "        const int64_t batch = indices_mat(i, 0);",
        "        const Tidx bin = values_flat(i);",
        "        OP_REQUIRES(",
        "            ctx, batch < out.dimension(0),",
        "            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,",
        "                                    \") must be less than the dimension size (\","
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/40adbe4dd15b582b0210dfbf40c243a62f5119fa",
    "API name": "tf.raw_ops.SparseBincount"
},
{
    "Patch Formated": [
        "\"Patch\": \"['        ragged_nested_splits_len);', '    for (int i = 0; i < ragged_nested_splits_len; i++) {', '      OP_REQUIRES(context, ragged_nested_splits_in[i].IsInitialized(), errors::InvalidArgument(\\\"Expected nested splits to have been initialized.\\\"));', '      batched_ragged_input.append_splits(ragged_nested_splits_in[i]);', '    }', '', '    if (!batched_input_) {', '      // Encode as a Scalar Variant Tensor.', '      Tensor* encoded_scalar;', '      OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}),', '                                                       &encoded_scalar));']\""
    ],
    "Actual Clean Code": [
        "        ragged_nested_splits_len);",
        "    for (int i = 0; i < ragged_nested_splits_len; i++) {",
        "      OP_REQUIRES(context, ragged_nested_splits_in[i].dims() == 1,",
        "                  errors::InvalidArgument(\"Requires nested_row_splits[\", i, \"]\",",
        "                                          \" to be rank 1 but is rank \",",
        "                                          ragged_nested_splits_in[i].dims()));",
        "      batched_ragged_input.append_splits(ragged_nested_splits_in[i]);",
        "    }",
        "",
        "    if (!batched_input_) {"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/88f93dfe691563baa4ae1e80ccde2d5c7a143821",
    "API name": "tf.raw_ops.RaggedTensorToVariant"
},
{
    "Patch Formated": [
        "\"Patch\": \"['      CHECK(input_backprop_index >= 0 &&', '            input_backprop_index < num_total_inputs && input_backprop_index < input_backprop_flat.size())', '          << \\\"Invalid input backprop index: \\\" << input_backprop_index << \\\", \\\"', '          << num_total_inputs;', '      input_backprop_flat(input_backprop_index) += out_backprop_flat(index);', '    }', '  }', '', ' private:', '  bool overlapping_;', '};', '']\""
    ],
    "Actual Clean Code": [
        "    for (int index = 0; index < num_total_outputs; ++index) {",
        "      int input_backprop_index = out_arg_max_flat(index);",
        "      OP_REQUIRES(",
        "          context,",
        "          input_backprop_index >= 0 && input_backprop_index < num_total_inputs,",
        "          errors::InvalidArgument(",
        "              \"Invalid input backprop index: \", input_backprop_index, \", \",",
        "              num_total_inputs));",
        "      input_backprop_flat(input_backprop_index) += out_backprop_flat(index);",
        "    }",
        "  }",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/8741e57d163a079db05a7107a7609af70931def4",
    "API name": "tf.raw_ops.FractionalMaxPoolGrad"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"",
        "    'min_as_quantized = std::max(0.0f, min_input);',",
        "    'max_as_quantized = std::min(6.0f, max_input);',",
        "    'output->flat<quint8>().data());',",
        "    '} else {',",
        "    'output->flat<T>().device(context->eigen_cpu_device()) =',",
        "    'input.flat<T>()',",
        "    '    .cwiseMax(min_as_quantized)',",
        "    '    .cwiseMin(max_as_quantized)',",
        "    '    .template cast<T>();',",
        "    '}',",
        "    '',",
        "    'Tensor* output_min = nullptr;',",
        "    'OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));',",
        "    'output_min->flat<float>()(0) = min_as_quantized;',",
        "    'Tensor* output_max = nullptr;',",
        "    'OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));',",
        "    'output_max->flat<float>()(0) = max_as_quantized;',",
        "    '}',",
        "    '};',",
        "    '',",
        "    'REGISTER_KERNEL_BUILDER(Name(\\\"QuantizedRelu\\\")',",
        "    '.Device(DEVICE_CPU)'",
        "\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "  void Compute(OpKernelContext* context) override {",
        "    const Tensor& input = context->input(0);",
        "    const Tensor& min_input_tensor = context->input(1);",
        "    const Tensor& max_input_tensor = context->input(2);",
        "",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),",
        "        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",",
        "                                min_input_tensor.dims()));",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),",
        "        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",",
        "                                max_input_tensor.dims()));",
        "",
        "    const float min_input = min_input_tensor.scalar<float>()();",
        "    const float max_input = max_input_tensor.scalar<float>()();",
        "",
        "    Tensor* output = nullptr;",
        "    OP_REQUIRES_OK(context,",
        "                   context->allocate_output(0, input.shape(), &output));",
        "    const T min_as_quantized = FloatToQuantized<T>(0.0f, min_input, max_input);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/49b3824d83af706df0ad07e4e677d88659756d89",
    "API name": "tf.raw_ops.QuantizedRelu6"
},
{
    "Patch Formated": [
        "\"Patch\":\"void Compute(OpKernelContext* ctx) override {\\n\\",
        "    const Tensor& input = ctx->input(0);\\n\\",
        "    OP_REQUIRES(ctx, input.NumElements() <= tensorflow::kint32max, errors::InvalidArgument('Too many elements in tensor'));\\n\\",
        "    const float input_min_float = ctx->input(1).flat<float>()(0);\\n\\",
        "    const float input_max_float = ctx->input(2).flat<float>()(0);\\n\\",
        "    Tensor* output = nullptr;\\n\\",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\\n\\",
        "    Tensor* output_min = nullptr;\\n\\",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\\n\\",
        "    Tensor* output_max = nullptr;\\n\\",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));\\n\\",
        "    auto input_array = input.flat<T1>();\\n\\",
        "    const int32_t input_lowest_quantized = static_cast<int32>(Eigen::NumTraits<T1>::lowest());\\n\\",
        "    const int32_t input_highest_quantized = static_cast<int32>(Eigen::NumTraits<T1>::highest());\\n\\",
        "    T1 actual_min_quantized = input_highest_quantized;\\n\\",
        "    T1 actual_max_quantized = input_lowest_quantized;\\n\\",
        "    for (int i = 0; i < input_array.size(); ++i) {\\n\\",
        "    OP_REQUIRES(ctx, (i >= input_lowest_quantized) && (i <= input_highest_quantized), errors::InvalidArgument('index out of range'));\\n}\""
    ],
    "Actual Clean Code": [
        "  void Compute(OpKernelContext* ctx) override {",
        "    const Tensor& input = ctx->input(0);",
        "    const Tensor& input_min = ctx->input(1);",
        "    const Tensor& input_max = ctx->input(2);",
        "",
        "    OP_REQUIRES(",
        "        ctx, TensorShapeUtils::IsScalar(input_min.shape()),",
        "        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",",
        "                                input_min.dims()));",
        "    OP_REQUIRES(",
        "        ctx, TensorShapeUtils::IsScalar(input_max.shape()),",
        "        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",",
        "                                input_max.dims()));",
        "",
        "    const float input_min_float = input_min.scalar<float>()();",
        "    const float input_max_float = input_max.scalar<float>()();",
        "    Tensor* output = nullptr;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));",
        "    Tensor* output_min = nullptr;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/73ad1815ebcfeb7c051f9c2f7ab5024380ca8613",
    "API name": "tf.raw_ops.QuantizeDownAndShrinkRange"
},
{
    "Patch Formated": [
        "{\"Patch\": \"const float min_a = context->input(2).flat<float>()(0);\\nconst float max_a = context->input(3).flat<float>()(0);\\nconst float min_b = context->input(4).flat<float>()(0);\\nconst float max_b = context->input(5).flat<float>()(0);\\n\\n// Make sure that we have valid quantization ranges for the input buffers.\\n// If the difference between the min and max is negative or zero, it makes\\n// it hard to do meaningful intermediate operations on the values.\\nOP_REQUIRES(context, (max_a > min_a),\\n            errors::InvalidArgument('max_a must be larger than min_a.'));\\nOP_REQUIRES(context, (max_b > min_b),\\n            errors::InvalidArgument('max_b must be larger than min_b.'));\\nconst int32_t offset_a = FloatToQuantizedUnclamped<T1>(0.0f, min_a, max_a);\\nconst int32_t offset_b = FloatToQuantizedUnclamped<T2>(0.0f, min_b, max_b);\\nconst int32_t offset_c = 0;\\nconst int32_t mult_c = 1;\\nconst int32_t shift_c = 0;\\n\\n// Check that the dimensions of the two matrices are valid.\\nOP_REQUIRES(context, TensorShapeUtils::IsMatrix(a.shape()),\\n            errors::InvalidArgument('In[0] is not a matrix'));\\nOP_REQUIRES(context, TensorShapeUtils::IsMatrix(b.shape()),\\n            errors::InvalidArgument('In[1] is not a matrix'));\\n\"}"
    ],
    "Actual Clean Code": [
        "    const Tensor& a = context->input(0);",
        "    const Tensor& b = context->input(1);",
        "    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(2).shape()),",
        "                errors::InvalidArgument(\"min_a must be a scalar, but got shape\",",
        "                                        context->input(2).shape()));",
        "    const float min_a = context->input(2).flat<float>()(0);",
        "    OP_REQUIRES(context, context->input(3).NumElements() == 1,",
        "                errors::InvalidArgument(\"max_a must be a scalar, but got shape\",",
        "                                        context->input(3).shape()));",
        "    const float max_a = context->input(3).flat<float>()(0);",
        "    OP_REQUIRES(context, context->input(4).NumElements() == 1,",
        "                errors::InvalidArgument(\"min_b must be a scalar, but got shape\",",
        "                                        context->input(4).shape()));",
        "    const float min_b = context->input(4).flat<float>()(0);",
        "    OP_REQUIRES(context, context->input(5).NumElements() == 1,",
        "                errors::InvalidArgument(\"max_b must be a scalar, but got shape\",",
        "                                        context->input(5).shape()));",
        "    const float max_b = context->input(5).flat<float>()(0);",
        "",
        "    // Make sure that we have valid quantization ranges for the input buffers.",
        "    // If the difference between the min and max is negative or zero, it makes"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/aca766ac7693bf29ed0df55ad6bfcc78f35e7f48",
    "API name": "tf.raw_ops.QuantizedMatMul"
},
{
    "Patch Formated": [
        "\"Patch\": \"['                                \\\" was \\\", max.dim_size(0)));', '', '    Tensor* output;', '    OP_REQUIRES_OK(context,', '                   context->allocate_output(0, input.shape(), &output));', '', '    FakeQuantWithMinMaxVarsPerChannelFunctor<Device> functor;', '    functor(context->eigen_device<Device>(), input.flat_inner_dims<float, 2>(),', '            min.vec<float>().size() > 0 ? min.vec<float>() : NULL, max.vec<float>().size() > 0 ? max.vec<float>() : NULL, quant_min_, quant_max_,', '            output->flat_inner_dims<float, 2>());', '  }', '', ' private:', '  int quant_min_;', '  int quant_max_;', '};', '']\""
    ],
    "Actual Clean Code": [
        "    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.",
        "    const Tensor& min = context->input(1);",
        "    const Tensor& max = context->input(2);",
        "",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(min.shape()),",
        "        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(max.shape()),",
        "        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));",
        "    OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/785d67a78a1d533759fcd2f5e8d6ef778de849e0",
    "API name": "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel"
},
{
    "Patch Formated": [
        "\"Patch\": \"int64 input_dims = -1;\\nOP_REQUIRES_OK(context,\\nConv2DBackpropComputeInputShape(input_sizes, filter.shape(),\\nout_backprop.shape(),\\ndata_format_, &input_shape, &input_dims));\\n\\nOP_REQUIRES(context, input_dims == 4,\\nerrors::InvalidArgument(\\\"input must be 4-dimensional\\\"));\\n\\nConvBackpropDimensions dims;\\nOP_REQUIRES_OK(context,\\nConvBackpropComputeDimensionsV2(\\n\\\"Conv2DCustomBackpropInput\\\", /*num_spatial_dims=*/2,\\ninput_shape, filter.shape(), out_backprop.shape(),\\n/*dilations=*/{1, 1, 1, 1}, strides_, padding_));\""
    ],
    "Actual Clean Code": [
        "    const Tensor& filter = context->input(1);",
        "    const Tensor& out_backprop = context->input(2);",
        "    OP_REQUIRES(",
        "        context, out_backprop.dims() == 4,",
        "        errors::InvalidArgument(\"input_sizes must be 4-dimensional, got: \",",
        "                                out_backprop.dims()));",
        "",
        "    TensorShape input_shape;",
        "    OP_REQUIRES_OK(context,",
        "                   Conv2DBackpropComputeInputShape(input_sizes, filter.shape(),"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/50156d547b9a1da0144d7babe665cf690305b33c",
    "API name": "tf.raw_ops.Conv2DBackpropInput"
},
{
    "Patch Formated": [
        "\"Patch\": \"['    auto shape_vec = tensor_in_shape.vec<int32>();', '    for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {', '      if (i < shape_vec.size()) {', '          output_shape.AddDim(shape_vec(i));',  '       }', '    }', '    if (output_shape.num_elements() == 0) {', '      Tensor* output = nullptr;', '      OP_REQUIRES_OK(context,\""
    ],
    "Actual Clean Code": [
        "    auto shape_vec = tensor_in_shape.vec<int32>();",
        "    for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {",
        "      OP_REQUIRES_OK(context, output_shape.AddDimWithStatus(shape_vec(i)));",
        "    }",
        "    if (output_shape.num_elements() == 0) {",
        "      Tensor* output = nullptr;",
        "      OP_REQUIRES_OK(context,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/3a6ac52664c6c095aa2b114e742b0aa17fdce78f",
    "API name": "tf.raw_ops.AvgPoolGrad"
},
