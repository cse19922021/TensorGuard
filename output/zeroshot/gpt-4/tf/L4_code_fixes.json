{
    "Patch Formated": [
        "\"Patch\": \"['    }', '    OP_REQUIRES(', '        context, pooling_ratio_[0] > 0 && pooling_ratio_[0] <= 1 && pooling_ratio_[3] > 0 && pooling_ratio_[3] <= 1,', '        errors::Unimplemented(\\\"Fractional average pooling is not yet \\\"', '                              \\\"supported on the batch nor channel dimension.\\\"));', '    OP_REQUIRES_OK(context, context->GetAttr(\\\"deterministic\\\", &deterministic_));', '    OP_REQUIRES_OK(context, context->GetAttr(\\\"seed\\\", &seed_));']\""
    ],
    "Actual Clean Code": [
        "    }",
        "    OP_REQUIRES(",
        "        context, pooling_ratio_[0] == 1 && pooling_ratio_[3] == 1,",
        "        errors::Unimplemented(\"Fractional average pooling is not yet \"",
        "                              \"supported on the batch nor channel dimension.\"));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));",
        "    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/ee50d1e00f81f62a4517453f721c634bbb478307",
    "API name": "tensorflow.python.ops.nn_ops.fractional_avg_pool_v2"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"",
        "      DimensionHandle unused;",
        "      for (int i = 0; i < c->num_inputs(); ++i) {",
        "        ShapeHandle input_shape = c->input(i);",
        "        if (!c->RankKnown(input_shape) || c->Rank(input_shape) < 1) {",
        "           return errors::InvalidArgument('Rank of tensor is less than 1');",
        "         } ",
        "        if (!c->WithValue(c->Dim(input_shape, 0), 1, &unused).ok()) {",
        "          return errors::InvalidArgument('Size of first dimension must be 1.');",
        "        }",
        "        TF_RETURN_WITH_CONTEXT_IF_ERROR(c->Merge(input_shape, cur, &cur),",
        "                                        'From merging shape ', i, ",
        "                                        ' with other shapes.');",
        "      }",
        "      c->set_output(0, passed_shape);",
        "\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "              \"All input shapes must be fully defined.\");",
        "        }",
        "        if (c->Rank(c->input(i)) < 1) {",
        "          return errors::InvalidArgument(",
        "              \"The rank of all input shapes must be greater than 0, \"",
        "              \"but input \",",
        "              i, \" had rank \", c->Rank(c->input(i)), \".\");",
        "        }",
        "        DimensionHandle unused;",
        "        if (!c->WithValue(c->Dim(c->input(i), 0), 1, &unused).ok()) {",
        "          return errors::InvalidArgument(\"Size of first dimension must be 1.\");",
        "        }"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/da66bc6d5ff466aee084f9e7397980a24890cd15",
    "API name": "tf.raw_ops.ParallelConcat"
},
{
    "Patch Formated": [
        "\"Patch\":\"['                  errors::Unimplemented(\\\"All lengths have to be the same\\\"));', '    }', '    OP_REQUIRES(', '        ctx, element_dims[0] % length == 0,', '        errors::Unimplemented(\\\"Buffer size has to be a multiple of length\\\"));', '    OP_REQUIRES(ctx, length > 0, errors::InvalidArgument(\\\"length must be greater than 0\\\"));', '    std::vector<int64_t> new_dims = {element_dims[0] / length, length};', '    for (int i = 1; i < element_dims.size(); i++) {', '      new_dims.push_back(element_dims[i]);']\""
    ],
    "Actual Clean Code": [
        "                  errors::Unimplemented(\"All lengths have to be the same\"));",
        "    }",
        "    OP_REQUIRES(ctx, length,",
        "                errors::Unimplemented(\"All lengths must be positive\"));",
        "    OP_REQUIRES(",
        "        ctx, element_dims[0] % length == 0,",
        "        errors::Unimplemented(\"Buffer size has to be a multiple of length\"));",
        "    std::vector<int64_t> new_dims = {element_dims[0] / length, length};"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/728113a3be690facad6ce436660a0bc1858017fa",
    "API name": "tf.raw_ops.RandomShuffle"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "    \"Patch\": \"['      ShapeHandle keys;', '      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(1), 1, &keys));', '      DimensionHandle unused;', '      TF_RETURN_IF_ERROR(', '          c->Merge(c->Dim(keys, 0), c->Dim(c->input(2), 0), &unused));', '      return OkStatus();', '    });', '', 'Status MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,', '                             const ShapeHandle& value) {']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "      ShapeHandle keys;",
        "      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &keys));",
        "      ShapeHandle values;",
        "      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(2), 1, &values));",
        "      DimensionHandle unused;",
        "      TF_RETURN_IF_ERROR(c->Merge(c->Dim(keys, 0), c->Dim(values, 0), &unused));",
        "      return OkStatus();",
        "    });",
        "",
        "Status MutableHashTableShape(InferenceContext* c, const ShapeHandle& key,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/980b22536abcbbe1b4a5642fc940af33d8c19b69",
    "API name": "tf.raw_ops.LookupTableImportV2"
},
{
    "Patch Formated": [
        "\"Patch\": \"if (ksize[i] <= 0) {\\\\n      return errors::InvalidArgument('For AvgPoolGrad, ksize[', i, '] has to be positive\\\\n    }\""
    ],
    "Actual Clean Code": [
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateKernelSizes(ksize_));",
        "    OP_REQUIRES(ctx, stride_.size() == num_dims(),",
        "                errors::InvalidArgument(\"Sliding window strides field must \"",
        "                                        \"specify \",",
        "                                        num_dims(), \" dimensions\"));",
        "    OP_REQUIRES_OK(ctx, ValidateStrides(stride_));",
        "",
        "    const TensorShape tensor_in_shape = ctx->InputShape(0);",
        "    const TensorShape tensor_out_shape = ctx->InputShape(1);",
        "    const TensorShape out_backprop_shape = ctx->InputShape(2);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/1295ae4dbb52fe06b19733b0257e2340d7b63b8d",
    "API name": "tf.raw_ops.AvgPoolGrad"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"REGISTER_KERNEL_BUILDER(                                                     \\\\",
        "      Name(\\\"QuantizeAndDequantizeV3\\\").Device(DEVICE_GPU).HostMemory(\\\"num_bits\\\").TypeConstraint<tensorflow::int32>(\\\"num_bits\\\").TypeConstraint<T>(\\\"T\\\"), ",
        "      QuantizeAndDequantizeV3Op<GPUDevice, T>);\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV3\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .HostMemory(\"num_bits\")                          \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV3Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4\")                      \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV2Op<GpuDevice, T>);            \\",
        "  REGISTER_KERNEL_BUILDER(Name(\"QuantizeAndDequantizeV4Grad\")                  \\",
        "                              .Device(DEVICE_GPU)                              \\",
        "                              .HostMemory(\"input_min\")                         \\",
        "                              .HostMemory(\"input_max\")                         \\",
        "                              .TypeConstraint<T>(\"T\"),                         \\",
        "                          QuantizeAndDequantizeV4GradientOp<GpuDevice, T>);    \\",
        "  REGISTER_KERNEL_BUILDER(                                                     \\",
        "      Name(\"QuantizeAndDequantize\").Device(DEVICE_GPU).TypeConstraint<T>(\"T\"), \\",
        "      QuantizeAndDequantizeOp<GpuDevice, T>);",
        "TF_CALL_float(REGISTER_GPU_KERNEL);",
        "TF_CALL_double(REGISTER_GPU_KERNEL);",
        "#undef REGISTER_GPU_KERNEL",
        "#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3f9cb38ecfe5a8a703f2c4a8fead434ef291713",
    "API name": "tf.raw_ops.QuantizeAndDequantizeV3"
},
{
    "Patch Formated": [
        "{\"Patch\": \"['    OP_REQUIRES(context, max.dim_size(0) == depth,', '                InvalidArgument(\\\"max has incorrect size, expected \\\", depth,', '                                \\\" was \\\", max.dim_size(0)));\\n', '    OP_REQUIRES(context, min.dim_size(0) == 0,', '                InvalidArgument(\\\"min has incorrect rank, expected 0,\\\",', '                                \\\" was \\\", min.dim_size(0)));\\n', '    Tensor* grad_wrt_input;', '    OP_REQUIRES_OK(context,', '                   context->allocate_output(0, input.shape(), &grad_wrt_input));\\n', '    TensorShape min_max_shape({input.dim_size(input.dims() - 1)});', '    Tensor* grad_wrt_min;', '    OP_REQUIRES_OK(context,', '                   context->allocate_output(1, min_max_shape, &grad_wrt_min));\\n', '    Tensor* grad_wrt_max;', '    OP_REQUIRES_OK(context,', '                   context->allocate_output(2, min_max_shape, &grad_wrt_max));']\"}"
    ],
    "Actual Clean Code": [
        "    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.",
        "    const Tensor& min = context->input(2);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(min.shape()),",
        "        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "    const Tensor& max = context->input(3);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(max.shape()),",
        "        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));",
        "    OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3cf67ac5705f4f04721d15e485e192bb319feed",
    "API name": "tf.quantization.fake_quant_with_min_max_vars_gradient"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"    const int64_t samples_per_alpha = samples_shape.num_elements(); ",
        "    ",
        "    // Check for size.",
        "    CHECK_LT(samples_per_alpha, INT_MAX) << \\\"Sample size is too large!\\\";",
        "",
        "    samples_shape.AppendShape(alpha_t.shape()); ",
        "",
        "    // Allocate output samples. ",
        "    Tensor* samples_t = nullptr; ",
        "",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, samples_shape, &samples_t)); \"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    const int64_t samples_per_alpha = samples_shape.num_elements();",
        "",
        "    OP_REQUIRES_OK(ctx, samples_shape.AppendShapeWithStatus(alpha_t.shape()));",
        "    // Allocate output samples.",
        "    Tensor* samples_t = nullptr;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, samples_shape, &samples_t));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/552bfced6ce4809db5f3ca305f60ff80dd40c5a3",
    "API name": "tf.random.gamma"
},
{
    "Patch Formated": [
        "\"Patch\": \"['', '    if (!context->input(2).dims() == 0) {', '        return errors::InvalidArgument(\\\"Expected scalar for argument 2, but got something else.\\\");', '    }', '    const int64_t batch_key = context->input(2).scalar<int64_t>()();', '    const bool nonempty_input = batch_index_t.dim_size(0) > 0;', '', '    // If we have a non-empty tensor, slice it up.', '    // (It is important to do this outside of the critical section below.)', '    // The following variables are populated iff nonempty_input==true.', '    std::vector<int64_t> sizes;', '    std::vector<int64_t> batch_keys;', '    std::vector<Tensor> split_inputs;', '    if (nonempty_input) {', '      auto batch_indices =']\""
    ],
    "Actual Clean Code": [
        "    }",
        "",
        "    if (!TensorShapeUtils::IsScalar(context->input(2).shape())) {",
        "      return errors::InvalidArgument(",
        "          \"Input id should be scalar; \"",
        "          \"Got: \",",
        "          context->input(2).DebugString(), \".\");",
        "    }",
        "    const int64_t batch_key = context->input(2).scalar<int64_t>()();",
        "    const bool nonempty_input = batch_index_t.dim_size(0) > 0;",
        "",
        "    // If we have a non-empty tensor, slice it up."
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/4419d10d576adefa36b0e0a9425d2569f7c0189f",
    "API name": "tf.raw_ops.Unbatch"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['for (int64_t b = 0; b < batch_size; ++b) {', 'const int64_t num_boxes = boxes.dim_size(1);', 'const auto tboxes = boxes.tensor<T, 3>();', 'for (int64_t bb = 0; bb < num_boxes; ++bb) {', 'int64_t color_index = bb % color_table.size();', 'const auto min_box_row =', 'static_cast<float>(tboxes(b, bb, 0)) * (height - 1);']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    for (int64_t b = 0; b < batch_size; ++b) {",
        "      const int64_t num_boxes = boxes.dim_size(1);",
        "      const auto tboxes = boxes.tensor<float, 3>();",
        "      for (int64_t bb = 0; bb < num_boxes; ++bb) {",
        "        int64_t color_index = bb % color_table.size();",
        "        const int64_t min_box_row =",
        "            static_cast<float>(tboxes(b, bb, 0)) * (height - 1);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/da0d65cdc1270038e72157ba35bf74b85d9bda11",
    "API name": "tf.raw_ops.DrawBoundingBoxes"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\":\"void LinearAlgebraOp<InputScalar, OutputScalar>::ComputeTensorSlice(\\n    OpKernelContext* context, int64_t matrix_index, const TensorInputs& inputs,\\n    const TensorShapes& input_matrix_shapes, const TensorOutputs& outputs,\\n    const TensorShapes& output_matrix_shapes) {\\n\\n  CHECK(MathUtil::GCD<int64>(sizeof(InputScalar), Eigen::kManualAlignment) == 1)\\n      << 'Eigen tensors have not been allocated with proper alignment. Please see '\\n      << 'http://eigen.tuxfamily.org/dox-devel/group__TopicUnalignedArrayAssert.html '\\n      << 'for details.';\\n\\n  InputConstMatrixMaps matrix_inputs;\\n  for (size_t i = 0; i < inputs.size(); ++i) {\\n    // Handle alignment. Eigen::Map is\\n    // unaligned by default.\\n    matrix_inputs.emplace_back(\\n        inputs[i]->flat<InputScalar>().data() + \\n            matrix_index * input_matrix_shapes[i].num_elements(), \\n        input_matrix_shapes[i].dimensions());\\n  }\\n}\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "                                  output_idx, output_tensor_shape, &out));",
        "    }",
        "    OP_REQUIRES(",
        "        context, out->dtype() == DataTypeToEnum<OutputScalar>::v(),",
        "        errors::InvalidArgument(\"Invalid output dtype \", out->dtype(), \" vs \",",
        "                                DataTypeToEnum<OutputScalar>::v()));",
        "",
        "    outputs->emplace_back(out);",
        "  }",
        "}",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/aed36912609fc07229b4d0a7b44f3f48efc00fd0",
    "API name": "tf.raw_ops.Eig"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "    \"Patch\":\"",
        "    OP_REQUIRES_OK(",
        "        context,",
        "        GetWindowedOutputSizeVerboseV2(",
        "            dims.spatial_dims[0].input_size, dims.spatial_dims[0].filter_size,",
        "            dims.spatial_dims[0].stride, padding_,",
        "            &dims.spatial_dims[0].output_size, &pad_top, &pad_bottom));",
        "    OP_REQUIRES_OK(",
        "        context,",
        "        GetWindowedOutputSizeVerboseV2(",
        "            dims.spatial_dims[1].input_size, dims.spatial_dims[1].filter_size,",
        "            dims.spatial_dims[1].stride, padding_,",
        "            &dims.spatial_dims[1].output_size, &pad_left, &pad_right));",
        "    ",
        "    if (dims.spatial_dims[0].output_size > 0 && dims.spatial_dims[1].output_size > 0) {",
        "        if (pad_left == pad_right && pad_top == pad_bottom) {",
        "            if (LaunchXsmmBackwardInputConvolution<Device, T>()(",
        "                        context, context->eigen_device<Device>(),",
        "                        in_backprop->tensor<T, 4>(), filter.tensor<T, 4>(),",
        "                        ...",
        "        }",
        "    }\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    }",
        "",
        "    // If shapes are valid but `out_backprop` is empty, in_backprop should be",
        "    // set to all zeros.  Otherwise, cudnn/dnnl fail with an empty input.",
        "    if (out_backprop.NumElements() == 0) {",
        "      functor::SetZeroFunctor<Device, T> set_zero;",
        "      set_zero(context->eigen_device<Device>(),",
        "               in_backprop->template flat<T>());",
        "      return;",
        "    }",
        "",
        "// TODO(ezhulenev): Remove custom kernel and move XSMM support to",
        "// LaunchConv2DBackpropInputOp functor.",
        "#if defined TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS && \\",
        "    defined TENSORFLOW_USE_LIBXSMM_BACKWARD_CONVOLUTIONS"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/27a65a43cf763897fecfa5cdb5cc653fc5dd0346",
    "API name": "tf.raw_ops.Conv2DBackpropInput"
},
{
    "Patch Formated": [
        "\"Patch\": \"",
        "if (t.NumElements() != 1) {",
        "  return errors::InvalidArgument(",
        "  'The only valid scalar shape tensor is the fully unknown shape ',",
        "  'specified as -1.');",
        "}",
        "",
        "if (t.dtype() == DT_INT32) {",
        "  return PartialTensorShape::MakePartialShape(t.vec<int32>().data(),",
        "                                              t.NumElements(), out);",
        "} else if (t.dtype() == DT_INT64) {",
        "  return PartialTensorShape::MakePartialShape(t.vec<int64_t>().data(),",
        "                                              t.NumElements(), out);",
        "}",
        "\""
    ],
    "Actual Clean Code": [
        "        \"The only valid scalar shape tensor is the fully unknown shape \"",
        "        \"specified as -1.\");",
        "  } else if (t.shape().dims() != 1) {",
        "    return errors::InvalidArgument(\"Shape must be at most rank 1 but is rank \",",
        "                                   t.shape().dims());",
        "  }",
        "  if (t.dtype() == DT_INT32) {",
        "    return PartialTensorShape::MakePartialShape(t.vec<int32>().data(),",
        "                                                t.NumElements(), out);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/c8ba76d48567aed347508e0552a257641931024d",
    "API name": "tf.raw_ops.EmptyTensorList"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"if (!sep_t->dims().empty()) {",
        "      errors::InvalidArgument('Input separator should be a scalar, not a shape ', ",
        "      sep_t->shape().DebugString());",
        "    }",
        "",
        "    const tstring separator = sep_t->scalar<tstring>()();",
        "",
        "    std::vector<std::unique_ptr<ColumnInterface<tstring>>> columns =",
        "        GenerateColumnsFromInput<tstring>(indices_list_in, values_list_in,",
        "                                          shapes_list_in, dense_list_in);",
        "",
        "    Tensor* indices_out;",
        "    Tensor* values_out;",
        "    Tensor* shape_out;",
        "",
        "    const int64_t batch_size = CalculateBatchSize(shapes_list_in, dense_list_in);\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    const Tensor* sep_t;",
        "    OP_REQUIRES_OK(context, context->input(\"sep\", &sep_t));",
        "    OP_REQUIRES(context, TensorShapeUtils::IsScalar(sep_t->shape()),",
        "                errors::InvalidArgument(\"Input separator should be a scalar. \"",
        "                                        \"Received: \",",
        "                                        sep_t->DebugString()));",
        "    const tstring separator = sep_t->scalar<tstring>()();",
        "",
        "    std::vector<std::unique_ptr<ColumnInterface<tstring>>> columns =",
        "        GenerateColumnsFromInput<tstring>(indices_list_in, values_list_in,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/83dcb4dbfa094e33db084e97c4d0531a559e0ebf",
    "API name": "tf.sparse.cross"
},
{
    "Patch Formated": [
        "\"Patch\": \"\\n#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS\\n    if (params_.padding != EXPLICIT && !input.empty() &&\\n        LaunchXsmmConvOp<Device, T>::Run(\\n            context, input, filter, dimensions.batch, dimensions.input_rows,\\n            dimensions.input_cols, dimensions.in_depth, dimensions.filter_rows,\\n            dimensions.filter_cols, dimensions.pad_rows_before,\\n            dimensions.pad_cols_before, dimensions.out_rows,\\n            dimensions.out_cols, dimensions.out_depth, dimensions.dilation_rows,\\n            dimensions.dilation_cols, dimensions.stride_rows,\\n            dimensions.stride_cols, output, params_.data_format)) {\\n      return;\\n    }\\n#endif\\n\""
    ],
    "Actual Clean Code": [
        "    }",
        "",
        "    // If the input is empty, result can only be due to padding.",
        "    if (input.NumElements() == 0) {",
        "      // Zero-out output and return.",
        "      functor::SetZeroFunctor<Device, T>()(context->eigen_device<Device>(),",
        "                                           output->template flat<T>());",
        "",
        "      return;",
        "    }",
        "",
        "#ifdef TENSORFLOW_USE_LIBXSMM_CONVOLUTIONS",
        "    if (params_.padding != EXPLICIT &&",
        "        LaunchXsmmConvOp<Device, T>::Run(",
        "            context, input, filter, dimensions.batch, dimensions.input_rows,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/611d80db29dd7b0cfb755772c69d60ae5bca05f9",
    "API name": "tf.raw_ops.Conv2D"
},
{
    "Patch Formated": [
        "{\"Patch\": \"['    if (!has_sample_rate_attr_) {', '      const Tensor& sample_rate_tensor = c->input(2);', '      OP_REQUIRES(c, TensorShapeUtils::IsScalar(sample_rate_tensor.shape()),', '                errors::InvalidArgument(\\\"sample_rate must be a scalar\\\"));', '      sample_rate = sample_rate_tensor.scalar<float>()();', '    }', '    OP_REQUIRES(c, sample_rate > 0.0f,', '                errors::InvalidArgument(\\\"sample_rate must be > 0\\\"));', '', '    const int batch_size = tensor.dim_size(0);', '    const int64_t length_frames = tensor.dim_size(1);', '    const int64_t num_channels =', '        tensor.dims() == 2 ? 1 : tensor.dim_size(tensor.dims() - 1);']\"}"
    ],
    "Actual Clean Code": [
        "    if (!has_sample_rate_attr_) {",
        "      const Tensor& sample_rate_tensor = c->input(2);",
        "      OP_REQUIRES(c,",
        "                  sample_rate_tensor.IsAligned() &&",
        "                      sample_rate_tensor.NumElements() == 1,",
        "                  errors::InvalidArgument(",
        "                      \"sample_rate must be rank-0 or contain a single value\"));",
        "      sample_rate = sample_rate_tensor.scalar<float>()();",
        "    }",
        "    OP_REQUIRES(c, sample_rate > 0.0f,",
        "                errors::InvalidArgument(\"sample_rate must be > 0\"));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bf6b45244992e2ee543c258e519489659c99fb7f",
    "API name": "tf.raw_ops.AudioSummaryV2"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['                        DoneCallback done) override {', '    auto output_shape = c->input(0).shape();', '    if(!output_shape.dims())', '       return errors::InvalidArgument(\\\"Input tensor should have rank > 0\\\");', '    output_shape.set_dim(', '        0, output_shape.dim_size(0) * col_params_->group.group_size);', '    col_params_->instance.shape = output_shape;', '', '    // Allocate output on the first pass through this function.  This must be', '    // done immediately, while we\\'re still in the executor thread.  Otherwise', '    // the memory is not guaranteed to be unused by any concurrently executing', '    // GPU kernel.']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "                        DoneCallback done) override {",
        "    auto output_shape = c->input(0).shape();",
        "    OP_REQUIRES_ASYNC(c, output_shape.dims() > 0,",
        "                      errors::InvalidArgument(\"input should have rank > 0, \",",
        "                                              \"recieved \", output_shape.dims()),",
        "                      done);",
        "    output_shape.set_dim(",
        "        0, output_shape.dim_size(0) * col_params_->group.group_size);",
        "    col_params_->instance.shape = output_shape;",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/c1f491817dec39a26be3c574e86a88c30f3c4770",
    "API name": "tf.raw_ops.CollectiveGather"
},
{
    "Patch Formated": [
        "\"Patch\": \"['  // Assume row-major order.', '  TensorShape shape;', '  auto shape_tensor = ctx->input(base_index + 2);', '  if (!shape_tensor.dims() == 1) {', '    return errors::InvalidArgument(\\\"Shape must be a 1D tensor\\\");', '  }', '  TF_RETURN_IF_ERROR(TensorShape::BuildTensorShape(', '      shape_tensor.vec<int64_t>(), &shape));', '  CheckRankAtLeast2(ctx, shape);', '  std::vector<int64_t> order(shape.dims());', '  std::iota(order.begin(), order.end(), 0);', '', '  Status status = sparse::SparseTensor::Create(', '      ctx->input(base_index), ctx->input(base_index + 1), shape, order, tensor);', '', '  if (!validate_indices || !status.ok()) return status;']\""
    ],
    "Actual Clean Code": [
        "  // Assume row-major order.",
        "  TensorShape shape;",
        "  const Tensor& shape_tensor = ctx->input(base_index + 2);",
        "  if (shape_tensor.dims() != 1) {",
        "    return errors::InvalidArgument(\"Shape must be a 1D tensor.\");",
        "  }",
        "  TF_RETURN_IF_ERROR(",
        "      TensorShape::BuildTensorShape(shape_tensor.vec<int64_t>(), &shape));",
        "  CheckRankAtLeast2(ctx, shape);",
        "  std::vector<int64_t> order(shape.dims());",
        "  std::iota(order.begin(), order.end(), 0);",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/cf70b79d2662c0d3c6af74583641e345fc939467",
    "API name": "tf.raw_ops.SetSize"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['    OP_REQUIRES_OK(c, c->allocate_output(0, {}, &output_tensor, attr));', '    PartialTensorShape element_shape;', '    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(1), &element_shape));', '    TensorList output_list;', '    const Tensor& t = c->input(0);', '    output_list.element_dtype = t.dtype();', '    OP_REQUIRES(c, TensorShapeUtils::IsVectorOrHigher(t.shape()),', '                errors::InvalidArgument(', '                    \\\"Tensor must be at least a vector, but saw shape: \\\",', '                    t.shape().DebugString()));', ' OP_REQUIRES(c, TensorShapeUtils::IsScalarOrVector(element_shape),', '                errors::InvalidArgument(', '                    \\\"element_shape must be a scalar or a vector, but got: \\\",', '                    element_shape.DebugString()));', '    TensorShape output_shape(t.shape());']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    OP_REQUIRES_OK(c, c->allocate_output(0, {}, &output_tensor, attr));",
        "    PartialTensorShape element_shape;",
        "    OP_REQUIRES(",
        "        c, !TensorShapeUtils::IsMatrixOrHigher(c->input(1).shape()),",
        "        errors::InvalidArgument(",
        "            \"TensorListFromTensor: element_shape must be at most rank 1 but \",",
        "            \"has the shape of \", c->input(1).shape().DebugString()));",
        "    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(1), &element_shape));",
        "    TensorList output_list;",
        "    const Tensor& t = c->input(0);",
        "    output_list.element_dtype = t.dtype();"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/3db59a042a38f4338aa207922fa2f476e000a6ee",
    "API name": "tf.raw_ops.TensorListFromTensor"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\":",
        "\"{",
        "  'fix_code': [",
        "    'Tensor indices = c->input(1);',",
        "    'PartialTensorShape element_shape;',",
        "    'OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(2), &element_shape));',",
        "    'OP_REQUIRES(c, element_shape.dims() <= 1, errors::InvalidArgument(',",
        "            '\\\"element_shape must be at most rank 1, found: \\\", element_shape.dims()));',",
        "    '// TensorListScatterV2 passes the num_elements input, TensorListScatter does not.',",
        "    'int num_elements = c->num_inputs() >= 4 ? c->input(3).scalar<int>()() : -1;',",
        "    'OP_REQUIRES(c, num_elements >= -1,',",
        "                'errors::InvalidArgument(',",
        "                    '\\\"TensorListScatter expects num_elements >= -1, found: \\\",',",
        "                    'num_elements));',",
        "    'TensorList output_list;'",
        "  ]",
        "}\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    Tensor indices = c->input(1);",
        "    PartialTensorShape element_shape;",
        "    OP_REQUIRES(",
        "        c, !TensorShapeUtils::IsMatrixOrHigher(c->input(2).shape()),",
        "        errors::InvalidArgument(",
        "            \"TensorListScatter: element_shape must be at most rank 1 but has \",",
        "            \"the shape of \", c->input(2).shape().DebugString()));",
        "    OP_REQUIRES_OK(c, TensorShapeFromTensor(c->input(2), &element_shape));",
        "    // TensorListScatterV2 passes the num_elements input, TensorListScatter does",
        "    // not.",
        "    int num_elements = c->num_inputs() >= 4 ? c->input(3).scalar<int>()() : -1;"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bb03fdf4aae944ab2e4b35c7daa051068a8b7f61",
    "API name": "tf.raw_ops.TensorListScatter"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['    CHECK(IsVector(max));', '    OP_REQUIRES(context, max.dim_size(0) == depth,', '                InvalidArgument(\\\"max has incorrect size, expected \\\", depth,', '                                \\\" was \\\", max.dim_size(0)));', '', '    Tensor* grad_wrt_input;', '    OP_REQUIRES_OK(context,', '                   context->allocate_output(0, input.shape(), &grad_wrt_input));', '', '    TensorShape min_max_shape({input.dim_size(input.dims() - 1)});', '    Tensor* grad_wrt_min;', '    OP_REQUIRES_OK(context,', '                   context->allocate_output(1, min_max_shape, &grad_wrt_min));', '', '    Tensor* grad_wrt_max;', '    OP_REQUIRES_OK(context,', '                   context->allocate_output(2, min_max_shape, &grad_wrt_max));']\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.",
        "    const Tensor& min = context->input(2);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(min.shape()),",
        "        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "    const Tensor& max = context->input(3);",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(max.shape()),",
        "        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));",
        "    OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/f3cf67ac5705f4f04721d15e485e192bb319feed",
    "API name": "tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient"
},
{
    "Patch Formated": [
        "[\"Patch\": \"",
        "    params.out_width, params.depth);",
        "",
        "    constexpr bool is_int8x4 = std::is_same<T, qint8>::value;",
        "    OP_REQUIRES(context, (is_int8x4 == (data_format_ == FORMAT_NCHW_VECT_C)),",
        "                errors::InvalidArgument(",
        "                    'qint8 should be used with data_format NCHW_VECT_C.'));",
        "",
        "#if CUDNN_VERSION >= 7300",
        "    if (ksize_ <= tensor_in.dim_size(1) && ksize_ <= tensor_in.dim_size(2)) {",
        "        DnnPoolingOp<T>::Compute(context, se::dnn::PoolingMode::kMaximum, ksize_,",
        "                                 stride_, padding_, explicit_paddings_,",
        "                                 data_format_, tensor_in, out_shape,",
        "                                 propagate_nans_);",
        "    } else {",
        "        context->set_output(0, Tensor());",
        "    }",
        "\"]"
    ],
    "Actual Clean Code": [
        "                        params.out_width, params.depth);",
        "",
        "    // Degenerate pooling output should return an empty tensor.",
        "    if (out_shape.num_elements() == 0) {",
        "      Tensor* output = nullptr;",
        "      OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));",
        "      return;",
        "    }",
        "",
        "    // Assuming qint8 <--> NCHW_VECT_C (int8x4) here.",
        "    constexpr bool is_int8x4 = std::is_same<T, qint8>::value;",
        "    OP_REQUIRES(context, (is_int8x4 == (data_format_ == FORMAT_NCHW_VECT_C)),",
        "                errors::InvalidArgument("
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/32d7bd3defd134f21a4e344c8dfd40099aaf6b18",
    "API name": "tf.raw_ops.MaxPool"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"",
        "['                         done);', ",
        "'', ",
        "'    if (n == 0 || m == 0) {', ",
        "'      if (n == m || !compute_uv_ || !full_matrices_) {', ",
        "'        // S, U, and V are all empty. Nothing to do.', ",
        "'        done();', ",
        "'        return;', ",
        "'      }', ",
        "'      auto device = context->eigen_device<GPUDevice>();', ",
        "'      functor::EyeFunctor<GPUDevice, Scalar> eye;', ",
        "'      if (m > 0) {', ",
        "'        // Return a full canonical basis for the column space.'",
        "'      }'",
        "'      else {'",
        "'        // Case when there are zero batches, return early.'",
        "'        done();', ",
        "'        return;', ",
        "'      }'",
        "]\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "                         done);",
        "",
        "    // If there are zero batches, we are done.",
        "    if (shapeRaw.num_elements() == 0) {",
        "      done();",
        "      return;",
        "    }",
        "",
        "    if (n == 0 || m == 0) {",
        "      if (n == m || !compute_uv_ || !full_matrices_) {",
        "        // S, U, and V are all empty. Nothing to do.",
        "        done();"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/c55b476aa0e0bd4ee99d0f3ad18d9d706cd1260a",
    "API name": "tf.linalg.matrix_rank"
},
{
    "Patch Formated": [
        "\"Patch\": \"['errors::InvalidArgument(\\\"Shape must be rank 0 but is rank \\\",', 'size_t.dims()));', 'Tidx size = size_t.scalar<Tidx>()();', 'OP_REQUIRES(', 'ctx, size >= 0,', 'errors::InvalidArgument(\\\"size (\\\", size, \\\") must be non-negative\\\"));', '', 'Tensor* out_t;', 'functor::SetZeroFunctor<Device, T> fill;', 'if (data.dims() == 1 && ((weights.dims() == 1 && weights.dim_size(0) == data.dim_size(0)) || weights.dim_size(0) == 0)) {', 'OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));', 'auto out = out_t->flat<T>();', 'fill(ctx->eigen_device<Device>(), out);', 'if (binary_output_) {']\""
    ],
    "Actual Clean Code": [
        "                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",",
        "                                        size_t.dims()));",
        "    OP_REQUIRES(ctx,",
        "                weights.shape() == data.shape() || weights.NumElements() == 0,",
        "                errors::InvalidArgument(",
        "                    \"`weights` must be the same shape as `arr` or a length-0 \"",
        "                    \"`Tensor`, in which case it acts as all weights equal to \"",
        "                    \"1. Received \",",
        "                    weights.shape().DebugString()));",
        "",
        "    Tidx size = size_t.scalar<Tidx>()();",
        "    OP_REQUIRES(",
        "        ctx, size >= 0,",
        "        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bf4c14353c2328636a18bfad1e151052c81d5f43",
    "API name": "tf.raw_ops.DenseBincount"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\":\"",
        "    int batch_idx = 0;",
        "",
        "    OP_REQUIRES(ctx, splits.size() > 0,",
        "                errors::InvalidArgument('Splits must not be empty'));",
        "",
        "    OP_REQUIRES(ctx, splits(0) == 0,",
        "                errors::InvalidArgument('Splits must start with 0, not with ',",
        "                                        splits(0)));",
        "",
        "    OP_REQUIRES(ctx, splits(num_rows) == num_values,",
        "                errors::InvalidArgument(",
        "                    'Splits must end with the number of values, got ',",
        "                    splits(num_values)));",
        "\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    int batch_idx = 0;",
        "",
        "    OP_REQUIRES(ctx, splits.size() > 0,",
        "                errors::InvalidArgument(\"Splits must be non-empty\"));",
        "",
        "    OP_REQUIRES(ctx, splits(0) == 0,",
        "                errors::InvalidArgument(\"Splits must start with 0, not with \",",
        "                                        splits(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/7a4591fd4f065f4fa903593bc39b2f79530a74b8",
    "API name": "tf.raw_ops.RaggedBincount"
},
{
    "Patch Formated": [
        "\"Patch\": \"OP_REQUIRES(ctx, in_image.dim_size(0) == batch && in_image.dim_size(1) == rows &&\\n in_image.dim_size(2) == cols && in_image.dim_size(3) == depth &&\\n out_image.dim_size(0) == batch && out_image.dim_size(1) == rows &&\\n out_image.dim_size(2) == cols && out_image.dim_size(3) == depth,\\n errors::InvalidArgument(\\n 'input_grads, input_image, and out_image should have the same ',\\n 'shape'));\\n\\nTensor* output = nullptr;\""
    ],
    "Actual Clean Code": [
        "            in_image.dim_size(2) == cols && in_image.dim_size(3) == depth &&",
        "            out_image.dim_size(0) == batch && out_image.dim_size(1) == rows &&",
        "            out_image.dim_size(2) == cols && out_image.dim_size(3) == depth &&",
        "            out_image.dims() == 4,",
        "        errors::InvalidArgument(",
        "            \"input_grads, input_image, and out_image should have the same \"",
        "            \"shape\"));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/bd90b3efab4ec958b228cd7cfe9125be1c0cf255",
    "API name": "tf.raw_ops.LRNGrad"
},
{
    "Patch Formated": [
        "{\"Patch\": \"int64_t num_batches = shape_tensor.flat<int64>()(0);\\n\\nint64_t samples_per_batch = 1;\\nconst int32_t num_dims = shape_tensor.dim_size(0);\\nfor (int32_t i = 1; i < num_dims; i++) {\\n  samples_per_batch *= shape_tensor.flat<int64>()(i);\\n}\\nconst int64_t num_elements = num_batches * samples_per_batch;\\n\\nauto shape_vec = shape_tensor.flat<int64>();\\nTensorShape tensor_shape;\\nOP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\\n                        shape_vec.data(), shape_vec.size(), &tensor_shape));\\nTensor* samples_tensor;\\nOP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));\"}"
    ],
    "Actual Clean Code": [
        "                errors::InvalidArgument(\"Shape tensor must not be empty, got \",",
        "                                        shape_tensor.DebugString()));",
        "    TensorShape tensor_shape;",
        "    OP_REQUIRES_OK(ctx, tensor::MakeShape(shape_tensor, &tensor_shape));",
        "",
        "    int32_t num_batches = tensor_shape.dim_size(0);",
        "    int32_t samples_per_batch = 1;",
        "    const int32_t num_dims = tensor_shape.dims();",
        "    for (int32_t i = 1; i < num_dims; i++) {",
        "      samples_per_batch *= tensor_shape.dim_size(i);",
        "    }",
        "    const int32_t num_elements = num_batches * samples_per_batch;",
        "",
        "    // Allocate the output before fudging num_batches and samples_per_batch.",
        "    Tensor* samples_tensor;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));",
        "",
        "    // Parameters must be 0-d or 1-d."
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/72180be03447a10810edca700cbc9af690dfeb51",
    "API name": "tf.raw_ops.ParameterizedTruncatedNormal"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"size_t size_bound = ss->ByteSize() + kTensorProtoHeaderBytes + (num_elements * MaxBytesPerElement(DT_INT64)); for (int64_t i = 0; i < num_elements; ++i) { size_bound += data[i].size(); } if (size_bound > kMaxMessageBytes) { return errors::InvalidArgument( 'Tensor slice is too large to serialize (conservative estimate: ',}\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    case DT_BFLOAT16:",
        "    default:",
        "      return 0;",
        "  }",
        "}",
        "",
        "template <>",
        "Status TensorSliceWriter::SaveData(const tstring* data, int64_t num_elements,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/5dd7b86b84a864b834c6fa3d7f9f51c87efa99d4",
    "API name": "tf.raw_ops.Save"
},
{
    "Patch Formated": [
        "\"Patch\": \"",
        "        OP_REQUIRES(",
        "            ctx, batch >= 0,",
        "            errors::InvalidArgument(\\\"Index out of bound.  `batch` (\\\", batch,",
        "                                    \\\") must be non-negative.\\\"));",
        "        OP_REQUIRES(",
        "            ctx, bin >= 0,",
        "            errors::InvalidArgument(\\\"Index out of bound. `bin` (\\\", bin,",
        "                                    \\\") must be non-negative.\\\"));",
        "        OP_REQUIRES(",
        "            ctx, batch < out.dimension(0),",
        "            errors::InvalidArgument(\\\"Index out of bound. `batch` (\\\", batch,",
        "                                    \\\") must be less than the dimension size (\\\", out.dimension(0), \\\").\\\"));",
        "        OP_REQUIRES(",
        "            ctx, bin < out.dimension(1),",
        "            errors::InvalidArgument(\\\"Index out ouf bound. `bin` (\\\", bin,",
        "                                    \\\") must be less then the dimension size (\\\", out.dimension(1), \\\").\\\"));",
        "            \""
    ],
    "Actual Clean Code": [
        "      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {",
        "        const int64_t batch = indices_mat(i, 0);",
        "        const Tidx bin = values_flat(i);",
        "        OP_REQUIRES(",
        "            ctx, batch < out.dimension(0),",
        "            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,",
        "                                    \") must be less than the dimension size (\","
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/40adbe4dd15b582b0210dfbf40c243a62f5119fa",
    "API name": "tf.raw_ops.SparseBincount"
},
{
    "Patch Formated": [
        "\"Patch\": \"['        ragged_nested_splits_len);', '    // Add check for rank of row_splits', '    OP_REQUIRES(context, Input::Tensor::dim_size(0) == 1,', '        errors::InvalidArgument('row_splits must have rank 1');', '    for (int i = 0; i < ragged_nested_splits_len; i++) {', '      batched_ragged_input.append_splits(ragged_nested_splits_in[i]);', '    }', '', '    if (!batched_input_) {', '      // Encode as a Scalar Variant Tensor.', '      Tensor* encoded_scalar;', '      OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}),', '                                                       &encoded_scalar));']\""
    ],
    "Actual Clean Code": [
        "        ragged_nested_splits_len);",
        "    for (int i = 0; i < ragged_nested_splits_len; i++) {",
        "      OP_REQUIRES(context, ragged_nested_splits_in[i].dims() == 1,",
        "                  errors::InvalidArgument(\"Requires nested_row_splits[\", i, \"]\",",
        "                                          \" to be rank 1 but is rank \",",
        "                                          ragged_nested_splits_in[i].dims()));",
        "      batched_ragged_input.append_splits(ragged_nested_splits_in[i]);",
        "    }",
        "",
        "    if (!batched_input_) {"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/88f93dfe691563baa4ae1e80ccde2d5c7a143821",
    "API name": "tf.raw_ops.RaggedTensorToVariant"
},
{
    "Patch Formated": [
        "\"Patch\": \"['      CHECK(input_backprop_index >= 0 &&', '            input_backprop_index < num_total_inputs)', '          << \\\"Invalid input backprop index: \\\" << input_backprop_index << \\\", \\\"', '          << num_total_inputs;', '      if (&input_backprop_flat(input_backprop_index) != &out_backprop_flat(index)) {', '          input_backprop_flat(input_backprop_index) += out_backprop_flat(index);', '      }', '    }', '  }', '', ' private:', '  bool overlapping_;', '};', '']\""
    ],
    "Actual Clean Code": [
        "    for (int index = 0; index < num_total_outputs; ++index) {",
        "      int input_backprop_index = out_arg_max_flat(index);",
        "      OP_REQUIRES(",
        "          context,",
        "          input_backprop_index >= 0 && input_backprop_index < num_total_inputs,",
        "          errors::InvalidArgument(",
        "              \"Invalid input backprop index: \", input_backprop_index, \", \",",
        "              num_total_inputs));",
        "      input_backprop_flat(input_backprop_index) += out_backprop_flat(index);",
        "    }",
        "  }",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/8741e57d163a079db05a7107a7609af70931def4",
    "API name": "tf.raw_ops.FractionalMaxPoolGrad"
},
{
    "Patch Formated": [
        "\"Patch\": \"['CHECK(IsScalar(min_input)) << \\\"QuantizedOp: min_input is not scalar\\\";', 'CHECK(IsScalar(max_input)) << \\\"QuantizedOp: max_input is not scalar\\\";', 'min_as_quantized, max_as_quantized,', 'output->flat<quint8>().data());', '    } else {', '      output->flat<T>().device(context->eigen_cpu_device()) =', '          input.flat<T>()', '              .cwiseMax(min_as_quantized)', '              .cwiseMin(max_as_quantized)', '              .template cast<T>();', '    }', '', '    Tensor* output_min = nullptr;', '    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));', '    output_min->flat<float>()(0) = min_input;', '    Tensor* output_max = nullptr;', '    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));', '    output_max->flat<float>()(0) = max_input;', '  }', '};', '', 'REGISTER_KERNEL_BUILDER(Name(\\\"QuantizedRelu\\\")', '                            .Device(DEVICE_CPU)']\""
    ],
    "Actual Clean Code": [
        "  void Compute(OpKernelContext* context) override {",
        "    const Tensor& input = context->input(0);",
        "    const Tensor& min_input_tensor = context->input(1);",
        "    const Tensor& max_input_tensor = context->input(2);",
        "",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),",
        "        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",",
        "                                min_input_tensor.dims()));",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),",
        "        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",",
        "                                max_input_tensor.dims()));",
        "",
        "    const float min_input = min_input_tensor.scalar<float>()();",
        "    const float max_input = max_input_tensor.scalar<float>()();",
        "",
        "    Tensor* output = nullptr;",
        "    OP_REQUIRES_OK(context,",
        "                   context->allocate_output(0, input.shape(), &output));",
        "    const T min_as_quantized = FloatToQuantized<T>(0.0f, min_input, max_input);"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/49b3824d83af706df0ad07e4e677d88659756d89",
    "API name": "tf.raw_ops.QuantizedRelu6"
},
{
    "Patch Formated": [
        "\"Patch\": \"\\n  void Compute(OpKernelContext* ctx) override {\\n    const Tensor& input = ctx->input(0);\\n    const Tensor& input_min_tensor = ctx->input(1);\\n    const Tensor& input_max_tensor = ctx->input(2);\\n\\n    OP_REQUIRES(ctx, input_min_tensor.dims() == 0,\\n                errors::InvalidArgument(\\\"input_min must be scalar: \\\", input_min_tensor.shape().DebugString()));\\n    const float input_min_float = input_min_tensor.scalar<float>()();\\n\\n    OP_REQUIRES(ctx, input_max_tensor.dims() == 0,\\n                errors::InvalidArgument(\\\"input_max must be scalar: \\\", input_max_tensor.shape().DebugString()));\\n    const float input_max_float = input_max_tensor.scalar<float>()();\\n\\n    Tensor* output = nullptr;\\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\\n    Tensor* output_min = nullptr;\\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\\n    Tensor* output_max = nullptr;\\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(2, TensorShape({}), &output_max));\\n\\n    auto input_array = input.flat<T1>();\\n    const int32_t input_lowest_quantized =\\n        static_cast<int32>(Eigen::NumTraits<T1>::lowest());\\n    const int32_t input_highest_quantized =\\n        static_cast<int32>(Eigen::NumTraits<T1>::highest());\\n    T1 actual_min_quantized = input_highest_quantized;\\n    T1 actual_max_quantized = input_lowest_quantized;\\n\\n    for (int i = 0; i < input_array.size(); ++i) {\\n  }\\n}\""
    ],
    "Actual Clean Code": [
        "  void Compute(OpKernelContext* ctx) override {",
        "    const Tensor& input = ctx->input(0);",
        "    const Tensor& input_min = ctx->input(1);",
        "    const Tensor& input_max = ctx->input(2);",
        "",
        "    OP_REQUIRES(",
        "        ctx, TensorShapeUtils::IsScalar(input_min.shape()),",
        "        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",",
        "                                input_min.dims()));",
        "    OP_REQUIRES(",
        "        ctx, TensorShapeUtils::IsScalar(input_max.shape()),",
        "        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",",
        "                                input_max.dims()));",
        "",
        "    const float input_min_float = input_min.scalar<float>()();",
        "    const float input_max_float = input_max.scalar<float>()();",
        "    Tensor* output = nullptr;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));",
        "    Tensor* output_min = nullptr;",
        "    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/73ad1815ebcfeb7c051f9c2f7ab5024380ca8613",
    "API name": "tf.raw_ops.QuantizeDownAndShrinkRange"
},
{
    "Patch Formated": [
        "{\"Patch\": \"    const float min_a = context->input(2).scalar<float>()();\\n    const float max_a = context->input(3).scalar<float>()();\\n    const float min_b = context->input(4).scalar<float>()();\\n    const float max_b = context->input(5).scalar<float>()();\\n\\n    OP_REQUIRES(context, (max_a > min_a),\\n                errors::InvalidArgument(\\\"max_a must be larger than min_a.\\\"));\\n    OP_REQUIRES(context, (max_b > min_b),\\n                errors::InvalidArgument(\\\"max_b must be larger than min_b.\\\"));\\n    const int32_t offset_a = FloatToQuantizedUnclamped<T1>(0.0f, min_a, max_a);\\n    const int32_t offset_b = FloatToQuantizedUnclamped<T2>(0.0f, min_b, max_b);\\n    const int32_t offset_c = 0;\\n    const int32_t mult_c = 1;\\n    const int32_t shift_c = 0;\\n\\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(a.shape()),\\n                errors::InvalidArgument(\\\"In[0] is not a matrix\\\"));\\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(b.shape()),\\n                errors::InvalidArgument(\\\"In[1] is not a matrix\\\"));\"}"
    ],
    "Actual Clean Code": [
        "    const Tensor& a = context->input(0);",
        "    const Tensor& b = context->input(1);",
        "    OP_REQUIRES(context, TensorShapeUtils::IsScalar(context->input(2).shape()),",
        "                errors::InvalidArgument(\"min_a must be a scalar, but got shape\",",
        "                                        context->input(2).shape()));",
        "    const float min_a = context->input(2).flat<float>()(0);",
        "    OP_REQUIRES(context, context->input(3).NumElements() == 1,",
        "                errors::InvalidArgument(\"max_a must be a scalar, but got shape\",",
        "                                        context->input(3).shape()));",
        "    const float max_a = context->input(3).flat<float>()(0);",
        "    OP_REQUIRES(context, context->input(4).NumElements() == 1,",
        "                errors::InvalidArgument(\"min_b must be a scalar, but got shape\",",
        "                                        context->input(4).shape()));",
        "    const float min_b = context->input(4).flat<float>()(0);",
        "    OP_REQUIRES(context, context->input(5).NumElements() == 1,",
        "                errors::InvalidArgument(\"max_b must be a scalar, but got shape\",",
        "                                        context->input(5).shape()));",
        "    const float max_b = context->input(5).flat<float>()(0);",
        "",
        "    // Make sure that we have valid quantization ranges for the input buffers.",
        "    // If the difference between the min and max is negative or zero, it makes"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/aca766ac7693bf29ed0df55ad6bfcc78f35e7f48",
    "API name": "tf.raw_ops.QuantizedMatMul"
},
{
    "Patch Formated": [
        "\"Patch\": \"",
        "        OP_REQUIRES(context, ",
        "                    input.dims() >= 2,",
        "                    errors::InvalidArgument(\\\"FakeQuantWithMinMaxVarsPerChannel requires input of at least 2-dimensions, got shape\\\", input.shape()));",
        "",
        "        Tensor* output;",
        "",
        "        OP_REQUIRES_OK(context, ",
        "                      context->allocate_output(0, input.shape(), &output)); ",
        "        ",
        "        FakeQuantWithMinMaxVarsPerChannelFunctor<Device> functor;",
        "",
        "        functor(context->eigen_device<Device>(), input.flat_inner_dims<float, 2>(),",
        "                min.vec<float>(), max.vec<float>(), quant_min_, quant_max_,",
        "                output->flat_inner_dims<float, 2>());",
        "    \""
    ],
    "Actual Clean Code": [
        "    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.",
        "    const Tensor& min = context->input(1);",
        "    const Tensor& max = context->input(2);",
        "",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(min.shape()),",
        "        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));",
        "    OP_REQUIRES(context, min.dim_size(0) == depth,",
        "                InvalidArgument(\"min has incorrect size, expected \", depth,",
        "                                \" was \", min.dim_size(0)));",
        "    OP_REQUIRES(",
        "        context, TensorShapeUtils::IsVector(max.shape()),",
        "        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));",
        "    OP_REQUIRES(context, max.dim_size(0) == depth,",
        "                InvalidArgument(\"max has incorrect size, expected \", depth,",
        "                                \" was \", max.dim_size(0)));",
        ""
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/785d67a78a1d533759fcd2f5e8d6ef778de849e0",
    "API name": "tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "\"Patch\": \"['           TensorShape input_shape;', '           OP_REQUIRES(context, input_shape.dims() == 4, errors::InvalidArgument('Expected input to be 4-dimensional, not ', input_shape.dims(), ' dimensions.'));', '            Conv2DBackpropComputeInputShape(input_sizes, filter.shape(),', '                                                   out_backprop.shape(),', '                                                   data_format_, &input_shape));', '', '    ConvBackpropDimensions dims;', '    OP_REQUIRES_OK(context,', '                   ConvBackpropComputeDimensionsV2(', '                       \\\"Conv2DCustomBackpropInput\\\", /*num_spatial_dims=*/2,', '                       input_shape, filter.shape(), out_backprop.shape(),', '                       /*dilations=*/{1, 1, 1, 1}, strides_, padding_,\"]\"",
        "<answer json end>"
    ],
    "Actual Clean Code": [
        "    const Tensor& filter = context->input(1);",
        "    const Tensor& out_backprop = context->input(2);",
        "    OP_REQUIRES(",
        "        context, out_backprop.dims() == 4,",
        "        errors::InvalidArgument(\"input_sizes must be 4-dimensional, got: \",",
        "                                out_backprop.dims()));",
        "",
        "    TensorShape input_shape;",
        "    OP_REQUIRES_OK(context,",
        "                   Conv2DBackpropComputeInputShape(input_sizes, filter.shape(),"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/50156d547b9a1da0144d7babe665cf690305b33c",
    "API name": "tf.raw_ops.Conv2DBackpropInput"
},
{
    "Patch Formated": [
        "<answer json start>,",
        "    \"Patch\": \"",
        "    auto shape_vec = tensor_in_shape.vec<int32>();",
        "    for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) { ",
        "        output_shape.AddDim(shape_vec(i)); ",
        "    } ",
        "    //fixed code",
        "    if (output_shape.num_elements() <= 0) { ",
        "        Tensor* output = nullptr; ",
        "        context->SetStatus(errors::InvalidArgument(\\\"Expected orig_input_shape to have positive dimensions\\\")); ",
        "        return; ",
        "    } ",
        "    OP_REQUIRES_OK(context,\""
    ],
    "Actual Clean Code": [
        "    auto shape_vec = tensor_in_shape.vec<int32>();",
        "    for (int64_t i = 0; i < tensor_in_shape.NumElements(); ++i) {",
        "      OP_REQUIRES_OK(context, output_shape.AddDimWithStatus(shape_vec(i)));",
        "    }",
        "    if (output_shape.num_elements() == 0) {",
        "      Tensor* output = nullptr;",
        "      OP_REQUIRES_OK(context,"
    ],
    "Link": "https://github.com/tensorflow/tensorflow/commit/3a6ac52664c6c095aa2b114e742b0aa17fdce78f",
    "API name": "tf.raw_ops.AvgPoolGrad"
},
