0,https://github.com/tensorflow/tensorflow/commit/35af094b674629be64b2efa02c8d5a8f15de5f18,third_party/xla/xla/service/gpu/fusions/reduction_mlir.cc,"-  // for the iteration. Otherwise, some threads might not be part of the loop.
-  // The constraints are still checked inside the loop.
","+  // for the iteration. Otherwise, some threads might not be part of the loop,
+  // resulting in incorrect results for the warp shuffle.
+  // The constraints are still checked inside the loop in the
+  // PredicatedExtractOp.
+  // The constraints may have reduced the upper bound of the dimension. If
+  // that's the case, we reset it to a multiple of the warp size.
+  auto& bound = loop_indexing.GetMutableDimensionBound(0);
+  bound.upper = RoundUpTo(bound.upper + 1, WarpSize()) - 1;
","// Make sure the size of d0 is a multiple of the warp size to include all threads in the iteration
d0 = ((n0 + blockDim.x - 1) / blockDim.x) * blockDim.x;  // Adjust the size of d0 to include all threads in the warp

// for the iteration. Otherwise, some threads might not be part of the loop.
// The constraints are still checked inside the loop.",YES,"The root cause of the bug is that the indexing map creation in the shared memory phase of row reductions results in shrinking the size of the d0 dimension (thread ID). This causes only the first 20 threads to produce a value for the shuffle. However, an unmasked shuffle with the entire warp is performed, leading to undefined values being obtained.","The fixing pattern in the code change is adding additional comments to explain the importance of ensuring all threads are part of the loop iteration, particularly for correct results in the warp shuffle operation. Additionally, the code change includes a reset to the upper bound of a dimension to ensure it is a multiple of the warp size, which may have been reduced due to constraints."
0,https://github.com/tensorflow/tensorflow/commit/c3b618f959f8afcec8c42a6ab9639c047e61c518,third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc,"-  if (is_profiling) {
","+  if (timer.has_value()) {
",No,NO
0,https://github.com/tensorflow/tensorflow/commit/c3b618f959f8afcec8c42a6ab9639c047e61c518,third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc,"-  if (is_profiling) {
","+  if (timer.has_value()) {
",No,NO
0,https://github.com/tensorflow/tensorflow/commit/c3b618f959f8afcec8c42a6ab9639c047e61c518,third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc,"-    if (is_profiling) {
","+    if (timer.has_value()) {
",No,NO
0,https://github.com/tensorflow/tensorflow/commit/c3b618f959f8afcec8c42a6ab9639c047e61c518,third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc,"-    if (profile_result) {
","+    if (timer.has_value()) {
",No,NO
0,https://github.com/tensorflow/tensorflow/commit/c3b618f959f8afcec8c42a6ab9639c047e61c518,third_party/xla/xla/stream_executor/gpu/gpu_timer.cc,"-/*static*/ absl::StatusOr<std::optional<GpuTimer>> GpuTimer::CreateIfNeeded(
-    Stream* stream, bool use_delay_kernel, bool is_needed) {
-  if (is_needed) {
-    TF_ASSIGN_OR_RETURN(GpuTimer t, GpuTimer::Create(stream, use_delay_kernel));
-    return {std::make_optional(std::move(t))};
-  }
-  return std::nullopt;
-}
-
",,No,NO
0,https://github.com/tensorflow/tensorflow/commit/c3b618f959f8afcec8c42a6ab9639c047e61c518,third_party/xla/xla/stream_executor/gpu/gpu_timer.h,"-  // An ugly but a very convenient helper: creates a timer only when we need
-  // one, but always returns an object. If `is_needed` is false, returns an
-  // empty optional, acts like `Create` otherwise.
-  static absl::StatusOr<std::optional<GpuTimer>> CreateIfNeeded(
-      Stream* stream, bool use_delay_kernel, bool is_needed);
-
",,No,NO
0,https://github.com/tensorflow/tensorflow/commit/c3b618f959f8afcec8c42a6ab9639c047e61c518,third_party/xla/xla/stream_executor/rocm/rocm_blas.cc,"-  TF_ASSIGN_OR_RETURN(
-      auto timer,
-      GpuTimer::CreateIfNeeded(
-          stream, profile_result && profile_result->warmup_run_executed(),
-          profile_result != nullptr));
","+  std::optional<GpuTimer> timer = std::nullopt;
+  if (profile_result != nullptr) {
+    TF_ASSIGN_OR_RETURN(
+        timer, GpuTimer::Create(stream, profile_result->warmup_run_executed()));
+  }
",No,NO
0,https://github.com/tensorflow/tensorflow/commit/c3b618f959f8afcec8c42a6ab9639c047e61c518,third_party/xla/xla/stream_executor/rocm/rocm_blas.cc,"-  TF_ASSIGN_OR_RETURN(
-      auto timer,
-      GpuTimer::CreateIfNeeded(
-          stream, profile_result && profile_result->warmup_run_executed(),
-          profile_result != nullptr));
","+  std::optional<GpuTimer> timer = std::nullopt;
+  if (profile_result != nullptr) {
+    TF_ASSIGN_OR_RETURN(
+        timer, GpuTimer::Create(stream, profile_result->warmup_run_executed()));
+  }
",No,NO
0,https://github.com/tensorflow/tensorflow/commit/c9c83bb391380374bf476b30448693e139e8752c,third_party/xla/xla/service/spmd/shardy/round_trip_common/shard_map_import.cc,"-      if (!op.getCallee().starts_with(""shmap_body"")) {
","+      if (!op.getCallee().contains(""shmap_body"")) {
","-      if (!op.getCallee().contains(""shmap_body"")) {",NO,"The root cause of the bug is that the code is only checking if ""shmap_body"" is a prefix of the function name, instead of verifying if it is contained within the function name. This results in false positives where a prefix is added to the name before ""shmap_body"", leading to incorrect detection of the function names that should be processed.",The fixing pattern in the code change is from using the `starts_with` function to using the `contains` function.
0,https://github.com/tensorflow/tensorflow/commit/c9c83bb391380374bf476b30448693e139e8752c,third_party/xla/xla/service/spmd/shardy/shardy_call_inliner.cc,"-#include <string>
-#include <string_view>
-
-  auto prefix_check = [](std::string_view name) {
-    const std::string prefix = ""shmap_body"";
-    return name.substr(0, prefix.size()) == prefix;
-  };
-           prefix_check(instruction->to_apply()->name()));
","+#include ""absl/strings/match.h""
+           absl::StrContains(instruction->to_apply()->name(), ""shmap_body""));
","#include <string>
#include <string_view>
#include ""absl/strings/match.h""

  absl::StrContains(instruction->to_apply()->name(), ""shmap_body"")",NO,"The root cause of the bug is that the code was only checking if ""shmap_body"" was a prefix of the function name, rather than checking if it was contained within the function name. This caused issues in cases where a prefix was added to the name before ""shmap_body"".","The fixing pattern in the code change is replacing the custom prefix checking lambda function with a call to the `absl::StrContains` function from the ""absl/strings/match.h"" header. This change makes the code more concise and leverages existing functionality from the absl library for string matching operations."
0,https://github.com/tensorflow/tensorflow/commit/3f9da0a5aa614b8832f4e8ec657ff2a306274f0a,third_party/xla/xla/service/collective_ops_utils.cc,,"+#include ""absl/status/statusor.h""
","Patch:
         /* Check if all subgroups are exclusively cross module */
         bool all_subgroups_exclusively_cross_module(const std::vector<std::vector<int>>& replica_ids) {
            for (const auto& subgroup : replica_ids) {
               if (std::any_of(subgroup.begin(), subgroup.end(), [&](int id) { return id != subgroup[0]; })) {
                  return false;
               }
            }
            return true;
         }",NO,The root cause of the bug is that the added function may not properly handle cases where not all participants within a subgroup have the same replica ID. This could lead to incorrect results when checking if all subgroups in a collective Op are exclusively cross module.,"This is an ""Add Dependencies"" fix pattern."
0,https://github.com/tensorflow/tensorflow/commit/3f9da0a5aa614b8832f4e8ec657ff2a306274f0a,third_party/xla/xla/service/collective_ops_utils.h,,"+// Returns true if all subgroups in replica_groups are exclusively cross-module.
+absl::StatusOr<bool> IsExclusivelyCrossModule(
+    absl::Span<const ReplicaGroup> replica_groups, bool use_global_ids,
+    bool has_channel_id, const DeviceAssignment& device_assignment);
+
",No,NO
0,https://github.com/tensorflow/tensorflow/commit/5b43e7d9f8773988ce71efb958df833f03b1d0f5,third_party/xla/xla/service/algebraic_simplifier.cc,"-        return *value >= 0.0;
","+        // return false for -0.0, -Inf, NaNs and negative values
+        return !std::signbit(*value) && !std::isnan(*value);
",        return !std::signbit(*value) && !std::isnan(*value) && *value >= 0.0;,YES,"The root cause of the bug is that the `IsNonNegative` function in `AlgebraicSimplifierVisitor` returns true for `Constant(-0.0)`. This is a problem because certain optimization patterns rely on distinguishing between negative and non-negative values. In C++, `0.0` is considered equal to `-0.0`, so a simple check for non-negativity would return true for `-0.0` as well. To address this issue, the implementation should use `std::signbit` to properly distinguish between `-0.0` and `0.0`.","The fixing pattern in the code change is to modify the condition to return `false` for specific cases including when the value is `-0.0`, `-Inf`, `NaN`, and negative values. This is achieved by using `std::signbit(*value)` to check if the value is negative and `std::isnan(*value)` to check if it is NaN, and returning `false` in those cases."
