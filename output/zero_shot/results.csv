0,https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c,YES,torch/csrc/jit/frontend/function_schema_parser.cpp,patch_0,"-  explicit SchemaParser(const std::string& str)
-        type_parser(L, /*parse_complete_tensor_types*/ false) {}
",No
0,https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c,YES,torch/csrc/jit/frontend/function_schema_parser.cpp,patch_1,"-    const std::string& schemaOrName) {
-  return SchemaParser(schemaOrName).parseExactlyOneDeclaration();
-FunctionSchema parseSchema(const std::string& schema) {
-  auto parsed = parseSchemaOrName(schema);
",No
0,https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c,YES,torch/csrc/jit/frontend/function_schema_parser.h,patch_0,"-    const std::string& schemaOrName);
-TORCH_API c10::FunctionSchema parseSchema(const std::string& schema);
",No
0,https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c,YES,torch/csrc/jit/frontend/schema_type_parser.cpp,patch_0,"-    if (!text.empty() && islower(text[0])) {
-    throw ErrorReport(tok.range) << ""unknown type specifier"";
",No
0,https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c,YES,torch/csrc/jit/frontend/schema_type_parser.h,patch_0,"-  SchemaTypeParser(Lexer& L, bool parse_complete_tensor_types)
-      : complete_tensor_types(parse_complete_tensor_types), L(L) {}
",No
0,https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c,YES,torch/csrc/jit/frontend/schema_type_parser.h,patch_1,,"+  bool allow_typevars_;
","@@ -8,9 +8,17 @@
 namespace torch {
 namespace jit {

+// allow_typevars: If true, we assume that lowercase types that we don't
+// understand are type variables. This is only needed for TorchScript (and not
+// not needed for custom ops).
+// If false, we disallow typevars, except in certain cases for BC reason (i.e.
+// your op is in the aten or prim namespace).
 TORCH_API std::variant<c10::OperatorName, c10::FunctionSchema> parseSchemaOrName(
-    const std::string& schemaOrName);
-TORCH_API c10::FunctionSchema parseSchema(const std::string& schema);
+    const std::string& schemaOrName,
+    bool allow_typevars = true);
+TORCH_API c10::FunctionSchema parseSchema(
+    const std::string& schema,
+    bool allow_typevars = true);
 TORCH_API c10::OperatorName parseName(const std::string& name);
 
 } // namespace jit"
0,https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c,YES,torch/csrc/jit/ir/irparser.cpp,patch_0,"-        type_parser(L, /*parse_complete_tensor_types*/ true),
","+        type_parser(
+            L,
+            /*parse_complete_tensor_types*/ true,
+            /*allow_type_vars*/ true),
","@@ -20,8 +20,13 @@ struct TORCH_API SchemaTypeParser {
     std::optional<at::ScalarType> parseTensorDType(const std::string& dtype);
     TypePtr parseRefinedTensor();
 
-    SchemaTypeParser(Lexer& L, bool parse_complete_tensor_types)
+    SchemaTypeParser(
+        Lexer& L,
+        bool parse_complete_tensor_types,
+        bool allow_typevars)
         : complete_tensor_types(parse_complete_tensor_types),
-          L(L) {}
+          L(L),
+          allow_typevars_(allow_typevars) {}
 
     private:
     std::optional<bool> tryToParseRequiresGrad();"
0,https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c,YES,torch/csrc/jit/python/init.cpp,patch_0,"-  m.def(""parse_schema"", parseSchema);
","+  m.def(
+      ""parse_schema"",
+      &parseSchema,
+      py::arg(""schema""),
+      py::arg(""allow_typevars"") = true);
","@@ -1765,7 +1765,11 @@ void initJITBindings(PyObject* module) {
       },
       py::arg(""input""),
       py::arg(""parse_tensor_constants"") = false);
-  m.def(""parse_schema"", parseSchema);
+  m.def(
+      ""parse_schema"",
+      &parseSchema,
+      py::arg(""schema""),
+      py::arg(""allow_typevars"") = true);
   m.def(""unify_type_list"", [](const std::vector<TypePtr>& types) {
     std::ostringstream s;
     auto type = unifyTypeList(types, s);"
0,https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c,YES,torch/csrc/jit/runtime/static/passes.cpp,patch_0,"-      ""aten::slice.t(t[] l, int? start=None, int? end=None, int step=1) -> t[]"");
","+      ""aten::slice.t(t[] l, int? start=None, int? end=None, int step=1) -> t[]"",
+      /*allow_typevars*/ true);
","@@ -167,7 +173,12 @@
   }
 }

-static void slice_test(const std::vector<int64_t>& size, int64_t dim, c10::optional<int64_t> start, c10::optional<int64_t> end, int64_t step) {
+static void slice_test(
+    const std::vector<int64_t>& size,
+    int64_t dim,
+    c10::optional<int64_t> start,
+    c10::optional<int64_t> end,
+    int64_t step) {
   // Arrange
   const auto in_cpu = at::rand(size, at::device(at::kCPU).dtype(at::kFloat));
   const auto in_vulkan = in_cpu.vulkan();"
0,https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c,YES,torch/library.h,patch_0,"-inline c10::FunctionSchema schema(const char* str, c10::AliasAnalysisKind k) {
-  c10::FunctionSchema s = torch::jit::parseSchema(str);
","+inline c10::FunctionSchema schema(const char* str, c10::AliasAnalysisKind k, bool allow_typevars=false) {
+  c10::FunctionSchema s = torch::jit::parseSchema(str, /*allow_typevars*/allow_typevars);
","@@ -415,8 +415,8 @@ 
 inline c10::FunctionSchema schema(const char* str, c10::AliasAnalysisKind k) {
-  c10::FunctionSchema s = torch::jit::parseSchema(str);
+  c10::FunctionSchema s = torch::jit::parseSchema(str, /*allow_typevars*/allow_typevars);"
0,https://github.com/PyTorch/PyTorch/commit/5b98d43488bed0836b4da5996a50bafd0dd2c11c,YES,torch/library.h,patch_1,"-inline c10::FunctionSchema schema(const char* s) {
-  return schema(s, c10::AliasAnalysisKind::FROM_SCHEMA);
","+inline c10::FunctionSchema schema(const char* s, bool allow_typevars=false) {
+  return schema(s, c10::AliasAnalysisKind::FROM_SCHEMA, allow_typevars);
","@@ -415,8 +415,8 @@ inline c10::FunctionSchema schema(const char* str, c10::AliasAnalysisKind k) {
 /// Function schemas can be directly constructed from string literals.
 ///
 /// \ingroup torch-schema-overloads
-inline c10::FunctionSchema schema(const char* s) {
-  return schema(s, c10::AliasAnalysisKind::FROM_SCHEMA);
+inline c10::FunctionSchema schema(const char* s, bool allow_typevars=false) {
+  return schema(s, c10::AliasAnalysisKind::FROM_SCHEMA, allow_typevars);
 }
 
 /// \private"
0,https://github.com/PyTorch/PyTorch/commit/fbc7559ceb372d88b55c96ef6984accbaa0ec3ec,YES,torch/_library/infer_schema.py,patch_0,,"+from typing import List, Optional, Sequence, Union  # noqa: F401
+import torch  # noqa: F401
","@@ -1,3 +1,5 @@
+import itertools
+
 from typing import List, NamedTuple, Optional

 import torch"
0,https://github.com/PyTorch/PyTorch/commit/fbc7559ceb372d88b55c96ef6984accbaa0ec3ec,YES,torch/_library/infer_schema.py,patch_1,,"+    - string type annotations ""device, dtype, Tensor, types"" without library specification
+      are assumed to be torch.*. Similarly, string type annotations ""Optional, List, Sequence, Union""
+      without library specification are assumed to be typing.*.
","@@ -20,8 +20,13 @@
 struct TORCH_API SchemaTypeParser {
   c10::optional<at::ScalarType> parseTensorDType(const std::string& dtype);
   TypePtr parseRefinedTensor();

-  SchemaTypeParser(Lexer& L, bool parse_complete_tensor_types)
-      : complete_tensor_types(parse_complete_tensor_types), L(L) {}
+  SchemaTypeParser(
+      Lexer& L,
+      bool parse_complete_tensor_types,
+      bool allow_typevars)
+      : complete_tensor_types(parse_complete_tensor_types),
+        L(L),
+        allow_typevars_(allow_typevars) {}

 private:
   c10::optional<bool> tryToParseRequiresGrad();"
0,https://github.com/PyTorch/PyTorch/commit/fbc7559ceb372d88b55c96ef6984accbaa0ec3ec,YES,torch/_library/infer_schema.py,patch_2,,"+    def convert_type_string(annotation_type: str):
+        try:
+            return eval(annotation_type)
+        except Exception as e:
+            error_fn(
+                f""Unsupported type annotation {annotation_type}. It is not a type.""
+            )
+
","@@ -298,7 +298,10 @@ def out_wrapper(*out_names: str, exact_dtype: bool = False):
             annotation=out_type,
         )
         # Mark that the function now returns a tuple
-        assert sig.return_annotation in (sig.empty, out_type)
+        assert isinstance(sig.return_annotation, str) or sig.return_annotation in (
+            sig.empty,
+            out_type,
+        )
         params = chain(sig.parameters.values(), (out_param,))
         _fn.__signature__ = inspect.Signature(  # type: ignore[attr-defined]
             parameters=params, return_annotation=return_type  # type: ignore[arg-type]
-        assert isinstance(sig.return_annotation, str)
-        out_type = eval(sig.return_annotation)
-        assert out_type in _valid_typing_types, f""{sig.return_annotation} is incomplete or invalid""
+        if isinstance(sig.return_annotation, str):
+            out_type = convert_type_string(sig.return_annotation)
+            assert out_type in _valid_typing_types, f""{sig.return_annotation} is incomplete or invalid""
+        else:
+            assert sig.return_annotation in (sig.empty, out_type)"
0,https://github.com/PyTorch/PyTorch/commit/fbc7559ceb372d88b55c96ef6984accbaa0ec3ec,YES,torch/_library/infer_schema.py,patch_3,"-        if param.annotation not in SUPPORTED_PARAM_TYPES.keys():
-        schema_type = SUPPORTED_PARAM_TYPES[param.annotation]
","+        # The annotation might be converted to a string by annotation,
+        # we convert it to the actual type.
+        annotation_type = param.annotation
+        if type(annotation_type) == str:
+            annotation_type = convert_type_string(annotation_type)
+
+        if annotation_type not in SUPPORTED_PARAM_TYPES.keys():
+        schema_type = SUPPORTED_PARAM_TYPES[annotation_type]
","@@ -298,7 +298,10 @@ def out_wrapper(*out_names: str, exact_dtype: bool = False):
             annotation=out_type,
         )
         # Mark that the function now returns a tuple
-        assert sig.return_annotation in (sig.empty, out_type)
+        assert isinstance(sig.return_annotation, str) or sig.return_annotation in (
+            sig.empty,
+            out_type,
+        )
         params = chain(sig.parameters.values(), (out_param,))
         _fn.__signature__ = inspect.Signature(  # type: ignore[attr-defined]
             parameters=params, return_annotation=return_type  # type: ignore[arg-type]"
0,https://github.com/PyTorch/PyTorch/commit/fbc7559ceb372d88b55c96ef6984accbaa0ec3ec,YES,torch/_library/infer_schema.py,patch_4,"-    ret = parse_return(sig.return_annotation, error_fn)
","+    return_annotation = sig.return_annotation
+    if type(return_annotation) == str:
+        return_annotation = convert_type_string(return_annotation)
+    ret = parse_return(return_annotation, error_fn)
","-    return_annotation = sig.return_annotation
+    return_annotation = sig.return_annotation
+    if type(return_annotation) == str:
+        return_annotation = eval(return_annotation)
     ret = parse_return(return_annotation, error_fn)"
0,https://github.com/PyTorch/PyTorch/commit/ac768333be2ad7cd3435c1ae257bdaf297595714,YES,torch/_prims_common/__init__.py,patch_0,,"+def _maybe_get_pytype(t):
+    if t is torch.SymFloat:
+        return float
+    elif t is torch.SymInt:
+        return int
+    elif t is torch.SymBool:
+        return bool
+    else:
+        return t
+
+
","@@ -131,6 +131,15 @@ def decode_dtype(dtype: int):
     return dtype
 
 
+def value_to_dtype(value: Any) -> torch.dtype:
+    if isinstance(value, sympy.Expr):
+        if value.is_integer:  # type: ignore[attr-defined]
+            return torch.long
+        if value.is_real:
+            return torch.get_default_dtype()
+    return type_to_dtype(type(value))
+
+
 def is_integer_type(x):
     if isinstance(x, TensorBox):
         return is_integer_dtype(x.get_dtype()) or is_boolean_dtype(x.get_dtype())
+
+
+def _maybe_get_pytype(t):
+    if t is torch.SymFloat:
+        return float
+    elif t is torch.SymInt:
+        return int
+    elif t is torch.SymBool:
+        return bool
+    else:
+        return t"
0,https://github.com/PyTorch/PyTorch/commit/ac768333be2ad7cd3435c1ae257bdaf297595714,YES,torch/_prims_common/__init__.py,patch_1,"-    assert a in _ordered_types
-    assert b in _ordered_types
","+    a, b = _maybe_get_pytype(a), _maybe_get_pytype(b)
+    if a not in _ordered_types or b not in _ordered_types:
+        raise RuntimeError(f""Expected builtin numeric types, found {a}, {b}"")
","
-    a, b = _maybe_get_pytype(a), _maybe_get_pytype(b)
-    if a not in _ordered_types or b not in _ordered_types:
-        raise RuntimeError(f""Expected builtin numeric types, found {a}, {b}"")"
0,https://github.com/PyTorch/PyTorch/commit/ac768333be2ad7cd3435c1ae257bdaf297595714,YES,torch/_prims_common/__init__.py,patch_2,"-    ordered_types = (
-        bool,
-        int,
-        float,
-        complex,
-    )
-    assert a in ordered_types
-    assert b in ordered_types
-    for typ in ordered_types:
","+    a, b = _maybe_get_pytype(a), _maybe_get_pytype(b)
+    if a not in _ordered_types or b not in _ordered_types:
+        raise RuntimeError(f""Expected builtin numeric types, found {a}, {b}"")
+
+    for typ in _ordered_types:
","@@ -406,18 +406,15 @@ def istype(obj, allowed_types):

 
 def is_typing(value):
-    if sys.version_info < (3, 9):
-        return isinstance(value, typing._GenericAlias)
-    else:
-        return isinstance(
-            # `_SpecialForm`` is the parent class of `Optional`
-            value,
-            (
-                typing._SpecialGenericAlias,
-                typing._UnionGenericAlias,
-                typing._SpecialForm,
-            ),
-        )
+    # _Final catches most of typing classes:
+    #   - Any
+    #   - Callable
+    #   - Union
+    ...
+    #
+    # NB: we intentionally ignore classes that inherit from Generic, since they
+    # can be used as both TypingVariable as well as UserDefinedClassVariable.
+    return isinstance(value, typing._Final) or value is typing.Generic


 def is_numpy_int_type(value):"
0,https://github.com/PyTorch/PyTorch/commit/6d4ec9b2ecba7d26e885bdbb0faeeaa1e148cfd6,NO,torch/distributed/checkpoint/planner.py,patch_0,,"+
+
+class _Checkpointable:
+    """"""
+    Interface for checkpointable objects.
+    This is to allow arbitrary objects/tensor subclasses to hook into DCP seamlessly through implementing the interface.
+    """"""
+
+    @abc.abstractmethod
+    def _create_write_items(self, fqn: str, object: Any) -> List[WriteItem]:
+        """"""
+        Return a list of WriteItems based on object's contents.
+        """"""
+        raise NotImplementedError(
+            ""_Checkpointable._create_write_items is not implemented""
+        )
+
+    @abc.abstractmethod
+    def _create_chunk_list(self, tensor: torch.Tensor) -> List[ChunkStorageMetadata]:
+        """"""
+        Return a list of `ChunkStorageMetadata` based on object's contents.
+        """"""
+        raise NotImplementedError(
+            ""_Checkpointable._create_chunk_list is not implemented""
+        )
+
+    @abc.abstractmethod
+    def _get_tensor_shard(
+        self, tensor: torch.Tensor, index: MetadataIndex
+    ) -> torch.Tensor:
+        """"""
+        Return a 'torch.Tensor' shard based on 'MetadataIndex'.
+        """"""
+        raise NotImplementedError(
+            ""_Checkpointable._get_tensor_shard is not implemented""
+        )
","@@ -1,3 +1,39 @@
 
+from typing import Any, List
+import abc
+import torch
+
+
+class _Checkpointable(abc.ABC):
+    """"""
+    Interface for checkpointable objects.
+    This is to allow arbitrary objects/tensor subclasses to hook into DCP seamlessly through implementing the interface.
+    """"""
+
+    @abc.abstractmethod
+    def _create_write_items(self, fqn: str, object: Any) -> List[WriteItem]:
+        """"""
+        Return a list of WriteItems based on object's contents.
+        """"""
+        raise NotImplementedError(
+            ""_Checkpointable._create_write_items is not implemented""
+        )
+
+    @abc.abstractmethod
+    def _create_chunk_list(self, tensor: torch.Tensor) -> List[ChunkStorageMetadata]:
+        """"""
+        Return a list of `ChunkStorageMetadata` based on object's contents.
+        """"""
+        raise NotImplementedError(
+            ""_Checkpointable._create_chunk_list is not implemented""
+        )
+
+    @abc.abstractmethod
+    def _get_tensor_shard(
+        self, tensor: torch.Tensor, index: MetadataIndex
+    ) -> torch.Tensor:
+        """"""
+        Return a 'torch.Tensor' shard based on 'MetadataIndex'.
+        """"""
+        raise NotImplementedError(
+            ""_Checkpointable._get_tensor_shard is not implemented""
+        )"
0,https://github.com/PyTorch/PyTorch/commit/6d4ec9b2ecba7d26e885bdbb0faeeaa1e148cfd6,NO,torch/distributed/checkpoint/planner_helpers.py,patch_0,,"+from torch.distributed.checkpoint.planner import _Checkpointable
","@@ -7,6 +7,7 @@ import torch
 import torch.distributed as dist
 import torch.nn.functional as F
 from torch.distributed._tensor import DeviceMesh, DTensor, Replicate, Shard, distribute_tensor
+from torch.distributed._tensor.debug import CommDebugMode
 from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (
     checkpoint_wrapper,
     CheckpointImpl,
+from torch.distributed.checkpoint.planner import _Checkpointable"
0,https://github.com/PyTorch/PyTorch/commit/6d4ec9b2ecba7d26e885bdbb0faeeaa1e148cfd6,NO,torch/distributed/checkpoint/planner_helpers.py,patch_1,"-    if isinstance(object, DTensor):
",No
0,https://github.com/PyTorch/PyTorch/commit/6d4ec9b2ecba7d26e885bdbb0faeeaa1e148cfd6,NO,torch/distributed/checkpoint/planner_helpers.py,patch_2,"-    if isinstance(tensor, DTensor):
",No
0,https://github.com/PyTorch/PyTorch/commit/6d4ec9b2ecba7d26e885bdbb0faeeaa1e148cfd6,NO,torch/distributed/checkpoint/utils.py,patch_0,,"+from torch.distributed.checkpoint.planner import _Checkpointable
","@@ -7,6 +7,7 @@ import torch\n import torch.distributed as dist\n import torch.nn.functional as F\n from torch.distributed._tensor import DeviceMesh, DTensor, Replicate, Shard, distribute_tensor\n+from torch.distributed._tensor.debug import CommDebugMode\n from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n     checkpoint_wrapper,\n     CheckpointImpl,\n from torch.distributed.checkpoint.planner import _Checkpointable\n"
0,https://github.com/PyTorch/PyTorch/commit/6d4ec9b2ecba7d26e885bdbb0faeeaa1e148cfd6,NO,torch/distributed/checkpoint/utils.py,patch_1,"-    if isinstance(tensor, DTensor):
","+    if isinstance(tensor, _Checkpointable):
+        return tensor._get_tensor_shard(tensor, index)
+    elif isinstance(tensor, DTensor):
+        # DTensor can contain a local tensor that is a tensor subclass
+        if isinstance(tensor.to_local(), _Checkpointable):
+            return tensor.to_local()._get_tensor_shard(tensor, index)  # type: ignore[arg-type]
","@@ -1,5 +1,8 @@
-    if isinstance(tensor, DTensor):
+    if isinstance(tensor, _Checkpointable):
+        return tensor._get_tensor_shard(tensor, index)
     elif isinstance(tensor, DTensor):
+        # DTensor can contain a local tensor that is a tensor subclass
+        if isinstance(tensor.to_local(), _Checkpointable):
+            return tensor.to_local()._get_tensor_shard(tensor, index)  # type: ignore[arg-type]"
0,https://github.com/PyTorch/PyTorch/commit/ff432c048d353ff6944ccf50591c171372a924f7,YES,torch/fx/experimental/symbolic_shapes.py,patch_0,"-            if guard.expr in issued:
","+            if expr in issued:
","@@ -6,6 +6,20 @@
 #if AT_MKLDNN_ENABLED()
 #include <ideep.hpp>
 
+#ifndef IDEEP_PREREQ
+// Please find definitions of version numbers in ideep.hpp
+#if defined(IDEEP_VERSION_MAJOR) && defined(IDEEP_VERSION_MINOR) && \
+  defined(IDEEP_VERSION_PATCH) && defined(IDEEP_VERSION_REVISION)
+#define IDEEP_PREREQ(major, minor, patch, revision) \
+  (((IDEEP_VERSION_MAJOR << 16) + (IDEEP_VERSION_MINOR << 8) + \
+   (IDEEP_VERSION_PATCH << 0)) >= \
+   ((major << 16) + (minor << 8) + (patch << 0)) && \
+   (IDEEP_VERSION_REVISION >= revision))
+#else
+#define IDEEP_PREREQ(major, minor, patch, revision) 0
+#endif
+#endif
+
 namespace at { namespace native {
 
 // Mapping ScalarType to ideep tensor data_type"
0,https://github.com/PyTorch/PyTorch/commit/c36b31d5302d31746f3f3bd64ed8d9acd8e36155,YES,torch/csrc/api/src/nn/modules/adaptive.cpp,patch_0,,"+  TORCH_CHECK(
+      options.cutoffs().size() > 0,
+      ""cutoffs should be a sequence of length larger than 0"");
","@@ -80,6 +80,12 @@ void RNNImplBase<Derived>::reset() {
         options_base.num_layers());
   }
 
+  TORCH_CHECK(
+      options_base.hidden_size() > 0, ""hidden_size must be greater than zero"");
+
+  TORCH_CHECK(
+      options_base.num_layers() > 0, ""num_layers must be greater than zero"");
+
   TORCH_CHECK(
       0 <= options_base.proj_size() &&
           options_base.proj_size() < options_base.hidden_size(),

+  TORCH_CHECK(
+      options.cutoffs().size() > 0,
+      ""cutoffs should be a sequence of length larger than 0"");
"
0,https://github.com/PyTorch/PyTorch/commit/c36b31d5302d31746f3f3bd64ed8d9acd8e36155,YES,torch/nn/modules/adaptive.py,patch_0,,"+        if (len(cutoffs) == 0):
+            raise ValueError(""cutoffs should be a sequence of length larger than 0"")
+
","@@ -6342,7 +6342,7 @@ utils_device.CURRENT_DEVICE == None"""""".split(
 
     def test_guard_failure_fn_shape_control(self):
         def fn(x, y):
-            if x.shape[0] < 3:
+            if x.shape[0] < 4:
                 if y.shape[0] < 3:
                     return x * y
                 else:"
0,https://github.com/PyTorch/PyTorch/commit/c1d960aadd23bb261724c11f34d4a852ffaacfd6,NO,torch/_inductor/fx_passes/quantization.py,patch_0,,"+    # * the two inputs of binary node should have attribute ""meta"" and should be tensors
+    # * the two inputs of binary node should have the same shape
","@@ -3856,6 +3856,20 @@
                     # use CL stride for the output
                     output_stride = make_channels_last_strides_for(new_size)
                     break
+        any_input_is_storage_and_layout = any(is_storage_and_layout(x) for x in inputs)
+        fx_node_args = V.graph.current_node.args[0]
+        assert V.graph.current_node.target in [aten.cat, aten.cat.default]
+        assert isinstance(fx_node_args, list)
+        # If any of the inputs has meta tensor and the meta tensor is in CL format, use CL format for the output
+        if any_input_is_storage_and_layout is False and any(
+            ""val"" in arg.meta
+            and (
+                arg.meta[""val""].is_contiguous(memory_format=torch.channels_last)
+                or arg.meta[""val""].is_contiguous(memory_format=torch.channels_last_3d)
+            )
+            for arg in fx_node_args
+        ):
+            output_stride = make_channels_last_strides_for(new_size)
 
         concat_kernel = ConcatKernel(
             name=None,  "
0,https://github.com/PyTorch/PyTorch/commit/c1d960aadd23bb261724c11f34d4a852ffaacfd6,NO,torch/_inductor/fx_passes/quantization.py,patch_1,,"+            if not (
+                hasattr(binary_node_inputs[0], ""meta"")
+                and isinstance(
+                    binary_node_inputs[0].meta.get(""val"", None), torch.Tensor
+                )
+            ) or not (
+                hasattr(binary_node_inputs[1], ""meta"")
+                and isinstance(
+                    binary_node_inputs[1].meta.get(""val"", None), torch.Tensor
+                )
+            ):
+                return False
+            if (
+                binary_node_inputs[0].meta[""val""].size()
+                != binary_node_inputs[1].meta[""val""].size()
+            ):
+                return False
","@@ -197,15 +197,9 @@ if torch._C._has_mkldnn:
     def _binary_fusion_v2(computation_call, binary_fn):
         return CallFunction(binary_fn, computation_call, KeywordArg(""other""))
 
-    def _is_single_computation_op(computation_op, lowp_dtype=None):
+    def _is_single_computation_op(computation_op):

         def fn(match):
             computation_nodes = filter_nodes(match.nodes, computation_op)
 
             if len(computation_nodes) < 1:
                 return False
             if any(n.args[-3] != ""none"" for n in computation_nodes):
                 return False
-            if not (
-                hasattr(binary_node_inputs[0], ""meta"")
-                and isinstance(
-                    binary_node_inputs[0].meta.get(""val"", None), torch.Tensor
-                )
-            ) or not (
-                hasattr(binary_node_inputs[1], ""meta"")
-                and isinstance(
-                    binary_node_inputs[1].meta.get(""val"", None), torch.Tensor
-                )
-            ):
-                return False
-            if (
-                binary_node_inputs[0].meta[""val""].size()
-                != binary_node_inputs[1].meta[""val""].size()
-            ):
-                return False"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_0,"-from ._utils import flatten_args, modify_graph_op_device
","+from ._utils import flatten_args, modify_graph_op_device, validate_tensors_metadata
","--- a/graph_module.py
        +++ b/graph_module.py
        @@ -1097,7 +1097,7 @@ class GraphModuleSerializer(metaclass=Final):
                         # undefined Tensor which will be implicitly converted to None in Python.
                         output_arguments.append(Argument.create(as_none=()))
                     elif isinstance(meta, FakeTensor):
        -                assert isinstance(return_schema.real_type, torch.TensorType)
        +                assert isinstance(return_schema.real_type, (torch.OptionalType, torch.TensorType))
                         user_node = _output_node_at_index(node, idx)
                         name = (
                             user_node.name
        @@ -10,3 +10,4 @@ from ._utils import flatten_args, modify_graph_op_device, validate_tensors_metadata
         
         __all__ = [
             'GraphModule',
        +    'validate_tensors_metadata'
         ]"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_1,"-    pass
","+    def __init__(self, tensor):
+        self.meta = tensor.to(""meta"")
","-    pass
+    def __init__(self, tensor):
+        self.meta = tensor.detach().to(""meta"")"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_2,,"+        self._outputs_meta: Optional[Tuple[torch.Tensor, ...]] = None
","@@ -20,6 +20,7 @@ class PipelineModule(torch.nn.Module):
         self._outputs_meta: Optional[Tuple[torch.Tensor, ...]] = None
         self._exportable: bool = True
         self._buffers: Dict[str, torch.Tensor] = OrderedDict()
+        self._inputs_meta: Optional[Tuple[torch.Tensor, ...]] = None
         self._outputs_meta: Optional[Tuple[torch.Tensor, ...]] = None
         self._attributes: Dict[str, Any] = {}
         self._states: Dict[str, Any] = OrderedDict()"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_3,,"+    def _configure_outputs_meta(self, outputs_meta: Tuple[torch.Tensor, ...]):
+        """"""
+        Track the output shapes/dtype of this stage since they determine the send operation(s) which must match
+        recv operations of the next stage.  The next stage _will_ be freezing its recv buffers based on its initial
+        configuration, so it's important to also freeze/validate the output side to avoid any send/recv mismatches
+        which could show up as hangs, silent corruption, or other errors.
+        """"""
+        assert (
+            self._outputs_meta is None
+        ), ""Attempting to reconfigure output_meta, which is not supported""
+        self._outputs_meta = tuple(outputs_meta)  # type: ignore[assignment]
+
+    def get_outputs_meta(self) -> Tuple[torch.Tensor, ...]:
+        """"""Get the output metadata (meta tensors) reprensenting the outputs of this stage""""""
+        assert (
+            self._outputs_meta is not None
+        ), ""Attempted to get_outputs_meta() without configuring output meta""
+        return self._outputs_meta
+
","@@ -1294,5 +1336,37 @@ class MetaConverter:
             # non-Tensor types don't count as hit or miss
             return t
     
+        # Describe the tensor.  NB: do NOT disable ambient modes, we may need
+        # to query them when figuring out what to put in here
+        t_desc = self.describer.describe_tensor(t)
+
+        # Do the meta-fication.  Here, we disable all the ambient modes, to
+        # better simulate what would be like to re-fakeify from a fresh
+        # process
+        with contextlib.ExitStack() as exit_stack:
+            exit_stack.enter_context(torch._dispatch.python.suspend_functionalization())
+            st = peek_interpreter_stack()
+            if st is not None:
+                exit_stack.enter_context(
+                    torch._functorch.pyfunctorch.temporarily_clear_interpreter_stack()
+                )
+            exit_stack.enter_context(torch._C._DisableFuncTorch())
+
+            r = self.meta_tensor(
+                t_desc,
+                shape_env=shape_env,
+                callback=callback,
+                source=source,
+                symbolic_context=symbolic_context,
+            )
+
+        if type(t) is torch.nn.Parameter:
+            # NB: Cannot directly use Parameter constructor
+            # because that would force a detach, not desirable
+            r._is_param = True
+
+        # TODO: return the description for later
+        return r
+
     import torch._prims_common as utils

+    def _configure_outputs_meta(self, outputs_meta: Tuple[torch.Tensor, ...]):
+        """"""
+        Track the output shapes/dtype of this stage since they determine the send operation(s) which must match
+        recv operations of the next stage.  The next stage _will_ be freezing its recv buffers based on its initial
+        configuration, so it's important to also freeze/validate the output side to avoid any send/recv mismatches
+        which could show up as hangs, silent corruption, or other errors.
+        """"""
+        assert (
+            self._outputs_meta is None
+        ), ""Attempting to reconfigure output_meta, which is not supported""
+        self._outputs_meta = tuple(outputs_meta)  # type: ignore[assignment]
+
+    def get_outputs_meta(self) -> Tuple[torch.Tensor, ...]:
+        """"""Get the output metadata (meta tensors) reprensenting the outputs of this stage""""""
+        assert (
+            self._outputs_meta is not None
+        ), ""Attempted to get_outputs_meta() without configuring output meta""
+        return self._outputs_meta
+"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_4,,"+        self._validate_fwd_input(args, kwargs)
+
","@@ -779,13 +778,13 @@ def _export(
             fake_kwargs,
             equalities_inputs,
             original_signature,
-        ) = make_fake_inputs(f, args, kwargs, constraints)
+        ) = make_fake_inputs(mod, args, kwargs, constraints)

         fake_params_buffers = make_fake_params_buffers(
-            fake_mode, _get_params_buffers(f)
+            fake_mode, _get_params_buffers(mod)
         )
         ep_non_strict = _export_non_strict(
-            f,
+            mod,
             fake_args,
             fake_kwargs,
             fake_params_buffers,"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_5,,"+        self._validate_fwd_outputs(output_tuple)
","@@ -298,7 +298,10 @@
             annotation=out_type,
         )
         # Mark that the function now returns a tuple
-        assert sig.return_annotation in (sig.empty, out_type)
+        assert isinstance(sig.return_annotation, str) or sig.return_annotation in (
+            sig.empty,
+            out_type,
+        )
         params = chain(sig.parameters.values(), (out_param,))
         _fn.__signature__ = inspect.Signature(  # type: ignore[attr-defined]
             parameters=params, return_annotation=return_type  # type: ignore[arg-type]
-        self._validate_fwd_outputs(output_tuple)
+        return output_tuple"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_6,,"+    def _validate_fwd_input(self, args, kwargs):
+        """"""Raises a RuntimeError if shapes of input args/kwargs do not match the shapes configured for this stage.""""""
+
+        if self.is_first:
+            # TODO why is there a separate recv_info for each pipeline chunk?
+            expected_args = self.args_recv_info[self.fwd_chunk_id]
+        else:
+            expected_args = tuple()
+
+        if len(kwargs):
+            # TODO- need a mapping of kwarg to position in self.args_recv_info
+            # without it, we just validate shapes for args and ignore kwargs
+            expected_args = expected_args[: len(expected_args) - len(kwargs)]
+
+        # TODO- need a mapping of kwarg to position in self.args_recv_info
+        # maybe it's impossible to tell whether the len mismatches because
+        # (a) the user passed an extra arg or missed an arg
+        # (b) the user did not pass a kwarg, which has a default value baked into expected_args
+        expected_tensors_meta = [
+            e.meta if isinstance(e, _RootArgPlaceholder) else e.buffer
+            for e in expected_args
+        ]
+        validate_tensors_metadata(""forward input args"", expected_tensors_meta, args)
+
+    def _validate_fwd_outputs(self, outputs: Tuple[torch.Tensor, ...]):
+        """"""Raises a RuntimeError if this stage produces an output of unexpected shape/dtype.
+        Most likely, this could be cause either by incorrect user specification of output shapes, or becuase
+        shape inference was done on the original model but then at runtime the model is wrapped with something like
+        mixed precision which changes output dtype.
+        """"""
+        expected_tensors_meta = self.get_outputs_meta()
+        validate_tensors_metadata(""forward outputs"", expected_tensors_meta, outputs)
+
","@@ -943,106 +804,90 @@ class PipelineScheduleMulti(_PipelineSchedule):
                 all_next_ranks.add(self.stage_index_to_group_rank[stage_index + 1])
 
         for time_step, action in enumerate(self.pipeline_order[self.rank]):
-            try:
-                ops: List[dist.P2POp] = []
-                if action is not None:
-                    computation_type, mb_index, stage_index = action
-                    if computation_type == _ComputationType.FORWARD:
-                        # perform forward computation
-                        stage = stage_index_to_stage[stage_index]
-                        output = stage.forward_one_chunk(
-                            mb_index, arg_mbs[mb_index], kwarg_mbs[mb_index]
+            ops: List[dist.P2POp] = []
+            if action is not None:
+                computation_type, mb_index, stage_index = action
+                if computation_type == _ComputationType.FORWARD:
+                    # perform forward computation
+                    stage = stage_index_to_stage[stage_index]
+                    output = stage.forward_one_chunk(
+                        mb_index, arg_mbs[mb_index], kwarg_mbs[mb_index]
+                    )
+                    self._validate_fwd_outputs(output)
+                    ops.extend(stage.get_fwd_send_ops(mb_index))
                 elif computation_type == _ComputationType.BACKWARD:
-                        # perform backward computation
-                        stage = stage_index_to_stage[stage_index]
-                        loss = self._maybe_get_loss(stage, mb_index)
-                        stage.backward_one_chunk(
-                            mb_index, loss=loss, full_backward=self.use_full_backward
-                        )
-                        ops.extend(stage.get_bwd_send_ops(mb_index))
-                    elif computation_type == _ComputationType.WEIGHT:
-                        # perform weight update
-                        if self.use_full_backward:
-                            raise ValueError(
-                                f""We detected a weight update in the pipeline schedule, but \\\
-                                {self.use_full_backward=}""
+                    # perform backward computation
+                    stage = stage_index_to_stage[stage_index]
+                    loss = self._maybe_get_loss(stage, mb_index)
+                    stage.backward_one_chunk(
+                        mb_index, loss=loss, full_backward=self.use_full_backward
+                    )
+                    ops.extend(stage.get_bwd_send_ops(mb_index))
+                elif computation_type == _ComputationType.WEIGHT:
+                    # perform weight update
+                    if self.use_full_backward:
+                        raise ValueError(
+                            f""We detected a weight update in the pipeline schedule, but \\\
+                            {self.use_full_backward=}""
                         )
-                        stage = stage_index_to_stage[stage_index]
-                        stage.backward_weight_one_chunk(mb_index)
+                    stage = stage_index_to_stage[stage_index]
+                    stage.backward_weight_one_chunk(mb_index)
+                else:
+                    raise ValueError(f""Unknown computation type {computation_type}"")
+
+            # Look at the neighboring ranks for this current timestep and determine whether
+            # this current rank needs to do any recv communication
+            for prev_rank in all_prev_ranks:
+                prev_rank_ops = self.pipeline_order[prev_rank]
+                prev_rank_action = None
+                if time_step < len(prev_rank_ops):
+                    prev_rank_action = prev_rank_ops[time_step]
+                if prev_rank_action is not None:
+                    computation_type, mb_index, stage_index = prev_rank_action
+                    # Only handle sends for the forward from a previous rank
+                    if computation_type == _ComputationType.FORWARD:
+                        # If not the last stage, then receive fwd activations
+                        if stage_index + 1 in stage_index_to_stage:
+                            # TODO: We are assuming that stage will always receive from stage-1
+                            # however that is not necessarily true of get_fwd_recv_ops
+                            stage = stage_index_to_stage[stage_index + 1]
+                            ops.extend(stage.get_fwd_recv_ops(mb_index))
+                    elif (
+                        computation_type == _ComputationType.BACKWARD
+                        or computation_type == _ComputationType.WEIGHT
+                    ):
+                        # Previous rank doing backward or weight update has no influence for the current rank forward recv
+                        pass
+                    else:
+                        raise ValueError(f""Unknown computation type {computation_type}"")
+
+            for next_rank in all_next_ranks:
+                next_rank_ops = self.pipeline_order[next_rank]
+                next_rank_action = None
+                if time_step < len(next_rank_ops):
+                    next_rank_action = next_rank_ops[time_step]
+                if next_rank_action is not None:
+                    computation_type, mb_index, stage_index = next_rank_action
+                    # Only handle receives for the backwards from a next rank
+                    if (
+                        computation_type == _ComputationType.FORWARD
+                        or computation_type == _ComputationType.WEIGHT
+                    ):
+                        # Next rank doing forward or weight update has no influence for the current rank backward recv
+                        pass
                     elif computation_type == _ComputationType.BACKWARD:
-                        # If not the first stage, then receive bwd gradients
-                        if stage_index - 1 in stage_index_to_stage:
-                            # TODO: We are assuming that stage will always receive from stage+1
-                            # however that is not necessarily true of get_bwd_recv_ops
-                            stage = stage_index_to_stage[stage_index - 1]
-                            ops.extend(stage.get_bwd_recv_ops(mb_index))
+                        # If not the first stage, then receive bwd gradients
+                        if stage_index - 1 in stage_index_to_stage:
+                            # TODO: We are assuming that stage will always receive from stage+1
+                            # however that is not necessarily true of get_bwd_recv_ops
+                            stage = stage_index_to_stage[stage_index - 1]
+                            ops.extend(stage.get_bwd_recv_ops(mb_index))
                     else:
                         raise ValueError(f""Unknown computation type {computation_type}"")
-                # do the communication
-                if ops:
-                    _batch_p2p(ops).wait()
-            except Exception as e:
-                logger.error(
-                    ""[Rank %s] pipeline schedule %s caught the following exception \\\
-                     at time_step %s when running action %s"",
-                    self.rank,
-                    self.__class__.__name__,
-                    time_step,
-                    action,
-                )
-                logger.error(""%s"", _format_pipeline_order(self.pipeline_order))
-                raise e
+
+            # do the communication
+            if ops:
+                _batch_p2p(ops).wait()
         # Return losses if there is a container passed in
         self._update_losses(self._stages, losses)
 
+    def _validate_fwd_input(self, args, kwargs):
+        """"""Raises a RuntimeError if shapes of input args/kwargs do not match the shapes configured for this stage.""""""
+
+        if self.is_first:
+            # TODO why is there a separate recv_info for each pipeline chunk?
+            expected_args = self.args_recv_info[self.fwd_chunk_id]
+        else:
+            expected_args = tuple()
+
+        if len(kwargs):
+            # TODO- need a mapping of kwarg to position in self.args_recv_info
+            # without it, we just validate shapes for args and ignore kwargs
+            expected_args = expected_args[: len(expected_args) - len(kwargs)]
+
+        # TODO- need a mapping of kwarg to position in self.args_recv_info
+        # maybe it's impossible to tell whether the len mismatches because
+        # (a) the user passed an extra arg or missed an arg
+        # (b) the user did not pass a kwarg, which has a default value baked into expected_args
+        expected_tensors_meta = [
+            e.meta if isinstance(e, _RootArgPlaceholder) else e.buffer
+            for e in expected_args
+        ]
+        validate_tensors_metadata(""forward input args"", expected_tensors_meta, args)
+
+    def _validate_fwd_outputs(self, outputs: Tuple[torch.Tensor, ...]):
+        """"""Raises a RuntimeError if this stage produces an output of unexpected shape/dtype.
+        Most likely, this could be cause either by incorrect user specification of output shapes, or becuase
+        shape inference was done on the original model but then at runtime the model is wrapped with something like
+        mixed precision which changes output dtype.
+        """"""
+        expected_tensors_meta = self.get_outputs_meta()
+        validate_tensors_metadata(""forward outputs"", expected_tensors_meta, outputs)
+"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_7,"-                return _RootArgPlaceholder()
","+            example_value = placeholder.meta[""val""]
+                return _RootArgPlaceholder(example_value)
","-                return _RootArgPlaceholder()
+            example_value = placeholder.meta[""val""]
+            return _RootArgPlaceholder(example_value)"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_8,"-            example_value = placeholder.meta[""val""]
",,"@@ -0,0 +1,1 @@
+            example_value = placeholder.meta[""val""].clone().detach().requires_grad_(True)"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_9,,"+        output_node = self._get_output_node()
+        output_vals: Tuple[torch.Tensor] = tuple(
+            v.meta[""val""] for v in flatten_args(output_node.args)
+        )
+        self._configure_outputs_meta(output_vals)
+
+    def _get_output_node(self):
+        output_nodes = [node for node in self.submod.graph.nodes if node.op == ""output""]
+        assert len(output_nodes) == 1
+        output_node = output_nodes[0]
+        return output_node
+
","@@ -1401,13 +1392,6 @@ def _shape_env_from_inputs(inputs: List[torch.Tensor]):
     return None
 
 
-def output_node(gm: torch.fx.GraphModule):
-    """"""Get the output node from an FX graph""""""
-    last_node = next(iter(reversed(gm.graph.nodes)))
-    assert last_node.op == ""output""
-    return last_node
-
-
 def graph_returns_tuple(gm: torch.fx.GraphModule):
     """"""True if a FX graph returns a tuple""""""
     if not isinstance(gm, torch.fx.GraphModule):
-        return False
+        return False
     	elif hasattr(gm, 'return_types'):
         	return isinstance(gm.return_types, tuple)
     elif hasattr(gm, 'graph'):"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_10,"-        output_nodes = [node for node in self.submod.graph.nodes if node.op == ""output""]
-        assert len(output_nodes) == 1
-        output_node = output_nodes[0]
","+        output_node = self._get_output_node()
+
","@@ -163,6 +201,9 @@ class Verifier(metaclass=_VerifierMeta):

                 elif node.op == ""placeholder"":
                     _check_val(node)
+                # TODO(zhxchen17)
+                # elif node.op == ""output"":
+                #     _check_flattened_outputs()
 
         self.check_additional(gm)
 

-        output_nodes = [node for node in self.submod.graph.nodes if node.op == ""output""]
-        assert len(output_nodes) == 1
-        output_node = output_nodes[0]
+        output_node = self._get_output_node()
+        +
        "
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_11,,"+        self._configure_outputs_meta(tuple(self.outputs))
+
","@@ -1155,8 +1159,8 @@ def compile_fx(
 
     def get_patched_config_dict(config_patches=None):
-    with config.patch(config_patches):
-        return config.get_config_copy()
+    with config.patch(config_patches):  # type: ignore[attr-defined]
+        return config.get_config_copy()  # type: ignore[attr-defined]
 
 
 def _shape_env_from_inputs(inputs: List[torch.Tensor]):
+        self._configure_outputs_meta(tuple(self.outputs))
+        
"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/PipelineStage.py,patch_12,"-                    [_RootArgPlaceholder() for _ in self.inputs]
","+                    [_RootArgPlaceholder(i) for i in self.inputs]
","@@ -457,7 +457,7 @@ def _str_intern(inp, *, tensor_contents=None):
                 indices_str = ""...""
             else:
                 indices_str = _tensor_str(indices, indent + len(indices_prefix))
-            if indices.numel() == 0 or is_meta:
+            if is_meta or indices.numel() == 0:
                 indices_str += "", size="" + str(tuple(indices.shape))
             values_prefix = ""values=tensor(""
             values = self._values().detach()"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/_utils.py,patch_0,"-from typing import Dict, Optional
","+from typing import Dict, List, Optional, Tuple, Union
","- from typing import Dict, List, Optional, Tuple, Union
+ from typing import Dict, Optional"
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,NO,torch/distributed/pipelining/_utils.py,patch_1,,"+
+
+class PipeliningShapeError(RuntimeError):
+    """"""Shape mismatch between configured and runtime values.""""""
+
+
+def validate_tensor_metadata(desc, expected, given):
+    if not expected.shape == given.shape:
+        raise PipeliningShapeError(
+            f""{desc} has a shape mismatch: expected {expected.shape} actual {given.shape}""
+        )
+    if not expected.dtype == given.dtype:
+        raise PipeliningShapeError(
+            f""{desc} has a dtype mismatch: expected {expected.dtype} actual {given.dtype}""
+        )
+    if not expected.stride() == given.stride():
+        raise PipeliningShapeError(
+            f""{desc} has a stride mismatch: expected {expected.stride()} actual {given.stride()}""
+        )
+
+
+def validate_tensors_metadata(
+    desc,
+    expected_tensors: Union[List[torch.Tensor], Tuple[torch.Tensor, ...]],
+    actual_tensors: Union[List[torch.Tensor], Tuple[torch.Tensor, ...]],
+):
+    if len(expected_tensors) != len(actual_tensors):
+        raise PipeliningShapeError(
+            f""Number of {desc} ({len(actual_tensors)}) does not match expected number ({len(expected_tensors)})""
+        )
+    for i in range(len(expected_tensors)):
+        validate_tensor_metadata(f""{desc}[{i}]"", expected_tensors[i], actual_tensors[i])
","--- a
+++ b
@@ -1,3 +1,14 @@
 
+class PipeliningShapeError(RuntimeError):
+    """"""Shape mismatch between configured and runtime values.""""""
+
+
+def validate_tensor_metadata(desc, expected, given):
+    if not expected.shape == given.shape:
+        raise PipeliningShapeError(
+            f""{desc} has a shape mismatch: expected {expected.shape} actual {given.shape}""
+        )
+    if not expected.dtype == given.dtype:
+        raise PipeliningShapeError(
+            f""{desc} has a dtype mismatch: expected {expected.dtype} actual {given.dtype}""
+        )
+
+
 def validate_tensors_metadata(
     desc,
     expected_tensors: Union[List[torch.Tensor], Tuple[torch.Tensor, ...]],
     actual_tensors: Union[List[torch.Tensor], Tuple[torch.Tensor, ...]],
 ):
     if len(expected_tensors) != len(actual_tensors):
         raise PipeliningShapeError(
             f""Number of {desc} ({len(actual_tensors)}) does not match expected number ({len(expected_tensors)})""
         )
     for i in range(len(expected_tensors)):
         validate_tensor_metadata(f""{desc}[{i}]"", expected_tensors[i], actual_tensors[i])"
0,https://github.com/PyTorch/PyTorch/commit/362bc6d7cbcac57466a52701fac3ba3bfb668000,YES,aten/src/ATen/native/MaxPooling.cpp,patch_0,"-#include <ATen/native/Pool.h>
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/362bc6d7cbcac57466a52701fac3ba3bfb668000,YES,aten/src/ATen/native/MaxPooling.cpp,patch_1,"-static void check_max_pool1d(
-    const Tensor& self,
-    IntArrayRef kernel_size,
-    IntArrayRef stride,
-    IntArrayRef padding,
-    IntArrayRef dilation,
-    bool ceil_mode) {
-
-  TORCH_CHECK(
-      self.dim() == 2 || self.dim() == 3,
-      ""max_pool1d() Expected 2D or 3D input tensor, but got "", self.sym_sizes());
-  TORCH_CHECK(
-      kernel_size.size() == 1,
-      ""max_pool1d() kernel_size must be an int, list of ints or tuple of ints of size 1 but got size "",
-      kernel_size.size());
-  TORCH_CHECK(
-      stride.empty() || stride.size() == 1,
-      ""max_pool1d() stride must be None, an int, list of ints, or tuple of ints of size 1 but got size "",
-      stride.size());
-  TORCH_CHECK(
-      padding.size() == 1,
-      ""max_pool1d() padding must be an int, list of ints, or tuple of ints of size 1 but got size "",
-      padding.size());
-  TORCH_CHECK(
-      dilation.size() == 1,
-      ""max_pool1d() dilation must be an int, list of ints or tuple of ints of size 1 but got size "",
-      dilation.size());
-
-  // If stride=None then set it to kernel_size
-  if (stride.empty()) {
-    stride = kernel_size;
-  }
-
-  TORCH_CHECK(
-      kernel_size[0] > 0,
-      ""max_pool1d() kernel_size must be greater than zero, but got "",
-      kernel_size[0]);
-  TORCH_CHECK(
-      stride[0] > 0, ""max_pool1d() stride must be greater than zero, but got "", stride[0]);
-  TORCH_CHECK(
-      padding[0] >= 0, ""max_pool1d() padding must be non-negative, but got "", padding[0]);
-  TORCH_CHECK(
-      padding[0] <= kernel_size[0] / 2,
-      ""max_pool1d() padding should be at most half of kernel size, but got padding="",
-      padding[0],
-      "" and kernel_size="",
-      kernel_size[0]);
-  TORCH_CHECK(
-      dilation[0] > 0, ""max_pool1d() dilation must be greater than zero, but got "", dilation[0]);
-
-  const int64_t OW = pooling_output_shape(self.sym_size(-1).guard_int(__FILE__, __LINE__), kernel_size[0], padding[0], stride[0], dilation[0], ceil_mode);
-  TORCH_CHECK(OW > 0, ""max_pool1d() Invalid computed output size: "", OW);
-}
-
-} // namespace
-
-namespace {
-
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/362bc6d7cbcac57466a52701fac3ba3bfb668000,YES,aten/src/ATen/native/MaxPooling.h,patch_0,,"+#include <ATen/native/Pool.h>
+static void check_max_pool1d(
+    const Tensor& self,
+    IntArrayRef kernel_size,
+    IntArrayRef stride,
+    IntArrayRef padding,
+    IntArrayRef dilation,
+    bool ceil_mode) {
+
+  TORCH_CHECK(
+      self.dim() == 2 || self.dim() == 3,
+      ""max_pool1d() Expected 2D or 3D input tensor, but got "", self.sym_sizes());
+  TORCH_CHECK(
+      kernel_size.size() == 1,
+      ""max_pool1d() kernel_size must be an int, list of ints or tuple of ints of size 1 but got size "",
+      kernel_size.size());
+  TORCH_CHECK(
+      stride.empty() || stride.size() == 1,
+      ""max_pool1d() stride must be None, an int, list of ints, or tuple of ints of size 1 but got size "",
+      stride.size());
+  TORCH_CHECK(
+      padding.size() == 1,
+      ""max_pool1d() padding must be an int, list of ints, or tuple of ints of size 1 but got size "",
+      padding.size());
+  TORCH_CHECK(
+      dilation.size() == 1,
+      ""max_pool1d() dilation must be an int, list of ints or tuple of ints of size 1 but got size "",
+      dilation.size());
+
+  // If stride=None then set it to kernel_size
+  if (stride.empty()) {
+    stride = kernel_size;
+  }
+
+  TORCH_CHECK(
+      kernel_size[0] > 0,
+      ""max_pool1d() kernel_size must be greater than zero, but got "",
+      kernel_size[0]);
+  TORCH_CHECK(
+      stride[0] > 0, ""max_pool1d() stride must be greater than zero, but got "", stride[0]);
+  TORCH_CHECK(
+      padding[0] >= 0, ""max_pool1d() padding must be non-negative, but got "", padding[0]);
+  TORCH_CHECK(
+      padding[0] <= kernel_size[0] / 2,
+      ""max_pool1d() padding should be at most half of kernel size, but got padding="",
+      padding[0],
+      "" and kernel_size="",
+      kernel_size[0]);
+  TORCH_CHECK(
+      dilation[0] > 0, ""max_pool1d() dilation must be greater than zero, but got "", dilation[0]);
+
+  const int64_t OW = pooling_output_shape(self.sym_size(-1).guard_int(__FILE__, __LINE__), kernel_size[0], padding[0], stride[0], dilation[0], ceil_mode);
+  TORCH_CHECK(OW > 0, ""max_pool1d() Invalid computed output size: "", OW);
+}
+
","@@ -310,7 +310,7 @@ static void check_max_pool1d(
     TORCH_CHECK(
         kernel_size[0] > 0,
         ""max_pool1d() kernel_size must be greater than zero, but got "",
         kernel_size[0]);
-    TORCH_CHECK(
-        stride[0] > 0, ""max_pool1d() stride must be greater than zero, but got "", stride[0]);
+    TORCH_CHECK(stride[0] > 0, ""max_pool1d() stride must be greater than zero, but got "", stride[0]);
     TORCH_CHECK(
         padding[0] >= 0, ""max_pool1d() padding must be non-negative, but got "", padding[0]);
     TORCH_CHECK("
0,https://github.com/PyTorch/PyTorch/commit/362bc6d7cbcac57466a52701fac3ba3bfb668000,YES,aten/src/ATen/native/quantized/cpu/Pooling.cpp,patch_0,,"+#include <ATen/native/MaxPooling.h>
","@@ -155,6 +155,10 @@ QuantizedMaxPool1dImpl::initialize(
 void QuantizedMaxPool1dImpl::initialize(
     const std::vector<int64_t>& kernel_size) {
+  // Check if the kernel size is empty
+  if (kernel_size.size() == 0) {
+    return;
+  }
   AT_ASSERT(kernel_size.size() == 1);
   kernel_size_ = kernel_size[0];
 }"
0,https://github.com/PyTorch/PyTorch/commit/362bc6d7cbcac57466a52701fac3ba3bfb668000,YES,aten/src/ATen/native/quantized/cpu/Pooling.cpp,patch_1,,"+  check_max_pool1d(qx, kernel_size, stride, padding, dilation, ceil_mode);
","@@ -4245,32 +4261,46 @@ def adaptive_max_pool2d(x, output_size):
         # Kernel size too big. Results in hard-to-optimize Triton code. Use fallback.
         return fallback_adaptive_max_pool2d(x, output_size)
 
-    inner_func_max_val = _adaptive_pooling_idx_max(
+    def start_index(index, out_dim, inp_dim):
+        return FloorDiv((index * inp_dim), out_dim)
+
+    def end_index(index, out_dim, inp_dim):
+        return FloorDiv((index + 1) * inp_dim + out_dim - 1, out_dim)
+
+    inner_func_max_val = _adaptive_pooling_fn(
+        start_index=start_index,
+        end_index=end_index,
         kernel_maxes=[h_kernel_max, w_kernel_max],
         in_sizes=[h_in, w_in],
         out_sizes=[h_out, w_out],
-        return_index=False,
-        loader=pad_adaptive_loader(x, float(""-inf"")),
+        pooling_fn=ops.maximum,
     )
 
-    inner_func_max_idx = _adaptive_pooling_idx_max(
+    inner_func_max_idx = _adaptive_pooling_fn_with_idx(
+        start_index=start_index,
+        end_index=end_index,
         kernel_maxes=[h_kernel_max, w_kernel_max],
         in_sizes=[h_in, w_in],
         out_sizes=[h_out, w_out],
-        return_index=True,
-        loader=pad_adaptive_loader(x, float(""-inf"")),
+        pooling_fn=ops.maximum,
     )
 
+    def inner_fn_max_val(idx):
+        return inner_func_max_val(idx, pad_adaptive_loader(x, float(""-inf"")))
+
+    def inner_fn_max_idx(idx):
+        return inner_func_max_idx(idx, pad_adaptive_loader(x, float(""-inf"")))
+
     rv = Pointwise.create(
         device=x.get_device(),
         dtype=dtype,
-        inner_fn=inner_func_max_val,
+        inner_fn=inner_fn_max_val,
         ranges=new_size,
     )
     ri = Pointwise.create(
         device=x.get_device(),
         dtype=torch.int64,
-        inner_fn=inner_func_max_idx,
+        inner_fn=inner_fn_max_idx,
         ranges=new_size,
     )
     return rv, ri
+    check_max_pool1d(qx, kernel_size, stride, padding, dilation, ceil_mode);"
0,https://github.com/PyTorch/PyTorch/commit/43069c460e87ba77fd69d2193635179779328e27,YES,torch/csrc/jit/runtime/register_special_ops.cpp,patch_0,"-      elem_type != BoolType::get()) {
","+      !elem_type->isSubtypeOf(*BoolType::get())) {
","-      elem_type != BoolType::get()) {
+      !elem_type->isSubtypeOf(*BoolType::get())) {"
0,https://github.com/PyTorch/PyTorch/commit/40e8675fcbb233c98ec532607d5cd421ec850253,NO,aten/src/ATen/cudnn/Descriptors.h,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/40e8675fcbb233c98ec532607d5cd421ec850253,NO,aten/src/ATen/native/LossCTC.cpp,patch_0,"-          {log_probs, targets, input_lengths, target_lengths})) {
",No
0,https://github.com/PyTorch/PyTorch/commit/40e8675fcbb233c98ec532607d5cd421ec850253,NO,aten/src/ATen/native/cudnn/LossCTC.cpp,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/40e8675fcbb233c98ec532607d5cd421ec850253,NO,aten/src/ATen/native/cudnn/LossCTC.cpp,patch_1,,No
0,https://github.com/PyTorch/PyTorch/commit/40e8675fcbb233c98ec532607d5cd421ec850253,NO,aten/src/ATen/native/cudnn/LossCTC.cpp,patch_2,"-      (log_probs.device().type() == at::kCUDA) &&
-      (log_probs.dim() == 3);
",No
0,https://github.com/PyTorch/PyTorch/commit/40e8675fcbb233c98ec532607d5cd421ec850253,NO,aten/src/ATen/native/cudnn/LossCTC.cpp,patch_3,"-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();
-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();
-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());
-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());
-  return at::_use_cudnn_ctc_loss(log_probs, targets, il, tl, BLANK);
","+  auto& ctx = at::globalContext();
+
+  bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&
+      (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&
+      (targets.scalar_type() == at::kInt) &&
+      (log_probs.device().type() == at::kCUDA) && (targets.is_contiguous()) &&
+      (log_probs.dim() == 3) && (input_lengths.scalar_type() == at::kInt) &&
+      (target_lengths.scalar_type() == at::kInt);
+
+  if (at::cuda::currentStreamCaptureStatus() == at::cuda::CaptureStatus::None) {
+    Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();
+    IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());
+    for (const auto b : c10::irange(tl.size())) {
+      // target length < 256 is documented, but we see illegal memory accesses
+      // when target lengths > input lengths for CuDNN
+      Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();
+      Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();
+      IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());
+      IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());
+      use_cudnn = use_cudnn && (tl[b] < 256) && (tl[b] <= il[b]);
+      if (!use_cudnn) {
+        tensor_failed_target_lengths_check = true;
+        break;
+      }
+    }
+  } else {
+    use_cudnn = use_cudnn && !tensor_failed_target_lengths_check;
+    if (tensor_failed_target_lengths_check) {
+      TORCH_WARN(
+          ""cuDNN max target length restriction < 256 cannot be checked during graph capture,""
+          "" but target length >= 256 was observed previously e.g., during warmup, so we""
+          "" presume it is unsafe to dispatch to cuDNN ctc_loss."");
+    }
+  }
+
+  return use_cudnn;
","@@ -42,15 +42,17 @@
   Tensor one_hot(const Tensor &self, int64_t num_classes) {
     }
 
     // non-empty tensor
-    if (self.device().type() != at::kCUDA && self.device().type() != at::kMPS) {
-      //for cuda, rely on device assert thrown by scatter
+    if (self.device().type() != at::kCUDA && self.device().type() != at::kMPS &&
+        self.device().type() != at::kPrivateUse1) {
+      // for cuda, rely on device assert thrown by scatter
       TORCH_CHECK(self.min().item().toLong() >= 0, ""Class values must be non-negative."");
     }
     if (num_classes == -1) {
         num_classes = self.max().item().toLong() + 1;
     } else {
-        if (self.device().type() != at::kCUDA && self.device().type() != at::kMPS) {
-          //rely on device asserts from scatter to avoid sync here
+        if (self.device().type() != at::kCUDA && self.device().type() != at::kMPS &&
+            self.device().type() != at::kPrivateUse1) {
+          // rely on device asserts from scatter to avoid sync here
           TORCH_CHECK(num_classes > self.max().item().toLong(), ""Class values must be smaller than num_classes."");
         } else {
             //for cuda, assert that num_classes is at least 1
+          }
       }
   }

+  auto& ctx = at::globalContext();
+  
+  bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&
+      (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&
+      (targets.scalar_type() == at::kInt) &&
+      (log_probs.device().type() == at::kCUDA) && (targets.is_contiguous()) &&
+      (log_probs.dim() == 3) && (input_lengths.scalar_type() == at::kInt) &&
+      (target_lengths.scalar_type() == at::kInt);
+
+  if (at::cuda::currentStreamCaptureStatus() == at::cuda::CaptureStatus::None) {
+    Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();
+    IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());
+    for (const auto b : c10::irange(tl.size())) {
+      // target length < 256 is documented, but we see illegal memory accesses
+      // when target lengths > input lengths for CuDNN
+      Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();
+      Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();
+      IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());
+      IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());
+      use_cudnn = use_cudnn && (tl[b] < 256) && (tl[b] <= il[b]);
+      if (!use_cudnn) {
+        tensor_failed_target_lengths_check = true;
+        break;
+      }
+    }
+  } else {
+    use_cudnn = use_cudnn && !tensor_failed_target_lengths_check;
+    if (tensor_failed_target_lengths_check) {
+      TORCH_WARN(
+          ""cuDNN max target length restriction < 256 cannot be checked during graph capture,""
+          "" but target length >= 256 was observed previously e.g., during warmup, so we""
+          "" presume it is unsafe to dispatch to cuDNN ctc_loss."");
+    }
+  }
+
+  return use_cudnn;"
0,https://github.com/PyTorch/PyTorch/commit/40e8675fcbb233c98ec532607d5cd421ec850253,NO,aten/src/ATen/native/cudnn/LossCTC.cpp,patch_4,"-    const Tensor& log_probs,
-    const Tensor& targets,
-  Tensor ilc = input_lengths.to(Device(at::kCPU), at::kLong).contiguous();
-  Tensor tlc = target_lengths.to(Device(at::kCPU), at::kLong).contiguous();
-  IntArrayRef il(ilc.data_ptr<int64_t>(), ilc.numel());
-  IntArrayRef tl(tlc.data_ptr<int64_t>(), tlc.numel());
-  return at::_cudnn_ctc_loss(
-      log_probs, targets, il, tl, BLANK, deterministic, zero_infinity);
","+    const Tensor& log_probs_t,
+    const Tensor& targets_t,
+  Tensor targets_t_ = targets_t;
+  if (targets_t.device().type() == at::kCPU) {
+    targets_t_ = targets_t.to(Device(at::kCUDA));
+  }
+  const CheckedFrom c = ""cudnn_ctc_loss"";
+  const TensorArg log_probs{log_probs_t, ""log_probs"", 1};
+  const TensorArg targets{targets_t_, ""targets"", 2};
+  checkDim(c, log_probs, 3);
+  checkScalarType(c, log_probs, kFloat);
+  checkDim(c, targets, 1);
+  checkScalarType(c, targets, kInt);
+  checkContiguous(c, targets); // ?
+  checkBackend(c, {*log_probs}, Backend::CUDA);
+  checkBackend(c, {*targets}, Backend::CUDA);
+  const auto batch_size = log_probs->size(1);
+  int64_t input_lengths_size =
+      input_lengths.sizes().size() ? input_lengths.size(0) : 1;
+  int64_t target_lengths_size =
+      target_lengths.sizes().size() ? target_lengths.size(0) : 1;
+  TORCH_CHECK(
+      input_lengths_size == batch_size,
+      ""input_lengths needs to have size to match batch_size"");
+  TORCH_CHECK(
+      target_lengths_size == batch_size,
+      ""target_lengths needs to have size to match batch_size"");
+
+  TORCH_CHECK(BLANK == 0, ""blank must be label 0 for cudnn_ctc_loss"");
+  // checked in dispatch:
+  // assert other conditions for cudnnCTCLoss: all label lengths <= 256
+  // all input lengths = logprob.size(0)
+
+  const auto handle = getCudnnHandle();
+
+  const cudnnCTCLossAlgo_t algo =
+      (deterministic ? CUDNN_CTC_LOSS_ALGO_DETERMINISTIC
+                     : CUDNN_CTC_LOSS_ALGO_NON_DETERMINISTIC);
+
+  CTCLossDescriptor ctc_loss_desc;
+
+  // so the CuDNN gradient semantics have changed between 7.1 and 7.6,
+  // this is CuDNN 7.6 only, see PyTorch 1.2 for older CuDNN.
+  ctc_loss_desc.set_v9(
+      CUDNN_DATA_FLOAT,
+      CUDNN_LOSS_NORMALIZATION_SOFTMAX,
+      CUDNN_CTC_SKIP_OOB_GRADIENTS,
+      255);
+  TensorDescriptor log_probs_desc{log_probs_t};
+  Tensor grad = at::empty_like(log_probs_t, LEGACY_CONTIGUOUS_MEMORY_FORMAT);
+  TensorDescriptor grad_desc{grad};
+
+  size_t workspace_size;
+  AT_CUDNN_CHECK(cudnnGetCTCLossWorkspaceSize_v8(
+      handle,
+      algo,
+      ctc_loss_desc.desc(),
+      log_probs_desc.desc(),
+      grad_desc.desc(),
+      &workspace_size));
+  Tensor workspace =
+      at::empty(workspace_size, log_probs->options().dtype(kByte));
+  Tensor costs = at::empty({log_probs->size(1)}, log_probs->options());
+
+  AT_CUDNN_CHECK(cudnnCTCLoss_v8(
+      handle,
+      algo,
+      ctc_loss_desc.desc(),
+      log_probs_desc.desc(),
+      log_probs_t.data_ptr(),
+      targets_t_.data_ptr<int>(),
+      target_lengths.data_ptr<int>(),
+      input_lengths.data_ptr<int>(),
+      costs.data_ptr(),
+      grad_desc.desc(),
+      grad.data_ptr(),
+      workspace_size,
+      workspace.data_ptr()
+
+          ));
+  return std::make_tuple(costs, grad);
","--- a/torch/csrc/api/src/nn/modules/ctc_loss.cpp
+++ b/torch/csrc/api/src/nn/modules/ctc_loss.cpp
     const Tensor& log_probs_t,
     const Tensor& targets_t,
+  Tensor targets_t_ = targets_t;
+  if (targets_t.device().type() == at::kCPU) {
+    targets_t_ = targets_t.to(Device(at::kCUDA));
+  }
+  const CheckedFrom c = ""cudnn_ctc_loss"";
+  const TensorArg log_probs{log_probs_t, ""log_probs"", 1};
+  const TensorArg targets{targets_t_, ""targets"", 2};
+  checkDim(c, log_probs, 3);
+  checkScalarType(c, log_probs, kFloat);
+  checkDim(c, targets, 1);
+  checkScalarType(c, targets, kInt);
+  checkContiguous(c, targets); // ?
+  checkBackend(c, {*log_probs}, Backend::CUDA);
+  checkBackend(c, {*targets}, Backend::CUDA);
+  const auto batch_size = log_probs->size(1);
+  int64_t input_lengths_size =
+      input_lengths.sizes().size() ? input_lengths.size(0) : 1;
+  int64_t target_lengths_size =
+      target_lengths.sizes().size() ? target_lengths.size(0) : 1;
+  TORCH_CHECK(
+      input_lengths_size == batch_size,
+      ""input_lengths needs to have size to match batch_size"");
+  TORCH_CHECK(
+      target_lengths_size == batch_size,
+      ""target_lengths needs to have size to match batch_size"");
+
+  TORCH_CHECK(BLANK == 0, ""blank must be label 0 for cudnn_ctc_loss"");
+  // checked in dispatch:
+  // assert other conditions for cudnnCTCLoss: all label lengths <= 256
+  // all input lengths = logprob.size(0)
+
+  const auto handle = getCudnnHandle();
+
+  const cudnnCTCLossAlgo_t algo =
+      (deterministic ? CUDNN_CTC_LOSS_ALGO_DETERMINISTIC
+                     : CUDNN_CTC_LOSS_ALGO_NON_DETERMINISTIC);
+
+  CTCLossDescriptor ctc_loss_desc;
+
+  // so the CuDNN gradient semantics have changed between 7.1 and 7.6,
+  // this is CuDNN 7.6 only, see PyTorch 1.2 for older CuDNN.
+  ctc_loss_desc.set_v9(
+      CUDNN_DATA_FLOAT,
+      CUDNN_LOSS_NORMALIZATION_SOFTMAX,
+      CUDNN_CTC_SKIP_OOB_GRADIENTS,
+      255);
+  TensorDescriptor log_probs_desc{log_probs_t};
+  Tensor grad = at::empty_like(log_probs_t, LEGACY_CONTIGUOUS_MEMORY_FORMAT);
+  TensorDescriptor grad_desc{grad};
+
+  size_t workspace_size;
+  AT_CUDNN_CHECK(cudnnGetCTCLossWorkspaceSize_v8(
+      handle,
+      algo,
+      ctc_loss_desc.desc(),
+      log_probs_desc.desc(),
+      grad_desc.desc(),
+      &workspace_size));
+  Tensor workspace =
+      at::empty(workspace_size, log_probs->options().dtype(kByte));
+  Tensor costs = at::empty({log_probs->size(1)}, log_probs->options());
+
+  AT_CUDNN_CHECK(cudnnCTCLoss_v8(
+      handle,
+      algo,
+      ctc_loss_desc.desc(),
+      log_probs_desc.desc(),
+      log_probs_t.data_ptr(),
+      targets_t_.data_ptr<int>(),
+      target_lengths.data_ptr<int>(),
+      input_lengths.data_ptr<int>(),
+      costs.data_ptr(),
+      grad_desc.desc(),
+      grad.data_ptr(),
+      workspace_size,
+      workspace.data_ptr()
+
+          ));
+  return std::make_tuple(costs, grad);"
0,https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba,YES,torch/_dynamo/config.py,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba,YES,torch/_dynamo/repro/after_aot.py,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba,YES,torch/_dynamo/repro/after_aot.py,patch_1,"-        if not same_two_models(mod, compiled, args, only_fwd=True):
","+        if not same_two_models(
+            mod,
+            compiled,
+            args,
+            only_fwd=True,
+            ignore_non_fp=config.repro_ignore_non_fp,
+        ):
","-        if not same_two_models(
+            mod,
+            compiled,
+            args,
+            only_fwd=True,
+            ignore_non_fp=config.repro_ignore_non_fp,
        ):"
0,https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba,YES,torch/_dynamo/repro/after_dynamo.py,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba,YES,torch/_dynamo/repro/after_dynamo.py,patch_1,"-                if backend_accuracy_fails(gm, example_inputs, compiler_fn):
","+                if _accuracy_fails(gm, example_inputs, compiler_fn):
","@@ -2524,6 +2524,22 @@ def module_error_inputs_torch_nn_LSTMCell(module_info, device, dtype, requires_gn
     return samples
 
 
+def module_error_inputs_torch_nn_RNN_GRU(module_info, device, dtype, requires_grad, training, **kwargs):
+    samples = [
+        ErrorModuleInput(
+            ModuleInput(constructor_input=FunctionInput(10, 0, 1)),
+            error_on=ModuleErrorEnum.CONSTRUCTION_ERROR,
+            error_type=ValueError,
+            error_regex=""hidden_size must be greater than zero""
+        ),
+        ErrorModuleInput(
+            ModuleInput(constructor_input=FunctionInput(10, 10, 0)),
+            error_on=ModuleErrorEnum.CONSTRUCTION_ERROR,
+            error_type=ValueError,
+            error_regex=""num_layers must be greater than zero""
+        ),
+    ]
+    return samples
 
 def module_error_inputs_torch_nn_Pad1d(module_info, device, dtype, requires_grad, training, **kwargs):
     make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)"
0,https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba,YES,torch/_dynamo/repro/after_dynamo.py,patch_2,"-    if backend_accuracy_fails(
-        gm, example_inputs, compiler_fn, only_fwd=config.repro_forward_only
-    ):
-            backend_accuracy_fails,
-            only_fwd=config.repro_forward_only,
","+    if _accuracy_fails(gm, example_inputs, compiler_fn):
+            _accuracy_fails,
","@@ -532,12 +531,12 @@ def fx_codegen_and_compile(
     # on node.meta[""val""]. if in the future we rely on these being
     # correct we will need to fix.
 
-    with V.set_fake_mode(fake_mode):
+    with V.set_fake_mode(fake_mode):  # type: ignore[call-arg]
         # has some issues with memory in training
         post_grad_passes(gm, is_inference=is_inference)
         V.debug.fx_graph_transformed(gm, example_inputs)
 
-    with V.set_fake_mode(fake_mode):
+    with V.set_fake_mode(fake_mode):  # type: ignore[call-arg]
         graph = GraphLowering(
             gm,
             shape_env=shape_env,
@@ -544,5 +543,5 @@ def fx_codegen_and_compile(
         )
 
     if _accuracy_fails(gm, example_inputs, compiler_fn):
-            _accuracy_fails,
+            _accuracy_fails,"
0,https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba,YES,torch/_dynamo/repro/after_dynamo.py,patch_3,"-            if not same_two_models(mod, opt_mod, args):
",No
0,https://github.com/PyTorch/PyTorch/commit/f8d60e0e0a4420def0cf6b3bc0a0c0d46c93de1c,YES,torch/_inductor/codegen/cpp.py,patch_0,"-def get_lowp_to_fp32_expr(lowp_var, kernel):
-        return f""at::vec::convert<float>({lowp_var})""
-        return f""c10::convert<float>({lowp_var})""
","+def get_lowp_to_high_prec_expr(lowp_var, dtype, kernel):
+        return f""at::vec::convert<{DTYPE_TO_CPP[dtype]}>({lowp_var})""
+        return f""c10::convert<{DTYPE_TO_CPP[dtype]}>({lowp_var})""
","@@ -510,4 +470,6 @@ Tensor& nan_to_num_out_mps(const Tensor& self,
 }
 
+REGISTER_DISPATCH(where_kernel, &where_kernel_mps);
+
 } // namespace at::native"
0,https://github.com/PyTorch/PyTorch/commit/f8d60e0e0a4420def0cf6b3bc0a0c0d46c93de1c,YES,torch/_inductor/codegen/cpp.py,patch_1,"-    def cache_fp32_cse_var_before_lowp_store(self, var_to_store):
","+    def cache_high_prec_cse_var_before_lowp_store(self, var_to_store):
","@@ -565,10 +564,13 @@ class TritonBenchmarkRequest(BenchmarkRequest):
                 *self.extra_args,
                 grid=self.grid,
                 **warmup_arg,
-                num_stages=self.num_stages,
-                num_warps=self.num_warps,
+                stream=get_raw_stream(self.output_tensor_meta.device.index),
             )
 
+    def precompile(self):
+        mod = PyCodeCache.load_by_key_path(self.module_cache_key, self.module_path)
+        getattr(mod, self.kernel_name).precompile()
+
     def __str__(self) -> str:
         return f""{self.kernel_name=}, {self.module_path=}, {self.module_cache_key=}""
 "
0,https://github.com/PyTorch/PyTorch/commit/f8d60e0e0a4420def0cf6b3bc0a0c0d46c93de1c,YES,torch/_inductor/codegen/cpp.py,patch_2,"-        def find_fp32_var(var, cache):
-            fp32_cse_var = None
-            fp32_cse_var_name = None
-                            fp32_cse_var_name = m.group()
-            if fp32_cse_var_name:
-                    if cse_var.name == fp32_cse_var_name:
-                        fp32_cse_var = cse_var
-                assert fp32_cse_var is not None
-            return fp32_cse_var
-        fp32_var = find_fp32_var(var_to_store, self.cse.cache)
-        if fp32_var:
-            self.cse.cache[get_lowp_to_fp32_expr(var_to_store, self)] = fp32_var
","+        def find_high_prec_var(var, cache):
+            high_prec_cse_var = None
+            high_prec_cse_var_name = None
+                            high_prec_cse_var_name = m.group()
+            if high_prec_cse_var_name:
+                    if cse_var.name == high_prec_cse_var_name:
+                        high_prec_cse_var = cse_var
+                assert high_prec_cse_var is not None
+            return high_prec_cse_var
+        high_prec_var = find_high_prec_var(var_to_store, self.cse.cache)
+        if high_prec_var and high_prec_var.dtype in DTYPE_TO_CPP:
+            cache_key = get_lowp_to_high_prec_expr(
+                var_to_store, high_prec_var.dtype, self
+            )
+            self.cse.cache[cache_key] = high_prec_var
","@@ -565,10 +564,13 @@ class TritonBenchmarkRequest(BenchmarkRequest):
                 *self.extra_args,
                 grid=self.grid,
                 **warmup_arg,
-                num_stages=self.num_stages,
-                num_warps=self.num_warps,
+                stream=get_raw_stream(self.output_tensor_meta.device.index),
             )

+    def precompile(self):
+        mod = PyCodeCache.load_by_key_path(self.module_cache_key, self.module_path)
+        getattr(mod, self.kernel_name).precompile()
+
     def __str__(self) -> str:
         return f""{self.kernel_name=}, {self.module_path=}, {self.module_cache_key=}""
 
-        def find_fp32_var(var, cache):
-            fp32_cse_var = None
-            fp32_cse_var_name = None
-                            fp32_cse_var_name = m.group()
-            if fp32_cse_var_name:
-                    if cse_var.name == fp32_cse_var_name:
-                        fp32_cse_var = cse_var
-                assert fp32_cse_var is not None
-            return fp32_cse_var
-        fp32_var = find_fp32_var(var_to_store, self.cse.cache)
-        if fp32_var:
-            self.cse.cache[get_lowp_to_fp32_expr(var_to_store, self)] = fp32_var
+        def find_high_prec_var(var, cache):
+            high_prec_cse_var = None
+            high_prec_cse_var_name = None
+                            high_prec_cse_var_name = m.group()
+            if high_prec_cse_var_name:
+                    if cse_var.name == high_prec_cse_var_name:
+                        high_prec_cse_var = cse_var
+                assert high_prec_cse_var is not None
+            return high_prec_cse_var
+        high_prec_var = find_high_prec_var(var_to_store, self.cse.cache)
+        if high_prec_var and high_prec_var.dtype in DTYPE_TO_CPP:
+            cache_key = get_lowp_to_high_prec_expr(
+                var_to_store, high_prec_var.dtype, self
+            )
+            self.cse.cache[cache_key] = high_prec_var"
0,https://github.com/PyTorch/PyTorch/commit/f8d60e0e0a4420def0cf6b3bc0a0c0d46c93de1c,YES,torch/_inductor/codegen/cpp.py,patch_3,"-        self.cache_fp32_cse_var_before_lowp_store(value)
","+        self.cache_high_prec_cse_var_before_lowp_store(value)
","@@ -270,7 +270,7 @@ class PersistentCache(CacheBase):

             1. Check global_cache[op][inputs][choice][precision], return benchmark if cached.
             2. Check local_cache[op][inputs][choice][precision], return benchmark if cached.
-            3.
+            3. If benchmark is not None:
                 a. `max_autotune_gemm=True`: benchmark the choice, update
                     local_cache[op][inputs][choice], and return the benchmark.
                 b. `max_autotune_gemm=False`: don't benchmark the choice, return nothing.
-        self.cache_fp32_cse_var_before_lowp_store(value)
+        self.cache_high_prec_cse_var_before_lowp_store(value)"
0,https://github.com/PyTorch/PyTorch/commit/f8d60e0e0a4420def0cf6b3bc0a0c0d46c93de1c,YES,torch/_inductor/codegen/cpp.py,patch_4,"-        self.cache_fp32_cse_var_before_lowp_store(value)
","+        self.cache_high_prec_cse_var_before_lowp_store(value)
","@@ -270,7 +270,7 @@ class PersistentCache(CacheBase):

             1. Check global_cache[op][inputs][choice][precision], return benchmark if cached.
             2. Check local_cache[op][inputs][choice][precision], return benchmark if cached.
-            3.
+            3. If benchmark is not None:
                 a. `max_autotune_gemm=True`: benchmark the choice, update
                     local_cache[op][inputs][choice], and return the benchmark.
                 b. `max_autotune_gemm=False`: don't benchmark the choice, return nothing."
0,https://github.com/PyTorch/PyTorch/commit/7553c495147f3e21a1e27d392d277906a47768e7,YES,torch/csrc/distributed/c10d/ProcessGroupWrapper.cpp,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/7553c495147f3e21a1e27d392d277906a47768e7,YES,torch/csrc/distributed/c10d/ProcessGroupWrapper.cpp,patch_1,"-  runCollectiveChecks(OpType::ALLGATHER, inputTensors);
","+  if (check_same_size(outputTensors.back())) {
+    runCollectiveChecks(OpType::ALLGATHER, inputTensors);
+  } else {
+    runCollectiveChecks(OpType::ALLGATHER, {});
+  }
","@@ -539,7 +543,7 @@ inline std::vector<c10::SymInt> PythonArgs::symintlist(int i) {
     const auto size1 = signature.params[i].size;
     if (size1 > 0 && THPUtils_checkLong(args[i])) {
       return std::vector<c10::SymInt>(
-          size1, c10::SymInt(THPUtils_unpackIndex(args[i])));
+          size1, c10::SymInt(THPUtils_unpackLong(args[i])));
     }
 
     if (size1 > 0 && torch::is_symint(py::handle(args[i]))) {
-
-  runCollectiveChecks(OpType::ALLGATHER, inputTensors);
+  if (check_same_size(outputTensors.back())) {
+    runCollectiveChecks(OpType::ALLGATHER, inputTensors);
+  } else {
+    runCollectiveChecks(OpType::ALLGATHER, {});
+  }"
0,https://github.com/PyTorch/PyTorch/commit/7553c495147f3e21a1e27d392d277906a47768e7,YES,torch/csrc/distributed/c10d/ProcessGroupWrapper.cpp,patch_2,"-  runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);
","+  if (check_same_size(inputTensors.back())) {
+    runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);
+  } else {
+    runCollectiveChecks(OpType::REDUCE_SCATTER, {});
+  }
","@@ -207,6 +209,18 @@ class TestFullyShardCollectiveOps(FSDPTestMultiThread):
                 reduce_scatter_dtype=torch.float32,
             )
 
+    @unittest.skipIf(not TEST_CUDA, ""no cuda"")
+    def test_reduce_scatter_fp16(self):
+        param_sizes = self._get_param_sizes()
+        default_stream = torch.cuda.current_stream()
+        stream = torch.cuda.Stream()
+        for reduce_scatter_stream in (default_stream, stream):
+            if (check_same_size(param_sizes[-1])):
+                self._test_reduce_scatter(
+                    param_sizes,
+                    reduce_scatter_stream=reduce_scatter_stream,
+                    reduce_scatter_dtype=torch.float16,
+                )
+            else:
+                self._test_reduce_scatter(
+                    param_sizes,
+                    reduce_scatter_stream=reduce_scatter_stream,
+                    reduce_scatter_dtype=torch.float16,
+                )
 
     def _test_reduce_scatter(
         self,
         param_sizes: List[torch.Size],
"
0,https://github.com/PyTorch/PyTorch/commit/e14d1d10ef9d24bf43366ac1f05a5aa8b732707b,YES,torch/_inductor/codegen/simd.py,patch_0,"-from torch.utils._sympy.functions import FloorDiv, ModularIndexing
","+from torch.utils._sympy.functions import FloorDiv, Identity, ModularIndexing
","@@ -16,7 +16,7 @@
 import sympy
 import torch
 import torch.fx
 from torch._inductor import dependencies
-from torch._prims_common import is_float_dtype
+from torch._prims_common import is_float_dtype, is_integer_dtype
 from torch.utils import _pytree as pytree
 from torch.utils._sympy.functions import CeilDiv, FloorDiv, ModularIndexing, Identity
 from torch.utils._sympy.symbol import free_symbol_is_type, symbol_is_type, SymT"
0,https://github.com/PyTorch/PyTorch/commit/e14d1d10ef9d24bf43366ac1f05a5aa8b732707b,YES,torch/_inductor/codegen/simd.py,patch_1,"-        return self.codegen_indexing(self.simplify_indexing(index))
","+        simp_index = self.simplify_indexing(index)
+
+        # Now that we are done simplifying we can unwrap Identity so that downstream handling
+        # for its contained expression will work. previously, tl.full wrapping of sympy.Integer
+        # would not occur
+        simp_index = (
+            simp_index if not isinstance(simp_index, Identity) else simp_index.args[0]
+        )
+
+        return self.codegen_indexing(simp_index)
","@@ -1222,9 +1221,12 @@ class CppKernel(Kernel):
         new_index = sympy_subs(index, replacement)
         return new_index
 
-    @staticmethod
-    def indirect_indexing(index_var, size, check=True):
-        return sympy_symbol(str(index_var))
+    def index_to_str(self, index: sympy.Expr) -> str:
+        """"""
+        Convert an index expr to a string that can be used in cpp code.
+        e.g. a sympy expression ""s2"" may actually appear as ""ks1"" in the cpp kernel.
+        """"""
+        return cexpr(self.rename_indexing(index))
 
     def load(self, name: str, index: sympy.Expr):
         var = self.args.input(name)"
0,https://github.com/PyTorch/PyTorch/commit/15745a52b0d7dc5c3d0ecc061b1604288064f641,YES,torch/_inductor/ir.py,patch_0,"-                    x, freeze=True, want_contiguous=False, stride_order=order
","+                # If the the FlexibleLayout already has the size and stride in the required order,
+                # freeze it to a FixedLayout by using its current size and stride.
+                # The behavior of using its current size and stride or the given order can be different
+                # if the size and stride has ambiguilty, for example for a 4D input where the iC = 1:
+                # size=[s0, 1, 28, 28], stride=[784, 784, 28, 1]. If the required order is [3, 0, 2, 1] (channels last),
+                # the current size and stride already satisfies this order.
+                # However by freezing it to the required order, the layout will be changed to:
+                # size=[s0, 1, 28, 28], stride=[784, 1, 28, 1]), which is not actually necessary.
+
+                    x,
+                    freeze=True,
+                    want_contiguous=False,
+                    stride_order=get_stride_order(
+                        V.graph.sizevars.size_hints(x.get_layout().stride)
+                    )
+                    if is_stride_order_storage_and_layout(x, order)
+                    else order,
","@@ -2029,8 +2072,8 @@ def forward(self, x_1):
     eq = sym_size_int == 4;  sym_size_int = None
     true_graph_0 = self.true_graph_0
     false_graph_0 = self.false_graph_0
-    conditional = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None
-    getitem = conditional[0];  conditional = None
+    cond = torch.ops.higher_order.cond(eq, true_graph_0, false_graph_0, [x_1]);  eq = true_graph_0 = false_graph_0 = x_1 = None
+    getitem = cond[0];  cond = None
     return getitem"""""",  # noqa: B950
         )
 
-                    x, freeze=True, want_contiguous=False, stride_order=order
+                x,
+                freeze=True,
+                want_contiguous=False,
+                stride_order=get_stride_order(
+                    V.graph.sizevars.size_hints(x.get_layout().stride)
+                )
+                if is_stride_order_storage_and_layout(x, order)
+                else order,"
0,https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244,YES,CMakeLists.txt,patch_0,"-# --- [ Check that minimal gcc version is 9.4+
-if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.4)
-  message(FATAL ""GCC-9.4 or newer is required to compile PyTorch, but found ${CMAKE_CXX_COMPILER_VERSION}"")
-endif()
-
-if(LINUX)
","+if(CMAKE_SYSTEM_NAME STREQUAL ""Linux"")
","--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -1,7 +1,13 @@
 # --- [ Check that minimal gcc version is 9.4+
-if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.4)
-  message(FATAL ""GCC-9.4 or newer is required to compile PyTorch, but found ${CMAKE_CXX_COMPILER_VERSION}"")
-endif()
-
-if(LINUX)
+if(CMAKE_SYSTEM_NAME STREQUAL ""Linux"")
     # Add Linux-specific code here
 endif()
+@@ -48,6 +50,11 @@ _DNNL_RUNTIME_OMP = {
+     ""#cmakedefine01 BUILD_XEHPG"": ""#define BUILD_XEHPG 0"",
+     ""#cmakedefine01 BUILD_XEHPC"": ""#define BUILD_XEHPC 0"",
+     ""#cmakedefine01 BUILD_XEHP"": ""#define BUILD_XEHP 0"",
++    ""#cmakedefine01 BUILD_GEMM_KERNELS_ALL"": ""#define BUILD_GEMM_KERNELS_ALL 0"",
++    ""#cmakedefine01 BUILD_GEMM_KERNELS_NONE"": ""#define BUILD_GEMM_KERNELS_NONE 0"",
++    ""#cmakedefine01 BUILD_GEMM_SSE41"": ""#define BUILD_GEMM_SSE41 0"",
++    ""#cmakedefine01 BUILD_GEMM_AVX2"": ""#define BUILD_GEMM_AVX2 0"",
++    ""#cmakedefine01 BUILD_GEMM_AVX512"": ""#define BUILD_GEMM_AVX512 0"",
+ }
+ 
+ template_rule("
0,https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244,YES,CMakeLists.txt,patch_1,"-if(CMAKE_COMPILER_IS_GNUCXX AND NOT ANDROID)
","+if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 4.8.0 AND NOT ANDROID)
","--- a/aten/src/ATen/CMakeLists.txt
+++ b/aten/src/ATen/CMakeLists.txt
@@ -450,19 +450,7 @@ if(NOT MSVC AND NOT EMSCRIPTEN AND NOT INTERN_BUILD_MOBILE)
         set(DISABLE_SVE ON CACHE BOOL ""Xcode\'s clang-12.5 crashes while trying to compile SVE code"" FORCE)
       endif()
     endif()
-    if(""${CMAKE_C_COMPILER_ID}"" STREQUAL ""GNU"" AND
-        CMAKE_C_COMPILER_VERSION VERSION_GREATER 6.9 AND CMAKE_C_COMPILER_VERSION VERSION_LESS 8)
-      set(GCC_7 True)
-    else()
-      set(GCC_7 False)
-    endif()
-    if(GCC_7)
-      set(CMAKE_BUILD_TYPE Release)  # Always build Sleef as a Release build to work around a gcc-7 bug
-    endif()
     add_subdirectory(""${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef"" ${CMAKE_BINARY_DIR}/sleef)
-    if(GCC_7)
-      set(CMAKE_BUILD_TYPE ${OLD_CMAKE_BUILD_TYPE})
-    endif()
     set_property(TARGET sleef PROPERTY FOLDER ""dependencies"")
     list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)
     link_directories(${CMAKE_BINARY_DIR}/sleef/lib)"
0,https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244,YES,CMakeLists.txt,patch_2,"-  append_cxx_flag_if_supported(""-Wno-stringop-overflow"" CMAKE_CXX_FLAGS)
-  append_cxx_flag_if_supported(""-Wsuggest-override"" CMAKE_CXX_FLAGS)
","+  if(CMAKE_COMPILER_IS_GNUCXX AND NOT (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.0.0))
+    string(APPEND CMAKE_CXX_FLAGS "" -Wno-stringop-overflow"")
+  endif()
+  if(NOT CMAKE_COMPILER_IS_GNUCXX OR GCC_VERSION VERSION_GREATER_EQUAL 9.2)
+    # Prior to GCC 9.2, this warning misfires when a method is
+    # labeled ""final"".
+    # https://gcc.gnu.org/bugzilla/show_bug.cgi?id=78010
+    append_cxx_flag_if_supported(""-Wsuggest-override"" CMAKE_CXX_FLAGS)
+  endif()
","--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -450,19 +450,7 @@ if(NOT MSVC AND NOT EMSCRIPTEN AND NOT INTERN_BUILD_MOBILE)
         set(DISABLE_SVE ON CACHE BOOL ""Xcode\'s clang-12.5 crashes while trying to compile SVE code"" FORCE)
       endif()
     endif()
-    if(""${CMAKE_C_COMPILER_ID}"" STREQUAL ""GNU"" AND
-        CMAKE_C_COMPILER_VERSION VERSION_GREATER 6.9 AND CMAKE_C_COMPILER_VERSION VERSION_LESS 8)
-      set(GCC_7 True)
-    else()
-      set(GCC_7 False)
-    endif()
-    if(GCC_7)
-      set(CMAKE_BUILD_TYPE Release)  # Always build Sleef as a Release build to work around a gcc-7 bug
-    endif()
     add_subdirectory(""${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef"" ${CMAKE_BINARY_DIR}/sleef)
-    if(GCC_7)
-      set(CMAKE_BUILD_TYPE ${OLD_CMAKE_BUILD_TYPE})
-    endif()
     set_property(TARGET sleef PROPERTY FOLDER ""dependencies"")
     list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)
     link_directories(${CMAKE_BINARY_DIR}/sleef/lib)"
0,https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244,YES,caffe2/CMakeLists.txt,patch_0,"-if(CMAKE_COMPILER_IS_GNUCXX)
","+if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))
","@@ -450,19 +450,7 @@ if(NOT MSVC AND NOT EMSCRIPTEN AND NOT INTERN_BUILD_MOBILE)
         set(DISABLE_SVE ON CACHE BOOL ""Xcode's clang-12.5 crashes while trying to compile SVE code"" FORCE)
       endif()
     endif()
-    if(""${CMAKE_C_COMPILER_ID}"" STREQUAL ""GNU"" AND
-        CMAKE_C_COMPILER_VERSION VERSION_GREATER 6.9 AND CMAKE_C_COMPILER_VERSION VERSION_LESS 8)
-      set(GCC_7 True)
-    else()
-      set(GCC_7 False)
-    endif()
-    if(GCC_7)
-      set(CMAKE_BUILD_TYPE Release)  # Always build Sleef as a Release build to work around a gcc-7 bug
-    endif()
+    if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))
     add_subdirectory(""${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef"" ${CMAKE_BINARY_DIR}/sleef)
-    if(GCC_7)
-      set(CMAKE_BUILD_TYPE ${OLD_CMAKE_BUILD_TYPE})
-    endif()
     set_property(TARGET sleef PROPERTY FOLDER ""dependencies"")
     list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)
     link_directories(${CMAKE_BINARY_DIR}/sleef/lib)"
0,https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244,YES,caffe2/python/pybind_state.h,patch_0,"-      bool in_place) override {
","+      bool in_place) {
","--- a/pybind11/tests/test_operators.cpp
+++ b/pybind11/tests/test_operators.cpp
@@ -251,7 +251,6 @@ def boolean_ops():
     return (
         ""is_inf"",
         ""is_nan"",
-        ""bitwise_xor"",
         ""logical_not"",
         ""signbit"",
         ""le"",
         ""lt"",
         ""eq"",
         ""ne"",
         ""gt"",
         ""ge"",
     )"
0,https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244,YES,cmake/Codegen.cmake,patch_0,"-      if(CMAKE_COMPILER_IS_GNUCXX)
","+      if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))
","--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -450,19 +450,7 @@ if(NOT MSVC AND NOT EMSCRIPTEN AND NOT INTERN_BUILD_MOBILE)
         set(DISABLE_SVE ON CACHE BOOL ""Xcode\'s clang-12.5 crashes while trying to compile SVE code"" FORCE)
       endif()
     endif()
-    if(""${CMAKE_C_COMPILER_ID}"" STREQUAL ""GNU"" AND
-        CMAKE_C_COMPILER_VERSION VERSION_GREATER 6.9 AND CMAKE_C_COMPILER_VERSION VERSION_LESS 8)
-      set(GCC_7 True)
-    else()
-      set(GCC_7 False)
-    endif()
-    if(GCC_7)
-      set(CMAKE_BUILD_TYPE Release)  # Always build Sleef as a Release build to work around a gcc-7 bug
-    endif()
     add_subdirectory(""${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef"" ${CMAKE_BINARY_DIR}/sleef)
-    if(GCC_7)
-      set(CMAKE_BUILD_TYPE ${OLD_CMAKE_BUILD_TYPE})
-    endif()
     set_property(TARGET sleef PROPERTY FOLDER ""dependencies"")
     list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)
     link_directories(${CMAKE_BINARY_DIR}/sleef/lib)
-      if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))"
0,https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244,YES,cmake/Dependencies.cmake,patch_0,,"+
+    # Workaround for https://github.com/pytorch/pytorch/issues/47292
+    if(CMAKE_BUILD_TYPE STREQUAL ""Debug"" AND CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.5.0))
+      # Compiling qu8-requantization/precise-psimd.c without any optimization flags on gcc-7.4 or older i
+      # Fails with internal compiler error
+      # Workaround by forcing -O1 for XNNPACK (i.e. build it with RelWithDebInfo)
+      set_property(TARGET XNNPACK APPEND_STRING PROPERTY COMPILE_FLAGS ""-O1"")
+    endif()
","@@ -8,6 +8,17 @@
 #include <type_traits>
 #include <utility>
 
+#if !defined(__clang__) && !defined(_MSC_VER) && defined(__GNUC__) && \
+    __GNUC__ < 9
+#error \
+    ""You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later.""
+#endif
+
+#if defined(__clang__) && __clang_major__ < 9
+#error \
+    ""You're trying to build PyTorch with a too old version of Clang. We need Clang 9 or later.""
+#endif
+
 #if (defined(_MSC_VER) && (!defined(_MSVC_LANG) || _MSVC_LANG < 201703L)) || \
     (!defined(_MSC_VER) && __cplusplus < 201703L)
 #error You need C++17 to compile PyTorch"
0,https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244,YES,modules/observers/net_observer_reporter_print.h,patch_0,"-  void report(NetBase* net, std::map<std::string, PerformanceInformation>&) override;
","+  void report(NetBase* net, std::map<std::string, PerformanceInformation>&);
","--- a/torch/lib/c10d/ProcessGroupNCCL.cpp
+++ b/torch/lib/c10d/ProcessGroupNCCL.cpp
@@ -1504,9 +1519,8 @@ std::string ProcessGroupNCCL::getNCCLWatchdogDebugInfo() {
 std::string ProcessGroupNCCL::createLogPrefix() const {
   if (!pg_desc_.empty() && pg_desc_ != ""undefined"") {
     return c10::str(""[PG "", pg_name_, "" ("", pg_desc_, "") Rank "", rank_, ""] "");
-  } else {
-    return c10::str(""[PG "", pg_name_, "" Rank "", rank_, ""] "");
   }
+  return c10::str(""[PG "", pg_name_, "" Rank "", rank_, ""] "");
 }
 
 const std::string& ProcessGroupNCCL::logPrefix() const {"
0,https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244,YES,torch/CMakeLists.txt,patch_0,"-    if(CMAKE_COMPILER_IS_GNUCXX)
","+    if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))
","--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -450,19 +450,7 @@ if(NOT MSVC AND NOT EMSCRIPTEN AND NOT INTERN_BUILD_MOBILE)
         set(DISABLE_SVE ON CACHE BOOL ""Xcode's clang-12.5 crashes while trying to compile SVE code"" FORCE)
       endif()
     endif()
-    if(""${CMAKE_C_COMPILER_ID}"" STREQUAL ""GNU"" AND
-        CMAKE_C_COMPILER_VERSION VERSION_GREATER 6.9 AND CMAKE_C_COMPILER_VERSION VERSION_LESS 8)
-      set(GCC_7 True)
-    else()
-      set(GCC_7 False)
-    endif()
-    if(GCC_7)
-      set(CMAKE_BUILD_TYPE Release)  # Always build Sleef as a Release build to work around a gcc-7 bug
-    endif()
+    if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))
     add_subdirectory(""${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef"" ${CMAKE_BINARY_DIR}/sleef)
-    if(GCC_7)
-      set(CMAKE_BUILD_TYPE ${OLD_CMAKE_BUILD_TYPE})
-    endif()
     set_property(TARGET sleef PROPERTY FOLDER ""dependencies"")
     list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)
     link_directories(${CMAKE_BINARY_DIR}/sleef/lib)"
0,https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244,YES,torch/CMakeLists.txt,patch_1,"-if(CMAKE_COMPILER_IS_GNUCXX)
","+if(CMAKE_COMPILER_IS_GNUCXX AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 9.0.0))
","--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -450,19 +450,7 @@ if(NOT MSVC AND NOT EMSCRIPTEN AND NOT INTERN_BUILD_MOBILE)
         set(DISABLE_SVE ON CACHE BOOL ""Xcode\'s clang-12.5 crashes while trying to compile SVE code"" FORCE)
       endif()
     endif()
-    if(""${CMAKE_C_COMPILER_ID}"" STREQUAL ""GNU"" AND
-        CMAKE_C_COMPILER_VERSION VERSION_GREATER 6.9 AND CMAKE_C_COMPILER_VERSION VERSION_LESS 8)
-      set(GCC_7 True)
-    else()
-      set(GCC_7 False)
-    endif()
-    if(GCC_7)
-      set(CMAKE_BUILD_TYPE Release)  # Always build Sleef as a Release build to work around a gcc-7 bug
-    endif()
+    add_subdirectory(""${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef"" ${CMAKE_BINARY_DIR}/sleef)
-    if(GCC_7)
-      set(CMAKE_BUILD_TYPE ${OLD_CMAKE_BUILD_TYPE})
-    endif()
     set_property(TARGET sleef PROPERTY FOLDER ""dependencies"")
     list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)
     link_directories(${CMAKE_BINARY_DIR}/sleef/lib)"
0,https://github.com/PyTorch/PyTorch/commit/679ca510b0fc5d027c69f2946db8785c16cbb244,YES,torch/csrc/autograd/python_hook.h,patch_0,"-      torch::dynamo::autograd::SwapSavedVariables& saved) override;
","+      torch::dynamo::autograd::SwapSavedVariables& saved);
","@@ -1 +1 @@
-      torch::dynamo::autograd::SwapSavedVariables& saved) override;
+      torch::dynamo::autograd::SwapSavedVariables& saved;"
0,https://github.com/PyTorch/PyTorch/commit/0fef82b3dfb4a5da1555a3c046950594b83e2898,YES,torch/distributed/fsdp/_shard_utils.py,patch_0,"-    return DTensor.from_local(tensor, device_mesh, replicate_placements).redistribute(
-        device_mesh=device_mesh,
","+    return DTensor.from_local(
+        tensor, device_mesh, replicate_placements, run_check=False
+    ).redistribute(
","--- a/deeptensor/redistribute.py
        +++ b/deeptensor/redistribute.py
        @@ -278,19 +278,24 @@ class Redistribute(torch.autograd.Function):
             current_spec = input._spec
             ctx.current_spec = current_spec
             ctx.async_op = async_op
        -        target_spec = DTensorSpec(
        -            device_mesh, placements, tensor_meta=input._spec.tensor_meta
        -        )
         
        -        local_tensor = input._local_tensor
        -        output = redistribute_local_tensor(
        -            local_tensor, current_spec, target_spec, async_op=async_op
        -        )
        +        if current_spec.placements != placements:
        +            target_spec = DTensorSpec(
        +                device_mesh, placements, tensor_meta=input._spec.tensor_meta
        +            )
        +
        +            local_tensor = input._local_tensor
        +            output = redistribute_local_tensor(
        +                local_tensor, current_spec, target_spec, async_op=async_op
        +            )
        +        else:
        +            # use the same local tensor if placements are the same.
        +            output = input._local_tensor
         
             return dtensor.DTensor(
                 output,
                 device_mesh,
        -            target_spec.placements,
        +            placements,
                 shape=input.shape,
                 dtype=input.dtype,
                 requires_grad=input.requires_grad,"
0,https://github.com/PyTorch/PyTorch/commit/9782439277fb545ec4e14340ecf75df86ddc4f0f,YES,torch/autograd/profiler.py,patch_0,"-        VALID_DEVICE_OPTIONS = [""cuda"", ""xpu""]
-        if _get_privateuse1_backend_name() != ""privateuseone"":
-            VALID_DEVICE_OPTIONS.append(_get_privateuse1_backend_name())
-        if self.use_device not in VALID_DEVICE_OPTIONS:
-            warn(f""The {self.use_device} is not a valid device option."")
-            self.use_device = None
-
-        if self.use_device == ""cuda"" and not torch.cuda.is_available():
-            warn(""CUDA is not available, disabling CUDA profiling"")
-            self.use_cuda = False
-            self.use_device = None
-
-        if self.use_device == ""xpu"" and not torch.xpu.is_available():
-            warn(""XPU is not available, disabling XPU profiling"")
-            self.use_device = None
","+        if self.use_device is not None:
+            VALID_DEVICE_OPTIONS = [""cuda"", ""xpu""]
+            if _get_privateuse1_backend_name() != ""privateuseone"":
+                VALID_DEVICE_OPTIONS.append(_get_privateuse1_backend_name())
+            if self.use_device not in VALID_DEVICE_OPTIONS:
+                warn(f""The {self.use_device} is not a valid device option."")
+                self.use_device = None
+
+            if self.use_device == ""cuda"" and not torch.cuda.is_available():
+                warn(""CUDA is not available, disabling CUDA profiling"")
+                self.use_cuda = False
+                self.use_device = None
+
+            if self.use_device == ""xpu"" and not torch.xpu.is_available():
+                warn(""XPU is not available, disabling XPU profiling"")
+                self.use_device = None
","@@ -1236,6 +1236,7 @@ TEST_SCIPY = _check_module_exists('scipy')
 TEST_MKL = torch.backends.mkl.is_available()
 TEST_MPS = torch.backends.mps.is_available()
 TEST_XPU = torch.xpu.is_available()
+TEST_HPU = True if (hasattr(torch, ""hpu"") and torch.hpu.is_available()) else False
 TEST_CUDA = torch.cuda.is_available()
 custom_device_mod = getattr(torch, torch._C._get_privateuse1_backend_name(), None)
 custom_device_is_available = hasattr(custom_device_mod, ""is_available"") and custom_device_mod.is_available()
+
+        if self.use_device is not None:
+            VALID_DEVICE_OPTIONS = [""cuda"", ""xpu""]
+            if _get_privateuse1_backend_name() != ""privateuseone"":
+                VALID_DEVICE_OPTIONS.append(_get_privateuse1_backend_name())
+            if self.use_device not in VALID_DEVICE_OPTIONS:
+                warn(f""The {self.use_device} is not a valid device option."")
+                self.use_device = None
+
+            if self.use_device == ""cuda"" and not torch.cuda.is_available():
+                warn(""CUDA is not available, disabling CUDA profiling"")
+                self.use_cuda = False
+                self.use_device = None
+
+            if self.use_device == ""xpu"" and not torch.xpu.is_available():
+                warn(""XPU is not available, disabling XPU profiling"")
+                self.use_device = None"
0,https://github.com/PyTorch/PyTorch/commit/bf998a2c5d549cf4856c7becfca4a169bf68b709,NO,torch/ao/quantization/pt2e/prepare.py,patch_0,"-    _get_arg_as_input_act_obs_or_fq,
-    _get_output_act_obs_or_fq,
-    _get_dtype_and_is_dynamic,
",No
0,https://github.com/PyTorch/PyTorch/commit/bf998a2c5d549cf4856c7becfca4a169bf68b709,NO,torch/ao/quantization/pt2e/prepare.py,patch_1,"-    QuantizationAnnotation,
",No
0,https://github.com/PyTorch/PyTorch/commit/bf998a2c5d549cf4856c7becfca4a169bf68b709,NO,torch/ao/quantization/pt2e/prepare.py,patch_2,"-    quantization_annotation = node.meta.get(""quantization_annotation"", QuantizationAnnotation())
-    arg_as_input_act_obs_or_fq = _get_arg_as_input_act_obs_or_fq(arg, node, named_modules, obs_or_fq_map, is_qat)
-    arg_as_input_target_dtype, arg_as_input_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_input_act_obs_or_fq)
-
-    arg_as_output_act_obs_or_fq = _get_output_act_obs_or_fq(arg, named_modules, obs_or_fq_map, is_qat)
-    arg_as_output_target_dtype, arg_as_output_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_output_act_obs_or_fq)
-
-    if arg_as_input_target_is_dynamic or arg_as_input_target_dtype not in [torch.float, None]:
-        if arg_as_input_target_dtype == arg_as_output_target_dtype and \
-           arg_as_input_target_is_dynamic == arg_as_output_target_is_dynamic:
-            assert _is_activation_post_process_node(arg, named_modules)
-            assert arg_as_input_act_obs_or_fq is not None
-            observed_arg = arg.args[0]
-            assert isinstance(observed_arg, Node), f""expect observed argument to be a Node, but got: {type(observed_arg)}""
-            assert observed_arg in obs_or_fq_map, \
-                f""can't find a sharing group for node: {observed_arg}""
-            # reuse the existing obs/fq
-            arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]
-            # we don't need to insert new observer node
-            new_arg = arg
-        else:
-            # skip inserting new observers if there is an observer inserted for the arg before
-            # that has the same dtype that we want to insert here
-            # alternatively we could have a dedup pass after we insert all observers to deduplicate
-            # observers
-            # Example:
-            # arg -> existing_obs -> conv1
-            #    \ -> conv2
-            #
-            # instead of inserting new observers we will have:
-            # arg -> existing_obs -> conv1
-            #                   \ -> conv2
-            existing_obs_node = None
-            for maybe_obs_node in arg.users.keys():
-                if maybe_obs_node.op == 'call_module':
-                    maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]
-                    if (
-                        type(maybe_obs_mod) == type(arg_as_input_act_obs_or_fq) and
-                        maybe_obs_mod.dtype == arg_as_input_target_dtype
-                    ):
-                        arg_as_input_act_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]
-                        existing_obs_node = maybe_obs_node
-                        break
-
-            assert arg_as_input_act_obs_or_fq is not None
-            if existing_obs_node is None:
-                maybe_observed_arg = arg
-                # When quantizing two layers with different configs we can have
-                # conv2d (int8) -> avgpool(uint8)
-                # In this case observer insertion for avgpool will come here but the input
-                # to avgpool will be output observer of conv2d
-                # Now the obs map that we update must correspond to the original input of
-                # avgpool and not the output obs of conv2d
-                # This is because when referring to the edge, quantizer would refer to
-                # original input and not the observed one.
-                while _is_activation_post_process_node(arg, named_modules):
-                    arg = arg.args[0]  # type: ignore[assignment]
-                arg_as_input_act_obs_or_fq = obs_or_fq_map[(arg, node)]
-                new_obs_node = _insert_obs_or_fq(
-                    maybe_observed_arg, arg_as_input_act_obs_or_fq, model, named_modules, model.graph)
-                # override this arg to be the observed arg
-                new_arg = new_obs_node
-            else:
-                new_arg = existing_obs_node
",No
0,https://github.com/PyTorch/PyTorch/commit/bf998a2c5d549cf4856c7becfca4a169bf68b709,NO,torch/ao/quantization/quantizer/quantizer.py,patch_0,"-    input_qspec_map: Dict[Node, QuantizationSpecBase] = field(default_factory=dict)
",No
0,https://github.com/PyTorch/PyTorch/commit/bf998a2c5d549cf4856c7becfca4a169bf68b709,NO,torch/ao/quantization/quantizer/xnnpack_quantizer.py,patch_0,"-    bias_observer_or_fake_quant_ctr: _ObserverOrFakeQuantizeConstructor = (
-        PlaceholderObserver
-    )
-    bias_quantization_spec = QuantizationSpec(
-        dtype=torch.float, observer_or_fake_quant_ctr=bias_observer_or_fake_quant_ctr
-    )
",No
0,https://github.com/PyTorch/PyTorch/commit/9aa7699185e4ec39077e3046dfd63244dffa9ddb,NO,torch/distributed/_composable/fsdp/_fsdp_collectives.py,patch_0,"-    divide_factors: Union[Tuple[None, None], Tuple[float, float]],
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/9aa7699185e4ec39077e3046dfd63244dffa9ddb,NO,torch/distributed/_composable/fsdp/_fsdp_collectives.py,patch_1,"-    predivide_factor, postdivide_factor = divide_factors
","+    predivide_factor, postdivide_factor = _get_gradient_divide_factors(
+        reduce_scatter_group, all_reduce_group, reduce_dtype
+    )
","@@ -142,7 +141,9 @@ def foreach_reduce(
     )
     grad_dtype = unsharded_grads[0].dtype
     reduce_dtype = reduce_dtype or grad_dtype
-    predivide_factor, postdivide_factor = divide_factors
+    predivide_factor, postdivide_factor = _get_gradient_divide_factors(
+        reduce_scatter_group, all_reduce_group, reduce_dtype
+    )
     world_size = reduce_scatter_group.size()
     padded_unsharded_sizes = tuple(
         _get_dim0_padded_size(grad.size(), world_size) for grad in unsharded_grads"
0,https://github.com/PyTorch/PyTorch/commit/9aa7699185e4ec39077e3046dfd63244dffa9ddb,NO,torch/distributed/_composable/fsdp/_fsdp_collectives.py,patch_2,"-        _reduce_scatter(
-            post_reduce_output,
-            reduce_scatter_input,
-            reduce_scatter_group,
-            divide_factors,
-            _all_reduce(post_reduce_output, all_reduce_group, divide_factors)
","+        dist.reduce_scatter_tensor(
+            output=post_reduce_output,
+            input=reduce_scatter_input,
+            group=reduce_scatter_group,
+            op=ReduceOp.AVG if predivide_factor is None else ReduceOp.SUM,
+            dist.all_reduce(
+                post_reduce_output,
+                group=all_reduce_group,
+                op=ReduceOp.AVG if predivide_factor is None else ReduceOp.SUM,
+            )
","@@ -166,18 +167,22 @@ def foreach_reduce(
             (reduce_scatter_output_numel,)
         )
         _div_if_needed(reduce_scatter_input, predivide_factor)
-        _reduce_scatter(
-            post_reduce_output,
-            reduce_scatter_input,
-            reduce_scatter_group,
-            divide_factors,
+        dist.reduce_scatter_tensor(
+            output=post_reduce_output,
+            input=reduce_scatter_input,
+            group=reduce_scatter_group,
+            op=ReduceOp.AVG if predivide_factor is None else ReduceOp.SUM,
         )
     view_out_stream = reduce_scatter_stream
     if all_reduce_group is not None:
         view_out_stream = all_reduce_stream
         all_reduce_stream.wait_stream(reduce_scatter_stream)
         with torch.cuda.stream(all_reduce_stream):
-            _all_reduce(post_reduce_output, all_reduce_group, divide_factors)
+            dist.all_reduce(
+                post_reduce_output,
+                group=all_reduce_group,
+                op=ReduceOp.AVG if predivide_factor is None else ReduceOp.SUM,
+            )
     with torch.cuda.stream(view_out_stream):
         _div_if_needed(post_reduce_output, postdivide_factor)
         post_reduce_output = _to_dtype_if_needed(post_reduce_output, orig_dtype)"
0,https://github.com/PyTorch/PyTorch/commit/9aa7699185e4ec39077e3046dfd63244dffa9ddb,NO,torch/distributed/_composable/fsdp/_fsdp_collectives.py,patch_3,"-def _reduce_scatter(
-    output: torch.Tensor,
-    input: torch.Tensor,
-    group: dist.ProcessGroup,
-    divide_factors: Union[Tuple[None, None], Tuple[float, float]],
-) -> None:
-    if divide_factors[0]:
-        dist.reduce_scatter_tensor(output, input, group=group)
-    else:
-        # Using NCCL's reduce-scatter to do the division by world size saves
-        # extra memory read/write from a separate division kernel
-        dist.reduce_scatter_tensor(output, input, op=ReduceOp.AVG, group=group)
-
-
-def _all_reduce(
-    tensor: torch.Tensor,
-    group: dist.ProcessGroup,
-    divide_factors: Union[Tuple[None, None], Tuple[float, float]],
-) -> None:
-    if divide_factors[0]:
-        dist.all_reduce(tensor, group=group)
-    else:
-        # saves extra memory read/write from a separate division kernel
-        dist.all_reduce(tensor, op=ReduceOp.AVG, group=group)
","+def _get_gradient_divide_factors(
+    reduce_scatter_group: dist.ProcessGroup,
+    all_reduce_group: Optional[dist.ProcessGroup],
+    reduce_dtype: torch.dtype,
+) -> Union[Tuple[None, None], Tuple[float, float]]:
+    # For fp32/bf16, we do not need to worry about overflow/underflow, so we
+    # use NCCL's built-in division to avoid separate div kernels
+    if reduce_dtype in (torch.float32, torch.bfloat16):
+        return None, None
+    data_parallel_size = reduce_scatter_group.size()
+    if all_reduce_group is not None:
+        data_parallel_size *= all_reduce_group.size()
+    # Since fp16 has smaller dynamic range than fp32/bf16, we want to avoid
+    # overflow/underflow. For N data parallel workers, each worker computes
+    # g_i, and they collectively reduce (g_1 + ... + g_N) / N. To avoid
+    # overflow/underflow, we divide by ~sqrt(N) before/after the reduction.
+    factor: int = 1
+    while data_parallel_size % factor == 0 and data_parallel_size / factor > factor:
+        factor *= 2
+    factor = float(factor)
+    return (factor, data_parallel_size / factor)
","--- a/module.py
+++ b/module.py
@@ -164,32 +164,6 @@ class FSDPParamGroup:
             )
         self._reduce_dtype = next(iter(reduce_dtypes))
 
-    def _init_grad_divide_factors(self):
-        data_parallel_world_size = 1
-        data_parallel_world_size *= self.mesh_info.shard_mesh_size
-        if self._is_hsdp:
-            data_parallel_world_size *= self.mesh_info.replicate_mesh_size
-        if self._reduce_dtype in (torch.float32, torch.bfloat16):
-            # Use NCCL's AVG op to divide after reduction since it is more
-            # performant and fp32 has sufficient precision
-            self._grad_divide_factors: Union[Tuple[None, None], Tuple[float, float]] = (
-                None,
-                None,
-            )
-            return
-        # Since fp16 has smaller dynamic range than fp32/bf16, we want to avoid
-        # overflow/underflow. For N data parallel workers, each worker computes
-        # g_i, and they collectively reduce (g_1 + ... + g_N) / N. To avoid
-        # overflow/underflow, we divide by ~sqrt(N) before/after the reduction.
-        factor: int = 1
-        while (
-            data_parallel_world_size % factor == 0
-            and data_parallel_world_size / factor > factor
-        ):
-            factor *= 2
-        factor = float(factor)
-        self._grad_divide_factors = (factor, data_parallel_world_size / factor)
-
     def lazy_init(self):
         # Lazy init should be idempotent
         param_names_on_meta = [

+def _get_gradient_divide_factors(
+    reduce_scatter_group: dist.ProcessGroup,
+    all_reduce_group: Optional[dist.ProcessGroup],
+    reduce_dtype: torch.dtype,
+) -> Union[Tuple[None, None], Tuple[float, float]]:
+    # For fp32/bf16, we do not need to worry about overflow/underflow, so we
+    # use NCCL's built-in division to avoid separate div kernels
+    if reduce_dtype in (torch.float32, torch.bfloat16):
+        return None, None
+    data_parallel_size = reduce_scatter_group.size()
+    if all_reduce_group is not None:
+        data_parallel_size *= all_reduce_group.size()
+    # Since fp16 has smaller dynamic range than fp32/bf16, we want to avoid
+    # overflow/underflow. For N data parallel workers, each worker computes
+    # g_i, and they collectively reduce (g_1 + ... + g_N) / N. To avoid
+    # overflow/underflow, we divide by ~sqrt(N) before/after the reduction.
+    factor: int = 1
+    while data_parallel_size % factor == 0 and data_parallel_size / factor > factor:
+        factor *= 2
+    factor = float(factor)
+    return (factor, data_parallel_size / factor)"
0,https://github.com/PyTorch/PyTorch/commit/9aa7699185e4ec39077e3046dfd63244dffa9ddb,NO,torch/distributed/_composable/fsdp/_fsdp_param_group.py,patch_0,"-from typing import Any, cast, Dict, List, NamedTuple, Optional, Set, Tuple, Union
","+from typing import Any, cast, Dict, List, NamedTuple, Optional, Set, Tuple
","- from typing import Any, cast, Dict, List, NamedTuple, Optional, Set, Tuple, Union
+ from typing import Any, cast, Dict, List, NamedTuple, Optional, Set, Tuple"
0,https://github.com/PyTorch/PyTorch/commit/9aa7699185e4ec39077e3046dfd63244dffa9ddb,NO,torch/distributed/_composable/fsdp/_fsdp_param_group.py,patch_1,"-    def _init_grad_divide_factors(self):
-        data_parallel_world_size = 1
-        data_parallel_world_size *= self.mesh_info.shard_mesh_size
-        if self._is_hsdp:
-            data_parallel_world_size *= self.mesh_info.replicate_mesh_size
-        if self._reduce_dtype in (torch.float32, torch.bfloat16):
-            # Use NCCL's AVG op to divide after reduction since it is more
-            # performant and fp32 has sufficient precision
-            self._grad_divide_factors: Union[Tuple[None, None], Tuple[float, float]] = (
-                None,
-                None,
-            )
-            return
-        # Since fp16 has smaller dynamic range than fp32/bf16, we want to avoid
-        # overflow/underflow. For N data parallel workers, each worker computes
-        # g_i, and they collectively reduce (g_1 + ... + g_N) / N. To avoid
-        # overflow/underflow, we divide by ~sqrt(N) before/after the reduction.
-        factor: int = 1
-        while (
-            data_parallel_world_size % factor == 0
-            and data_parallel_world_size / factor > factor
-        ):
-            factor *= 2
-        factor = float(factor)
-        self._grad_divide_factors = (factor, data_parallel_world_size / factor)
-
",,"@@ -0,0 +1,1 @@
+            # Compute the gradient divide factors at runtime based on the process groups
            self._grad_divide_factors = (1.0, float(data_parallel_world_size))
"
0,https://github.com/PyTorch/PyTorch/commit/9aa7699185e4ec39077e3046dfd63244dffa9ddb,NO,torch/distributed/_composable/fsdp/_fsdp_param_group.py,patch_2,"-        self._init_grad_divide_factors()
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/9aa7699185e4ec39077e3046dfd63244dffa9ddb,NO,torch/distributed/_composable/fsdp/_fsdp_param_group.py,patch_3,"-                self._grad_divide_factors,
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/0f81473d7b4a1bf09246410712df22541be7caf3,YES,torch/_refs/__init__.py,patch_0,,"+    if isinstance(a, TensorLike) and isinstance(b, TensorLike):
+        torch._check(
+            not utils.is_boolean_dtype(a.dtype) and not utils.is_boolean_dtype(b.dtype),
+            lambda: (
+                ""Subtraction, the `-` operator, with two bool tensors is not supported. ""
+                ""Use the `^` or `logical_xor()` operator instead.""
+            ),
+        )
+
","@@ -139,3 +139,13 @@ def _to_dtype_if_needed(
     if dtype is not None and tensor.dtype != dtype:
         return tensor.to(dtype)
     return tensor
+
+
+def _cast_fp_tensor(dtype: torch.dtype, x: torch.Tensor) -> torch.Tensor:
+    if (
+        not isinstance(x, torch.Tensor)
+        or not torch.is_floating_point(x)
+        or x.dtype == dtype
+    ):
+        return x
+    return x.to(dtype)"
0,https://github.com/PyTorch/PyTorch/commit/610f64d72a53c644591e83fbecdd8a8f1702f548,NO,torch/_inductor/codegen/cpp.py,patch_0,"-                with CppVecKernelChecker(
-                    deepcopy(self.kernel_group.args),
-                    parallel_num_threads(),
-                    tiling_factor,
-                    tiling_indices[-1],
-                ) as vec_checker:
-                    run(vec_checker)
-                if vec_checker.simd_vec:
","+                could_vec = True
+                for tiling_indice in tiling_indices:
+                    with CppVecKernelChecker(
+                        deepcopy(self.kernel_group.args),
+                        parallel_num_threads(),
+                        tiling_factor,
+                        tiling_indice,
+                    ) as vec_checker:
+                        run(vec_checker)
+                        could_vec = could_vec and vec_checker.simd_vec
+                        if not could_vec:
+                            break
+                if could_vec:
","@@ -1711,18 +1777,6 @@ class CppVecKernel(CppKernel):
         self.tiling_idx = tiling_idx
         metrics.generated_cpp_vec_kernel_count += 1
        
-    def index_is_vector_invariant(self, index: sympy.Expr):
-        """"""`index` is either independent from the tiling itervar or unchanged in the vector range""""""
-        tiling_var = self.itervars[self.tiling_idx]
-        if not self.index_depends_on(index, tiling_var):
-            return True
-        if not self.index_indirect_depends_on(index, tiling_var):
-            vec_range = [
-                sympy_subs(index, {tiling_var: i}) for i in range(self.tiling_factor)
-            ]
-            return all(expr == vec_range[0] for expr in vec_range)
-        return False
-
     def _get_vec_load_line(
         self,
         var: str,
-
-                with CppVecKernelChecker(
-                    deepcopy(self.kernel_group.args),
-                    parallel_num_threads(),
-                    tiling_factor,
-                    tiling_indices[-1],
-                ) as vec_checker:
-                    run(vec_checker)
-                if vec_checker.simd_vec:
+                could_vec = True
+                for tiling_indice in tiling_indices:
+                    with CppVecKernelChecker(
+                        deepcopy(self.kernel_group.args),
+                        parallel_num_threads(),
+                        tiling_factor,
+                        tiling_indice,
+                    ) as vec_checker:
+                        run(vec_checker)
+                        could_vec = could_vec and vec_checker.simd_vec
+                        if not could_vec:
+                            break
+                if could_vec:"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,c10/core/SafePyObject.h,patch_0,,"+// A newtype wrapper around SafePyObject for type safety when a python object
+// represents a specific type. Note that `T` is only used as a tag and isn't
+// actually used for any true purpose.
+template <typename T>
+struct SafePyObjectT : private SafePyObject {
+  SafePyObjectT(PyObject* data, c10::impl::PyInterpreter* pyinterpreter)
+      : SafePyObject(data, pyinterpreter) {}
+  SafePyObjectT(SafePyObjectT&& other) noexcept : SafePyObject(other) {}
+  SafePyObjectT(SafePyObjectT const&) = delete;
+  SafePyObjectT& operator=(SafePyObjectT const&) = delete;
+
+  using SafePyObject::ptr;
+  using SafePyObject::pyinterpreter;
+  using SafePyObject::release;
+};
+
","@@ -406,18 +406,15 @@ def istype(obj, allowed_types):
 
 
 def is_typing(value):
-    if sys.version_info < (3, 9):
-        return isinstance(value, typing._GenericAlias)
-    else:
-        return isinstance(
-            # `_SpecialForm`` is the parent class of `Optional`
-            value,
-            (
-                typing._SpecialGenericAlias,
-                typing._UnionGenericAlias,
-                typing._SpecialForm,
-            ),
-        )
+    # _Final catches most of typing classes:
+    #   - Any
+    #   - Callable
+    #   - Union
+    #   ...
+    #
+    # NB: we intentionally ignore classes that inherit from Generic, since they
+    # can be used as both TypingVariable as well as UserDefinedClassVariable.
+    return isinstance(value, typing._Final) or value is typing.Generic
 
 
 def is_numpy_int_type(value):"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,c10/core/impl/TorchDispatchModeTLS.cpp,patch_0,"-    std::shared_ptr<SafePyObject> mode) {
","+    std::shared_ptr<PyObject_TorchDispatchMode> mode) {
","@@ -422,6 +424,69 @@ static PyMethodDef TensorGuards_methods[] = {\n \n static PyTypeObject TensorGuardsType = {PyVarObject_HEAD_INIT(nullptr, 0)};\n \n+struct GlobalStateGuard {\n+  PyObject_HEAD;\n+\n+  inline void init() {\n+    auto& ctx = at::globalContext();\n+    _grad_mode = at::GradMode::is_enabled();\n+    _torch_function = torch::torch_function_enabled();\n+    _deterministic_algorithms = ctx.deterministicAlgorithms();\n+    _allow_tf32 = ctx.allowTF32CuBLAS();\n+    _allow_fp16_reduce = ctx.allowFP16ReductionCuBLAS();\n+    _allow_bf16_reduce = ctx.allowBF16ReductionCuBLAS();\n+    _num_threads = at::get_num_threads();\n+  }\n+\n+  inline bool check() {\n+    auto& ctx = at::globalContext();\n+    return (\n+        _grad_mode == at::GradMode::is_enabled() &&\n+        _torch_function == torch::torch_function_enabled() &&\n+        _deterministic_algorithms == ctx.deterministicAlgorithms() &&\n+        _allow_tf32 == ctx.allowTF32CuBLAS() &&\n+        _allow_fp16_reduce == ctx.allowFP16ReductionCuBLAS() &&\n+        _allow_bf16_reduce == ctx.allowBF16ReductionCuBLAS() &&\n+        _num_threads == at::get_num_threads());\n+  }\n+\n+  bool _grad_mode;\n+  bool _torch_function;\n+  bool _deterministic_algorithms;\n+  bool _allow_tf32;\n+  bool _allow_fp16_reduce;\n+  bool _allow_bf16_reduce;\n+  int _num_threads;\n+  // TODO(jansel): we should guard on more state as inductor starts using it\n+};\n+\n+int GlobalStateGuard_init(\n+    GlobalStateGuard* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n+  self->init();\n+  return 0;\n+}\n+\n+PyObject* GlobalStateGuard_check(\n+    GlobalStateGuard* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n+  if (self->check()) {\n+    Py_RETURN_TRUE;\n+  } else {\n+    Py_RETURN_FALSE;\n+  }\n+}\n+\n+static PyMethodDef GlobalStateGuard_methods[] = {\n+    {""check"",\n+     (PyCFunction)(void*)GlobalStateGuard_check,\n+     METH_NOARGS,\n+     ""Return true if global state was the same as at creation time""},\n+    {nullptr}};\n+static PyTypeObject GlobalStateGuardType = {PyVarObject_HEAD_INIT(NULL, 0)};\n+
 static PyObject* check_type_id(PyObject* dummy, PyObject* args) {\n   // faster `lambda obj, expected: id(type(obj)) == expected`\n   PyObject* obj = nullptr;\n'  "
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,c10/core/impl/TorchDispatchModeTLS.cpp,patch_1,"-const std::shared_ptr<SafePyObject> TorchDispatchModeTLS::pop_stack() {
-  std::shared_ptr<SafePyObject> out;
","+const std::shared_ptr<PyObject_TorchDispatchMode> TorchDispatchModeTLS::
+    pop_stack() {
+  std::shared_ptr<PyObject_TorchDispatchMode> out;
","@@ -422,6 +424,69 @@ static PyMethodDef TensorGuards_methods[] = {\n \n static PyTypeObject TensorGuardsType = {PyVarObject_HEAD_INIT(nullptr, 0)};\n \n+struct GlobalStateGuard {\n+  PyObject_HEAD;\n+\n+  inline void init() {\n+    auto& ctx = at::globalContext();\n+    _grad_mode = at::GradMode::is_enabled();\n+    _torch_function = torch::torch_function_enabled();\n+    _deterministic_algorithms = ctx.deterministicAlgorithms();\n+    _allow_tf32 = ctx.allowTF32CuBLAS();\n+    _allow_fp16_reduce = ctx.allowFP16ReductionCuBLAS();\n+    _allow_bf16_reduce = ctx.allowBF16ReductionCuBLAS();\n+    _num_threads = at::get_num_threads();\n+  }\n+\n+  inline bool check() {\n+    auto& ctx = at::globalContext();\n+    return (\n+        _grad_mode == at::GradMode::is_enabled() &&\n+        _torch_function == torch::torch_function_enabled() &&\n+        _deterministic_algorithms == ctx.deterministicAlgorithms() &&\n+        _allow_tf32 == ctx.allowTF32CuBLAS() &&\n+        _allow_fp16_reduce == ctx.allowFP16ReductionCuBLAS() &&\n+        _allow_bf16_reduce == ctx.allowBF16ReductionCuBLAS() &&\n+        _num_threads == at::get_num_threads());\n+  }\n+\n+  bool _grad_mode;\n+  bool _torch_function;\n+  bool _deterministic_algorithms;\n+  bool _allow_tf32;\n+  bool _allow_fp16_reduce;\n+  bool _allow_bf16_reduce;\n+  int _num_threads;\n+  // TODO(jansel): we should guard on more state as inductor starts using it\n+};\n+\n+int GlobalStateGuard_init(\n+    GlobalStateGuard* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n+  self->init();\n+  return 0;\n+}\n+\n+PyObject* GlobalStateGuard_check(\n+    GlobalStateGuard* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n+  if (self->check()) {\n+    Py_RETURN_TRUE;\n+  } else {\n+    Py_RETURN_FALSE;\n+  }\n+}\n+\n+static PyMethodDef GlobalStateGuard_methods[] = {\n+    {""check"",\n+     (PyCFunction)(void*)GlobalStateGuard_check,\n+     METH_NOARGS,\n+     ""Return true if global state was the same as at creation time""},\n+    {nullptr}};\n+static PyTypeObject GlobalStateGuardType = {PyVarObject_HEAD_INIT(nullptr, 0)};\n+\n static PyObject* check_type_id(PyObject* dummy, PyObject* args) {\n   // faster `lambda obj, expected: id(type(obj)) == expected`\n   PyObject* obj = nullptr;\n
-const std::shared_ptr<SafePyObject> TorchDispatchModeTLS::pop_stack() {
-  std::shared_ptr<SafePyObject> out;
+const std::shared_ptr<PyObject_TorchDispatchMode> TorchDispatchModeTLS::
+    pop_stack() {
+  std::shared_ptr<PyObject_TorchDispatchMode> out;"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,c10/core/impl/TorchDispatchModeTLS.cpp,patch_2,"-const std::tuple<std::shared_ptr<SafePyObject>, TorchDispatchModeKey>
-TorchDispatchModeTLS::pop_highest_infra_mode() {
","+const std::
+    tuple<std::shared_ptr<PyObject_TorchDispatchMode>, TorchDispatchModeKey>
+    TorchDispatchModeTLS::pop_highest_infra_mode() {
","--- a/torch/csrc/jit/runtime/interpreter.cpp
+++ b/torch/csrc/jit/runtime/interpreter.cpp
@@ -159,6 +159,15 @@
               static_cast<size_t>(inst.X) >= code.constants_.size()) {
             TORCH_CHECK(false, ""Can't load constant with index: "", inst.X);
           }
+          if (inst.N == 0 || inst.N > stack.size()) {
+            TORCH_CHECK(
+                false,
+                ""INTERFACE_CALL N="",
+                inst.N,
+                "" not in range [1, "",
+                stack.size(),
+                ""]"");
+          }
           torch::jit::Function& method =
               peek(stack, 0, inst.N)
                   .toObject()"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,c10/core/impl/TorchDispatchModeTLS.cpp,patch_3,"-const std::shared_ptr<SafePyObject>& TorchDispatchModeTLS::get_stack_at(
-    int64_t idx) {
","+const std::shared_ptr<PyObject_TorchDispatchMode>& TorchDispatchModeTLS::
+    get_stack_at(int64_t idx) {
","@@ -159,6 +159,15 @@ bool InterpreterState::run(Stack& stack) {
               static_cast<size_t>(inst.X) >= code.constants_.size()) {
             TORCH_CHECK(false, ""Can\'t load constant with index: "", inst.X);
           }
+          if (inst.N == 0 || inst.N > stack.size()) {
+            TORCH_CHECK(
+                false,
+                ""INTERFACE_CALL N="",
+                inst.N,
+                "" not in range [1, "",
+                stack.size(),
+                ""]"");
+          }
           torch::jit::Function& method =
               peek(stack, 0, inst.N)
                   .toObject()"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,c10/core/impl/TorchDispatchModeTLS.cpp,patch_4,"-const c10::optional<std::shared_ptr<SafePyObject>> TorchDispatchModeTLS::
-    get_mode(TorchDispatchModeKey mode_key) {
-    const std::shared_ptr<SafePyObject>& mode,
","+const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>
+TorchDispatchModeTLS::get_mode(TorchDispatchModeKey mode_key) {
+    const std::shared_ptr<PyObject_TorchDispatchMode>& mode,
","@@ -422,6 +424,69 @@ static PyMethodDef TensorGuards_methods[] = {\n \n static PyTypeObject TensorGuardsType = {PyVarObject_HEAD_INIT(nullptr, 0)};\n \n+struct GlobalStateGuard {\n+  PyObject_HEAD;\n+\n+  inline void init() {\n+    auto& ctx = at::globalContext();\n+    _grad_mode = at::GradMode::is_enabled();\n+    _torch_function = torch::torch_function_enabled();\n+    _deterministic_algorithms = ctx.deterministicAlgorithms();\n+    _allow_tf32 = ctx.allowTF32CuBLAS();\n+    _allow_fp16_reduce = ctx.allowFP16ReductionCuBLAS();\n+    _allow_bf16_reduce = ctx.allowBF16ReductionCuBLAS();\n+    _num_threads = at::get_num_threads();\n+  }\n+\n+  inline bool check() {\n+    auto& ctx = at::globalContext();\n+    return (\n+        _grad_mode == at::GradMode::is_enabled() &&\n+        _torch_function == torch::torch_function_enabled() &&\n+        _deterministic_algorithms == ctx.deterministicAlgorithms() &&\n+        _allow_tf32 == ctx.allowTF32CuBLAS() &&\n+        _allow_fp16_reduce == ctx.allowFP16ReductionCuBLAS() &&\n+        _allow_bf16_reduce == ctx.allowBF16ReductionCuBLAS() &&\n+        _num_threads == at::get_num_threads());\n+  }\n+\n+  bool _grad_mode;\n+  bool _torch_function;\n+  bool _deterministic_algorithms;\n+  bool _allow_tf32;\n+  bool _allow_fp16_reduce;\n+  bool _allow_bf16_reduce;\n+  int _num_threads;\n+  // TODO(jansel): we should guard on more state as inductor starts using it\n+};\n+\n+int GlobalStateGuard_init(\n+    GlobalStateGuard* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n+  self->init();\n+  return 0;\n+}\n+\n+PyObject* GlobalStateGuard_check(\n+    GlobalStateGuard* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n+  if (self->check()) {\n+    Py_RETURN_TRUE;\n+  } else {\n+    Py_RETURN_FALSE;\n+  }\n+}\n+\n+static PyMethodDef GlobalStateGuard_methods[] = {\n+    {""check"",\n+     (PyCFunction)(void*)GlobalStateGuard_check,\n+     METH_NOARGS,\n+     ""Return true if global state was the same as at creation time""},\n+    {nullptr}};\n+static PyTypeObject GlobalStateGuardType = {PyVarObject_HEAD_INIT(nullptr, 0)};\n+\n const c10::optional<std::shared_ptr<SafePyObject>> TorchDispatchModeTLS::\n-    get_mode(TorchDispatchModeKey mode_key) {\n-    const std::shared_ptr<SafePyObject>& mode,\n+    get_mode(TorchDispatchModeKey mode_key) {\n+    const std::shared_ptr<PyObject_TorchDispatchMode>& mode,
   ''"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,c10/core/impl/TorchDispatchModeTLS.cpp,patch_5,"-const c10::optional<std::shared_ptr<SafePyObject>> TorchDispatchModeTLS::
-    unset_mode(TorchDispatchModeKey mode_key) {
","+const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>
+TorchDispatchModeTLS::unset_mode(TorchDispatchModeKey mode_key) {
","
--- a/torch/csrc/autograd/variable_type_undef.cpp
+++ b/torch/csrc/autograd/variable_type_undef.cpp
@@ -422,6 +424,69 @@ static PyMethodDef TensorGuards_methods[] = {
 
 static PyTypeObject TensorGuardsType = {PyVarObject_HEAD_INIT(nullptr, 0)};
 
+struct GlobalStateGuard {
+  PyObject_HEAD;
+
+  inline void init() {
+    auto& ctx = at::globalContext();
+    _grad_mode = at::GradMode::is_enabled();
+    _torch_function = torch::torch_function_enabled();
+    _deterministic_algorithms = ctx.deterministicAlgorithms();
+    _allow_tf32 = ctx.allowTF32CuBLAS();
+    _allow_fp16_reduce = ctx.allowFP16ReductionCuBLAS();
+    _allow_bf16_reduce = ctx.allowBF16ReductionCuBLAS();
+    _num_threads = at::get_num_threads();
+  }
+
+  inline bool check() {
+    auto& ctx = at::globalContext();
+    return (
+        _grad_mode == at::GradMode::is_enabled() &&
+        _torch_function == torch::torch_function_enabled() &&
+        _deterministic_algorithms == ctx.deterministicAlgorithms() &&
+        _allow_tf32 == ctx.allowTF32CuBLAS() &&
+        _allow_fp16_reduce == ctx.allowFP16ReductionCuBLAS() &&
+        _allow_bf16_reduce == ctx.allowBF16ReductionCuBLAS() &&
+        _num_threads == at::get_num_threads());
+  }
+
+  bool _grad_mode;
+  bool _torch_function;
+  bool _deterministic_algorithms;
+  bool _allow_tf32;
+  bool _allow_fp16_reduce;
+  bool _allow_bf16_reduce;
+  int _num_threads;
+  // TODO(jansel): we should guard on more state as inductor starts using it
+};
+
+int GlobalStateGuard_init(
+    GlobalStateGuard* self,
+    PyObject* args,
+    PyObject* kwargs) {
+  self->init();
+  return 0;
+}
+
+PyObject* GlobalStateGuard_check(
+    GlobalStateGuard* self,
+    PyObject* args,
+    PyObject* kwargs) {
+  if (self->check()) {
+    Py_RETURN_TRUE;
+  } else {
+    Py_RETURN_FALSE;
+  }
+}
+
+static PyMethodDef GlobalStateGuard_methods[] = {
+    {""check"",
+     (PyCFunction)(void*)GlobalStateGuard_check,
+     METH_NOARGS,
+     ""Return true if global state was the same as at creation time""},
+    {nullptr}};
+static PyTypeObject GlobalStateGuardType = {PyVarObject_HEAD_INIT(nullptr, 0)};
+
 static PyObject* check_type_id(PyObject* dummy, PyObject* args) {
   // faster `lambda obj, expected: id(type(obj)) == expected`
   PyObject* obj = nullptr;"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,c10/core/impl/TorchDispatchModeTLS.h,patch_0,"-      std::shared_ptr<SafePyObject> mode);
-  static const std::shared_ptr<SafePyObject> pop_stack();
-  static const std::tuple<std::shared_ptr<SafePyObject>, TorchDispatchModeKey>
-  pop_highest_infra_mode();
-  static const std::shared_ptr<SafePyObject>& get_stack_at(int64_t idx);
-  static const c10::optional<std::shared_ptr<SafePyObject>> get_mode(
-      TorchDispatchModeKey mode_key);
-  static const c10::optional<std::shared_ptr<SafePyObject>> unset_mode(
-      TorchDispatchModeKey mode_key);
-      const std::shared_ptr<SafePyObject>& mode,
","+using PyObject_TorchDispatchMode = SafePyObjectT<TorchDispatchModeKey>;
+
+      std::shared_ptr<PyObject_TorchDispatchMode> mode);
+  static const std::shared_ptr<PyObject_TorchDispatchMode> pop_stack();
+  static const std::
+      tuple<std::shared_ptr<PyObject_TorchDispatchMode>, TorchDispatchModeKey>
+      pop_highest_infra_mode();
+  static const std::shared_ptr<PyObject_TorchDispatchMode>& get_stack_at(
+      int64_t idx);
+  static const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>
+  get_mode(TorchDispatchModeKey mode_key);
+  static const c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>
+  unset_mode(TorchDispatchModeKey mode_key);
+      const std::shared_ptr<PyObject_TorchDispatchMode>& mode,
","@@ -422,6 +424,69 @@ static PyMethodDef TensorGuards_methods[] = {
 
 static PyTypeObject TensorGuardsType = {PyVarObject_HEAD_INIT(nullptr, 0)};
 
+struct GlobalStateGuard {
+  PyObject_HEAD;
+
+  inline void init() {
+    auto& ctx = at::globalContext();
+    _grad_mode = at::GradMode::is_enabled();
+    _torch_function = torch::torch_function_enabled();
+    _deterministic_algorithms = ctx.deterministicAlgorithms();
+    _allow_tf32 = ctx.allowTF32CuBLAS();
+    _allow_fp16_reduce = ctx.allowFP16ReductionCuBLAS();
+    _allow_bf16_reduce = ctx.allowBF16ReductionCuBLAS();
+    _num_threads = at::get_num_threads();
+  }
+
+  inline bool check() {
+    auto& ctx = at::globalContext();
+    return (
+        _grad_mode == at::GradMode::is_enabled() &&
+        _torch_function == torch::torch_function_enabled() &&
+        _deterministic_algorithms == ctx.deterministicAlgorithms() &&
+        _allow_tf32 == ctx.allowTF32CuBLAS() &&
+        _allow_fp16_reduce == ctx.allowFP16ReductionCuBLAS() &&
+        _allow_bf16_reduce == ctx.allowBF16ReductionCuBLAS() &&
+        _num_threads == at::get_num_threads());
+  }
+
+  bool _grad_mode;
+  bool _torch_function;
+  bool _deterministic_algorithms;
+  bool _allow_tf32;
+  bool _allow_fp16_reduce;
+  bool _allow_bf16_reduce;
+  int _num_threads;
+  // TODO(jansel): we should guard on more state as inductor starts using it
+};
+
+int GlobalStateGuard_init(
+    GlobalStateGuard* self,
+    PyObject* args,
+    PyObject* kwargs) {
+  self->init();
+  return 0;
+}
+
+PyObject* GlobalStateGuard_check(
+    GlobalStateGuard* self,
+    PyObject* args,
+    PyObject* kwargs) {
+  if (self->check()) {
+    Py_RETURN_TRUE;
+  } else {
+    Py_RETURN_FALSE;
+  }
+}
+
+static PyMethodDef GlobalStateGuard_methods[] = {
+    {""check"",
+     (PyCFunction)(void*)GlobalStateGuard_check,
+     METH_NOARGS,
+     ""Return true if global state was the same as at creation time""},
+    {nullptr}};
+static PyTypeObject GlobalStateGuardType = {PyVarObject_HEAD_INIT(nullptr, 0)};
+
 static PyObject* check_type_id(PyObject* dummy, PyObject* args) {
   // faster `lambda obj, expected: id(type(obj)) == expected`
   PyObject* obj = nullptr;"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,c10/core/impl/TorchDispatchModeTLS.h,patch_1,"-  std::vector<std::shared_ptr<c10::SafePyObject>> stack_;
-      c10::optional<std::shared_ptr<c10::SafePyObject>>,
","+  std::vector<std::shared_ptr<PyObject_TorchDispatchMode>> stack_;
+      c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>,
","@@ -1,4 +1,4 @@
-  std::vector<std::shared_ptr<c10::SafePyObject>> stack_;
-      c10::optional<std::shared_ptr<c10::SafePyObject>>,
+  std::vector<std::shared_ptr<PyObject_TorchDispatchMode>> stack_;
+      c10::optional<std::shared_ptr<PyObject_TorchDispatchMode>>, "
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_C/__init__.pyi.in,patch_0,,"+from torch.utils._python_dispatch import TorchDispatchMode
","@@ -1,6 +1,8 @@
 # mypy: allow-untyped-defs
 import contextlib
 
+import logging
+
 import torch
 import torch._subclasses.functional_tensor
 import torch.utils._pytree as pytree
+from torch.utils._python_dispatch import TorchDispatchMode"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_C/__init__.pyi.in,patch_1,"-def _push_on_torch_dispatch_stack(cls: Any) -> None: ...
-def _unset_dispatch_mode(mode: torch._C._TorchDispatchModeKey) -> Any: ...
-def _set_dispatch_mode(mode: Any) -> None: ...
","+def _push_on_torch_dispatch_stack(cls: TorchDispatchMode) -> None: ...
+def _unset_dispatch_mode(mode: torch._C._TorchDispatchModeKey) -> Optional[TorchDispatchMode]: ...
+def _set_dispatch_mode(mode: TorchDispatchMode) -> None: ...
","@@ -79,8 +80,14 @@
 from torch.testing._internal.common_utils import (
 )
 from torch.utils._mode_utils import no_dispatch
-from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+from torch.utils.checkpoint import (
+    checkpoint,
+    checkpoint_sequential,
+    CheckpointPolicy,
+    create_selective_checkpoint_contexts,
+)
 from torch.utils.cpp_extension import load_inline
+from torch.utils.flop_counter import FlopCounterMode
 from torch.utils.hooks import RemovableHandle  # noqa: TCH001
 
 
-def _push_on_torch_dispatch_stack(cls: Any) -> None: ...
-def _unset_dispatch_mode(mode: torch._C._TorchDispatchModeKey) -> Any: ...
-def _set_dispatch_mode(mode: Any) -> None: ...
+def _push_on_torch_dispatch_stack(cls: TorchDispatchMode) -> None: ...
+def _unset_dispatch_mode(mode: torch._C._TorchDispatchModeKey) -> Optional[TorchDispatchMode]: ...
+def _set_dispatch_mode(mode: TorchDispatchMode) -> None: ..."
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_C/__init__.pyi.in,patch_2,,No
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_C/__init__.pyi.in,patch_3,,No
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_ops.py,patch_0,"-ops = _Ops()
","+ops: _Ops = _Ops()
","['-ops = _Ops()\n', '+ops: _Ops = _Ops()\n']"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_0,"-# mypy: ignore-errors
-
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_1,"-from typing import Any, Dict, List, Optional, Tuple, Type, TYPE_CHECKING, TypeVar
","+from typing import (
+    Any,
+    cast,
+    Dict,
+    List,
+    Optional,
+    Tuple,
+    Type,
+    TYPE_CHECKING,
+    TypeVar,
+    Union,
+)
","---fake_tensor.py
@@ -12,6 +12,7 @@
     List,
     NamedTuple,
+    no_type_check,
     Optional,
     Sequence,
     Set,"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_2,,"+from torch.types import _bool
","@@ -1,5 +1,6 @@
 # Owner(s): [""module: fx""]
 
+import copy
 from typing import Set, Type
 
 import torch"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_3,,"+
+class _Unassigned:
+    pass
+
+
+_UNASSIGNED = _Unassigned()
+
","@@ -83,9 +109,7 @@ def config_of(
                 offset_aligned = V.graph.sizevars.statically_known_multiple_of(
                     x.offset * x.dtype.itemsize, alignment  # type: ignore[arg-type]
                 )
-                return offset_aligned and not V.graph.scheduler.is_unaligned_buffer(
-                    x.buffer
-                )
+                return offset_aligned and not is_unaligned_buffer(x)
             else:
                 return False
         if isinstance(x, SizeArg):"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_4,"-    memory_format = suggest_memory_format(t)
","+    memory_format: Optional[torch.memory_format] = suggest_memory_format(t)
","@@ -15,6 +15,8 @@
         ""torch._C._construct_CUDA_Tensor_From_Storage_And_Metadata"",
         ""torch._C._construct_storage_from_data_pointer"",
         ""torch._C._conv_determine_backend_memory_format"",
+        ""torch._C._cpu._is_cpu_support_avx2"",
+        ""torch._C._cpu._is_cpu_support_avx512"",
         ""torch._C._cpu._is_cpu_support_vnni"",
         ""torch._C._crash_if_aten_asan"",
         ""torch._C._crash_if_csrc_asan"","
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_5,"-    cache_bypasses = defaultdict(int)
","+    cache_bypasses: Dict[str, int] = defaultdict(int)
+    in_kernel_invocation: bool = False
","@@ -24,7 +24,7 @@ class TorchDispatchModeTLS(threading.local):\n             # access the cache dictionary to update its state.
             'base_types',
             'mode',
-            'cache_bypasses',
+            'cache_bypasses: Dict[str, int]',
             'in_kernel_invocation',
             'mode_state',
         )
"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_6,"-        self.enter_stack: List[Tuple[bool, Optional[FakeTensorMode]]] = []
","+        self.enter_stack: List[
+            Tuple[bool, Optional[TorchDispatchMode], Optional[_bool]]
+        ] = []
","@@ -1,3 +1,5 @@
+import itertools
+
 from typing import List, NamedTuple, Optional
 
 import torch"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_7,"-        output = unassigned = object()
","+        output: Union[FakeTensor, _Unassigned] = _UNASSIGNED
","@@ -8,7 +8,7 @@ SymExpr:
     type: str
   hint:
     type: Optional[SymExprHint]
-      default: None
+      default: None
 SymExprHint:
   kind: union
   fields:"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_8,"-        if output is unassigned:
","+        if output is _UNASSIGNED:
","@@ -241,7 +241,7 @@ def _get_expr_decorate(self, expr, p, is_super, *flags, **kwargs):
     if is_super and (expr == 0):
         return """"
 
-    if output is unassigned:
+    if output is _UNASSIGNED:
         return """"
 
     return self.paren(output)"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_9,"-        result = []
","+        result: List[Any] = []
","@@ -1,2 +1,2 @@
-        result = []
+        result: List[SafePyObjectT[Any]] = []"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_10,"-        assert not metadata.is_sparse
","+        assert metadata and not metadata.is_sparse
","@@ -25,6 +25,6 @@ def test_to_module(self):
 
         assert wrapper_mode == orig_mode
 
-        assert not metadata.is_sparse
+        assert metadata and not metadata.is_sparse
 
         assert loss.grad_fn.metadata is metadata"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_11,"-            storage = args[entry.view_idx].untyped_storage()
","+            storage = args[cast(int, entry.view_idx)].untyped_storage()
","@@ -1910,6 +1910,9 @@ class BaseView(IRNode):
     def is_extern(self):
         return self.data.is_extern()  # type: ignore[attr-defined]
 
+    def is_module_buffer(self):
+        return self.data.is_module_buffer()  # type: ignore[attr-defined]
+
     def get_reads(self):
         with patch.object(FlexibleLayout, ""allow_indexing"", True):
             return extract_read_writes("
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_12,"-    def _dispatch_impl(self, func, types, args, kwargs):
","+    def _dispatch_impl(self, func, types, args, kwargs) -> FakeTensor:
","@@ -11,6 +11,7 @@ class FakeTensorMode(TorchDispatchMode):
         # from the cache matches the output created by normal dispatch.
         self._crosscheck_cache_output(output, func, types, args, kwargs)
     else:
+        self._validate_cache_key(func, args, kwargs)
         output = self._dispatch_impl(func, types, args, kwargs)
         entry = self._make_cache_entry(key, func, args, kwargs, output)
         FakeTensorMode.cache[key] = entry"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_13,"-        flat_arg_fake_tensors = []
","+        flat_arg_fake_tensors: List[Any] = []
","@@ -122,53 +118,47 @@ void multi_tensor_apply(
       tensor_lists.size() == depth,
       ""Number of tensor lists has to match the depth."");
   const size_t n_tensors = tensor_lists[0].size();
-  size_t n_zero_tensors = 0;
   using scalar_vals_t = typename T::opmath_t;
   TensorListScalarListMetadata<scalar_vals_t, depth> tensorListMeta;
 
   int loc_block_info = 0;
   int loc_tensor_info = 0;
   for (size_t t = 0; t < n_tensors; t++) {
+    // short-circuit to avoid adding empty tensors to tensorListMeta
     if (tensor_lists[0][t].numel() == 0) {
-      n_zero_tensors++;
-      if (t != n_tensors - 1) {
-        continue;
-      }
-    } else {
-      tensorListMeta.scalar_vals[loc_tensor_info] = scalars[t].to<scalar_T>();
-      tensorListMeta.numel_for_tensor[loc_tensor_info] =
-          tensor_lists[0][t].numel();
-      for (int d = 0; d < depth; d++) {
-        tensorListMeta.addresses[d][loc_tensor_info] =
-            tensor_lists[d][t].const_data_ptr();
-      }
-      loc_tensor_info++;
-    }
-
-    if (n_zero_tensors == n_tensors) {
       continue;
     }
-    const auto chunks = (tensor_lists[0]
-                                     [t -
-                                      static_cast<size_t>(
-                                          (t == n_tensors - 1) &&
-                                          (tensor_lists[0][t].numel() == 0))]
-                                         .numel() +
-                         kChunkSize - 1) /
-        kChunkSize;
+    tensorListMeta.scalar_vals[loc_tensor_info] = scalars[t].to<scalar_T>();
+    tensorListMeta.numel_for_tensor[loc_tensor_info] =
+        tensor_lists[0][t].numel();
+    for (int d = 0; d < depth; d++) {
+      tensorListMeta.addresses[d][loc_tensor_info] =
+          tensor_lists[d][t].const_data_ptr();
+    }
+    loc_tensor_info++;
+
+    // now we enter [chunking territory].
+    // we will launch a kernel when EITHER the blocks get filled up OR
+    // the tensors get filled up. There will always be at least one block
+    // per tensor since the zero-sized ones will not enter the loop, so
+    // the nested for loop within represents iterating through the chunks
+    // of a single tensor.
+    const auto chunks =
+        (tensor_lists[0][t].numel() + kChunkSize - 1) / kChunkSize;
     for (auto chunk = 0; chunk < chunks; chunk++) {
       tensorListMeta.block_to_tensor[loc_block_info] = loc_tensor_info - 1;
       tensorListMeta.block_to_chunk[loc_block_info] = chunk;
       loc_block_info++;
 
+      // a tensor is not considered full unless all its chunks have been
+      // processed
       const bool tensors_full =
           (loc_tensor_info == depth_to_max_tensors_scalarlist[depth - 1] &&
            chunk == chunks - 1);
       const bool blocks_full =
           (loc_block_info == depth_to_max_blocks[depth - 1]);
-      const bool last_chunk = (t == n_tensors - 1 && chunk == chunks - 1);
 
-      if (tensors_full || blocks_full || last_chunk) {
+      if (tensors_full || blocks_full) {
         multi_tensor_apply_kernel<<<
             loc_block_info,
             kBlockSize,"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/_subclasses/fake_tensor.py,patch_14,"-        shape_env = self.shape_env
","+        shape_env: Optional[ShapeEnv] = self.shape_env
","--- a/torch/testing/_internal/common_utils/torch_dispatch_mode.py
+++ b/torch/testing/_internal/common_utils/torch_dispatch_mode.py
@@ -34,6 +34,9 @@
 print_specializations = False
 # lists, and incorrectly issue guards.
 inject_EVALUATE_EXPR_flip_equality_TESTING_ONLY = False
 
+# [@compile_ignored: debug] Validate that ShapeEnv's version key is updated correctly
+validate_shape_env_verison_key = False
+
 from torch.utils._config_module import install_config_module

 install_config_module(sys.modules[__name__])"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/csrc/autograd/init.cpp,patch_0,"-          std::make_shared<c10::SafePyObject>(arg, getPyInterpreter()),
-          std::make_shared<c10::SafePyObject>(arg, getPyInterpreter()));
","+          std::make_shared<c10::impl::PyObject_TorchDispatchMode>(
+              arg, getPyInterpreter()),
+          std::make_shared<c10::impl::PyObject_TorchDispatchMode>(
+              arg, getPyInterpreter()));
","@@ -1569,7 +1615,14 @@
     _set_gpu_runtime_env()
     from torch.utils import cpp_extension
     
-    macros = vec_isa.build_macro() if vec_isa != invalid_vec_isa else """"
+    # Remove below in the further
+    # macros = ""-D {}"".format(vec_isa.build_macro()) if vec_isa != invalid_vec_isa else """"
+    macros = """"
+    if vec_isa != invalid_vec_isa:
+        for x in vec_isa.build_macro():
+            macros_def = f""-D{x} ""
+            macros += macros_def
+
     build_arch_flags = """"
     if sys.platform == ""linux"" and (
         include_pytorch"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/csrc/autograd/init.cpp,patch_1,"-      std::make_shared<c10::SafePyObject>(mode, getPyInterpreter()), mode_key);
","+      std::make_shared<c10::impl::PyObject_TorchDispatchMode>(
+          mode, getPyInterpreter()),
+      mode_key);
","@@ -422,6 +424,69 @@ static PyMethodDef TensorGuards_methods[] = {

 static PyTypeObject TensorGuardsType = {PyVarObject_HEAD_INIT(nullptr, 0)};
 
+struct GlobalStateGuard {
+  PyObject_HEAD;
+
+  inline void init() {
+    auto& ctx = at::globalContext();
+    _grad_mode = at::GradMode::is_enabled();
+    _torch_function = torch::torch_function_enabled();
+    _deterministic_algorithms = ctx.deterministicAlgorithms();
+    _allow_tf32 = ctx.allowTF32CuBLAS();
+    _allow_fp16_reduce = ctx.allowFP16ReductionCuBLAS();
+    _allow_bf16_reduce = ctx.allowBF16ReductionCuBLAS();
+    _num_threads = at::get_num_threads();
+  }
+
+  inline bool check() {
+    auto& ctx = at::globalContext();
+    return (
+        _grad_mode == at::GradMode::is_enabled() &&
+        _torch_function == torch::torch_function_enabled() &&
+        _deterministic_algorithms == ctx.deterministicAlgorithms() &&
+        _allow_tf32 == ctx.allowTF32CuBLAS() &&
+        _allow_fp16_reduce == ctx.allowFP16ReductionCuBLAS() &&
+        _allow_bf16_reduce == ctx.allowBF16ReductionCuBLAS() &&
+        _num_threads == at::get_num_threads());
+  }
+
+  bool _grad_mode;
+  bool _torch_function;
+  bool _deterministic_algorithms;
+  bool _allow_tf32;
+  bool _allow_fp16_reduce;
+  bool _allow_bf16_reduce;
+  int _num_threads;
+  // TODO(jansel): we should guard on more state as inductor starts using it
+};
+
+int GlobalStateGuard_init(
+    GlobalStateGuard* self,
+    PyObject* args,
+    PyObject* kwargs) {
+  self->init();
+  return 0;
+}
+
+PyObject* GlobalStateGuard_check(
+    GlobalStateGuard* self,
+    PyObject* args,
+    PyObject* kwargs) {
+  if (self->check()) {
+    Py_RETURN_TRUE;
+  } else {
+    Py_RETURN_FALSE;
+  }
+}
+
+static PyMethodDef GlobalStateGuard_methods[] = {
+    {""check"",
+     (PyCFunction)(void*)GlobalStateGuard_check,
+     METH_NOARGS,
+     ""Return true if global state was the same as at creation time""},
+    {nullptr}};
+static PyTypeObject GlobalStateGuardType = {PyVarObject_HEAD_INIT(nullptr, 0)};
+
 static PyObject* check_type_id(PyObject* dummy, PyObject* args) {
   // faster `lambda obj, expected: id(type(obj)) == expected`
   PyObject* obj = nullptr;"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/csrc/utils/torch_dispatch_mode.h,patch_0,"-  const std::shared_ptr<c10::SafePyObject>& get_cur_mode() {
-  std::shared_ptr<at::SafePyObject> saved_mode_;
","+  const std::shared_ptr<c10::impl::PyObject_TorchDispatchMode>& get_cur_mode() {
+  std::shared_ptr<c10::impl::PyObject_TorchDispatchMode> saved_mode_;
","@@ -128,32 +139,134 @@ void* DispatchStubImpl::get_call_ptr(
           , ZVECTOR
 #endif
         );
-        cpu_dispatch_ptr.store(fptr, std::memory_order_relaxed);
+        if (!std::holds_alternative<ErrorType>(result)) {
+          cpu_dispatch_ptr.store(fptr, std::memory_order_relaxed);
+        }
+      return result;
       }
-      return fptr;
+      return DispatchResult(fptr);
     }
 
     case DeviceType::CUDA:
-      TORCH_INTERNAL_ASSERT(cuda_dispatch_ptr, ""DispatchStub: missing CUDA kernel"");
-      return cuda_dispatch_ptr;
+      return cuda_dispatch_ptr != nullptr ? DispatchResult(cuda_dispatch_ptr) : ErrorType::MissingDeviceKernel;
 
     case DeviceType::HIP:
-      TORCH_INTERNAL_ASSERT(hip_dispatch_ptr, ""DispatchStub: missing HIP kernel"");
-      return hip_dispatch_ptr;
+      return hip_dispatch_ptr != nullptr ? DispatchResult(hip_dispatch_ptr) : ErrorType::MissingDeviceKernel;
 
 #if defined(USE_MPS)
     case DeviceType::MPS:
-      TORCH_INTERNAL_ASSERT(mps_dispatch_ptr, ""DispatchStub: missing MPS kernel"");
-      return mps_dispatch_ptr;
+      return mps_dispatch_ptr != nullptr ? DispatchResult(mps_dispatch_ptr) : ErrorType::MissingDeviceKernel;
 #endif
 
     case DeviceType::PrivateUse1:
-      TORCH_INTERNAL_ASSERT(privateuse1_dispatch_ptr, ""DispatchStub: missing PrivateUse1 kernel"");
-      return privateuse1_dispatch_ptr;
+      return privateuse1_dispatch_ptr != nullptr ? DispatchResult(privateuse1_dispatch_ptr) : ErrorType::MissingDeviceKernel;
 
     default:
-      AT_ERROR(""DispatchStub: unsupported device type"", device_type);
+      TORCH_INTERNAL_ASSERT(false, ""An unexpected device type was provided "", device_type);
+      return ErrorType::DeviceNotSupported;
+  }
+}
+
+void* DispatchStubImpl::get_call_ptr(
+  const DeviceType device_type
+  , void *DEFAULT
+#ifdef HAVE_AVX512_CPU_DEFINITION
+  , void *AVX512
+#endif
+#ifdef HAVE_AVX2_CPU_DEFINITION
+  , void *AVX2
+#endif
+#ifdef HAVE_VSX_CPU_DEFINITION
+  , void *VSX
+#endif
+#ifdef HAVE_ZVECTOR_CPU_DEFINITION
+  , void *ZVECTOR
+#endif
+) {
+
+  auto result = try_get_call_ptr(
+      device_type,
+      DEFAULT
+#ifdef HAVE_AVX512_CPU_DEFINITION
+      ,
+      AVX512
+#endif
+#ifdef HAVE_AVX2_CPU_DEFINITION
+      ,
+      AVX2
+#endif
+#ifdef HAVE_VSX_CPU_DEFINITION
+      ,
+      VSX
+#endif
+#ifdef HAVE_ZVECTOR_CPU_DEFINITION
+      ,
+      ZVECTOR
+#endif
+  );
+  if (std::holds_alternative<ErrorType>(result)) {
+    auto error = std::get<ErrorType>(result);
+    switch (error) {
+      case ErrorType::MissingDeviceKernel:
+        TORCH_INTERNAL_ASSERT(
+            false, ""DispatchStub: missing kernel for "", device_type);
+        return nullptr;
+      case ErrorType::DeviceNotSupported:
+        AT_ERROR(""DispatchStub: unsupported device type"", device_type);
+    }
+  }
+
+  void* fptr = std::get<void*>(result);
+  return fptr;
+}
+
+DispatchResult DispatchStubImpl::try_choose_cpu_impl(
+    void *DEFAULT
+#ifdef HAVE_AVX512_CPU_DEFINITION
+    , void *AVX512
+#endif
+#ifdef HAVE_AVX2_CPU_DEFINITION
+    , void *AVX2
+#endif
+#ifdef HAVE_VSX_CPU_DEFINITION
+    , void *VSX
+#endif
+#ifdef HAVE_ZVECTOR_CPU_DEFINITION
+    , void *ZVECTOR
+#endif
+  ){
+
+  auto capability = static_cast<int>(get_cpu_capability());
+  (void)capability;
+#ifdef HAVE_AVX512_CPU_DEFINITION
+  if (capability >= static_cast<int>(CPUCapability::AVX512)) {
+    // Quantization kernels have also been disabled on Windows
+    // for AVX512 because some of their tests are flaky on Windows.
+    // Ideally, we should have AVX512 kernels for all kernels.
+    if (C10_UNLIKELY(!AVX512)) {
+      // dispatch to AVX2, since the AVX512 kernel is missing
+      return AVX2 != nullptr ? DispatchResult(AVX2) : ErrorType::MissingDeviceKernel;
+    } else {
+      return DispatchResult(AVX512);
+    }
+  }
+#endif
+#ifdef HAVE_AVX2_CPU_DEFINITION
+  if (capability >= static_cast<int>(CPUCapability::AVX2)) {
+    return AVX2 != nullptr ? DispatchResult(AVX2) : ErrorType::MissingDeviceKernel;
+  }
+#endif
+#ifdef HAVE_VSX_CPU_DEFINITION
+  if (capability >= static_cast<int>(CPUCapability::VSX)) {
+    return VSX != nullptr ? DispatchResult(VSX) : ErrorType::MissingDeviceKernel;
+  }
+#endif
+#ifdef HAVE_ZVECTOR_CPU_DEFINITION
+  if (capability >= static_cast<int>(CPUCapability::ZVECTOR)) {
+    return ZVECTOR != nullptr ? DispatchResult(ZVECTOR) : ErrorType::MissingDeviceKernel;
+  }
+#endif
+  return DEFAULT != nullptr ? DispatchResult(DEFAULT) : ErrorType::MissingDeviceKernel;
 }"
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,YES,torch/utils/_python_dispatch.py,patch_0,"-def _push_mode(mode):
","+def _push_mode(mode: TorchDispatchMode):
","@@ -6,7 +6,7 @@ def _push_mode(mode: TorchDispatchMode):
  def _shallow_copy_dispatch_modules(stack: List[DispatchModule]) -> List[DispatchModule]:
      return [SafeTorchDispatchModule(m.module, m.dispatch_mode) for m in stack]
  
- class SafePyObject(metaclass=ABCMeta):
+ class SafePyObjectT(typing.Generic[T], metaclass=ABCMeta):
      pass
  
  class SafeModule(SafePyObject, metaclass=ABCMeta):"
0,https://github.com/PyTorch/PyTorch/commit/4240304da4ddc42335b0219bae11f072ca240fe5,YES,torch/distributed/elastic/multiprocessing/errors/__init__.py,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/9a95b4bc7bbc66f0513d01e1c2af8e1ead5c1077,YES,torch/distributed/_tensor/op_schema.py,patch_0,"-    static_argnum: int = -1
","+    static_argnum: int = 100
","@@ -29,6 +29,9 @@ class ConstantValueMap {
   static void SetAllGraphInputsStatic(bool all_static);
   static c10::optional<bool> GetAllGraphInputsStatic();
 
+  static void SetAllGraphInputsReliableComputed(bool computed);
+  static bool GetAllGraphInputsReliableComputed();
+
   static void SetShape(
       const std::string& tensorName,
       const c10::SymbolicShape& shapeValue);"
0,https://github.com/PyTorch/PyTorch/commit/9a95b4bc7bbc66f0513d01e1c2af8e1ead5c1077,YES,torch/distributed/_tensor/op_schema.py,patch_1,"-        op_arg_type = self.op._schema.arguments[arg_idx].type
-        is_tensor = isinstance(op_arg_type, torch.TensorType)
-        is_list_like = isinstance(op_arg_type, torch.ListType)
-        if not is_list_like:
-        elem_type = op_arg_type.getElementType()
-        return isinstance(elem_type, torch.TensorType) or (
-            isinstance(elem_type, torch.OptionalType)
-            and isinstance(elem_type.getElementType(), torch.TensorType)
-        )
","+        arg = self.args_schema[arg_idx]
+        is_tensor = isinstance(arg, DTensorSpec)
+        if not isinstance(arg, list):
+        return all(isinstance(e, DTensorSpec) or e is None for e in arg)
","@@ -476,8 +476,8 @@ def _if_scalar_type_as(self, tensor):
 
 
 @_beartype.beartype
-def _is_none(x: _C.Value) -> bool:
-    return x.node().mustBeNone()
+def _is_none(x: Any) -> bool:
+    return x is None or (x.node().mustBeNone() if isinstance(x, _C.Value) else False)
 
 
 @_beartype.beartype"
0,https://github.com/PyTorch/PyTorch/commit/56ef200c2dc8a1f1e269861b7a6e02e99d3b56a1,YES,aten/src/ATen/core/boxing/KernelFunction.h,patch_0,"-    std::is_same<c10::SymInt, std::decay_t<T>>,
-    std::is_same<c10::SymIntArrayRef, std::decay_t<T>>,
-    std::is_same<at::OptionalSymIntArrayRef, std::decay_t<T>>,
-    std::is_same<c10::optional<c10::SymInt>, std::decay_t<T>>
",No
0,https://github.com/PyTorch/PyTorch/commit/56ef200c2dc8a1f1e269861b7a6e02e99d3b56a1,YES,aten/src/ATen/core/boxing/KernelFunction.h,patch_1,,No
0,https://github.com/PyTorch/PyTorch/commit/56ef200c2dc8a1f1e269861b7a6e02e99d3b56a1,YES,aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h,patch_0,"-  // The following specialisations of assert_is_valid_input_type are technically not
-  // necessary since we would hit the base case and show an error message
-  // there if they didn't exist, but we can show a better error message
-  // in some common error scenarios.
",No
0,https://github.com/PyTorch/PyTorch/commit/56ef200c2dc8a1f1e269861b7a6e02e99d3b56a1,YES,aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h,patch_1,,No
0,https://github.com/PyTorch/PyTorch/commit/56ef200c2dc8a1f1e269861b7a6e02e99d3b56a1,YES,aten/src/ATen/core/dispatch/OperatorEntry.h,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/7a4e4511845dbeefe4d16c321b2a93ac72b76d93,YES,torch/_dynamo/variables/torch.py,patch_0,"-                any(has_torch_function(a) for a in args),
","+            elems = (
+                args[0].unpack_var_sequence(tx)
+                if len(args) == 1 and isinstance(args[0], TupleVariable)
+                else args
+            )
+                any(has_torch_function(x) for x in elems),
","@@ -1412,10 +1412,10 @@ class WrapperCodeGen(CodeGen):
     def enter_context(self, ctx):
         self.lines.append(LineContext(ctx))
 
-    def val_to_cpp_arg_str(self, type_, val) -> str:
+    def val_to_cpp_arg_str(self, val, type_) -> str:
         raise NotImplementedError
 
-    def val_to_arg_str(self, s):
+    def val_to_arg_str(self, s, type_=None):
         from torch.utils._triton import dtype_to_string, has_triton_package
 
         if has_triton_package():
-            elems = (
-                args[0].unpack_var_sequence(tx)
-                if len(args) == 1 and isinstance(args[0], TupleVariable)
-                else args
-            )
+            elems = (
+                x.unpack_var_sequence(tx) if isinstance(x, TupleVariable) else x
+                for x in s
+            )"
0,https://github.com/PyTorch/PyTorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f,NO,torch/_inductor/compile_fx.py,patch_0,"-import itertools
","+from itertools import count
","@@ -5,7 +5,6 @@ import itertools
 import logging
 import operator
 import re
-from collections import namedtuple
+from itertools import count
 from typing import (
     Any,"
0,https://github.com/PyTorch/PyTorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f,NO,torch/_inductor/compile_fx.py,patch_1,"-_graph_counter = itertools.count(0)
","+_graph_counter = count(0)
","@@ -1,4 +1,4 @@
-_graph_counter = itertools.count(0)
+_graph_counter = count(0)"
0,https://github.com/PyTorch/PyTorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f,NO,torch/_inductor/compile_fx.py,patch_2,,No
0,https://github.com/PyTorch/PyTorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f,NO,torch/_inductor/exc.py,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f,NO,torch/_inductor/graph.py,patch_0,"-from . import config, ir, metrics
","+from . import config, ir
","@@ -84,6 +84,7 @@ from .source import (
     GlobalStateSource,
     GlobalWeakRefSource,
     GradSource,
+    is_unspecialized_builtin_nnmodule_attr,
     LocalSource,
     NNModuleSource,
     NumpyTensorSource,"
0,https://github.com/PyTorch/PyTorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f,NO,torch/_inductor/graph.py,patch_1,,No
0,https://github.com/PyTorch/PyTorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f,NO,torch/_inductor/graph.py,patch_2,"-    def disable_cpp_wrapper(self, cond):
-        metrics.disable_cpp_wrapper += 1
-        self.cpp_wrapper = False
-        log.debug(""Set cpp_wrapper to False due to %s"", cond)
-
",,"@@ -0,0 +1,1 @@
+metrics.disable_cpp_wrapper += 1"
0,https://github.com/PyTorch/PyTorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f,NO,torch/_inductor/graph.py,patch_3,,No
0,https://github.com/PyTorch/PyTorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f,NO,torch/_inductor/graph.py,patch_4,"-    def check_cpp_codegen_disabled(self):
-            self.disable_cpp_wrapper(""cpp codegen disabled"")
-    def check_platform(self):
-            self.disable_cpp_wrapper(""platform not linux"")
-    def check_input_for_cpp_buffer(self):
","+    def validate_can_generate_cpp_wrapper(self):
+            raise CppWrapperCodeGenError(""C++ codegen is disabled"")
+            raise CppWrapperCodeGenError(f""Unsupported platform {sys.platform}"")
","@@ -1588,6 +1597,14 @@ class CppWrapperCodeGen(WrapperCodeGen):
             args = [name, size, stride, offset]
             return f""reinterpret_tensor({', '.join(args)})""
 
+    def codegen_device_copy(self, src, dst):
+        if config.aot_inductor.abi_compatible:
+            self.writeline(
+                f""AOTI_TORCH_ERROR_CODE_CHECK(aoti_torch_tensor_copy_({src}, {dst}));""
+            )
+        else:
+            self.writeline(f""{dst}.copy_({src});"")
+
     def generate_extern_kernel_args_decl_if_needed(
         self, op_overload, raw_args, output_args
     ):"
0,https://github.com/PyTorch/PyTorch/commit/8209bbbd061c563cd6461c883afa44b0fc1b8f9f,NO,torch/_inductor/graph.py,patch_5,"-                self.disable_cpp_wrapper(""unsupported inputs dtype"")
-
-    @contextmanager
-    def set_current_node(self, node: torch.fx.Node):
-        old = self.current_node
-        try:
-            self.current_node = node
-            yield
-        finally:
-            self.current_node = old
-
-    def check_cpp_wrapper(self):
-        self.check_cpp_codegen_disabled()
-        self.check_platform()
-        self.check_input_for_cpp_buffer()
-            self.check_cpp_wrapper()
-            # Re-check self.cpp_wrapper because it might be disabled due to failed checking
-            if self.cuda:
-                assert self.cpp_wrapper, ""CudaWrapperCodeGen hit unsupported case""
-
-            if self.cpp_wrapper:
-                self.wrapper_code = (
-                    CudaWrapperCodeGen() if self.cuda else CppWrapperCodeGen()
-                )
-                return
","+                raise CppWrapperCodeGenError(f""Unsupported input dtype {dtype}"")
+            self.validate_can_generate_cpp_wrapper()
+            self.wrapper_code = (
+                CudaWrapperCodeGen() if self.cuda else CppWrapperCodeGen()
+            )
+            return
","@@ -973,7 +975,9 @@ def compile_fx(
                 ""triton.unique_kernel_names"": True,
             }
-        ), V.set_real_inputs(example_inputs_):
+        ), V.set_real_inputs(
+            example_inputs_
+        ):  # type: ignore[call-arg]
             return compile_fx(
                 model_,
                 example_inputs_, 
-                self.disable_cpp_wrapper(""unsupported inputs dtype"")
+                raise CppWrapperCodeGenError(f""Unsupported input dtype {dtype}"")
+            self.validate_can_generate_cpp_wrapper()
             self.wrapper_code = (
                 CudaWrapperCodeGen() if self.cuda else CppWrapperCodeGen()
             )
             return"
0,https://github.com/PyTorch/PyTorch/commit/73ba226d986711e2474da52ec67bf9719c902611,NO,torch/_inductor/scheduler.py,patch_0,"-        again = True  # repeat until a fixed point
-        while again:
-            updated_nodes = []
-            for node in self.nodes:
-                def can_eliminate_user(user: NodeUser) -> bool:
-                    return user.is_weak or user.get_name() in V.graph.removed_buffers
-                can_eliminate = not node.has_side_effects() and all(
-                    can_eliminate_user(u) for u in node.users
-                )
-                if not can_eliminate:
-                    updated_nodes.append(node)
-                else:
-                    # dead code
-                    log.debug(""removed dead node: %s"", node.get_name())
-                    V.graph.removed_buffers.add(node.get_name())
-            again = len(self.nodes) > len(updated_nodes)
-            self.nodes = updated_nodes
","+        # self.nodes is in topological order, so by iterating in reverse order
+        # we have visited (and potentially removed) all users before visiting a
+        # given node.
+        updated_nodes = []
+        for node in reversed(self.nodes):
+            def can_eliminate_user(user: NodeUser) -> bool:
+                return user.is_weak or user.get_name() in V.graph.removed_buffers
+            can_eliminate = not node.has_side_effects() and all(
+                can_eliminate_user(u) for u in node.users
+            )
+            if not can_eliminate:
+                updated_nodes.append(node)
+            else:
+                # dead code
+                log.debug(""removed dead node: %s"", node.get_name())
+                V.graph.removed_buffers.add(node.get_name())
+        self.nodes = list(reversed(updated_nodes))
","@@ -1574,12 +1573,17 @@ class Graph:
                             m_itr = new_m_itr
 
     @compatibility(is_backward_compatible=True)
-    def eliminate_dead_code(self):
+    def eliminate_dead_code(self, is_impure_node: Optional[Callable[[Node], bool]] = None):
         """"""
         Remove all dead code from the graph, based on each node's number of
         users, and whether the nodes have any side effects. The graph must be
         topologically sorted before calling.
 
+        Args:
+            is_impure_node (Optional[Callable[[Node], bool]]): A function that returns
+            whether a node is impure. If this is None, then the default behavior is to
+            use Node.is_impure.
+
         Returns:
           bool: Whether the graph was changed as a result of the pass.
 
-        again = True  # repeat until a fixed point
-        while again:
-            updated_nodes = []
-            for node in self.nodes:
-                def can_eliminate_user(user: NodeUser) -> bool:
-                    return user.is_weak or user.get_name() in V.graph.removed_buffers
-                can_eliminate = not node.has_side_effects() and all(
-                    can_eliminate_user(u) for u in node.users
-                )
-                if not can_eliminate:
-                    updated_nodes.append(node)
-                else:
-                    # dead code
-                    log.debug(""removed dead node: %s"", node.get_name())
-                    V.graph.removed_buffers.add(node.get_name())
-            again = len(self.nodes) > len(updated_nodes)
-            self.nodes = updated_nodes
+        # self.nodes is in topological order, so by iterating in reverse order
+        # we have visited (and potentially removed) all users before visiting a
+        # given node.
+        updated_nodes = []
+        for node in reversed(self.nodes):
+            def can_eliminate_user(user: NodeUser) -> bool:
+                return user.is_weak or user.get_name() in V.graph.removed_buffers
+            can_eliminate = not node.has_side_effects() and all(
+                can_eliminate_user(u) for u in node.users
+            )
+            if not can_eliminate:
+                updated_nodes.append(node)
+            else:
+                # dead code
+                log.debug(""removed dead node: %s"", node.get_name())
+                V.graph.removed_buffers.add(node.get_name())
+        self.nodes = list(reversed(updated_nodes))"
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,YES,torch/_ops.py,patch_0,"-# it should always have two keys with
","+# it should always have three keys with
+#
+# SchemaCheckMode is separate from the other 2,
+# and is only valid when the stack is empty.
+# SchemaCheckMode is for testing purposes, and
+# is meant to run in eager mode on concrete inputs,
+# checking for incorrect schemas in regards to
+# aliasing or mutating ops.
+        self._schema_check_mode = None
","['-# it should always have two keys with\n',
 '+# it should always have three keys with\n+# SchemaCheckMode is separate from the other 2,\n+# and is only valid when the stack is empty.\n+# SchemaCheckMode is for testing purposes, and\n+# is meant to run in eager mode on concrete inputs,\n+# checking for incorrect schemas in regards to\n+# aliasing or mutating ops.\n+        self._schema_check_mode = None\n']"
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,YES,torch/_ops.py,patch_1,"-        return len([i for i in self.__infra_modes if i is not None])
-def unset_mode_pre_dispatch(mode_key):
-    assert mode_key in (
-        else:
","+        return len([i for i in self.__infra_modes if i is not None]) + int(
+            self._schema_check_mode is not None
+        )
+def unset_mode_pre_dispatch(mode_key, schema_check=False):
+    assert mode_key is None or mode_key in (
+    if schema_check:
+        assert mode_key is None
+        elif mode_key == torch._C._TorchDispatchModeKey.FUNCTIONAL:
+        else:
+            current_mode = mode_stack_state_for_pre_dispatch()._schema_check_mode
+            mode_stack_state_for_pre_dispatch()._schema_check_mode = None
+            return current_mode
","@@ -726,6 +724,27 @@ def multi_device_op_out(fake_mode, func, *args, **kwargs):
     return new_kwargs[""input""]
 
 
+@register_op_impl(aten.index_put.default)
+@register_op_impl(aten.index_put_.default)
+def index_put_impl(fake_mode, func, *args, **kwargs):
+    _, new_kwargs = normalize_function(
+        func, args=args, kwargs=kwargs, normalize_to_only_use_kwargs=True
+    )
+
+    values = new_kwargs[""values""]
+    self_device = new_kwargs[""input""].fake_device
+    torch._check(
+        self_device == values.fake_device or (values.ndim == 0 and values.numel() == 1),
+        lambda: f""Mismatching {func} device between self ({self_device}) and values ({values.device})"",
+    )
+
+    out = run_and_return_new_tensor_of_input_device(fake_mode, func, args, kwargs)
+    if func is aten.index_put_.default:
+        return new_kwargs[""input""]
+    else:
+        return out
+
+
 @register_op_impl(lambda fn: fn in _device_not_kwarg_ops)
 def nyi(fake_mode, func, *args, **kwargs):
     assert func not in _device_not_kwarg_ops, f""NYI: {func}"""
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,YES,torch/_ops.py,patch_2,"-    assert isinstance(mode, (FunctionalTensorMode, ProxyTorchDispatchMode))
-    if isinstance(mode, FunctionalTensorMode):
","+    from torch._subclasses.schema_check_mode import SchemaCheckMode
+    assert isinstance(
+        mode,
+        (
+            FunctionalTensorMode,
+            ProxyTorchDispatchMode,
+            SchemaCheckMode,
+        ),
+    )
+    if isinstance(mode, SchemaCheckMode):
+        current_mode = mode_stack_state_for_pre_dispatch()._schema_check_mode
+        if previous_mode_stack_len > 0:
+            raise AssertionError(
+                ""SchemaCheckMode for pre-dispatch must be used exclusively, found other modes on the stack""
+            )
+        mode_stack_state_for_pre_dispatch()._schema_check_mode = mode
+    elif isinstance(mode, FunctionalTensorMode):
","        --- a/torch/csrc/utils/dispatch/legacy_mode.cpp
        +++ b/torch/csrc/utils/dispatch/legacy_mode.cpp
        @@ -196,6 +196,17 @@ dynamic shape operator: _torch_testing.numpy_nonzero.default
                 res = torch._library.utils.is_functional_schema(schema)
                 self.assertEqual(res, expected)
         
        +    def test_incorrect_schema_types(self):
        +        with torch.library._scoped_library(""mylib"", ""FRAGMENT"") as lib:
        +            with self.assertRaisesRegex(RuntimeError, ""unknown type specifier""):
        +                lib.define(""foo12(Tensor a) -> asdfasdf"")
        +            with self.assertRaisesRegex(RuntimeError, ""unknown type specifier""):
        +                lib.define(""foo12(asdf a) -> Tensor"")
        +            with self.assertRaisesRegex(RuntimeError, ""Use `SymInt` or `int`""):
        +                lib.define(""foo12(int64_t a) -> Tensor"")
        +            with self.assertRaisesRegex(RuntimeError, ""Use `float`""):
        +                lib.define(""foo12(double a) -> Tensor"")
        +
         def test_is_tensorlist_like_type(self):
             tensorlists = [
                 # Tensor[]"
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,YES,torch/_ops.py,patch_3,"-
","+    if mode_stack._schema_check_mode is not None:
+        return unset_mode_pre_dispatch(None, schema_check=True)
","@@ -0,0 +1,5 @@
+    if mode_stack._schema_check_mode is not None:
+        unset_mode_pre_dispatch(None, schema_check=True)
+        return
+    return unset_mode_pre_dispatch(None, schema_check=True)
+"
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,YES,torch/_ops.py,patch_4,"-    return mode_stack_state_for_pre_dispatch().get(1)
-    stack_len = mode_stack_state_for_pre_dispatch().count()
-    if stack_len == 2:
-        return mode_stack_state_for_pre_dispatch().get(1)
-    if stack_len == 1:
-        return (
-            mode_stack_state_for_pre_dispatch().get(1)
-            if mode_stack_state_for_pre_dispatch().get(1) is not None
-            else mode_stack_state_for_pre_dispatch().get(0)
-        )
","+    else:
+        return mode_stack_state_for_pre_dispatch().get(1)
+    if mode_stack_state_for_pre_dispatch()._schema_check_mode is not None:
+        return mode_stack_state_for_pre_dispatch()._schema_check_mode
+    else:
+        stack_len = mode_stack_state_for_pre_dispatch().count()
+        if stack_len == 2:
+            return mode_stack_state_for_pre_dispatch().get(1)
+        if stack_len == 1:
+            return (
+                mode_stack_state_for_pre_dispatch().get(1)
+                if mode_stack_state_for_pre_dispatch().get(1) is not None
+                else mode_stack_state_for_pre_dispatch().get(0)
+            )
","@@ -145,7 +145,20 @@ class FSDP:
         to the module and freeing the unsharded parameters if needed. This
         method is *not* recursive.
         """"""
-        if (state := _get_module_fsdp_state(cast(nn.Module, self))) is None:
-            raise AssertionError(f""No FSDP state found on {self}"")
+        state = self._get_fsdp_state()
         if fsdp_param_group := state._fsdp_param_group:
             fsdp_param_group.reshard()
+
+    def set_is_last_backward(self, is_last_backward: bool) -> None:
+        """"""
+        Sets whether the next backward is the last one, meaning that FSDP
+        should wait for gradient reduction to finish and clear internal data
+        structures used for explicit prefetching.
+        """"""
+        state = self._get_fsdp_state()
+        state._state_ctx.is_last_backward = is_last_backward
+
+    def _get_fsdp_state(self) -> FSDPState:
+        if (state := _get_module_fsdp_state(cast(nn.Module, self))) is None:
+            raise AssertionError(f""No FSDP state found on {self}"")
+        return state
-
-    return mode_stack_state_for_pre_dispatch().get(1)
-    stack_len = mode_stack_state_for_pre_dispatch().count()
-    if stack_len == 2:
-        return mode_stack_state_for_pre_dispatch().get(1)
-    if stack_len == 1:
-        return (
-            mode_stack_state_for_pre_dispatch().get(1)
-            if mode_stack_state_for_pre_dispatch().get(1) is not None
-            else mode_stack_state_for_pre_dispatch().get(0)
-        )
+    else:
+        return mode_stack_state_for_pre_dispatch().get(1)
+    if mode_stack_state_for_pre_dispatch()._schema_check_mode is not None:
+        return mode_stack_state_for_pre_dispatch()._schema_check_mode
+    else:
+        stack_len = mode_stack_state_for_pre_dispatch().count()
+        if stack_len == 2:
+            return mode_stack_state_for_pre_dispatch().get(1)
+        if stack_len == 1:
+            return (
+                mode_stack_state_for_pre_dispatch().get(1)
+                if mode_stack_state_for_pre_dispatch().get(1) is not None
+                else mode_stack_state_for_pre_dispatch().get(0)
+            )"
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,YES,torch/_subclasses/schema_check_mode.py,patch_0,"-from torch.testing._internal.jit_utils import clone_inputs
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,YES,torch/_subclasses/schema_check_mode.py,patch_1,,"+# move these 2 functions here to avoid numpy dependency in testing/_internal/common_utils.py
+
+
+def is_iterable_of_tensors(iterable):
+    # Tensor itself is iterable so we check this first
+    if isinstance(iterable, torch.Tensor):
+        return False
+    try:
+        if len(iterable) == 0:
+            return False
+        for t in iter(iterable):
+            if not isinstance(t, torch.Tensor):
+                return False
+    except TypeError as te:
+        return False
+    return True
+
+
+def clone_inputs(args):
+    inputs = []
+
+    for arg in args:
+        if isinstance(arg, torch.Tensor):
+            inputs.append(arg.detach().clone())
+        elif is_iterable_of_tensors(arg):
+            inputs.append([t.detach().clone() for t in arg])
+        else:
+            inputs.append(arg)
+
+    return inputs
+
+
","@@ -161,13 +161,13 @@ def is_concrete_bool(a: Union[bool, SymBool]):
 def tensor_has_hints(t):
     return all(has_hint(s) for s in t.size())
 
-def _iterate_exprs(val: Union[SymInt, torch.Tensor]) -> Iterable[sympy.Basic]:
+def _iterate_exprs(val: Union[SymInt, torch.Tensor]) -> Iterable[sympy.Expr]:
     if isinstance(val, SymTypes):
         # This allow applies to the jagged layout NestedTensor case as
         # singleton ints are not symbolic
         if is_symbolic(val):
             yield val.node.expr
-    elif isinstance(val, sympy.Basic):
+    elif isinstance(val, sympy.Expr):
         yield val
     elif isinstance(val, (int, float, bool)):
         pass"
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,YES,torch/utils/_python_dispatch.py,patch_0,,"+    from torch._subclasses.schema_check_mode import SchemaCheckMode
","@@ -35,6 +40,7 @@ struct TORCH_API SchemaTypeParser {
   bool complete_tensor_types;
   Lexer& L;
   size_t next_id = 0;
+  bool allow_typevars_;
 };
 } // namespace jit
 } // namespace torch"
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,YES,torch/utils/_python_dispatch.py,patch_1,,"+    has_schema_check_mode_in_pre_dispatch = False
+        if isinstance(i, SchemaCheckMode):
+            has_schema_check_mode_in_pre_dispatch = True
","@@ -2054,3 +2059,28 @@
         [fqn for fqn, _ in _named_parameters_with_duplicates(model)],
         fqn_to_param_info,
     )
+
+
+@no_type_check
+def _set_optim_use_dtensor(
+    module: nn.Module,
+    state_dict_settings: StateDictSettings,
+) -> None:
+    # If device_mesh is passed in when initalizing FSDP, we automatically turn the
+    # _use_dtensor flag to be true for ShardedOptimStateDictConfig() if state_dict_type
+    # has to be set to SHARDED_STATE_DICT.
+    if getattr(module, ""device_mesh"", None):
+        state_dict_type = state_dict_settings.state_dict_type
+        if state_dict_type == StateDictType.LOCAL_STATE_DICT:
+            raise RuntimeError(
+                ""Found state_dict_type LOCAL_STATE_DICT."",
+                ""DeviceMesh is not compatible with LOCAL_STATE_DICT."",
+                ""Please set state_dict_type to SHARDED_STATE_DICT to get DTensor state_dict."",
+            )
+        elif state_dict_type == StateDictType.FULL_STATE_DICT:
+            logger.warning(
+                ""Found both state_dict_type FULL_STATE_DICT and device_mesh. ""  # noqa: G004
+                ""Please set state_dict_type to SHARDED_STATE_DICT to get DTensor state_dict.""
+            )
+        else:
+            state_dict_settings.optim_state_dict_config._use_dtensor = True"
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,YES,torch/utils/_python_dispatch.py,patch_2,,"+        if (
+            isinstance(old, SchemaCheckMode)
+            and has_schema_check_mode_in_pre_dispatch
+        ):
+            raise AssertionError(
+                ""Can't have SchemaCheckMode available both in PreDispatch and Python Key""
+            )
","@@ -279,16 +279,9 @@ def _verify_state_dict(
     optim_state_dict: OptimizerStateType,
     info: _StateDictInfo,
 ) -> None:
-    # FSDP root must exist otherwise FSDP state_dict will be incorrect.
-    has_fsdp_root = False
     for module in info.fsdp_modules:
         fsdp_state = _get_module_fsdp_state_if_fully_sharded_module(module)
         assert fsdp_state is not None, ""Expected a fsdp_state with a fsdp module.""
-        if fsdp_state._is_root:
-            has_fsdp_root = True
-            break
-    if info.fsdp_modules and not has_fsdp_root:
-        raise RuntimeError(""The model has FSDP modules but no FSDP root module exists."")
 
     # Verify if the model_state_dict and optim_state_dict are valid. This API
     # should give the users an explicit error message to debug or report.
+    if (
+        isinstance(old, SchemaCheckMode)
+        and has_schema_check_mode_in_pre_dispatch
+    ):
+        raise AssertionError(
+            ""Can't have SchemaCheckMode available both in PreDispatch and Python Key""
+        )"
0,https://github.com/PyTorch/PyTorch/commit/d3e8b8bf47206c27b6c5fdc021f7c2c3a8009521,YES,aten/src/ATen/cuda/CUDAGeneratorImpl.cpp,patch_0,"-  // Ensures that the RNG state is not currently being captured.
-  at::cuda::assertNotCapturing(
-      ""Cannot unregister the state during capturing stage."");
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/e98135d1ad2f999fec649ecd21b35f3d5676be43,YES,torch/_functorch/_aot_autograd/runtime_wrappers.py,patch_0,"-            with torch.autograd._force_original_view_tracking(True):
","+
+            # It's possible to have trace_joint inside user specified with no_grad() region,
+            # if there is a nested with enable_grad(), that forces some outputs to require gradients.
+            # Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.
+            with torch.autograd._force_original_view_tracking(
+                True
+            ), torch.enable_grad():
","@@ -304,7 +304,13 @@ def _create_runtime_wrapper(
             for idx in indices_of_inps_to_detach:
                 if isinstance(args_[idx], torch.Tensor):
                     args_[idx] = args_[idx].detach()
-            with torch.autograd._force_original_view_tracking(True):
+
+            # It's possible to have trace_joint inside user specified with no_grad() region,
+            # if there is a nested with enable_grad(), that forces some outputs to require gradients.
+            # Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.
+            with torch.autograd._force_original_view_tracking(
+                True
+            ), torch.enable_grad():
                 all_outs = call_func_at_runtime_with_args(
                     compiled_fn, args_, disable_amp=disable_amp, steal_args=True
                 )"
0,https://github.com/PyTorch/PyTorch/commit/e98135d1ad2f999fec649ecd21b35f3d5676be43,YES,torch/_functorch/aot_autograd.py,patch_0,"-        needs_autograd = (
-            any(x.requires_grad for x in fake_flat_args if isinstance(x, Tensor))
-            and torch.is_grad_enabled()
","+        needs_autograd = any(
+            x.requires_grad for x in fake_flat_args if isinstance(x, Tensor)
","--- a/torch/csrc/jit/frontend/schema_inference.cpp
+++ b/torch/csrc/jit/frontend/schema_inference.cpp
@@ -299,11 +299,26 @@ static void autogradNotImplementedFallbackImpl(
       num_arguments);
 
   const bool any_requires_grad = !tensors_requiring_grad_on_stack.empty();
+  const bool has_out_arg = std::any_of(
+      schema.arguments().begin(),
+      schema.arguments().end(),
+      [](const c10::Argument& arg) { return arg.is_out(); });
 
   _foreach_tensor(
       [&](size_t _, size_t i, const at::Tensor& t) {
         if (schema.is_mutable({c10::SchemaArgType::input, i})) {
-          check_inplace(t, any_requires_grad);
+          if (has_out_arg) {
+            // Normally out argument overloads would not support any arguments
+            // that require grad. However, we loosen this check to maintain
+            // backward compatibility.
+            // See https://github.com/pytorch/pytorch/issues/120988
+            if (can_mutate_inplace(t, any_requires_grad) !=
+                can_mutate_inplace_result::success) {
+              throw_error_out_requires_grad(schema.name().c_str());
+            }
+          } else {
+            check_inplace(t, any_requires_grad);
+          }
         }
       },
       stack,"
0,https://github.com/PyTorch/PyTorch/commit/e98135d1ad2f999fec649ecd21b35f3d5676be43,YES,torch/_functorch/aot_autograd.py,patch_1,"-                    x.requires_grad for x in fw_metadata.output_info
","+                    x.requires_grad
+                    # view-type operations preserve requires_grad even in no_grad.
+                    # Do not count aliases of inputs with requires_grad as reason to make a training graph,
+                    # as AOTAutograd will perform view-replay to regenerate the view outputs at runtime,
+                    # setting their grad_fn properly.
+                    and not (
+                        x.output_type
+                        in (OutputType.alias_of_input, OutputType.is_input)
+                        and fw_metadata.input_info[x.base_idx].requires_grad
+                    )
+                    for x in fw_metadata.output_info
","@@ -593,7 +592,17 @@ def create_aot_dispatcher_function(
                 )
 
                 output_and_mutation_safe = not any(
-                    x.requires_grad for x in fw_metadata.output_info
+                    x.requires_grad
+                    # view-type operations preserve requires_grad even in no_grad.
+                    # Do not count aliases of inputs with requires_grad as reason to make a training graph,
+                    # as AOTAutograd will perform view-replay to regenerate the view outputs at runtime,
+                    # setting their grad_fn properly.
+                    and not (
+                        x.output_type
+                        in (OutputType.alias_of_input, OutputType.is_input)
+                        and fw_metadata.input_info[x.base_idx].requires_grad
+                    )
+                    for x in fw_metadata.output_info
                 ) and not any(
                     x.requires_grad
                     and x.mutates_data"
0,https://github.com/PyTorch/PyTorch/commit/e98135d1ad2f999fec649ecd21b35f3d5676be43,YES,torch/_functorch/partitioners.py,patch_0,"-    new_graph.output(output_values)
","+    new_graph.output(tuple(output_values))
","@@ -204,7 +204,7 @@ def _extract_graph_with_inputs_outputs(
             output_values.append(env[x])
         else:
             output_values.append(x)
-    new_graph.output(output_values)
+    new_graph.output(tuple(output_values))

     new_graph.eliminate_dead_code()
     new_graph.lint()"
0,https://github.com/PyTorch/PyTorch/commit/e98135d1ad2f999fec649ecd21b35f3d5676be43,YES,torch/_functorch/partitioners.py,patch_1,"-        + fw_rng_state_outputs
","+        + tuple(fw_rng_state_outputs)
","-        + fw_rng_state_outputs
+        + tuple(fw_rng_state_outputs)"
0,https://github.com/PyTorch/PyTorch/commit/0fd1fc17c3a53cb4cede1992d431d5384b2813f3,YES,aten/src/ATen/native/mps/MPSGraphSonomaOps.h,patch_0,,"+-(MPSGraphTensor * _Nonnull) realPartOfTensor:(MPSGraphTensor * _Nonnull) tensor
+                                         name:(NSString * _Nullable) name;
+
+
","@@ -3,6 +3,7 @@
 
 #include <ATen/core/Tensor.h>
 #include <ATen/Dispatch.h>
+#include <ATen/ExpandUtils.h>
 #include <ATen/FunctionalTensorWrapper.h>
 #include <ATen/TensorIterator.h>
 #include <ATen/native/quantized/Copy.h>"
0,https://github.com/PyTorch/PyTorch/commit/0fd1fc17c3a53cb4cede1992d431d5384b2813f3,YES,aten/src/ATen/native/mps/operations/UnaryOps.mm,patch_0,"-static void unary_op(const Tensor& self,
-                     const Tensor& output_,
-                     std::string op_name,
-                     UnaryOpBlock unaryBlock,
-                     is_noop_p is_noop = is_empty_tensor) {
-  if (!output_.is_same_size(self)) {
-    output_.resize_(self.sizes());
-  }
-
-  if (is_noop(self)) {
-    output_.copy_(self);
-    return;
-  }
-
","+static void unary_op_noresize(const Tensor& self, const Tensor& output_, std::string op_name, UnaryOpBlock unaryBlock) {
","@@ -950,6 +950,9 @@ void _to_sparse_check_arguments(const std::string& funcname, const Tensor& self,
   }
 
   if (blocksize.has_value()) {
+    if (blocksize.value().size() != 2) {
+      AT_ERROR(funcname, "": blocksize needs to be a tuple of size 2, but got "", blocksize.value().size());
+    }
     auto blocksize_to = *blocksize;
     if (blocksize_to[0] <= 0 || blocksize_to[1] <= 0) {
       AT_ERROR(funcname, "": blocksize needs to be positive, but got "", blocksize_to);"
0,https://github.com/PyTorch/PyTorch/commit/0fd1fc17c3a53cb4cede1992d431d5384b2813f3,YES,aten/src/ATen/native/mps/operations/UnaryOps.mm,patch_1,,"+static void unary_op(const Tensor& self,
+                     const Tensor& output_,
+                     std::string op_name,
+                     UnaryOpBlock unaryBlock,
+                     is_noop_p is_noop = is_empty_tensor) {
+  if (!output_.is_same_size(self)) {
+    output_.resize_(self.sizes());
+  }
+
+  if (is_noop(self)) {
+    output_.copy_(self);
+    return;
+  }
+
+  unary_op_noresize(self, output_, op_name, unaryBlock);
+}
+
","@@ -950,6 +950,9 @@ void _to_sparse_check_arguments(const std::string& funcname, const Tensor& self,
   }
 
   if (blocksize.has_value()) {
+    if (blocksize.value().size() != 2) {
+      AT_ERROR(funcname, "": blocksize needs to be a tuple of size 2, but got "", blocksize.value().size());
+    }
     auto blocksize_to = *blocksize;
     if (blocksize_to[0] <= 0 || blocksize_to[1] <= 0) {
       AT_ERROR(funcname, "": blocksize needs to be positive, but got "", blocksize_to);
"
0,https://github.com/PyTorch/PyTorch/commit/0fd1fc17c3a53cb4cede1992d431d5384b2813f3,YES,aten/src/ATen/native/mps/operations/UnaryOps.mm,patch_2,,"+static MPSGraphTensor* lengthOfComplexAsReal(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {
+  auto squares = [mpsGraph squareWithTensor:inputTensor name:nil];
+  auto sumSquares = [mpsGraph reductionSumWithTensor:squares axis:-1 name:nil];
+  return [mpsGraph squareRootWithTensor:sumSquares name:nil];
+}
+
","@@ -74,6 +74,7 @@ void fuseGraph(std::shared_ptr<Graph>& g) {
     GRAPH_DUMP(""After PrepareBinaryForLLGA. Before DeferSizeCheck"", g);
     DeferSizeCheck(g);
     GRAPH_DUMP(""After DeferSizeCheck. Before CreateLlgaSubgraphs"", g);
+    dnnl::graph::set_constant_tensor_cache(true);
     CreateLlgaSubgraphs(g);
     GRAPH_DUMP(""After CreateLlgaSubgraphs. Before PropagateLayout"", g);
     PropagateLayout(g);"
0,https://github.com/PyTorch/PyTorch/commit/0fd1fc17c3a53cb4cede1992d431d5384b2813f3,YES,aten/src/ATen/native/mps/operations/UnaryOps.mm,patch_3,"-#define CREATE_MPS_UNARY_TORCH_IMPL_FUNC(func_out, func_stub)                                                    \
-  Tensor& func_out(const Tensor& self, Tensor& output) {                                                         \
-    mps::unary_op(self, output, #func_out, ^MPSGraphTensor*(MPSGraph * mpsGraph, MPSGraphTensor * inputTensor) { \
-      return [mpsGraph func_stub##WithTensor:inputTensor name:nil];                                              \
-    });                                                                                                          \
-    return output;                                                                                               \
-  }
-
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/0fd1fc17c3a53cb4cede1992d431d5384b2813f3,YES,aten/src/ATen/native/mps/operations/UnaryOps.mm,patch_4,"-CREATE_MPS_UNARY_TORCH_IMPL_FUNC(abs_out_mps, absolute)
","+Tensor& abs_out_mps(const Tensor& self, Tensor& output) {
+  using namespace mps;
+
+  if (!output.is_same_size(self)) {
+    output.resize_(self.sizes());
+  }
+
+  if (self.numel() == 0) {
+    return output;
+  }
+
+  if (supportsComplex() || !self.is_complex()) {
+    unary_op_noresize(self, output, ""abs_out_mps"", ^MPSGraphTensor*(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {
+      auto rc = [mpsGraph absoluteWithTensor:inputTensor name:nil];
+      if (self.is_complex()) {
+        rc = [mpsGraph realPartOfTensor:rc name:nil];
+      }
+      return rc;
+    });
+  } else {
+    Tensor realInput = at::view_as_real(self);
+    unary_op_noresize(
+        realInput, output, ""abs_out_mps"", ^MPSGraphTensor*(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {
+          auto rc = lengthOfComplexAsReal(mpsGraph, inputTensor);
+          return [mpsGraph reshapeTensor:rc withShape:getMPSShape(output) name:nil];
+        });
+  }
+  return output;
+}
","--- a/torch/csrc/jit/codegen/cuda/mutator.h
+++ b/torch/csrc/jit/codegen/cuda/mutator.h
@@ -367,51 +372,6 @@ Tensor& where_self_out_mps(const Tensor& condition, const Tensor& self, const Te
   auto feeds = dictionaryFromPlaceholders(conditionPlaceholder, selfPlaceholder, otherPlaceholder);
   runMPSGraph(stream, cachedGraph->graph(), feeds, outputPlaceholder);
 }
 
-  return out;
-}
-
-Tensor where_mps(const Tensor& condition, const Tensor& self, const Tensor& other) {
-  auto max_dim = std::max(condition.dim(), std::max(self.dim(), other.dim()));
-
-  // How many leading dimensions do we broadcast across for each Tensor?
-  int cond_num_implicit_ones = (max_dim - condition.dim());
-  int self_num_implicit_ones = (max_dim - self.dim());
-  int other_num_implicit_ones = (max_dim - other.dim());
-
-  std::vector<int64_t> out_arr(max_dim);
-
-  // Broadcasted output shape
-  for (int i = 0; i < max_dim; i++) {
-    // Use up the leading broadcast dimensions for each Tensor, then continue from the start of the ""actual"" shape
-    int64_t cond_idx = i < cond_num_implicit_ones ? 1 : (condition.size(i - cond_num_implicit_ones));
-    int64_t self_idx = i < self_num_implicit_ones ? 1 : (self.size(i - self_num_implicit_ones));
-    int64_t other_idx = i < other_num_implicit_ones ? 1 : (other.size(i - other_num_implicit_ones));
-
-    auto max_idx = std::max({cond_idx, self_idx, other_idx});
-
-    TORCH_CHECK(cond_idx == max_idx || cond_idx == 1 || (cond_idx == 0 && max_idx == 1),
-                i,
-                ""'th index "",
-                cond_idx,
-                "" of condition tensor does not match the other tensors"")
-    TORCH_CHECK(self_idx == max_idx || self_idx == 1 || (self_idx == 0 && max_idx == 1),
-                i,
-                ""'th index "",
-                self_idx,
-                "" of x tensor does not match the other tensors"")
-    TORCH_CHECK(other_idx == max_idx || other_idx == 1 || (other_idx == 0 && max_idx == 1),
-                i,
-                ""'th index "",
-                other_idx,
-                "" of x tensor does not match the other tensors"")
-
-    out_arr[i] = (cond_idx == 0 || self_idx == 0 || other_idx == 0) ? 0 : max_idx;
-  }
-
-  Tensor ret = at::empty(
-      IntArrayRef(out_arr), self.scalar_type(), c10::nullopt, kMPS, c10::nullopt, self.suggest_memory_format());
-  return where_self_out_mps(condition, self, other, ret);
-}
-
 Tensor& abs_out_mps(const Tensor& self, Tensor& output) {
   using namespace mps;
 
@@ -421,7 +382,8 @@ Tensor& abs_out_mps(const Tensor& self, Tensor& output) {
       auto rc = lengthOfComplexAsReal(mpsGraph, inputTensor);
       return [mpsGraph reshapeTensor:rc withShape:getMPSShape(output) name:nil];
     });
-  return output;
+  } else {
+    Tensor realInput = at::view_as_real(self);
+    unary_op_noresize(
+        realInput, output, ""abs_out_mps"", ^MPSGraphTensor*(MPSGraph* mpsGraph, MPSGraphTensor* inputTensor) {
+          auto rc = lengthOfComplexAsReal(mpsGraph, inputTensor);
+          return [mpsGraph reshapeTensor:rc withShape:getMPSShape(output) name:nil];
+        });
   return output;
+}
```"
0,https://github.com/PyTorch/PyTorch/commit/0fd1fc17c3a53cb4cede1992d431d5384b2813f3,YES,aten/src/ATen/native/mps/operations/UnaryOps.mm,patch_5,"-    MPSGraphTensor* squares = [mpsGraph squareWithTensor:inputTensor name:nil];
-    MPSGraphTensor* sumSquares = [mpsGraph reductionSumWithTensor:squares axis:-1 name:nil];
-    MPSGraphTensor* norm = [mpsGraph squareRootWithTensor:sumSquares name:nil];
","+    MPSGraphTensor* norm = mps::lengthOfComplexAsReal(mpsGraph, inputTensor);
","@@ -308,8 +312,9 @@ Tensor& where_self_out_mps(const Tensor& condition, const Tensor& self, const Te
   MPSStream* stream = getCurrentMPSStream();

   // Empty output
-  if (out.numel() == 0)
-    return out;
+  if (out.numel() == 0) {
+    return;
+  }

   // Derive from MPSCachedGraph
   struct CachedGraph : public MPSCachedGraph {"
0,https://github.com/PyTorch/PyTorch/commit/7731c97e065cffe4efe82698980a2c8297816dbd,YES,torch/_inductor/codegen/common.py,patch_0,"-    def _print_Relational(self, expr):
-        return f"" {expr.rel_op} "".join(map(self.paren, map(self._print, expr.args)))
","+    def _print_Unequality(self, expr):
+        return "" != "".join(map(self.paren, map(self._print, expr.args)))
","@@ -311,8 +311,8 @@ class ExprPrinter(Printer):
         else:  # exp == 0
             return ""1""
 
-    def _print_Unequality(self, expr):
-        return "" != "".join(map(self.paren, map(self._print, expr.args)))
+    def _print_Relational(self, expr):
+        return f"" {expr.rel_op} "".join(map(self.paren, map(self._print, expr.args)))
 
     def _print_Mul(self, expr):
         return ""*"".join(map(self.paren, map(self._print, expr.args)))"
0,https://github.com/PyTorch/PyTorch/commit/7731c97e065cffe4efe82698980a2c8297816dbd,YES,torch/fx/experimental/symbolic_shapes.py,patch_0,"-def _iterate_exprs(val: Union[SymInt, torch.Tensor]) -> Iterable[sympy.Basic]:
-    elif isinstance(val, sympy.Basic):
","+def _iterate_exprs(val: Union[SymInt, torch.Tensor]) -> Iterable[sympy.Expr]:
+    elif isinstance(val, sympy.Expr):
","
@@ -161,13 +161,13 @@ def is_concrete_bool(a: Union[bool, SymBool]):
 def tensor_has_hints(t):
     return all(has_hint(s) for s in t.size())
 
-def _iterate_exprs(val: Union[SymInt, torch.Tensor]) -> Iterable[sympy.Basic]:
+def _iterate_exprs(val: Union[SymInt, torch.Tensor]) -> Iterable[sympy.Expr]:
     if isinstance(val, SymTypes):
         # This allow applies to the jagged layout NestedTensor case as
         # singleton ints are not symbolic
         if is_symbolic(val):
             yield val.node.expr
-    elif isinstance(val, sympy.Basic):
+    elif isinstance(val, sympy.Expr):
         yield val
     elif isinstance(val, (int, float, bool)):
         pass"
0,https://github.com/PyTorch/PyTorch/commit/2ead6c2f6eaa76eb897d8dd87061bbbdf0824314,YES,torch/_dynamo/config.py,patch_0,,"+add_runtime_assertions_for_inline_constraints = True
+
","--- aot_inductor/codegen.py
+++ aot_inductor/codegen.py
@@ -589,7 +593,7 @@ def constrain_unify(a, b):
     """"""
     # TODO: this does not install a deferred runtime assert yet
 
-    # TODO: Maybe dedupe this with _maybe_guard_eq?
+    # TODO: Maybe dedupe this with _maybe_guard_rel?
     if not isinstance(a, SymInt):
         if not isinstance(b, SymInt):
             assert a == b"
0,https://github.com/PyTorch/PyTorch/commit/2ead6c2f6eaa76eb897d8dd87061bbbdf0824314,YES,torch/_export/__init__.py,patch_0,"-    if len(range_constraints) > 0 or len(equality_constraints) > 0:
-        exported_program = exported_program._transform(
-            _AddRuntimeAssertionsForInlineConstraintsPass(range_constraints, equality_constraints)
-        )
","+    if torch._dynamo.config.add_runtime_assertions_for_inline_constraints:
+        if len(range_constraints) > 0 or len(equality_constraints) > 0:
+            exported_program = exported_program._transform(
+                _AddRuntimeAssertionsForInlineConstraintsPass(range_constraints, equality_constraints)
+            )
","-    if torch._dynamo.config.add_runtime_assertions_for_inline_constraints:
+    if len(range_constraints) > 0 or len(equality_constraints) > 0:
         exported_program = exported_program._transform(
             _AddRuntimeAssertionsForInlineConstraintsPass(range_constraints, equality_constraints)
         )"
0,https://github.com/PyTorch/PyTorch/commit/2ead6c2f6eaa76eb897d8dd87061bbbdf0824314,YES,torch/_inductor/codegen/wrapper.py,patch_0,"-from torch.fx.experimental.symbolic_shapes import SymTypes
","+from torch.fx.experimental.symbolic_shapes import free_unbacked_symbols, SymTypes
","--- a.py
+++ b.py
@@ -1,3 +1,4 @@
-from torch.fx.experimental.symbolic_shapes import SymTypes
+from torch.fx.experimental.symbolic_shapes import free_unbacked_symbols, SymTypes
+from torch.fx import immutable_collections, Interpreter

 # Bug fixing code goes here"
0,https://github.com/PyTorch/PyTorch/commit/2ead6c2f6eaa76eb897d8dd87061bbbdf0824314,YES,torch/_inductor/codegen/wrapper.py,patch_1,,"+
+                bool is_non_zero() {
+                    return grid_x > 0 && grid_y > 0 && grid_z > 0;
+                }
","@@ -1647,21 +1647,19 @@ def grid(*numels):
             return numel
         return ceildiv(numel, block)
 
-    max_grid_dims = config.triton.max_tiles
-
     def grid_fn(meta):
         x_grid = get_grid_dim(xnumel, meta.get(""XBLOCK"", 1))
         y_grid = get_grid_dim(ynumel, meta.get(""YBLOCK"", None))
 
-        MAX_Y_GRID = get_max_y_grid()
-        if znumel is None and max_grid_dims <= 2:
-            div = ceildiv(y_grid, MAX_Y_GRID)
+        max_y_grid = get_max_y_grid()
+        if znumel is None:
+            div = ceildiv(y_grid, max_y_grid)
             y_grid = y_grid // div
             z_grid = div
         else:
             z_grid = get_grid_dim(znumel, meta.get(""ZBLOCK"", None))
             torch._check(
-                y_grid <= MAX_Y_GRID,
+                y_grid <= max_y_grid,
                 lambda: f""Generated y grid beyond 2^16 ({y_grid}) not supported with z dimension present. File issue"",
             )
-"
0,https://github.com/PyTorch/PyTorch/commit/2ead6c2f6eaa76eb897d8dd87061bbbdf0824314,YES,torch/_inductor/codegen/wrapper.py,patch_2,"-        grid_args = [
-            self.grid_expr_printer(V.graph.sizevars.simplify(item)) for item in grid
-        ]
","+        grid = [V.graph.sizevars.simplify(item) for item in grid]
+        grid_has_unbacked_symbols = any(free_unbacked_symbols(item) for item in grid)
+        grid_args = [self.grid_expr_printer(item) for item in grid]
+        if grid_has_unbacked_symbols:
+            self.writeline(f""if ({grid_name}.is_non_zero()) {{"")
","@@ -557,7 +556,7 @@ def fx_codegen_and_compile(
                 for out in graph.graph_outputs:
                     if hasattr(out, ""layout""):
                         context.output_strides.append(
-                            tuple(
+                            tuple(  # type: ignore[arg-type]
                                 V.graph.sizevars.size_hint(s) for s in out.layout.stride
                             )
                         )
-        grid_args = [
-            self.grid_expr_printer(V.graph.sizevars.simplify(item)) for item in grid
-        ]
+        grid = [V.graph.sizevars.simplify(item) for item in grid]
+        grid_has_unbacked_symbols = any(free_unbacked_symbols(item) for item in grid)
+        grid_args = [self.grid_expr_printer(item) for item in grid]
+        if grid_has_unbacked_symbols:
+            self.writeline(f""if ({grid_name}.is_non_zero()) {{"")
"
0,https://github.com/PyTorch/PyTorch/commit/2ead6c2f6eaa76eb897d8dd87061bbbdf0824314,YES,torch/_inductor/codegen/wrapper.py,patch_3,,"+        if grid_has_unbacked_symbols:
+            self.writeline(""}"")
","@@ -3283,5 +3283,5 @@ class CudaWrapperCodeGen(CppWrapperCodeGen):
                 stream,
             )
         )
-        if grid_has_unbacked_symbols:
+        if grid_uses_symbolic_shapes:
             self.writeline(""}"")"
0,https://github.com/PyTorch/PyTorch/commit/2ead6c2f6eaa76eb897d8dd87061bbbdf0824314,YES,torch/_inductor/ir.py,patch_0,"-                wrapper.writeline(f""{s} = {self.get_name()}.size({i})"")
-                wrapper.writeline(f""{s} = {self.get_name()}.stride({i})"")
-            wrapper.writeline(f""{s} = {self.get_name()}.storage_offset()"")
","+                wrapper.writeline(
+                    f""{wrapper.declare}{s} = {self.get_name()}.size({i}){wrapper.ending}""
+                )
+                wrapper.writeline(
+                    f""{wrapper.declare}{s} = {self.get_name()}.stride({i}){wrapper.ending}""
+                )
+            wrapper.writeline(
+                f""{wrapper.declare}{s} = {self.get_name()}.storage_offset(){wrapper.ending}""
+            )
","@@ -3585,13 +3585,9 @@ class DeviceCopy(ExternKernelOut):
         args = self.codegen_args()
         assert len(args) == 1
         if self.output_view:
-            wrapper.writeline(
-                f""{self.output_view.codegen_reference()}.copy_({args[0]}){V.graph.wrapper_code.ending}""
-            )
+            wrapper.codegen_device_copy(args[0], self.output_view.codegen_reference())
         else:
-            wrapper.writeline(
-                f""{self.codegen_reference()}.copy_({args[0]}){V.graph.wrapper_code.ending}""
-            )
+            wrapper.codegen_device_copy(args[0], self.codegen_reference())
 
 
 class DynamicScalar(IRNode):"
0,https://github.com/PyTorch/PyTorch/commit/606c4f1367c7eb4a49aa4a9538dd2b1eb92485d6,YES,torch/distributed/_shard/sharded_tensor/api.py,patch_0,"-        current_rank = dist.get_rank(self._process_group)
","+        current_rank = dist.get_rank()  # global rank
","@@ -1128,7 +1128,7 @@ def get_pg_count() -> int:
     """"""
     return _world.group_count
 
-def get_node_local_rank() -> int:
+def get_node_local_rank(fallback_rank: Optional[int] = None) -> int:
     """"""
     Return the local rank of the current process relative to the node.
 "
0,https://github.com/PyTorch/PyTorch/commit/606c4f1367c7eb4a49aa4a9538dd2b1eb92485d6,YES,torch/distributed/_shard/sharded_tensor/utils.py,patch_0,"-    if not c10d._rank_not_in_group(pg):
","+    if rank is not None and not c10d._rank_not_in_group(pg):
","@@ -1128,7 +1128,7 @@ def get_pg_count() -> int:
     """"""
     return _world.group_count
 
-def get_node_local_rank() -> int:
+def get_node_local_rank(fallback_rank: Optional[int] = None) -> int:
     """"""
     Return the local rank of the current process relative to the node. 
     
-    if not c10d._rank_not_in_group(pg):
+    if rank is not None and not c10d._rank_not_in_group(pg):"
0,https://github.com/PyTorch/PyTorch/commit/4926146537df5a39846dae0e551d394001588b13,YES,torch/_inductor/fx_passes/mkldnn_fusion.py,patch_0,"-            if any(len(n.args[other_index].users) > 1 for n in binary_nodes):
","+    def _get_remaining_users(extra_input_node, compute_node):
+        # Think about this pattern:
+        #      ReLU
+        #     /   \
+        #  Conv1
+        #   /      \
+        # Conv2
+        #   \      /
+        #      Add
+        # Although, the extra input node (ReLU) has more than 1 users: Conv1 and Add.
+        # The Conv1 is the ancestor node of the current compute node (Conv2).
+        # This indicates that the buffer of ReLU has completed all its usage,
+        # So we can safely make changes to it now by doing Conv2->Add inplace fusion.
+        # Take above case as example:
+        # * extra_input_node: ReLU
+        # * compute_node: Conv2
+        # _get_remaining_users will return the users of extra_input_node which are not
+        # ancestor node of compute_node.
+        def _is_ancestor_node(_current_node, _ancestor_node):
+            # Check whether _ancestor_node is the ancestor node of _current_node
+            _node_list = [_current_node]
+            _visited_nodes = set()
+            while len(_node_list) != 0:
+                _current_node = _node_list.pop(0)
+                if _current_node not in _visited_nodes:
+                    _visited_nodes.add(_current_node)
+                    if _current_node == _ancestor_node:
+                        return True
+                    elif isinstance(
+                        _current_node, torch.fx.Node
+                    ) and _current_node.op not in [""placeholder"", ""output"", ""get_attr""]:
+                        for input in _current_node.all_input_nodes:
+                            _node_list.append(input)  # noqa: PERF402
+            return False
+
+        return [
+            user
+            for user in list(extra_input_node.users)
+            if not _is_ancestor_node(compute_node, user)
+        ]
+
+
+            def _get_compute_node(_binary_node, _other_index):
+                assert (
+                    len(_binary_node.all_input_nodes) == 2
+                ), ""Binary node should have 2 input nodes.""
+                _compute_index = 1 if (_other_index == 0) else 0
+                return _binary_node.args[_compute_index]
+
+            if any(
+                len(
+                    _get_remaining_users(
+                        n.args[other_index], _get_compute_node(n, other_index)
+                    )
+                )
+                > 1
+                for n in binary_nodes
+            ):
","@@ -435,15 +435,19 @@ if torch._C._has_mkldnn:
                 _compute_index = 1 if (_other_index == 0) else 0
                 return _binary_node.args[_compute_index]
 
+            def _other_input_not_inplaceable(_binary_node, _other_index):
+                _compute_node = _get_compute_node(_binary_node, _other_index)
+                return (
+                    len(
+                        _get_remaining_users(
+                            _binary_node.args[_other_index], _compute_node
+                        )
+                    )
+                    > 1
+                    or _binary_node.args[_other_index] == _compute_node.args[0]
+                )
+
             if any(_other_input_not_inplaceable(n, other_index) for n in binary_nodes):
                 return False
             if any(
                 n.args[other_index].op in [""placeholder"", ""output""]                 for n in binary_nodes
             ):
-                return False
+                return False
 
+    if any(_other_input_not_inplaceable(n, other_index) for n in binary_nodes):"
0,https://github.com/PyTorch/PyTorch/commit/1877b7896c237567285804ecc138bc86180a7ced,NO,docs/source/checkpoint.rst,patch_0,,"+.. autoclass:: CheckpointPolicy
+.. autoclass:: SelectiveCheckpointContext
+.. autofunction:: create_selective_checkpoint_contexts
","@@ -35,3 +35,6 @@
 torch.utils.checkpoint
 .. autofunction:: checkpoint
 .. autofunction:: checkpoint_sequential
 .. autofunction:: set_checkpoint_debug_enabled
+.. autoclass:: CheckpointPolicy
+.. autoclass:: SelectiveCheckpointContext
+.. autofunction:: create_selective_checkpoint_contexts"
0,https://github.com/PyTorch/PyTorch/commit/1877b7896c237567285804ecc138bc86180a7ced,NO,torch/_higher_order_ops/wrap.py,patch_0,"-from torch.utils.checkpoint import checkpoint, uid
-
","+import itertools
+from torch.utils.checkpoint import checkpoint
+
+uid = itertools.count(1)
","@@ -1,4 +1,5 @@
-import itertools
-from torch.utils.checkpoint import checkpoint
+
+import itertools
+from torch.utils.checkpoint import checkpoint, CheckpointPolicy, create_selective_checkpoint_contexts
+
 uid = itertools.count(1)"
0,https://github.com/PyTorch/PyTorch/commit/1877b7896c237567285804ecc138bc86180a7ced,NO,torch/utils/checkpoint.py,patch_0,"-from itertools import count
-from typing import (
-    Any,
-    Callable,
-    ContextManager,
-    DefaultDict,
-    Dict,
-    Iterable,
-    List,
-    Optional,
-    Tuple,
-)
","+from typing import *  # noqa: F403
+import enum
","--- a.py
        import copy
        import dataclasses
+import math
        from enum import auto, Enum
+from typing import *  # noqa: F403
+import enum
      
        -from itertools import count
        -from typing import (
        -    Any,
        -    Callable,
        -    ContextManager,
        -    DefaultDict,
        -    Dict,
        -    Iterable,
        -    List,
        -    Optional,
        -    Tuple,
        -)
        +import enum"
0,https://github.com/PyTorch/PyTorch/commit/1877b7896c237567285804ecc138bc86180a7ced,NO,torch/utils/checkpoint.py,patch_1,,"+    ""CheckpointPolicy"",
+    ""SelectiveCheckpointContext"",
+    ""create_selective_checkpoint_contexts"",
+    ""SAC_IGNORED_OPS"",
","@@ -39,6 +29,9 @@
     ""set_checkpoint_early_stop"",
     ""DefaultDeviceType"",
     ""set_checkpoint_debug_enabled"",
+    ""CheckpointPolicy"",
+    ""SelectiveCheckpointContext"",
+    ""create_selective_checkpoint_contexts"",
     ""SAC_IGNORED_OPS"",
 ]"
0,https://github.com/PyTorch/PyTorch/commit/1877b7896c237567285804ecc138bc86180a7ced,NO,torch/utils/checkpoint.py,patch_2,"-def _detach(x):
-    if isinstance(x, torch.Tensor):
-        return x.detach()
-uid = count(1)
-# NOTE: torch.utils.checkpoint internal logic will call these two functions unknown number of times
-# (i.e. there could be _CachedTorchDispatchMode calls that doesn't map to a _CachingTorchDispatchMode call),
-# so we ignore these ops and just always recompute them.
-_ignored_ops = {
-    torch.ops.prim.device.default,
-    r""""""
-    A :class:`TorchDispatchMode` to implement selective activation checkpointing
-    that's compatible with torch.compile. Used together with _CachedTorchDispatchMode.
-    """"""
-    def push_into_storage(self, out, func, args, kwargs):
-        out_detached = tree_map(_detach, out)
-        self.storage[func].append(out_detached)
-    def _handle_compile_in_forward_ctx(self, should_not_recompute, func, args, kwargs):
-        if should_not_recompute:
-        # NOTE: Here we just store and reuse output of all ops, since in torch.compile mode
-        # we decide and handle recomputation in the partitioner.
-        self.push_into_storage(out, func, args, kwargs)
-        return out
-    def __torch_dispatch__(self, func, types, args=(), kwargs=None):
-        if kwargs is None:
-            kwargs = {}
-        if func in _ignored_ops:
-            return func(*args, **kwargs)
-        should_not_recompute = self.policy_fn(""forward"", func, *args, **kwargs)
-        if _is_compiling(func, args, kwargs):
-            return self._handle_compile_in_forward_ctx(should_not_recompute, func, args, kwargs)
-        else:
-            if should_not_recompute:
-                out = func(*args, **kwargs)
-                self.push_into_storage(out, func, args, kwargs)
-            else:
-                out = func(*args, **kwargs)
-            return out
-    r""""""
-    A :class:`TorchDispatchMode` to implement selective activation checkpointing
-    that's compatible with torch.compile. Used together with _CachingTorchDispatchMode.
-    """"""
-    def __init__(self, policy_fn, storage):
-
-    def pop_from_storage(self, func, args, kwargs):
-        assert func in self.storage
-        out = self.storage[func].pop(0)
-        return out
-
-    def _handle_compile_in_recompute_ctx(self, should_not_recompute, func, args, kwargs):
-        out = self.pop_from_storage(func, args, kwargs)
-        return out
-        if kwargs is None:
-            kwargs = {}
-        if func in _ignored_ops:
-        should_not_recompute = self.policy_fn(""recompute"", func, *args, **kwargs)
-        if _is_compiling(func, args, kwargs):
-            return self._handle_compile_in_recompute_ctx(should_not_recompute, func, args, kwargs)
-            if should_not_recompute:
-                out = self.pop_from_storage(func, args, kwargs)
-            else:
-                out = func(*args, **kwargs)
-            return out
-def _pt2_selective_checkpoint_context_fn_gen(policy_fn):
-    A helper function that generates a pair of contexts to be later passed into
-    `torch.utils.checkpoint` API to implment selective checkpointing.
-    .. warning::
-        This is context_fn is intended for use with torch.compile only.
-        policy_fn (Callable[[Callable, List[Any], Dict[str, Any]], bool]): Policy function
-            to decide whether a particular op should be recomputed in backward pass or not.
-            In eager mode:
-                If policy_fn(...) returns True, the op is guaranteed to NOT be recomputed.
-                If policy_fn(...) returns False, the op is guaranteed to be recomputed.
-            In torch.compile mode:
-                If policy_fn(...) returns True, the op is guaranteed to NOT be recomputed.
-                If policy_fn(...) returns False, the op may or may not be recomputed
-                (it's up to the partitioner to decide).
-
-        A pair of generated contexts.
-        >>> def get_custom_policy():
-        >>>     no_recompute_list = [
-        >>>         torch.ops.aten.mm.default,
-        >>>     ]
-        >>>     def custom_policy(mode, func, *args, **kwargs):
-        >>>         return func in no_recompute_list
-        >>>     return custom_policy
-        >>> def selective_checkpointing_context_fn():
-        >>>     return _pt2_selective_checkpoint_context_fn_gen(get_custom_policy())
-        >>> def gn(x, y):
-        >>>     return torch.sigmoid(torch.matmul(torch.matmul(x, y), y)) * y
-        >>> def fn(x, y):
-        >>>     return torch.utils.checkpoint.checkpoint(
-        >>>         gn, x, y,
-        >>>         use_reentrant=False,
-        >>>         context_fn=selective_checkpointing_context_fn,
-        >>>     )
-        >>> x = torch.randn(4, 4, requires_grad=True)
-        >>> y = torch.randn(4, 4, requires_grad=True)
-        >>> compiled_fn = torch.compile(fn)
-    storage: Dict[Any, List[Any]] = defaultdict(list)
-    return _CachingTorchDispatchMode(policy_fn, storage), _CachedTorchDispatchMode(policy_fn, storage)
",No
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,YES,BUILD.bazel,patch_0,,"+        ""torch/csrc/distributed/c10d/Utils.cu"",
","@@ -830,6 +831,7 @@ cc_library(
             ""torch/csrc/cuda/python_nccl.cpp"",
             ""torch/csrc/cuda/nccl.cpp"",
             ""torch/csrc/distributed/c10d/intra_node_comm.cu"",
+            ""torch/csrc/distributed/c10d/Utils.cu"",
             ""torch/csrc/distributed/c10d/quantization/quantization_gpu.cu"",
         ],
     )) + torch_sources,"
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,YES,BUILD.bazel,patch_1,,"+            ""torch/csrc/distributed/c10d/Utils.cu"",
","@@ -830,6 +831,7 @@
             ""torch/csrc/cuda/python_nccl.cpp"",
             ""torch/csrc/cuda/nccl.cpp"",
             ""torch/csrc/distributed/c10d/intra_node_comm.cu"",
+            ""torch/csrc/distributed/c10d/Utils.cu"",
             ""torch/csrc/distributed/c10d/quantization/quantization_gpu.cu"",
         ],
     )) + torch_sources,"
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,YES,build_variables.bzl,patch_0,,"+    ""torch/csrc/distributed/c10d/Utils.cu"",
","@@ -830,6 +831,7 @@ cc_library(
             ""torch/csrc/cuda/python_nccl.cpp"",
             ""torch/csrc/cuda/nccl.cpp"",
             ""torch/csrc/distributed/c10d/intra_node_comm.cu"",
+            ""torch/csrc/distributed/c10d/Utils.cu"",
             ""torch/csrc/distributed/c10d/quantization/quantization_gpu.cu"",
         ],
     )) + torch_sources,"
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,YES,torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,patch_0,,"+  enableNanCheck_ = getCvarBool(TORCH_NCCL_NAN_CHECK, false);
","@@ -422,6 +424,69 @@ static PyMethodDef TensorGuards_methods[] = {\n \n static PyTypeObject TensorGuardsType = {PyVarObject_HEAD_INIT(nullptr, 0)};\n \n+struct GlobalStateGuard {\n+  PyObject_HEAD;\n+\n+  inline void init() {\n+    auto& ctx = at::globalContext();\n+    _grad_mode = at::GradMode::is_enabled();\n+    _torch_function = torch::torch_function_enabled();\n+    _deterministic_algorithms = ctx.deterministicAlgorithms();\n+    _allow_tf32 = ctx.allowTF32CuBLAS();\n+    _allow_fp16_reduce = ctx.allowFP16ReductionCuBLAS();\n+    _allow_bf16_reduce = ctx.allowBF16ReductionCuBLAS();\n+    _num_threads = at::get_num_threads();\n+  }\n+\n+  inline bool check() {\n+    auto& ctx = at::globalContext();\n+    return (\n+        _grad_mode == at::GradMode::is_enabled() &&\n+        _torch_function == torch::torch_function_enabled() &&\n+        _deterministic_algorithms == ctx.deterministicAlgorithms() &&\n+        _allow_tf32 == ctx.allowTF32CuBLAS() &&\n+        _allow_fp16_reduce == ctx.allowFP16ReductionCuBLAS() &&\n+        _allow_bf16_reduce == ctx.allowBF16ReductionCuBLAS() &&\n+        _num_threads == at::get_num_threads());\n+  }\n+\n+  bool _grad_mode;\n+  bool _torch_function;\n+  bool _deterministic_algorithms;\n+  bool _allow_tf32;\n+  bool _allow_fp16_reduce;\n+  bool _allow_bf16_reduce;\n+  int _num_threads;\n+  // TODO(jansel): we should guard on more state as inductor starts using it\n+};\n+\n+int GlobalStateGuard_init(\n+    GlobalStateGuard* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n+  self->init();\n+  return 0;\n+}\n+\n+PyObject* GlobalStateGuard_check(\n+    GlobalStateGuard* self,\n+    PyObject* args,\n+    PyObject* kwargs) {\n+  if (self->check()) {\n+    Py_RETURN_TRUE;\n+  } else {\n+    Py_RETURN_FALSE;\n+  }\n+}\n+\n+static PyMethodDef GlobalStateGuard_methods[] = {\n+    {""check"",\n+     (PyCFunction)(void*)GlobalStateGuard_check,\n+     METH_NOARGS,\n+     ""Return true if global state was the same as at creation time""},\n+    {nullptr}};\n+static PyTypeObject GlobalStateGuardType = {PyVarObject_HEAD_INIT(nullptr, 0)};\n+\n static PyObject* check_type_id(PyObject* dummy, PyObject* args) {\n   // faster `lambda obj, expected: id(type(obj)) == expected`\n   PyObject* obj = nullptr;\n'
+  enableNanCheck_ = getCvarBool(TORCH_NCCL_NAN_CHECK, false);\n'"
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,YES,torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,patch_1,,"+            << "", TORCH_NCCL_NAN_CHECK: "" << enableNanCheck_
","@@ -1024,6 +1026,9 @@ class TORCH_API ProcessGroupNCCL : public Backend {
   // timeout and nccl errors.
   bool dumpOnException_;

+  // Whether or not to enable nan check for input tensors to collectives.
+  bool enableNanCheck_;
+
   // Whether or not to create start CUDAEvent and enable timing for start
   // and end events. Note that enableTiming_ is always true if desyncDebug_
   // is set to true."
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,YES,torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,patch_2,,"+  if (enableNanCheck_) {
+    checkForNan(input);
+  }
","@@ -376,8 +376,8 @@ void TuningContext::EnableNumericsCheck(bool value) {
 
 bool TuningContext::IsNumericsCheckEnabled() const {
   static const char *env = getenv(""PYTORCH_TUNABLEOP_NUMERICAL_CHECK"");
-  if (env != nullptr && strcmp(env, ""0"") == 0) {
-    return false;
+  if (env != nullptr && strcmp(env, ""1"") == 0) {
+    return true;
   }
   return numerics_check_enable_;
 } 

 if (enableNanCheck_) {
   checkForNan(input);
 }"
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,YES,torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,patch_3,,"+  if (enableNanCheck_) {
+    checkForNan(tensor);
+  }
","@@ -278,6 +268,16 @@ void multi_tensor_apply(
       }
     }
   }
+
+  // see note: [finishing what we started]
+  if (loc_block_info != 0) {
+    multi_tensor_apply_kernel<<<
+        loc_block_info,
+        kBlockSize,
+        0,
+        at::cuda::getCurrentCUDAStream()>>>(tensorListMeta, callable, args...);
+    C10_CUDA_KERNEL_LAUNCH_CHECK();
+  }
 }

 template <int depth, typename T, typename... ArgTypes>"
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,YES,torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,patch_0,,"+static std::vector<std::string> TORCH_NCCL_NAN_CHECK = {""TORCH_NCCL_NAN_CHECK""};
+
","@@ -100,6 +100,8 @@
 static std::vector<std::string> TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC = {
     ""TORCH_NCCL_COORD_CHECK_MILSEC""};
 
+static std::vector<std::string> TORCH_NCCL_NAN_CHECK = {""TORCH_NCCL_NAN_CHECK""};
+
 constexpr const char* NCCL_BACKEND_NAME = ""nccl"";
 
 constexpr const char* EXCEPTION_DUMP = ""exception_dump"";"
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,YES,torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,patch_1,,"+  // Whether or not to enable nan check for input tensors to collectives.
+  bool enableNanCheck_;
+
","@@ -770,17 +770,13 @@ AOTITorchError aoti_torch_repeat_interleave_Tensor(
 }

 // Function to check existence of inf and NaN
-AOTITorchError aoti_check_inf_and_nan(AtenTensorHandle tensor) {
+AOTITorchError aoti_torch_check_inf_and_nan(
+    const char* tensor_name,
+    AtenTensorHandle tensor) {
   AOTI_TORCH_CONVERT_EXCEPTION_TO_ERROR_CODE({
     at::Tensor* check_tensor = tensor_handle_to_tensor_pointer(tensor);
-    auto flattened = check_tensor->view({-1});
 
-    for (int64_t i = 0; i < flattened.numel(); i++) {
-      auto value = flattened[i].item<float>();
-      if (std::isinf(value) || std::isnan(value)) {
-        assert(false);
-      }
-    }
+    assert_inf_and_nan(tensor_name, *check_tensor);
   });
 }
 
+  // Whether or not to enable nan check for input tensors to collectives.
+  bool enableNanCheck_;"
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,YES,torch/csrc/distributed/c10d/Utils.cu,patch_0,,"+#include <ATen/Dispatch.h>
+#include <ATen/cuda/CUDAContext.h>
+#include <c10/cuda/CUDAGuard.h>
+#include <torch/csrc/distributed/c10d/Utils.hpp>
+#include <torch/torch.h>
+#include <algorithm>
+
+namespace c10d {
+
+// CUDA kernel to check if data has NAN, device side assert
+// is raised if NAN is found
+template <typename T>
+__global__ void checkForNaN(T* data, size_t size) {
+  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
+  size_t stride = blockDim.x * gridDim.x;
+
+  for (size_t i = tid; i < size; i += stride) {
+    CUDA_KERNEL_ASSERT(!isnan(data[i]));
+  }
+}
+
+// CHECK if a Tensor contains NAN in any of its element
+void checkForNan(const at::Tensor& tensor) {
+  // skip check for non float types
+  if (!torch::is_floating_point(tensor)) {
+    return;
+  }
+  const size_t maxNumThreadsPerBlock = 512;
+  const size_t maxNumBlocks = 24;
+  const size_t numThreadsPerBlock =
+      std::min<size_t>(maxNumThreadsPerBlock, tensor.numel());
+
+  const size_t numBlocks = std::min<size_t>(
+      maxNumBlocks,
+      (tensor.numel() + numThreadsPerBlock - 1) / numThreadsPerBlock);
+
+  AT_DISPATCH_FLOATING_TYPES_AND_HALF(tensor.scalar_type(), ""checkForNaN"", [&] {
+    checkForNaN<scalar_t><<<numBlocks, numThreadsPerBlock>>>(
+        tensor.data_ptr<scalar_t>(), tensor.numel());
+    C10_CUDA_KERNEL_LAUNCH_CHECK();
+  });
+
+}
+
+} // namespace c10d
","#include <ATen/Dispatch.h>
#include <ATen/cuda/CUDAContext.h>
#include <c10/cuda/CUDAGuard.h>
#include <torch/csrc/distributed/c10d/Utils.hpp>
#include <torch/torch.h>
#include <algorithm>

namespace c10d {

// CUDA kernel to check if data has NAN, device side assert
// is raised if NAN is found
template <typename T>
__global__ void checkForNaN(T* data, size_t size) {
  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
  size_t stride = blockDim.x * gridDim.x;

  for (size_t i = tid; i < size; i += stride) {
    if (isnan(data[i])) {
      assert(false);
    }
  }
}

// CHECK if a Tensor contains NAN in any of its element
void checkForNan(const at::Tensor& tensor) {
  // skip check for non float types
  if (!torch::is_floating_point(tensor)) {
    return;
  }
  const size_t maxNumThreadsPerBlock = 256;
  const size_t maxNumBlocks = 24;
  const size_t numThreadsPerBlock = std::min<size_t>(maxNumThreadsPerBlock, tensor.numel());

  const size_t numBlocks = std::min<size_t>(
      maxNumBlocks,
      (tensor.numel() + numThreadsPerBlock - 1) / numThreadsPerBlock);

  AT_DISPATCH_FLOATING_TYPES_AND_HALF(tensor.scalar_type(), ""checkForNaN"", [&] {
    checkForNaN<scalar_t><<<numBlocks, numThreadsPerBlock>>>(
        tensor.data_ptr<scalar_t>(), tensor.numel());
    C10_CUDA_KERNEL_LAUNCH_CHECK();
  });
}

} // namespace c10d"
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,YES,torch/csrc/distributed/c10d/Utils.hpp,patch_0,,"+void checkForNan(const at::Tensor& tensor);
+
","@@ -48,6 +48,21 @@
   return tensor_pointer_to_tensor_handle(new_tensor);
 }
 
+inline void assert_inf_and_nan(
+    const std::string& tensor_name,
+    at::Tensor& check_tensor) {
+  auto flattened = check_tensor.view({-1});
+
+  for (int64_t i = 0; i < flattened.numel(); i++) {
+    auto value = flattened[i].item<float>();
+    if (std::isinf(value)) {
+      throw std::runtime_error(""At least one INF in "" + tensor_name);
+    } else if (std::isnan(value)) {
+      throw std::runtime_error(""At least one NaN in "" + tensor_name);
+    }
+  }
+}
+
 // utility functions to convert a pointer to an optional value
 template <class T>
 inline std::optional<T> pointer_to_optional(T* ptr) {"
0,https://github.com/PyTorch/PyTorch/commit/62732bdcdb8b6112e01366d4ad1c2a50e61da1ed,YES,torch/_inductor/fx_passes/group_batch_fusion.py,patch_0,,"+        and is_node_meta_valid(input)
+        and is_node_meta_valid(weight)
","@@ -41,7 +41,14 @@ def _check_val(node: torch.fx.Node) -> None:
             return all(_check_correct_val(x) for x in val)
         return False
 
+    def _no_returns(op):
+        if not isinstance(op, OpOverload):
+            return False
+        return len(op._schema.returns) == 0
+
     if ""val"" not in node.meta:
+        if node.op == ""call_function"" and _no_returns(node.target):
+            return
         raise SpecViolationError(f""Node.meta {node.name} is missing val field."")
 
     val = node.meta[""val""]"
0,https://github.com/PyTorch/PyTorch/commit/175ccfc4c8443bcc65c87d9c942272d3ebf16b0b,YES,torch/csrc/jit/mobile/flatbuffer_loader.cpp,patch_0,"-  TORCH_CHECK(ivalues != nullptr, ""Corrupted ivalues field"")
-      reinterpret_cast<const char*>(ivalues) < end, ""Corrupted ivalues field"")
","+      ivalues && module->object_types(),
+      ""Parsing flatbuffer module: Corrupted ivalues/object_types field"");
+  TORCH_CHECK(
+      reinterpret_cast<const char*>(ivalues) < end, ""Corrupted ivalues field"");
","-  TORCH_CHECK(ivalues != nullptr && module->object_types() != nullptr,
+      ""Parsing flatbuffer module: Corrupted ivalues/object_types field"");
+  TORCH_CHECK(
+      reinterpret_cast<const char*>(ivalues) < end, ""Corrupted ivalues field"");
+  TORCH_CHECK(
+      module->storage_data_size() >= 0,
+      ""Parsing flatbuffer module: illegal storage_data_size: "",
+      module->storage_data_size(),
+      "", expected to be non negative"");
   all_ivalues_.resize(ivalues->size());
   all_types_.resize(module->object_types()->size());
   storages_.resize(module->storage_data_size());"
0,https://github.com/PyTorch/PyTorch/commit/3e382456c109820c836b104570104c3c5aff5632,YES,torch/utils/cpp_extension.py,patch_0,"-    version_string = subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)
","+    try:
+        version_string = subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)
+    except Exception as e:
+        try:
+            version_string = subprocess.check_output([compiler, '--version'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)
+        except Exception as e:
+            return False
","@@ -96,7 +96,11 @@ def _get_model_bytecode_version(f_input) -> int:
         version = _get_model_bytecode_version(""path/to/model.ptl"")
 
     """"""
-    if isinstance(f_input, str):
+    if isinstance(f_input, (str, os.PathLike)):
         if not os.path.exists(f_input):
             raise ValueError(f""The provided filename {f_input} does not exist"")
         if os.path.isdir(f_input):
+    try:
         try:
             version_string = subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)
         except Exception as e:
             try:
                 version_string = subprocess.check_output([compiler, '--version'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)
             except Exception as e:
                 return False
+    except Exception as e:
+        return False"
0,https://github.com/PyTorch/PyTorch/commit/50a084070fe782e898edc8ba42c9e5f945c9c149,NO,.lintrunner.toml,patch_0,"-    'torch/_inductor/coordinate_descent_tuner.py',
-    'torch/_inductor/hooks.py',
-    'torch/_inductor/config.py',
-    'torch/_inductor/test_operators.py',
-    'torch/_inductor/inductor_prims.py',
-    'torch/_inductor/triton_helpers.py',
-    'torch/_inductor/cuda_properties.py',
-    'torch/_inductor/codegen/__init__.py',
-    'torch/_inductor/fx_passes/binary_folding.py',
-    'torch/_inductor/fx_passes/replace_random.py',
-    'torch/_inductor/fx_passes/__init__.py',
-    'torch/_inductor/fx_passes/freezing_patterns.py',
",,"['@@ -0,0 +1,1 @@\n+24a3fe9cb57e5cda3c923df29743f9767194cc27\n']"
0,https://github.com/PyTorch/PyTorch/commit/50a084070fe782e898edc8ba42c9e5f945c9c149,NO,torch/_inductor/codecache.py,patch_0,"-def _cuda_compiler() -> str:
",No
0,https://github.com/PyTorch/PyTorch/commit/e06bff8bbea84ce672285bc34690b2e45a1b63ab,YES,torch/_inductor/graph.py,patch_0,"-    def add_device_idx(self, idx: Optional[int]):
-        if idx is not None:
-            self.device_idxs.add(idx)
","+    def add_device_info(self, device: torch.device):
+        self.device_types.add(device.type)
+        if device.index is not None:
+            self.device_idxs.add(device.index)
","-    def add_device_idx(self, idx: Optional[int]):
-        if idx is not None:
-            self.device_idxs.add(idx)
+    def add_device_info(self, device: torch.device):
+        self.device_types.add(device.type)
+        if device.index is not None:
+            self.device_idxs.add(device.index)"
0,https://github.com/PyTorch/PyTorch/commit/e06bff8bbea84ce672285bc34690b2e45a1b63ab,YES,torch/_inductor/graph.py,patch_1,,"+        # Skip empty CPU tensor so that CUDA graphs can succeed, see https://github.com/pytorch/pytorch/pull/114144
+        if not isinstance(buffer, ir.ComputedBuffer) or not buffer.is_zero_elements():
+            self.add_device_info(buffer.get_device())
","@@ -584,10 +584,10 @@ class CUDAWarmupNode:
         }
 
         def get_non_cudagraph_inps():
-            non_cudagraph_inps = set()
+            non_cudagraph_inps = []
             for t in itertools.chain(new_inputs, self.wrapped_function.constants):
                 if (
                     isinstance(t, torch.Tensor)
                     and t.untyped_storage().data_ptr() not in existing_path_data_ptrs
                 ):
-                    non_cudagraph_inps.add(t.untyped_storage().data_ptr())
+                    non_cudagraph_inps.append(weakref.ref(t.untyped_storage()))
             return non_cudagraph_inps
 
-        non_cudagraph_inps = get_non_cudagraph_inps()
+        non_cudagraph_inps_storages = get_non_cudagraph_inps()
 
         if config.triton.slow_path_cudagraph_asserts and not self.already_warm:
             refs = list(self.path_live_weakrefs())"
0,https://github.com/PyTorch/PyTorch/commit/e06bff8bbea84ce672285bc34690b2e45a1b63ab,YES,torch/_inductor/graph.py,patch_2,"-        self.device_types.add(example.device.type)
-        self.add_device_idx(example.device.index)
","+        self.add_device_info(example.device)
","@@ -416,9 +420,11 @@ class WrapperCodeGen(CodeGen):

     def codegen_device_guard_enter(self, device_idx):
         self.writeline(
-            EnterCudaDeviceContextManagerLine(device_idx, self.first_device_guard)
+            EnterCudaDeviceContextManagerLine(
+                device_idx, self.last_seen_device_guard_index
+            )
         )
-        self.first_device_guard = False
+        self.last_seen_device_guard_index = device_idx

     def codegen_device_guard_exit(self):
         self.writeline(ExitCudaDeviceContextManagerLine())

+    def add_device_info(self, device):
+        self.device_types.add(device.type)
+        self.add_device_idx(device.index)"
0,https://github.com/PyTorch/PyTorch/commit/e06bff8bbea84ce672285bc34690b2e45a1b63ab,YES,torch/_inductor/graph.py,patch_3,"-        # In terms of some operations that don't have input tensors, we need to
-        # check the device of the buffers.
-        for buffer in self.buffers:
-            device_types.add(buffer.get_device().type)
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/e06bff8bbea84ce672285bc34690b2e45a1b63ab,YES,torch/_inductor/graph.py,patch_4,"-                    ), ""Unknown type when creating real inputs""
","+                    ), ""Unknown type when creating real inputs"" + str(type(x))
","@@ -4405,7 +4405,7 @@ class ExternKernel(InputsKernel):
                     type_ = self.arg_properties[i].get(""type"")
                     args.append(
                         V.graph.wrapper_code.val_to_cpp_arg_str(  # type: ignore[arg-type]
-                            type_, x
+                            x, type_
                         )
                     )
                 else:"
0,https://github.com/PyTorch/PyTorch/commit/e06bff8bbea84ce672285bc34690b2e45a1b63ab,YES,torch/_inductor/ir.py,patch_0,"-        V.graph.device_types.add(device.type)
-        V.graph.add_device_idx(device.index)
-        V.graph.device_types.add(x.get_device().type)
-        V.graph.add_device_idx(x.get_device().index)
","+        V.graph.add_device_info(device)
+        V.graph.add_device_info(x.get_device())
","@@ -3585,13 +3585,9 @@ class DeviceCopy(ExternKernelOut):
         args = self.codegen_args()
         assert len(args) == 1
         if self.output_view:
-            wrapper.writeline(
-                f""{self.output_view.codegen_reference()}.copy_({args[0]}){V.graph.wrapper_code.ending}""
-            )
+            wrapper.codegen_device_copy(args[0], self.output_view.codegen_reference())
         else:
-            wrapper.writeline(
-                f""{self.codegen_reference()}.copy_({args[0]}){V.graph.wrapper_code.ending}""
-            )
+            wrapper.codegen_device_copy(args[0], self.codegen_reference())


 class DynamicScalar(IRNode):"
0,https://github.com/PyTorch/PyTorch/commit/e06bff8bbea84ce672285bc34690b2e45a1b63ab,YES,torch/_inductor/scheduler.py,patch_0,"-        V.graph.device_types.add(device.type)
-        V.graph.add_device_idx(device.index)
","+        V.graph.add_device_info(device)
","@@ -144,7 +144,7 @@ class MemoryPlanningState:
 @dataclasses.dataclass
 class EnterCudaDeviceContextManagerLine:
     device_idx: int
-    first_time: bool
+    last_seen_device_guard_index: Optional[int]
 
     def codegen(self, code: IndentedBuffer, device_cm_stack: contextlib.ExitStack):
         if V.graph.cpp_wrapper:
-        V.graph.device_types.add(device.type)
-        V.graph.add_device_idx(device.index)
+        V.graph.add_device_info(device)"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,c10/core/ScalarType.cpp,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,c10/core/ScalarType.h,patch_0,,"+#include <unordered_map>
","@@ -460,9 +544,9 @@ 
 template <typename AT, typename BT, typename CT, BlasOp ALayout, BlasOp BLayout,
 auto GetHipBlasLtTypeStringAndOps() {
   hipblasOperation_t transa_outer = MapLayoutToHipBlasLt(ALayout);
   hipblasOperation_t transb_outer = MapLayoutToHipBlasLt(BLayout);
-  auto a_datatype = HipBlasDataTypeFor<AT>();
-  auto b_datatype = HipBlasDataTypeFor<BT>();
-  auto in_out_datatype = HipBlasDataTypeFor<CT>();
+  
+  auto a_datatype = HipDataTypeFor<AT>();
+  auto b_datatype = HipDataTypeFor<BT>();
+  auto in_out_datatype = HipDataTypeFor<CT>();
   
   std::vector<hipblasLtMatmulHeuristicResult_t> heuristic_result;
 
   hipblasLtHandle_t handle;
"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,c10/core/ScalarType.h,patch_1,,"+// Returns a pair of strings representing the names for each dtype.
+// The returned pair is (name, legacy_name_if_applicable)
+C10_API std::pair<std::string, std::string> getDtypeNames(
+    c10::ScalarType scalarType);
+
+// Returns a map of string name to dtype.
+C10_API const std::unordered_map<std::string, ScalarType>& getStringToDtypeMap();
+
","--- a/torch/csrc/jit/ir/irparser.h
+++ b/torch/csrc/jit/ir/irparser.h
@@ -23,14 +23,14 @@ namespace torch::jit {
 
 namespace {
 struct SchemaParser {
-  explicit SchemaParser(const std::string& str)
+  explicit SchemaParser(const std::string& str, bool allow_typevars)
       : L(std::make_shared<Source>(
             c10::string_view(str),
             c10::nullopt,
             0,
             nullptr,
             Source::DONT_COPY)),
-        type_parser(L, /*parse_complete_tensor_types*/ false) {}
+        type_parser(L, /*parse_complete_tensor_types*/ false, allow_typevars) {}
 
   std::variant<OperatorName, FunctionSchema> parseDeclaration() {
     OperatorName name = parseName();"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/_library/infer_schema.py,patch_0,,"+            elif isinstance(param.default, torch.dtype):
+                dtype_repr = str(param.default)
+                torch_dot = ""torch.""
+                assert dtype_repr.startswith(torch_dot)
+                default_repr = dtype_repr[len(torch_dot) :]
","@@ -2040,7 +2040,7 @@ def _to_dtype(
     non_blocking: bool = False,
     copy: bool = False,
     memory_format: Optional[torch.memory_format] = None,
-) -> Dict[str, Any]:
+) -> Union[Dict[str, Any], str]:
     kwargs = {
         ""dtype"": dtype,
         ""non_blocking"": non_blocking,"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/TypeInfo.cpp,patch_0,"-  auto primary_name = torch::utils::getDtypeNames(self->type).first;
","+  auto primary_name = c10::getDtypeNames(self->type).first;
","@@ -318,6 +320,7 @@ SymExpr:
        type: str
      hint:
        type: Optional[SymExprHint]
+      default: None
 SymExprHint:
   kind: union
   fields: 

-  auto primary_name = torch::utils::getDtypeNames(self->type).first;
+  auto primary_name = c10::getDtypeNames(self->type).first;"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/TypeInfo.cpp,patch_1,"-  auto primary_name = torch::utils::getDtypeNames(self->type).first;
","+  auto primary_name = c10::getDtypeNames(self->type).first;
","@@ -318,6 +320,7 @@ SymExpr:
       type: str
     hint:
       type: Optional[SymExprHint]
+      default: None
 SymExprHint:
   kind: union
   fields:
 "
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/jit/frontend/function_schema_parser.cpp,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/jit/frontend/function_schema_parser.cpp,patch_1,"-        default_value = parseDefaultValue(*fake_type, fake_type->kind(), N);
","+        default_value =
+            parseDefaultValue(*fake_type, fake_type->kind(), *real_type, N);
","@@ -82,12 +82,27 @@ TypePtr SchemaTypeParser::parseBaseType() {
   
   auto it = type_map.find(text);
   if (it == type_map.end()) {
-    if (!text.empty() && islower(text[0])) {
+    if (allow_typevars_ && !text.empty() && islower(text[0])) {
       // lower case identifiers that are not otherwise valid types
       // are treated as type variables
       return c10::TypeFactory::createNamed<VarType>(text);
     }
-    throw ErrorReport(tok.range) << ""unknown type specifier"";
+    if (text == ""double"") {
+      throw ErrorReport(tok.range)
+          << ""Use `float` instead of `double` in an operator\'s schema string. ""
+             ""`float` in schema corresponds to the double type in C++"";
+    }
+    if (text == ""int64_t"") {
+      throw ErrorReport(tok.range)
+          << ""Use `SymInt` or `int` instead of `int64_t` in an operator\'s schema string. ""
+             ""`SymInt` corresponds to c10::SymInt in C++ while `int` in schema corresponds ""
+             ""to the int64_t type in C++."";
+    }
+    throw ErrorReport(tok.range)
+        << ""unknown type specifier. Common valid schema types include ""
+           ""Tensor, SymInt, int, float, bool, Scalar; ""
+           ""for a full list, please see ""
+           ""https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#func "";
   }
   return it->second;
 }
"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/jit/frontend/function_schema_parser.cpp,patch_2,"-  IValue parseSingleConstant(const c10::Type& type, TypeKind kind) {
-          type, type.expectRef<c10::DynamicType>().dynamicKind());
","+
+  bool isPossiblyOptionalScalarType(const c10::Type& type) {
+    if (type.kind() == at::ScalarTypeType::Kind) {
+      return true;
+    }
+    if (type.kind() == at::OptionalType::Kind) {
+      for (const auto& inner : type.containedTypes()) {
+        if (isPossiblyOptionalScalarType(*inner))
+          return true;
+      }
+    }
+    return false;
+  }
+
+  IValue parseSingleConstant(
+      const c10::Type& type,
+      TypeKind kind,
+      const c10::Type& real_type) {
+          type, type.expectRef<c10::DynamicType>().dynamicKind(), real_type);
+    const auto& str2dtype = c10::getStringToDtypeMap();
","@@ -361,16 +361,24 @@ struct SchemaParser {
   }
   Lexer L;
   SchemaTypeParser type_parser;
+  bool allow_typevars_;
 };

 } // namespace

 std::variant<OperatorName, FunctionSchema> parseSchemaOrName(
-    const std::string& schemaOrName) {
-  return SchemaParser(schemaOrName).parseExactlyOneDeclaration();
+    const std::string& schemaOrName,
+    bool allow_typevars) {
+  // We're ignoring aten and prim for BC reasons
+  if (schemaOrName.rfind(""aten::"", 0) == 0 ||
+      schemaOrName.rfind(""prim::"", 0) == 0) {
+    allow_typevars = true;
+  }
+  return SchemaParser(schemaOrName, allow_typevars)
+      .parseExactlyOneDeclaration();
 }

-FunctionSchema parseSchema(const std::string& schema) {
-  auto parsed = parseSchemaOrName(schema);
+FunctionSchema parseSchema(const std::string& schema, bool allow_typevars) {
+  auto parsed = parseSchemaOrName(schema, allow_typevars);
   TORCH_CHECK(
       std::holds_alternative<FunctionSchema>(parsed),
       ""Tried to parse a function schema but only the operator name was given"");
+  const auto& str2dtype = c10::getStringToDtypeMap();
 }"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/jit/frontend/function_schema_parser.cpp,patch_3,,"+        // NB: float/complex/long are here for BC purposes. Other dtypes
+        // are handled via str2dtype.
+        // Please don't add more cases to this if-else block.
","@@ -306,7 +323,9 @@ if RUN_CPU:
         BaseTest(""test_sum_dtype""),  # float64
         BaseTest(""test_sum_int""),  # bool, int64, int8, uint8
         BaseTest(""test_tensor2""),  # constant input
-        BaseTest(""test_transpose""),  # multiple outputs, buffer clear
+        BaseTest(
+            ""test_transpose"", code_string_count={"".reset();"": 2}
+        ),  # multiple outputs, buffer clear
         BaseTest(""test_view_as_complex""),
         BaseTest(""test_view_as_real""),
     ]:"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/jit/frontend/function_schema_parser.cpp,patch_4,,"+        } else if (
+            isPossiblyOptionalScalarType(real_type) &&
+            str2dtype.count(text) > 0) {
+          return static_cast<int64_t>(str2dtype.at(text));
","@@ -82,12 +82,27 @@ TypePtr SchemaTypeParser::parseBaseType() {  
  
   auto it = type_map.find(text);  
   if (it == type_map.end()) {  
-    if (!text.empty() && islower(text[0])) {  
+    if (allow_typevars_ && !text.empty() && islower(text[0])) {  
       // lower case identifiers that are not otherwise valid types  
       // are treated as type variables  
       return c10::TypeFactory::createNamed<VarType>(text);  
     }  
-    throw ErrorReport(tok.range) << ""unknown type specifier"";  
+    if (text == ""double"") {  
+      throw ErrorReport(tok.range)  
+          << ""Use `float` instead of `double` in an operator\'s schema string. ""\  
+             ""`float` in schema corresponds to the double type in C++"";  
+    }  
+    if (text == ""int64_t"") {  
+      throw ErrorReport(tok.range)  
+          << ""Use `SymInt` or `int` instead of `int64_t` in an operator\'s schema string. ""\  
+             ""`SymInt` corresponds to c10::SymInt in C++ while `int` in schema corresponds ""\  
+             ""to the int64_t type in C++."";  
+    }  
+    throw ErrorReport(tok.range)  
+        << ""unknown type specifier. Common valid schema types include ""\  
+           ""Tensor, SymInt, int, float, bool, Scalar; ""\  
+           ""for a full list, please see ""\  
+           ""https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#func "";  
   }  
   return it->second;  
 }  "
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/jit/frontend/function_schema_parser.cpp,patch_5,"-  IValue parseConstantList(const c10::Type& type, TypeKind kind) {
-        vs.push_back(parseSingleConstant(type, kind));
","+  IValue parseConstantList(
+      const c10::Type& type,
+      TypeKind kind,
+      const c10::Type& real_type) {
+        vs.push_back(parseSingleConstant(type, kind, real_type));
","@@ -1765,7 +1765,11 @@ void initJITBindings(PyObject* module) {
   },
   py::arg(""input""),
   py::arg(""parse_tensor_constants"") = false);
-  m.def(""parse_schema"", parseSchema);
+  m.def(
+      ""parse_schema"",
+      &parseSchema,
+      py::arg(""schema""),
+      py::arg(""allow_typevars"") = true);
 m.def(""unify_type_list"", [](const std::vector<TypePtr>& types) {
   std::ostringstream s;
   auto type = unifyTypeList(types, s);"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/jit/frontend/function_schema_parser.cpp,patch_6,,"+      const c10::Type& real_type,
","@@ -153,8 +153,9 @@ 
 bool has_tensorimpl(const char* name) {
   return has_aten_tensor_impl(name) || has_caffe2_tensor_impl(name);
 }
-const c10::Type& get_py_torch_type(const char* name) {
-  return c10::typeMetaToScalarType(torch::getTHPDtype(name));
+const c10::Type& get_py_torch_type(const char* name, bool allow_typevars=false) {
+  auto dtype = torch::getTHPDtype(name);
+  return c10::typeMetaToScalarType(dtype, allow_typevars);
 }"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/jit/frontend/function_schema_parser.cpp,patch_7,"-        return parseSingleConstant(arg_type, kind);
","+        return parseSingleConstant(arg_type, kind, real_type);
","
--- a/codegen.py
+++ b/codegen.py
@@ -2157,11 +2157,11 @@ class CppWrapperCodeGen(WrapperCodeGen):
                         new_int_args.extend([str(a) for a in arg])
                 else:
                     assert isinstance(
-                        arg_type.getElementType(), static_arg_types
+                        arg_type.getElementType(), static_arg_types  # type: ignore[arg-type]
                     ), f""Fall through arguments must be one of static_arg_types, got {type(arg_type)}""
             else:
                 assert isinstance(
-                    arg_type, static_arg_types
+                    arg_type, static_arg_types  # type: ignore[arg-type]
                 ), f""Fall through arguments must be one of static_arg_types, got {type(arg_type)}""
 
         for arg, arg_type in zip(raw_args, arg_types):   
 
-        return parseSingleConstant(arg_type, kind);
+        return parseSingleConstant(arg_type, kind, real_type);"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/jit/frontend/function_schema_parser.cpp,patch_8,"-          IValue v = parseSingleConstant(*elem_type, elem_type->kind());
-          return parseConstantList(*elem_type, elem_type->kind());
","+        auto real_elem_type = real_type.containedType(0);
+          IValue v = parseSingleConstant(
+              *elem_type, elem_type->kind(), *real_elem_type);
+          return parseConstantList(
+              *elem_type, elem_type->kind(), *real_elem_type);
+            real_type,
","@@ -82,12 +82,27 @@ TypePtr SchemaTypeParser::parseBaseType() {
 
   auto it = type_map.find(text);
   if (it == type_map.end()) {
-    if (!text.empty() && islower(text[0])) {
+    if (allow_typevars_ && !text.empty() && islower(text[0])) {
       // lower case identifiers that are not otherwise valid types
       // are treated as type variables
       return c10::TypeFactory::createNamed<VarType>(text);
     }
-    throw ErrorReport(tok.range) << ""unknown type specifier"";
+    if (text == ""double"") {
+      throw ErrorReport(tok.range)
+          << ""Use `float` instead of `double` in an operator\'s schema string. ""
+             ""`float` in schema corresponds to the double type in C++"";
+    }
+    if (text == ""int64_t"") {
+      throw ErrorReport(tok.range)
+          << ""Use `SymInt` or `int` instead of `int64_t` in an operator\'s schema string. ""
+             ""`SymInt` corresponds to c10::SymInt in C++ while `int` in schema corresponds ""
+             ""to the int64_t type in C++."";
+    }
+    throw ErrorReport(tok.range)
+        << ""unknown type specifier. Common valid schema types include ""
+           ""Tensor, SymInt, int, float, bool, Scalar; ""
+           ""for a full list, please see ""
+           ""https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#func "";
   }
   return it->second;
 }
"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/jit/frontend/schema_type_parser.cpp,patch_0,,"+#undef DEFINE_SCALAR_TYPE
","['@@ -476,8 +476,8 @@ def _if_scalar_type_as(self, tensor):\n \n \n @_beartype.beartype\n-def _is_none(x: _C.Value) -> bool:\n-    return x.node().mustBeNone()\n+def _is_none(x: Any) -> bool:\n+    return x is None or (x.node().mustBeNone() if isinstance(x, _C.Value) else False)\n \n \n @_beartype.beartype\n',
 '+#undef DEFINE_SCALAR_TYPE\n']"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/utils/tensor_dtypes.cpp,patch_0,"-std::pair<std::string, std::string> getDtypeNames(at::ScalarType scalarType) {
-  switch (scalarType) {
-    case at::ScalarType::UInt1:
-      return std::make_pair(""uint1"", ""bit"");
-    case at::ScalarType::UInt2:
-      return std::make_pair(""uint2"", """");
-    case at::ScalarType::UInt3:
-      return std::make_pair(""uint3"", """");
-    case at::ScalarType::UInt4:
-      return std::make_pair(""uint4"", """");
-    case at::ScalarType::UInt5:
-      return std::make_pair(""uint5"", """");
-    case at::ScalarType::UInt6:
-      return std::make_pair(""uint6"", """");
-    case at::ScalarType::UInt7:
-      return std::make_pair(""uint7"", """");
-    case at::ScalarType::Byte:
-      // no ""byte"" because byte is signed in numpy and we overload
-      // byte to mean bool often
-      return std::make_pair(""uint8"", """");
-    case at::ScalarType::UInt16:
-      return std::make_pair(""uint16"", """");
-    case at::ScalarType::UInt32:
-      return std::make_pair(""uint32"", """");
-    case at::ScalarType::UInt64:
-      return std::make_pair(""uint64"", """");
-    case at::ScalarType::Char:
-      // no ""char"" because it is not consistently signed or unsigned; we want
-      // to move to int8
-      return std::make_pair(""int8"", """");
-    case at::ScalarType::Double:
-      return std::make_pair(""float64"", ""double"");
-    case at::ScalarType::Float:
-      return std::make_pair(""float32"", ""float"");
-    case at::ScalarType::Int:
-      return std::make_pair(""int32"", ""int"");
-    case at::ScalarType::Long:
-      return std::make_pair(""int64"", ""long"");
-    case at::ScalarType::Short:
-      return std::make_pair(""int16"", ""short"");
-    case at::ScalarType::Half:
-      return std::make_pair(""float16"", ""half"");
-    case at::ScalarType::ComplexHalf:
-      return std::make_pair(""complex32"", ""chalf"");
-    case at::ScalarType::ComplexFloat:
-      return std::make_pair(""complex64"", ""cfloat"");
-    case at::ScalarType::ComplexDouble:
-      return std::make_pair(""complex128"", ""cdouble"");
-    case at::ScalarType::Bool:
-      return std::make_pair(""bool"", """");
-    case at::ScalarType::QInt8:
-      return std::make_pair(""qint8"", """");
-    case at::ScalarType::QUInt8:
-      return std::make_pair(""quint8"", """");
-    case at::ScalarType::QInt32:
-      return std::make_pair(""qint32"", """");
-    case at::ScalarType::BFloat16:
-      return std::make_pair(""bfloat16"", """");
-    case at::ScalarType::QUInt4x2:
-      return std::make_pair(""quint4x2"", """");
-    case at::ScalarType::QUInt2x4:
-      return std::make_pair(""quint2x4"", """");
-    case at::ScalarType::Bits1x8:
-      return std::make_pair(""bits1x8"", """");
-    case at::ScalarType::Bits2x4:
-      return std::make_pair(""bits2x4"", """");
-    case at::ScalarType::Bits4x2:
-      return std::make_pair(""bits4x2"", """");
-    case at::ScalarType::Bits8:
-      return std::make_pair(""bits8"", """");
-    case at::ScalarType::Bits16:
-      return std::make_pair(""bits16"", """");
-    case at::ScalarType::Float8_e5m2:
-      return std::make_pair(""float8_e5m2"", """");
-    case at::ScalarType::Float8_e4m3fn:
-      return std::make_pair(""float8_e4m3fn"", """");
-    case at::ScalarType::Float8_e5m2fnuz:
-      return std::make_pair(""float8_e5m2fnuz"", """");
-    case at::ScalarType::Float8_e4m3fnuz:
-      return std::make_pair(""float8_e4m3fnuz"", """");
-    default:
-      throw std::runtime_error(""Unimplemented scalar type"");
-  }
-}
-
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,NO,torch/csrc/utils/tensor_dtypes.cpp,patch_1,"-    auto [primary_name, legacy_name] = getDtypeNames(scalarType);
","+#undef DEFINE_SCALAR_TYPE
+
+    auto [primary_name, legacy_name] = c10::getDtypeNames(scalarType);
","@@ -1765,7 +1765,11 @@
       },
       py::arg(""input""),
       py::arg(""parse_tensor_constants"") = false);
-  m.def(""parse_schema"", parseSchema);
+  m.def(
+      ""parse_schema"",
+      &parseSchema,
+      py::arg(""schema""),
+      py::arg(""allow_typevars"") = true);
   m.def(""unify_type_list"", [](const std::vector<TypePtr>& types) {
     std::ostringstream s;
     auto type = unifyTypeList(types, s);"
0,https://github.com/PyTorch/PyTorch/commit/ff1e3ff5a503a520c1a310c8e72a383657f9a4bc,YES,aten/src/ATen/native/ForeachUtils.h,patch_0,"-    ArrayRef<TensorList> tensorLists) {
-    return tensor.dtype() == expected_dtype &&
",No
0,https://github.com/PyTorch/PyTorch/commit/ff1e3ff5a503a520c1a310c8e72a383657f9a4bc,YES,aten/src/ATen/native/cuda/ForeachBinaryOpList.cu,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/ff1e3ff5a503a520c1a310c8e72a383657f9a4bc,YES,aten/src/ATen/native/cuda/ForeachBinaryOpList.cu,patch_1,"-template <typename T>
-struct Identity {
-  __device__ __forceinline__ T operator()(const T& x) {
-    return x;
-  if (!can_use_fast_route(
-          self, src, /* does_op_promote_integer_inputs_to_float */ false)) {
",No
0,https://github.com/PyTorch/PyTorch/commit/ff1e3ff5a503a520c1a310c8e72a383657f9a4bc,YES,aten/src/ATen/native/cuda/ForeachBinaryOpList.cu,patch_2,"-        multi_tensor_apply<2>(
-            tensor_lists,
-            UnaryOpFunctor<
-                scalar_t,
-                /* depth */ 2,
-                /* r_args_depth */ 1,
-                /* res_arg_index */ 1>(),
-            Identity<opmath_t>());
",No
0,https://github.com/PyTorch/PyTorch/commit/16e539e0e68a9a7c100e6b4d7d4ba20d67fad2eb,YES,aten/src/ATen/TensorIndexing.h,patch_0,"-        size >= -index && size > index,
","+    // Note: `size >= -index` is not equivalent to `size > -1 - index` if index
+    // is INT64_MIN For std::numeric_limits<int64_t>::min() result of unary
+    // minus is undefined by the standard but in practice is equal to self. On
+    // the other hand, indexing wraping is valid for all negative int64_t
+    // values, as x[INT64_MIN] is the same as x[INT64_MAX]
+        size > -1 - index && size > index,
","@@ -311,6 +311,6 @@ def stride_at(var: sympy.Symbol, index: sympy.Expr):
     return sympy.simplify(new_index - index)
 
 
-        size >= -index && size > index,
+    // Note: `size >= -index` is not equivalent to `size > -1 - index` if index
+    // is INT64_MIN For std::numeric_limits<int64_t>::min() result of unary
+    // minus is undefined by the standard but in practice is equal to self. On
+    // the other hand, indexing wraping is valid for all negative int64_t
+    // values, as x[INT64_MIN] is the same as x[INT64_MAX]
+        size > -1 - index && size > index,"
0,https://github.com/PyTorch/PyTorch/commit/16e539e0e68a9a7c100e6b4d7d4ba20d67fad2eb,YES,aten/src/ATen/native/TensorShape.cpp,patch_0,"-  if (size < -index || size <= index) {
","+  // Note: `size < -index` is not equivalent to `size <= -1 - index` if index is INT64_MIN
+  // For std::numeric_limits<int64_t>::min() result of unary minus is undefined by the standard
+  // but in practice is equal to self. On the other hand, indexing wraping is valid for all
+  // negative int64_t values, as x[INT64_MIN] is the same as x[INT64_MAX]
+  if (size <= -1 - index || size <= index) {
","@@ -311,6 +311,69 @@ def stride_at(var: sympy.Symbol, index: sympy.Expr):
     return sympy.simplify(new_index - index)
 
 
+@functools.lru_cache
+def simplify_index_in_vec_range(index: sympy.Expr, var: sympy.Expr, vec_length: int):
+    """"""
+    Simplifies the index expression within the range of a vectorized loop.
+    Given a vectorized loop variable `var` in the range of a loop with `vec_length`,
+    this function transforms the `index` into an equivalent form. It handles
+    simplifications for cases where `var` can be expressed as `vec_length * a + b`,
+    where `b` ranges from 0 to `vec_length - 1`. The function reduces occurrences
+    of `FloorDiv` and `ModularIndexing` in the `index` with best-effort optimizations.
+
+    NOTE:
+    The simplified index expression is intended for analysis purposes only, not
+    for code generation. It replaces `FloorDiv` and `ModularIndexing` with free variables
+    which are not dependent on the loop variable `var` in the vectorized range. Check
+    https://github.com/pytorch/pytorch/pull/117221#discussion_r1449746217 for more details.
+
+    Examples:
+    1. If `var` is `x3` and `vec_length` is 16, and `x3 = 16*a + b`, then
+       `FloorDiv(x3, div)` or `ModularIndexing(x3, div, mod)` becomes a free variable
+       when `div` is divisible by 16.
+    2. `ModularIndexing(x3, 1, mod)` can be simplified to `x3 + c` where `c` is a free
+       variable when `mod` is divisible by 16.
+    """"""
+
+    div_freevar_id = 0
+    mod_freevar_id = 0
+
+    def visit_indexing_div(divisor):
+        nonlocal div_freevar_id
+        result = FloorDiv(var, divisor)
+        if sympy.gcd(divisor, vec_length) == vec_length:
+            result = sympy.Symbol(f""{var}_div_c{div_freevar_id}"")
+            div_freevar_id += 1
+        return result
+
+    def visit_modular_indexing(divisor, modulus):
+        nonlocal mod_freevar_id
+        result = ModularIndexing(var, divisor, modulus)
+        if sympy.gcd(divisor, vec_length) == vec_length:
+            result = sympy.Symbol(f""{var}_mod_c{mod_freevar_id}"")
+            mod_freevar_id += 1
+        elif divisor == 1 and sympy.gcd(modulus, vec_length) == vec_length:
+            result = var + sympy.Symbol(f""{var}_mod_c{mod_freevar_id}"")
+            mod_freevar_id += 1
+        return result
+
+    original_index = index
+
+    div = sympy.Wild(""divisor"")
+    if index.has(FloorDiv):
+        index = index.replace(FloorDiv(var, div), visit_indexing_div)
+
+    mod = sympy.Wild(""modulus"")
+    if index.has(ModularIndexing):
+        index = index.replace(ModularIndexing(var, div, mod), visit_modular_indexing)
+
+    index = sympy.simplify(index)
+    if index != original_index:
+        return simplify_index_in_vec_range(index, var, vec_length)
+
+    return index
+
+
 class CppPrinter(ExprPrinter):
     def _print_Integer(self, expr):
         return f""{int(expr)}L""
      "
0,https://github.com/PyTorch/PyTorch/commit/41902a6ebc1806e7f4d6ce1da604cc9921c6515e,NO,torch/_dynamo/eval_frame.py,patch_0,,"+def always_false():
+    return False
+
+
","--- a/overheads.py
+++ b/overheads.py
@@ -1901,6 +1923,27 @@ def forward(self, arg0_1):
         def f(x, y):
             return control_flow.cond(x.shape[0] > 4, true_fn, false_fn, [y])
 
+        example_inputs = (
+            torch.ones(3, 2, 4, requires_grad=True),
+            torch.ones(4, requires_grad=True),
+        )
+        # Due to x.shape[0] can be statically evaluated to be False, we can evaluate
+        # the backward.
+        f(*example_inputs).sum().backward()
+
+        # Ensure no error is thrown when not running backward
+        f(*example_inputs)
+
+    def test_cond_autograd_fail(self):
+        def true_fn(x):
+            return x.cos()
+
+        def false_fn(x):
+            return x.sin()
+
+        def f(x, y):
+            return control_flow.cond(x.sum() > 4, true_fn, false_fn, [y])
+
     example_inputs = (
         torch.ones(3, 2, 4, requires_grad=True),
         torch.ones(4, requires_grad=True),"
0,https://github.com/PyTorch/PyTorch/commit/41902a6ebc1806e7f4d6ce1da604cc9921c6515e,NO,torch/_dynamo/eval_frame.py,patch_1,"-            if torch.fx._symbolic_trace.is_fx_tracing() and not isinstance(
-                self, DisableContext
-            ):
","+        if isinstance(self, DisableContext):
+            is_jit_tracing = always_false
+            is_fx_tracing = always_false
+        else:
+            is_jit_tracing = torch._C._is_tracing
+            is_fx_tracing = torch.fx._symbolic_trace.is_fx_tracing
+
+            if is_fx_tracing():
","@@ -1140,7 +1144,7 @@ def compile_fx(
         torch._guards.TracingContext.get() or torch._guards.TracingContext(fake_mode)
     )
 
-    with V.set_fake_mode(fake_mode), torch._guards.tracing(
+    with V.set_fake_mode(fake_mode), torch._guards.tracing(  # type: ignore[call-arg]
         tracing_context
     ), compiled_autograd.disable():
         return aot_autograd("
0,https://github.com/PyTorch/PyTorch/commit/41902a6ebc1806e7f4d6ce1da604cc9921c6515e,NO,torch/_dynamo/eval_frame.py,patch_2,"-            if torch.jit.is_tracing():
","+            if is_jit_tracing():
","['@@ -1,3 +1,3 @@\n if torch.jit.is_tracing():\n+if is_jit_tracing():\n']"
0,https://github.com/PyTorch/PyTorch/commit/e385bf8ef8f84968c0ff6e47ca5e06d4948e8e62,YES,torch/_inductor/codegen/common.py,patch_0,"-    REDUCE_TO_SINGLE_ELEMENT = auto()
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/e385bf8ef8f84968c0ff6e47ca5e06d4948e8e62,YES,torch/_inductor/codegen/cpp.py,patch_0,"-            BackendFeature.REDUCE_TO_SINGLE_ELEMENT,
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/e385bf8ef8f84968c0ff6e47ca5e06d4948e8e62,YES,torch/_inductor/codegen/halide.py,patch_0,"-                BackendFeature.REDUCE_TO_SINGLE_ELEMENT,
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/e385bf8ef8f84968c0ff6e47ca5e06d4948e8e62,YES,torch/_inductor/ir.py,patch_0,"-            not V.graph.has_feature(device, BackendFeature.REDUCE_TO_SINGLE_ELEMENT)
","+            is_gpu(get_device_type(device))
","@@ -471,11 +472,41 @@ class TestCheckpoint(TestCase):
         self.assertTrue(isinstance(device_states[0], torch.Tensor))
         self.assertTrue(isinstance(device_states[1], torch.Tensor))
 
-    def test_infer_device_state_recursive(self):
+    def test_infer_device_state_recursive_meta(self):
         inp = {'foo' : torch.rand(10, device=""meta"")}
         device_type = _infer_device_type(inp)
         self.assertEqual(""meta"", device_type)
 
+    @unittest.skipIf(not TEST_MULTIGPU, ""multi-GPU not supported"")
+    def test_infer_device_state_recursive_multi_cuda(self):
+        # Check that no warning is issued for either cuda:0, cuda:1 or
+        # cuda:0, cuda:0 cases since they are both the same device type
+        inp = {'foo' : torch.rand(10, device=""cuda:0""), 'bar': [torch.rand(10, device=""cuda:1"")]}
+        with warnings.catch_warnings():
+            warnings.simplefilter(""error"")
+            device_type = _infer_device_type(inp)
+            self.assertEqual(""cuda"", device_type)
+        inp = {'foo' : torch.rand(10, device=""cuda:0""), 'bar': [torch.rand(10, device=""cuda:0"")]}
+        with warnings.catch_warnings():
+            warnings.simplefilter(""error"")
+            device_type = _infer_device_type(inp)
+            self.assertEqual(""cuda"", device_type)
+        # Check that a warning is issued for cuda:0, meta and that it includes
+        # device type information
+        inp = {'foo' : torch.rand(10, device=""cuda:0""), 'bar': [torch.rand(10, device=""meta"")]}
+        with warnings.catch_warnings(record=True) as w:
+            device_type = _infer_device_type(inp)
+            self.assertEqual(""cuda"", device_type)
+        self.assertEqual(len(w), 1)
+        warning_msg = str(w[-1].message)
+        self.assertTrue(
+            ""Tensor arguments, excluding CPU tensors, are detected on at least two types of devices""
+            in warning_msg
+        )
+        self.assertTrue(""Device types: ['cuda', 'meta']"" in warning_msg)
+        self.assertTrue(""first device type: cuda"" in warning_msg)
+        
 
 class TestDataLoaderUtils(TestCase):
     MAX_TIMEOUT_IN_SECOND = 300"
0,https://github.com/PyTorch/PyTorch/commit/e31038d574712d383fdc4c2f1bb63fc82f256ed0,YES,aten/src/ATen/native/TensorAdvancedIndexing.cpp,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/3db0095ea2f351b9935de036a03f29e619093e16,NO,torch/ao/quantization/pt2e/prepare.py,patch_0,"-    _get_arg_as_input_act_obs_or_fq,
-    _get_output_act_obs_or_fq,
-    _get_dtype_and_is_dynamic,
",,"@@ -0,0 +1,1 @@
+24a3fe9cb57e5cda3c923df29743f9767194cc27"
0,https://github.com/PyTorch/PyTorch/commit/3db0095ea2f351b9935de036a03f29e619093e16,NO,torch/ao/quantization/pt2e/prepare.py,patch_1,"-    QuantizationAnnotation,
",No
0,https://github.com/PyTorch/PyTorch/commit/3db0095ea2f351b9935de036a03f29e619093e16,NO,torch/ao/quantization/pt2e/prepare.py,patch_2,"-    quantization_annotation = node.meta.get(""quantization_annotation"", QuantizationAnnotation())
-    arg_as_input_act_obs_or_fq = _get_arg_as_input_act_obs_or_fq(arg, node, named_modules, obs_or_fq_map, is_qat)
-    arg_as_input_target_dtype, arg_as_input_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_input_act_obs_or_fq)
-
-    arg_as_output_act_obs_or_fq = _get_output_act_obs_or_fq(arg, named_modules, obs_or_fq_map, is_qat)
-    arg_as_output_target_dtype, arg_as_output_target_is_dynamic = _get_dtype_and_is_dynamic(arg_as_output_act_obs_or_fq)
-
-    if arg_as_input_target_is_dynamic or arg_as_input_target_dtype not in [torch.float, None]:
-        if arg_as_input_target_dtype == arg_as_output_target_dtype and \
-           arg_as_input_target_is_dynamic == arg_as_output_target_is_dynamic:
-            assert _is_activation_post_process_node(arg, named_modules)
-            assert arg_as_input_act_obs_or_fq is not None
-            observed_arg = arg.args[0]
-            assert isinstance(observed_arg, Node), f""expect observed argument to be a Node, but got: {type(observed_arg)}""
-            assert observed_arg in obs_or_fq_map, \
-                f""can't find a sharing group for node: {observed_arg}""
-            # reuse the existing obs/fq
-            arg_as_input_act_obs_or_fq = obs_or_fq_map[observed_arg]
-            # we don't need to insert new observer node
-            new_arg = arg
-        else:
-            # skip inserting new observers if there is an observer inserted for the arg before
-            # that has the same dtype that we want to insert here
-            # alternatively we could have a dedup pass after we insert all observers to deduplicate
-            # observers
-            # Example:
-            # arg -> existing_obs -> conv1
-            #    \ -> conv2
-            #
-            # instead of inserting new observers we will have:
-            # arg -> existing_obs -> conv1
-            #                   \ -> conv2
-            existing_obs_node = None
-            for maybe_obs_node in arg.users.keys():
-                if maybe_obs_node.op == 'call_module':
-                    maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]
-                    if (
-                        type(maybe_obs_mod) == type(arg_as_input_act_obs_or_fq) and
-                        maybe_obs_mod.dtype == arg_as_input_target_dtype
-                    ):
-                        arg_as_input_act_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]
-                        existing_obs_node = maybe_obs_node
-                        break
-
-            assert arg_as_input_act_obs_or_fq is not None
-            if existing_obs_node is None:
-                maybe_observed_arg = arg
-                # When quantizing two layers with different configs we can have
-                # conv2d (int8) -> avgpool(uint8)
-                # In this case observer insertion for avgpool will come here but the input
-                # to avgpool will be output observer of conv2d
-                # Now the obs map that we update must correspond to the original input of
-                # avgpool and not the output obs of conv2d
-                # This is because when referring to the edge, quantizer would refer to
-                # original input and not the observed one.
-                while _is_activation_post_process_node(arg, named_modules):
-                    arg = arg.args[0]  # type: ignore[assignment]
-                arg_as_input_act_obs_or_fq = obs_or_fq_map[(arg, node)]
-                new_obs_node = _insert_obs_or_fq(
-                    maybe_observed_arg, arg_as_input_act_obs_or_fq, model, named_modules, model.graph)
-                # override this arg to be the observed arg
-                new_arg = new_obs_node
-            else:
-                new_arg = existing_obs_node
","+    # find the original `arg` node to the current node, skipping inserted observer/fake_quant nodes
+    original_arg = arg
+    while _is_activation_post_process_node(original_arg, named_modules):
+        original_arg = original_arg.args[0]  # type: ignore[assignment]
+    assert isinstance(original_arg, Node), f""expect original argument to be a Node, but got: {type(original_arg)}""
+
+    input_edge = (original_arg, node)
+    if input_edge not in obs_or_fq_map:
+        return new_arg
+    # input_edge needs to be observed
+    input_edge_obs_or_fq = obs_or_fq_map[input_edge]
+    if input_edge_obs_or_fq is None:
+        return new_arg
+
+    arg_as_output_obs_or_fq = obs_or_fq_map.get(original_arg, None)
+    # the arg is observed as the output and is using the same instance as the input_edge
+    # we'll reuse the inserted observer/fake_quant
+    if arg_as_output_obs_or_fq is not None and id(arg_as_output_obs_or_fq) == id(input_edge_obs_or_fq):
+        return new_arg
+
+    # otherwise, we'll insert a new observer/fake_quant node
+
+    existing_obs_node = None
+    # skip inserting new observers if there is an observer inserted for the arg before
+    # that has the same dtype that we want to insert here
+    # alternatively we could have a dedup pass after we insert all observers to deduplicate
+    # observers
+    # Example:
+    # conv1 -> obs1 -> existing_obs -> conv2
+    #             \ -> conv3
+    #
+    # instead of inserting new observers we will have:
+    # conv1 -> obs1 -> existing_obs -> conv2
+    #                            \ -> conv3
+    for maybe_obs_node in arg.users.keys():
+        if not _is_activation_post_process_node(maybe_obs_node, named_modules):
+            continue
+        maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]
+        if (
+            type(maybe_obs_mod) == type(input_edge_obs_or_fq) and
+            maybe_obs_mod.dtype == input_edge_obs_or_fq.dtype
+        ):
+            input_edge_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]
+            existing_obs_node = maybe_obs_node
+            break
+
+    if existing_obs_node is None:
+        new_arg = _insert_obs_or_fq(arg, input_edge_obs_or_fq, model, named_modules, model.graph)
+    else:
+        new_arg = existing_obs_node
","-    quantization_annotation = node.meta.get(""quantization_annotation"", QuantizationAnnotation())
+    # find the original `arg` node to the current node, skipping inserted observer/fake_quant nodes
+    original_arg = arg
+    while _is_activation_post_process_node(original_arg, named_modules):
+        original_arg = original_arg.args[0]  # type: ignore[assignment]
+    assert isinstance(original_arg, Node), f""expect original argument to be a Node, but got: {type(original_arg)}""
+
+    input_edge = (original_arg, node)
+    if input_edge not in obs_or_fq_map:
+        return new_arg
+    # input_edge needs to be observed
+    input_edge_obs_or_fq = obs_or_fq_map[input_edge]
+    if input_edge_obs_or_fq is None:
+        return new_arg
+
+    arg_as_output_obs_or_fq = obs_or_fq_map.get(original_arg, None)
+    # the arg is observed as the output and is using the same instance as the input_edge
+    # we'll reuse the inserted observer/fake_quant
+    if arg_as_output_obs_or_fq is not None and id(arg_as_output_obs_or_fq) == id(input_edge_obs_or_fq):
+        return new_arg
+
+    # otherwise, we'll insert a new observer/fake_quant node
+
+    existing_obs_node = None
+    # skip inserting new observers if there is an observer inserted for the arg before
+    # that has the same dtype that we want to insert here
+    # alternatively we could have a dedup pass after we insert all observers to deduplicate
+    # observers
+    # Example:
+    # conv1 -> obs1 -> existing_obs -> conv2
+    #             \\ -> conv3
+    #
+    # instead of inserting new observers we will have:
+    # conv1 -> obs1 -> existing_obs -> conv2
+    #                            \\ -> conv3
+    for maybe_obs_node in arg.users.keys():
+        if not _is_activation_post_process_node(maybe_obs_node, named_modules):
+            continue
+        maybe_obs_mod = named_modules[maybe_obs_node.target]  # type: ignore[index]
+        if (
+            type(maybe_obs_mod) == type(input_edge_obs_or_fq) and
+            maybe_obs_mod.dtype == input_edge_obs_or_fq.dtype
+        ):
+            input_edge_obs_or_fq = maybe_obs_mod  # type: ignore[assignment]
+            existing_obs_node = maybe_obs_node
+            break
+
+    if existing_obs_node is None:
+        new_arg = _insert_obs_or_fq(arg, input_edge_obs_or_fq, model, named_modules, model.graph)
+    else:
+        new_arg = existing_obs_node"
0,https://github.com/PyTorch/PyTorch/commit/3db0095ea2f351b9935de036a03f29e619093e16,NO,torch/ao/quantization/quantizer/quantizer.py,patch_0,"-    input_qspec_map: Dict[Node, QuantizationSpecBase] = field(default_factory=dict)
",No
0,https://github.com/PyTorch/PyTorch/commit/3db0095ea2f351b9935de036a03f29e619093e16,NO,torch/ao/quantization/quantizer/xnnpack_quantizer.py,patch_0,"-    bias_observer_or_fake_quant_ctr: _ObserverOrFakeQuantizeConstructor = (
-        PlaceholderObserver
-    )
-    bias_quantization_spec = QuantizationSpec(
-        dtype=torch.float, observer_or_fake_quant_ctr=bias_observer_or_fake_quant_ctr
-    )
",No
0,https://github.com/PyTorch/PyTorch/commit/8629939a51813def63363ff3bdfe1a6e56c69e18,NO,c10/macros/Macros.h,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/db7a3cc436df016cf4d1b9edf7557621f5cdbf47,YES,c10/cuda/CUDACachingAllocator.cpp,patch_0,,"+  void* nvml_handle = DriverAPI::get_nvml_handle();
+  if (!nvml_handle) {
+    return """";
+  }
","@@ -174,6 +239,30 @@ public:
     impl.privateuse1_dispatch_ptr = reinterpret_cast<void*>(fn_ptr);
   }
 
+  // Returns true if the dispatcher has a kernel registered for this device
+  // type.
+  bool is_device_supported(const c10::DeviceType device_type) {
+    auto result = impl.try_get_call_ptr(device_type
+      , reinterpret_cast<void*>(DEFAULT)
+#ifdef HAVE_AVX512_CPU_DEFINITION
+      , reinterpret_cast<void*>(AVX512)
+#endif
+#ifdef HAVE_AVX2_CPU_DEFINITION
+      , reinterpret_cast<void*>(AVX2)
+#endif
+#ifdef HAVE_VSX_CPU_DEFINITION
+      , reinterpret_cast<void*>(VSX)
+#endif
+#ifdef HAVE_ZVECTOR_CPU_DEFINITION
+      , reinterpret_cast<void*>(ZVECTOR)
+#endif
+      );
+    if (std::holds_alternative<ErrorType>(result)){
+      return false;
+    }
+    return true;
+  };
+
   static TORCH_API FnPtr DEFAULT;
 #ifdef HAVE_AVX512_CPU_DEFINITION
   static TORCH_API FnPtr AVX512;"
0,https://github.com/PyTorch/PyTorch/commit/db7a3cc436df016cf4d1b9edf7557621f5cdbf47,YES,c10/cuda/driver_api.cpp,patch_0,,"+#include <c10/util/CallOnce.h>
","@@ -1 +1,2 @@
 #include <c10/util/CallOnce.h>
+#include <c10/util/Array.h>"
0,https://github.com/PyTorch/PyTorch/commit/db7a3cc436df016cf4d1b9edf7557621f5cdbf47,YES,c10/cuda/driver_api.cpp,patch_1,"-DriverAPI create_driver_api() {
-#define OPEN_LIBRARIES(name, n)               \
-  void* handle_##n = dlopen(name, RTLD_LAZY); \
-  TORCH_INTERNAL_ASSERT(handle_##n);
-  C10_FORALL_DRIVER_LIBRARIES(OPEN_LIBRARIES)
-#undef OPEN_LIBRARIES
-#define LOOKUP_ENTRY(name, n)                              \
-  r.name##_ = ((decltype(&name))dlsym(handle_##n, #name)); \
-  C10_FORALL_DRIVER_API(LOOKUP_ENTRY)
-#undef LOOKUP_ENTRY
","+DriverAPI create_driver_api() {
+  void* handle_0 = dlopen(""libcuda.so"", RTLD_LAZY | RTLD_NOLOAD);
+  TORCH_INTERNAL_ASSERT(handle_0);
+  void* handle_1 = DriverAPI::get_nvml_handle();
+#define LOOKUP_LIBCUDA_ENTRY(name)                       \
+  r.name##_ = ((decltype(&name))dlsym(handle_0, #name)); \
+  TORCH_INTERNAL_ASSERT(r.name##_)
+  C10_LIBCUDA_DRIVER_API(LOOKUP_LIBCUDA_ENTRY)
+#undef LOOKUP_LIBCUDA_ENTRY
+
+  if (handle_1) {
+#define LOOKUP_NVML_ENTRY(name)                          \
+  r.name##_ = ((decltype(&name))dlsym(handle_1, #name)); \
+    C10_NVML_DRIVER_API(LOOKUP_NVML_ENTRY)
+#undef LOOKUP_NVML_ENTRY
+  }
+void* DriverAPI::get_nvml_handle() {
+  static c10::once_flag once;
+  static void* handle_1;
+  c10::call_once(
+      once, [] { handle_1 = dlopen(""libnvidia-ml.so.1"", RTLD_LAZY); });
+  return handle_1;
+}
+
","@@ -15,7 +15,7 @@
     import os
     import torch
     from torch.testing._internal.common_utils import TestCase, TEST_WITH_ROCM, TEST_MKL, \
         skipCUDANonDefaultStreamIf, TEST_WITH_ASAN, TEST_WITH_UBSAN, TEST_WITH_TSAN, \
-        IS_SANDCASTLE, IS_FBCODE, IS_REMOTE_GPU, IS_WINDOWS, TEST_MPS, TEST_XPU, \
+        IS_SANDCASTLE, IS_FBCODE, IS_REMOTE_GPU, IS_WINDOWS, TEST_MPS, TEST_XPU, TEST_HPU, \
         _TestParametrizer, compose_parametrize_fns, dtype_name, \
         TEST_WITH_MIOPEN_SUGGEST_NHWC, NATIVE_DEVICES, skipIfTorchDynamo, \
         get_tracked_input, clear_tracked_input, PRINT_REPRO_ON_FAILURE, \


-DriverAPI create_driver_api() {
-#define OPEN_LIBRARIES(name, n)               \
-  void* handle_##n = dlopen(name, RTLD_LAZY); \
-  TORCH_INTERNAL_ASSERT(handle_##n);
-  C10_FORALL_DRIVER_LIBRARIES(OPEN_LIBRARIES)
-#undef OPEN_LIBRARIES
-#define LOOKUP_ENTRY(name, n)                              \
-  r.name##_ = ((decltype(&name))dlsym(handle_##n, #name)); \
-  C10_FORALL_DRIVER_API(LOOKUP_ENTRY)
-#undef LOOKUP_ENTRY
+DriverAPI create_driver_api() {
+  void* handle_0 = dlopen(""libcuda.so"", RTLD_LAZY | RTLD_NOLOAD);
+  TORCH_INTERNAL_ASSERT(handle_0);
+  void* handle_1 = DriverAPI::get_nvml_handle();
+#define LOOKUP_LIBCUDA_ENTRY(name)                       \
+  r.name##_ = ((decltype(&name))dlsym(handle_0, #name)); \
+  TORCH_INTERNAL_ASSERT(r.name##_)
+  C10_LIBCUDA_DRIVER_API(LOOKUP_LIBCUDA_ENTRY)
+#undef LOOKUP_LIBCUDA_ENTRY
+
+  if (handle_1) {
+#define LOOKUP_NVML_ENTRY(name)                          \
+  r.name##_ = ((decltype(&name))dlsym(handle_1, #name)); \
+    C10_NVML_DRIVER_API(LOOKUP_NVML_ENTRY)
+#undef LOOKUP_NVML_ENTRY
+  }
+void* DriverAPI::get_nvml_handle() {
+  static c10::once_flag once;
+  static void* handle_1;
+  c10::call_once(
+      once, [] { handle_1 = dlopen(""libnvidia-ml.so.1"", RTLD_LAZY); });
+  return handle_1;
+}
+"
0,https://github.com/PyTorch/PyTorch/commit/db7a3cc436df016cf4d1b9edf7557621f5cdbf47,YES,c10/cuda/driver_api.h,patch_0,"-#define C10_FORALL_DRIVER_LIBRARIES(_) \
-  _(""libcuda.so"", 0)                   \
-  _(""libnvidia-ml.so.1"", 1)
-#define C10_FORALL_DRIVER_API(_)         \
-  _(cuMemAddressReserve, 0)              \
-  _(cuMemRelease, 0)                     \
-  _(cuMemMap, 0)                         \
-  _(cuMemAddressFree, 0)                 \
-  _(cuMemSetAccess, 0)                   \
-  _(cuMemUnmap, 0)                       \
-  _(cuMemCreate, 0)                      \
-  _(cuGetErrorString, 0)                 \
-  _(nvmlInit_v2, 1)                      \
-  _(nvmlDeviceGetHandleByPciBusId_v2, 1) \
-  _(nvmlDeviceGetComputeRunningProcesses, 1)
-#define CREATE_MEMBER(name, n) decltype(&name) name##_;
-  C10_FORALL_DRIVER_API(CREATE_MEMBER)
","+#define C10_LIBCUDA_DRIVER_API(_) \
+  _(cuMemAddressReserve)          \
+  _(cuMemRelease)                 \
+  _(cuMemMap)                     \
+  _(cuMemAddressFree)             \
+  _(cuMemSetAccess)               \
+  _(cuMemUnmap)                   \
+  _(cuMemCreate)                  \
+  _(cuGetErrorString)
+#define C10_NVML_DRIVER_API(_)        \
+  _(nvmlInit_v2)                      \
+  _(nvmlDeviceGetHandleByPciBusId_v2) \
+  _(nvmlDeviceGetComputeRunningProcesses)
+#define CREATE_MEMBER(name) decltype(&name) name##_;
+  C10_LIBCUDA_DRIVER_API(CREATE_MEMBER)
+  C10_NVML_DRIVER_API(CREATE_MEMBER)
+  static void* get_nvml_handle();
","--- a/c10/cuda/driver_api.cpp
+++ b/c10/cuda/driver_api.cpp
@@ -174,6 +239,30 @@ public:
     impl.privateuse1_dispatch_ptr = reinterpret_cast<void*>(fn_ptr);
   }
 
+  // Returns true if the dispatcher has a kernel registered for this device
+  // type.
+  bool is_device_supported(const c10::DeviceType device_type) {
+    auto result = impl.try_get_call_ptr(device_type
+      , reinterpret_cast<void*>(DEFAULT)
+#ifdef HAVE_AVX512_CPU_DEFINITION
+      , reinterpret_cast<void*>(AVX512)
+#endif
+#ifdef HAVE_AVX2_CPU_DEFINITION
+      , reinterpret_cast<void*>(AVX2)
+#endif
+#ifdef HAVE_VSX_CPU_DEFINITION
+      , reinterpret_cast<void*>(VSX)
+#endif
+#ifdef HAVE_ZVECTOR_CPU_DEFINITION
+      , reinterpret_cast<void*>(ZVECTOR)
+#endif
+      );
+    if (std::holds_alternative<ErrorType>(result)){
+      return false;
+    }
+    return true;
+  };
+
   static TORCH_API FnPtr DEFAULT;
 #ifdef HAVE_AVX512_CPU_DEFINITION
   static TORCH_API FnPtr AVX512;
 
--- a/c10/cuda/CUDACachingAllocator.cpp
+++ b/c10/cuda/CUDACachingAllocator.cpp
@@ -179,6 +179,26 @@ private:
   void update_size_in_bytes(c10::DeviceIndex device_index, size_t new_size_in_bytes);
 
   void* cuda_device_allocator(int device);
+
+#ifdef USE_NVML
+  // nvml library
+  void release_nvml();
+  void* nvml_handle_;
+  bool has_nvml_;
+
+  // Returns true if nvml library is available.
+  bool query_nvml() {
+    nvmlReturn_t ret = nvmlInit_v2();
+    if (ret != NVML_SUCCESS) {
+      return false;
+    }
+    ret = nvmlDeviceGetHandleByPciBusId_v2(""0000:00:00.0"", &nvml_handle_);
+    if (ret != NVML_SUCCESS) {
+      nvmlShutdown();
+      return false;
+    }
+    return true;
+  }
+#endif
+
 public:
   CUDACachingAllocator()
     : CUDACachingAllocator(/* device_ordinal = */ -1, /* empty_cache = */ true) {}"
0,https://github.com/PyTorch/PyTorch/commit/bb89a9e48c869b3056a42d026f963c2ce4fb41a4,NO,torch/utils/cpp_extension.py,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/eb1d6ed9f9d9731401b04382f526a64e6d27b6e6,YES,torch/_inductor/fx_passes/mkldnn_fusion.py,patch_0,,"+        # mkldnn linear only supports beta=1or0 and alpha=1
+        if linear_node.target == aten.addmm.default:
+            alpha = linear_node.kwargs.get(""alpha"", 1.0)
+            beta = linear_node.kwargs.get(""beta"", 1.0)
+            if (beta != 0.0 and beta != 1.0) or alpha != 1.0:
+                return False
","--- a/test_tensorexpr.py
+++ b/test_tensorexpr.py
@@ -430,19 +478,29 @@ class TestPatternMatcher(TestPatternMatcherBase):
                 else:
                     return self.binary_fn(x1, x2)
 
+        dtypes = [
+            torch.float,
+        ]
+        if torch.ops.mkldnn._is_mkldnn_bf16_supported():
+            dtypes.append(torch.bfloat16)
+        if torch.ops.mkldnn._is_mkldnn_fp16_supported():
+            dtypes.append(torch.float16)
         cl_format = torch.channels_last if dim == 4 else torch.channels_last_3d
         test_memory_format = [torch.contiguous_format, cl_format]
         options = itertools.product(
             binary_list,
             [True, False],
             test_memory_format,
+            dtypes,
         )
 
         for (
             binary_fn,
             has_relu,
             memory_format,
+            dtype,
         ) in options:
+            metrics.reset()
             if dim == 4:
                 x_shape = (1, 3, 56, 56)
             else:"
0,https://github.com/PyTorch/PyTorch/commit/eb1d6ed9f9d9731401b04382f526a64e6d27b6e6,YES,torch/_inductor/fx_passes/mkldnn_fusion.py,patch_1,"-            CallFunction(aten.addmm.default, Arg(), Arg(), Arg()),
","+            CallFunction(
+                aten.addmm.default,
+                Arg(),
+                Arg(),
+                Arg(),
+                beta=KeywordArg(""beta""),
+                alpha=KeywordArg(""alpha""),
+            ),
","-            CallFunction(aten.addmm.default, Arg(), Arg(), Arg()),
+            CallFunction(
+                aten.addmm.default,
+                Arg(),
+                Arg(),
+                Arg(),
+                beta=KeywordArg(""beta""),
+                alpha=KeywordArg(""alpha"", 1),
+            ),"
0,https://github.com/PyTorch/PyTorch/commit/eb1d6ed9f9d9731401b04382f526a64e6d27b6e6,YES,torch/_inductor/fx_passes/mkldnn_fusion.py,patch_2,"-            bias = None if linear_node.target == aten.mm.default else args[0]
","+            bias = (
+                None
+                if linear_node.target == aten.mm.default
+                or (
+                    linear_node.target == aten.addmm.default
+                    and linear_node.kwargs.get(""beta"", 1.0) == 0.0
+                )
+                else args[0]
+            )
","--- a/torch/testing/_internal/common_utils.py
+++ b/torch/testing/_internal/common_utils.py
@@ -536,7 +489,7 @@ class TestPatternMatcher(TestPatternMatcherBase):
         )
         out_feature = 30
         for binary_fn, input_shape, bias, dtype in options:
-            bias = None if linear_node.target == aten.mm.default else args[0]
+            bias = (
+                None
+                if linear_node.target == aten.mm.default
+                or (
+                    linear_node.target == aten.addmm.default
+                    and linear_node.kwargs.get(""beta"", 1.0) == 0.0
+                )
+                else args[0]
+            )  "
0,https://github.com/PyTorch/PyTorch/commit/fe3e6878c4bb2a6001045c179fd7fa9838242558,YES,torch/_dynamo/variables/higher_order_ops.py,patch_0,,"+
+        # Specialize into one of the branches since pred is constant
+        if type(args[0]) is ConstantVariable:
+            log.warning(
+                ""Pred is a Python constant. When used with torch.cond, it executes only one of the branches.""
+                "" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.""
+            )
+            if args[0].as_python_constant():
+                return args[1].call_function(tx, args[3].unpack_var_sequence(tx), {})
+            else:
+                return args[2].call_function(tx, args[3].unpack_var_sequence(tx), {})
+
","--- a/torch/csrc/jit/tensorexpr/py_wrapper.h
+++ b/torch/csrc/jit/tensorexpr/py_wrapper.h
@@ -595,6 +595,18 @@ class CondHigherOrderVariable(TorchHigherOrderOperatorVariable):
                 f""Expected 4 arguments but got {len(args)}.\n""
                 f""Usage: cond(pred, true_fn, false_fn, operands)"",
             )
+
+        // Specialize into one of the branches since pred is constant
+        if (args[0].type() == ConstantType::get()) {
+            log.warning(
+                ""Pred is a Python constant. When used with torch.cond, it executes only one of the branches.""
+                "" If you want torch.cond to preserve two branches, please make the predicate a boolean tensor or a SymBool.""
+            );
+            if (args[0].as_python_constant()) {
+                return args[1].call_function(tx, args[3].unpack_var_sequence(tx), {});
+            } else {
+                return args[2].call_function(tx, args[3].unpack_var_sequence(tx), {});
+            }
+        }
 
         // predicate
         if (args[0].type() != ConstantType::get() && args[0].type() != TensorType::get() && args[0].type() != SymType::get()) {
            unimplemented("
0,https://github.com/PyTorch/PyTorch/commit/fe3e6878c4bb2a6001045c179fd7fa9838242558,YES,torch/_higher_order_ops/cond.py,patch_0,,"+import logging
+
","--- a/pytorch_fix.py
+++ b/pytorch_fix.py
@@ -5,7 +5,6 @@ import itertools
 import logging
 import operator
 import re
-from collections import namedtuple
 from itertools import chain
 from typing import (
     Any,"
0,https://github.com/PyTorch/PyTorch/commit/fe3e6878c4bb2a6001045c179fd7fa9838242558,YES,torch/_higher_order_ops/cond.py,patch_1,,"+log = logging.getLogger(__name__)
+
","@@ -223,6 +223,7 @@ def set_logs(
     modules: Optional[Dict[str, Union[int, bool]]] = None,
     cudagraphs: bool = False,
     sym_node: bool = False,
+    compiled_autograd_verbose: bool = False,
 ):
     """"""
     Sets the log level for individual components and toggles individual log
"
0,https://github.com/PyTorch/PyTorch/commit/fe3e6878c4bb2a6001045c179fd7fa9838242558,YES,torch/_higher_order_ops/cond.py,patch_2,,"+    if isinstance(pred, (bool, int, float)):
+        log.warning(
+            ""Pred is a Python constant. When used with torch.cond, it executes only one of the branches.""
+            "" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.""
+        )
+        if pred:
+            return true_fn(*operands)
+        else:
+            return false_fn(*operands)
+
","@@ -107,6 +110,16 @@ def cond(pred, true_fn, false_fn, operands):
     if torch.compiler.is_dynamo_compiling():
         return cond_op(pred, true_fn, false_fn, operands)
 
+    if isinstance(pred, (bool, int, float)):
+        log.warning(
+            ""Pred is a Python constant. When used with torch.cond, it executes only one of the branches.""
+            "" If you want torch.cond to perserve two branches, please make the predicate a boolean tensor or a SymBool.""
+        )
+        if pred:
+            return true_fn(*operands)
+        else:
+            return false_fn(*operands)
+
     def _validate_input(pred, true_fn, false_fn, operands):
         if not isinstance(pred, (bool, torch.Tensor, torch.SymBool)):
             raise RuntimeError(f""Expected pred to be bool or tensor, but got {pred}."")"
0,https://github.com/PyTorch/PyTorch/commit/fe3e6878c4bb2a6001045c179fd7fa9838242558,YES,torch/_higher_order_ops/cond.py,patch_3,"-        ""call_function"", func_overload, proxy_args, {}, name=""conditional""
","+        ""call_function"", func_overload, proxy_args, {}
","@@ -200,7 +213,7 @@ def trace_cond(proxy_mode, func_overload, pred, true_fn, false_fn, operands):
     proxy_args = pytree.tree_map(proxy_mode.tracer.unwrap_proxy, args)
 
     out_proxy = proxy_mode.tracer.create_proxy(
-        ""call_function"", func_overload, proxy_args, {}, name=""conditional""
+        ""call_function"", func_overload, proxy_args, {}
     )
 
     # At this point, we're *guaranteed* that whether an output came from the"
0,https://github.com/PyTorch/PyTorch/commit/490d72e4e6eb07ed7c65ee6f7c48bb393483c01c,NO,cmake/Dependencies.cmake,patch_0,"-  if(USE_MAGMA)
-    find_package(MAGMA)
-  endif()
-  if((USE_CUDA OR USE_ROCM) AND MAGMA_FOUND)
-    set(USE_MAGMA 1)
-    message(STATUS ""Compiling with MAGMA support"")
-    message(STATUS ""MAGMA INCLUDE DIRECTORIES: ${MAGMA_INCLUDE_DIR}"")
-    message(STATUS ""MAGMA LIBRARIES: ${MAGMA_LIBRARIES}"")
-    message(STATUS ""MAGMA V2 check: ${MAGMA_V2}"")
-  else()
-    message(STATUS ""MAGMA not found. Compiling without MAGMA support"")
-    caffe2_update_option(USE_MAGMA OFF)
","+  if(USE_CUDA OR USE_ROCM)
+    if(USE_MAGMA)
+      find_package(MAGMA)
+      if(MAGMA_FOUND)
+        message(STATUS ""Compiling with MAGMA support"")
+        message(STATUS ""MAGMA INCLUDE DIRECTORIES: ${MAGMA_INCLUDE_DIR}"")
+        message(STATUS ""MAGMA LIBRARIES: ${MAGMA_LIBRARIES}"")
+        message(STATUS ""MAGMA V2 check: ${MAGMA_V2}"")
+      else()
+        message(STATUS ""MAGMA not found. Compiling without MAGMA support"")
+        caffe2_update_option(USE_MAGMA OFF)
+      endif()
+    endif()
","@@ -939,7 +939,6 @@ if(USE_ROCM)
    hip_add_library(torch_hip ${Caffe2_HIP_SRCS})
    if(USE_FLASH_ATTENTION)
      target_link_libraries(torch_hip PRIVATE __caffe2_aotriton)
-    add_dependencies(torch_hip aotriton_external)
    endif()
    set(CUDA_LINK_LIBRARIES_KEYWORD)
    torch_compile_options(torch_hip)  # see cmake/public/utils.cmake
+  if(USE_CUDA OR USE_ROCM)
+    if(USE_MAGMA)
+      find_package(MAGMA)
+      if(MAGMA_FOUND)
+        message(STATUS ""Compiling with MAGMA support"")
+        message(STATUS ""MAGMA INCLUDE DIRECTORIES: ${MAGMA_INCLUDE_DIR}"")
+        message(STATUS ""MAGMA LIBRARIES: ${MAGMA_LIBRARIES}"")
+        message(STATUS ""MAGMA V2 check: ${MAGMA_V2}"")
+      else()
+        message(STATUS ""MAGMA not found. Compiling without MAGMA support"")
+        caffe2_update_option(USE_MAGMA OFF)
+      endif()
+    endif()
+  endif()"
0,https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9,YES,.lintrunner.toml,patch_0,"-    'torch/distributed/distributed_c10d.py',
",No
0,https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9,YES,torch/_C/_distributed_c10d.pyi,patch_0,,No
0,https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9,YES,torch/_C/_distributed_c10d.pyi,patch_1,,No
0,https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9,YES,torch/_C/_distributed_c10d.pyi,patch_2,,No
0,https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9,YES,torch/_C/_distributed_c10d.pyi,patch_3,,No
0,https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9,YES,torch/_C/_distributed_c10d.pyi,patch_4,"-    class Options: ...
-
-    def __init__(self): ...
",No
0,https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9,YES,torch/_C/_distributed_c10d.pyi,patch_5,,No
0,https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9,YES,torch/_C/_distributed_c10d.pyi,patch_6,,No
0,https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9,YES,torch/_C/_distributed_c10d.pyi,patch_7,,No
0,https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9,YES,torch/_C/_distributed_c10d.pyi,patch_8,,No
0,https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9,YES,torch/_C/_distributed_c10d.pyi,patch_9,"-    wrapped_pg: ProcessGroup
-    class Options: ...
",No
0,https://github.com/PyTorch/PyTorch/commit/937d616e825e70b8d786c1f514ae9cec9c8d4ee9,YES,torch/_C/_distributed_c10d.pyi,patch_10,,No
