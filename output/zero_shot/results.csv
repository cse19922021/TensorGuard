0,https://github.com/PyTorch/PyTorch/commit/fbc7559ceb372d88b55c96ef6984accbaa0ec3ec,zero,YES,torch/_library/infer_schema.py,patch_0,"-        if param.annotation not in SUPPORTED_PARAM_TYPES.keys():
-        schema_type = SUPPORTED_PARAM_TYPES[param.annotation]
","+        # The annotation might be converted to a string by annotation,
+        # we convert it to the actual type.
+        annotation_type = param.annotation
+        if type(annotation_type) == str:
+            annotation_type = convert_type_string(annotation_type)
+
+        if annotation_type not in SUPPORTED_PARAM_TYPES.keys():
+        schema_type = SUPPORTED_PARAM_TYPES[annotation_type]
","+        if isinstance(param.annotation, str):
+            schema_type = eval(param.annotation)
+        elif param.annotation not in SUPPORTED_PARAM_TYPES.keys():
+            schema_type = SUPPORTED_PARAM_TYPES[param.annotation]","The root cause of the bug was that existing custom ops did not consider strings as valid types when using `from __future__ import annotations`, which converts type annotations to strings to facilitate forwards references. As a result, the custom ops would encounter errors when trying to use string type annotations. The fix involved checking if the argument and return type annotations were strings and attempting to convert them to the appropriate type using `eval`."
0,https://github.com/PyTorch/PyTorch/commit/fbc7559ceb372d88b55c96ef6984accbaa0ec3ec,zero,YES,torch/_library/infer_schema.py,patch_1,"-    ret = parse_return(sig.return_annotation, error_fn)
","+    return_annotation = sig.return_annotation
+    if type(return_annotation) == str:
+        return_annotation = convert_type_string(return_annotation)
+    ret = parse_return(return_annotation, error_fn)
","+    if isinstance(sig.return_annotation, str):
+        ret = parse_return(eval(sig.return_annotation), error_fn)
+    else:
+        ret = parse_return(sig.return_annotation, error_fn) ","The root cause of the bug was that the existing custom operations did not consider strings to be valid types for annotations. This was an issue because the `from __future__ import annotations` feature converts type annotations to strings to facilitate forwards references. The fix for the bug involved checking if the argument and return type annotations were strings, and using `eval` to convert them to actual types if they were."
0,https://github.com/PyTorch/PyTorch/commit/6d4ec9b2ecba7d26e885bdbb0faeeaa1e148cfd6,zero,NO,torch/distributed/checkpoint/planner_helpers.py,patch_0,"-    if isinstance(object, DTensor):
","+    if isinstance(object, _Checkpointable):
+        return object._create_write_items(fqn, object)
+    elif isinstance(object, DTensor):
+        # DTensor can contain a local tensor that is a tensor subclass
+        if isinstance(object.to_local(), _Checkpointable):
+            return object.to_local()._create_write_items(fqn, object)  # type: ignore[arg-type]
","+    if isinstance(object, DTensor) or isinstance(object, CheckpointableTensor):","The root cause of the bug is that the CheckpointableTensor interface was not implemented correctly, causing issues with DCP's ability to checkpoint tensor subclasses."
0,https://github.com/PyTorch/PyTorch/commit/6d4ec9b2ecba7d26e885bdbb0faeeaa1e148cfd6,zero,NO,torch/distributed/checkpoint/planner_helpers.py,patch_1,"-    if isinstance(tensor, DTensor):
","+    if isinstance(tensor, _Checkpointable):
+        local_chunks = tensor._create_chunk_list(tensor)
+    elif isinstance(tensor, DTensor):
+        # DTensor can contain a local tensor that is a tensor subclass
+        if isinstance(tensor.to_local(), _Checkpointable):
+            return tensor.to_local()._create_chunk_list(tensor)  # type: ignore[arg-type]
","+    if isinstance(tensor, DTensor) and hasattr(tensor, 'requires_grad') and local_tensor.requires_grad and not requires_grad:","The root cause of the bug is that the CheckpointableTensor interface was not properly implemented to support arbitrary tensor subclasses for checkpointing in DCP. This caused issues with TorchRec sharding migration from ShardedTensor to DTensor, impacting the functionality of the tensor subclasses."
0,https://github.com/PyTorch/PyTorch/commit/6d4ec9b2ecba7d26e885bdbb0faeeaa1e148cfd6,zero,NO,torch/distributed/checkpoint/utils.py,patch_0,"-    if isinstance(tensor, DTensor):
","+    if isinstance(tensor, _Checkpointable):
+        return tensor._get_tensor_shard(tensor, index)
+    elif isinstance(tensor, DTensor):
+        # DTensor can contain a local tensor that is a tensor subclass
+        if isinstance(tensor.to_local(), _Checkpointable):
+            return tensor.to_local()._get_tensor_shard(tensor, index)  # type: ignore[arg-type]
","+    # Params are DTensors in backward
+    # with SHARD_GRAD_OP + TP
+    from torch.distributed._tensor import DTensor

+    if isinstance(tensor, DTensor):
+        tensor = tensor._local_tensor","The root cause of the bug is that the CheckpointableTensor interface was not implemented correctly or was not implemented at all, causing issues with the compatibility of tensor subclasses with DCP."
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,zero,NO,torch/distributed/pipelining/PipelineStage.py,patch_0,,"+        output_node = self._get_output_node()
+        output_vals: Tuple[torch.Tensor] = tuple(
+            v.meta[""val""] for v in flatten_args(output_node.args)
+        )
+        self._configure_outputs_meta(output_vals)
+
+    def _get_output_node(self):
+        output_nodes = [node for node in self.submod.graph.nodes if node.op == ""output""]
+        assert len(output_nodes) == 1
+        output_node = output_nodes[0]
+        return output_node
+
","
-        # Verify input and output shapes and data types before proceeding
+        assert input_shape == expected_input_shape, ""Input shape does not match expected input shape""
+        assert input_dtype == expected_input_dtype, ""Input data type does not match expected input data type""
+        assert output_shape == expected_output_shape, ""Output shape does not match expected output shape""
+        assert output_dtype == expected_output_dtype, ""Output data type does not match expected output data type""","The root cause of the bug is related to the validation of the input and output shapes and data types in a pipeline stage. The bug arises from user errors such as unintentional dynamic shapes usage or a mismatch of configuration time and run time data shapes/dtypes. This can lead to different issues such as errors being thrown within the stage-module forward code, silent correctness issues inside the stage-module forward, or incorrect outputs that do not match the expectation of the following stage, resulting in a hang or correctness issue down the line.

The commit message indicates that by validating the input and output shapes and data types, the following can be addressed:
- Improve debugability of errors thrown within the stage-module forward code
- Guard against silent correctness issues happening inside the stage-module forward
- Prevent issues where the output of a stage does not match the expectation of the following stage

Overall, the validation of input and output shapes and data types helps ensure that the pipeline stages produce the expected outputs and can help prevent potential issues that may arise from shape/dtype mismatches."
0,https://github.com/PyTorch/PyTorch/commit/6b39146b3f33c2d00d2e1c0ae60ec32951333a08,zero,NO,torch/distributed/pipelining/PipelineStage.py,patch_1,"-        output_nodes = [node for node in self.submod.graph.nodes if node.op == ""output""]
-        assert len(output_nodes) == 1
-        output_node = output_nodes[0]
","+        output_node = self._get_output_node()
+
","-        output_nodes = [node for node in self.submod.graph.nodes if node.op == ""output""]
-        assert len(output_nodes) == 1
+        output_nodes = [node for node in self.submod.graph.nodes if node.op == ""output""]
+        assert len(output_nodes) == 1, ""There should be exactly one output node""
         output_node = output_nodes[0]","The root cause of the bug is related to shape and data type mismatches in the input and output of different stages in a pipeline. This can lead to various classes of user errors:

- Errors thrown within the stage-module forward code due to shape or data type mismatches, making it hard to understand or trace back to an input issue.
- Silent correctness issues occurring inside the stage-module forward, where the correct output shape is still produced, but the data may not be correct.
- Output from a stage-module may be locally correct but not matching the expectation of the following stage, leading to hangs or correctness issues down the line.

Validation of input shapes on the first stage and output shapes of all stages helps improve debuggability of errors, guards against silent correctness issues, and ensures that the output shapes match the expectations of subsequent stages. This validation is crucial as it directly affects the number of bytes on the wire."
0,https://github.com/PyTorch/PyTorch/commit/40e8675fcbb233c98ec532607d5cd421ec850253,zero,NO,aten/src/ATen/cudnn/Descriptors.h,patch_0,,"+  void set_v9(
+      cudnnDataType_t datatype,
+      cudnnLossNormalizationMode_t normMode,
+      cudnnCTCGradMode_t gradMode,
+      int maxLabelLength) {
+    AT_CUDNN_CHECK(
+        cudnnSetCTCLossDescriptor_v9(mut_desc(), datatype, normMode, gradMode, maxLabelLength));
+  }
+
","+          check_cudnn_hardware_support,
+          check_all_tensors_on_device,
+          check_cudnn_tensor_shapes,
+          check_cudnn_layout,
+          check_for_nested_inputs,
+          check_cudnn_requires_grad,
+          check_dtypes_low_precision);","The root cause of the bug is related to the introduction of a graph-capturable CTCLoss in cuDNN v8.x, which aligns neatly with the `Tensor` variant. However, there is a restriction in cuDNN on the maximum target length (255) which is not checkable during graph capture. The fix involved checking the maximum target length restriction during warmup runs and aborting the capture if this constraint was violated. Considerations around warning/error messages needed to be adjusted to ensure a better user experience."
0,https://github.com/PyTorch/PyTorch/commit/40e8675fcbb233c98ec532607d5cd421ec850253,zero,NO,aten/src/ATen/native/LossCTC.cpp,patch_0,"-          {log_probs, targets, input_lengths, target_lengths})) {
","+  // we don't want to convert to IntArrayRef if we can dispatch to cuDNN (this allows graph-capturable ctc_loss)
+  bool use_cudnn =
+      (log_probs.device().type() == at::kCUDA) &&
+      at::_use_cudnn_ctc_loss(
+          log_probs, targets, input_lengths, target_lengths, BLANK);
+          {log_probs, targets, input_lengths, target_lengths}) || use_cudnn) {
","+from .utils import get_dtype_size, has_incompatible_cudagraph_ops, output_node","The bug was caused by not checking the restriction on the maximum target length (255) in the graph-capture case when using cuDNN v8.x's CTCLoss. As a result, the constraint was only enforced during the warmup run(s) and the capture would fail if the constraint was violated during warmup. This caused potential issues with user experience around warnings and error messages."
0,https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba,zero,YES,torch/_dynamo/repro/after_aot.py,patch_0,"-        if not same_two_models(mod, compiled, args, only_fwd=True):
","+        if not same_two_models(
+            mod,
+            compiled,
+            args,
+            only_fwd=True,
+            ignore_non_fp=config.repro_ignore_non_fp,
+        ):
",+        if not _disable_forced_specializations and not self._is_supported_equivalence(expr):,"The root cause of the bug is that the minifier, when minifying, ignores non-floating point values by default but still checks them when running the initial graph dump step. This inconsistency can lead to capturing a graph that doesn't fail the tester but has no meaningful divergence. This can be problematic when dealing with functions like `elu(x)` whose derivative depends on `x > 0`, as non-floating point values can trigger accuracy failures. To address this issue, a config flag was added to allow ignoring these non-floating point values."
0,https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba,zero,YES,torch/_dynamo/repro/after_dynamo.py,patch_0,"-                if backend_accuracy_fails(gm, example_inputs, compiler_fn):
","+                if _accuracy_fails(gm, example_inputs, compiler_fn):
","+                if backend_accuracy_fails(gm, example_inputs, compiler_fn, ignore_non_float=True):","The root cause of the bug is that during minification, non-floating point values were being ignored by default, leading to the creation of graphs that did not accurately reflect the behavior of the original code. This could result in inaccurate results when running tests that depend on these values. By adding a new config flag to ignore these non-floating point values, the issue was resolved."
0,https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba,zero,YES,torch/_dynamo/repro/after_dynamo.py,patch_1,"-    if backend_accuracy_fails(
-        gm, example_inputs, compiler_fn, only_fwd=config.repro_forward_only
-    ):
-            backend_accuracy_fails,
-            only_fwd=config.repro_forward_only,
","+    if _accuracy_fails(gm, example_inputs, compiler_fn):
+            _accuracy_fails,
","+        if backend_accuracy_fails(
+            gm, example_inputs, compiler_fn, only_fwd=config.repro_forward_only,
+            ignore_non_float=config.ignore_non_float
+        ):
+            backend_accuracy_fails,
+            ignore_non_float=config.ignore_non_float","The root cause of the bug is that the after-aot minifier in the codebase was ignoring non-floating point values by default during minification, but it was checking them when running the initial graph dump step. This caused a discrepancy where certain values were being captured in the graph that didn't fail the tester but led to accuracy failures later on. The bug was fixed by adding a configuration flag to ignore these non-floating point values."
0,https://github.com/PyTorch/PyTorch/commit/8865425ff7f68a172f02963f2734e4d5b005cdba,zero,YES,torch/_dynamo/repro/after_dynamo.py,patch_2,"-            if not same_two_models(mod, opt_mod, args):
","+            if not same_two_models(
+                mod,
+                opt_mod,
+                args,
+                only_fwd=config.repro_forward_only,
+                ignore_non_fp=config.repro_ignore_non_fp,
+            ):
","+            if (
+                type(mod) == type(opt_mod) and
+                mod.dtype == opt_mod.dtype
+            ):
+                continue","The root cause of the bug is that the minifier was ignoring non-floating point values by default during the minification process, but still checking them during the initial graph dump step. This caused a discrepancy where certain non-floating point values were included in the graph output, leading to accuracy failures in certain cases. The issue was resolved by adding a configuration flag to ignore these non-floating point values during the minification process."
0,https://github.com/PyTorch/PyTorch/commit/e14d1d10ef9d24bf43366ac1f05a5aa8b732707b,zero,YES,torch/_inductor/codegen/simd.py,patch_0,"-        return self.codegen_indexing(self.simplify_indexing(index))
","+        simp_index = self.simplify_indexing(index)
+
+        # Now that we are done simplifying we can unwrap Identity so that downstream handling
+        # for its contained expression will work. previously, tl.full wrapping of sympy.Integer
+        # would not occur
+        simp_index = (
+            simp_index if not isinstance(simp_index, Identity) else simp_index.args[0]
+        )
+
+        return self.codegen_indexing(simp_index)
","+
+        index = self.simplify_indexing(index)
+        return self.codegen_indexing(index)","The root cause of the bug was that the indexing calculation was wrapped in `Identity`, which prevented the index from simplifying to an integer. This prevented the code from hitting an intended path that would do wrapping with `tl.full`. The issue occurred because int32 intermediates were not being expanded to int64 when necessary."
0,https://github.com/PyTorch/PyTorch/commit/0f81473d7b4a1bf09246410712df22541be7caf3,zero,YES,torch/_refs/__init__.py,patch_0,,"+    if isinstance(a, TensorLike) and isinstance(b, TensorLike):
+        torch._check(
+            not utils.is_boolean_dtype(a.dtype) and not utils.is_boolean_dtype(b.dtype),
+            lambda: (
+                ""Subtraction, the `-` operator, with two bool tensors is not supported. ""
+                ""Use the `^` or `logical_xor()` operator instead.""
+            ),
+        )
+
","+    if isinstance(vec1, bool) or isinstance(vec2, bool):
+        torch._check(
+            utils.is_boolean_dtype(self.dtype),
+            lambda: f""Boolean alpha or beta only supported for Boolean results."",
+        )",The root cause of the bug was that the error checks for subtracting a fake tensor from a bool tensor were not updated. This led to incorrect behavior or crashes when performing this operation.
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,zero,YES,torch/_subclasses/fake_tensor.py,patch_0,"-        if output is unassigned:
","+        if output is _UNASSIGNED:
","+        if output == self.none_str:
+            continue","The root cause of the bug was that there were mypy errors present in `fake_tensor.py` that were being ignored. This led to potential type mismatches and type errors that were not being caught during static type checking. To address this issue, the mypy errors were fixed and a new tagged wrapper, `SafePyObjectT<T>`, was introduced to provide static type checking for `SafePyObject`. Additionally, `SafePyObjectT<TorchDispatchModeKey>` was used in some of the `TorchDispatchModeTLS` API to ensure that the correct type is being used."
0,https://github.com/PyTorch/PyTorch/commit/609c958281e2142a9a9911cdb383dcac7d2af332,zero,YES,torch/_subclasses/fake_tensor.py,patch_1,"-        assert not metadata.is_sparse
","+        assert metadata and not metadata.is_sparse
","+        assert not metadata.is_sparse
+        assert not is_fake(
+            buffer
+        assert not is_fake(
+            param","The root cause of the bug was that fake_tensor.py had mypy errors ignored, which is not desirable. Additionally, the commit introduced a new type `SafePyObjectT<T>` to provide static type checking, and used `SafePyObjectT<TorchDispatchModeKey>` in some of the TorchDispatchModeTLS API to ensure type safety."
0,https://github.com/PyTorch/PyTorch/commit/4240304da4ddc42335b0219bae11f072ca240fe5,zero,YES,torch/distributed/elastic/multiprocessing/errors/__init__.py,patch_0,,"+            except SystemExit as se:
+                # For run_path based entrypoints, SystemExit with code = 0 will never exit.
+                # Handling it here by returning a value:
+                if se.code == 0:
+                    return None
+                else:
+                    raise
","        if exitstatus == 0:
+           return None
          if not self.run_single:","The root cause of the bug was that the `--run-path` option failed to exit when the script exited with a non-error status code (exit code 0). The fix addressed this issue by ensuring that when the script exits with code 0, the return value is set to None so that the MP code can detect an exit properly. This resolved the issue of the script not exiting when it should have with a non-error status code."
0,https://github.com/PyTorch/PyTorch/commit/7a4e4511845dbeefe4d16c321b2a93ac72b76d93,zero,YES,torch/_dynamo/variables/torch.py,patch_0,"-                any(has_torch_function(a) for a in args),
","+            elems = (
+                args[0].unpack_var_sequence(tx)
+                if len(args) == 1 and isinstance(args[0], TupleVariable)
+                else args
+            )
+                any(has_torch_function(x) for x in elems),
","+                any(has_torch_function(a) if not isinstance(a, Tuple) else any(has_torch_function(i) for i in a) for a in args),","The root cause of the bug was that the code intended to iterate over each element to check for the existence of `__torch_function__`, but encountered a `TupleVariable` instead of the expected behavior. This was due to the use of the ordinary `has_torch_function()` function, which required further unpacking in this specific case. This issue was fixed by properly handling the unpacking process."
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,zero,YES,torch/_ops.py,patch_0,"-
","+    if mode_stack._schema_check_mode is not None:
+        return unset_mode_pre_dispatch(None, schema_check=True)
","+    if len(schema.returns) == 1 and isinstance(schema.returns[0].type, torch.NoneType):
+        # Skip schema returns -> None
+        return True","The root cause of the bug is that there are certain ops in PyTorch which claim to be functional in their schemas but actually exhibit aliasing or mutating behavior when running on concrete inputs. This discrepancy was identified during the new SchemaCheckMode testing for pre-dispatch export, leading to the creation of a new dispatch mode called PreDispatchSchemaCheckMode. Ops such as 'aten.atleast_1d.default', 'aten.atleast_2d.default', 'aten.atleast_3d.default', 'aten.cartesian_prod.default', 'aten.conj_physical.default', 'aten.alpha_dropout.default', 'aten.feature_dropout.default', 'aten.feature_alpha_dropout.default', and 'aten.unsafe_chunk.default' were found to fail the testing due to claiming to be functional but showing aliasing or mutating behavior. These ops need to be further analyzed and potentially revised to ensure consistency between their stated functionality and actual behavior."
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,zero,YES,torch/utils/_python_dispatch.py,patch_0,,"+    has_schema_check_mode_in_pre_dispatch = False
+        if isinstance(i, SchemaCheckMode):
+            has_schema_check_mode_in_pre_dispatch = True
","+    if len(schema.returns) == 1 and isinstance(schema.returns[0].type, torch.NoneType):
+        # Skip schema returns -> None
+        return True","The root cause of the bug is that certain ops in PyTorch were incorrectly claiming to be functional, but were actually aliasing or mutating the input data. The new dispatch mode, PreDispatchSchemaCheckMode, was added to verify op schemas for functionalization for PreDispatch IR by running in eager mode on concrete inputs. The testing identified that ops such as `aten.atleast_1d.default`, `aten.atleast_2d.default`, `aten.atleast_3d.default`, `aten.cartesian_prod.default`, `aten.conj_physical.default`, `aten.alpha_dropout.default`, `aten.feature_dropout.default`, `aten.feature_alpha_dropout.default`, and `aten.unsafe_chunk.default` were failing the schema check for incorrect functionalization claims. This could lead to issues when these ops were decomposed further without proper validation."
0,https://github.com/PyTorch/PyTorch/commit/2973c9bb884df8f3d7e846738ee6679fb022609e,zero,YES,torch/utils/_python_dispatch.py,patch_1,,"+        if (
+            isinstance(old, SchemaCheckMode)
+            and has_schema_check_mode_in_pre_dispatch
+        ):
+            raise AssertionError(
+                ""Can't have SchemaCheckMode available both in PreDispatch and Python Key""
+            )
","+        # Check if an impure function based on schema.
+            schema = getattr(self.target, ""_schema"", None)
+            schema_mutable = schema is not None and schema.is_mutable
+            return schema_mutable or self.target in _side_effectful_functions","The root cause of the bug is that during the testing of the new PreDispatchSchemaCheckMode, it was found that certain ops were failing the schema check for functionalization. These ops, which include `aten.atleast_1d.default`, `aten.atleast_2d.default`, `aten.atleast_3d.default`, `aten.cartesian_prod.default`, `aten.conj_physical.default`, `aten.alpha_dropout.default`, `aten.feature_dropout.default`, `aten.feature_alpha_dropout.default`, and `aten.unsafe_chunk.default`, were found to incorrectly claim to be functional while exhibiting aliasing or mutating behavior. This led to the ops failing the schema check and needing to be addressed before further decomposition."
0,https://github.com/PyTorch/PyTorch/commit/e98135d1ad2f999fec649ecd21b35f3d5676be43,zero,YES,torch/_functorch/_aot_autograd/runtime_wrappers.py,patch_0,"-            with torch.autograd._force_original_view_tracking(True):
","+
+            # It's possible to have trace_joint inside user specified with no_grad() region,
+            # if there is a nested with enable_grad(), that forces some outputs to require gradients.
+            # Therefore, we unconditionally turn on enable_grad() for compiled_fn execution.
+            with torch.autograd._force_original_view_tracking(
+                True
+            ), torch.enable_grad():
","
+        with torch.autograd._force_original_view_tracking(True), torch.enable_grad():","The root cause of the bug was that the needs_autograd function was checking torch.is_grad_enabled() instead of only checking if any of the inputs required grad. This caused the compiled region to always run under with.enable_grad(), even in scenarios where it should not have. Additionally, there were changes in the partitioner that caused differences in the return container (list/tuple) between inference and training graphs. This caused issues with models like hf_Reformer during inference, where outputs were aliases to inputs that were nn.Parameters with requires_grad=True. This led to compiling the training graph instead of the inference graph. fix was made in aot_autograd.py to handle outputs that are aliases to inputs that require grad, ensuring that the outputs' requires_grad status does not trigger the generation of a training graph."
0,https://github.com/PyTorch/PyTorch/commit/e98135d1ad2f999fec649ecd21b35f3d5676be43,zero,YES,torch/_functorch/aot_autograd.py,patch_0,"-        needs_autograd = (
-            any(x.requires_grad for x in fake_flat_args if isinstance(x, Tensor))
-            and torch.is_grad_enabled()
","+        needs_autograd = any(
+            x.requires_grad for x in fake_flat_args if isinstance(x, Tensor)
","+        needs_autograd = any(
+            x.requires_grad for x in fake_flat_args if isinstance(x, Tensor)
+        )","The root cause of the bug was that the needs_autograd function was checking torch.is_grad_enabled() instead of solely focusing on whether any of the inputs require gradients. This led to a situation where even if a user ran a region under no_grad(), if there was an input Tensor with requires_grad, the function would still go into the training scenario instead of the inference scenario. This caused issues with compiled graphs and resulted in incorrect behavior during inference, such as in the case of the hf_Reformer model. The fix involved ensuring that needs_autograd only checks for inputs that require gradients and handling outputs that are aliases to inputs that require gradients appropriately to avoid generating a training graph when not needed."
0,https://github.com/PyTorch/PyTorch/commit/606c4f1367c7eb4a49aa4a9538dd2b1eb92485d6,zero,YES,torch/distributed/_shard/sharded_tensor/utils.py,patch_0,"-    if not c10d._rank_not_in_group(pg):
","+    if rank is not None and not c10d._rank_not_in_group(pg):
",+    if dist.get_rank(group) == 0:,"The root cause of the bug is that the rank validation changes introduced in a previous commit (https://github.com/pytorch/pytorch/pull/123230) to support sub groups inadvertently broke some unit tests. Although some of the failing tests were fixed in a subsequent commit (https://github.com/pytorch/pytorch/pull/123778), there was still one remaining failure reported by DanilBaibak. This commit (https://github.com/pytorch/pytorch/pull/124103) aims to fix the remaining failing test by addressing the issues introduced by the rank validation changes."
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,zero,YES,torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,patch_0,,"+  if (enableNanCheck_) {
+    checkForNan(input);
+  }
","+  if (enableNanCheck_) {
+    checkForNan(input);
+  }",The root cause of the bug is that the NAN check was being performed on the GPU side without copying the necessary data from GPU to CPU. This led to runtime errors during collective operations in the unit test for collectives.
0,https://github.com/PyTorch/PyTorch/commit/6db32710074f0944305b2d1e4571bb4ce571bf6a,zero,YES,torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,patch_1,,"+  if (enableNanCheck_) {
+    checkForNan(tensor);
+  }
","+  if (enableNanCheck_) {
+    checkForNan(tensor);
+  }","The root cause of the bug is that the NAN check was implemented using device-side assert without copying the necessary data from the GPU to the CPU. This caused runtime errors during collective operations, as shown in the test output where assertions for NAN values failed, triggering device-side assert errors."
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,zero,NO,torch/_library/infer_schema.py,patch_0,,"+            elif isinstance(param.default, torch.dtype):
+                dtype_repr = str(param.default)
+                torch_dot = ""torch.""
+                assert dtype_repr.startswith(torch_dot)
+                default_repr = dtype_repr[len(torch_dot) :]
","+def value_to_dtype(value: Any) -> torch.dtype:
+    if isinstance(value, sympy.Expr):
+        if value.is_integer:  # type: ignore[attr-defined]
+            return torch.long
+        if value.is_real:
+            return torch.get_default_dtype()
+    return type_to_dtype(type(value))",The root cause of the bug is that the dtype-string utilities were not properly organized and the parser did not check for dtype names before pulling the C++ dtype mapping. This led to issues with default dtype values.
0,https://github.com/PyTorch/PyTorch/commit/856541c701f10e075c13cb4be31006ac234fa451,zero,NO,torch/csrc/jit/frontend/function_schema_parser.cpp,patch_0,,"+        } else if (
+            isPossiblyOptionalScalarType(real_type) &&
+            str2dtype.count(text) > 0) {
+          return static_cast<int64_t>(str2dtype.at(text));
","+    if (allow_typevars_ && !text.empty() && islower(text[0])) {
+      if (text == ""double"") {
+        throw ErrorReport(tok.range)
+            << ""Use `float` instead of `double` in an operator's schema string. ""
+               ""`float` in schema corresponds to the double type in C++"";
+      }
+      if (text == ""int64_t"") {
+        throw ErrorReport(tok.range)
+            << ""Use `SymInt` or `int` instead of `int64_t` in an operator's schema string. ""
+               ""`SymInt` corresponds to c10::SymInt in C++ while `int` in schema corresponds ""
+               ""to the int64_t type in C++."";
+      }
+      throw ErrorReport(tok.range)
+          << ""unknown type specifier. Common valid schema types include ""
+             ""Tensor, SymInt, int, float, bool, Scalar; ""
+             ""for a full list, please see ""
+             ""https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#func "";
+    }","The root cause of the bug is related to the handling of default dtype values in custom operations. Prior to the changes introduced in the mentioned pull request, the dtype-string utilities were not properly integrated with the ScalarType class, leading to issues when determining the C++ dtype from the dtype name. This caused errors or unexpected behavior when working with default dtype values in custom operations. The changes in the PR aim to address this by enhancing the dtype-string utilities and ensuring that the parser checks if the string represents a dtype name, retrieving the corresponding C++ dtype from the mapping."
0,https://github.com/PyTorch/PyTorch/commit/41902a6ebc1806e7f4d6ce1da604cc9921c6515e,zero,NO,torch/_dynamo/eval_frame.py,patch_0,"-            if torch.fx._symbolic_trace.is_fx_tracing() and not isinstance(
-                self, DisableContext
-            ):
","+        if isinstance(self, DisableContext):
+            is_jit_tracing = always_false
+            is_fx_tracing = always_false
+        else:
+            is_jit_tracing = torch._C._is_tracing
+            is_fx_tracing = torch.fx._symbolic_trace.is_fx_tracing
+
+            if is_fx_tracing():
","+    tracing_context = torch._guards.TracingContext.get()
+    assert tracing_context is not None
+    params_flat = tracing_context.params_flat
+    assert params_flat is not None","The root cause of the bug was inefficient tracing checks in the Dynamo compiler, which were causing unnecessary overhead in the code execution. By optimizing the `is_tracing` checks, the overhead was reduced from 10.4us to 9.9us in the microbenchmark, resulting in improved performance."
0,https://github.com/PyTorch/PyTorch/commit/41902a6ebc1806e7f4d6ce1da604cc9921c6515e,zero,NO,torch/_dynamo/eval_frame.py,patch_1,"-            if torch.jit.is_tracing():
","+            if is_jit_tracing():
",+            if not torch.jit._tracer.is_tracing(use_jit=False):,"The root cause of the bug was inefficient code related to checking whether tracing is enabled. This inefficiency was optimized in the pull request, resulting in a performance improvement in the benchmarks for overheads in the Dynamo library."
0,https://github.com/PyTorch/PyTorch/commit/eb1d6ed9f9d9731401b04382f526a64e6d27b6e6,zero,YES,torch/_inductor/fx_passes/mkldnn_fusion.py,patch_0,,"+        # mkldnn linear only supports beta=1or0 and alpha=1
+        if linear_node.target == aten.addmm.default:
+            alpha = linear_node.kwargs.get(""alpha"", 1.0)
+            beta = linear_node.kwargs.get(""beta"", 1.0)
+            if (beta != 0.0 and beta != 1.0) or alpha != 1.0:
+                return False
","+#if IDEEP_PREREQ(3, 1, 0, 1)
+#elif IDEEP_PREREQ(3, 1, 0, 0)
      wgt_scales[i] = 1.0f / weight.q_per_channel_scales()[i].item<float>(); // Scales of ONEDNN and PyTorch are reciprocal","The bug was caused by a mismatch between the parameters accepted by `mkl_linear` or `mkldnn_linear` and the parameters `beta` and `alpha` being used in the `addmm` fusion check. This check has been disabled for cases where `beta` is not equal to 1 or 0, or `alpha` is not equal to 1, in order to prevent any functional issues."
0,https://github.com/PyTorch/PyTorch/commit/eb1d6ed9f9d9731401b04382f526a64e6d27b6e6,zero,YES,torch/_inductor/fx_passes/mkldnn_fusion.py,patch_1,"-            bias = None if linear_node.target == aten.mm.default else args[0]
","+            bias = (
+                None
+                if linear_node.target == aten.mm.default
+                or (
+                    linear_node.target == aten.addmm.default
+                    and linear_node.kwargs.get(""beta"", 1.0) == 0.0
+                )
+                else args[0]
+            )
","+            if linear_node.target == aten.mm.default and beta == 1 and alpha == 1:
+                bias = None
+            else:
+                bias = args[0]","The root cause of the bug was that the `addmm` fusion check did not take into account cases where `beta` was not equal to 1 or 0, or where `alpha` was not equal to 1. This caused a functional issue because the `mkl_linear` or `mkldnn_linear` functions do not accept `beta` or `alpha` as parameters. To address this issue, the pattern match for `addmm` was disabled in these specific cases."
